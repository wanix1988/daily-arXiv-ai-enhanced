<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 69]
- [cs.CL](#cs.CL) [Total: 63]
- [cs.AI](#cs.AI) [Total: 19]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.PF](#cs.PF) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Reading or Reasoning? Format Decoupled Reinforcement Learning for Document OCR](https://arxiv.org/abs/2601.08834)
*Yufeng Zhong,Lei Chen,Zhixiong Zeng,Xuanle Zhao,Deyang Jiang,Liming Zheng,Jing Huang,Haibo Qiu,Peng Shi,Siqi Yang,Lin Ma*

Main category: cs.CV

TL;DR: 本文提出了一种名为格式解耦强化学习（FD-RL）的方法，通过利用高熵模式对格式密集型实例进行目标优化，从而提高了OCR模型在处理格式敏感文档时的性能。FD-RL在OmniDocBench基准测试中取得了90.41的平均得分，创造了新的记录。


<details>
  <summary>Details</summary>
Motivation: 现有的OCR模型在处理格式敏感文档时表现出较高的输出不确定性，因此需要一种新的方法来提高OCR模型在这些文档上的性能。

Method: 本文提出了一种新的格式解耦强化学习（FD-RL）方法。该方法通过熵为基础的数据过滤策略识别格式密集型实例，并采用针对不同格式类型的设计的格式解耦奖励，允许在格式级别上进行验证，而不是在标记级别上进行记忆。

Result: FD-RL在OmniDocBench基准测试中达到了90.41的平均得分，创下了此基准测试中的新纪录。

Conclusion: 本文提出的FD-RL方法在处理格式敏感文档时表现出色，并且在广泛的研究中得到了充分验证。

Abstract: Reading text from images or scanned documents via OCR models has been a longstanding focus of researchers. Intuitively, text reading is perceived as a straightforward perceptual task, and existing work primarily focuses on constructing enriched data engineering to enhance SFT capabilities. In this work, we observe that even advanced OCR models exhibit significantly higher entropy in formatted text (\emph{e.g.}, formula, table, etc.) compared to plain text, often by an order of magnitude. These statistical patterns reveal that advanced OCR models struggle with high output uncertainty when dealing with format sensitive document, suggesting that reasoning over diverse reading pathways may improve OCR performance. To address this, we propose format decoupled reinforcement learning (FD-RL), which leverages high-entropy patterns for targeted optimization. Our approach employs entropy-based data filtration strategy to identify format-intensive instances, and adopt format decoupled rewards tailored to different format types, enabling format-level validation rather than token-level memorization. FD-RL achieves an average score of 90.41 on OmniDocBench, setting a new record for end-to-end models on this highly popular benchmark. More importantly, we conduct comprehensive ablation studies over data, training, filtering, and rewarding strategies, thoroughly validating their effectiveness.

</details>


### [2] [Bias Detection and Rotation-Robustness Mitigation in Vision-Language Models and Generative Image Models](https://arxiv.org/abs/2601.08860)
*Tarannum Mithila*

Main category: cs.CV

TL;DR: 本文研究了先进视觉语言和生成模型在图像旋转和分布偏移下的偏见传播和鲁棒性退化问题，提出了一种结合数据增强、表示对齐和模型正则化的鲁棒性增强策略，实验表明该方法能显著提高鲁棒性并减少偏见放大，同时保持整体性能。


<details>
  <summary>Details</summary>
Motivation: 探讨视觉语言模型和生成模型在转换输入时的鲁棒性和公平性，尤其是在图像旋转和分布变化方面，解决现有系统中的关键局限性。

Method: 通过对旋转扰动影响模型预测、信心校准和人口统计偏见模式的分析，提出结合数据增强、表示对齐和模型层面正则化的鲁棒性缓解策略。

Result: 在多个数据集上的实验结果证明，所提方法显著提高了鲁棒性并减少了偏见放大，未牺牲整体性能。

Conclusion: 本研究强调了当前多模态系统的关键限制，并提供了构建更可靠和公平的AI模型的实用缓解技术。

Abstract: Vision-Language Models (VLMs) and generative image models have achieved remarkable performance across multimodal tasks, yet their robustness and fairness under input transformations remain insufficiently explored. This work investigates bias propagation and robustness degradation in state-of-the-art vision-language and generative models, with a particular focus on image rotation and distributional shifts. We analyze how rotation-induced perturbations affect model predictions, confidence calibration, and demographic bias patterns. To address these issues, we propose rotation-robust mitigation strategies that combine data augmentation, representation alignment, and model-level regularization. Experimental results across multiple datasets demonstrate that the proposed methods significantly improve robustness while reducing bias amplification without sacrificing overall performance. This study highlights critical limitations of current multimodal systems and provides practical mitigation techniques for building more reliable and fair AI models.

</details>


### [3] [R$^2$BD: A Reconstruction-Based Method for Generalizable and Efficient Detection of Fake Images](https://arxiv.org/abs/2601.08867)
*Qingyu Liu,Zhongjie Ba,Jianmin Guo,Qiu Wang,Zhibo Wang,Jie Shi,Kui Ren*

Main category: cs.CV

TL;DR: 本文提出了一种名为R$^2$BD的新颖伪造图像检测框架，该框架通过引入统一重建模型G-LDM和残差偏置计算模块，实现了比现有方法更快的检测速度，并且在多个数据集上表现出优越的检测准确度。


<details>
  <summary>Details</summary>
Motivation: 现有基于重建的方法依赖于多步骤的逆向和重建过程，且大多使用扩散模型作为基础，限制了对其他生成模型（如GANs）的泛化能力。本文旨在提供一种新的检测框架，以解决上述问题。

Method: R$^2$BD框架包含两个关键设计：1. 统一重建模型G-LDM，能模拟VAEs、GANs和扩散模型的行为，以扩展检测的适用范围；2. 残差偏置计算模块，允现单一推理步骤区分真实和伪造的图像，提高效率。

Result: 相比于现有最先进方法，R$^2$BD在多个基准数据集上快了22多倍，并且在跨数据集评估中平均超过9%的性能提升，展示了强大的效率和跨不同生成模型的一致泛化能力。

Conclusion: R$^2$BD通过其创新的检测方法，有效提升了伪造图像检测的效率和应用于不同类型生成模型的通用性，为相关研究提供了新的思路和方法。

Abstract: Recently, reconstruction-based methods have gained attention for AIGC image detection. These methods leverage pre-trained diffusion models to reconstruct inputs and measure residuals for distinguishing real from fake images. Their key advantage lies in reducing reliance on dataset-specific artifacts and improving generalization under distribution shifts. However, they are limited by significant inefficiency due to multi-step inversion and reconstruction, and their reliance on diffusion backbones further limits generalization to other generative paradigms such as GANs.
  In this paper, we propose a novel fake image detection framework, called R$^2$BD, built upon two key designs: (1) G-LDM, a unified reconstruction model that simulates the generation behaviors of VAEs, GANs, and diffusion models, thereby broadening the detection scope beyond prior diffusion-only approaches; and (2) a residual bias calculation module that distinguishes real and fake images in a single inference step, which is a significant efficiency improvement over existing methods that typically require 20$+$ steps.
  Extensive experiments on the benchmark from 10 public datasets demonstrate that R$^2$BD is over 22$\times$ faster than existing reconstruction-based methods while achieving superior detection accuracy. In cross-dataset evaluations, it outperforms state-of-the-art methods by an average of 13.87\%, showing strong efficiency and generalization across diverse generative methods. The code and dataset used for evaluation are available at https://github.com/QingyuLiu/RRBD.

</details>


### [4] [ForensicFormer: Hierarchical Multi-Scale Reasoning for Cross-Domain Image Forgery Detection](https://arxiv.org/abs/2601.08873)
*Hema Hariharan Samson*

Main category: cs.CV

TL;DR: ForensicFormer 提出了一种多尺度框架，用于跨领域伪造检测，达到了86.8%的平均准确率，相比之下，之前的单一范式方法仅达到75%的准确率，特别是在JPEG压缩和GAN生成图像测试方面，它表现出了更好的鲁棒性和定位伪造区域的能力。


<details>
  <summary>Details</summary>
Motivation: 为了应对AI生成的图像和复杂编辑工具带来的挑战，传统的图像法医手段难以有效检测跨领域的图像伪造。因此，需要一种更强大和安全的方法来解决这一问题。

Method: ForensicFormer 使用了一种层次化的多尺度框架，结合低级别特征的检测、中层次边界的分析和高级别语义推理。通过跨注意力变换器来统一这三个阶段。

Result: ForensicFormer 在多种不同类型的数据集上平均达到86.8%的准确率，尤其是在JPEG压缩和基于GAN的图像中表现出色，JPEG压缩70时准确率为83%，且能够实现像素级的伪造定位，F1分数为0.76。

Conclusion: 研究结果表明，ForensicFormer 框架在未知操纵技术的现实应用中提供了有效的法医解决方案。各层次组件的贡献促使准确率提高了4-10%，这表明该方法是全面而有效的。

Abstract: The proliferation of AI-generated imagery and sophisticated editing tools has rendered traditional forensic methods ineffective for cross-domain forgery detection. We present ForensicFormer, a hierarchical multi-scale framework that unifies low-level artifact detection, mid-level boundary analysis, and high-level semantic reasoning via cross-attention transformers. Unlike prior single-paradigm approaches, which achieve <75% accuracy on out-of-distribution datasets, our method maintains 86.8% average accuracy across seven diverse test sets, spanning traditional manipulations, GAN-generated images, and diffusion model outputs - a significant improvement over state-of-the-art universal detectors. We demonstrate superior robustness to JPEG compression (83% accuracy at Q=70 vs. 66% for baselines) and provide pixel-level forgery localization with a 0.76 F1-score. Extensive ablation studies validate that each hierarchical component contributes 4-10% accuracy improvement, and qualitative analysis reveals interpretable forensic features aligned with human expert reasoning. Our work bridges classical image forensics and modern deep learning, offering a practical solution for real-world deployment where manipulation techniques are unknown a priori.

</details>


### [5] [Learning Domain-Invariant Representations for Cross-Domain Image Registration via Scene-Appearance Disentanglement](https://arxiv.org/abs/2601.08875)
*Jiahao Qin,Yiwen Wang*

Main category: cs.CV

TL;DR: SAR-Net 提出了一种统一框架，通过将图像分解为不变场景表示和特定领域外观代码来解决域移位下图像配准的挑战，显著改进了基线方法的效果，同时保持实时性能。


<details>
  <summary>Details</summary>
Motivation: 现有的图像配准方法在遇到系统强度差异时效果不佳，该论文旨在提出一种新框架来克服这一问题。

Method: SAR-Net 方法利用场景和外观的分离表示，并通过重建方法而非直接强度匹配来实现配准。

Result: 实验中，SAR-Net 在显微镜成像中表现突出，SSIM 达到 0.885，NCC 达到 0.979，比最强基线提高了 3.1 倍，同时保持了实时性能。

Conclusion: 证明场景一致性损失在共享潜在空间中提供了几何对应性的充分条件。消融研究显示两个损失项都至关重要，单个缺失会导致显著性能下降。

Abstract: Image registration under domain shift remains a fundamental challenge in computer vision and medical imaging: when source and target images exhibit systematic intensity differences, the brightness constancy assumption underlying conventional registration methods is violated, rendering correspondence estimation ill-posed. We propose SAR-Net, a unified framework that addresses this challenge through principled scene-appearance disentanglement. Our key insight is that observed images can be decomposed into domain-invariant scene representations and domain-specific appearance codes, enabling registration via re-rendering rather than direct intensity matching. We establish theoretical conditions under which this decomposition enables consistent cross-domain alignment (Proposition 1) and prove that our scene consistency loss provides a sufficient condition for geometric correspondence in the shared latent space (Proposition 2). Empirically, we validate SAR-Net on bidirectional scanning microscopy, where coupled domain shift and geometric distortion create a challenging real-world testbed. Our method achieves 0.885 SSIM and 0.979 NCC, representing 3.1x improvement over the strongest baseline, while maintaining real-time performance (77 fps). Ablation studies confirm that both scene consistency and domain alignment losses are necessary: removing either degrades performance by 90% SSIM or causes 223x increase in latent alignment error, respectively. Code and data are available at https://github.com/D-ST-Sword/SAR-NET.

</details>


### [6] [The Semantic Lifecycle in Embodied AI: Acquisition, Representation and Storage via Foundation Models](https://arxiv.org/abs/2601.08876)
*Shuai Chen,Hao Chen,Yuanchen Bei,Tianyang Zhao,Zhibo Zhou,Feiran Huang*

Main category: cs.CV

TL;DR: 本文综述了一种新的统一框架——语义生命周期，该框架旨在通过基础模型塑造的领域泛化能力和丰富的语义先验，表征基于基础模型的实体AI中语义知识的演化过程，同时从获取、表示和存储三个关键阶段分析和对比了近年的进展。


<details>
  <summary>Details</summary>
Motivation: 在复杂环境下的开放任务中，实体AI需要更通用和鲁棒的语义处理能力，传统的语义处理方法往往被视为孤立模块或独立任务，无法全面捕获语义知识的连续流动和维护。因此，本文提出了一种新的统一框架——语义生命周期。

Method: 文章首先介绍了实体AI的语义处理现状，然后提出了语义生命周期框架，并根据该框架分析和比较了三个关键阶段（获取、表示、存储）的研究进展。

Result: 本文提出了语义生命周期框架，揭示了基础模型如何影响实体AI中的语义处理，对比了获取、表示、存储阶段的进展，并概述了未来的研究方向。

Conclusion: 语义生命周期框架为理解实体AI中的语义处理提供了新的视角，并指出了未来的研究方向。


Abstract: Semantic information in embodied AI is inherently multi-source and multi-stage, making it challenging to fully leverage for achieving stable perception-to-action loops in real-world environments. Early studies have combined manual engineering with deep neural networks, achieving notable progress in specific semantic-related embodied tasks. However, as embodied agents encounter increasingly complex environments and open-ended tasks, the demand for more generalizable and robust semantic processing capabilities has become imperative. Recent advances in foundation models (FMs) address this challenge through their cross-domain generalization abilities and rich semantic priors, reshaping the landscape of embodied AI research. In this survey, we propose the Semantic Lifecycle as a unified framework to characterize the evolution of semantic knowledge within embodied AI driven by foundation models. Departing from traditional paradigms that treat semantic processing as isolated modules or disjoint tasks, our framework offers a holistic perspective that captures the continuous flow and maintenance of semantic knowledge. Guided by this embodied semantic lifecycle, we further analyze and compare recent advances across three key stages: acquisition, representation, and storage. Finally, we summarize existing challenges and outline promising directions for future research.

</details>


### [7] [Compressing Vision Transformers in Geospatial Transfer Learning with Manifold-Constrained Optimization](https://arxiv.org/abs/2601.08882)
*Thomas Snyder,H. Lexie Yang,Stefan Schnake,Steffen Schotthöfer*

Main category: cs.CV

TL;DR: 本文利用DLRT框架在迁移学习过程中对大型视觉变换器地理空间基础模型进行压缩，通过强制执行与下游任务目标对齐的结构低维参数化，实现了强压缩并保持特定任务的精度。实验表明，该方法在减少参数量的同时保持了高精度，并优于现成的低秩方法LoRA，允许在设备上运行高性能地理空间模型。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的边缘设备上部署地理空间基础模型时，压缩大型机器学习模型以保持高精度是一个挑战。本文的动机是提出一种方法来克服参数规模大和压缩导致的精度损失，实现压缩的同时保持特定任务的精度。

Method: 本文的方法是采用DLRT框架来压缩大型视觉变换器地理空间基础模型。通过强制执行与下游任务目标对齐的结构低维参数化，实现压缩和精度的平衡。

Result: 通过本文的方法，实现了显著的参数缩减，同时保持了高精度。实验结果表明，该方法优于现成的低秩方法LoRA，能减少参数量并保持精度。

Conclusion: 总之，本文提出的方法通过利用DLRT框架成功实现了在不降低精度的情况下压缩大型视觉变换器地理空间基础模型的目标。这使得高性能的地理空间模型可以在资源受限的边缘设备上运行。

Abstract: Deploying geospatial foundation models on resource-constrained edge devices demands compact architectures that maintain high downstream performance. However, their large parameter counts and the accuracy loss often induced by compression limit practical adoption. In this work, we leverage manifold-constrained optimization framework DLRT to compress large vision transformer-based geospatial foundation models during transfer learning. By enforcing structured low-dimensional parameterizations aligned with downstream objectives, this approach achieves strong compression while preserving task-specific accuracy. We show that the method outperforms of-the-shelf low-rank methods as LoRA. Experiments on diverse geospatial benchmarks confirm substantial parameter reduction with minimal accuracy loss, enabling high-performing, on-device geospatial models.

</details>


### [8] [Variance-Penalized MC-Dropout as a Learned Smoothing Prior for Brain Tumour Segmentation](https://arxiv.org/abs/2601.08956)
*Satyaki Roy Chowdhury,Golrokh Mirzaei*

Main category: cs.CV

TL;DR: UAMSA-UNet通过引入蒙特卡洛丢弃学习预测的先验平滑分布，结合多尺度特征和注意力图捕捉细节与全局语境，提高Dice和IoU得分，同时减少计算量。


<details>
  <summary>Details</summary>
Motivation: 为了克服基于CNN和U-Net的方法在肿瘤浸润区域产生模糊边界的问题，提出了一种新的UAMSA-UNet模型，旨在提高肿瘤分割的质量和效率。

Method: UAMSA-UNet模型利用蒙特卡洛丢弃学习预测的先验平滑分布，结合多尺度特征和注意力图来捕捉细节与全局语境，并通过加入平滑正则化损失来减少不稳定的错误波动。

Result: 在BraTS2023和BraTS2024数据集上，UAMSA-UNet分别比U-Net提升了3.3%和4.5%的Dice系数，以及2.7%和4.0%的IoU指标，同时相较于U-Net++减少了42.5%的FLOPs。

Conclusion: UAMSA-UNet结合多尺度注意力机制和学习到的平滑先验，实现了更好的分割质量和计算效率，为未来的进一步增强分割结果提供了灵活的基础。

Abstract: Brain tumor segmentation is essential for diagnosis and treatment planning, yet many CNN and U-Net based approaches produce noisy boundaries in regions of tumor infiltration. We introduce UAMSA-UNet, an Uncertainty-Aware Multi-Scale Attention-based Bayesian U-Net that in- stead leverages Monte Carlo Dropout to learn a data-driven smoothing prior over its predictions, while fusing multi-scale features and attention maps to capture both fine details and global context. Our smoothing-regularized loss augments binary cross-entropy with a variance penalty across stochas- tic forward passes, discouraging spurious fluctuations and yielding spatially coherent masks. On BraTS2023, UAMSA- UNet improves Dice Similarity Coefficient by up to 3.3% and mean IoU by up to 2.7% over U-Net; on BraTS2024, it delivers up to 4.5% Dice and 4.0% IoU gains over the best baseline. Remarkably, it also reduces FLOPs by 42.5% rel- ative to U-Net++ while maintaining higher accuracy. These results demonstrate that, by combining multi-scale attention with a learned smoothing prior, UAMSA-UNet achieves both better segmentation quality and computational efficiency, and provides a flexible foundation for future integration with transformer-based modules for further enhanced segmenta- tion results.

</details>


### [9] [Thermo-LIO: A Novel Multi-Sensor Integrated System for Structural Health Monitoring](https://arxiv.org/abs/2601.08977)
*Chao Yang,Haoyuan Zheng,Yue Ma*

Main category: cs.CV

TL;DR: Thermo-LIO是一种新型多传感器系统，通过结合红外热成像和高分辨率LiDAR，实现了结构健康监测中的精确温差和缺陷检测，尤其适用于复杂几何结构和难以到达区域。经过实际桥梁和大厅建筑案例研究验证，Thermo-LIO在缺陷检测精度和覆盖范围上显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统二维热成像检测技术在缺陷鉴定中具有非侵入性和广泛适用性，但在处理复杂几何形态、难以触及区域和内部缺陷方面存在局限性。

Method: 研究通过开发一种多模态融合方法，结合红外热成像和LiDAR，实现了多模态数据流的精确校准同步，从而创建出建筑物温度分布的精准表示。此外，该方法还与LiDAR-惯性里程计（LIO）集成，实现了大规模结构的全面覆盖，并能够进行详尽的温度变化和缺陷检测监测。

Result: 实验验证表明，Thermo-LIO系统能够更准确地检测出详细的热异常和结构缺陷，相较传统方法有显著提升。其具备诊断精度高、实时处理能力强、检测范围广等优势。

Conclusion: Thermo-LIO系统在大型民用基础设施结构健康监测中展现出了多传感器集成的重要作用，推动了SHM技术的发展。

Abstract: Traditional two-dimensional thermography, despite being non-invasive and useful for defect detection in the construction field, is limited in effectively assessing complex geometries, inaccessible areas, and subsurface defects. This paper introduces Thermo-LIO, a novel multi-sensor system that can enhance Structural Health Monitoring (SHM) by fusing thermal imaging with high-resolution LiDAR. To achieve this, the study first develops a multimodal fusion method combining thermal imaging and LiDAR, enabling precise calibration and synchronization of multimodal data streams to create accurate representations of temperature distributions in buildings. Second, it integrates this fusion approach with LiDAR-Inertial Odometry (LIO), enabling full coverage of large-scale structures and allowing for detailed monitoring of temperature variations and defect detection across inspection cycles. Experimental validations, including case studies on a bridge and a hall building, demonstrate that Thermo-LIO can detect detailed thermal anomalies and structural defects more accurately than traditional methods. The system enhances diagnostic precision, enables real-time processing, and expands inspection coverage, highlighting the crucial role of multimodal sensor integration in advancing SHM methodologies for large-scale civil infrastructure.

</details>


### [10] [SAM-pose2seg: Pose-Guided Human Instance Segmentation in Crowds](https://arxiv.org/abs/2601.08982)
*Constantin Kolomiiets,Miroslav Purkrabek,Jiri Matas*

Main category: cs.CV

TL;DR: 通过引入PoseMaskRefine策略，适应SAM2.1进行基于姿态的关键点引导分割，简化提示方法并增强对遮挡的鲁棒性与准确性。


<details>
  <summary>Details</summary>
Motivation: 针对SAM在遮挡下性能下降的问题，利用姿态关键点的高可见性进行模型微调，以提升其在复杂场景下的表现。

Method: 使用PoseMaskRefine策略，通过姿态关键点的引导，在迭代修正过程中改进SAM模型，简化提示仅选择高可见度的三个关键点。

Result: 实验结果表明，姿态引导微调后的SAM模型在多个数据集上都表现出更强的鲁棒性和准确性，即使少量关键点也能得到准确的分割结果。

Conclusion: 姿态引导的微调增强SAM模型在遮挡下的人体分割能力，保持了原始模型的泛化能力，并提供了支持该方法的代码和预训练模型可供使用。

Abstract: Segment Anything (SAM) provides an unprecedented foundation for human segmentation, but may struggle under occlusion, where keypoints may be partially or fully invisible. We adapt SAM 2.1 for pose-guided segmentation with minimal encoder modifications, retaining its strong generalization. Using a fine-tuning strategy called PoseMaskRefine, we incorporate pose keypoints with high visibility into the iterative correction process originally employed by SAM, yielding improved robustness and accuracy across multiple datasets. During inference, we simplify prompting by selecting only the three keypoints with the highest visibility. This strategy reduces sensitivity to common errors, such as missing body parts or misclassified clothing, and allows accurate mask prediction from as few as a single keypoint. Our results demonstrate that pose-guided fine-tuning of SAM enables effective, occlusion-aware human segmentation while preserving the generalization capabilities of the original model. The code and pretrained models will be available at https://mirapurkrabek.github.io/BBox-MaskPose.

</details>


### [11] [Depth-Wise Representation Development Under Blockwise Self-Supervised Learning for Video Vision Transformers](https://arxiv.org/abs/2601.09040)
*Jonas Römer,Timo Dickscheid*

Main category: cs.CV

TL;DR: 通过将编码器划分为块并使用局部遮罩重建损失进行优化，研究者发现块划分训练可以收敛并产生接近端到端基线的表示，特别是在中间表示的分析中揭示了较高的层级结构，并指出后期块饱和和界面形成可能是剩余差距的原因。


<details>
  <summary>Details</summary>
Motivation: 最近在块自监督学习(BWSSL)方面的进展激发了研究者探索是否可以在没有端到端反向传播的情况下训练遮掩视频变换器。这一动机旨在填补关于BWSSL和端到端训练在学习动态和深度表示发展方面的分析不足。

Method: 研究者提出了块自监督学习方法，在一个遮掩视变变换器中通过将编码器划分为多个块，并使用局部遮罩重建损失优化每个块。这种方法避免了端到端反向传播所需的全局错误信号，从而能够协调学习同时减少长距离的信用分配。

Result: 研究结果显示，块划分训练在不同模型尺寸和划分粒度下均能收敛并产生接近端到端基线的表示。块划分训练能够在中间表示中揭示较高的层级结构，并触发后期块的饱和，同时保持几何不变。此外，研究发现了与强早期混合相关但汇总指标可能忽略的token级变化。

Conclusion: 该研究得出结论，块自监督学习方法在遮掩视频变换器的训练中是可行的，且能够提供有意义的表示，特别是对于较高的层级结构。然而，块饱和的后期阶段和界面形成的早期阶段可能是剩余性能差距的原因，未来的工作可以着重于这些因素来进一步提高表示性能。

Abstract: End-to-end backpropagation couples all layers through a global error signal, enabling coordinated learning but requiring long-range credit assignment. Motivated by recent progress in blockwise self-supervised learning (BWSSL), we ask whether masked video transformers can be trained without end-to-end backpropagation. Applying BWSSL to masked video modeling remains relatively underexplored and must handle spatiotemporal context and long-range temporal structure. More broadly, analyses that compare BWSSL and end-to-end training in terms of learning dynamics and depth-wise representation development remain sparse. We apply blockwise learning to a masked autoencoding video vision transformer by partitioning the encoder into blocks, each of which is optimized with a local masked reconstruction loss. Across model sizes and partition granularities, training converges and yields representations close to matched end-to-end baselines under linear-probe and retrieval proxies. In order to compare intermediate representations, we analyze depth-wise decodability, inter-block similarity, and patch-level diagnostics. Blockwise training exposes higher-level structure earlier, while later blocks saturate and operate in a more geometry-preserving regime. It can also induce token-level shifts consistent with stronger early mixing that pooled metrics can miss. These findings point to late-block saturation and interface formation as contributors to the remaining gap.

</details>


### [12] [Exploring Reliable Spatiotemporal Dependencies for Efficient Visual Tracking](https://arxiv.org/abs/2601.09078)
*Junze Shi,Yang Yu,Jian Shi,Haibo Luo*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Recent advances in transformer-based lightweight object tracking have established new standards across benchmarks, leveraging the global receptive field and powerful feature extraction capabilities of attention mechanisms. Despite these achievements, existing methods universally employ sparse sampling during training--utilizing only one template and one search image per sequence--which fails to comprehensively explore spatiotemporal information in videos. This limitation constrains performance and cause the gap between lightweight and high-performance trackers. To bridge this divide while maintaining real-time efficiency, we propose STDTrack, a framework that pioneers the integration of reliable spatiotemporal dependencies into lightweight trackers. Our approach implements dense video sampling to maximize spatiotemporal information utilization. We introduce a temporally propagating spatiotemporal token to guide per-frame feature extraction. To ensure comprehensive target state representation, we disign the Multi-frame Information Fusion Module (MFIFM), which augments current dependencies using historical context. The MFIFM operates on features stored in our constructed Spatiotemporal Token Maintainer (STM), where a quality-based update mechanism ensures information reliability. Considering the scale variation among tracking targets, we develop a multi-scale prediction head to dynamically adapt to objects of different sizes. Extensive experiments demonstrate state-of-the-art results across six benchmarks. Notably, on GOT-10k, STDTrack rivals certain high-performance non-real-time trackers (e.g., MixFormer) while operating at 192 FPS(GPU) and 41 FPS(CPU).

</details>


### [13] [Vision Foundation Models for Domain Generalisable Cross-View Localisation in Planetary Ground-Aerial Robotic Teams](https://arxiv.org/abs/2601.09107)
*Lachlan Holden,Feras Dayoub,Alberto Candela,David Harvey,Tat-Jun Chin*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Accurate localisation in planetary robotics enables the advanced autonomy required to support the increased scale and scope of future missions. The successes of the Ingenuity helicopter and multiple planetary orbiters lay the groundwork for future missions that use ground-aerial robotic teams. In this paper, we consider rovers using machine learning to localise themselves in a local aerial map using limited field-of-view monocular ground-view RGB images as input. A key consideration for machine learning methods is that real space data with ground-truth position labels suitable for training is scarce. In this work, we propose a novel method of localising rovers in an aerial map using cross-view-localising dual-encoder deep neural networks. We leverage semantic segmentation with vision foundation models and high volume synthetic data to bridge the domain gap to real images. We also contribute a new cross-view dataset of real-world rover trajectories with corresponding ground-truth localisation data captured in a planetary analogue facility, plus a high volume dataset of analogous synthetic image pairs. Using particle filters for state estimation with the cross-view networks allows accurate position estimation over simple and complex trajectories based on sequences of ground-view images.

</details>


### [14] [Small but Mighty: Dynamic Wavelet Expert-Guided Fine-Tuning of Large-Scale Models for Optical Remote Sensing Object Segmentation](https://arxiv.org/abs/2601.09108)
*Yanguang Sun,Chao Wang,Jian Yang,Lei Luo*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Accurately localizing and segmenting relevant objects from optical remote sensing images (ORSIs) is critical for advancing remote sensing applications. Existing methods are typically built upon moderate-scale pre-trained models and employ diverse optimization strategies to achieve promising performance under full-parameter fine-tuning. In fact, deeper and larger-scale foundation models can provide stronger support for performance improvement. However, due to their massive number of parameters, directly adopting full-parameter fine-tuning leads to pronounced training difficulties, such as excessive GPU memory consumption and high computational costs, which result in extremely limited exploration of large-scale models in existing works. In this paper, we propose a novel dynamic wavelet expert-guided fine-tuning paradigm with fewer trainable parameters, dubbed WEFT, which efficiently adapts large-scale foundation models to ORSIs segmentation tasks by leveraging the guidance of wavelet experts. Specifically, we introduce a task-specific wavelet expert extractor to model wavelet experts from different perspectives and dynamically regulate their outputs, thereby generating trainable features enriched with task-specific information for subsequent fine-tuning. Furthermore, we construct an expert-guided conditional adapter that first enhances the fine-grained perception of frozen features for specific tasks by injecting trainable features, and then iteratively updates the information of both types of feature, allowing for efficient fine-tuning. Extensive experiments show that our WEFT not only outperforms 21 state-of-the-art (SOTA) methods on three ORSIs datasets, but also achieves optimal results in camouflage, natural, and medical scenarios. The source code is available at: https://github.com/CSYSI/WEFT.

</details>


### [15] [SAM-Aug: Leveraging SAM Priors for Few-Shot Parcel Segmentation in Satellite Time Series](https://arxiv.org/abs/2601.09110)
*Kai Hu,Yaozu Feng,Vladimir Lysenko,Ya Guo Member,Huayi Wu*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Few-shot semantic segmentation of time-series remote sensing images remains a critical challenge, particularly in regions where labeled data is scarce or costly to obtain. While state-of-the-art models perform well under full supervision, their performance degrades significantly under limited labeling, limiting their real-world applicability. In this work, we propose SAM-Aug, a new annotation-efficient framework that leverages the geometry-aware segmentation capability of the Segment Anything Model (SAM) to improve few-shot land cover mapping. Our approach constructs cloud-free composite images from temporal sequences and applies SAM in a fully unsupervised manner to generate geometry-aware mask priors. These priors are then integrated into training through a proposed loss function called RegionSmoothLoss, which enforces prediction consistency within each SAM-derived region across temporal frames, effectively regularizing the model to respect semantically coherent structures. Extensive experiments on the PASTIS-R benchmark under a 5 percent labeled setting demonstrate the effectiveness and robustness of SAM-Aug. Averaged over three random seeds (42, 2025, 4090), our method achieves a mean test mIoU of 36.21 percent, outperforming the state-of-the-art baseline by +2.33 percentage points, a relative improvement of 6.89 percent. Notably, on the most favorable split (seed=42), SAM-Aug reaches a test mIoU of 40.28 percent, representing an 11.2 percent relative gain with no additional labeled data. The consistent improvement across all seeds confirms the generalization power of leveraging foundation model priors under annotation scarcity. Our results highlight that vision models like SAM can serve as useful regularizers in few-shot remote sensing learning, offering a scalable and plug-and-play solution for land cover monitoring without requiring manual annotations or model fine-tuning.

</details>


### [16] [Towards Open Environments and Instructions: General Vision-Language Navigation via Fast-Slow Interactive Reasoning](https://arxiv.org/abs/2601.09111)
*Yang Li,Aming Wu,Zihao Zhang,Yahong Han*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Vision-Language Navigation aims to enable agents to navigate to a target location based on language instructions. Traditional VLN often follows a close-set assumption, i.e., training and test data share the same style of the input images and instructions. However, the real world is open and filled with various unseen environments, posing enormous difficulties for close-set methods. To this end, we focus on the General Scene Adaptation (GSA-VLN) task, aiming to learn generalized navigation ability by introducing diverse environments and inconsistent intructions.Towards this task, when facing unseen environments and instructions, the challenge mainly lies in how to enable the agent to dynamically produce generalized strategies during the navigation process. Recent research indicates that by means of fast and slow cognition systems, human beings could generate stable policies, which strengthen their adaptation for open world. Inspired by this idea, we propose the slow4fast-VLN, establishing a dynamic interactive fast-slow reasoning framework. The fast-reasoning module, an end-to-end strategy network, outputs actions via real-time input. It accumulates execution records in a history repository to build memory. The slow-reasoning module analyze the memories generated by the fast-reasoning module. Through deep reflection, it extracts experiences that enhance the generalization ability of decision-making. These experiences are structurally stored and used to continuously optimize the fast-reasoning module. Unlike traditional methods that treat fast-slow reasoning as independent mechanisms, our framework enables fast-slow interaction. By leveraging the experiences from slow reasoning. This interaction allows the system to continuously adapt and efficiently execute navigation tasks when facing unseen scenarios.

</details>


### [17] [LP-LLM: End-to-End Real-World Degraded License Plate Text Recognition via Large Multimodal Models](https://arxiv.org/abs/2601.09116)
*Haoyan Gong,Hongbin Liu*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Real-world License Plate Recognition (LPR) faces significant challenges from severe degradations such as motion blur, low resolution, and complex illumination. The prevailing "restoration-then-recognition" two-stage paradigm suffers from a fundamental flaw: the pixel-level optimization objectives of image restoration models are misaligned with the semantic goals of character recognition, leading to artifact interference and error accumulation. While Vision-Language Models (VLMs) have demonstrated powerful general capabilities, they lack explicit structural modeling for license plate character sequences (e.g., fixed length, specific order). To address this, we propose an end-to-end structure-aware multimodal reasoning framework based on Qwen3-VL. The core innovation lies in the Character-Aware Multimodal Reasoning Module (CMRM), which introduces a set of learnable Character Slot Queries. Through a cross-attention mechanism, these queries actively retrieve fine-grained evidence corresponding to character positions from visual features. Subsequently, we inject these character-aware representations back into the visual tokens via residual modulation, enabling the language model to perform autoregressive generation based on explicit structural priors. Furthermore, combined with the LoRA parameter-efficient fine-tuning strategy, the model achieves domain adaptation while retaining the generalization capabilities of the large model. Extensive experiments on both synthetic and real-world severely degraded datasets demonstrate that our method significantly outperforms existing restoration-recognition combinations and general VLMs, validating the superiority of incorporating structured reasoning into large models for low-quality text recognition tasks.

</details>


### [18] [LPCAN: Lightweight Pyramid Cross-Attention Network for Rail Surface Defect Detection Using RGB-D Data](https://arxiv.org/abs/2601.09118)
*Jackie Alex,Guoqiang Huan*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This paper addresses the limitations of current vision-based rail defect detection methods, including high computational complexity, excessive parameter counts, and suboptimal accuracy. We propose a Lightweight Pyramid Cross-Attention Network (LPCANet) that leverages RGB-D data for efficient and accurate defect identification. The architecture integrates MobileNetv2 as a backbone for RGB feature extraction with a lightweight pyramid module (LPM) for depth processing, coupled with a cross-attention mechanism (CAM) for multimodal fusion and a spatial feature extractor (SFE) for enhanced structural analysis. Comprehensive evaluations on three unsupervised RGB-D rail datasets (NEU-RSDDS-AUG, RSDD-TYPE1, RSDD-TYPE2) demonstrate that LPCANet achieves state-of-the-art performance with only 9.90 million parameters, 2.50 G FLOPs, and 162.60 fps inference speed. Compared to 18 existing methods, LPCANet shows significant improvements, including +1.48\% in $S_α$, +0.86\% in IOU, and +1.77\% in MAE over the best-performing baseline. Ablation studies confirm the critical roles of CAM and SFE, while experiments on non-rail datasets (DAGM2007, MT, Kolektor-SDD2) validate its generalization capability. The proposed framework effectively bridges traditional and deep learning approaches, offering substantial practical value for industrial defect inspection. Future work will focus on further model compression for real-time deployment.

</details>


### [19] [Beyond Seen Bounds: Class-Centric Polarization for Single-Domain Generalized Deep Metric Learning](https://arxiv.org/abs/2601.09121)
*Xin Yuan,Meiqi Wan,Wei Liu,Xin Xu,Zheng Wang*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Single-domain generalized deep metric learning (SDG-DML) faces the dual challenge of both category and domain shifts during testing, limiting real-world applications. Therefore, aiming to learn better generalization ability on both unseen categories and domains is a realistic goal for the SDG-DML task. To deliver the aspiration, existing SDG-DML methods employ the domain expansion-equalization strategy to expand the source data and generate out-of-distribution samples. However, these methods rely on proxy-based expansion, which tends to generate samples clustered near class proxies, failing to simulate the broad and distant domain shifts encountered in practice. To alleviate the problem, we propose CenterPolar, a novel SDG-DML framework that dynamically expands and constrains domain distributions to learn a generalizable DML model for wider target domain distributions. Specifically, \textbf{CenterPolar} contains two collaborative class-centric polarization phases: (1) Class-Centric Centrifugal Expansion ($C^3E$) and (2) Class-Centric Centripetal Constraint ($C^4$). In the first phase, $C^3E$ drives the source domain distribution by shifting the source data away from class centroids using centrifugal expansion to generalize to more unseen domains. In the second phase, to consolidate domain-invariant class information for the generalization ability to unseen categories, $C^4$ pulls all seen and unseen samples toward their class centroids while enforcing inter-class separation via centripetal constraint. Extensive experimental results on widely used CUB-200-2011 Ext., Cars196 Ext., DomainNet, PACS, and Office-Home datasets demonstrate the superiority and effectiveness of our CenterPolar over existing state-of-the-art methods. The code will be released after acceptance.

</details>


### [20] [SkinFlow: Efficient Information Transmission for Open Dermatological Diagnosis via Dynamic Visual Encoding and Staged RL](https://arxiv.org/abs/2601.09136)
*Lijun Liu,Linwei Chen,Zhishou Zhang,Meng Tian,Hengfu Cui,Ruiyang Li,Zhaocheng Liu,Qiang Ju,Qianxi Li,Hong-Yu Zhou*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: General-purpose Large Vision-Language Models (LVLMs), despite their massive scale, often falter in dermatology due to "diffuse attention" - the inability to disentangle subtle pathological lesions from background noise. In this paper, we challenge the assumption that parameter scaling is the only path to medical precision. We introduce SkinFlow, a framework that treats diagnosis as an optimization of visual information transmission efficiency. Our approach utilizes a Virtual-Width Dynamic Vision Encoder (DVE) to "unfold" complex pathological manifolds without physical parameter expansion, coupled with a two-stage Reinforcement Learning strategy. This strategy sequentially aligns explicit medical descriptions (Stage I) and reconstructs implicit diagnostic textures (Stage II) within a constrained semantic space. Furthermore, we propose a clinically grounded evaluation protocol that prioritizes diagnostic safety and hierarchical relevance over rigid label matching. Empirical results are compelling: our 7B model establishes a new state-of-the-art on the Fitzpatrick17k benchmark, achieving a +12.06% gain in Top-1 accuracy and a +28.57% boost in Top-6 accuracy over the massive general-purpose models (e.g., Qwen3VL-235B and GPT-5.2). These findings demonstrate that optimizing geometric capacity and information flow yields superior diagnostic reasoning compared to raw parameter scaling.

</details>


### [21] [From Snow to Rain: Evaluating Robustness, Calibration, and Complexity of Model-Based Robust Training](https://arxiv.org/abs/2601.09153)
*Josué Martínez-Martínez,Olivia Brown,Giselle Zeno,Pooya Khorrami,Rajmonda Caceres*

Main category: cs.CV

TL;DR: 研究通过学习难辨因素模型生成真实化干扰，结合随机覆盖和对抗性精炼，提升模型在恶劣环境下的鲁棒性。研究表明，基于模型的方法在多种干扰条件下表现优于基本范例，尽管需要较高的计算成本。


<details>
  <summary>Details</summary>
Motivation: 在安全敏感领域中，需要提高深度学习模型对自然干扰的鲁棒性，以确保模型的可靠性。

Method: 采用学习到的次因素模型生成真实化干扰，并引入随机覆盖与对抗性精炼相结合的混合策略，评估模型在不同干扰程度下的准确率、校准性和训练复杂性。

Result: 结果显示，基于模型的方法在准确率、校准度和鲁棒性方面优于基线模型（如Vanilla、对抗性训练和AugMix），模型构建的对抗性训练提供了最佳的跨干扰鲁棒性，但计算成本较高。基于模型的数据增强在计算复杂度降低的情况下达到了与原始模型相当的表现。

Conclusion: 研究表明，学习到的难辨因素模型对于捕捉自然变异具有重要性，未来的研究可探索降低计算成本的方法，以实现更加稳健和校准的模型。

Abstract: Robustness to natural corruptions remains a critical challenge for reliable deep learning, particularly in safety-sensitive domains. We study a family of model-based training approaches that leverage a learned nuisance variation model to generate realistic corruptions, as well as new hybrid strategies that combine random coverage with adversarial refinement in nuisance space. Using the Challenging Unreal and Real Environments for Traffic Sign Recognition dataset (CURE-TSR), with Snow and Rain corruptions, we evaluate accuracy, calibration, and training complexity across corruption severities. Our results show that model-based methods consistently outperform baselines Vanilla, Adversarial Training, and AugMix baselines, with model-based adversarial training providing the strongest robustness under across all corruptions but at the expense of higher computation and model-based data augmentation achieving comparable robustness with $T$ less computational complexity without incurring a statistically significant drop in performance. These findings highlight the importance of learned nuisance models for capturing natural variability, and suggest a promising path toward more resilient and calibrated models under challenging conditions.

</details>


### [22] [Architecture inside the mirage: evaluating generative image models on architectural style, elements, and typologies](https://arxiv.org/abs/2601.09169)
*Jamie Magrill,Leah Gornstein,Sandra Seekins,Barry Magrill*

Main category: cs.CV

TL;DR: 该研究评估了五种流行的生成人工智能（GenAI）文本转图像系统，应用于建筑学，发现虽然整体准确性有限，但Common提示优于Rare提示，不同平台间准确性和表现差异显著。


<details>
  <summary>Details</summary>
Motivation: 研究旨在评估GenAI在再现规范严谨的建筑学领域的实际应用能力，强调视觉准确性的重要性。

Method: 研究者使用30个涵盖各种建筑风格、类型和规范元素的提示，随机分发给五种常见的GenAI图像生成平台。每个提示-生成器配对产生四幅图像，总共产生600幅图像。两组独立的建筑史学家进行了评分，合并后的结果展示了平台间和提示类型间的准确性差异。

Result: 研究结果显示，通用提示下生成的图像比罕见提示更为准确（p < 0.05）。五个平台上存在准确性差异，最高准确率仅为52%，最低为32%，平均值为42%。不同平台在完全正确的结果上表现相似，但在完全错误的结果上存在显著差异。图像显示常见问题包括过度装饰、混淆中世纪风格与其后世复兴、描述提示的误读等。

Conclusion: 研究强调需在生成图像中清晰标注GenAI的内容，制定来源标准供未来训练数据集参考，并谨慎教育使用GenAI的建筑图片。

Abstract: Generative artificial intelligence (GenAI) text-to-image systems are increasingly used to generate architectural imagery, yet their capacity to reproduce accurate images in a historically rule-bound field remains poorly characterized. We evaluated five widely used GenAI image platforms (Adobe Firefly, DALL-E 3, Google Imagen 3, Microsoft Image Generator, and Midjourney) using 30 architectural prompts spanning styles, typologies, and codified elements. Each prompt-generator pair produced four images (n = 600 images total). Two architectural historians independently scored each image for accuracy against predefined criteria, resolving disagreements by consensus. Set-level performance was summarized as zero to four accurate images per four-image set. Image output from Common prompts was 2.7-fold more accurate than from Rare prompts (p < 0.05). Across platforms, overall accuracy was limited (highest accuracy score 52 percent; lowest 32 percent; mean 42 percent). All-correct (4 out of 4) outcomes were similar across platforms. By contrast, all-incorrect (0 out of 4) outcomes varied substantially, with Imagen 3 exhibiting the fewest failures and Microsoft Image Generator exhibiting the highest number of failures. Qualitative review of the image dataset identified recurring patterns including over-embellishment, confusion between medieval styles and their later revivals, and misrepresentation of descriptive prompts (for example, egg-and-dart, banded column, pendentive). These findings support the need for visible labeling of GenAI synthetic content, provenance standards for future training datasets, and cautious educational use of GenAI architectural imagery.

</details>


### [23] [From Performance to Practice: Knowledge-Distilled Segmentator for On-Premises Clinical Workflows](https://arxiv.org/abs/2601.09191)
*Qizhen Lan,Aaron Choi,Jun Ma,Bo Wang,Zhaogming Zhao,Xiaoqian Jiang,Yu-Chun Hsu*

Main category: cs.CV

TL;DR: 该研究提出了一种利用知识蒸馏将高性能分割模型转化为高效且可扩展的学生模型的框架，以适应医院环境中的实际部署需求。


<details>
  <summary>Details</summary>
Motivation: 现有的高性能分割模型在医院环境中难以实现有效部署，因为它们需要大量的计算资源且存在治理和安全限制。因此，研究旨在开发一种框架，将这些复杂的模型转化为更紧凑且易部署的学生模型。

Method: 该研究使用知识蒸馏方法将高性能的教师模型转化为一系列紧凑的学生模型，同时保持了与现有临床系统的兼容性。

Result: 实验结果表明，即使在参数减少了94%的情况下，蒸馏过的学生模型仍然保持了近99%的准确度，并且能够提供显著的性能提升，如CPU推理延迟最多降低了67%，而无需额外的部署成本。

Conclusion: 研究证明，知识蒸馏是一种实用且可靠的方法，可用于将研究级别的分割模型转换为在医院环境中可以轻松维护并部署的组件，以适应实际的临床工作流程。

Abstract: Deploying medical image segmentation models in routine clinical workflows is often constrained by on-premises infrastructure, where computational resources are fixed and cloud-based inference may be restricted by governance and security policies. While high-capacity models achieve strong segmentation accuracy, their computational demands hinder practical deployment and long-term maintainability in hospital environments. We present a deployment-oriented framework that leverages knowledge distillation to translate a high-performing segmentation model into a scalable family of compact student models, without modifying the inference pipeline. The proposed approach preserves architectural compatibility with existing clinical systems while enabling systematic capacity reduction. The framework is evaluated on a multi-site brain MRI dataset comprising 1,104 3D volumes, with independent testing on 101 curated cases, and is further examined on abdominal CT to assess cross-modality generalizability. Under aggressive parameter reduction (94%), the distilled student model preserves nearly all of the teacher's segmentation accuracy (98.7%), while achieving substantial efficiency gains, including up to a 67% reduction in CPU inference latency without additional deployment overhead. These results demonstrate that knowledge distillation provides a practical and reliable pathway for converting research-grade segmentation models into maintainable, deployment-ready components for on-premises clinical workflows in real-world health systems.

</details>


### [24] [Point Tracking as a Temporal Cue for Robust Myocardial Segmentation in Echocardiography Videos](https://arxiv.org/abs/2601.09207)
*Bahar Khodabakhshian,Nima Hashemi,Armin Saadat,Zahra Gholami,In-Chang Hwang,Samira Sojoudi,Christina Luong,Purang Abolmaesumi,Teresa Tsang*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Purpose: Myocardium segmentation in echocardiography videos is a challenging task due to low contrast, noise, and anatomical variability. Traditional deep learning models either process frames independently, ignoring temporal information, or rely on memory-based feature propagation, which accumulates error over time. Methods: We propose Point-Seg, a transformer-based segmentation framework that integrates point tracking as a temporal cue to ensure stable and consistent segmentation of myocardium across frames. Our method leverages a point-tracking module trained on a synthetic echocardiography dataset to track key anatomical landmarks across video sequences. These tracked trajectories provide an explicit motion-aware signal that guides segmentation, reducing drift and eliminating the need for memory-based feature accumulation. Additionally, we incorporate a temporal smoothing loss to further enhance temporal consistency across frames. Results: We evaluate our approach on both public and private echocardiography datasets. Experimental results demonstrate that Point-Seg has statistically similar accuracy in terms of Dice to state-of-the-art segmentation models in high quality echo data, while it achieves better segmentation accuracy in lower quality echo with improved temporal stability. Furthermore, Point-Seg has the key advantage of pixel-level myocardium motion information as opposed to other segmentation methods. Such information is essential in the computation of other downstream tasks such as myocardial strain measurement and regional wall motion abnormality detection. Conclusion: Point-Seg demonstrates that point tracking can serve as an effective temporal cue for consistent video segmentation, offering a reliable and generalizable approach for myocardium segmentation in echocardiography videos. The code is available at https://github.com/DeepRCL/PointSeg.

</details>


### [25] [Pairing-free Group-level Knowledge Distillation for Robust Gastrointestinal Lesion Classification in White-Light Endoscopy](https://arxiv.org/abs/2601.09209)
*Qiang Hu,Qimei Wang,Yingjie Guo,Qiang Li,Zhiwei Wang*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: White-Light Imaging (WLI) is the standard for endoscopic cancer screening, but Narrow-Band Imaging (NBI) offers superior diagnostic details. A key challenge is transferring knowledge from NBI to enhance WLI-only models, yet existing methods are critically hampered by their reliance on paired NBI-WLI images of the same lesion, a costly and often impractical requirement that leaves vast amounts of clinical data untapped. In this paper, we break this paradigm by introducing PaGKD, a novel Pairing-free Group-level Knowledge Distillation framework that that enables effective cross-modal learning using unpaired WLI and NBI data. Instead of forcing alignment between individual, often semantically mismatched image instances, PaGKD operates at the group level to distill more complete and compatible knowledge across modalities. Central to PaGKD are two complementary modules: (1) Group-level Prototype Distillation (GKD-Pro) distills compact group representations by extracting modality-invariant semantic prototypes via shared lesion-aware queries; (2) Group-level Dense Distillation (GKD-Den) performs dense cross-modal alignment by guiding group-aware attention with activation-derived relation maps. Together, these modules enforce global semantic consistency and local structural coherence without requiring image-level correspondence. Extensive experiments on four clinical datasets demonstrate that PaGKD consistently and significantly outperforms state-of-the-art methods, achieving relative AUC improvements of 3.3%, 1.1%, 2.8%, and 3.2%, respectively, establishing a new direction for cross-modal learning from unpaired data.

</details>


### [26] [Affostruction: 3D Affordance Grounding with Generative Reconstruction](https://arxiv.org/abs/2601.09211)
*Chunghyun Park,Seunghyeon Lee,Minsu Cho*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This paper addresses the problem of affordance grounding from RGBD images of an object, which aims to localize surface regions corresponding to a text query that describes an action on the object. While existing methods predict affordance regions only on visible surfaces, we propose Affostruction, a generative framework that reconstructs complete geometry from partial observations and grounds affordances on the full shape including unobserved regions. We make three core contributions: generative multi-view reconstruction via sparse voxel fusion that extrapolates unseen geometry while maintaining constant token complexity, flow-based affordance grounding that captures inherent ambiguity in affordance distributions, and affordance-driven active view selection that leverages predicted affordances for intelligent viewpoint sampling. Affostruction achieves 19.1 aIoU on affordance grounding (40.4\% improvement) and 32.67 IoU for 3D reconstruction (67.7\% improvement), enabling accurate affordance prediction on complete shapes.

</details>


### [27] [Annealed Relaxation of Speculative Decoding for Faster Autoregressive Image Generation](https://arxiv.org/abs/2601.09212)
*Xingyao Li,Fengzhuo Zhang,Cunxiao Du,Hui Ji*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Despite significant progress in autoregressive image generation, inference remains slow due to the sequential nature of AR models and the ambiguity of image tokens, even when using speculative decoding. Recent works attempt to address this with relaxed speculative decoding but lack theoretical grounding. In this paper, we establish the theoretical basis of relaxed SD and propose COOL-SD, an annealed relaxation of speculative decoding built on two key insights. The first analyzes the total variation (TV) distance between the target model and relaxed speculative decoding and yields an optimal resampling distribution that minimizes an upper bound of the distance. The second uses perturbation analysis to reveal an annealing behaviour in relaxed speculative decoding, motivating our annealed design. Together, these insights enable COOL-SD to generate images faster with comparable quality, or achieve better quality at similar latency. Experiments validate the effectiveness of COOL-SD, showing consistent improvements over prior methods in speed-quality trade-offs.

</details>


### [28] [SpikeVAEDiff: Neural Spike-based Natural Visual Scene Reconstruction via VD-VAE and Versatile Diffusion](https://arxiv.org/abs/2601.09213)
*Jialu Li,Taiyan Zhou*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Reconstructing natural visual scenes from neural activity is a key challenge in neuroscience and computer vision. We present SpikeVAEDiff, a novel two-stage framework that combines a Very Deep Variational Autoencoder (VDVAE) and the Versatile Diffusion model to generate high-resolution and semantically meaningful image reconstructions from neural spike data. In the first stage, VDVAE produces low-resolution preliminary reconstructions by mapping neural spike signals to latent representations. In the second stage, regression models map neural spike signals to CLIP-Vision and CLIP-Text features, enabling Versatile Diffusion to refine the images via image-to-image generation.
  We evaluate our approach on the Allen Visual Coding-Neuropixels dataset and analyze different brain regions. Our results show that the VISI region exhibits the most prominent activation and plays a key role in reconstruction quality. We present both successful and unsuccessful reconstruction examples, reflecting the challenges of decoding neural activity. Compared with fMRI-based approaches, spike data provides superior temporal and spatial resolution. We further validate the effectiveness of the VDVAE model and conduct ablation studies demonstrating that data from specific brain regions significantly enhances reconstruction performance.

</details>


### [29] [Disentangle Object and Non-object Infrared Features via Language Guidance](https://arxiv.org/abs/2601.09228)
*Fan Liu,Ting Wu,Chuanyi Zhang,Liang Yao,Xing Ma,Yuhui Zheng*

Main category: cs.CV

TL;DR: 提出了一种新的基于视觉-语言表示学习的红外物体检测方法，通过引入语义特征对齐模块和对象特征解耦模块，提升检测性能。


<details>
  <summary>Details</summary>
Motivation: 在低光照条件下，红外图像的对比度和边缘信息较低，难以提取具有判别性的物体特征。引入视觉-语言表示学习来解决这个问题。

Method: 该方法包含语义特征对齐模块（SFA）和对象特征解耦模块（OFD），通过将物体特征与文本特征对齐，并解耦物体特征和非物体特征，提高检测性能。

Result: 该方法在两个基准数据集M³FD和FLIR上分别取得了83.7%和86.1%的mAP。

Conclusion: 通过使用视觉-语言表示学习，该方法显著提升了红外物体检测的性能。

Abstract: Infrared object detection focuses on identifying and locating objects in complex environments (\eg, dark, snow, and rain) where visible imaging cameras are disabled by poor illumination. However, due to low contrast and weak edge information in infrared images, it is challenging to extract discriminative object features for robust detection. To deal with this issue, we propose a novel vision-language representation learning paradigm for infrared object detection. An additional textual supervision with rich semantic information is explored to guide the disentanglement of object and non-object features. Specifically, we propose a Semantic Feature Alignment (SFA) module to align the object features with the corresponding text features. Furthermore, we develop an Object Feature Disentanglement (OFD) module that disentangles text-aligned object features and non-object features by minimizing their correlation. Finally, the disentangled object features are entered into the detection head. In this manner, the detection performance can be remarkably enhanced via more discriminative and less noisy features. Extensive experimental results demonstrate that our approach achieves superior performance on two benchmarks: M\textsuperscript{3}FD (83.7\% mAP), FLIR (86.1\% mAP). Our code will be publicly available once the paper is accepted.

</details>


### [30] [CLIDD: Cross-Layer Independent Deformable Description for Efficient and Discriminative Local Feature Representation](https://arxiv.org/abs/2601.09230)
*Haodi Yao,Fenghua He,Ning Hao,Yao Su*

Main category: cs.CV

TL;DR: CLIDD提供了一种高效且精确的局部特征描述方法，通过独立特征层次直接采样，并采用可学习偏移捕捉不同尺度的细微结构细节。硬件感知内核融合策略在保持实时性能的同时优化了推理 throughput。该框架整合了轻量级架构和结合度量学习与知识蒸馏的训练方案，产生多种适用于不同部署需求的模型变体。实验证明，CLIDD同时具备卓越的匹配准确性和计算效率，在多任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前任务需求高分辨特征描述具有强大的区分能力和高效的计算性能，得到可靠对应关系，以支持时空智能任务如机器人导航和增强现实。

Method: CLIDD采用独立特征层次直接采样，利用可学习偏移捕捉不同尺度的细微结构细节，减少了统一密集表示的计算负担，并通过硬件感知的内核融合策略增强了实时性能。并通过结合度量学习与知识蒸馏的技术，提出了一种可扩展的框架来生成轻量级的模型变体。

Result: CLIDD达到了优越的匹配精度和计算效率，超紧凑变体仅使用0.004M参数就达到了与SuperPoint相当的精度，使模型大小减少了99.7%。高配置版本的性能超过了所有现有的最优方法，同时在边缘设备上超过了200 FPS。

Conclusion: CLIDD提供了一种有效的局部特征配准方法，适用于实时时空智能任务，兼具高精度和低资源占用。

Abstract: Robust local feature representations are essential for spatial intelligence tasks such as robot navigation and augmented reality. Establishing reliable correspondences requires descriptors that provide both high discriminative power and computational efficiency. To address this, we introduce Cross-Layer Independent Deformable Description (CLIDD), a method that achieves superior distinctiveness by sampling directly from independent feature hierarchies. This approach utilizes learnable offsets to capture fine-grained structural details across scales while bypassing the computational burden of unified dense representations. To ensure real-time performance, we implement a hardware-aware kernel fusion strategy that maximizes inference throughput. Furthermore, we develop a scalable framework that integrates lightweight architectures with a training protocol leveraging both metric learning and knowledge distillation. This scheme generates a wide spectrum of model variants optimized for diverse deployment constraints. Extensive evaluations demonstrate that our approach achieves superior matching accuracy and exceptional computational efficiency simultaneously. Specifically, the ultra-compact variant matches the precision of SuperPoint while utilizing only 0.004M parameters, achieving a 99.7% reduction in model size. Furthermore, our high-performance configuration outperforms all current state-of-the-art methods, including high-capacity DINOv2-based frameworks, while exceeding 200 FPS on edge devices. These results demonstrate that CLIDD delivers high-precision local feature matching with minimal computational overhead, providing a robust and scalable solution for real-time spatial intelligence tasks.

</details>


### [31] [DeTracker: Motion-decoupled Vehicle Detection and Tracking in Unstabilized Satellite Videos](https://arxiv.org/abs/2601.09240)
*Jiajun Chen,Jing Xiao,Shaohan Cao,Yuming Zhu,Liang Liao,Jun Pan,Mi Wang*

Main category: cs.CV

TL;DR: DeTracker 提出了一种适用于不稳定卫星视频的检测和跟踪框架，通过全局-局部运动解耦模块和跨帧时间特征融合模块，提高了小目标的跟踪性能，并且在新的基准数据集 SDM-Car-SU 和真实卫星视频数据上均取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 针对不稳定卫星视频下多目标跟踪的挑战，现有方法在平台抖动和物体微弱外观共同作用下导致跟踪性能下降，因此需要开发一种专用的多目标跟踪框架以改善性能。

Method: DeTracker 引入了全局-局部运动解耦模块和跨帧时间特征融合模块。全局-局部运动解耦模块通过全局对齐和局部细化将卫星平台运动从真实物体运动中分离出来；跨帧时间特征融合模块通过跨帧特征融合增强小物体的连续性和可分性。

Result: 通过在新基准数据集 SDM-Car-SU 和真实卫星视频数据上的大量实验，DeTracker 在 MOTA 指标上显著优于现有方法，分别达到 61.1% 和 47.3% 的 MOTA。

Conclusion: DeTracker 通过引入全局-局部运动解耦模块和跨帧时间特征融合模块，为不稳定卫星视频下的多目标跟踪提供了有效的解决方案，提高了小目标的跟踪精度和鲁棒性。

Abstract: Satellite videos provide continuous observations of surface dynamics but pose significant challenges for multi-object tracking (MOT), especially under unstabilized conditions where platform jitter and the weak appearance of tiny objects jointly degrade tracking performance. To address this problem, we propose DeTracker, a joint detection-and-tracking framework tailored for unstabilized satellite videos. DeTracker introduces a Global--Local Motion Decoupling (GLMD) module that explicitly separates satellite platform motion from true object motion through global alignment and local refinement, leading to improved trajectory stability and motion estimation accuracy. In addition, a Temporal Dependency Feature Pyramid (TDFP) module is developed to perform cross-frame temporal feature fusion, enhancing the continuity and discriminability of tiny-object representations. We further construct a new benchmark dataset, SDM-Car-SU, which simulates multi-directional and multi-speed platform motions to enable systematic evaluation of tracking robustness under varying motion perturbations. Extensive experiments on both simulated and real unstabilized satellite videos demonstrate that DeTracker significantly outperforms existing methods, achieving 61.1% MOTA on SDM-Car-SU and 47.3% MOTA on real satellite video data.

</details>


### [32] [A$^2$TG: Adaptive Anisotropic Textured Gaussians for Efficient 3D Scene Representation](https://arxiv.org/abs/2601.09243)
*Sheng-Chi Hsu,Ting-Yu Yen,Shih-Hsuan Hung,Hung-Kuo Chu*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Gaussian Splatting has emerged as a powerful representation for high-quality, real-time 3D scene rendering. While recent works extend Gaussians with learnable textures to enrich visual appearance, existing approaches allocate a fixed square texture per primitive, leading to inefficient memory usage and limited adaptability to scene variability. In this paper, we introduce adaptive anisotropic textured Gaussians (A$^2$TG), a novel representation that generalizes textured Gaussians by equipping each primitive with an anisotropic texture. Our method employs a gradient-guided adaptive rule to jointly determine texture resolution and aspect ratio, enabling non-uniform, detail-aware allocation that aligns with the anisotropic nature of Gaussian splats. This design significantly improves texture efficiency, reducing memory consumption while enhancing image quality. Experiments on multiple benchmark datasets demonstrate that A TG consistently outperforms fixed-texture Gaussian Splatting methods, achieving comparable rendering fidelity with substantially lower memory requirements.

</details>


### [33] [Integrating Diverse Assignment Strategies into DETRs](https://arxiv.org/abs/2601.09247)
*Yiwei Zhang,Jin Gao,Hanshi Wang,Fudong Ge,Guan Luo,Weiming Hu,Zhipeng Zhang*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Label assignment is a critical component in object detectors, particularly within DETR-style frameworks where the one-to-one matching strategy, despite its end-to-end elegance, suffers from slow convergence due to sparse supervision. While recent works have explored one-to-many assignments to enrich supervisory signals, they often introduce complex, architecture-specific modifications and typically focus on a single auxiliary strategy, lacking a unified and scalable design. In this paper, we first systematically investigate the effects of ``one-to-many'' supervision and reveal a surprising insight that performance gains are driven not by the sheer quantity of supervision, but by the diversity of the assignment strategies employed. This finding suggests that a more elegant, parameter-efficient approach is attainable. Building on this insight, we propose LoRA-DETR, a flexible and lightweight framework that seamlessly integrates diverse assignment strategies into any DETR-style detector. Our method augments the primary network with multiple Low-Rank Adaptation (LoRA) branches during training, each instantiating a different one-to-many assignment rule. These branches act as auxiliary modules that inject rich, varied supervisory gradients into the main model and are discarded during inference, thus incurring no additional computational cost. This design promotes robust joint optimization while maintaining the architectural simplicity of the original detector. Extensive experiments on different baselines validate the effectiveness of our approach. Our work presents a new paradigm for enhancing detectors, demonstrating that diverse ``one-to-many'' supervision can be integrated to achieve state-of-the-art results without compromising model elegance.

</details>


### [34] [Hybrid guided variational autoencoder for visual place recognition](https://arxiv.org/abs/2601.09248)
*Ni Wang,Zihan You,Emre Neftci,Thorben Schoepe*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Autonomous agents such as cars, robots and drones need to precisely localize themselves in diverse environments, including in GPS-denied indoor environments. One approach for precise localization is visual place recognition (VPR), which estimates the place of an image based on previously seen places. State-of-the-art VPR models require high amounts of memory, making them unwieldy for mobile deployment, while more compact models lack robustness and generalization capabilities. This work overcomes these limitations for robotics using a combination of event-based vision sensors and an event-based novel guided variational autoencoder (VAE). The encoder part of our model is based on a spiking neural network model which is compatible with power-efficient low latency neuromorphic hardware. The VAE successfully disentangles the visual features of 16 distinct places in our new indoor VPR dataset with a classification performance comparable to other state-of-the-art approaches while, showing robust performance also under various illumination conditions. When tested with novel visual inputs from unknown scenes, our model can distinguish between these places, which demonstrates a high generalization capability by learning the essential features of location. Our compact and robust guided VAE with generalization capabilities poses a promising model for visual place recognition that can significantly enhance mobile robot navigation in known and unknown indoor environments.

</details>


### [35] [PhyRPR: Training-Free Physics-Constrained Video Generation](https://arxiv.org/abs/2601.09255)
*Yibo Zhao,Hengjia Li,Xiaofei He,Boxi Wu*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Recent diffusion-based video generation models can synthesize visually plausible videos, yet they often struggle to satisfy physical constraints. A key reason is that most existing approaches remain single-stage: they entangle high-level physical understanding with low-level visual synthesis, making it hard to generate content that require explicit physical reasoning. To address this limitation, we propose a training-free three-stage pipeline,\textit{PhyRPR}:\textit{Phy\uline{R}eason}--\textit{Phy\uline{P}lan}--\textit{Phy\uline{R}efine}, which decouples physical understanding from visual synthesis. Specifically, \textit{PhyReason} uses a large multimodal model for physical state reasoning and an image generator for keyframe synthesis; \textit{PhyPlan} deterministically synthesizes a controllable coarse motion scaffold; and \textit{PhyRefine} injects this scaffold into diffusion sampling via a latent fusion strategy to refine appearance while preserving the planned dynamics. This staged design enables explicit physical control during generation. Extensive experiments under physics constraints show that our method consistently improves physical plausibility and motion controllability.

</details>


### [36] [Magnifying change: Rapid burn scar mapping with multi-resolution, multi-source satellite imagery](https://arxiv.org/abs/2601.09262)
*Maria Sdraka,Dimitrios Michail,Ioannis Papoutsis*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Delineating wildfire affected areas using satellite imagery remains challenging due to irregular and spatially heterogeneous spectral changes across the electromagnetic spectrum. While recent deep learning approaches achieve high accuracy when high-resolution multispectral data are available, their applicability in operational settings, where a quick delineation of the burn scar shortly after a wildfire incident is required, is limited by the trade-off between spatial resolution and temporal revisit frequency of current satellite systems. To address this limitation, we propose a novel deep learning model, namely BAM-MRCD, which employs multi-resolution, multi-source satellite imagery (MODIS and Sentinel-2) for the timely production of detailed burnt area maps with high spatial and temporal resolution. Our model manages to detect even small scale wildfires with high accuracy, surpassing similar change detection models as well as solid baselines. All data and code are available in the GitHub repository: https://github.com/Orion-AI-Lab/BAM-MRCD.

</details>


### [37] [BrainSegNet: A Novel Framework for Whole-Brain MRI Parcellation Enhanced by Large Models](https://arxiv.org/abs/2601.09263)
*Yucheng Li,Xiaofan Wang,Junyi Wang,Yijie Li,Xi Zhu,Mubai Du,Dian Sheng,Wei Zhang,Fan Zhang*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Whole-brain parcellation from MRI is a critical yet challenging task due to the complexity of subdividing the brain into numerous small, irregular shaped regions. Traditionally, template-registration methods were used, but recent advances have shifted to deep learning for faster workflows. While large models like the Segment Anything Model (SAM) offer transferable feature representations, they are not tailored for the high precision required in brain parcellation. To address this, we propose BrainSegNet, a novel framework that adapts SAM for accurate whole-brain parcellation into 95 regions. We enhance SAM by integrating U-Net skip connections and specialized modules into its encoder and decoder, enabling fine-grained anatomical precision. Key components include a hybrid encoder combining U-Net skip connections with SAM's transformer blocks, a multi-scale attention decoder with pyramid pooling for varying-sized structures, and a boundary refinement module to sharpen edges. Experimental results on the Human Connectome Project (HCP) dataset demonstrate that BrainSegNet outperforms several state-of-the-art methods, achieving higher accuracy and robustness in complex, multi-label parcellation.

</details>


### [38] [GaussianFluent: Gaussian Simulation for Dynamic Scenes with Mixed Materials](https://arxiv.org/abs/2601.09265)
*Bei Huang,Yixin Chen,Ruijie Lu,Gang Zeng,Hongbin Zha,Yuru Pei,Siyuan Huang*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a prominent 3D representation for high-fidelity and real-time rendering. Prior work has coupled physics simulation with Gaussians, but predominantly targets soft, deformable materials, leaving brittle fracture largely unresolved. This stems from two key obstacles: the lack of volumetric interiors with coherent textures in GS representation, and the absence of fracture-aware simulation methods for Gaussians. To address these challenges, we introduce GaussianFluent, a unified framework for realistic simulation and rendering of dynamic object states. First, it synthesizes photorealistic interiors by densifying internal Gaussians guided by generative models. Second, it integrates an optimized Continuum Damage Material Point Method (CD-MPM) to enable brittle fracture simulation at remarkably high speed. Our approach handles complex scenarios including mixed-material objects and multi-stage fracture propagation, achieving results infeasible with previous methods. Experiments clearly demonstrate GaussianFluent's capability for photo-realistic, real-time rendering with structurally consistent interiors, highlighting its potential for downstream application, such as VR and Robotics.

</details>


### [39] [Multi-Modal LLM based Image Captioning in ICT: Bridging the Gap Between General and Industry Domain](https://arxiv.org/abs/2601.09298)
*Lianying Chao,Haoran Cai,Xubin Li,Kai Zhang,Sijie Wu,Rui Xu*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In the information and communications technology (ICT) industry, training a domain-specific large language model (LLM) or constructing a retrieval-augmented generation system requires a substantial amount of high-value domain knowledge. However, the knowledge is not only hidden in the textual modality but also in the image modality. Traditional methods can parse text from domain documents but dont have image captioning ability. Multi-modal LLM (MLLM) can understand images, but they do not have sufficient domain knowledge. To address the above issues, this paper proposes a multi-stage progressive training strategy to train a Domain-specific Image Captioning Model (DICModel) in ICT, and constructs a standard evaluation system to validate the performance of DICModel. Specifically, this work first synthesizes about 7K image-text pairs by combining the Mermaid tool and LLMs, which are used for the first-stage supervised-fine-tuning (SFT) of DICModel. Then, ICT-domain experts manually annotate about 2K image-text pairs for the second-stage SFT of DICModel. Finally, experts and LLMs jointly synthesize about 1.5K visual question answering data for the instruction-based SFT. Experimental results indicate that our DICModel with only 7B parameters performs better than other state-of-the-art models with 32B parameters. Compared to the SOTA models with 7B and 32B parameters, our DICModel increases the BLEU metric by approximately 56.8% and 20.8%, respectively. On the objective questions constructed by ICT domain experts, our DICModel outperforms Qwen2.5-VL 32B by 1% in terms of accuracy rate. In summary, this work can efficiently and accurately extract the logical text from images, which is expected to promote the development of multimodal models in the ICT domain.

</details>


### [40] [Frequency Error-Guided Under-sampling Optimization for Multi-Contrast MRI Reconstruction](https://arxiv.org/abs/2601.09316)
*Xinming Fang,Chaoyan Huang,Juncheng Li,Jun Wang,Jun Shi,Guixu Zhang*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Magnetic resonance imaging (MRI) plays a vital role in clinical diagnostics, yet it remains hindered by long acquisition times and motion artifacts. Multi-contrast MRI reconstruction has emerged as a promising direction by leveraging complementary information from fully-sampled reference scans. However, existing approaches suffer from three major limitations: (1) superficial reference fusion strategies, such as simple concatenation, (2) insufficient utilization of the complementary information provided by the reference contrast, and (3) fixed under-sampling patterns. We propose an efficient and interpretable frequency error-guided reconstruction framework to tackle these issues. We first employ a conditional diffusion model to learn a Frequency Error Prior (FEP), which is then incorporated into a unified framework for jointly optimizing both the under-sampling pattern and the reconstruction network. The proposed reconstruction model employs a model-driven deep unfolding framework that jointly exploits frequency- and image-domain information. In addition, a spatial alignment module and a reference feature decomposition strategy are incorporated to improve reconstruction quality and bridge model-based optimization with data-driven learning for improved physical interpretability. Comprehensive validation across multiple imaging modalities, acceleration rates (4-30x), and sampling schemes demonstrates consistent superiority over state-of-the-art methods in both quantitative metrics and visual quality. All codes are available at https://github.com/fangxinming/JUF-MRI.

</details>


### [41] [Detail Loss in Super-Resolution Models Based on the Laplacian Pyramid and Repeated Upscaling and Downscaling Process](https://arxiv.org/abs/2601.09410)
*Sangjun Han,Youngmi Hur*

Main category: cs.CV

TL;DR: 本研究提出了一种两步方法来增强超分辨率图像的高频细节：基于拉普拉斯金字塔的细节损失和重复上采样和下采样过程。这些方法在不同模型结构的实验中均表现出色，尤其是在基于注意机制的模型中添加了细节损失后，性能得到提升。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能的发展，图像处理技术得到了广泛关注。特别是在图像超分辨率领域，能够显著提升图像质量和突出高频细节的方法具有重要意义。

Method: 本文提出了两种方法来增强超分辨率图像的高频细节，分别为基于拉普拉斯金字塔的细节损失方法和重复上采样与下采样过程。其中，总损失包含细节损失，能够引导模型独立生成和控制超分辨率图像和细节图像。

Result: 使用本文提出的方法，作者设计了一个基于CNN的模型，其结果超越了当前所有基于CNN的方法，甚至一些基于注意力机制的方法。此外，将本文方法应用于现有的一些基于注意力机制的模型（小型实验），也显示出性能提升。

Conclusion: 总之，本文的方法有效地提升了不同模型结构中的图像超分辨率效果，尤其是结合了基于注意力机制的模型。

Abstract: With advances in artificial intelligence, image processing has gained significant interest. Image super-resolution is a vital technology closely related to real-world applications, as it enhances the quality of existing images. Since enhancing fine details is crucial for the super-resolution task, pixels that contribute to high-frequency information should be emphasized. This paper proposes two methods to enhance high-frequency details in super-resolution images: a Laplacian pyramid-based detail loss and a repeated upscaling and downscaling process. Total loss with our detail loss guides a model by separately generating and controlling super-resolution and detail images. This approach allows the model to focus more effectively on high-frequency components, resulting in improved super-resolution images. Additionally, repeated upscaling and downscaling amplify the effectiveness of the detail loss by extracting diverse information from multiple low-resolution features. We conduct two types of experiments. First, we design a CNN-based model incorporating our methods. This model achieves state-of-the-art results, surpassing all currently available CNN-based and even some attention-based models. Second, we apply our methods to existing attention-based models on a small scale. In all our experiments, attention-based models adding our detail loss show improvements compared to the originals. These results demonstrate our approaches effectively enhance super-resolution images across different model structures.

</details>


### [42] [Radiomics-Integrated Deep Learning with Hierarchical Loss for Osteosarcoma Histology Classification](https://arxiv.org/abs/2601.09416)
*Yaxi Chen,Zi Ye,Shaheer U. Saeed,Oliver Yu,Simin Ni,Jie Huang,Yipeng Hu*

Main category: cs.CV

TL;DR: 本文提出了一种新的方法，利用放射omics特征和层次损失函数改进了骨肉瘤肿瘤区域评估模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的骨肉瘤肿瘤区域评估方法存在劳动密集、主观性强、结果易受观察者影响的问题，而基于深度学习的方法可以实现自动化评估但性能下降。因此，本文旨在改进模型性能。

Method: 方法包括引入放射omics特征作为辅助输入，以及采用层次损失函数优化两个二分类任务。

Result: 实验表明，引入放射omics特征和层次损失函数分别提高了模型性能，并且他们的组合达到了该领域的新最佳性能。

Conclusion: 本文提出的方法在TCIA OS Tumor Assessment数据集上达到了新的性能标准，为骨肉瘤治疗规划提供了更准确的评估工具。

Abstract: Osteosarcoma (OS) is an aggressive primary bone malignancy. Accurate histopathological assessment of viable versus non-viable tumor regions after neoadjuvant chemotherapy is critical for prognosis and treatment planning, yet manual evaluation remains labor-intensive, subjective, and prone to inter-observer variability. Recent advances in digital pathology have enabled automated necrosis quantification. Evaluating on test data, independently sampled on patient-level, revealed that the deep learning model performance dropped significantly from the tile-level generalization ability reported in previous studies. First, this work proposes the use of radiomic features as additional input in model training. We show that, despite that they are derived from the images, such a multimodal input effectively improved the classification performance, in addition to its added benefits in interpretability. Second, this work proposes to optimize two binary classification tasks with hierarchical classes (i.e. tumor-vs-non-tumor and viable-vs-non-viable), as opposed to the alternative ``flat'' three-class classification task (i.e. non-tumor, non-viable tumor, viable tumor), thereby enabling a hierarchical loss. We show that such a hierarchical loss, with trainable weightings between the two tasks, the per-class performance can be improved significantly. Using the TCIA OS Tumor Assessment dataset, we experimentally demonstrate the benefits from each of the proposed new approaches and their combination, setting a what we consider new state-of-the-art performance on this open dataset for this application. Code and trained models: https://github.com/YaxiiC/RadiomicsOS.git.

</details>


### [43] [Video-MSR: Benchmarking Multi-hop Spatial Reasoning Capabilities of MLLMs](https://arxiv.org/abs/2601.09430)
*Rui Zhu,Xin Shen,Shuchen Wu,Chenxi Miao,Xin Yu,Yang Li,Weikang Li,Deguo Xia,Jizhou Huang*

Main category: cs.CV

TL;DR: Video-MSR是一个新颖的基准，旨在评估动态视频中多步骤空间推理（MSR）的能力，包含4052个视频实例和4993个问答对，通过多层次的任务设计和大规模模型评估展示了现有模型在MSR任务上的局限性，并提出一套专门的指令调整数据集，显著提升了模型的MSR能力。


<details>
  <summary>Details</summary>
Motivation: 当前的基准测试主要集中在单一感知-判断任务上，未能充分探索复杂视觉-空间逻辑链的需求。为了弥补这一空白，作者引入了Video-MSR基准测试，旨在评估MLLMs在动态视频中的多步骤空间推理能力。

Method: Video-MSR通过创建一个可扩展、视觉导向的生成管道，结合先进的模型生成和严格的人员验证来构建数据集，并针对20个状态最先进模型进行全面评估，根据评估结果发现模型在多步骤推理中的困难，并通过专门的指令调整数据集和微调模型来提升其MSR能力。

Result: 通过20个当前最先进模型的全面评估，作者发现模型在MSR任务上的表现显著低于在表面感知任务上的表现，经常在多步推理中出现空间混乱和幻想。通过使用新的指令调整数据集和对模型进行微调，Qwen-VL实现了Video-MSR上绝对改进7.82%。

Conclusion: 作者强调，多层次的空间推理指令数据的有效性，并将Video-MSR确立为未来研究的重要基础。

Abstract: Spatial reasoning has emerged as a critical capability for Multimodal Large Language Models (MLLMs), drawing increasing attention and rapid advancement. However, existing benchmarks primarily focus on single-step perception-to-judgment tasks, leaving scenarios requiring complex visual-spatial logical chains significantly underexplored. To bridge this gap, we introduce Video-MSR, the first benchmark specifically designed to evaluate Multi-hop Spatial Reasoning (MSR) in dynamic video scenarios. Video-MSR systematically probes MSR capabilities through four distinct tasks: Constrained Localization, Chain-based Reference Retrieval, Route Planning, and Counterfactual Physical Deduction. Our benchmark comprises 3,052 high-quality video instances with 4,993 question-answer pairs, constructed via a scalable, visually-grounded pipeline combining advanced model generation with rigorous human verification. Through a comprehensive evaluation of 20 state-of-the-art MLLMs, we uncover significant limitations, revealing that while models demonstrate proficiency in surface-level perception, they exhibit distinct performance drops in MSR tasks, frequently suffering from spatial disorientation and hallucination during multi-step deductions. To mitigate these shortcomings and empower models with stronger MSR capabilities, we further curate MSR-9K, a specialized instruction-tuning dataset, and fine-tune Qwen-VL, achieving a +7.82% absolute improvement on Video-MSR. Our results underscore the efficacy of multi-hop spatial instruction data and establish Video-MSR as a vital foundation for future research. The code and data will be available at https://github.com/ruiz-nju/Video-MSR.

</details>


### [44] [Do Transformers Understand Ancient Roman Coin Motifs Better than CNNs?](https://arxiv.org/abs/2601.09433)
*David Reid,Ognjen Arandjelovic*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Automated analysis of ancient coins has the potential to help researchers extract more historical insights from large collections of coins and to help collectors understand what they are buying or selling. Recent research in this area has shown promise in focusing on identification of semantic elements as they are commonly depicted on ancient coins, by using convolutional neural networks (CNNs). This paper is the first to apply the recently proposed Vision Transformer (ViT) deep learning architecture to the task of identification of semantic elements on coins, using fully automatic learning from multi-modal data (images and unstructured text). This article summarises previous research in the area, discusses the training and implementation of ViT and CNN models for ancient coins analysis and provides an evaluation of their performance. The ViT models were found to outperform the newly trained CNN models in accuracy.

</details>


### [45] [PrivLEX: Detecting legal concepts in images through Vision-Language Models](https://arxiv.org/abs/2601.09449)
*Darya Baranouskaya,Andrea Cavallaro*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We present PrivLEX, a novel image privacy classifier that grounds its decisions in legally defined personal data concepts. PrivLEX is the first interpretable privacy classifier aligned with legal concepts that leverages the recognition capabilities of Vision-Language Models (VLMs). PrivLEX relies on zero-shot VLM concept detection to provide interpretable classification through a label-free Concept Bottleneck Model, without requiring explicit concept labels during training. We demonstrate PrivLEX's ability to identify personal data concepts that are present in images. We further analyse the sensitivity of such concepts as perceived by human annotators of image privacy datasets.

</details>


### [46] [MAD: Motion Appearance Decoupling for efficient Driving World Models](https://arxiv.org/abs/2601.09452)
*Ahmad Rahimi,Valentin Gerard,Eloi Zablocki,Matthieu Cord,Alexandre Alahi*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Recent video diffusion models generate photorealistic, temporally coherent videos, yet they fall short as reliable world models for autonomous driving, where structured motion and physically consistent interactions are essential. Adapting these generalist video models to driving domains has shown promise but typically requires massive domain-specific data and costly fine-tuning. We propose an efficient adaptation framework that converts generalist video diffusion models into controllable driving world models with minimal supervision. The key idea is to decouple motion learning from appearance synthesis. First, the model is adapted to predict structured motion in a simplified form: videos of skeletonized agents and scene elements, focusing learning on physical and social plausibility. Then, the same backbone is reused to synthesize realistic RGB videos conditioned on these motion sequences, effectively "dressing" the motion with texture and lighting. This two-stage process mirrors a reasoning-rendering paradigm: first infer dynamics, then render appearance. Our experiments show this decoupled approach is exceptionally efficient: adapting SVD, we match prior SOTA models with less than 6% of their compute. Scaling to LTX, our MAD-LTX model outperforms all open-source competitors, and supports a comprehensive suite of text, ego, and object controls. Project page: https://vita-epfl.github.io/MAD-World-Model/

</details>


### [47] [Towards Robust Cross-Dataset Object Detection Generalization under Domain Specificity](https://arxiv.org/abs/2601.09497)
*Ritabrata Chakraborty,Hrishit Mitra,Shivakumara Palaiahnakote,Umapada Pal*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Object detectors often perform well in-distribution, yet degrade sharply on a different benchmark. We study cross-dataset object detection (CD-OD) through a lens of setting specificity. We group benchmarks into setting-agnostic datasets with diverse everyday scenes and setting-specific datasets tied to a narrow environment, and evaluate a standard detector family across all train--test pairs. This reveals a clear structure in CD-OD: transfer within the same setting type is relatively stable, while transfer across setting types drops substantially and is often asymmetric. The most severe breakdowns occur when transferring from specific sources to agnostic targets, and persist after open-label alignment, indicating that domain shift dominates in the hardest regimes. To disentangle domain shift from label mismatch, we compare closed-label transfer with an open-label protocol that maps predicted classes to the nearest target label using CLIP similarity. Open-label evaluation yields consistent but bounded gains, and many corrected cases correspond to semantic near-misses supported by the image evidence. Overall, we provide a principled characterization of CD-OD under setting specificity and practical guidance for evaluating detectors under distribution shift. Code will be released at \href{[https://github.com/Ritabrata04/cdod-icpr.git}{https://github.com/Ritabrata04/cdod-icpr}.

</details>


### [48] [V-DPM: 4D Video Reconstruction with Dynamic Point Maps](https://arxiv.org/abs/2601.09499)
*Edgar Sucar,Eldar Insafutdinov,Zihang Lai,Andrea Vedaldi*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Powerful 3D representations such as DUSt3R invariant point maps, which encode 3D shape and camera parameters, have significantly advanced feed forward 3D reconstruction. While point maps assume static scenes, Dynamic Point Maps (DPMs) extend this concept to dynamic 3D content by additionally representing scene motion. However, existing DPMs are limited to image pairs and, like DUSt3R, require post processing via optimization when more than two views are involved. We argue that DPMs are more useful when applied to videos and introduce V-DPM to demonstrate this. First, we show how to formulate DPMs for video input in a way that maximizes representational power, facilitates neural prediction, and enables reuse of pretrained models. Second, we implement these ideas on top of VGGT, a recent and powerful 3D reconstructor. Although VGGT was trained on static scenes, we show that a modest amount of synthetic data is sufficient to adapt it into an effective V-DPM predictor. Our approach achieves state of the art performance in 3D and 4D reconstruction for dynamic scenes. In particular, unlike recent dynamic extensions of VGGT such as P3, DPMs recover not only dynamic depth but also the full 3D motion of every point in the scene.

</details>


### [49] [Video Joint-Embedding Predictive Architectures for Facial Expression Recognition](https://arxiv.org/abs/2601.09524)
*Lennart Eing,Cristina Luna-Jiménez,Silvan Mertes,Elisabeth André*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This paper introduces a novel application of Video Joint-Embedding Predictive Architectures (V-JEPAs) for Facial Expression Recognition (FER). Departing from conventional pre-training methods for video understanding that rely on pixel-level reconstructions, V-JEPAs learn by predicting embeddings of masked regions from the embeddings of unmasked regions. This enables the trained encoder to not capture irrelevant information about a given video like the color of a region of pixels in the background. Using a pre-trained V-JEPA video encoder, we train shallow classifiers using the RAVDESS and CREMA-D datasets, achieving state-of-the-art performance on RAVDESS and outperforming all other vision-based methods on CREMA-D (+1.48 WAR). Furthermore, cross-dataset evaluations reveal strong generalization capabilities, demonstrating the potential of purely embedding-based pre-training approaches to advance FER. We release our code at https://github.com/lennarteingunia/vjepa-for-fer.

</details>


### [50] [GlovEgo-HOI: Bridging the Synthetic-to-Real Gap for Industrial Egocentric Human-Object Interaction Detection](https://arxiv.org/abs/2601.09528)
*Alfio Spoto,Rosario Leonardi,Francesco Ragusa,Giovanni Maria Farinella*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Egocentric Human-Object Interaction (EHOI) analysis is crucial for industrial safety, yet the development of robust models is hindered by the scarcity of annotated domain-specific data. We address this challenge by introducing a data generation framework that combines synthetic data with a diffusion-based process to augment real-world images with realistic Personal Protective Equipment (PPE). We present GlovEgo-HOI, a new benchmark dataset for industrial EHOI, and GlovEgo-Net, a model integrating Glove-Head and Keypoint- Head modules to leverage hand pose information for enhanced interaction detection. Extensive experiments demonstrate the effectiveness of the proposed data generation framework and GlovEgo-Net. To foster further research, we release the GlovEgo-HOI dataset, augmentation pipeline, and pre-trained models at: GitHub project.

</details>


### [51] [Bipartite Mode Matching for Vision Training Set Search from a Hierarchical Data Server](https://arxiv.org/abs/2601.09531)
*Yue Yao,Ruining Yang,Tom Gedeon*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We explore a situation in which the target domain is accessible, but real-time data annotation is not feasible. Instead, we would like to construct an alternative training set from a large-scale data server so that a competitive model can be obtained. For this problem, because the target domain usually exhibits distinct modes (i.e., semantic clusters representing data distribution), if the training set does not contain these target modes, the model performance would be compromised. While prior existing works improve algorithms iteratively, our research explores the often-overlooked potential of optimizing the structure of the data server. Inspired by the hierarchical nature of web search engines, we introduce a hierarchical data server, together with a bipartite mode matching algorithm (BMM) to align source and target modes. For each target mode, we look in the server data tree for the best mode match, which might be large or small in size. Through bipartite matching, we aim for all target modes to be optimally matched with source modes in a one-on-one fashion. Compared with existing training set search algorithms, we show that the matched server modes constitute training sets that have consistently smaller domain gaps with the target domain across object re-identification (re-ID) and detection tasks. Consequently, models trained on our searched training sets have higher accuracy than those trained otherwise. BMM allows data-centric unsupervised domain adaptation (UDA) orthogonal to existing model-centric UDA methods. By combining the BMM with existing UDA methods like pseudo-labeling, further improvement is observed.

</details>


### [52] [Hot-Start from Pixels: Low-Resolution Visual Tokens for Chinese Language Modeling](https://arxiv.org/abs/2601.09566)
*Shuyang Xiang,Hao Guan*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large language models typically represent Chinese characters as discrete index-based tokens, largely ignoring their visual form. For logographic scripts, visual structure carries semantic and phonetic information, which may aid prediction. We investigate whether low-resolution visual inputs can serve as an alternative for character-level modeling. Instead of token IDs, our decoder receives grayscale images of individual characters, with resolutions as low as $8 \times 8$ pixels. Remarkably, these inputs achieve 39.2\% accuracy, comparable to the index-based baseline of 39.1\%. Such low-resource settings also exhibit a pronounced \emph{hot-start} effect: by 0.4\% of total training, accuracy reaches above 12\%, while index-based models lag at below 6\%. Overall, our results demonstrate that minimal visual structure can provide a robust and efficient signal for Chinese language modeling, offering an alternative perspective on character representation that complements traditional index-based approaches.

</details>


### [53] [Trustworthy Longitudinal Brain MRI Completion: A Deformation-Based Approach with KAN-Enhanced Diffusion Model](https://arxiv.org/abs/2601.09572)
*Tianli Tao,Ziyang Wang,Delong Yang,Han Zhang,Le Zhang*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Longitudinal brain MRI is essential for lifespan study, yet high attrition rates often lead to missing data, complicating analysis. Deep generative models have been explored, but most rely solely on image intensity, leading to two key limitations: 1) the fidelity or trustworthiness of the generated brain images are limited, making downstream studies questionable; 2) the usage flexibility is restricted due to fixed guidance rooted in the model structure, restricting full ability to versatile application scenarios. To address these challenges, we introduce DF-DiffCom, a Kolmogorov-Arnold Networks (KAN)-enhanced diffusion model that smartly leverages deformation fields for trustworthy longitudinal brain image completion. Trained on OASIS-3, DF-DiffCom outperforms state-of-the-art methods, improving PSNR by 5.6% and SSIM by 0.12. More importantly, its modality-agnostic nature allows smooth extension to varied MRI modalities, even to attribute maps such as brain tissue segmentation results.

</details>


### [54] [OpenVoxel: Training-Free Grouping and Captioning Voxels for Open-Vocabulary 3D Scene Understanding](https://arxiv.org/abs/2601.09575)
*Sheng-Yu Huang,Jaesung Choe,Yu-Chiang Frank Wang,Cheng Sun*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We propose OpenVoxel, a training-free algorithm for grouping and captioning sparse voxels for the open-vocabulary 3D scene understanding tasks. Given the sparse voxel rasterization (SVR) model obtained from multi-view images of a 3D scene, our OpenVoxel is able to produce meaningful groups that describe different objects in the scene. Also, by leveraging powerful Vision Language Models (VLMs) and Multi-modal Large Language Models (MLLMs), our OpenVoxel successfully build an informative scene map by captioning each group, enabling further 3D scene understanding tasks such as open-vocabulary segmentation (OVS) or referring expression segmentation (RES). Unlike previous methods, our method is training-free and does not introduce embeddings from a CLIP/BERT text encoder. Instead, we directly proceed with text-to-text search using MLLMs. Through extensive experiments, our method demonstrates superior performance compared to recent studies, particularly in complex referring expression segmentation (RES) tasks. The code will be open.

</details>


### [55] [Show, don't tell -- Providing Visual Error Feedback for Handwritten Documents](https://arxiv.org/abs/2601.09586)
*Said Yasin,Torsten Zesch*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Handwriting remains an essential skill, particularly in education. Therefore, providing visual feedback on handwritten documents is an important but understudied area. We outline the many challenges when going from an image of handwritten input to correctly placed informative error feedback. We empirically compare modular and end-to-end systems and find that both approaches currently do not achieve acceptable overall quality. We identify the major challenges and outline an agenda for future research.

</details>


### [56] [Iterative Differential Entropy Minimization (IDEM) method for fine rigid pairwise 3D Point Cloud Registration: A Focus on the Metric](https://arxiv.org/abs/2601.09601)
*Emmanuele Barberi,Felice Sfravara,Filippo Cucinotta*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Point cloud registration is a central theme in computer vision, with alignment algorithms continuously improving for greater robustness. Commonly used methods evaluate Euclidean distances between point clouds and minimize an objective function, such as Root Mean Square Error (RMSE). However, these approaches are most effective when the point clouds are well-prealigned and issues such as differences in density, noise, holes, and limited overlap can compromise the results. Traditional methods, such as Iterative Closest Point (ICP), require choosing one point cloud as fixed, since Euclidean distances lack commutativity. When only one point cloud has issues, adjustments can be made, but in real scenarios, both point clouds may be affected, often necessitating preprocessing. The authors introduce a novel differential entropy-based metric, designed to serve as the objective function within an optimization framework for fine rigid pairwise 3D point cloud registration, denoted as Iterative Differential Entropy Minimization (IDEM). This metric does not depend on the choice of a fixed point cloud and, during transformations, reveals a clear minimum corresponding to the best alignment. Multiple case studies are conducted, and the results are compared with those obtained using RMSE, Chamfer distance, and Hausdorff distance. The proposed metric proves effective even with density differences, noise, holes, and partial overlap, where RMSE does not always yield optimal alignment.

</details>


### [57] [Sim2real Image Translation Enables Viewpoint-Robust Policies from Fixed-Camera Datasets](https://arxiv.org/abs/2601.09605)
*Jeremiah Coholich,Justin Wit,Robert Azarcon,Zsolt Kira*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Vision-based policies for robot manipulation have achieved significant recent success, but are still brittle to distribution shifts such as camera viewpoint variations. Robot demonstration data is scarce and often lacks appropriate variation in camera viewpoints. Simulation offers a way to collect robot demonstrations at scale with comprehensive coverage of different viewpoints, but presents a visual sim2real challenge. To bridge this gap, we propose MANGO -- an unpaired image translation method with a novel segmentation-conditioned InfoNCE loss, a highly-regularized discriminator design, and a modified PatchNCE loss. We find that these elements are crucial for maintaining viewpoint consistency during sim2real translation. When training MANGO, we only require a small amount of fixed-camera data from the real world, but show that our method can generate diverse unseen viewpoints by translating simulated observations. In this domain, MANGO outperforms all other image translation methods we tested. Imitation-learning policies trained on data augmented by MANGO are able to achieve success rates as high as 60\% on views that the non-augmented policy fails completely on.

</details>


### [58] [GRCF: Two-Stage Groupwise Ranking and Calibration Framework for Multimodal Sentiment Analysis](https://arxiv.org/abs/2601.09606)
*Manning Gao,Leheng Zhang,Shiqin Han,Haifeng Hu,Yuncheng Jiang,Sijie Mai*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Most Multimodal Sentiment Analysis research has focused on point-wise regression. While straightforward, this approach is sensitive to label noise and neglects whether one sample is more positive than another, resulting in unstable predictions and poor correlation alignment. Pairwise ordinal learning frameworks emerged to address this gap, capturing relative order by learning from comparisons. Yet, they introduce two new trade-offs: First, they assign uniform importance to all comparisons, failing to adaptively focus on hard-to-rank samples. Second, they employ static ranking margins, which fail to reflect the varying semantic distances between sentiment groups. To address this, we propose a Two-Stage Group-wise Ranking and Calibration Framework (GRCF) that adapts the philosophy of Group Relative Policy Optimization (GRPO). Our framework resolves these trade-offs by simultaneously preserving relative ordinal structure, ensuring absolute score calibration, and adaptively focusing on difficult samples. Specifically, Stage 1 introduces a GRPO-inspired Advantage-Weighted Dynamic Margin Ranking Loss to build a fine-grained ordinal structure. Stage 2 then employs an MAE-driven objective to align prediction magnitudes. To validate its generalizability, we extend GRCF to classification tasks, including multimodal humor detection and sarcasm detection. GRCF achieves state-of-the-art performance on core regression benchmarks, while also showing strong generalizability in classification tasks.

</details>


### [59] [CogRail: Benchmarking VLMs in Cognitive Intrusion Perception for Intelligent Railway Transportation Systems](https://arxiv.org/abs/2601.09613)
*Yonglin Tian,Qiyao Zhang,Wei Xu,Yutong Wang,Yihao Wu,Xinyi Li,Xingyuan Dai,Hui Zhang,Zhiyong Cui,Baoqing Guo,Zujun Yu,Yisheng Lv*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Accurate and early perception of potential intrusion targets is essential for ensuring the safety of railway transportation systems. However, most existing systems focus narrowly on object classification within fixed visual scopes and apply rule-based heuristics to determine intrusion status, often overlooking targets that pose latent intrusion risks. Anticipating such risks requires the cognition of spatial context and temporal dynamics for the object of interest (OOI), which presents challenges for conventional visual models. To facilitate deep intrusion perception, we introduce a novel benchmark, CogRail, which integrates curated open-source datasets with cognitively driven question-answer annotations to support spatio-temporal reasoning and prediction. Building upon this benchmark, we conduct a systematic evaluation of state-of-the-art visual-language models (VLMs) using multimodal prompts to identify their strengths and limitations in this domain. Furthermore, we fine-tune VLMs for better performance and propose a joint fine-tuning framework that integrates three core tasks, position perception, movement prediction, and threat analysis, facilitating effective adaptation of general-purpose foundation models into specialized models tailored for cognitive intrusion perception. Extensive experiments reveal that current large-scale multimodal models struggle with the complex spatial-temporal reasoning required by the cognitive intrusion perception task, underscoring the limitations of existing foundation models in this safety-critical domain. In contrast, our proposed joint fine-tuning framework significantly enhances model performance by enabling targeted adaptation to domain-specific reasoning demands, highlighting the advantages of structured multi-task learning in improving both accuracy and interpretability. Code will be available at https://github.com/Hub-Tian/CogRail.

</details>


### [60] [Identifying Models Behind Text-to-Image Leaderboards](https://arxiv.org/abs/2601.09647)
*Ali Naseh,Yuefeng Peng,Anshuman Suri,Harsh Chaudhari,Alina Oprea,Amir Houmansadr*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Text-to-image (T2I) models are increasingly popular, producing a large share of AI-generated images online. To compare model quality, voting-based leaderboards have become the standard, relying on anonymized model outputs for fairness. In this work, we show that such anonymity can be easily broken. We find that generations from each T2I model form distinctive clusters in the image embedding space, enabling accurate deanonymization without prompt control or training data. Using 22 models and 280 prompts (150K images), our centroid-based method achieves high accuracy and reveals systematic model-specific signatures. We further introduce a prompt-level distinguishability metric and conduct large-scale analyses showing how certain prompts can lead to near-perfect distinguishability. Our findings expose fundamental security flaws in T2I leaderboards and motivate stronger anonymization defenses.

</details>


### [61] [AquaFeat+: an Underwater Vision Learning-based Enhancement Method for Object Detection, Classification, and Tracking](https://arxiv.org/abs/2601.09652)
*Emanuel da Costa Silva,Tatiana Taís Schein,José David García Ramos,Eduardo Lawson da Silva,Stephanie Loi Brião,Felipe Gomes de Oliveira,Paulo Lilles Jorge Drews-Jr*

Main category: cs.CV

TL;DR: 该研究提出了一种名为AquaFeat+的插件式流水线，专门针对自动视觉任务进行特征增强，而不是为了提高人类视觉感知质量。该流水线通过颜色校正、分层特征增强以及自适应残差输出模块训练，并在FishTrack23数据集中进行了训练和评估，显著提升了水下目标检测、分类和跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 水下视频分析由于低光照、颜色失真和浑浊等因素，严重影响了视觉数据质量和机器人应用中的感知模块性能。本研究旨在克服这些挑战，通过专门的特征增强方法提高水下环境下的视觉感知能力。

Method: AquaFeat+通过颜色校正、分层特征增强和自适应残差输出模块实现了自底向上的端到端训练。该流水线旨在优化自动化视觉任务，每一步都直接遵循最终应用的损失函数进行优化。

Result: AquaFeat+在FishTrack23数据集上的实验结果显示了显著的物体检测、分类和跟踪性能改进，证明了其在水下机器人应用中的有效性。

Conclusion: AquaFeat+为水下机器人系统的感知任务提供了高效的特征增强方案，验证了其在复杂水下环境下的应用价值。

Abstract: Underwater video analysis is particularly challenging due to factors such as low lighting, color distortion, and turbidity, which compromise visual data quality and directly impact the performance of perception modules in robotic applications. This work proposes AquaFeat+, a plug-and-play pipeline designed to enhance features specifically for automated vision tasks, rather than for human perceptual quality. The architecture includes modules for color correction, hierarchical feature enhancement, and an adaptive residual output, which are trained end-to-end and guided directly by the loss function of the final application. Trained and evaluated in the FishTrack23 dataset, AquaFeat+ achieves significant improvements in object detection, classification, and tracking metrics, validating its effectiveness for enhancing perception tasks in underwater robotic applications.

</details>


### [62] [Image2Garment: Simulation-ready Garment Generation from a Single Image](https://arxiv.org/abs/2601.09658)
*Selim Emir Can,Jan Ackermann,Kiyohiro Nakayama,Ruofan Liu,Tong Wu,Yang Zheng,Hugo Bertiche,Menglei Chai,Thabo Beeler,Gordon Wetzstein*

Main category: cs.CV

TL;DR: 该研究提出了一种无需多视图采集和昂贵的可微模拟直接从单张图像生成可模拟的服装数据的方法，通过两个新数据集和前向框架提高材料组成和织物属性的估计精度，从而实现更真实的模拟。


<details>
  <summary>Details</summary>
Motivation: 现有方法因需要多视角采集和昂贵的可微模拟，或是仅预测服装几何结构而无法提供用于真实模拟的材料属性，限制了从图像生成模拟就绪的服装的能力。

Method: 该方法首先微调一个视觉-语言模型以从真实图像推断材料组成和织物属性，然后使用包含材料-物理测量的小型数据集训练一个轻量级预测器，将这些属性映射到对应的物理织物参数。

Result: 实验表明，该方法在材料组成估计和织物属性预测方面具有更高的准确性，并且通过物理参数估计器进行模拟时，与最先进的图像到服装方法相比，获得了更高的保真度的模拟。

Conclusion: 该研究通过引入新数据集和一个新的前向框架，大大提高了从单张图像生成可用于模拟的服装的准确性与实际性。

Abstract: Estimating physically accurate, simulation-ready garments from a single image is challenging due to the absence of image-to-physics datasets and the ill-posed nature of this problem. Prior methods either require multi-view capture and expensive differentiable simulation or predict only garment geometry without the material properties required for realistic simulation. We propose a feed-forward framework that sidesteps these limitations by first fine-tuning a vision-language model to infer material composition and fabric attributes from real images, and then training a lightweight predictor that maps these attributes to the corresponding physical fabric parameters using a small dataset of material-physics measurements. Our approach introduces two new datasets (FTAG and T2P) and delivers simulation-ready garments from a single image without iterative optimization. Experiments show that our estimator achieves superior accuracy in material composition estimation and fabric attribute prediction, and by passing them through our physics parameter estimator, we further achieve higher-fidelity simulations compared to state-of-the-art image-to-garment methods.

</details>


### [63] [LiteEmbed: Adapting CLIP to Rare Classes](https://arxiv.org/abs/2601.09661)
*Aishwarya Agarwal,Srikrishna Karanam,Vineet Gandhi*

Main category: cs.CV

TL;DR: LiteEmbed通过在CLIP中优化文本嵌入，不重新训练编码器，实现了对少见类别的适应，特别是新出现的实体和文化特定类别的识别能力提升。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模视觉-语言模型如CLIP，在面对罕见类别时表现不佳，尤其在新增实体和文化特定类别上。因此，研究团队提出LiteEmbed框架，旨在提高在这些类别上的识别能力。

Method: LiteEmbed采用基于PCA的分解，将粗粒度语义方向与细粒度变化分离，并通过粗对齐和细分离两个互补目标，在保持全局语义一致性的同时增强视觉相似类别间的可区分性。

Result: 实验结果表明，LiteEmbed在多个任务（分类、检索、分割和检测）上的效果显著超过先前方法，尤其是在少见类别的识别上。

Conclusion: LiteEmbed提供了一种有效的方法来适配CLIP到未充分代表、稀有或未出现过的类别，为视觉语言模型的进一步应用提供了新的解决方案。

Abstract: Large-scale vision-language models such as CLIP achieve strong zero-shot recognition but struggle with classes that are rarely seen during pretraining, including newly emerging entities and culturally specific categories. We introduce LiteEmbed, a lightweight framework for few-shot personalization of CLIP that enables new classes to be added without retraining its encoders. LiteEmbed performs subspace-guided optimization of text embeddings within CLIP's vocabulary, leveraging a PCA-based decomposition that disentangles coarse semantic directions from fine-grained variations. Two complementary objectives, coarse alignment and fine separation, jointly preserve global semantic consistency while enhancing discriminability among visually similar classes. Once optimized, the embeddings are plug-and-play, seamlessly substituting CLIP's original text features across classification, retrieval, segmentation, and detection tasks. Extensive experiments demonstrate substantial gains over prior methods, establishing LiteEmbed as an effective approach for adapting CLIP to underrepresented, rare, or unseen classes.

</details>


### [64] [SCE-SLAM: Scale-Consistent Monocular SLAM via Scene Coordinate Embeddings](https://arxiv.org/abs/2601.09665)
*Yuchen Wu,Jiahe Li,Xiaohan Yu,Lina Yu,Jin Zheng,Xiao Bai*

Main category: cs.CV

TL;DR: 该研究提出了一种名为SCE-SLAM的端到端系统，通过场景坐标嵌入保持尺度一致性来解决单目视觉SLAM中的尺度漂移问题，实验结果显示在轨迹误差、实时性和尺度一致性上都有显著改进。


<details>
  <summary>Details</summary>
Motivation: 在单目视觉SLAM中，由于缺乏全局约束，帧到帧的方法会积累尺度漂移，影响系统的长期稳定性。因此，需要提出一种能够保持全局尺度一致性的方法。

Method: SCE-SLAM通过学习基于3D几何关系的场景坐标嵌入，利用几何导向聚合模块传播尺度信息和场景坐标束调整模块锚定当前估计值到参考尺度，从而实现尺度一致性。

Result: 在KITTI、Waymo和vKITTI的数据集上，该研究的方法在KITTIDataSet上比最佳现有方法的绝对轨迹误差降低了8.36m，同时保持了36 FPS的实时性能，并且在大规模场景中保持了尺度一致性。

Conclusion: SCE-SLAM通过引入场景坐标嵌入和相关模块，解决了单目视觉SLAM中的尺度漂移问题，在多个数据集上展示了显著的性能提升。

Abstract: Monocular visual SLAM enables 3D reconstruction from internet video and autonomous navigation on resource-constrained platforms, yet suffers from scale drift, i.e., the gradual divergence of estimated scale over long sequences. Existing frame-to-frame methods achieve real-time performance through local optimization but accumulate scale drift due to the lack of global constraints among independent windows. To address this, we propose SCE-SLAM, an end-to-end SLAM system that maintains scale consistency through scene coordinate embeddings, which are learned patch-level representations encoding 3D geometric relationships under a canonical scale reference. The framework consists of two key modules: geometry-guided aggregation that leverages 3D spatial proximity to propagate scale information from historical observations through geometry-modulated attention, and scene coordinate bundle adjustment that anchors current estimates to the reference scale through explicit 3D coordinate constraints decoded from the scene coordinate embeddings. Experiments on KITTI, Waymo, and vKITTI demonstrate substantial improvements: our method reduces absolute trajectory error by 8.36m on KITTI compared to the best prior approach, while maintaining 36 FPS and achieving scale consistency across large-scale scenes.

</details>


### [65] [STEP3-VL-10B Technical Report](https://arxiv.org/abs/2601.09668)
*Ailin Huang,Chengyuan Yao,Chunrui Han,Fanqi Wan,Hangyu Guo,Haoran Lv,Hongyu Zhou,Jia Wang,Jian Zhou,Jianjian Sun,Jingcheng Hu,Kangheng Lin,Liang Zhao,Mitt Huang,Song Yuan,Wenwen Qu,Xiangfeng Wang,Yanlin Lai,Yingxiu Zhao,Yinmin Zhang,Yukang Shi,Yuyang Chen,Zejia Weng,Ziyang Meng,Ang Li,Aobo Kong,Bo Dong,Changyi Wan,David Wang,Di Qi,Dingming Li,En Yu,Guopeng Li,Haiquan Yin,Han Zhou,Hanshan Zhang,Haolong Yan,Hebin Zhou,Hongbo Peng,Jiaran Zhang,Jiashu Lv,Jiayi Fu,Jie Cheng,Jie Zhou,Jisheng Yin,Jingjing Xie,Jingwei Wu,Jun Zhang,Junfeng Liu,Kaijun Tan,Kaiwen Yan,Liangyu Chen,Lina Chen,Mingliang Li,Qian Zhao,Quan Sun,Shaoliang Pang,Shengjie Fan,Shijie Shang,Siyuan Zhang,Tianhao You,Wei Ji,Wuxun Xie,Xiaobo Yang,Xiaojie Hou,Xiaoran Jiao,Xiaoxiao Ren,Xiangwen Kong,Xin Huang,Xin Wu,Xing Chen,Xinran Wang,Xuelin Zhang,Yana Wei,Yang Li,Yanming Xu,Yeqing Shen,Yuang Peng,Yue Peng,Yu Zhou,Yusheng Li,Yuxiang Yang,Yuyang Zhang,Zhe Xie,Zhewei Huang,Zhenyi Lu,Zhimin Fan,Zihui Cheng,Daxin Jiang,Qi Han,Xiangyu Zhang,Yibo Zhu,Zheng Ge*

Main category: cs.CV

TL;DR: STEP3-VL-10B 是一个轻量级开源基础模型，通过统一未冻结预训练策略和扩展后训练管道实现高效多模态智能，尽管只有10B参数，在多个评测指标上超过了比它大10到20倍的模型。


<details>
  <summary>Details</summary>
Motivation: 技术的进步使得在轻量级模型中实现高水平的多模态智能成为可能，STEP3-VL-10B旨在通过优化预训练和后训练过程来弥补小型模型在计算资源和性能之间的传统差距。

Method: 通过采用统一的未冻结预训练策略和增强的后训练管道，STEP3-VL-10B实现了高效的多模态处理。具体包括：1) 使用1.2T多模态 token 进行预训练，将语言对齐的感知编码器与Qwen3-8B解码器相结合；2) 实施了平行协调推理 (PaCoRe) 来扩展推理时间的计算能力，以便于探索和合成多样化的视觉假设。

Result: STEP3-VL-10B 在多个评测任务中表现出超越同类大型模型的性能，包括：1) MMBench 92.2%，MMMU 80.11%；2) AIME2025 94.43% 和 MathVision 75.95%。这表明尽管参数量较少，该模型仍能保持高质量的多模态处理能力。

Conclusion: STEP3-VL-10B 的发布为开源社区提供了一个高效且易复现的高性能基础模型，有望推动多模态 AI 的发展和应用。

Abstract: We present STEP3-VL-10B, a lightweight open-source foundation model designed to redefine the trade-off between compact efficiency and frontier-level multimodal intelligence. STEP3-VL-10B is realized through two strategic shifts: first, a unified, fully unfrozen pre-training strategy on 1.2T multimodal tokens that integrates a language-aligned Perception Encoder with a Qwen3-8B decoder to establish intrinsic vision-language synergy; and second, a scaled post-training pipeline featuring over 1k iterations of reinforcement learning. Crucially, we implement Parallel Coordinated Reasoning (PaCoRe) to scale test-time compute, allocating resources to scalable perceptual reasoning that explores and synthesizes diverse visual hypotheses. Consequently, despite its compact 10B footprint, STEP3-VL-10B rivals or surpasses models 10$\times$-20$\times$ larger (e.g., GLM-4.6V-106B, Qwen3-VL-235B) and top-tier proprietary flagships like Gemini 2.5 Pro and Seed-1.5-VL. Delivering best-in-class performance, it records 92.2% on MMBench and 80.11% on MMMU, while excelling in complex reasoning with 94.43% on AIME2025 and 75.95% on MathVision. We release the full model suite to provide the community with a powerful, efficient, and reproducible baseline.

</details>


### [66] [Efficient Camera-Controlled Video Generation of Static Scenes via Sparse Diffusion and 3D Rendering](https://arxiv.org/abs/2601.09697)
*Jieying Chen,Jeffrey Hu,Joan Lasenby,Ayush Tewari*

Main category: cs.CV

TL;DR: 本文提出了一种新的策略，通过使用基于扩散模型生成稀疏关键帧，然后通过3D重建和渲染合成完整的视频，以提高视频生成的速度，同时保持高质量和时间稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于扩散模型的视频生成模型虽然可以生成逼真的视频，但计算效率低下，不适合需要实时交互的应用场景。本文旨在解决这一问题。

Method: 提出了一种基于稀疏关键帧的3D重建和渲染方法，通过将关键帧提升到3D表示，渲染中间视图来摊销生成成本，同时保证几何一致性。

Result: 实验表明，相比基于扩散模型的基线，本文方法在生成20秒视频时速度快40倍以上，同时保持高质量和时间稳定性。

Conclusion: 本文提出的方法为高效的可控制视频合成提供了一条实际可行的道路。

Abstract: Modern video generative models based on diffusion models can produce very realistic clips, but they are computationally inefficient, often requiring minutes of GPU time for just a few seconds of video. This inefficiency poses a critical barrier to deploying generative video in applications that require real-time interactions, such as embodied AI and VR/AR. This paper explores a new strategy for camera-conditioned video generation of static scenes: using diffusion-based generative models to generate a sparse set of keyframes, and then synthesizing the full video through 3D reconstruction and rendering. By lifting keyframes into a 3D representation and rendering intermediate views, our approach amortizes the generation cost across hundreds of frames while enforcing geometric consistency. We further introduce a model that predicts the optimal number of keyframes for a given camera trajectory, allowing the system to adaptively allocate computation. Our final method, SRENDER, uses very sparse keyframes for simple trajectories and denser ones for complex camera motion. This results in video generation that is more than 40 times faster than the diffusion-based baseline in generating 20 seconds of video, while maintaining high visual fidelity and temporal stability, offering a practical path toward efficient and controllable video synthesis.

</details>


### [67] [COMPOSE: Hypergraph Cover Optimization for Multi-view 3D Human Pose Estimation](https://arxiv.org/abs/2601.09698)
*Tony Danjun Wang,Tolga Birdal,Nassir Navab,Lennart Bastian*

Main category: cs.CV

TL;DR: COMPOSE 提出了一种新颖的方法，将多视图姿态对应匹配问题重新定义为超图拆分问题，通过几何剪枝策略有效减少搜索空间，从而提高3D姿态估计的精度。


<details>
  <summary>Details</summary>
Motivation: 当前基于优化的方法在多视图姿态估计中主要依赖于单两两视图之间的关联，这在处理大量视图时容易因为错误关联而累积误差。COMPOSE 提出的框架旨在通过将问题转化为超图拆分问题来处理这种对应关系，以更好地保证全局一致性。

Method: COMPOSE 使用超图拆分方法解决多视图对应问题，并引入几何剪枝策略减小搜索空间。这使得在理论上呈指数增大的整数线性规划问题可以高效解决。

Result: COMPOSE 在平均精度上相比之前基于优化的方法提升了 23%，相比端到端自我监督学习方法提升了 11%，展示了在广泛研究问题上的有效性和优越性。

Conclusion: COMPOSE 提供了一种新的解决多视图姿态对应问题的方法，代表了该领域的一项进展。

Abstract: 3D pose estimation from sparse multi-views is a critical task for numerous applications, including action recognition, sports analysis, and human-robot interaction. Optimization-based methods typically follow a two-stage pipeline, first detecting 2D keypoints in each view and then associating these detections across views to triangulate the 3D pose. Existing methods rely on mere pairwise associations to model this correspondence problem, treating global consistency between views (i.e., cycle consistency) as a soft constraint. Yet, reconciling these constraints for multiple views becomes brittle when spurious associations propagate errors. We thus propose COMPOSE, a novel framework that formulates multi-view pose correspondence matching as a hypergraph partitioning problem rather than through pairwise association. While the complexity of the resulting integer linear program grows exponentially in theory, we introduce an efficient geometric pruning strategy to substantially reduce the search space. COMPOSE achieves improvements of up to 23% in average precision over previous optimization-based methods and up to 11% over self-supervised end-to-end learned methods, offering a promising solution to a widely studied problem.

</details>


### [68] [SAM3-DMS: Decoupled Memory Selection for Multi-target Video Segmentation of SAM3](https://arxiv.org/abs/2601.09699)
*Ruiqi Shen,Chang Liu,Henghui Ding*

Main category: cs.CV

TL;DR: SAM3-DMS 是一种无需训练的分时间段的策略，它在个体对象上使用细粒度的记忆选择。实验表明，该方法在保持身份一致性和跟踪稳定性方面表现出色，尤其是在多个目标密度增加时。


<details>
  <summary>Details</summary>
Motivation: 当前的 SAM3 方法在处理复杂的多对象场景时存在不足，因为它依赖同步决策并基于所有并发目标的平均性能，容易忽略个体可靠性。为此，SAM3-DMS 提出了一个不需要训练的分解策略，通过在个体对象上使用细粒度的记忆选择改进目标检测、分割和跟踪。

Method: SAM3-DMS 使用了一种无需训练的分时间段策略，在个体对象上进行细粒度的记忆选择，而不依赖于所有目标的整体性能评估。这种方法优化了目标检测和跟踪的个体可靠性。

Result: 实验结果表明，SAM3-DMS 能够在保持身份一致性和跟踪稳定性方面取得显著效果。特别是在目标密度增加的复杂多目标场景中，其优势更加明显。

Conclusion: 总之，SAM3-DMS 为同时在野外进行多目标视频分割提供了一个坚实的基础，展示了在复杂多目标环境中卓越的性能。

Abstract: Segment Anything 3 (SAM3) has established a powerful foundation that robustly detects, segments, and tracks specified targets in videos. However, in its original implementation, its group-level collective memory selection is suboptimal for complex multi-object scenarios, as it employs a synchronized decision across all concurrent targets conditioned on their average performance, often overlooking individual reliability. To this end, we propose SAM3-DMS, a training-free decoupled strategy that utilizes fine-grained memory selection on individual objects. Experiments demonstrate that our approach achieves robust identity preservation and tracking stability. Notably, our advantage becomes more pronounced with increased target density, establishing a solid foundation for simultaneous multi-target video segmentation in the wild.

</details>


### [69] [Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning](https://arxiv.org/abs/2601.09708)
*Chi-Pin Huang,Yunze Man,Zhiding Yu,Min-Hung Chen,Jan Kautz,Yu-Chiang Frank Wang,Fu-En Yang*

Main category: cs.CV

TL;DR: Fast-ThinkAct 是一种高效的推理框架，通过可表达的隐藏推理实现紧凑且高性能的规划，其在多种物质操作与推理基准上的实验表明，Fast-ThinkAct 在保持高效的长期规划、少量样本适应和故障恢复的同时，相较于现有的推理 VLA 方法，其推理延迟减少了高达 89.3%。


<details>
  <summary>Details</summary>
Motivation: 随着 VLA 任务变得越来越复杂，特别是在视觉场景中进行动态环境下的适应性操作，当前的方法尽管能够增强推理能力，但推理延迟较长，导致高推断延迟。因此，本文提出了 Fast-ThinkAct，旨在提高推理效率，同时保持强大的长期规划、少量样本适应和故障恢复能力。

Method: Fast-ThinkAct 通过从教师中提取可表达的隐式推理来学习高效推理，并由偏好导向的目标驱动，以实现操作轨迹的对齐。该框架在多次样本上训练，能够学习到既能沿用语言推理能力，又能利用视觉规划的模式。

Result: 在多种物质操作与推理基准上的广泛实验表明，与现有的推理 VLA 方法相比，Fast-ThinkAct 在推理延迟方面减少了高达 89.3%，同时保持了高效的长期规划、少量样本适应和故障恢复能力。

Conclusion: Fast-ThinkAct 通过实现高效且紧凑的推理来增强策略学习，成功地将紧凑推理与动作执行连接起来，并在多种物质操作与推理基准上展示了其强大的性能。

Abstract: Vision-Language-Action (VLA) tasks require reasoning over complex visual scenes and executing adaptive actions in dynamic environments. While recent studies on reasoning VLAs show that explicit chain-of-thought (CoT) can improve generalization, they suffer from high inference latency due to lengthy reasoning traces. We propose Fast-ThinkAct, an efficient reasoning framework that achieves compact yet performant planning through verbalizable latent reasoning. Fast-ThinkAct learns to reason efficiently with latent CoTs by distilling from a teacher, driven by a preference-guided objective to align manipulation trajectories that transfers both linguistic and visual planning capabilities for embodied control. This enables reasoning-enhanced policy learning that effectively connects compact reasoning to action execution. Extensive experiments across diverse embodied manipulation and reasoning benchmarks demonstrate that Fast-ThinkAct achieves strong performance with up to 89.3\% reduced inference latency over state-of-the-art reasoning VLAs, while maintaining effective long-horizon planning, few-shot adaptation, and failure recovery.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [70] [DeliberationBench: When Do More Voices Hurt? A Controlled Study of Multi-LLM Deliberation Protocols](https://arxiv.org/abs/2601.08835)
*Vaarunay Kaushal,Taranveer Singh*

Main category: cs.CL

TL;DR: 在270个问题上进行的实验表明，单一最佳响应的方法比多种对话协议的组合表现更优，尽管后者计算成本更高。


<details>
  <summary>Details</summary>
Motivation: 讨论多智能体系统中的大型语言模型（LLMs）如何达成共识的问题。研究者对比了单一最佳响应方法与多种对话协议的性能，试图明确复杂方法是否比简单方法更有效。

Method: 使用了一个名为DELIBERATIONBENCH的控制基准测试，包括三个对话协议和一个强基线（从模型输出池中选择最佳响应），在810次评估中进行了测试。

Result: 单一最佳响应的方法在82.5% (+- 3.3%) 的胜率下表现优于所有对话协议，表现显著优于最佳协议13.8% (+- 2.6%) 的胜率，并且这种6.0倍的性能差距在统计上是显著的（p < 0.01）。而在计算成本上，对话协议的计算成本是单一最佳响应方法的1.5到2.5倍。

Conclusion: 研究表明，对于多LLM系统而言，复杂性并不一定能提升质量。相反，简单的单一最佳响应方法可能更有效更经济。

Abstract: Multi-agent systems where Large Language Models (LLMs) deliberate to form consensus have gained significant attention, yet their practical value over simpler methods remains under-scrutinized. We introduce DELIBERATIONBENCH, a controlled benchmark evaluating three deliberation protocols against a strong baseline of selecting the best response from a pool of model outputs. Across 270 questions and three independent seeds (810 total evaluations), we find a striking negative result: the best-single baseline achieves an 82.5% +- 3.3% win rate, dramatically outperforming the best deliberation protocol(13.8% +- 2.6%). This 6.0x performance gap is statistically significant (p < 0.01) and comes at 1.5-2.5x higher computational cost. Our findings challenge assumptions that complexity enhances quality in multi-LLM systems.

</details>


### [71] [A Review: PTSD in Pre-Existing Medical Condition on Social Media](https://arxiv.org/abs/2601.08836)
*Zaber Al Hassan Ayon,Nur Hafieza Ismail,Nur Shazwani Kamarudin*

Main category: cs.CL

TL;DR: 该综述研究了2008年至2024年间PTSD与慢性病患者在社交媒体上的表现和管理，通过自然语言处理和机器学习技术，识别出潜在的PTSD病例，并强调了在线支持社区在应对策略和早期干预方面的角色。研究指出，应将预存的医疗条件纳入PTSD研究和治疗中，同时强调了社交媒体在监测和支持脆弱群体方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 文章旨在通过分析社交媒体数据，探讨PTSD患者和慢性病患者的共同挑战，以期发现新的治疗方法和策略。

Method: 文章通过系统分析2008年至2024年间的研究文献，利用自然语言处理和机器学习技术，从社交媒体平台如X和Facebook收集数据。

Result: 研究表明，自然语言处理和机器学习技术能够识别出潜在的PTSD病例，准确率在74%到90%之间，而且在线支持社区在帮助患者制定应对策略和促进早期干预方面发挥重要作用。

Conclusion: 该研究强调了在PTSD研究和治疗中应考虑预存的医疗条件，并认为社交媒体可以作为一种监测和支持工具。文章还提出了未来研究方向和临床应用建议。

Abstract: Post-Traumatic Stress Disorder (PTSD) is a multifaceted mental health condition, particularly challenging for individuals with pre-existing medical conditions. This review critically examines the intersection of PTSD and chronic illnesses as expressed on social media platforms. By systematically analyzing literature from 2008 to 2024, the study explores how PTSD manifests and is managed in individuals with chronic conditions such as cancer, heart disease, and autoimmune disorders, with a focus on online expressions on platforms like X (formally known as Twitter) and Facebook. Findings demonstrate that social media data offers valuable insights into the unique challenges faced by individuals with both PTSD and chronic illnesses. Specifically, natural language processing (NLP) and machine learning (ML) techniques can identify potential PTSD cases among these populations, achieving accuracy rates between 74% and 90%. Furthermore, the role of online support communities in shaping coping strategies and facilitating early interventions is highlighted. This review underscores the necessity of incorporating considerations of pre-existing medical conditions in PTSD research and treatment, emphasizing social media's potential as a monitoring and support tool for vulnerable groups. Future research directions and clinical implications are also discussed, with an emphasis on developing targeted interventions.

</details>


### [72] [From Adversarial Poetry to Adversarial Tales: An Interpretability Research Agenda](https://arxiv.org/abs/2601.08837)
*Piercosma Bisconti,Marcello Galisai,Matteo Prandi,Federico Pierucci,Olga Sorokoletova,Francesco Giarrusso,Vincenzo Suriani,Marcantonio Brancale,Daniele Nardi*

Main category: cs.CL

TL;DR: 该研究介绍了一种新的攻击技术Adversarial Tales，通过将有害内容嵌入赛博朋克叙事中，诱导语言模型进行功能分析。该技术在26个不同供应商的前沿模型中平均成功率为71.3%，表明结构性的监狱突破构成了一类广泛的漏洞，而不仅仅是孤立的技术。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于揭示语言模型的安全机制面临的新型攻击方式。现有安全措施可能无法有效应对通过文化编码结构重新框架的有害请求，因此需要探索新的防御策略。

Method: 该研究采用了一种新颖的攻击方法——Adversarial Tales，将有害内容隐含在赛博朋克叙事中，促使模型进行结构分解。研究比较了来自九家供应商的26个前沿模型的攻击成功率。

Result: 研究发现，在26个模型中，平均攻击成功率为71.3%，且无单一模型家族表现出可靠的抗攻击性能。这表明，结构性的监狱突破属于广泛的漏洞类别，而不仅仅是孤立的攻击技术。

Conclusion: 该研究表明，依赖于模式匹配的防御手段可能不足以应对文化编码框架下的有害意图。研究提出了机械可解释性研究议程，旨在探索叙述线索如何重塑模型表示以及模型是否可以学会独立于表现形式识别有害意图。

Abstract: Safety mechanisms in LLMs remain vulnerable to attacks that reframe harmful requests through culturally coded structures. We introduce Adversarial Tales, a jailbreak technique that embeds harmful content within cyberpunk narratives and prompts models to perform functional analysis inspired by Vladimir Propp's morphology of folktales. By casting the task as structural decomposition, the attack induces models to reconstruct harmful procedures as legitimate narrative interpretation. Across 26 frontier models from nine providers, we observe an average attack success rate of 71.3%, with no model family proving reliably robust. Together with our prior work on Adversarial Poetry, these findings suggest that structurally-grounded jailbreaks constitute a broad vulnerability class rather than isolated techniques. The space of culturally coded frames that can mediate harmful intent is vast, likely inexhaustible by pattern-matching defenses alone. Understanding why these attacks succeed is therefore essential: we outline a mechanistic interpretability research agenda to investigate how narrative cues reshape model representations and whether models can learn to recognize harmful intent independently of surface form.

</details>


### [73] [Companion Agents: A Table-Information Mining Paradigm for Text-to-SQL](https://arxiv.org/abs/2601.08838)
*Jiahui Chen,Lei Fu,Jian Cui,Yu Lei,Zhenning Dong*

Main category: cs.CL

TL;DR: 该研究提出了一种名为Companion Agents (CA) 的新框架，旨在通过在数据库侧预处理和构建证据，弥补查询时缺失的注解信息，从而提高大规模表之间多步推理的文本到SQL转换的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有文本到SQL系统在实际应用中效果受限，因为它们假设数据库注解完整且准确，但这一假设往往在工业环境中不成立。我们的研究旨在解决这一问题。

Method: 我们提出了一种新的基于数据库的方法，通过在数据库侧事先挖掘和整合关系表之间的隐含关系、值域分布、统计规律和潜在语义线索，从而在查询生成时自动构建证据。

Result: 实验结果表明，在BIRD基准数据集的完全缺失证据设置下，Companion Agents (CA) 框架在RSL-SQL / CHESS / DAIL-SQL数据集上分别恢复了 +4.49 / +4.37 / +14.13 的执行准确性，挑战子集上的改进更大，分别为 +9.65 / +7.58 / +16.71。

Conclusion: Companion Agents 框架通过在数据库侧进行自动挖掘和证据构建，从而提高文本到SQL的性能，减少了对外部人工注解的依赖，为工业级别的部署提供了实践途径。

Abstract: Large-scale Text-to-SQL benchmarks such as BIRD typically assume complete and accurate database annotations as well as readily available external knowledge, which fails to reflect common industrial settings where annotations are missing, incomplete, or erroneous. This mismatch substantially limits the real-world applicability of state-of-the-art (SOTA) Text-to-SQL systems. To bridge this gap, we explore a database-centric approach that leverages intrinsic, fine-grained information residing in relational databases to construct missing evidence and improve Text-to-SQL accuracy under annotation-scarce conditions. Our key hypothesis is that when a query requires multi-step reasoning over extensive table information, existing methods often struggle to reliably identify and utilize the truly relevant knowledge. We therefore propose to "cache" query-relevant knowledge on the database side in advance, so that it can be selectively activated at inference time. Based on this idea, we introduce Companion Agents (CA), a new Text-to-SQL paradigm that incorporates a group of agents accompanying database schemas to proactively mine and consolidate hidden inter-table relations, value-domain distributions, statistical regularities, and latent semantic cues before query generation. Experiments on BIRD under the fully missing evidence setting show that CA recovers +4.49 / +4.37 / +14.13 execution accuracy points on RSL-SQL / CHESS / DAIL-SQL, respectively, with larger gains on the Challenging subset +9.65 / +7.58 / +16.71. These improvements stem from CA's automatic database-side mining and evidence construction, suggesting a practical path toward industrial-grade Text-to-SQL deployment without reliance on human-curated evidence.

</details>


### [74] [Consistency-Aware Editing for Entity-level Unlearning in Language Models](https://arxiv.org/abs/2601.08840)
*Xiaoqi Han,Víctor Gutiérrez-Basulto,Ru Li,Xiaoli Li,Jiye Liang,Jeff Z. Pan*

Main category: cs.CL

TL;DR: 本研究提出了一个新的一致性感知编辑（CAE）框架，通过联合学习和一致性正则化来实现实体级别的知识删除，同时减少对无关知识的干扰。研究结果表明，与传统的知识删除和编辑方法相比，此方法能显著提高删除准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前在实现大型语言模型（LLMs）的实体级别知识删除时，大多数方法要么依赖全面重训练，要么依赖基于提示的干预，这两种方法在处理变形查询时都可能很昂贵且脆弱。为了解决这些问题，研究者提出了一种新的编辑技术适配框架，以更有效和高效的方式进行实体级别的知识删除。

Method: 该研究提出了一个一致性感知编辑（CAE）框架，该框架基于提示集的聚合和一致性正则化，通过低秩更新来引导编辑方向，从而实现全面而稳健的知识删除，同时最小化对无关知识的干扰。

Result: 研究在两个具有挑战性的基准测试RWKU和ToFU上评估了CAE方法，结果表明：（i）它为LLMs中的实体级别知识的内部表示和删除提供了有价值的见解，（ii）与传统的知识删除和编辑方法相比，它显著提高了删除准确性和鲁棒性，（iii）它仅使用少量精心选择的提示即可实现可扩展的实体删除。

Conclusion: 通过证明CAE方法的有效性，研究建议该框架可以作为一种更有效和高效的方法，用于大型语言模型的实体级别知识删除。这为未来的编辑技术的发展铺平了道路。

Abstract: Large language models (LLMs) risk retaining sensitive, copyrighted, or harmful information from their training data. Entity-level unlearning addresses this issue by removing all knowledge of a specific entity while preserving the model's overall capabilities. Existing approaches typically rely on full-model fine-tuning or prompt-based interventions, which can be computationally expensive or brittle when handling paraphrased queries. Recently, model editing has emerged as an efficient alternative for updating knowledge in LLMs, offering a promising direction for unlearning. However, existing editing techniques are typically designed for instance-level updates, modifying responses to specific attributes of an entity rather than eliminating all knowledge associated with the entity. In this paper, we investigate how editing techniques can be adapted for effective and efficient entity-level unlearning. To this end, we introduce a novel consistency-aware editing (CAE) framework. CAE aggregates a diverse set of prompts related to a target entity, including its attributes, relations, and adversarial paraphrases. It then jointly learns a low-rank update guided by a consistency regularizer that aligns the editing directions across prompts. This promotes robust and comprehensive forgetting while minimizing interference with unrelated knowledge. We further examine where different entities are stored within the model and how many diverse prompts are needed for successful unlearning. We evaluate CAE on two challenging benchmarks, RWKU and ToFU, and demonstrate that it (i) provides insights into how entity-level knowledge is internally represented and deleted in LLMs, (ii) significantly improves forgetting accuracy and robustness over traditional unlearning and editing baselines, and (iii) enables scalable entity removal using only tens of carefully selected prompts.

</details>


### [75] [Triples and Knowledge-Infused Embeddings for Clustering and Classification of Scientific Documents](https://arxiv.org/abs/2601.08841)
*Mihael Arcan*

Main category: cs.CL

TL;DR: 本研究表明，结合无结构文本和结构化知识（如主谓宾三元组）可以更有效地组织和分类科研论文。使用过滤后的arXiv语料库，研究通过多种文档表示形式（原始摘要、提取的三元组及两者结合的混合表示）进行实验，发现全摘要文本提供了最一致的聚类，但包含三元组的混合表示形式在分类上的性能持续提升，达到92.6%的准确率和0.925的宏F1值。


<details>
  <summary>Details</summary>
Motivation: 面对海量的科学文献，研究人员需要有效的方法来管理和理解这些资料。本研究旨在探索如何利用结构化知识，如主谓宾三元组，以便更好地聚类和分类科研论文，从而提高科研资料的理解和参考效率。

Method: 研究采用了模块化的流水线方法，结合了无监督聚类和监督分类，使用了四种最新的变换器模型（MiniLM、MPNet、SciBERT和SPECTER），分别对摘要文本、提取的三元组和混合表示格式进行嵌入，然后使用KMeans、GMM和HDBSCAN进行无监督聚类评估，对arXiv主题进行细调分类。

Result: 实验结果表明，全文摘要文本可以提供最一致的聚类结果，但混合表示（结合三元组）可以显著提高分类性能，达到92.6%的准确率和0.925的宏F1值。此外，轻量级句子编码器（MiniLM，MPNet）在聚类中表现出色，而SciBERT在结构化输入分类中表现出色。

Conclusion: 研究表明，结合无结构文本与结构化知识可为科学文档的语义组织提供互补的优势，发现结构化的输入表示形式对于提高分类准确性和模型性能具有重要意义。

Abstract: The increasing volume and complexity of scientific literature demand robust methods for organizing and understanding research documents. In this study, we explore how structured knowledge, specifically, subject-predicate-object triples, can enhance the clustering and classification of scientific papers. We propose a modular pipeline that combines unsupervised clustering and supervised classification over multiple document representations: raw abstracts, extracted triples, and hybrid formats that integrate both. Using a filtered arXiv corpus, we extract relational triples from abstracts and construct four text representations, which we embed using four state-of-the-art transformer models: MiniLM, MPNet, SciBERT, and SPECTER. We evaluate the resulting embeddings with KMeans, GMM, and HDBSCAN for unsupervised clustering, and fine-tune classification models for arXiv subject prediction. Our results show that full abstract text yields the most coherent clusters, but that hybrid representations incorporating triples consistently improve classification performance, reaching up to 92.6% accuracy and 0.925 macro-F1. We also find that lightweight sentence encoders (MiniLM, MPNet) outperform domain-specific models (SciBERT, SPECTER) in clustering, while SciBERT excels in structured-input classification. These findings highlight the complementary benefits of combining unstructured text with structured knowledge, offering new insights into knowledge-infused representations for semantic organization of scientific documents.

</details>


### [76] [Rubric-Conditioned LLM Grading: Alignment, Uncertainty, and Robustness](https://arxiv.org/abs/2601.08843)
*Haotian Deng,Chris Farber,Jiyoon Lee,David Tang*

Main category: cs.CL

TL;DR: 本研究系统性地评估了大语言模型在基于评分标准的自动简答题评分中的表现，特别是在评分标准复杂性、不确定性与准确性的权衡以及模型的鲁棒性方面。


<details>
  <summary>Details</summary>
Motivation: 自动简答题评分（ASAG）是一个具有挑战性的问题，尤其是对于大规模语言模型（LLMs）而言，其在评分标准指导下的自动评分可靠性需要严格的评估。

Method: 通过SciEntsBank基准数据集和Qwen 2.5-72B模型，研究从评分标准复杂性、不确定性与准确性的权衡以及模型的鲁棒性三个角度进行了系统性评估。

Result: 研究发现，大语言模型在二元任务上的对齐较强，但随着评分标准复杂性的增加而下降。通过“信任曲线”分析，低置信度预测去除后，整体准确性提升。模型对提示注入具有鲁棒性，但对同义词替换敏感。

Conclusion: 研究表明，基于评分标准的大语言模型在这种条件下的能力与限制，强调了不确定性估计和鲁棒性测试对于可靠部署的重要性。

Abstract: Automated short-answer grading (ASAG) remains a challenging task due to the linguistic variability of student responses and the need for nuanced, rubric-aligned partial credit. While Large Language Models (LLMs) offer a promising solution, their reliability as automated judges in rubric-based settings requires rigorous assessment. In this paper, we systematically evaluate the performance of LLM-judges for rubric-based short-answer grading. We investigate three key aspects: the alignment of LLM grading with expert judgment across varying rubric complexities, the trade-off between uncertainty and accuracy facilitated by a consensus-based deferral mechanism, and the model's robustness under random input perturbations and adversarial attacks. Using the SciEntsBank benchmark and Qwen 2.5-72B, we find that alignment is strong for binary tasks but degrades with increased rubric granularity. Our "Trust Curve" analysis demonstrates a clear trade-off where filtering low-confidence predictions improves accuracy on the remaining subset. Additionally, robustness experiments reveal that while the model is resilient to prompt injection, it is sensitive to synonym substitutions. Our work provides critical insights into the capabilities and limitations of rubric-conditioned LLM judges, highlighting the importance of uncertainty estimation and robustness testing for reliable deployment.

</details>


### [77] [Emissions and Performance Trade-off Between Small and Large Language Models](https://arxiv.org/abs/2601.08844)
*Anandita Garg,Uma Gaba,Deepan Muthirayan,Anish Roy Chowdhury*

Main category: cs.CL

TL;DR: 本研究通过对比大规模语言模型（LLMs）和细调的小规模语言模型（SLMs）在几个自然语言处理、推理和编程任务上的性能与碳排放，发现SLMs可以在显著降低碳排放的同时保持相当的性能。


<details>
  <summary>Details</summary>
Motivation: 由于大规模语言模型（LLMs）碳足迹庞大，从训练到推理都消耗大量能源，本文旨在探索细调的小规模语言模型（SLMs）作为LLMs的一种可持续替代方案的可能性。

Method: 本文选择了六个特定的NLP、推理和编程任务，对比LLMs和细调后的SLMs在任务性能和碳排放上的表现。

Result: 本文结果显示，在四个选定的任务中，SLMs在保证相当性能的同时显著减少了推理过程中的碳排放。

Conclusion: 研究证明，更小的模型可以作为减轻大型数据密集型LLMs环境影响的一种可行手段，从而推动了可持续的绿色人工智能的发展。

Abstract: The advent of Large Language Models (LLMs) has raised concerns about their enormous carbon footprint, starting with energy-intensive training and continuing through repeated inference. This study investigates the potential of using fine-tuned Small Language Models (SLMs) as a sustainable alternative for predefined tasks. Here, we present a comparative analysis of the performance-emissions trade-off between LLMs and fine-tuned SLMs across selected tasks under Natural Language Processing, Reasoning and Programming. Our results show that in four out of the six selected tasks, SLMs maintained comparable performances for a significant reduction in carbon emissions during inference. Our findings demonstrate the viability of smaller models in mitigating the environmental impact of resource-heavy LLMs, thus advancing towards sustainable, green AI.

</details>


### [78] [Directional Attractors in LLM Reasoning: How Similarity Retrieval Steers Iterative Summarization Based Reasoning](https://arxiv.org/abs/2601.08846)
*Cagatay Tekin,Charbel Barakat,Luis Joseph Luna Limgenco*

Main category: cs.CL

TL;DR: 该研究提出了一种增强迭代推理的新框架InftyThink with Cross-Chain Memory，该框架利用基于嵌入的语义缓存来存储以前成功的推理模式，以改善大型语言模型在结构化领域的推理准确性。


<details>
  <summary>Details</summary>
Motivation: 现有框架如InftyThink通过控制上下文增长来实现长期推理，但会重复生成相似的推理策略。本文研究旨在提出一种方法，允许模型在每一步推理中检索并条件化以前成功使用的语义相似的推理模式，而不盲目扩展上下文窗口。

Method: 该方法通过在InftyThink基础上添加一个基于嵌入的语义缓存，存储和检索以前成功的推理模式。该缓存在每次推理步骤中检索最大程度上与当前情境语义相似的已存储语句，以指导推理。

Result: 实验表明该方法在MATH500、AIME2024和GPQA-Diamond等数据集上的推理准确性得到了改善，但也发现了跨领域推理中的失败模式。几何分析表明缓存检索导致嵌入空间中呈现方向性偏差。

Conclusion: 该研究强调了基于相似性的记忆在自改进LSTM推理中的优势和局限性。

Abstract: Iterative summarization based reasoning frameworks such as InftyThink enable long-horizon reasoning in large language models (LLMs) by controlling context growth, but they repeatedly regenerate similar reasoning strategies across tasks. We introduce InftyThink with Cross-Chain Memory, an extension that augments iterative reasoning with an embedding-based semantic cache of previously successful reasoning patterns. At each reasoning step, the model retrieves and conditions on the most semantically similar stored lemmas, guiding inference without expanding the context window indiscriminately. Experiments on MATH500, AIME2024, and GPQA-Diamond demonstrate that semantic lemma retrieval improves accuracy in structured domains while exposing failure modes in tests that include heterogeneous domains. Geometric analyses of reasoning trajectories reveal that cache retrieval induces directional biases in embedding space, leading to consistent fix (improve baseline accuracy) and break (degradation in baseline accuracy) attractors. Our results highlight both the benefits and limits of similarity-based memory for self-improving LLM reasoning.

</details>


### [79] [Scalable and Reliable Evaluation of AI Knowledge Retrieval Systems: RIKER and the Coherent Simulated Universe](https://arxiv.org/abs/2601.08847)
*JV Roig*

Main category: cs.CL

TL;DR: RIKER是一种避免基准数据集污染的新基准和可复制方法，通过生成文档来自知真相而不是从文档中提取真相。这种方法可以在无需人工注释或参考模型的情况下实现确定性评分和可扩展评估，并通过再生性语料库实现抗污染。


<details>
  <summary>Details</summary>
Motivation: 传统的评鉴方法存在基准数据集易受污染、LLM评判具有系统偏差、以及人工构建真实性的高昂成本等问题，RIKER旨在解决这些问题。

Method: RIKER基于范式反转的策略，从已知真实信息生成文档，而非从文档中提取真实信息，以此实现无需人工注释或参考模型的确定性评分和可扩展评估。这种方法利用再生性语料库来提高抗污染能力。

Result: RIKER在210亿多的标记中对33个模型进行了评估，发现上下文长度的声明经常超出了可用容量，超过32K令牌后性能显著下降；跨文档聚合比单一文档提取困难得多；以及可靠的实地能力与幻觉抵抗能力是不同的能力。

Conclusion: RIKER提出了一种广泛适用的抗污染评估方法，用于生成模型评估中的合成文档，从而实现大规模、无需人工标注和抗污染的评估。这一研究成果有助于提高模型评估的准确性和可靠性。

Abstract: Evaluating knowledge systems (LLMs, RAG, knowledge graphs, etc) faces fundamental challenges: static benchmarks are vulnerable to contamination, LLM-based judges exhibit systematic biases, and ground truth extraction requires expensive human annotation. We present RIKER (Retrieval Intelligence and Knowledge Extraction Rating), both a benchmark and a replicable methodology based on paradigm inversion - generating documents from known ground truth rather than extracting ground truth from documents. This approach enables deterministic scoring and scalable evaluation without human annotation or reference models, and contamination resistance through regenerable corpora. Our evaluation of 33 models using over 21 billion tokens reveals that context length claims frequently exceed usable capacity, with significant degradation beyond 32K tokens; cross-document aggregation proves substantially harder than single-document extraction; and grounding ability and hallucination resistance are distinct capabilities - models excelling at finding facts that exist may still fabricate facts that do not. Beyond the specific benchmark, we contribute a domain-agnostic methodology for constructing scalable and contamination-resistant evaluations wherever synthetic documents can be generated from structured ground truth.

</details>


### [80] [PediaMind-R1: A Temperament-Aware Language Model for Personalized Early Childhood Care Reasoning via Cognitive Modeling and Preference Alignment](https://arxiv.org/abs/2601.08848)
*Zihe Zhang,Can Zhang,Yanheng Xu,Xin Hu,Jichao Leng*

Main category: cs.CL

TL;DR: PediaMind-R1 是一种专为智能育儿场景设计的领域特定大语言模型，通过利用发展心理学理论，尤其是气质理论，实现个性化建议。


<details>
  <summary>Details</summary>
Motivation: 当前的系统提供的是通用的建议，但PediaMind-R1旨在通过整合发展心理学理论，专注于天真和幼儿的情绪和行为模式，提供更个性化的建议。

Method: PediaMind-R1 采用了两种阶段的训练管道：首先是监督微调以教授结构化的链式思维推理，然后是基于 GRPO 的对齐阶段，以增强逻辑一致性、领域专业知识和同理心的照顾策略。

Result: 通过实施一个评估框架，包括情感敏感的多项选择测验和人类评估，PediaMind-R1 显示出能够准确解读婴幼儿气质特征并积极参与个性化推理。

Conclusion: 这项工作强调了将垂直领域建模与心理理论结合的重要性，它提供了一种新的方法来开发以用户为中心的大型语言模型，并推进了在敏感养育场景中实施个性化的方法。

Abstract: This paper presents PediaMind-R1, a domain-specialized large language model designed to achieve active personalization in intelligent parenting scenarios. Unlike conventional systems that provide generic suggestions, PediaMind-R1 draws on insights from developmental psychology. It introduces temperament theory from the Thomas-Chess framework and builds a temperament knowledge graph for infants and toddlers (0-3 years). Our two-stage training pipeline first uses supervised fine-tuning to teach structured chain-of-thought reasoning, and then applies a GRPO-based alignment stage to reinforce logical consistency, domain expertise, and empathetic caregiving strategies. We further design an evaluation framework comprising temperament-sensitive multiple-choice tests and human assessments. The results demonstrate that PediaMind-R1 can accurately interpret early childhood temperament profiles and proactively engage in individualized reasoning. This work highlights the value of integrating vertical-domain modeling with psychological theory. It offers a novel approach to developing user-centered LLMs that advance the practice of active personalization in sensitive caregiving contexts.

</details>


### [81] [Gaming the Answer Matcher: Examining the Impact of Text Manipulation on Automated Judgment](https://arxiv.org/abs/2601.08849)
*Manas Khatore,Sumana Sridharan,Kevork Sulahian,Benjamin J. Smith,Shi Feng*

Main category: cs.CL

TL;DR: 该研究分析了通过策略操纵文本以欺骗答案匹配模型的可能性，并验证了绝对评分比连续评分更不易受到攻击，表明答案匹配具有抵御成本低廉的文本操纵的能力。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs（大规模语言模型）的广泛应用，人们开始寻求自动化的评价方法作为人类评价的可行替代方案。然而，答案匹配模型的可靠性仍需解决其在面对故意误导或冗余回答时的有效性问题。

Method: 研究者设计了三种策略来测试答案匹配模型：生成冗长的回答、提供多个答案以及在开始的句子中包含与正确答案冲突的信息。通过自动答案匹配模型进行系统测试，评估这些策略对模型得分的影响。

Result: 研究发现，上述策略并未提高答案匹配模型的得分，反而经常导致得分下降。同时，采用绝对评分（仅需判断为“正确”或“错误”）比连续评分（要求判断部分正确性）更能抵御攻击。

Conclusion: 这些发现表明，答案匹配模型对于低成本的文本操纵较为稳健，并且在参考答案可用的情况下，答案匹配可以作为一种可靠的自动评价方式，替代传统的LLM担任裁判或人类评价。

Abstract: Automated answer matching, which leverages LLMs to evaluate free-text responses by comparing them to a reference answer, shows substantial promise as a scalable and aligned alternative to human evaluation. However, its reliability requires robustness against strategic attacks such as guesswork or verbosity that may artificially inflate scores without improving actual correctness. In this work, we systematically investigate whether such tactics deceive answer matching models by prompting examinee models to: (1) generate verbose responses, (2) provide multiple answers when unconfident, and (3) embed conflicting answers with the correct answer near the start of their response. Our results show that these manipulations do not increase scores and often reduce them. Additionally, binary scoring (which requires a matcher to answer with a definitive "correct" or "incorrect") is more robust to attacks than continuous scoring (which requires a matcher to determine partial correctness). These findings show that answer matching is generally robust to inexpensive text manipulation and is a viable alternative to traditional LLM-as-a-judge or human evaluation when reference answers are available.

</details>


### [82] [Más contexto no es mejor. Paradoja de la dilución vectorial en RAG corporativos](https://arxiv.org/abs/2601.08851)
*Alex Dantart*

Main category: cs.CL

TL;DR: 该研究探讨了通过注入摘要来改进RAG背景的“上下文化片段化”技术带来的优势和负面影响，展示了适度注入可提升召回率，但超过一定临界值会降低精确度。


<details>
  <summary>Details</summary>
Motivation: 为了解决RAG背景信息不足的问题，研究引入了上下文化片段化技术，利用摘要来补充增强背景信息。然而，这种技术也可能引入矢量稀释，影响局部内容的效果。

Method: 研究通过评估不同注入比例的效果，发现了一个倒U型曲线，并提出理论框架来计算最佳注入比例。

Result: 研究结果表明，适度的注入可以提升召回率18%，但如果超过临界值40%，则精准度会下降22%。针对特定查询，临界注入比大于0.4时，精确度会减少。

Conclusion: 研究提出了一种理论框架，用于计算上下文化片段化技术的最佳注入比例，以优化RAG系统的整体性能。

Abstract: Técnicas recientes de "Contextualized Chunking" inyectan resúmenes para mejorar el contexto en RAG, pero introducen una "dilución vectorial" que opaca el contenido local. Evaluando distintos ratios de inyección, demostramos una curva en "U invertida": una inyección moderada mejora el "Recall" (+18%), pero superar un umbral crítico (CIR > 0.4) reduce la precisión en un 22% para consultas específicas. Proponemos un marco teórico para calcular el ratio óptimo de inyección. --
  Recent "Contextualized Chunking" techniques inject summaries to improve RAG context but introduce "vector dilution" drowning out local content. Evaluating various injection ratios, we demonstrate an "inverted U" curve: moderate injection boosts Recall (+18%), but exceeding a critical threshold (CIR > 0.4) drops precision by 22% for specific queries. We propose a theoretical framework to calculate the optimal injection ratio.

</details>


### [83] [NewsScope: Schema-Grounded Cross-Domain News Claim Extraction with Open Models](https://arxiv.org/abs/2601.08852)
*Nidhi Pandya*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Automated news verification requires structured claim extraction, but existing approaches either lack schema compliance or generalize poorly across domains. This paper presents NewsScope, a cross-domain dataset, benchmark, and fine-tuned model for schema-grounded news claim extraction. The dataset contains 455 articles across politics, health, science/environment, and business, consisting of 395 in-domain articles and 60 out-of-source articles for generalization testing. LLaMA 3.1 8B was fine-tuned using LoRA on 315 training examples and evaluated on held-out in-domain (80 articles) and out-of-source (60 articles) test sets. Human evaluation on 400 claims shows NewsScope achieves 89.4% human-evaluated accuracy compared to GPT-4o-mini's 93.7% (p=0.07). NewsScope outperforms GPT-4o-mini on political claims (94.3% vs. 87.8%). A numeric grounding filter further improves accuracy to 91.6%, narrowing the gap to 2.1 percentage points. Inter-annotator agreement studies (160 claims) confirm labeling reliability (94.6% positive agreement on SUPPORTED judgments). The open-weight model enables offline deployment at approximately $15 on-demand compute (or $0 on free tiers). Code and benchmark are publicly released.

</details>


### [84] [Evaluating Role-Consistency in LLMs for Counselor Training](https://arxiv.org/abs/2601.08892)
*Eric Rudolph,Natalie Engert,Jens Albrecht*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The rise of online counseling services has highlighted the need for effective training methods for future counselors. This paper extends research on VirCo, a Virtual Client for Online Counseling, designed to complement traditional role-playing methods in academic training by simulating realistic client interactions. Building on previous work, we introduce a new dataset incorporating adversarial attacks to test the ability of large language models (LLMs) to maintain their assigned roles (role-consistency). The study focuses on evaluating the role consistency and coherence of the Vicuna model's responses, comparing these findings with earlier research. Additionally, we assess and compare various open-source LLMs for their performance in sustaining role consistency during virtual client interactions. Our contributions include creating an adversarial dataset, evaluating conversation coherence and persona consistency, and providing a comparative analysis of different LLMs.

</details>


### [85] [Imagine-then-Plan: Agent Learning from Adaptive Lookahead with World Models](https://arxiv.org/abs/2601.08955)
*Youwei Liu,Jian Wang,Hanlin Wang,Beichen Guo,Wenjie Li*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Recent advances in world models have shown promise for modeling future dynamics of environmental states, enabling agents to reason and act without accessing real environments. Current methods mainly perform single-step or fixed-horizon rollouts, leaving their potential for complex task planning under-exploited. We propose Imagine-then-Plan (\texttt{ITP}), a unified framework for agent learning via lookahead imagination, where an agent's policy model interacts with the learned world model, yielding multi-step ``imagined'' trajectories. Since the imagination horizon may vary by tasks and stages, we introduce a novel adaptive lookahead mechanism by trading off the ultimate goal and task progress. The resulting imagined trajectories provide rich signals about future consequences, such as achieved progress and potential conflicts, which are fused with current observations, formulating a partially \textit{observable} and \textit{imaginable} Markov decision process to guide policy learning. We instantiate \texttt{ITP} with both training-free and reinforcement-trained variants. Extensive experiments across representative agent benchmarks demonstrate that \texttt{ITP} significantly outperforms competitive baselines. Further analyses validate that our adaptive lookahead largely enhances agents' reasoning capability, providing valuable insights into addressing broader, complex tasks.

</details>


### [86] [Entropy Sentinel: Continuous LLM Accuracy Monitoring from Decoding Entropy Traces in STEM](https://arxiv.org/abs/2601.09001)
*Pedro Memoli Buffa,Luciano Del Corro*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Deploying LLMs raises two coupled challenges: (1) monitoring - estimating where a model underperforms as traffic and domains drift - and (2) improvement - prioritizing data acquisition to close the largest performance gaps. We test whether an inference-time signal can estimate slice-level accuracy under domain shift. For each response, we compute an output-entropy profile from final-layer next-token probabilities (from top-k logprobs) and summarize it with eleven statistics. A lightweight classifier predicts instance correctness, and averaging predicted probabilities yields a domain-level accuracy estimate. We evaluate on ten STEM reasoning benchmarks with exhaustive train/test compositions (k in {1,2,3,4}; all "10 choose k" combinations), across nine LLMs from six families (3B-20B). Estimates often track held-out benchmark accuracy, and several models show near-monotonic ordering of domains. Output-entropy profiles are thus an accessible signal for scalable monitoring and for targeting data acquisition.

</details>


### [87] [TranslateGemma Technical Report](https://arxiv.org/abs/2601.09012)
*Mara Finkelstein,Isaac Caswell,Tobias Domhan,Jan-Thorsten Peter,Juraj Juraska,Parker Riley,Daniel Deutsch,Cole Dilanni,Colin Cherry,Eleftheria Briakou,Elizabeth Nielsen,Jiaming Luo,Kat Black,Ryan Mullins,Sweta Agrawal,Wenda Xu,Erin Kats,Stephane Jaskiewicz,Markus Freitag,David Vilar*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We present TranslateGemma, a suite of open machine translation models based on the Gemma 3 foundation models. To enhance the inherent multilingual capabilities of Gemma 3 for the translation task, we employ a two-stage fine-tuning process. First, supervised fine-tuning is performed using a rich mixture of high-quality large-scale synthetic parallel data generated via state-of-the-art models and human-translated parallel data. This is followed by a reinforcement learning phase, where we optimize translation quality using an ensemble of reward models, including MetricX-QE and AutoMQM, targeting translation quality. We demonstrate the effectiveness of TranslateGemma with human evaluation on the WMT25 test set across 10 language pairs and with automatic evaluation on the WMT24++ benchmark across 55 language pairs. Automatic metrics show consistent and substantial gains over the baseline Gemma 3 models across all sizes. Notably, smaller TranslateGemma models often achieve performance comparable to larger baseline models, offering improved efficiency. We also show that TranslateGemma models retain strong multimodal capabilities, with enhanced performance on the Vistra image translation benchmark. The release of the open TranslateGemma models aims to provide the research community with powerful and adaptable tools for machine translation.

</details>


### [88] [Multicultural Spyfall: Assessing LLMs through Dynamic Multilingual Social Deduction Game](https://arxiv.org/abs/2601.09017)
*Haryo Akbarianto Wibowo,Alaa Elsetohy,Qinrong Cui,Alham Fikri Aji*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The rapid advancement of Large Language Models (LLMs) has necessitated more robust evaluation methods that go beyond static benchmarks, which are increasingly prone to data saturation and leakage. In this paper, we propose a dynamic benchmarking framework for evaluating multilingual and multicultural capabilities through the social deduction game Spyfall. In our setup, models must engage in strategic dialogue to either identify a secret agent or avoid detection, utilizing culturally relevant locations or local foods. Our results show that our game-based rankings align closely with the Chatbot Arena. However, we find a significant performance gap in non-English contexts: models are generally less proficient when handling locally specific entities and often struggle with rule-following or strategic integrity in non-English languages. We demonstrate that this game-based approach provides a scalable, leakage-resistant, and culturally nuanced alternative to traditional NLP benchmarks. The game history can be accessed here https://huggingface.co/datasets/haryoaw/cultural-spyfall.

</details>


### [89] [OpenDecoder: Open Large Language Model Decoding to Incorporate Document Quality in RAG](https://arxiv.org/abs/2601.09028)
*Fengran Mo,Zhan Su,Yuchen Hui,Jinghan Zhang,Jia Ao Sun,Zheyuan Liu,Chao Zhang,Tetsuya Sakai,Jian-Yun Nie*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The development of large language models (LLMs) has achieved superior performance in a range of downstream tasks, including LLM-based retrieval-augmented generation (RAG). The quality of generated content heavily relies on the usefulness of the retrieved information and the capacity of LLMs' internal information processing mechanism to incorporate it in answer generation. It is generally assumed that the retrieved information is relevant to the question. However, the retrieved information may have a variable degree of relevance and usefulness, depending on the question and the document collection. It is important to take into account the relevance of the retrieved information in answer generation. In this paper, we propose OpenDecoder, a new approach that leverages explicit evaluation of the retrieved information as quality indicator features for generation. We aim to build a RAG model that is more robust to varying levels of noisy context. Three types of explicit evaluation information are considered: relevance score, ranking score, and QPP (query performance prediction) score. The experimental results on five benchmark datasets demonstrate the effectiveness and better robustness of OpenDecoder by outperforming various baseline methods. Importantly, this paradigm is flexible to be integrated with the post-training of LLMs for any purposes and incorporated with any type of external indicators.

</details>


### [90] [SpectraQuery: A Hybrid Retrieval-Augmented Conversational Assistant for Battery Science](https://arxiv.org/abs/2601.09036)
*Sreya Vangara,Jagjit Nanda,Yan-Kai Tzeng,Eric Darve*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Scientific reasoning increasingly requires linking structured experimental data with the unstructured literature that explains it, yet most large language model (LLM) assistants cannot reason jointly across these modalities. We introduce SpectraQuery, a hybrid natural-language query framework that integrates a relational Raman spectroscopy database with a vector-indexed scientific literature corpus using a Structured and Unstructured Query Language (SUQL)-inspired design. By combining semantic parsing with retrieval-augmented generation, SpectraQuery translates open-ended questions into coordinated SQL and literature retrieval operations, producing cited answers that unify numerical evidence with mechanistic explanation. Across SQL correctness, answer groundedness, retrieval effectiveness, and expert evaluation, SpectraQuery demonstrates strong performance: approximately 80 percent of generated SQL queries are fully correct, synthesized answers reach 93-97 percent groundedness with 10-15 retrieved passages, and battery scientists rate responses highly across accuracy, relevance, grounding, and clarity (4.1-4.6/5). These results show that hybrid retrieval architectures can meaningfully support scientific workflows by bridging data and discourse for high-volume experimental datasets.

</details>


### [91] [Can LLMs interpret figurative language as humans do?: surface-level vs representational similarity](https://arxiv.org/abs/2601.09041)
*Samhita Bollepally,Aurora Sloman-Moll,Takashi Yamauchi*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large language models generate judgments that resemble those of humans. Yet the extent to which these models align with human judgments in interpreting figurative and socially grounded language remains uncertain. To investigate this, human participants and four instruction-tuned LLMs of different sizes (GPT-4, Gemma-2-9B, Llama-3.2, and Mistral-7B) rated 240 dialogue-based sentences representing six linguistic traits: conventionality, sarcasm, funny, emotional, idiomacy, and slang. Each of the 240 sentences was paired with 40 interpretive questions, and both humans and LLMs rated these sentences on a 10-point Likert scale. Results indicated that humans and LLMs aligned at the surface level with humans, but diverged significantly at the representational level, especially in interpreting figurative sentences involving idioms and Gen Z slang. GPT-4 most closely approximates human representational patterns, while all models struggle with context-dependent and socio-pragmatic expressions like sarcasm, slang, and idiomacy.

</details>


### [92] [Is Grokking Worthwhile? Functional Analysis and Transferability of Generalization Circuits in Transformers](https://arxiv.org/abs/2601.09049)
*Kaiyu He,Zhang Mian,Peilin Wu,Xinya Du,Zhiyu Chen*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: While Large Language Models (LLMs) excel at factual retrieval, they often struggle with the "curse of two-hop reasoning" in compositional tasks. Recent research suggests that parameter-sharing transformers can bridge this gap by forming a "Generalization Circuit" during a prolonged "grokking" phase. A fundamental question arises: Is a grokked model superior to its non-grokked counterparts on downstream tasks? Furthermore, is the extensive computational cost of waiting for the grokking phase worthwhile? In this work, we conduct a mechanistic study to evaluate the Generalization Circuit's role in knowledge assimilation and transfer. We demonstrate that: (i) The inference paths established by non-grokked and grokked models for in-distribution compositional queries are identical. This suggests that the "Generalization Circuit" does not represent the sudden acquisition of a new reasoning paradigm. Instead, we argue that grokking is the process of integrating memorized atomic facts into an naturally established reasoning path. (ii) Achieving high accuracy on unseen cases after prolonged training and the formation of a certain reasoning path are not bound; they can occur independently under specific data regimes. (iii) Even a mature circuit exhibits limited transferability when integrating new knowledge, suggesting that "grokked" Transformers do not achieve a full mastery of compositional logic.

</details>


### [93] [SITA: Learning Speaker-Invariant and Tone-Aware Speech Representations for Low-Resource Tonal Languages](https://arxiv.org/abs/2601.09050)
*Tianyi Xu,Xuan Ouyang,Binwei Yao,Shoua Xiong,Sara Misurelli,Maichou Lor,Junjie Hu*

Main category: cs.CL

TL;DR: 本文提出了一种名为SITA的轻量级调整配方，用于为低资源音调低语种开发鲁棒的声学表示，从而提高声学表示的音系一致性，并在特定数据集上取得了改进。


<details>
  <summary>Details</summary>
Motivation: 低资源音调语言在现代语音技术中受到忽视，为了克服性别噪音变量和保持音调敏感性，研究提出了SITA方法。

Method: SITA通过分阶段的多目标训练进行调整：（i）跨性别对比目标促进演讲者之间的词汇一致性，同时使用音调排斥损失防止音调坍塌；（ii）辅助的基于CTC的ASR目标可用于结构稳定。

Result: 在特定的掸族词汇语料库上，SITA在跨性别词汇检索准确率上有所改进，同时在ASR准确率上保持可接受水平，该方法还能适用于类似的语言，如汉语。

Conclusion: 研究表明，SITA是一种通用、即插即用的方法，可以适应多语言语音编码器以应对音调语言的问题。

Abstract: Tonal low-resource languages are widely spoken yet remain underserved by modern speech technology. A key challenge is learning representations that are robust to nuisance variation such as gender while remaining tone-aware for different lexical meanings. To address this, we propose SITA, a lightweight adaptation recipe that enforces Speaker-Invariance and Tone-Awareness for pretrained wav2vec-style encoders. SITA uses staged multi-objective training: (i) a cross-gender contrastive objective encourages lexical consistency across speakers, while a tone-repulsive loss prevents tone collapse by explicitly separating same-word different-tone realizations; and (ii) an auxiliary Connectionist Temporal Classification (CTC)-based ASR objective with distillation stabilizes recognition-relevant structure. We evaluate primarily on Hmong, a highly tonal and severely under-resourced language where off-the-shelf multilingual encoders fail to represent tone effectively. On a curated Hmong word corpus, SITA improves cross-gender lexical retrieval accuracy, while maintaining usable ASR accuracy relative to an ASR-adapted XLS-R teacher. We further observe similar gains when transferring the same recipe to Mandarin, suggesting SITA is a general, plug-in approach for adapting multilingual speech encoders to tonal languages.

</details>


### [94] [Efficient Multilingual Dialogue Processing via Translation Pipelines and Distilled Language Models](https://arxiv.org/abs/2601.09059)
*Santiago Martínez Novoa,Nicolás Rozo Fajardo,Diego Alejandro González Vargas,Nicolás Bedoya Figueroa*

Main category: cs.CL

TL;DR: 团队Kl33n3x为NLPAI4Health 2025共享任务开发了一个多语言对话摘要和问答系统，采用了翻译和多任务文本生成的方法，实现了在多个低资源语言上的良好表现。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在解决低资源语言在对话摘要和问答任务中的处理问题，通过利用知识蒸馏技术，证明了紧凑模型可以实现高性能，特别是在马拉地语、泰米尔语和印地语等语言上的表现尤为突出。

Method: 该方法包括三个阶段：从印地语到英语的正向翻译、使用2.55B参数的蒸馏语言模型的多任务文本生成，以及反向翻译回源语言。

Result: 系统在竞争任务中表现出色，特别是在马拉地语（86.7% 问答），泰米尔语（86.7% 问答）和印地语（80.0% 问答）上的表现尤为突出，证明了基于翻译的方法在低资源语言处理中的有效性，无需特定任务的微调。

Conclusion: 通过知识蒸馏，研究展示了紧凑模型即使在低资源语言环境下也能取得出色的性能，为未来多语言处理提供了新的思路。

Abstract: This paper presents team Kl33n3x's multilingual dialogue summarization and question answering system developed for the NLPAI4Health 2025 shared task. The approach employs a three-stage pipeline: forward translation from Indic languages to English, multitask text generation using a 2.55B parameter distilled language model, and reverse translation back to source languages. By leveraging knowledge distillation techniques, this work demonstrates that compact models can achieve highly competitive performance across nine languages. The system achieved strong win rates across the competition's tasks, with particularly robust performance on Marathi (86.7% QnA), Tamil (86.7% QnA), and Hindi (80.0% QnA), demonstrating the effectiveness of translation-based approaches for low-resource language processing without task-specific fine-tuning.

</details>


### [95] [Beyond Consensus: Perspectivist Modeling and Evaluation of Annotator Disagreement in NLP](https://arxiv.org/abs/2601.09065)
*Yinuo Xu,David Jurgens*

Main category: cs.CL

TL;DR: 本文探讨了处理自然语言处理中注释者分歧的新方法，从数据、任务和注释者三个角度分类，并提出了统一框架进行建模。


<details>
  <summary>Details</summary>
Motivation: 因为注释者分歧在主观且模糊的任务（如毒性检测和立场分析）中普遍存在，这些分歧不应被视为噪音，而应视为反映不同解释和视角的重要信号。

Method: 文章首先构建了一个涵盖数据、任务和注释者的分歧的通用分类法，并提出了基于预测目标和聚合结构的建模框架。此外，讨论了评估模型性能和注释者行为的指标，指出了目前大多数公平性评估仍然是描述性的。

Result: 研究梳理了最新的处理分歧的方法，展示了从共识学习向明确建模分歧和捕捉注释者关系的转变。

Conclusion: 文章提出了未来的研究方向，包括整合多种变异来源、开发基于分歧的解释框架以及处理视角建模的实际权衡。

Abstract: Annotator disagreement is widespread in NLP, particularly for subjective and ambiguous tasks such as toxicity detection and stance analysis. While early approaches treated disagreement as noise to be removed, recent work increasingly models it as a meaningful signal reflecting variation in interpretation and perspective. This survey provides a unified view of disagreement-aware NLP methods. We first present a domain-agnostic taxonomy of the sources of disagreement spanning data, task, and annotator factors. We then synthesize modeling approaches using a common framework defined by prediction targets and pooling structure, highlighting a shift from consensus learning toward explicitly modeling disagreement, and toward capturing structured relationships among annotators. We review evaluation metrics for both predictive performance and annotator behavior, and noting that most fairness evaluations remain descriptive rather than normative. We conclude by identifying open challenges and future directions, including integrating multiple sources of variation, developing disagreement-aware interpretability frameworks, and grappling with the practical tradeoffs of perspectivist modeling.

</details>


### [96] [Mi:dm 2.0 Korea-centric Bilingual Language Models](https://arxiv.org/abs/2601.09066)
*Donghoon Shin,Sejung Lee,Soonmin Bae,Hwijung Ryu,Changwon Ok,Hoyoun Jung,Hyesung Ji,Jeehyun Lim,Jehoon Lee,Ji-Eun Han,Jisoo Baik,Mihyeon Kim,Riwoo Chung,Seongmin Lee,Wonjae Park,Yoonseok Heo,Youngkyung Seo,Seyoun Won,Boeun Kim,Cheolhun Heo,Eunkyeong Lee,Honghee Lee,Hyeongju Ju,Hyeontae Seo,Jeongyong Shim,Jisoo Lee,Junseok Koh,Junwoo Kim,Minho Lee,Minji Kang,Minju Kim,Sangha Nam,Seongheum Park,Taehyeong Kim,Euijai Ahn,Hong Seok Jeung,Jisu Shin,Jiyeon Kim,Seonyeong Song,Seung Hyun Kong,Sukjin Hong,Taeyang Yun,Yu-Seon Kim,A-Hyun Lee,Chae-Jeong Lee,Hye-Won Yu,Ji-Hyun Ahn,Song-Yeon Kim,Sun-Woo Jung,Eunju Kim,Eunji Ha,Jinwoo Baek,Yun-ji Lee,Wanjin Park,Jeong Yeop Kim,Eun Mi Kim,Hyoung Jun Park,Jung Won Yoon,Min Sung Noh,Myung Gyo Oh,Wongyoung Lee,Yun Jin Park,Young S. Kwon,Hyun Keun Kim,Jieun Lee,YeoJoo Park*

Main category: cs.CL

TL;DR: Mi:dm 2.0 是一个专门针对韩国语系设计的大语言模型，通过高质量的数据处理技术和定制化的优化方法，提高了对韩国文化的理解和生成文化适应性响应的能力。它提供了两种配置，分别为适用于通用用途的11.5B参数基础版和适用于资源受限环境的2.3B参数迷你版，并在韩国特定基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型因缺乏高质量的韩国数据和文化对齐不足而存在局限性，Mi:dm 2.0旨在通过更全面的数据处理管道和韩国优化技术，解决这些问题，提升跨文化理解和响应生成的能力。

Method: Mi:dm 2.0 通过制定一个全面的数据处理管道，包括专有的数据清理，高质量的合成数据生成，基于课程学习的数据混合策略，以及定制化的朝鲜优化分词器来提高效率和覆盖范围。该模型的两种配置提供了灵活的选项，适用于不同的环境和任务。

Result: Mi:dm 2.0 在韩国特定基准测试中表现出色，特别是在零样本设置下取得了顶尖结果，并在语言、人文和社会科学研究任务中获得了强大的内部评估结果。

Conclusion: Mi:dm 2.0 作为一个开源模型，KT希望它能够加速韩国各行业、公共服务和教育的AI采用，并强化韩国AI开发者的社区，为实现更广泛的K-智能奠定了基础。

Abstract: We introduce Mi:dm 2.0, a bilingual large language model (LLM) specifically engineered to advance Korea-centric AI. This model goes beyond Korean text processing by integrating the values, reasoning patterns, and commonsense knowledge inherent to Korean society, enabling nuanced understanding of cultural contexts, emotional subtleties, and real-world scenarios to generate reliable and culturally appropriate responses. To address limitations of existing LLMs, often caused by insufficient or low-quality Korean data and lack of cultural alignment, Mi:dm 2.0 emphasizes robust data quality through a comprehensive pipeline that includes proprietary data cleansing, high-quality synthetic data generation, strategic data mixing with curriculum learning, and a custom Korean-optimized tokenizer to improve efficiency and coverage. To realize this vision, we offer two complementary configurations: Mi:dm 2.0 Base (11.5B parameters), built with a depth-up scaling strategy for general-purpose use, and Mi:dm 2.0 Mini (2.3B parameters), optimized for resource-constrained environments and specialized tasks. Mi:dm 2.0 achieves state-of-the-art performance on Korean-specific benchmarks, with top-tier zero-shot results on KMMLU and strong internal evaluation results across language, humanities, and social science tasks. The Mi:dm 2.0 lineup is released under the MIT license to support extensive research and commercial use. By offering accessible and high-performance Korea-centric LLMs, KT aims to accelerate AI adoption across Korean industries, public services, and education, strengthen the Korean AI developer community, and lay the groundwork for the broader vision of K-intelligence. Our models are available at https://huggingface.co/K-intelligence. For technical inquiries, please contact midm-llm@kt.com.

</details>


### [97] [From Symbolic to Natural-Language Relations: Rethinking Knowledge Graph Construction in the Era of Large Language Models](https://arxiv.org/abs/2601.09069)
*Kanyao Han,Yushang Lai*

Main category: cs.CL

TL;DR: 该论文提出了从符号到自然语言关系描述的知识图谱新设计原则，旨在更好地捕捉现实世界关系的语境和细微差。


<details>
  <summary>Details</summary>
Motivation: 当前的关系知识图谱设计存在缺陷，基于预定义符号关系模式构建，导致关键语义细节的丢失；而新兴的LLM支持直接在自然语言中生成领域事实，强调上下文丰富的自由形式文本，这促使重新思考关系的表示。

Method: 论文提出了混合设计原则，保留了知识图谱的结构基础，同时引入了自然语言描述的趋势，以适应基于LLM的语境敏感推理。

Result: 这一创新设计思路能够更精确地表示和管理关系数据，可能会带来更好的知识图谱应用效果和用户体验。

Conclusion: 知识图谱的发展趋势将从象征性语言向自然语言描述转移，以提高模型的有效性和适应性。

Abstract: Knowledge graphs (KGs) have commonly been constructed using predefined symbolic relation schemas, typically implemented as categorical relation labels. This design has notable shortcomings: real-world relations are often contextual, nuanced, and sometimes uncertain, and compressing it into discrete relation labels abstracts away critical semantic detail. Nevertheless, symbolic-relation KGs remain widely used because they have been operationally effective and broadly compatible with pre-LLM downstream models and algorithms, in which KG knowledge could be retrieved or encoded into quantified features and embeddings at scale. The emergence of LLMs has reshaped how knowledge is created and consumed. LLMs support scalable synthesis of domain facts directly in concise natural language, and prompting-based inference favors context-rich free-form text over quantified representations. This position paper argues that these changes call for rethinking the representation of relations themselves rather than merely using LLMs to populate conventional schemas more efficiently. We therefore advocate moving from symbolic to natural-language relation descriptions, and we propose hybrid design principles that preserve a minimal structural backbone while enabling more flexible and context-sensitive relational representations.

</details>


### [98] [How Many Human Judgments Are Enough? Feasibility Limits of Human Preference Evaluation](https://arxiv.org/abs/2601.09084)
*Wilson Y. Lee*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Human preference evaluations are widely used to compare generative models, yet it remains unclear how many judgments are required to reliably detect small improvements. We show that when preference signal is diffuse across prompts (i.e., all prompt types are similarly informative), proportional allocation is minimax-optimal: no allocation strategy substantially improves detectability. Empirical analysis of large-scale human preference datasets shows that most comparisons fall into this diffuse regime, exhibiting small preference margins that require far more judgments than typically collected, even in well-sampled comparisons. These limits persist across evaluation protocols and modalities, including chat, image generation, and code generation with execution feedback. In contrast, curated benchmarks that reduce prompt induced variability systematically induce larger margins and improve detectability through a $1.5\times$ reduction in prompt-level variance. Our results show that inconclusive or negative human evaluation outcomes frequently reflect underpowered evaluation rather than model equivalence, underscoring the need to account explicitly for effect size, budget, and protocol design.

</details>


### [99] [SubTokenTest: A Practical Benchmark for Real-World Sub-token Understanding](https://arxiv.org/abs/2601.09089)
*Shuyang Hou,Yi Hu,Muhan Zhang*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Recent advancements in large language models (LLMs) have significantly enhanced their reasoning capabilities. However, they continue to struggle with basic character-level tasks, such as counting letters in words, a problem rooted in their tokenization process. While existing benchmarks have highlighted this weakness through basic character operations, such failures are often dismissed due to lacking practical relevance. Yet, many real-world applications, such as navigating text-based maps or interpreting structured tables, rely heavily on precise sub-token understanding. In this regard, we introduce SubTokenTest, a comprehensive benchmark that assesses sub-token understanding through practical, utility-driven tasks. Our benchmark includes ten tasks across four domains and isolates tokenization-related failures by decoupling performance from complex reasoning. We provide a comprehensive evaluation of nine advanced LLMs. Additionally, we investigate the impact of test-time scaling on sub-token reasoning and explore how character-level information is encoded within the hidden states.

</details>


### [100] [Contrastive Bi-Encoder Models for Multi-Label Skill Extraction: Enhancing ESCO Ontology Matching with BERT and Attention Mechanisms](https://arxiv.org/abs/2601.09119)
*Yongming Sun*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Fine-grained labor market analysis increasingly relies on mapping unstructured job advertisements to standardized skill taxonomies such as ESCO. This mapping is naturally formulated as an Extreme Multi-Label Classification (XMLC) problem, but supervised solutions are constrained by the scarcity and cost of large-scale, taxonomy-aligned annotations--especially in non-English settings where job-ad language diverges substantially from formal skill definitions. We propose a zero-shot skill extraction framework that eliminates the need for manually labeled job-ad training data. The framework uses a Large Language Model (LLM) to synthesize training instances from ESCO definitions, and introduces hierarchically constrained multi-skill generation based on ESCO Level-2 categories to improve semantic coherence in multi-label contexts. On top of the synthetic corpus, we train a contrastive bi-encoder that aligns job-ad sentences with ESCO skill descriptions in a shared embedding space; the encoder augments a BERT backbone with BiLSTM and attention pooling to better model long, information-dense requirement statements. An upstream RoBERTa-based binary filter removes non-skill sentences to improve end-to-end precision. Experiments show that (i) hierarchy-conditioned generation improves both fluency and discriminability relative to unconstrained pairing, and (ii) the resulting multi-label model transfers effectively to real-world Chinese job advertisements, achieving strong zero-shot retrieval performance (F1@5 = 0.72) and outperforming TF--IDF and standard BERT baselines. Overall, the proposed pipeline provides a scalable, data-efficient pathway for automated skill coding in labor economics and workforce analytics.

</details>


### [101] [Adaptive Multi-Stage Patent Claim Generation with Unified Quality Assessment](https://arxiv.org/abs/2601.09120)
*Chen-Wei Liang,Bin Guo,Zhen-Yuan Wei,Mu-Jiang-Shan Wang*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Current patent claim generation systems face three fundamental limitations: poor cross-jurisdictional generalization, inadequate semantic relationship modeling between claims and prior art, and unreliable quality assessment. We introduce a novel three-stage framework that addresses these challenges through relationship-aware similarity analysis, domain-adaptive claim generation, and unified quality assessment. Our approach employs multi-head attention with eight specialized heads for explicit relationship modeling, integrates curriculum learning with dynamic LoRA adapter selection across five patent domains, and implements cross-attention mechanisms between evaluation aspects for comprehensive quality assessment. Extensive experiments on USPTO HUPD dataset, EPO patent collections, and Patent-CE benchmark demonstrate substantial improvements: 7.6-point ROUGE-L gain over GPT-4o, 8.3\% BERTScore enhancement over Llama-3.1-8B, and 0.847 correlation with human experts compared to 0.623 for separate evaluation models. Our method maintains 89.4\% cross-jurisdictional performance retention versus 76.2\% for baselines, establishing a comprehensive solution for automated patent prosecution workflows.

</details>


### [102] [Identity-Robust Language Model Generation via Content Integrity Preservation](https://arxiv.org/abs/2601.09141)
*Miao Zhang,Kelly Chen,Md Mehrab Tanjim,Rumi Chunara*

Main category: cs.CL

TL;DR: 该研究表明大型语言模型（LLM）的输出在用户社会人口统计学属性上存在差异，这种差异导致了在回答客观问题时事实准确性、实用性和安全性上的不均等。提出了一种轻量级的、无需训练的框架，能够选择性地消除非关键身份信息，同时保留语义上重要的属性，从而保持输出内容的完整性。


<details>
  <summary>Details</summary>
Motivation: 当前的大型语言模型已经在各种任务上取得了显著的成果，但实人在使用中会发现输出结果会因用户的社会人口特征产生较大差异，影响模型的可靠性。因此，研究者提出了这一轻量级框架，以减小这种影响，提高模型的核心生成质量。

Method: 研究者构建了一种轻量级的、无需训练的框架，通过选择性地消除非关键身份信息来保持输出内容的完整性和一致性。该框架在多项基准上进行了测试，并与传统的提示方法进行了对比。

Result: 实验结果表明，该框架相比普通的提示方法，平均减少了77%的依身份差异的偏见，相比基于提示的方法则减少了45%。

Conclusion: 研究者开发的框架能够有效地减轻用户社会人口特征对模型生成质量的影响，对于确保大型语言模型在各种应用场景中的公平性和可靠性具有重要意义。

Abstract: Large Language Model (LLM) outputs often vary across user sociodemographic attributes, leading to disparities in factual accuracy, utility, and safety, even for objective questions where demographic information is irrelevant. Unlike prior work on stereotypical or representational bias, this paper studies identity-dependent degradation of core response quality. We show empirically that such degradation arises from biased generation behavior, despite factual knowledge being robustly encoded across identities. Motivated by this mismatch, we propose a lightweight, training-free framework for identity-robust generation that selectively neutralizes non-critical identity information while preserving semantically essential attributes, thus maintaining output content integrity. Experiments across four benchmarks and 18 sociodemographic identities demonstrate an average 77% reduction in identity-dependent bias compared to vanilla prompting and a 45% reduction relative to prompt-based defenses. Our work addresses a critical gap in mitigating the impact of user identity cues in prompts on core generation quality.

</details>


### [103] [OrthoGeoLoRA: Geometric Parameter-Efficient Fine-Tuning for Structured Social Science Concept Retrieval on theWeb](https://arxiv.org/abs/2601.09185)
*Zeqiang Wang,Xinyue Wu,Chenxi Li,Zixi Chen,Nishanth Sastry,Jon Johnson,Suparna De*

Main category: cs.CL

TL;DR: 本文介绍了一种名为OrthoGeoLoRA的新方法，通过引入正交约束来改进低秩适应(LoRA)，从而在保持参数效率的同时提高语义检索性能。


<details>
  <summary>Details</summary>
Motivation: 小机构和非营利组织因全量微调成本较高而难以采用大型语言模型和文本编码器，本文旨在通过改进低秩适应方法来降低这一成本。

Method: 作者提出了OrthoGeoLoRA，通过正交约束来改进LoRA的几何缺陷，使低秩因子满足正交性条件，从而实现对创Rank因子的约束。这种方法允许使用标准优化器如Adam和现有的微调管道。

Result: 实验表明，与标准LoRA和其他几种强大的PEFT变体相比，在相同低秩预算下，OrthoGeoLoRA在排名指标上的性能更优。

Conclusion: 该方法为计算和参数受限环境下对预训练模型进行适应提供了一种更有成效的方法，特别是在Web4Good生态系统中的信息系统中。

Abstract: Large language models and text encoders increasingly power web-based information systems in the social sciences, including digital libraries, data catalogues, and search interfaces used by researchers, policymakers, and civil society. Full fine-tuning is often computationally and energy intensive, which can be prohibitive for smaller institutions and non-profit organizations in the Web4Good ecosystem. Parameter-Efficient Fine-Tuning (PEFT), especially Low-Rank Adaptation (LoRA), reduces this cost by updating only a small number of parameters. We show that the standard LoRA update $ΔW = BA^\top$ has geometric drawbacks: gauge freedom, scale ambiguity, and a tendency toward rank collapse. We introduce OrthoGeoLoRA, which enforces an SVD-like form $ΔW = BΣA^\top$ by constraining the low-rank factors to be orthogonal (Stiefel manifold). A geometric reparameterization implements this constraint while remaining compatible with standard optimizers such as Adam and existing fine-tuning pipelines. We also propose a benchmark for hierarchical concept retrieval over the European Language Social Science Thesaurus (ELSST), widely used to organize social science resources in digital repositories. Experiments with a multilingual sentence encoder show that OrthoGeoLoRA outperforms standard LoRA and several strong PEFT variants on ranking metrics under the same low-rank budget, offering a more compute- and parameter-efficient path to adapt foundation models in resource-constrained settings.

</details>


### [104] [ProFit: Leveraging High-Value Signals in SFT via Probability-Guided Token Selection](https://arxiv.org/abs/2601.09195)
*Tao Liu,Taiqiang Wu,Runming Yang,Shaoning Sun,Junjie Wang,Yujiu Yang*

Main category: cs.CL

TL;DR: 通过揭示高概率和低概率标记之间的语义差异，提出了ProFit方法，该方法有选择地掩蔽低概率标记以防止表面的过拟合，并在广泛的验证基准上表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前的监督微调通常通过单一的参考答案对语言进行对齐，这限制了模型对非核心表达式的学习，因此提出ProFit以解决这一问题。

Method: ProFit方法通过计算标记概率来识别高概率标记和低概率标记，进而揭示语义重要性。基于此，ProFit选择性地遮蔽低概率标记以防止表面的过拟合。

Result: ProFit在通用推理和数学基准上表现优异，优于传统的监督微调基线。

Conclusion: ProFit通过遮蔽低概率标记显著提高了模型泛化能力，为大规模语言模型的微调提供了一个新的方法。

Abstract: Supervised fine-tuning (SFT) is a fundamental post-training strategy to align Large Language Models (LLMs) with human intent. However, traditional SFT often ignores the one-to-many nature of language by forcing alignment with a single reference answer, leading to the model overfitting to non-core expressions. Although our empirical analysis suggests that introducing multiple reference answers can mitigate this issue, the prohibitive data and computational costs necessitate a strategic shift: prioritizing the mitigation of single-reference overfitting over the costly pursuit of answer diversity. To achieve this, we reveal the intrinsic connection between token probability and semantic importance: high-probability tokens carry the core logical framework, while low-probability tokens are mostly replaceable expressions. Based on this insight, we propose ProFit, which selectively masks low-probability tokens to prevent surface-level overfitting. Extensive experiments confirm that ProFit consistently outperforms traditional SFT baselines on general reasoning and mathematical benchmarks.

</details>


### [105] [A.X K1 Technical Report](https://arxiv.org/abs/2601.09200)
*Sung Jun Cheon,Jaekyung Cho,Seongho Choi,Hyunjun Eun,Seokhwan Jo,Jaehyun Jun,Minsoo Kang,Jin Kim,Jiwon Kim,Minsang Kim,Sungwan Kim,Seungsik Kim,Tae Yoon Kim,Youngrang Kim,Hyeongmun Lee,Sangyeol Lee,Sungeun Lee,Youngsoon Lee,Yujin Lee,Seongmin Ok,Chanyong Park,Hyewoong Park,Junyoung Park,Hyunho Yang,Subin Yi,Soohyun Bae,Dhammiko Arya,Yongseok Choi,Sangho Choi,Dongyeon Cho,Seungmo Cho,Gyoungeun Han,Yong-jin Han,Seokyoung Hong,Hyeon Hwang,Wonbeom Jang,Minjeong Ju,Wonjin Jung,Keummin Ka,Sungil Kang,Dongnam Kim,Joonghoon Kim,Jonghwi Kim,SaeRom Kim,Sangjin Kim,Seongwon Kim,Youngjin Kim,Seojin Lee,Sunwoo Lee,Taehoon Lee,Chanwoo Park,Sohee Park,Sooyeon Park,Yohan Ra,Sereimony Sek,Seungyeon Seo,Gun Song,Sanghoon Woo,Janghan Yoon,Sungbin Yoon*

Main category: cs.CL

TL;DR: A.X K1 是一个采用混合专家（MoE）架构的 519B 参数量语言模型，通过高效的数据预处理和梯度累积策略，实现了在不同计算预算下的性能优化。


<details>
  <summary>Details</summary>
Motivation: A.X K1 的设计旨在平衡模型的推理能力和推理效率，通过引入一个简单而有效的 Think-Fusion 训练方法，实现用户可以根据需要切换模型的思考和非思考模式。

Method: A.X K1 采用了混合专家架构，并通过自底向上的训练方式进行优化。它利用了扩展定律来选择最佳的训练配置和词汇表大小。数据预处理分为多阶段，提高了数据集的质量。此外，模型还通过局部梯度累积进行训练，以适应不同的计算资源。

Result: A.X K1 在多项评估中表现出色，达到了与开源模型相当的性能，特别是在韩语语言模型评估中具有明显优势。

Conclusion: A.X K1 通过其独特的 Think-Fusion 训练方法和其他优化策略，提供了一种既能高效推理又能灵活控制的解决方案，并通过实验证明了其实用性和性能。

Abstract: We introduce A.X K1, a 519B-parameter Mixture-of-Experts (MoE) language model trained from scratch. Our design leverages scaling laws to optimize training configurations and vocabulary size under fixed computational budgets. A.X K1 is pre-trained on a corpus of approximately 10T tokens, curated by a multi-stage data processing pipeline. Designed to bridge the gap between reasoning capability and inference efficiency, A.X K1 supports explicitly controllable reasoning to facilitate scalable deployment across diverse real-world scenarios. We propose a simple yet effective Think-Fusion training recipe, enabling user-controlled switching between thinking and non-thinking modes within a single unified model. Extensive evaluations demonstrate that A.X K1 achieves performance competitive with leading open-source models, while establishing a distinctive advantage in Korean-language benchmarks.

</details>


### [106] [When to Trust: A Causality-Aware Calibration Framework for Accurate Knowledge Graph Retrieval-Augmented Generation](https://arxiv.org/abs/2601.09241)
*Jing Ren,Bowen Li,Ziqi Xu,Xinkun Zhang,Haytham Fayek,Xiaodong Li*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Knowledge Graph Retrieval-Augmented Generation (KG-RAG) extends the RAG paradigm by incorporating structured knowledge from knowledge graphs, enabling Large Language Models (LLMs) to perform more precise and explainable reasoning. While KG-RAG improves factual accuracy in complex tasks, existing KG-RAG models are often severely overconfident, producing high-confidence predictions even when retrieved sub-graphs are incomplete or unreliable, which raises concerns for deployment in high-stakes domains. To address this issue, we propose Ca2KG, a Causality-aware Calibration framework for KG-RAG. Ca2KG integrates counterfactual prompting, which exposes retrieval-dependent uncertainties in knowledge quality and reasoning reliability, with a panel-based re-scoring mechanism that stabilises predictions across interventions. Extensive experiments on two complex QA datasets demonstrate that Ca2KG consistently improves calibration while maintaining or even enhancing predictive accuracy.

</details>


### [107] [TeachPro: Multi-Label Qualitative Teaching Evaluation via Cross-View Graph Synergy and Semantic Anchored Evidence Encoding](https://arxiv.org/abs/2601.09246)
*Xiangqian Wang,Yifan Jia,Yang Xiang,Yumin Zhang,Yanbin Wang,Ke Liu*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Standardized Student Evaluation of Teaching often suffer from low reliability, restricted response options, and response distortion. Existing machine learning methods that mine open-ended comments usually reduce feedback to binary sentiment, which overlooks concrete concerns such as content clarity, feedback timeliness, and instructor demeanor, and provides limited guidance for instructional improvement.We propose TeachPro, a multi-label learning framework that systematically assesses five key teaching dimensions: professional expertise, instructional behavior, pedagogical efficacy, classroom experience, and other performance metrics. We first propose a Dimension-Anchored Evidence Encoder, which integrates three core components: (i) a pre-trained text encoder that transforms qualitative feedback annotations into contextualized embeddings; (ii) a prompt module that represents five teaching dimensions as learnable semantic anchors; and (iii) a cross-attention mechanism that aligns evidence with pedagogical dimensions within a structured semantic space. We then propose a Cross-View Graph Synergy Network to represent student comments. This network comprises two components: (i) a Syntactic Branch that extracts explicit grammatical dependencies from parse trees, and (ii) a Semantic Branch that models latent conceptual relations derived from BERT-based similarity graphs. BiAffine fusion module aligns syntactic and semantic units, while a differential regularizer disentangles embeddings to encourage complementary representations. Finally, a cross-attention mechanism bridges the dimension-anchored evidence with the multi-view comment representations. We also contribute a novel benchmark dataset featuring expert qualitative annotations and multi-label scores. Extensive experiments demonstrate that TeachPro offers superior diagnostic granularity and robustness across diverse evaluation settings.

</details>


### [108] [When to Invoke: Refining LLM Fairness with Toxicity Assessment](https://arxiv.org/abs/2601.09250)
*Jing Ren,Bowen Li,Ziqi Xu,Renqiang Luo,Shuo Yu,Xin Ye,Haytham Fayek,Xiaodong Li,Feng Xia*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large Language Models (LLMs) are increasingly used for toxicity assessment in online moderation systems, where fairness across demographic groups is essential for equitable treatment. However, LLMs often produce inconsistent toxicity judgements for subtle expressions, particularly those involving implicit hate speech, revealing underlying biases that are difficult to correct through standard training. This raises a key question that existing approaches often overlook: when should corrective mechanisms be invoked to ensure fair and reliable assessments? To address this, we propose FairToT, an inference-time framework that enhances LLM fairness through prompt-guided toxicity assessment. FairToT identifies cases where demographic-related variation is likely to occur and determines when additional assessment should be applied. In addition, we introduce two interpretable fairness indicators that detect such cases and improve inference consistency without modifying model parameters. Experiments on benchmark datasets show that FairToT reduces group-level disparities while maintaining stable and reliable toxicity predictions, demonstrating that inference-time refinement offers an effective and practical approach for fairness improvement in LLM-based toxicity assessment systems. The source code can be found at https://aisuko.github.io/fair-tot/.

</details>


### [109] [MCGA: A Multi-task Classical Chinese Literary Genre Audio Corpus](https://arxiv.org/abs/2601.09270)
*Yexing Du,Kaiyuan Liu,Bihe Zhang,Youcheng Pan,Bo Yang,Liangyu Huo,Xiyuan Zhang,Jian Xie,Daojing He,Yang Xiang,Ming Liu,Bin Qin*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: With the rapid advancement of Multimodal Large Language Models (MLLMs), their potential has garnered significant attention in Chinese Classical Studies (CCS). While existing research has primarily focused on text and visual modalities, the audio corpus within this domain remains largely underexplored. To bridge this gap, we propose the Multi-task Classical Chinese Literary Genre Audio Corpus (MCGA). It encompasses a diverse range of literary genres across six tasks: Automatic Speech Recognition (ASR), Speech-to-Text Translation (S2TT), Speech Emotion Captioning (SEC), Spoken Question Answering (SQA), Speech Understanding (SU), and Speech Reasoning (SR). Through the evaluation of ten MLLMs, our experimental results demonstrate that current models still face substantial challenges when processed on the MCGA test set. Furthermore, we introduce an evaluation metric for SEC and a metric to measure the consistency between the speech and text capabilities of MLLMs. We release MCGA and our code to the public to facilitate the development of MLLMs with more robust multidimensional audio capabilities in CCS. MCGA Corpus: https://github.com/yxduir/MCGA

</details>


### [110] [ReGraM: Region-First Knowledge Graph Reasoning for Medical Question Answering](https://arxiv.org/abs/2601.09280)
*Chaerin Lee,Sohee Park,Hyunsik Na,Daseon Choi*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Recent studies in medical question answering (Medical QA) have actively explored the integration of large language models (LLMs) with biomedical knowledge graphs (KGs) to improve factual accuracy. However, most existing approaches still rely on traversing the entire KG or performing large-scale retrieval, which introduces substantial noise and leads to unstable multi-hop reasoning. We argue that the core challenge lies not in expanding access to knowledge, but in identifying and reasoning over the appropriate subset of evidence for each query. ReGraM is a region-first knowledge graph reasoning framework that addresses this challenge by constructing a query-aligned subgraph and performing stepwise reasoning constrained to this localized region under multiple evidence aware modes. By focusing inference on only the most relevant portion of the KG, ReGraM departs from the assumption that all relations are equally useful an assumption that rarely holds in domain-specific medical settings. Experiments on seven medical QA benchmarks demonstrate that ReGraM consistently outperforms a strong baseline (KGARevion), achieving an 8.04% absolute accuracy gain on MCQ, a 4.50% gain on SAQ, and a 42.9% reduction in hallucination rate. Ablation and qualitative analyses further show that aligning region construction with hop-wise reasoning is the primary driver of these improvements. Overall, our results highlight region-first KG reasoning as an effective paradigm for improving factual accuracy and consistency in medical QA.

</details>


### [111] [Understanding or Memorizing? A Case Study of German Definite Articles in Language Models](https://arxiv.org/abs/2601.09313)
*Jonathan Drechsel,Erisa Bytyqi,Steffen Herbold*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Language models perform well on grammatical agreement, but it is unclear whether this reflects rule-based generalization or memorization. We study this question for German definite singular articles, whose forms depend on gender and case. Using GRADIEND, a gradient-based interpretability method, we learn parameter update directions for gender-case specific article transitions. We find that updates learned for a specific gender-case article transition frequently affect unrelated gender-case settings, with substantial overlap among the most affected neurons across settings. These results argue against a strictly rule-based encoding of German definite articles, indicating that models at least partly rely on memorized associations rather than abstract grammatical rules.

</details>


### [112] [Improving Implicit Hate Speech Detection via a Community-Driven Multi-Agent Framework](https://arxiv.org/abs/2601.09342)
*Ewelina Gajewska,Katarzyna Budzynska,Jarosław A Chudziak*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This work proposes a contextualised detection framework for implicitly hateful speech, implemented as a multi-agent system comprising a central Moderator Agent and dynamically constructed Community Agents representing specific demographic groups. Our approach explicitly integrates socio-cultural context from publicly available knowledge sources, enabling identity-aware moderation that surpasses state-of-the-art prompting methods (zero-shot prompting, few-shot prompting, chain-of-thought prompting) and alternative approaches on a challenging ToxiGen dataset. We enhance the technical rigour of performance evaluation by incorporating balanced accuracy as a central metric of classification fairness that accounts for the trade-off between true positive and true negative rates. We demonstrate that our community-driven consultative framework significantly improves both classification accuracy and fairness across all target groups.

</details>


### [113] [Frame of Reference: Addressing the Challenges of Common Ground Representation in Situational Dialogs](https://arxiv.org/abs/2601.09365)
*Biswesh Mohapatra,Théo Charlot,Giovanni Duca,Mayank Palan,Laurent Romary,Justine Cassell*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Common ground plays a critical role in situated spoken dialogues, where interlocutors must establish and maintain shared references to entities, events, and relations to sustain coherent interaction. For dialog systems, the ability to correctly ground conversational content in order to refer back to it later is particularly important. Prior studies have demonstrated that LLMs are capable of performing grounding acts such as requesting clarification or producing acknowledgments, yet relatively little work has investigated how common ground can be explicitly represented and stored for later use. Without such mechanisms, it remains unclear whether acknowledgment or clarification behaviors truly reflect a grounded understanding. In this work, we evaluate a model's ability to establish and exploit common ground through relational references to entities within the shared context in a situational dialogue. We test multiple methods for representing common ground in situated dialogues and further propose approaches to improve both the establishment of common ground and its subsequent use in the conversation.

</details>


### [114] [Relation Extraction Capabilities of LLMs on Clinical Text: A Bilingual Evaluation for English and Turkish](https://arxiv.org/abs/2601.09367)
*Aidana Aidynkyzy,Oğuz Dikenelli,Oylum Alatlı,Şebnem Bora*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The scarcity of annotated datasets for clinical information extraction in non-English languages hinders the evaluation of large language model (LLM)-based methods developed primarily in English. In this study, we present the first comprehensive bilingual evaluation of LLMs for the clinical Relation Extraction (RE) task in both English and Turkish. To facilitate this evaluation, we introduce the first English-Turkish parallel clinical RE dataset, derived and carefully curated from the 2010 i2b2/VA relation classification corpus. We systematically assess a diverse set of prompting strategies, including multiple in-context learning (ICL) and Chain-of-Thought (CoT) approaches, and compare their performance to fine-tuned baselines such as PURE. Furthermore, we propose Relation-Aware Retrieval (RAR), a novel in-context example selection method based on contrastive learning, that is specifically designed to capture both sentence-level and relation-level semantics. Our results show that prompting-based LLM approaches consistently outperform traditional fine-tuned models. Moreover, evaluations for English performed better than their Turkish counterparts across all evaluated LLMs and prompting techniques. Among ICL methods, RAR achieves the highest performance, with Gemini 1.5 Flash reaching a micro-F1 score of 0.906 in English and 0.888 in Turkish. Performance further improves to 0.918 F1 in English when RAR is combined with a structured reasoning prompt using the DeepSeek-V3 model. These findings highlight the importance of high-quality demonstration retrieval and underscore the potential of advanced retrieval and prompting techniques to bridge resource gaps in clinical natural language processing.

</details>


### [115] [The Imperfective Paradox in Large Language Models](https://arxiv.org/abs/2601.09373)
*Bolei Ma,Yusuke Miyao*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Do Large Language Models (LLMs) genuinely grasp the compositional semantics of events, or do they rely on surface-level probabilistic heuristics? We investigate the Imperfective Paradox, a logical phenomenon where the past progressive aspect entails event realization for activities (e.g., running $\to$ ran) but not for accomplishments (e.g., building $\nrightarrow$ built). We introduce ImperfectiveNLI, a diagnostic dataset designed to probe this distinction across diverse semantic classes. Evaluating state-of-the-art open-weight models, we uncover a pervasive Teleological Bias: models systematically hallucinate completion for goal-oriented events, often overriding explicit textual negation. Representational analyses show that while internal embeddings often distinguish process from result, inference decisions are dominated by strong priors about goal attainment. We further find that prompting-based interventions reduce hallucinated completions but also increase incorrect rejections of valid entailments. Our findings suggest that current LLMs lack structural aspectual awareness, operating as predictive narrative engines rather than faithful logical reasoners.

</details>


### [116] [Ability Transfer and Recovery via Modularized Parameters Localization](https://arxiv.org/abs/2601.09398)
*Songyao Jin,Kun Zhou,Wenqi Li,Peng Wang,Biwei Huang*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型参数内的能力分布，并提出了ACT（基于激活的能力通道迁移）方法，通过差异激活定位相关能力通道并精简迁移参数。


<details>
  <summary>Details</summary>
Motivation: 动机在于避免模型在特定领域或语言上的精细化训练导致的泛化能力下降，甚至遗忘原有技能。

Method: 该方法通过对比分析相关模型的模块激活情况，识别出能力相关的激活主要集中在少数的通道中。ACT方法利用这些差异激活精确定位并迁移相关通道的参数，之后进行轻量级的微调以增强兼容性。

Result: 实验表明，ACT能够在恢复遗忘能力的同时保持原有技能，并且能够将多个专有模型的能力集成到单一模型中，且不会产生明显的干扰。

Conclusion: 研究提出的方法ACT证明了在大型语言模型中社区集中的激活区域能够有效传递特定领域的能力，同时保持模型的稳定性。

Abstract: Large language models can be continually pre-trained or fine-tuned to improve performance in specific domains, languages, or skills, but this specialization often degrades other capabilities and may cause catastrophic forgetting. We investigate how abilities are distributed within LLM parameters by analyzing module activations under domain- and language-specific inputs for closely related models. Across layers and modules, we find that ability-related activations are highly concentrated in a small set of channels (typically <5\%), and these channels are largely disentangled with good sufficiency and stability. Building on these observations, we propose ACT (Activation-Guided Channel-wise Ability Transfer), which localizes ability-relevant channels via activation differences and selectively transfers only the corresponding parameters, followed by lightweight fine-tuning for compatibility. Experiments on multilingual mathematical and scientific reasoning show that ACT can recover forgotten abilities while preserving retained skills. It can also merge multiple specialized models to integrate several abilities into a single model with minimal interference. Our code and data will be publicly released.

</details>


### [117] [Structured Knowledge Representation through Contextual Pages for Retrieval-Augmented Generation](https://arxiv.org/abs/2601.09402)
*Xinze Li,Zhenghao Liu,Haidong Xin,Yukun Yan,Shuo Wang,Zheni Zeng,Sen Mei,Ge Yu,Maosong Sun*

Main category: cs.CL

TL;DR: PAGER 提出了一种基于页面的知识表示框架，通过结构化的认知大纲和迭代检索来构建更连贯的知识表示，从而提高 Retrieval-Augmented Generation (RAG) 模型的知识质量，促进 LLM 的外部知识利用。


<details>
  <summary>Details</summary>
Motivation: 目前的 RAG 模型虽然能够利用外部知识，但这些知识往往是杂乱无章的，没有一个统一的组织结构，限制了更全面的知识表示。因此，PAGER 提出了一种新的框架，旨在提高知识质量，解决知识冲突，更有效地利用外部信息。

Method: PAGER 的方法包括两部分：首先，提示 LLM 构建包含多个知识方面槽的认知结构大纲。其次，通过迭代检索和精炼相关文档填充这些槽，最终生成一个连贯的页面作为指导答案生成的上下文输入。

Result: 实验结果表明，PAGER 在多个知识密集型基准测试上优于所有 RAG 基线。 PAGER 能够生成更高质量和信息密集的知识表示，并更好地缓和知识冲突，促进 LLM 利用外部知识。

Conclusion: PAGER 提出了一种创新的方法来增强 RAG 模型的知识利用，提高了知识质量和整体系统性能。

Abstract: Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by incorporating external knowledge. Recently, some works have incorporated iterative knowledge accumulation processes into RAG models to progressively accumulate and refine query-related knowledge, thereby constructing more comprehensive knowledge representations. However, these iterative processes often lack a coherent organizational structure, which limits the construction of more comprehensive and cohesive knowledge representations. To address this, we propose PAGER, a page-driven autonomous knowledge representation framework for RAG. PAGER first prompts an LLM to construct a structured cognitive outline for a given question, which consists of multiple slots representing a distinct knowledge aspect. Then, PAGER iteratively retrieves and refines relevant documents to populate each slot, ultimately constructing a coherent page that serves as contextual input for guiding answer generation. Experiments on multiple knowledge-intensive benchmarks and backbone models show that PAGER consistently outperforms all RAG baselines. Further analyses demonstrate that PAGER constructs higher-quality and information-dense knowledge representations, better mitigates knowledge conflicts, and enables LLMs to leverage external knowledge more effectively. All code is available at https://github.com/OpenBMB/PAGER.

</details>


### [118] [Improving Symbolic Translation of Language Models for Logical Reasoning](https://arxiv.org/abs/2601.09446)
*Ramya Keerthy Thatikonda,Jiuzhou Han,Wray Buntine,Ehsan Shareghi*

Main category: cs.CL

TL;DR: 本文介绍了一种使用形式语言进行演绎逻辑推理的新方法，通过分类常见错误并利用大量语言模型的数据合成进行微调，提出了分阶段推理和验证模块来提高小型语言模型的推理性能。


<details>
  <summary>Details</summary>
Motivation: 当前的小型语言模型在将自然语言转化为一阶逻辑方面存在困难，经常因格式和转换错误产生错误的符号输出。现有的方法依赖于自我迭代纠正这些错误，但这种方法对底层模型的能力有很高的依赖性，因此需要新的方法来提高小型语言模型的推理可靠性。

Method: 文章首先对常见错误进行分类，然后使用大型语言模型生成的数据进行微调，引入了增量推理，将其分成谓词生成和一阶逻辑翻译两个阶段，还引入了验证模块来专门针对谓词元数错误进行纠正。

Result: 经过这种方法的处理，小型语言模型的错误率降低，谓词覆盖范围增加，推理性能提高。这项研究在四个逻辑推理数据集上评估了三种模型系列。

Conclusion: 这种新的微调、增量推理和验证模块的方法，使得小型语言模型的演绎逻辑推理性能有了显著的提高，朝着开发可靠且易于访问的符号推理系统迈出了重要的一步。

Abstract: The use of formal language for deductive logical reasoning aligns well with language models (LMs), where translating natural language (NL) into first-order logic (FOL) and employing an external solver results in a verifiable and therefore reliable reasoning system. However, smaller LMs often struggle with this translation task, frequently producing incorrect symbolic outputs due to formatting and translation errors. Existing approaches typically rely on self-iteration to correct these errors, but such methods depend heavily on the capabilities of the underlying model. To address this, we first categorize common errors and fine-tune smaller LMs using data synthesized by large language models. The evaluation is performed using the defined error categories. We introduce incremental inference, which divides inference into two stages, predicate generation and FOL translation, providing greater control over model behavior and enhancing generation quality as measured by predicate metrics. This decomposition framework also enables the use of a verification module that targets predicate-arity errors to further improve performance. Our study evaluates three families of models across four logical-reasoning datasets. The comprehensive fine-tuning, incremental inference, and verification modules reduce error rates, increase predicate coverage, and improve reasoning performance for smaller LMs, moving us closer to developing reliable and accessible symbolic-reasoning systems.

</details>


### [119] [SlidesGen-Bench: Evaluating Slides Generation via Computational and Quantitative Metrics](https://arxiv.org/abs/2601.09487)
*Yunqiao Yang,Wenbo Li,Houxing Ren,Zimu Lu,Ke Wang,Zhiyuan Huang,Zhuofan Zong,Mingjie Zhan,Hongsheng Li*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The rapid evolution of Large Language Models (LLMs) has fostered diverse paradigms for automated slide generation, ranging from code-driven layouts to image-centric synthesis. However, evaluating these heterogeneous systems remains challenging, as existing protocols often struggle to provide comparable scores across architectures or rely on uncalibrated judgments. In this paper, we introduce SlidesGen-Bench, a benchmark designed to evaluate slide generation through a lens of three core principles: universality, quantification, and reliability. First, to establish a unified evaluation framework, we ground our analysis in the visual domain, treating terminal outputs as renderings to remain agnostic to the underlying generation method. Second, we propose a computational approach that quantitatively assesses slides across three distinct dimensions - Content, Aesthetics, and Editability - offering reproducible metrics where prior works relied on subjective or reference-dependent proxies. Finally, to ensure high correlation with human preference, we construct the Slides-Align1.5k dataset, a human preference aligned dataset covering slides from nine mainstream generation systems across seven scenarios. Our experiments demonstrate that SlidesGen-Bench achieves a higher degree of alignment with human judgment than existing evaluation pipelines. Our code and data are available at https://github.com/YunqiaoYang/SlidesGen-Bench.

</details>


### [120] [MVSS: A Unified Framework for Multi-View Structured Survey Generation](https://arxiv.org/abs/2601.09504)
*Yinqi Liu,Yueqi Zhu,Yongkang Zhang,Xinfeng Li,Feiran Liu,Yufei Sun,Xin Wang,Renzhao Liang,Yidong Wang,Cunxiang Wang*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Scientific surveys require not only summarizing large bodies of literature, but also organizing them into clear and coherent conceptual structures. Existing automatic survey generation methods typically focus on linear text generation and struggle to explicitly model hierarchical relations among research topics and structured methodological comparisons, resulting in gaps in structural organization compared to expert-written surveys. We propose MVSS, a multi-view structured survey generation framework that jointly generates and aligns citation-grounded hierarchical trees, structured comparison tables, and survey text. MVSS follows a structure-first paradigm: it first constructs a conceptual tree of the research domain, then generates comparison tables constrained by the tree, and finally uses both as structural constraints for text generation. This enables complementary multi-view representations across structure, comparison, and narrative. We introduce an evaluation framework assessing structural quality, comparative completeness, and citation fidelity. Experiments on 76 computer science topics show MVSS outperforms existing methods in organization and evidence grounding, achieving performance comparable to expert surveys.

</details>


### [121] [SERM: Self-Evolving Relevance Model with Agent-Driven Learning from Massive Query Streams](https://arxiv.org/abs/2601.09515)
*Chenglong Wang,Canjia Li,Xingzhao Zhu,Yifu Huo,Huiyu Wang,Weixiong Lin,Yun Yang,Qiaozhi He,Tianhua Zhou,Xiaojia Chang,Jingbo Zhu,Tong Xiao*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Due to the dynamically evolving nature of real-world query streams, relevance models struggle to generalize to practical search scenarios. A sophisticated solution is self-evolution techniques. However, in large-scale industrial settings with massive query streams, this technique faces two challenges: (1) informative samples are often sparse and difficult to identify, and (2) pseudo-labels generated by the current model could be unreliable. To address these challenges, in this work, we propose a Self-Evolving Relevance Model approach (SERM), which comprises two complementary multi-agent modules: a multi-agent sample miner, designed to detect distributional shifts and identify informative training samples, and a multi-agent relevance annotator, which provides reliable labels through a two-level agreement framework. We evaluate SERM in a large-scale industrial setting, which serves billions of user requests daily. Experimental results demonstrate that SERM can achieve significant performance gains through iterative self-evolution, as validated by extensive offline multilingual evaluations and online testing.

</details>


### [122] [Benchmarking Post-Training Quantization of Large Language Models under Microscaling Floating Point Formats](https://arxiv.org/abs/2601.09555)
*Manyi Zhang,Ji-Fu Li,Zhongao Sun,Haoli Bai,Hui-Ling Zhen,Zhenhua Dong,Xianzhi Yu*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Microscaling Floating-Point (MXFP) has emerged as a promising low-precision format for large language models (LLMs). Despite various post-training quantization (PTQ) algorithms being proposed, they mostly focus on integer quantization, while their applicability and behavior under MXFP formats remain largely unexplored. To address this gap, this work conducts a systematic investigation of PTQ under MXFP formats, encompassing over 7 PTQ algorithms, 15 evaluation benchmarks, and 3 LLM families. The key findings include: 1) MXFP8 consistently achieves near-lossless performance, while MXFP4 introduces substantial accuracy degradation and remains challenging; 2) PTQ effectiveness under MXFP depends strongly on format compatibility, with some algorithmic paradigms being consistently more effective than others; 3) PTQ performance exhibits highly consistent trends across model families and modalities, in particular, quantization sensitivity is dominated by the language model rather than the vision encoder in multimodal LLMs; 4) The scaling factor of quantization is a critical error source in MXFP4, and a simple pre-scale optimization strategy can significantly mitigate its impact. Together, these results provide practical guidance on adapting existing PTQ methods to MXFP quantization.

</details>


### [123] [Dialogue Telemetry: Turn-Level Instrumentation for Autonomous Information Gathering](https://arxiv.org/abs/2601.09570)
*Dimitris Panagopoulos,Adolfo Perrusquia,Weisi Guo*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Autonomous systems conducting schema-grounded information-gathering dialogues face an instrumentation gap, lacking turn-level observables for monitoring acquisition efficiency and detecting when questioning becomes unproductive. We introduce Dialogue Telemetry (DT), a measurement framework that produces two model-agnostic signals after each question-answer exchange: (i) a Progress Estimator (PE) quantifying residual information potential per category (with a bits-based variant), and (ii) a Stalling Index (SI) detecting an observable failure signature characterized by repeated category probing with semantically similar, low-marginal-gain responses. SI flags this pattern without requiring causal diagnosis, supporting monitoring in settings where attributing degradation to specific causes may be impractical. We validate DT in controlled search-and-rescue (SAR)-inspired interviews using large language model (LLM)-based simulations, distinguishing efficient from stalled dialogue traces and illustrating downstream utility by integrating DT signals into a reinforcement learning (RL) policy. Across these settings, DT provides interpretable turn-level instrumentation that improves policy performance when stalling carries operational costs.

</details>


### [124] [DPWriter: Reinforcement Learning with Diverse Planning Branching for Creative Writing](https://arxiv.org/abs/2601.09609)
*Qian Cao,Yahui Liu,Wei Bi,Yi Zhao,Ruihua Song,Xiting Wang,Ruiming Tang,Guorui Zhou,Han Li*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Reinforcement learning (RL)-based enhancement of large language models (LLMs) often leads to reduced output diversity, undermining their utility in open-ended tasks like creative writing. Current methods lack explicit mechanisms for guiding diverse exploration and instead prioritize optimization efficiency and performance over diversity. This paper proposes an RL framework structured around a semi-structured long Chain-of-Thought (CoT), in which the generation process is decomposed into explicitly planned intermediate steps. We introduce a Diverse Planning Branching method that strategically introduces divergence at the planning phase based on diversity variation, alongside a group-aware diversity reward to encourage distinct trajectories. Experimental results on creative writing benchmarks demonstrate that our approach significantly improves output diversity without compromising generation quality, consistently outperforming existing baselines.

</details>


### [125] [LLMs Got Rhythm? Hybrid Phonological Filtering for Greek Poetry Rhyme Detection and Generation](https://arxiv.org/abs/2601.09631)
*Stergios Chatzikyriakidis*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large Language Models (LLMs), despite their remarkable capabilities across NLP tasks, struggle with phonologically-grounded phenomena like rhyme detection and generation. This is even more evident in lower-resource languages such as Modern Greek. In this paper, we present a hybrid system that combines LLMs with deterministic phonological algorithms to achieve accurate rhyme identification/analysis and generation. Our approach implements a comprehensive taxonomy of Greek rhyme types, including Pure, Rich, Imperfect, Mosaic, and Identical Pre-rhyme Vowel (IDV) patterns, and employs an agentic generation pipeline with phonological verification. We evaluate multiple prompting strategies (zero-shot, few-shot, Chain-of-Thought, and RAG-augmented) across several LLMs including Claude 3.7 and 4.5, GPT-4o, Gemini 2.0 and open-weight models like Llama 3.1 8B and 70B and Mistral Large. Results reveal a significant "Reasoning Gap": while native-like models (Claude 3.7) perform intuitively (40\% accuracy in identification), reasoning-heavy models (Claude 4.5) achieve state-of-the-art performance (54\%) only when prompted with Chain-of-Thought. Most critically, pure LLM generation fails catastrophically (under 4\% valid poems), while our hybrid verification loop restores performance to 73.1\%. We release our system and a crucial, rigorously cleaned corpus of 40,000+ rhymes, derived from the Anemoskala and Interwar Poetry corpora, to support future research.

</details>


### [126] [TaxoBell: Gaussian Box Embeddings for Self-Supervised Taxonomy Expansion](https://arxiv.org/abs/2601.09633)
*Sahil Mishra,Srinitish Srinivasan,Srikanta Bedathur,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Taxonomies form the backbone of structured knowledge representation across diverse domains, enabling applications such as e-commerce catalogs, semantic search, and biomedical discovery. Yet, manual taxonomy expansion is labor-intensive and cannot keep pace with the emergence of new concepts. Existing automated methods rely on point-based vector embeddings, which model symmetric similarity and thus struggle with the asymmetric "is-a" relationships that are fundamental to taxonomies. Box embeddings offer a promising alternative by enabling containment and disjointness, but they face key issues: (i) unstable gradients at the intersection boundaries, (ii) no notion of semantic uncertainty, and (iii) limited capacity to represent polysemy or ambiguity. We address these shortcomings with TaxoBell, a Gaussian box embedding framework that translates between box geometries and multivariate Gaussian distributions, where means encode semantic location and covariances encode uncertainty. Energy-based optimization yields stable optimization, robust modeling of ambiguous concepts, and interpretable hierarchical reasoning. Extensive experimentation on five benchmark datasets demonstrates that TaxoBell significantly outperforms eight state-of-the-art taxonomy expansion baselines by 19% in MRR and around 25% in Recall@k. We further demonstrate the advantages and pitfalls of TaxoBell with error analysis and ablation studies.

</details>


### [127] [Creating a Hybrid Rule and Neural Network Based Semantic Tagger using Silver Standard Data: the PyMUSAS framework for Multilingual Semantic Annotation](https://arxiv.org/abs/2601.09648)
*Andrew Moore,Paul Rayson,Dawn Archer,Tim Czerniak,Dawn Knight,Daisy Lal,Gearóid Ó Donnchadha,Mícheál Ó Meachair,Scott Piao,Elaine Uí Dhonnchadha,Johanna Vuorinen,Yan Yabo,Xiaobin Yang*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Word Sense Disambiguation (WSD) has been widely evaluated using the semantic frameworks of WordNet, BabelNet, and the Oxford Dictionary of English. However, for the UCREL Semantic Analysis System (USAS) framework, no open extensive evaluation has been performed beyond lexical coverage or single language evaluation. In this work, we perform the largest semantic tagging evaluation of the rule based system that uses the lexical resources in the USAS framework covering five different languages using four existing datasets and one novel Chinese dataset. We create a new silver labelled English dataset, to overcome the lack of manually tagged training data, that we train and evaluate various mono and multilingual neural models in both mono and cross-lingual evaluation setups with comparisons to their rule based counterparts, and show how a rule based system can be enhanced with a neural network model. The resulting neural network models, including the data they were trained on, the Chinese evaluation dataset, and all of the code have been released as open resources.

</details>


### [128] [DeepResearchEval: An Automated Framework for Deep Research Task Construction and Agentic Evaluation](https://arxiv.org/abs/2601.09688)
*Yibo Wang,Lei Wang,Yue Deng,Keming Wu,Yao Xiao,Huanjin Yao,Liwei Kang,Hai Ye,Yongcheng Jing,Lidong Bing*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Deep research systems are widely used for multi-step web research, analysis, and cross-source synthesis, yet their evaluation remains challenging. Existing benchmarks often require annotation-intensive task construction, rely on static evaluation dimensions, or fail to reliably verify facts when citations are missing. To bridge these gaps, we introduce DeepResearchEval, an automated framework for deep research task construction and agentic evaluation. For task construction, we propose a persona-driven pipeline generating realistic, complex research tasks anchored in diverse user profiles, applying a two-stage filter Task Qualification and Search Necessity to retain only tasks requiring multi-source evidence integration and external retrieval. For evaluation, we propose an agentic pipeline with two components: an Adaptive Point-wise Quality Evaluation that dynamically derives task-specific evaluation dimensions, criteria, and weights conditioned on each generated task, and an Active Fact-Checking that autonomously extracts and verifies report statements via web search, even when citations are missing.

</details>


### [129] [Routing with Generated Data: Annotation-Free LLM Skill Estimation and Expert Selection](https://arxiv.org/abs/2601.09692)
*Tianyi Niu,Justin Chih-Yao Chen,Genta Indra Winata,Shi-Xiong Zhang,Supriyo Chakraborty,Sambit Sahu,Yue Zhang,Elias Stengel-Eskin,Mohit Bansal*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large Language Model (LLM) routers dynamically select optimal models for given inputs. Existing approaches typically assume access to ground-truth labeled data, which is often unavailable in practice, especially when user request distributions are heterogeneous and unknown. We introduce Routing with Generated Data (RGD), a challenging setting in which routers are trained exclusively on generated queries and answers produced from high-level task descriptions by generator LLMs. We evaluate query-answer routers (using both queries and labels) and query-only routers across four diverse benchmarks and 12 models, finding that query-answer routers degrade faster than query-only routers as generator quality decreases. Our analysis reveals two crucial characteristics of effective generators: they must accurately respond to their own questions, and their questions must produce sufficient performance differentiation among the model pool. We then show how filtering for these characteristics can improve the quality of generated data. We further propose CASCAL, a novel query-only router that estimates model correctness through consensus voting and identifies model-specific skill niches via hierarchical clustering. CASCAL is substantially more robust to generator quality, outperforming the best query-answer router by 4.6% absolute accuracy when trained on weak generator data.

</details>


### [130] [LLMs can Compress LLMs: Adaptive Pruning by Agents](https://arxiv.org/abs/2601.09694)
*Sai Varun Kodathala,Rakesh Vunnam*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: As Large Language Models (LLMs) continue to scale, post-training pruning has emerged as a promising approach to reduce computational costs while preserving performance. Existing methods such as SparseGPT and Wanda achieve high sparsity through layer-wise weight reconstruction or activation-aware magnitude pruning, but rely on uniform or hand-crafted heuristics to determine per-layer sparsity ratios. Moreover, recent work has shown that pruned LLMs suffer from severe factual knowledge degradation, with structured pruning methods experiencing near-total collapse in factual question-answering capabilities. We introduce agent-guided pruning, where a foundation model acts as an adaptive pruning agent to intelligently select which layers to prune at each iteration while preserving critical knowledge pathways. Our method constructs layer-wise sensitivity profiles by combining Wanda-inspired weight-activation metrics with gradient importance scores, normalized as z-scores for model-agnostic comparison. These statistics are processed by an LLM agent equipped with self-reflection capabilities, enabling it to learn from previous pruning outcomes and iteratively refine its strategy. A checkpoint rollback mechanism maintains model quality by reverting when perplexity degradation exceeds a threshold. We evaluate our approach on Qwen3 models (4B and 8B parameters) at approximately 45% sparsity, demonstrating substantial improvements over structured pruning baselines: 56% relative improvement in MMLU accuracy, 19x better factual knowledge retention on FreebaseQA, and 69% lower perplexity degradation. Notably, our framework requires no retraining, operates in a model-agnostic manner, and exhibits effective self-correction with only 2-4 rollbacks across 21-40 iterations, demonstrating that foundation models can effectively guide the compression of other foundation models.

</details>


### [131] [Empathy Applicability Modeling for General Health Queries](https://arxiv.org/abs/2601.09696)
*Shan Randhawa,Agha Ali Raza,Kentaro Toyama,Julie Hui,Mustafa Naseem*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: LLMs are increasingly being integrated into clinical workflows, yet they often lack clinical empathy, an essential aspect of effective doctor-patient communication. Existing NLP frameworks focus on reactively labeling empathy in doctors' responses but offer limited support for anticipatory modeling of empathy needs, especially in general health queries. We introduce the Empathy Applicability Framework (EAF), a theory-driven approach that classifies patient queries in terms of the applicability of emotional reactions and interpretations, based on clinical, contextual, and linguistic cues. We release a benchmark of real patient queries, dual-annotated by Humans and GPT-4o. In the subset with human consensus, we also observe substantial human-GPT alignment. To validate EAF, we train classifiers on human-labeled and GPT-only annotations to predict empathy applicability, achieving strong performance and outperforming the heuristic and zero-shot LLM baselines. Error analysis highlights persistent challenges: implicit distress, clinical-severity ambiguity, and contextual hardship, underscoring the need for multi-annotator modeling, clinician-in-the-loop calibration, and culturally diverse annotation. EAF provides a framework for identifying empathy needs before response generation, establishes a benchmark for anticipatory empathy modeling, and enables supporting empathetic communication in asynchronous healthcare.

</details>


### [132] [Value-Aware Numerical Representations for Transformer Language Models](https://arxiv.org/abs/2601.09706)
*Andreea Dutulescu,Stefan Ruseti,Mihai Dascalu*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Transformer-based language models often achieve strong results on mathematical reasoning benchmarks while remaining fragile on basic numerical understanding and arithmetic operations. A central limitation is that numbers are processed as symbolic tokens whose embeddings do not explicitly encode numerical value, leading to systematic errors. We introduce a value-aware numerical representation that augments standard tokenized inputs with a dedicated prefix token whose embedding is explicitly conditioned on the underlying numerical value. This mechanism injects magnitude information directly into the model's input space while remaining compatible with existing tokenizers and decoder-only Transformer architectures. Evaluation on arithmetic tasks shows that the proposed approach outperforms baselines across numerical formats, tasks, and operand lengths. These results indicate that explicitly encoding numerical value is an effective and efficient way to improve fundamental numerical robustness in language models.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [133] [ConvoLearn: A Dataset of Constructivist Tutor-Student Dialogue](https://arxiv.org/abs/2601.08950)
*Mayank Sharma,Roy Pea,Hari Subramonyam*

Main category: cs.AI

TL;DR: 本文提出了一种名为ConvoLearn的数据集，旨在通过六种核心教学维度（认知参与、形成性评估、责任感、文化响应性、元认知和权力动态）引导大语言模型向知识构建策略转变。通过使用QLoRA对Mistral 7B进行微调，并经由31名教师的人类评估，表明强化训练显著提升了模型的表现。


<details>
  <summary>Details</summary>
Motivation: 在教育应用中，现存的LLMs倾向于直接提供解决方案而不是支持对话式学习。因此，为了促进更有效的学习环境，作者提出ConvoLearn数据集以指导构建和评估秉持建构主义理念的AI导师。

Method: 作者构建了一个半合成数据集，包括1250个中学地球科学的学生-教师对话（每对话20轮），通过控制人类教师与模拟学生的交互产生。利用QLoRA技术对Mistral 7B进行微调。

Result: 经过人类教师的评估，与基础版本Mistral 7B（评分均值2.59，标准差1.11）和Claude Sonnet 4.5（评分均值2.87，标准差1.29）相比，微调后的Mistral 7B（评分均值4.10，标准差1.03）显示出了显著提升。

Conclusion: 该研究为未来的构造主义AI导师开发和评估提供了一个潜在框架，显示出通过合适的数据集和方法改善LLMs教育应用的有效性。

Abstract: In educational applications, LLMs exhibit several fundamental pedagogical limitations, such as their tendency to reveal solutions rather than support dialogic learning. We introduce ConvoLearn (https://huggingface.co/datasets/masharma/convolearn ), a dataset grounded in knowledge building theory that operationalizes six core pedagogical dimensions: cognitive engagement, formative assessment, accountability, cultural responsiveness, metacognition, and power dynamics. We construct a semi-synthetic dataset of 1250 tutor-student dialogues (20 turns each) in middle school Earth Science through controlled interactions between human teachers and a simulated student. Using QLoRA, we demonstrate that training on this dataset meaningfully shifts LLM behavior toward knowledge-building strategies. Human evaluation by 31 teachers shows our fine-tuned Mistral 7B (M = 4.10, SD = 1.03) significantly outperforms both its base version (M = 2.59, SD = 1.11) and Claude Sonnet 4.5 (M = 2.87, SD = 1.29) overall. This work establishes a potential framework to guide future development and evaluation of constructivist AI tutors.

</details>


### [134] [Programming over Thinking: Efficient and Robust Multi-Constraint Planning](https://arxiv.org/abs/2601.09097)
*Derrick Goh Xin Deik,Quanyu Long,Zhengyuan Liu,Nancy F. Chen,Wenya Wang*

Main category: cs.AI

TL;DR: SCOPE 提出了一种框架，能够从查询特定的推理中分离出通用代码执行，这一方法实现了在 TravelPlanner 任务上的最佳性能，同时大幅降低了推理成本和延迟。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在多约束规划领域存在局限性，纯推理方法容易出现一致性问题，而结合编程或求解器的方法缺乏灵活性。

Method: SCOPE 引入了一种分离推理与执行的框架，确保生成的求解函数一致、确定且可重用，只需少量输入参数更改。

Result: 使用 GPT-4o，SCOPE 在 TravelPlanner 任务中达到了 93.1% 的成功率，比最佳基线 CoT 高出 61.6%，同时减少推理成本 1.4 倍和时间约 4.67 倍。

Conclusion: 该研究通过分离查询特定的推理与通用代码执行，展现了 SCOPE 在多约束规划问题上的优越性能及经济效益。

Abstract: Multi-constraint planning involves identifying, evaluating, and refining candidate plans while satisfying multiple, potentially conflicting constraints. Existing large language model (LLM) approaches face fundamental limitations in this domain. Pure reasoning paradigms, which rely on long natural language chains, are prone to inconsistency, error accumulation, and prohibitive cost as constraints compound. Conversely, LLMs combined with coding- or solver-based strategies lack flexibility: they often generate problem-specific code from scratch or depend on fixed solvers, failing to capture generalizable logic across diverse problems. To address these challenges, we introduce the Scalable COde Planning Engine (SCOPE), a framework that disentangles query-specific reasoning from generic code execution. By separating reasoning from execution, SCOPE produces solver functions that are consistent, deterministic, and reusable across queries while requiring only minimal changes to input parameters. SCOPE achieves state-of-the-art performance while lowering cost and latency. For example, with GPT-4o, it reaches 93.1% success on TravelPlanner, a 61.6% gain over the best baseline (CoT) while cutting inference cost by 1.4x and time by ~4.67x. Code is available at https://github.com/DerrickGXD/SCOPE.

</details>


### [135] [DScheLLM: Enabling Dynamic Scheduling through a Fine-Tuned Dual-System Large language Model](https://arxiv.org/abs/2601.09100)
*Lixiang Zhang,Chenggong Zhao,Qing Gao,Xiaoke Zhao,Gengyi Bai,Jinhu Lv*

Main category: cs.AI

TL;DR: 该研究提出了一种名为DScheLLM的动态调度方法，利用细调的大语言模型在快慢系统推理架构中应对不同规模的干扰，展示了在动态环境下的潜在智能调度优化。


<details>
  <summary>Details</summary>
Motivation: 现有调度方法对动态干扰的适应性和泛化能力有限，常规方法依赖于事件特定的模型和精确分析公式。因此，本文探讨了如何通过细调大型语言模型，在快慢系统推理框架中应对动态环境中的不同规模干扰。

Method: 该方法构建了一个统一的大语言模型框架，利用来自运筹学求解器的精确调度数据训练快慢两种推理模式的训练集。华为OpenPangu嵌入式-7B模型在混合推理范式下进行微调。实验在标准车间调度基准上评估了快思模式高效生成高质量调度方案，以及慢思模式产生求解器兼容且格式良好的决策输入的能力。

Result: 实验结果表明，DScheLLM方法能够快速生成高质量的调度方案，并能在慢思模式下提供求解器兼容的良好格式决策输入。

Conclusion: 这项工作可能是最早使用大型语言模型进行动态车间调度研究之一，展示了在智能且适应性调度优化方面的巨大潜力。

Abstract: Production scheduling is highly susceptible to dynamic disruptions, such as variations in processing times, machine availability, and unexpected task insertions. Conventional approaches typically rely on event-specific models and explicit analytical formulations, which limits their adaptability and generalization across previously unseen disturbances. To overcome these limitations, this paper proposes DScheLLM, a dynamic scheduling approach that leverages fine-tuned large language models within a dual-system (fast-slow) reasoning architecture to address disturbances of different scales. A unified large language model-based framework is constructed to handle dynamic events, where training datasets for both fast and slow reasoning modes are generated using exact schedules obtained from an operations research solver. The Huawei OpenPangu Embedded-7B model is subsequently fine-tuned under the hybrid reasoning paradigms using LoRA. Experimental evaluations on standard job shop scheduling benchmarks demonstrate that the fast-thinking mode can efficiently generate high-quality schedules and the slow-thinking mode can produce solver-compatible and well-formatted decision inputs. To the best of our knowledge, this work represents one of the earliest studies applying large language models to job shop scheduling in dynamic environments, highlighting their considerable potential for intelligent and adaptive scheduling optimization.

</details>


### [136] [The AI Hippocampus: How Far are We From Human Memory?](https://arxiv.org/abs/2601.09113)
*Zixia Jia,Jiaqi Li,Yipeng Kang,Yuxuan Wang,Tong Wu,Quansen Wang,Xiaobo Wang,Shuyi Zhang,Junzhe Shen,Qing Li,Siyuan Qi,Yitao Liang,Di He,Zilong Zheng,Song-Chun Zhu*

Main category: cs.AI

TL;DR: 该论文综合分析了语言模型和多模态语言模型中的记忆机制，将其分为隐式、显式和自主记忆框架，并讨论了跨模态应用场景下的关键架构进展、基准任务以及开放挑战。


<details>
  <summary>Details</summary>
Motivation: 希望能提供一种系统性的视角来理解和评估已有研究在不同情境下的应用效果，从而促进这一领域的发展。

Method: 本文通过整理现有文献，构建了一种层次分明且分类清晰的税收框架，系统地总结了各种类型的记忆机制，并对其在语言模型和多模态语言模型中的应用进行了深入探讨。

Result: 论文总结出了三个主要的记忆框架，并分析了每一种记忆框架在语言模型和多模态语言模型中的应用，同时指出了建筑上的进展和存在的一些挑战。

Conclusion: 本文为未来的研究提供了一个有力的基础，有助于研究人员和从业人员更好地理解记忆机制在现代语言模型中的重要性以及它们如何推动模型向更强大的交互式学习系统进化。

Abstract: Memory plays a foundational role in augmenting the reasoning, adaptability, and contextual fidelity of modern Large Language Models and Multi-Modal LLMs. As these models transition from static predictors to interactive systems capable of continual learning and personalized inference, the incorporation of memory mechanisms has emerged as a central theme in their architectural and functional evolution. This survey presents a comprehensive and structured synthesis of memory in LLMs and MLLMs, organizing the literature into a cohesive taxonomy comprising implicit, explicit, and agentic memory paradigms. Specifically, the survey delineates three primary memory frameworks. Implicit memory refers to the knowledge embedded within the internal parameters of pre-trained transformers, encompassing their capacity for memorization, associative retrieval, and contextual reasoning. Recent work has explored methods to interpret, manipulate, and reconfigure this latent memory. Explicit memory involves external storage and retrieval components designed to augment model outputs with dynamic, queryable knowledge representations, such as textual corpora, dense vectors, and graph-based structures, thereby enabling scalable and updatable interaction with information sources. Agentic memory introduces persistent, temporally extended memory structures within autonomous agents, facilitating long-term planning, self-consistency, and collaborative behavior in multi-agent systems, with relevance to embodied and interactive AI. Extending beyond text, the survey examines the integration of memory within multi-modal settings, where coherence across vision, language, audio, and action modalities is essential. Key architectural advances, benchmark tasks, and open challenges are discussed, including issues related to memory capacity, alignment, factual consistency, and cross-system interoperability.

</details>


### [137] [Position on LLM-Assisted Peer Review: Addressing Reviewer Gap through Mentoring and Feedback](https://arxiv.org/abs/2601.09182)
*JungMin Yun,JuneHyoung Kwon,MiHyeon Kim,YoungBin Kim*

Main category: cs.AI

TL;DR: 本文批判了当前使用LLM自动生成评审的实践，提出了通过LLM辅助的引导系统和反馈系统来帮助评审员提升其长期能力和改进评审质量的方案，旨在增强评审员的专业技能，为建设更可持续的学术环境做出贡献。


<details>
  <summary>Details</summary>
Motivation: 由于AI研究的快速发展导致了评审员短缺的问题，本文关注于通过使用LLM工具来辅助和教育评审员，以提升评审质量并确保学术评价过程的可持续发展。

Method: 本文定义了高质量评审的核心原则，并提出了两种基于这些原则的互补系统：一种是LLM辅助的培养评审员长期能力的引导系统，另一种是帮助评审员完善其评审质量的LLM辅助反馈系统。

Result: 本文提出了一种以人为核心的方法，通过LLM辅助的系统来加强评审员的专业技能，并推动构建一个更可持续的学术生态系统。

Conclusion: 本文提出了具体的解决方案，旨在利用LLM技术改进评审流程，提升评审质量，促进学术研究的健康发展。

Abstract: The rapid expansion of AI research has intensified the Reviewer Gap, threatening the peer-review sustainability and perpetuating a cycle of low-quality evaluations. This position paper critiques existing LLM approaches that automatically generate reviews and argues for a paradigm shift that positions LLMs as tools for assisting and educating human reviewers. We define the core principles of high-quality peer review and propose two complementary systems grounded in these foundations: (i) an LLM-assisted mentoring system that cultivates reviewers' long-term competencies, and (ii) an LLM-assisted feedback system that helps reviewers refine the quality of their reviews. This human-centered approach aims to strengthen reviewer expertise and contribute to building a more sustainable scholarly ecosystem.

</details>


### [138] [Efficient Paths and Dense Rewards: Probabilistic Flow Reasoning for Large Language Models](https://arxiv.org/abs/2601.09260)
*Yan Liu,Feng Zhang,Zhanyu Ma,Jun Xu,Jiuchong Gao,Jinghua Hao,Renqing He,Han Liu,Yangdong Deng*

Main category: cs.AI

TL;DR: CoT-Flow 通过将离散推理步骤视为连续的概率流，量化每个步骤对正确答案的贡献，旨在提高大型语言模型的推理能力，通过流引导解码和基于流的强化学习实现高效推理和高性能。


<details>
  <summary>Details</summary>
Motivation: 当前范式在推理过程中缺乏对步骤间信息增益的量化机制，导致推理效率低下和优化困难。

Method: 提出 CoT-Flow 架构，将离散推理步骤视为连续的概率流，通过流引导解码和基于流的强化学习来量化每个步骤对正确答案的贡献。

Result: CoT-Flow 在多个具有挑战性的基准测试中展示出在推理效率和推理性能之间的优越平衡。

Conclusion: CoT-Flow 提供了一种新颖的方法来提高大型语言模型的推理能力，并且在实际应用中显示出良好的效果。

Abstract: High-quality chain-of-thought has demonstrated strong potential for unlocking the reasoning capabilities of large language models. However, current paradigms typically treat the reasoning process as an indivisible sequence, lacking an intrinsic mechanism to quantify step-wise information gain. This granularity gap manifests in two limitations: inference inefficiency from redundant exploration without explicit guidance, and optimization difficulty due to sparse outcome supervision or costly external verifiers. In this work, we propose CoT-Flow, a framework that reconceptualizes discrete reasoning steps as a continuous probabilistic flow, quantifying the contribution of each step toward the ground-truth answer. Built on this formulation, CoT-Flow enables two complementary methodologies: flow-guided decoding, which employs a greedy flow-based decoding strategy to extract information-efficient reasoning paths, and flow-based reinforcement learning, which constructs a verifier-free dense reward function. Experiments on challenging benchmarks demonstrate that CoT-Flow achieves a superior balance between inference efficiency and reasoning performance.

</details>


### [139] [Coordinated Pandemic Control with Large Language Model Agents as Policymaking Assistants](https://arxiv.org/abs/2601.09264)
*Ziyi Shi,Xusen Guo,Hongliang Lu,Mingxing Peng,Haotian Wang,Zheng Zhu,Zhenning Li,Yuxuan Liang,Xinhu Zheng,Hai Yang*

Main category: cs.AI

TL;DR: 本文提出了一个基于大语言模型（LLM）的多智能体决策框架，用于实现行政区域内下跨区域的协调与前瞻性的新冠疫情防控。该框架通过综合实际数据、病疫演变模拟器和智能体间结构化通信，支持智能体共同探索干预场景并形成协调的政策决定。


<details>
  <summary>Details</summary>
Motivation: 现有的疫情防控政策反应迟缓且碎片化，缺乏前瞻性和跨区域协调性。该框架旨在解决这一问题，通过人工智能辅助决策，提高新冠疫情等公共卫生危机应对的有效性。

Method: 作者构建了一个基于LLM的多智能体系统，每个行政区域被分配一个智能体，能够在考虑本地疫情动态的同时与其它智能体沟通，通过闭环模拟过程探索可能的干预措施并协同制定政策。

Result: 该框架在美国各州2020年4月至12月的COVID-19数据上进行了验证，结果显示，相较于实际结果，该框架能够将累计感染和死亡人数分别减少63.7%和40.1% (按州算), 并且在多州合并分析中这些数字分别为39.0%和27.0%。

Conclusion: 该研究证明了利用LLM来实现多智能体协同决策在新冠疫情等公共卫生事件中具有显著的协调与防控优势。

Abstract: Effective pandemic control requires timely and coordinated policymaking across administrative regions that are intrinsically interdependent. However, human-driven responses are often fragmented and reactive, with policies formulated in isolation and adjusted only after outbreaks escalate, undermining proactive intervention and global pandemic mitigation. To address this challenge, here we propose a large language model (LLM) multi-agent policymaking framework that supports coordinated and proactive pandemic control across regions. Within our framework, each administrative region is assigned an LLM agent as an AI policymaking assistant. The agent reasons over region-specific epidemiological dynamics while communicating with other agents to account for cross-regional interdependencies. By integrating real-world data, a pandemic evolution simulator, and structured inter-agent communication, our framework enables agents to jointly explore counterfactual intervention scenarios and synthesize coordinated policy decisions through a closed-loop simulation process. We validate the proposed framework using state-level COVID-19 data from the United States between April and December 2020, together with real-world mobility records and observed policy interventions. Compared with real-world pandemic outcomes, our approach reduces cumulative infections and deaths by up to 63.7% and 40.1%, respectively, at the individual state level, and by 39.0% and 27.0%, respectively, when aggregated across states. These results demonstrate that LLM multi-agent systems can enable more effective pandemic control with coordinated policymaking...

</details>


### [140] [RISER: Orchestrating Latent Reasoning Skills for Adaptive Activation Steering](https://arxiv.org/abs/2601.09269)
*Wencheng Ye,Liang Peng,Xiaoyang Yuan,Yi Bin,Pengpeng Zeng,Hengyu Jin,Heng Tao Shen*

Main category: cs.AI

TL;DR: RISER 提出了一种自适应的插件框架，通过轻量级路由器动态组合可重用的推理向量，提升大语言模型的零样本推理能力，展示出更高的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于参数更新以增强领域特定推理，这需要大量的训练，而 RISER 通过激活串联的认知原始能力，提供了一种参数效率更高的替代方案。

Method: RISER 使用一种基于路由的介入框架，它构建了一个可重用的推理向量库，并使用一个轻量级路由器根据输入动态组合这些向量。路由器通过强化学习在任务级奖励下优化，激活潜在的认知原语，以生成可解释和精确的控制策略。

Result: 在七个不同的基准测试中，RISER 的零样本推理准确度平均提高了 3.4%-6.5%，同时其Token效率比CoT风格的推理高2-3倍，并且展示了稳健的准确度增益。

Conclusion: RISER 通过对多种向量的自主组合，实现了更可控和高效的LLM推理，证明了该方法在扩大LSTM模型推理能力上的潜力。

Abstract: Recent work on domain-specific reasoning with large language models (LLMs) often relies on training-intensive approaches that require parameter updates. While activation steering has emerged as a parameter efficient alternative, existing methods apply static, manual interventions that fail to adapt to the dynamic nature of complex reasoning. To address this limitation, we propose RISER (Router-based Intervention for Steerable Enhancement of Reasoning), a plug-and-play intervention framework that adaptively steers LLM reasoning in activation space. RISER constructs a library of reusable reasoning vectors and employs a lightweight Router to dynamically compose them for each input. The Router is optimized via reinforcement learning under task-level rewards, activating latent cognitive primitives in an emergent and compositional manner. Across seven diverse benchmarks, RISER yields 3.4-6.5% average zero-shot accuracy improvements over the base model while surpassing CoT-style reasoning with 2-3x higher token efficiency and robust accuracy gains. Further analysis shows that RISER autonomously combines multiple vectors into interpretable, precise control strategies, pointing toward more controllable and efficient LLM reasoning.

</details>


### [141] [$A^3$-Bench: Benchmarking Memory-Driven Scientific Reasoning via Anchor and Attractor Activation](https://arxiv.org/abs/2601.09274)
*Jian Zhang,Yu He,Zhiyuan Wang,Zhangqi Wang,Kai He,Fangzhi Xu,Qika Lin,Jun Liu*

Main category: cs.AI

TL;DR: 该论文提出了A³-Bench基准，旨在通过基于锚点和吸引子的双重记忆机制来评估科学推理。该基准涵盖了从科学推理问题注释到实验验证的全过程。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试主要关注最终答案或步骤连贯性，忽略了支持人类推理的记忆驱动机制，如锚点和吸引子的激活。因此，该研究通过开发A³-Bench来弥补这一空白，以评估科学推理中的记忆驱动机制。

Method: 研究首先通过SAPM过程对2,198个跨学科的科学推理问题进行注释，然后提出了一种基于锚点和吸引子的双重记忆评估框架，并通过不同基模型和范式的实验验证了A³-Bench的有效性。

Result: 通过A³-Bench的实验，研究验证了不同基模型在记忆驱动科学推理中的表现，并通过AAUI指标评估了记忆激活率，从而分析了记忆激活对推理性能的影响。

Conclusion: A³-Bench提供了一个评估科学推理记忆驱动机制的新框架，可以帮助理解记忆如何影响人类科学推理能力。

Abstract: Scientific reasoning relies not only on logical inference but also on activating prior knowledge and experiential structures. Memory can efficiently reuse knowledge and enhance reasoning consistency and stability. However, existing benchmarks mainly evaluate final answers or step-by-step coherence, overlooking the \textit{memory-driven} mechanisms that underlie human reasoning, which involves activating anchors and attractors, then integrating them into multi-step inference. To address this gap, we propose $A^3$-Bench~ https://a3-bench.github.io, a benchmark designed to evaluate scientific reasoning through dual-scale memory-driven activation, grounded in Anchor and Attractor Activation. First, we annotate 2,198 science reasoning problems across domains using the SAPM process(subject, anchor & attractor, problem, and memory developing). Second, we introduce a dual-scale memory evaluation framework utilizing anchors and attractors, along with the AAUI(Anchor--Attractor Utilization Index) metric to measure memory activation rates. Finally, through experiments with various base models and paradigms, we validate $A^3$-Bench and analyze how memory activation impacts reasoning performance, providing insights into memory-driven scientific reasoning.

</details>


### [142] [STaR: Sensitive Trajectory Regulation for Unlearning in Large Reasoning Models](https://arxiv.org/abs/2601.09281)
*Jingjing Zhou,Gaoxiang Cong,Li Su,Liang Li*

Main category: cs.AI

TL;DR: STaR 是一种无需参数的推理时不可学习框架，能够通过语义感知检测敏感内容，安全提示前缀注入全局安全约束，轨迹感知抑制和标记级自适应过滤来动态阻止推理链中的敏感内容，最终通过多元解码一致性和多粒度成员推理攻击评估确保全面稳定的隐私保护。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法有效移除 LRMs 中从最终答案到中间步骤的所有敏感信息，导致持续的隐私泄露。STaR 提案旨在解决这一问题。

Method: STaR 提案通过以下步骤实现：首先，通过语义感知检测识别敏感内容；其次，通过安全提示前缀注入全局安全约束；再次，进行轨迹感知抑制，动态阻止敏感内容在整个推理链中传播；最后，应用标记级自适应过滤，防止生成精确性和同义性敏感标记。此外，引入了两种评估指标。

Result: 在 R-TOFU 基准上的实验表明，STaR 实现了最小的实用性损失下的全面和稳定不可学习，并设定了在 LRMs 中实现隐私保护的新标准。

Conclusion: STaR 提案提供了一种有效的解决方案，能够确保 LRMs 在整个推理过程中的隐私保护，并且评估方法确保了全面的保护效果。

Abstract: Large Reasoning Models (LRMs) have advanced automated multi-step reasoning, but their ability to generate complex Chain-of-Thought (CoT) trajectories introduces severe privacy risks, as sensitive information may be deeply embedded throughout the reasoning process. Existing Large Language Models (LLMs) unlearning approaches that typically focus on modifying only final answers are insufficient for LRMs, as they fail to remove sensitive content from intermediate steps, leading to persistent privacy leakage and degraded security. To address these challenges, we propose Sensitive Trajectory Regulation (STaR), a parameter-free, inference-time unlearning framework that achieves robust privacy protection throughout the reasoning process. Specifically, we first identify sensitive content via semantic-aware detection. Then, we inject global safety constraints through secure prompt prefix. Next, we perform trajectory-aware suppression to dynamically block sensitive content across the entire reasoning chain. Finally, we apply token-level adaptive filtering to prevent both exact and paraphrased sensitive tokens during generation. Furthermore, to overcome the inadequacies of existing evaluation protocols, we introduce two metrics: Multi-Decoding Consistency Assessment (MCS), which measures the consistency of unlearning across diverse decoding strategies, and Multi-Granularity Membership Inference Attack (MIA) Evaluation, which quantifies privacy protection at both answer and reasoning-chain levels. Experiments on the R-TOFU benchmark demonstrate that STaR achieves comprehensive and stable unlearning with minimal utility loss, setting a new standard for privacy-preserving reasoning in LRMs.

</details>


### [143] [Cluster Workload Allocation: Semantic Soft Affinity Using Natural Language Processing](https://arxiv.org/abs/2601.09282)
*Leszek Sliwko,Jolanta Mizeria-Pietraszko*

Main category: cs.AI

TL;DR: 该论文提出了一种使用自然语言处理的语义驱动调度范例，通过集成大型语言模型来实现集群系统的软亲和偏好配置，实验结果表明，该范例在多个场景中的调度质量优于标准Kubernetes配置。


<details>
  <summary>Details</summary>
Motivation: 在集群工作负载分配中，复杂的配置导致了易用性差距。论文通过引入基于自然语言处理的语义驱动调度范例，以简化用户配置过程，提升用户体验。

Method: 论文采用了通过Kubernetes调度器扩展集成大型语言模型的方法，解析用户提供的自然语言分配提示标记来设置软亲和偏好。通过开发原型并进行实证评价来验证模型的有效性和准确性。

Result: 实验结果显示，大型语言模型的解析准确率超过95%，优于基准引擎。在多个调度质量测试中，原型在多个场景中达到了与标准Kubernetes配置相似或更优的性能，特别是在复杂和定量的场景中表现出色。

Conclusion: 该研究确认了使用大型语言模型进行语义软亲和性配置的可行性，并指出异步处理可能是将其应用到生产环境的一个解决方案。

Abstract: Cluster workload allocation often requires complex configurations, creating a usability gap. This paper introduces a semantic, intent-driven scheduling paradigm for cluster systems using Natural Language Processing. The system employs a Large Language Model (LLM) integrated via a Kubernetes scheduler extender to interpret natural language allocation hint annotations for soft affinity preferences. A prototype featuring a cluster state cache and an intent analyzer (using AWS Bedrock) was developed. Empirical evaluation demonstrated high LLM parsing accuracy (>95% Subset Accuracy on an evaluation ground-truth dataset) for top-tier models like Amazon Nova Pro/Premier and Mistral Pixtral Large, significantly outperforming a baseline engine. Scheduling quality tests across six scenarios showed the prototype achieved superior or equivalent placement compared to standard Kubernetes configurations, particularly excelling in complex and quantitative scenarios and handling conflicting soft preferences. The results validate using LLMs for accessible scheduling but highlight limitations like synchronous LLM latency, suggesting asynchronous processing for production readiness. This work confirms the viability of semantic soft affinity for simplifying workload orchestration.

</details>


### [144] [Monte-Carlo Tree Search with Neural Network Guidance for Lane-Free Autonomous Driving](https://arxiv.org/abs/2601.09353)
*Ioannis Peridis,Dimitrios Troullinos,Georgios Chalkiadakis,Pantelis Giankoulidis,Ioannis Papamichail,Markos Papageorgiou*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Lane-free traffic environments allow vehicles to better harness the lateral capacity of the road without being restricted to lane-keeping, thereby increasing the traffic flow rates. As such, we have a distinct and more challenging setting for autonomous driving. In this work, we consider a Monte-Carlo Tree Search (MCTS) planning approach for single-agent autonomous driving in lane-free traffic, where the associated Markov Decision Process we formulate is influenced from existing approaches tied to reinforcement learning frameworks. In addition, MCTS is equipped with a pre-trained neural network (NN) that guides the selection phase. This procedure incorporates the predictive capabilities of NNs for a more informed tree search process under computational constraints. In our experimental evaluation, we consider metrics that address both safety (through collision rates) and efficacy (through measured speed). Then, we examine: (a) the influence of isotropic state information for vehicles in a lane-free environment, resulting in nudging behaviour--vehicles' policy reacts due to the presence of faster tailing ones, (b) the acceleration of performance for the NN-guided variant of MCTS, and (c) the trade-off between computational resources and solution quality.

</details>


### [145] [Long-term Task-oriented Agent: Proactive Long-term Intent Maintenance in Dynamic Environments](https://arxiv.org/abs/2601.09382)
*Qinglong Shi,Donghai Wang,Hantao Zhou,Jiguo Li,Jun Xu,Jiuchong Gao,Jinghua Hao,Renqing He*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Current large language model agents predominantly operate under a reactive paradigm, responding only to immediate user queries within short-term sessions. This limitation hinders their ability to maintain long-term user's intents and dynamically adapt to evolving external environments. In this paper, we propose a novel interaction paradigm for proactive Task-oriented Agents capable of bridging the gap between relatively static user's needs and a dynamic environment. We formalize proactivity through two key capabilities, (i) Intent-Conditioned Monitoring: The agent autonomously formulates trigger conditions based on dialog history; (ii) Event-Triggered Follow-up: The agent actively engages the user upon detecting useful environmental updates. We introduce a high-quality data synthesis pipeline to construct complex, multi-turn dialog data in a dynamic environment. Furthermore, we attempt to address the lack of evaluation criteria of task-oriented interaction in a dynamic environment by proposing a new benchmark, namely ChronosBench. We evaluated some leading close-source and open-source models at present and revealed their flaws in long-term task-oriented interaction. Furthermore, our fine-tuned model trained using synthetic data for supervised learning achieves a task completion rate of 85.19% for complex tasks including shifts in user intent, outperforming other models under test. And the result validated the effectiveness of our data-driven strategy.

</details>


### [146] [What Do LLM Agents Know About Their World? Task2Quiz: A Paradigm for Studying Environment Understanding](https://arxiv.org/abs/2601.09503)
*Siyuan Liu,Hongbang Yuan,Xinze Li,Ziyue Zhu,Yixin Cao,Yu-Gang Jiang*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large language model (LLM) agents have demonstrated remarkable capabilities in complex decision-making and tool-use tasks, yet their ability to generalize across varying environments remains a under-examined concern. Current evaluation paradigms predominantly rely on trajectory-based metrics that measure task success, while failing to assess whether agents possess a grounded, transferable model of the environment. To address this gap, we propose Task-to-Quiz (T2Q), a deterministic and automated evaluation paradigm designed to decouple task execution from world-state understanding. We instantiate this paradigm in T2QBench, a suite comprising 30 environments and 1,967 grounded QA pairs across multiple difficulty levels. Our extensive experiments reveal that task success is often a poor proxy for environment understanding, and that current memory machanism can not effectively help agents acquire a grounded model of the environment. These findings identify proactive exploration and fine-grained state representation as primary bottlenecks, offering a robust foundation for developing more generalizable autonomous agents.

</details>


### [147] [Omni-R1: Towards the Unified Generative Paradigm for Multimodal Reasoning](https://arxiv.org/abs/2601.09536)
*Dongjie Cheng,Yongqi Li,Zhixin Ma,Hongru Cai,Yupeng Hu,Wenjie Wang,Liqiang Nie,Wenjie Li*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Multimodal Large Language Models (MLLMs) are making significant progress in multimodal reasoning. Early approaches focus on pure text-based reasoning. More recent studies have incorporated multimodal information into the reasoning steps; however, they often follow a single task-specific reasoning pattern, which limits their generalizability across various multimodal tasks. In fact, there are numerous multimodal tasks requiring diverse reasoning skills, such as zooming in on a specific region or marking an object within an image. To address this, we propose unified generative multimodal reasoning, which unifies diverse multimodal reasoning skills by generating intermediate images during the reasoning process. We instantiate this paradigm with Omni-R1, a two-stage SFT+RL framework featuring perception alignment loss and perception reward, thereby enabling functional image generation. Additionally, we introduce Omni-R1-Zero, which eliminates the need for multimodal annotations by bootstrapping step-wise visualizations from text-only reasoning data. Empirical results show that Omni-R1 achieves unified generative reasoning across a wide range of multimodal tasks, and Omni-R1-Zero can match or even surpass Omni-R1 on average, suggesting a promising direction for generative multimodal reasoning.

</details>


### [148] [LLM for Large-Scale Optimization Model Auto-Formulation: A Lightweight Few-Shot Learning Approach](https://arxiv.org/abs/2601.09635)
*Kuo Liang,Yuhang Lu,Jianming Mao,Shuyi Sun,Chunwei Yang,Congcong Zeng,Xiao Jin,Hanzhang Qin,Ruihao Zhu,Chung-Piaw Teo*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large-scale optimization is a key backbone of modern business decision-making. However, building these models is often labor-intensive and time-consuming. We address this by proposing LEAN-LLM-OPT, a LightwEight AgeNtic workflow construction framework for LLM-assisted large-scale OPTimization auto-formulation. LEAN-LLM-OPT takes as input a problem description together with associated datasets and orchestrates a team of LLM agents to produce an optimization formulation. Specifically, upon receiving a query, two upstream LLM agents dynamically construct a workflow that specifies, step-by-step, how optimization models for similar problems can be formulated. A downstream LLM agent then follows this workflow to generate the final output. Leveraging LLMs' text-processing capabilities and common modeling practices, the workflow decomposes the modeling task into a sequence of structured sub-tasks and offloads mechanical data-handling operations to auxiliary tools. This design alleviates the downstream agent's burden related to planning and data handling, allowing it to focus on the most challenging components that cannot be readily standardized. Extensive simulations show that LEAN-LLM-OPT, instantiated with GPT-4.1 and the open source gpt-oss-20B, achieves strong performance on large-scale optimization modeling tasks and is competitive with state-of-the-art approaches. In addition, in a Singapore Airlines choice-based revenue management use case, LEAN-LLM-OPT demonstrates practical value by achieving leading performance across a range of scenarios. Along the way, we introduce Large-Scale-OR and Air-NRM, the first comprehensive benchmarks for large-scale optimization auto-formulation. The code and data of this work is available at https://github.com/CoraLiang01/lean-llm-opt.

</details>


### [149] [PersonalAlign: Hierarchical Implicit Intent Alignment for Personalized GUI Agent with Long-Term User-Centric Records](https://arxiv.org/abs/2601.09636)
*Yibo Lyu,Gongwei Chen,Rui Shao,Weili Guan,Liqiang Nie*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: While GUI agents have shown strong performance under explicit and completion instructions, real-world deployment requires aligning with users' more complex implicit intents. In this work, we highlight Hierarchical Implicit Intent Alignment for Personalized GUI Agent (PersonalAlign), a new agent task that requires agents to leverage long-term user records as persistent context to resolve omitted preferences in vague instructions and anticipate latent routines by user state for proactive assistance. To facilitate this study, we introduce AndroidIntent, a benchmark designed to evaluate agents' ability in resolving vague instructions and providing proactive suggestions through reasoning over long-term user records. We annotated 775 user-specific preferences and 215 routines from 20k long-term records across different users for evaluation. Furthermore, we introduce Hierarchical Intent Memory Agent (HIM-Agent), which maintains a continuously updating personal memory and hierarchically organizes user preferences and routines for personalization. Finally, we evaluate a range of GUI agents on AndroidIntent, including GPT-5, Qwen3-VL, and UI-TARS, further results show that HIM-Agent significantly improves both execution and proactive performance by 15.7% and 7.3%.

</details>


### [150] [Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning](https://arxiv.org/abs/2601.09667)
*Zhiyuan Hu,Yunhai Hu,Juncheng Liu,Shuyue Stella Li,Yucheng Wang,Zhen Xu,See-Kiong Ng,Anh Tuan Luu,Xinxing Xu,Bryan Hooi,Cynthia Breazeal,Hae Won Park*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Multi-agent systems have evolved into practical LLM-driven collaborators for many applications, gaining robustness from diversity and cross-checking. However, multi-agent RL (MARL) training is resource-intensive and unstable: co-adapting teammates induce non-stationarity, and rewards are often sparse and high-variance. Therefore, we introduce \textbf{Multi-Agent Test-Time Reinforcement Learning (MATTRL)}, a framework that injects structured textual experience into multi-agent deliberation at inference time. MATTRL forms a multi-expert team of specialists for multi-turn discussions, retrieves and integrates test-time experiences, and reaches consensus for final decision-making. We also study credit assignment for constructing a turn-level experience pool, then reinjecting it into the dialogue. Across challenging benchmarks in medicine, math, and education, MATTRL improves accuracy by an average of 3.67\% over a multi-agent baseline, and by 8.67\% over comparable single-agent baselines. Ablation studies examine different credit-assignment schemes and provide a detailed comparison of how they affect training outcomes. MATTRL offers a stable, effective and efficient path to distribution-shift-robust multi-agent reasoning without tuning.

</details>


### [151] [Automating Supply Chain Disruption Monitoring via an Agentic AI Approach](https://arxiv.org/abs/2601.09680)
*Sara AlMahri,Liming Xu,Alexandra Brintrup*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Modern supply chains are increasingly exposed to disruptions from geopolitical events, demand shocks, trade restrictions, to natural disasters. While many of these disruptions originate deep in the supply network, most companies still lack visibility beyond Tier-1 suppliers, leaving upstream vulnerabilities undetected until the impact cascades downstream. To overcome this blind-spot and move from reactive recovery to proactive resilience, we introduce a minimally supervised agentic AI framework that autonomously monitors, analyses, and responds to disruptions across extended supply networks. The architecture comprises seven specialised agents powered by large language models and deterministic tools that jointly detect disruption signals from unstructured news, map them to multi-tier supplier networks, evaluate exposure based on network structure, and recommend mitigations such as alternative sourcing options. \rev{We evaluate the framework across 30 synthesised scenarios covering three automotive manufacturers and five disruption classes. The system achieves high accuracy across core tasks, with F1 scores between 0.962 and 0.991, and performs full end-to-end analyses in a mean of 3.83 minutes at a cost of \$0.0836 per disruption. Relative to industry benchmarks of multi-day, analyst-driven assessments, this represents a reduction of more than three orders of magnitude in response time. A real-world case study of the 2022 Russia-Ukraine conflict further demonstrates operational applicability. This work establishes a foundational step toward building resilient, proactive, and autonomous supply chains capable of managing disruptions across deep-tier networks.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [152] [Annotated PIM Bibliography](https://arxiv.org/abs/2601.09002)
*Peter M. Kogge*

Main category: cs.AR

TL;DR: 本文试图提供一种对PIM技术的注解参考文献，旨在涵盖整个时间范围，并为即将发表的文章提供补充。


<details>
  <summary>Details</summary>
Motivation: 详细回顾PIM技术的历史和进展，澄清其概念，并指出技术实现的多样性，为未来的相关文章提供背景信息。

Method: 通过提供详细的文献分类和注解，对PIM技术进行全面回顾。

Result: 形成了一个涵盖PIM技术完整历史的时间线，澄清了相关技术概念，并提供了一个参考文献列表。

Conclusion: 本文强调了PIM技术的历史背景，并为相关领域的研究和讨论奠定了基础。

Abstract: Processing in Memory (PIM) and similar terms such as Compute In Memory (CIM), Logic in Memory (LIM), In Memory Computing (IMC), and Near Memory Computing (NMC) have gained attention recently as a potentially ``revolutionary new'' technique. The truth, however, is that many examples of the technology go back over 60 years. This document attempts to provide an annotated bibliography of PIM technology that attempts to cover the whole time-frame, and is organized to augment a forth-coming article.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [153] [Revisiting Disaggregated Large Language Model Serving for Performance and Energy Implications](https://arxiv.org/abs/2601.08833)
*Jiaxi Li,Yue Zhu,Eun Kyung Lee,Klara Nahrstedt*

Main category: cs.PF

TL;DR: 本文通过重新评估预填充-解码解耦合在不同KV缓存传输介质和优化策略下的表现，填补了现有研究中的空白。结果表明，预填充-解码解耦合的性能增益并非总是存在，且解耦合不必然带来能效节省。


<details>
  <summary>Details</summary>
Motivation: 现有的KV缓存传输路径和优化技术已经存在，但没有全面的系统性基准测试来比较它们的性能和能效。

Method: 通过引入新的共置服务基准，并评估在不同KV缓存传输路径下的解耦合布置，使用GPU调频电压频率调整（DVFS）进行GPU剖析，对比这些配置的性能与能效。

Result: 研究表明，预填充-解码解耦合的性能增益依赖于请求负载和KV传输介质；解耦合阶段的独立频率调整不会导致能效节约，因为解耦合自身就有较高的能耗。

Conclusion: 预填充和解码工作负载分离不总是提升性能，解耦合配置下的能效节省也不总是可实现的，需根据具体情况进行调整优化。

Abstract: Different from traditional Large Language Model (LLM) serving that colocates the prefill and decode stages on the same GPU, disaggregated serving dedicates distinct GPUs to prefill and decode workload. Once the prefill GPU completes its task, the KV cache must be transferred to the decode GPU. While existing works have proposed various KV cache transfer paths across different memory and storage tiers, there remains a lack of systematic benchmarking that compares their performance and energy efficiency. Meanwhile, although optimization techniques such as KV cache reuse and frequency scaling have been utilized for disaggregated serving, their performance and energy implications have not been rigorously benchmarked. In this paper, we fill this research gap by re-evaluating prefill-decode disaggregation under different KV transfer mediums and optimization strategies. Specifically, we include a new colocated serving baseline and evaluate disaggregated setups under different KV cache transfer paths. Through GPU profiling using dynamic voltage and frequency scaling (DVFS), we identify and compare the performance-energy Pareto frontiers across all setups to evaluate the potential energy savings enabled by disaggregation. Our results show that performance benefits from prefill-decode disaggregation are not guaranteed and depend on the request load and KV transfer mediums. In addition, stage-wise independent frequency scaling enabled by disaggregation does not lead to energy saving due to inherently higher energy consumption of disaggregated serving.

</details>


### [154] [LookAhead: The Optimal Non-decreasing Index Policy for a Time-Varying Holding Cost problem](https://arxiv.org/abs/2601.08960)
*Keerthana Gurushankar,Zhouzi Li,Mor Harchol-Balter,Alan Scheller-Wolf*

Main category: cs.PF

TL;DR: 本研究通过引入一种称为‘预览’的策略，为时变持有成本（TVHC）问题中的一种特殊案例提供了解最优解，该案例涉及两类M/M/1队列。


<details>
  <summary>Details</summary>
Motivation: 文章研究的动机在于解决时变持有成本问题在非渐近区间的优化问题，目前对此问题的优化结果知之甚少。因此旨在为这种特殊场景提供一种最优的调度策略。

Method: 通过引入一种称为‘预览’的策略，即在决定调度时考虑未来一定时间内的持有成本，从而提供了一种非递减的最优索引策略。

Result: 研究得出了最优的预览时间（lookahead amount），并证明了基于此预览时间的策略可以实现最优的总持有成本。

Conclusion: 文章的结论是，基于最优预览时间的策略能够精确地解决这种特定场景下的时变持有成本问题，并提供了该场景下的最优索引策略，即‘预览’策略（LookAhead）可以实现最优的总持有成本。

Abstract: In practice, the cost of delaying a job can grow as the job waits. Such behavior is modeled by the Time-Varying Holding Cost (TVHC) problem, where each job's instantaneous holding cost increases with its current age (a job's age is the time since it arrived). The goal of the TVHC problem is to find a scheduling policy that minimizes the time-average total holding cost across all jobs.
  However, no optimality results are known for the TVHC problem outside of the asymptotic regime. In this paper, we study a simple yet still challenging special case: A two-class M/M/1 queue in which class 1 jobs incur a non-decreasing, time-varying holding cost and class 2 jobs incur a constant holding cost.
  Our main contribution is deriving the first optimal (non-decreasing) index policy for this special case of the TVHC problem. Our optimal policy, called LookAhead, stems from the following idea: Rather than considering each job's current holding cost when making scheduling decisions, we should look at their cost some $X$ time into the future, where this $X$ is intuitively called the ``lookahead amount." This paper derives that optimal lookahead amount.

</details>
