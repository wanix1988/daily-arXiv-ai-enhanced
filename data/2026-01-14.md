<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 83]
- [cs.CL](#cs.CL) [Total: 59]
- [cs.AI](#cs.AI) [Total: 51]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.PF](#cs.PF) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Edge-AI Perception Node for Cooperative Road-Safety Enforcement and Connected-Vehicle Integration](https://arxiv.org/abs/2601.07845)
*Shree Charran R,Rahul Kumar Dubey*

Main category: cs.CV

TL;DR: 论文提出了一个实时路侧感知节点，用于多类交通违规分析和安全事件传播，利用YOLOv8 Nano、DeepSORT和OCR引擎，实现了高效准确的违规检测和车牌识别，部署在NVIDIA Jetson Nano上，性能优越。


<details>
  <summary>Details</summary>
Motivation: 印度快速机械化造成了执法不对称，传统方法无法应对大量违规记录，因此亟需一种自主的、协作的、节能的边缘AI感知基础设施。

Method: 节点采用YOLOv8 Nano进行高精度多对象检测，使用DeepSORT进行车辆时间一致跟踪，配备规则指导的OCR后处理引擎，识别合规的车牌。

Result: 系统使用NVIDIA Jetson Nano部署，通过TensorRT FP16量化，实现每秒28-30帧的推理，功耗9.6瓦。在五个违规类别上的违规检测准确率为97.7%，OCR精度为84.9%。与YOLOv4 Tiny、PP YOLOE S和Nano DetPlus相比，平均精度提高了10.7%，准确度功耗提高了一倍。

Conclusion: 该研究展示了边缘AI分析在智能车辆生态系统中的应用潜力，可以增强协作感知和预测性道路安全管理。

Abstract: Rapid motorization in emerging economies such as India has created severe enforcement asymmetries, with over 11 million recorded violations in 2023 against a human policing density of roughly one officer per 4000 vehicles. Traditional surveillance and manual ticketing cannot scale to this magnitude, motivating the need for an autonomous, cooperative, and energy efficient edge AI perception infrastructure. This paper presents a real time roadside perception node for multi class traffic violation analytics and safety event dissemination within a connected and intelligent vehicle ecosystem. The node integrates YOLOv8 Nano for high accuracy multi object detection, DeepSORT for temporally consistent vehicle tracking, and a rule guided OCR post processing engine capable of recognizing degraded or multilingual license plates compliant with MoRTH AIS 159 and ISO 7591 visual contrast standards. Deployed on an NVIDIA Jetson Nano with a 128 core Maxwell GPU and optimized via TensorRT FP16 quantization, the system sustains 28 to 30 frames per second inference at 9.6 W, achieving 97.7 percent violation detection accuracy and 84.9 percent OCR precision across five violation classes, namely signal jumping, zebra crossing breach, wrong way driving, illegal U turn, and speeding, without manual region of interest calibration. Comparative benchmarking against YOLOv4 Tiny, PP YOLOE S, and Nano DetPlus demonstrates a 10.7 percent mean average precision gain and a 1.4 times accuracy per watt improvement. Beyond enforcement, the node publishes standardized safety events of CAM and DENM type to connected vehicles and intelligent transportation system backends via V2X protocols, demonstrating that roadside edge AI analytics can augment cooperative perception and proactive road safety management within the IEEE Intelligent Vehicles ecosystem.

</details>


### [2] [Moonworks Lunara Aesthetic Dataset](https://arxiv.org/abs/2601.07941)
*Yan Wang,M M Sayeef Abdullah,Partho Hassan,Sabit Hassan*

Main category: cs.CV

TL;DR: 介绍了一个包含多样艺术风格的数据集，如中东、北欧、东亚和南亚的地域美学，以及素描和油画等通用类别。数据集中的所有图片都是使用Moonworks Lunara模型生成的，旨在体现各自独特的高质量美学风格，比现有美学数据集和通用数据集质量更高。此外，每个图像配有由人类精炼的提示和结构化的注释，数据集还注重美学质量、风格多样性及许可透明度，并采用Apache 2.0许可证发布。


<details>
  <summary>Details</summary>
Motivation: 构建一个高质量且结构化的艺术与美学数据集，超过现有美学和通用数据集，旨在为研究和学术领域提供广泛的支持，同时确保版权透明度。

Method: 利用Moonworks Lunara模型生成图片，并对其进行人类精炼处理，确保具有高质量和多样化风格。每个图像包含详细的结构化注释，数据集整体以一致性美学和风格为设计准则。

Result: 创建了一个独特的数据集，其中的图像在美学质量、风格多样性和许可透明度方面表现优异，与现有数据集相比有显著提升。

Conclusion: Lunara Aesthetic Dataset 以独特的艺术风格和高质量标准在美学研究领域内提供了新的可能性，适用于学术和商业用途，支持开放研究与共享。

Abstract: The dataset spans diverse artistic styles, including regionally grounded aesthetics from the Middle East, Northern Europe, East Asia, and South Asia, alongside general categories such as sketch and oil painting. All images are generated using the Moonworks Lunara model and intentionally crafted to embody distinct, high-quality aesthetic styles, yielding a first-of-its-kind dataset with substantially higher aesthetic scores, exceeding even aesthetics-focused datasets, and general-purpose datasets by a larger margin. Each image is accompanied by a human-refined prompt and structured annotations that jointly describe salient objects, attributes, relationships, and stylistic cues. Unlike large-scale web-derived datasets that emphasize breadth over precision, the Lunara Aesthetic Dataset prioritizes aesthetic quality, stylistic diversity, and licensing transparency, and is released under the Apache 2.0 license to support research and unrestricted academic and commercial use.

</details>


### [3] [3DGS-Drag: Dragging Gaussians for Intuitive Point-Based 3D Editing](https://arxiv.org/abs/2601.07963)
*Jiahua Dong,Yu-Xiong Wang*

Main category: cs.CV

TL;DR: 3DGS-Drag 提出了一种基于点的 3D 编辑框架，通过 3D 高斯散点图和扩散指导，实现了高效且直观的 3D 场景编辑，适用于运动变化、形状调整、内容修复和扩展等多种场景。


<details>
  <summary>Details</summary>
Motivation: 由于 2D 编辑难以应用于 3D 场景，现有的 3D 编辑方法因其几何修改不一致、内容校正不足等问题受到限制。3DGS-Drag 框架旨在解决这些局限性，提供一种能够直观地编辑真实 3D 场景的方法。

Method: 3DGS-Drag 框架采用了基于点的方法，结合 3D 高斯散点图和扩散引导，实现了几何变形指导和内容改进。此外，还提出了一种渐进编辑策略以支持激进的 3D 拖动编辑。

Result: 该方法在各种 3D 场景中表现出色，达到了几何相关的 3D 内容编辑的最新技术水平。实验证明编辑过程效率高，单个 RTX 4090 GPU 上耗时仅需 10 至 20 分钟。

Conclusion: 3DGS-Drag 填补了 3D 场景编辑中的关键空白，提供了一种高效且直观的方式来修改真实 3D 场景中的各种内容。

Abstract: The transformative potential of 3D content creation has been progressively unlocked through advancements in generative models. Recently, intuitive drag editing with geometric changes has attracted significant attention in 2D editing yet remains challenging for 3D scenes. In this paper, we introduce 3DGS-Drag -- a point-based 3D editing framework that provides efficient, intuitive drag manipulation of real 3D scenes. Our approach bridges the gap between deformation-based and 2D-editing-based 3D editing methods, addressing their limitations to geometry-related content editing. We leverage two key innovations: deformation guidance utilizing 3D Gaussian Splatting for consistent geometric modifications and diffusion guidance for content correction and visual quality enhancement. A progressive editing strategy further supports aggressive 3D drag edits. Our method enables a wide range of edits, including motion change, shape adjustment, inpainting, and content extension. Experimental results demonstrate the effectiveness of 3DGS-Drag in various scenes, achieving state-of-the-art performance in geometry-related 3D content editing. Notably, the editing is efficient, taking 10 to 20 minutes on a single RTX 4090 GPU.

</details>


### [4] [Sesame Plant Segmentation Dataset: A YOLO Formatted Annotated Dataset](https://arxiv.org/abs/2601.07970)
*Sunusi Ibrahim Muhammad,Ismail Ismail Tijjani,Saadatu Yusuf Jumare,Fatima Isah Jibrin*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This paper presents the Sesame Plant Segmentation Dataset, an open source annotated image dataset designed to support the development of artificial intelligence models for agricultural applications, with a specific focus on sesame plants. The dataset comprises 206 training images, 43 validation images, and 43 test images in YOLO compatible segmentation format, capturing sesame plants at early growth stages under varying environmental conditions. Data were collected using a high resolution mobile camera from farms in Jirdede, Daura Local Government Area, Katsina State, Nigeria, and annotated using the Segment Anything Model version 2 with farmer supervision. Unlike conventional bounding box datasets, this dataset employs pixel level segmentation to enable more precise detection and analysis of sesame plants in real world farm settings. Model evaluation using the Ultralytics YOLOv8 framework demonstrated strong performance for both detection and segmentation tasks. For bounding box detection, the model achieved a recall of 79 percent, precision of 79 percent, mean average precision at IoU 0.50 of 84 percent, and mean average precision from 0.50 to 0.95 of 58 percent. For segmentation, it achieved a recall of 82 percent, precision of 77 percent, mean average precision at IoU 0.50 of 84 percent, and mean average precision from 0.50 to 0.95 of 52 percent. The dataset represents a novel contribution to sesame focused agricultural vision datasets in Nigeria and supports applications such as plant monitoring, yield estimation, and agricultural research.

</details>


### [5] [An Efficient Additive Kolmogorov-Arnold Transformer for Point-Level Maize Localization in Unmanned Aerial Vehicle Imagery](https://arxiv.org/abs/2601.07975)
*Fei Li,Lang Qiao,Jiahao Fan,Yijia Xu,Shawn M. Kaeppler,Zhou Zhang*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: High-resolution UAV photogrammetry has become a key technology for precision agriculture, enabling centimeter-level crop monitoring and point-level plant localization. However, point-level maize localization in UAV imagery remains challenging due to (1) extremely small object-to-pixel ratios, typically less than 0.1%, (2) prohibitive computational costs of quadratic attention on ultra-high-resolution images larger than 3000 x 4000 pixels, and (3) agricultural scene-specific complexities such as sparse object distribution and environmental variability that are poorly handled by general-purpose vision models.
  To address these challenges, we propose the Additive Kolmogorov-Arnold Transformer (AKT), which replaces conventional multilayer perceptrons with Pade Kolmogorov-Arnold Network (PKAN) modules to enhance functional expressivity for small-object feature extraction, and introduces PKAN Additive Attention (PAA) to model multiscale spatial dependencies with reduced computational complexity. In addition, we present the Point-based Maize Localization (PML) dataset, consisting of 1,928 high-resolution UAV images with approximately 501,000 point annotations collected under real field conditions.
  Extensive experiments show that AKT achieves an average F1-score of 62.8%, outperforming state-of-the-art methods by 4.2%, while reducing FLOPs by 12.6% and improving inference throughput by 20.7%. For downstream tasks, AKT attains a mean absolute error of 7.1 in stand counting and a root mean square error of 1.95-1.97 cm in interplant spacing estimation. These results demonstrate that integrating Kolmogorov-Arnold representation theory with efficient attention mechanisms offers an effective framework for high-resolution agricultural remote sensing.

</details>


### [6] [Likelihood ratio for a binary Bayesian classifier under a noise-exclusion model](https://arxiv.org/abs/2601.07982)
*Howard C. Gifford*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We develop a new statistical ideal observer model that performs holistic visual search (or gist) processing in part by placing thresholds on minimum extractable image features. In this model, the ideal observer reduces the number of free parameters thereby shrinking down the system. The applications of this novel framework is in medical image perception (for optimizing imaging systems and algorithms), computer vision, benchmarking performance and enabling feature selection/evaluations. Other applications are in target detection and recognition in defense/security as well as evaluating sensors and detectors.

</details>


### [7] [Predicting Region of Interest in Human Visual Search Based on Statistical Texture and Gabor Features](https://arxiv.org/abs/2601.07998)
*Hongwei Lin,Diego Andrade,Mini Das,Howard C. Gifford*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Understanding human visual search behavior is a fundamental problem in vision science and computer vision, with direct implications for modeling how observers allocate attention in location-unknown search tasks. In this study, we investigate the relationship between Gabor-based features and gray-level co-occurrence matrix (GLCM) based texture features in modeling early-stage visual search behavior. Two feature-combination pipelines are proposed to integrate Gabor and GLCM features for narrowing the region of possible human fixations. The pipelines are evaluated using simulated digital breast tomosynthesis images. Results show qualitative agreement among fixation candidates predicted by the proposed pipelines and a threshold-based model observer. A strong correlation is observed between GLCM mean and Gabor feature responses, indicating that these features encode related image information despite their different formulations. Eye-tracking data from human observers further suggest consistency between predicted fixation regions and early-stage gaze behavior. These findings highlight the value of combining structural and texture-based features for modeling visual search and support the development of perceptually informed observer models.

</details>


### [8] [CASHEW: Stabilizing Multimodal Reasoning via Iterative Trajectory Aggregation](https://arxiv.org/abs/2601.08010)
*Chaoyu Li,Deeparghya Dutta Barua,Fei Tao,Pooyan Fazli*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Vision-language models achieve strong performance across a wide range of multimodal understanding and reasoning tasks, yet their multi-step reasoning remains unstable. Repeated sampling over the same input often produces divergent reasoning trajectories and inconsistent final predictions. To address this, we introduce two complementary approaches inspired by test-time scaling: (1) CASHEW, an inference-time framework that stabilizes reasoning by iteratively aggregating multiple candidate trajectories into higher-quality reasoning traces, with explicit visual verification filtering hallucinated steps and grounding reasoning in visual evidence, and (2) CASHEW-RL, a learned variant that internalizes this aggregation behavior within a single model. CASHEW-RL is trained using Group Sequence Policy Optimization (GSPO) with a composite reward that encourages correct answers grounded in minimal yet sufficient visual evidence, while adaptively allocating reasoning effort based on task difficulty. This training objective enables robust self-aggregation at inference. Extensive experiments on 13 image understanding, video understanding, and video reasoning benchmarks show significant performance improvements, including gains of up to +23.6 percentage points on ScienceQA and +8.1 percentage points on EgoSchema.

</details>


### [9] [TP-Blend: Textual-Prompt Attention Pairing for Precise Object-Style Blending in Diffusion Models](https://arxiv.org/abs/2601.08011)
*Xin Jin,Yichuan Zhong,Yapeng Tian*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Current text-conditioned diffusion editors handle single object replacement well but struggle when a new object and a new style must be introduced simultaneously. We present Twin-Prompt Attention Blend (TP-Blend), a lightweight training-free framework that receives two separate textual prompts, one specifying a blend object and the other defining a target style, and injects both into a single denoising trajectory. TP-Blend is driven by two complementary attention processors. Cross-Attention Object Fusion (CAOF) first averages head-wise attention to locate spatial tokens that respond strongly to either prompt, then solves an entropy-regularised optimal transport problem that reassigns complete multi-head feature vectors to those positions. CAOF updates feature vectors at the full combined dimensionality of all heads (e.g., 640 dimensions in SD-XL), preserving rich cross-head correlations while keeping memory low. Self-Attention Style Fusion (SASF) injects style at every self-attention layer through Detail-Sensitive Instance Normalization. A lightweight one-dimensional Gaussian filter separates low- and high-frequency components; only the high-frequency residual is blended back, imprinting brush-stroke-level texture without disrupting global geometry. SASF further swaps the Key and Value matrices with those derived from the style prompt, enforcing context-aware texture modulation that remains independent of object fusion. Extensive experiments show that TP-Blend produces high-resolution, photo-realistic edits with precise control over both content and appearance, surpassing recent baselines in quantitative fidelity, perceptual quality, and inference speed.

</details>


### [10] [Decoder Generates Manufacturable Structures: A Framework for 3D-Printable Object Synthesis](https://arxiv.org/abs/2601.08015)
*Abhishek Kumar*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This paper presents a novel decoder-based approach for generating manufacturable 3D structures optimized for additive manufacturing. We introduce a deep learning framework that decodes latent representations into geometrically valid, printable objects while respecting manufacturing constraints such as overhang angles, wall thickness, and structural integrity. The methodology demonstrates that neural decoders can learn complex mapping functions from abstract representations to valid 3D geometries, producing parts with significantly improved manufacturability compared to naive generation approaches. We validate the approach on diverse object categories and demonstrate practical 3D printing of decoder-generated structures.

</details>


### [11] [Representations of Text and Images Align From Layer One](https://arxiv.org/abs/2601.08017)
*Evžen Wybitul,Javier Rando,Florian Tramèr,Stanislav Fort*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We show that for a variety of concepts in adapter-based vision-language models, the representations of their images and their text descriptions are meaningfully aligned from the very first layer. This contradicts the established view that such image-text alignment only appears in late layers. We show this using a new synthesis-based method inspired by DeepDream: given a textual concept such as "Jupiter", we extract its concept vector at a given layer, and then use optimisation to synthesise an image whose representation aligns with that vector. We apply our approach to hundreds of concepts across seven layers in Gemma 3, and find that the synthesised images often depict salient visual features of the targeted textual concepts: for example, already at layer 1, more than 50 % of images depict recognisable features of animals, activities, or seasons. Our method thus provides direct, constructive evidence of image-text alignment on a concept-by-concept and layer-by-layer basis. Unlike previous methods for measuring multimodal alignment, our approach is simple, fast, and does not require auxiliary models or datasets. It also offers a new path towards model interpretability, by providing a way to visualise a model's representation space by backtracing through its image processing components.

</details>


### [12] [Training Free Zero-Shot Visual Anomaly Localization via Diffusion Inversion](https://arxiv.org/abs/2601.08022)
*Samet Hicsonmez,Abd El Rahman Shabayek,Djamila Aouada*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Zero-Shot image Anomaly Detection (ZSAD) aims to detect and localise anomalies without access to any normal training samples of the target data. While recent ZSAD approaches leverage additional modalities such as language to generate fine-grained prompts for localisation, vision-only methods remain limited to image-level classification, lacking spatial precision. In this work, we introduce a simple yet effective training-free vision-only ZSAD framework that circumvents the need for fine-grained prompts by leveraging the inversion of a pretrained Denoising Diffusion Implicit Model (DDIM). Specifically, given an input image and a generic text description (e.g., "an image of an [object class]"), we invert the image to obtain latent representations and initiate the denoising process from a fixed intermediate timestep to reconstruct the image. Since the underlying diffusion model is trained solely on normal data, this process yields a normal-looking reconstruction. The discrepancy between the input image and the reconstructed one highlights potential anomalies. Our method achieves state-of-the-art performance on VISA dataset, demonstrating strong localisation capabilities without auxiliary modalities and facilitating a shift away from prompt dependence for zero-shot anomaly detection research. Code is available at https://github.com/giddyyupp/DIVAD.

</details>


### [13] [A Highly Efficient Diversity-based Input Selection for DNN Improvement Using VLMs](https://arxiv.org/abs/2601.08024)
*Amin Abbasishahkoo,Mahboubeh Dadkhah,Lionel Briand*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Maintaining or improving the performance of Deep Neural Networks (DNNs) through fine-tuning requires labeling newly collected inputs, a process that is often costly and time-consuming. To alleviate this problem, input selection approaches have been developed in recent years to identify small, yet highly informative subsets for labeling. Diversity-based selection is one of the most effective approaches for this purpose. However, they are often computationally intensive and lack scalability for large input sets, limiting their practical applicability. To address this challenge, we introduce Concept-Based Diversity (CBD), a highly efficient metric for image inputs that leverages Vision-Language Models (VLM). Our results show that CBD exhibits a strong correlation with Geometric Diversity (GD), an established diversity metric, while requiring only a fraction of its computation time. Building on this finding, we propose a hybrid input selection approach that combines CBD with Margin, a simple uncertainty metric. We conduct a comprehensive evaluation across a diverse set of DNN models, input sets, selection budgets, and five most effective state-of-the-art selection baselines. The results demonstrate that the CBD-based selection consistently outperforms all baselines at guiding input selection to improve the DNN model. Furthermore, the CBD-based selection approach remains highly efficient, requiring selection times close to those of simple uncertainty-based methods such as Margin, even on larger input sets like ImageNet. These results confirm not only the effectiveness and computational advantage of the CBD-based approach, particularly compared to hybrid baselines, but also its scalability in repetitive and extensive input selection scenarios.

</details>


### [14] [FigEx2: Visual-Conditioned Panel Detection and Captioning for Scientific Compound Figures](https://arxiv.org/abs/2601.08026)
*Jifeng Song,Arun Das,Pan Wang,Hui Ji,Kun Zhao,Yufei Huang*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Scientific compound figures combine multiple labeled panels into a single image, but captions in real pipelines are often missing or only provide figure-level summaries, making panel-level understanding difficult. In this paper, we propose FigEx2, visual-conditioned framework that localizes panels and generates panel-wise captions directly from the compound figure. To mitigate the impact of diverse phrasing in open-ended captioning, we introduce a noise-aware gated fusion module that adaptively filters token-level features to stabilize the detection query space. Furthermore, we employ a staged optimization strategy combining supervised learning with reinforcement learning (RL), utilizing CLIP-based alignment and BERTScore-based semantic rewards to enforce strict multimodal consistency. To support high-quality supervision, we curate BioSci-Fig-Cap, a refined benchmark for panel-level grounding, alongside cross-disciplinary test suites in physics and chemistry. Experimental results demonstrate that FigEx2 achieves a superior 0.726 mAP@0.5:0.95 for detection and significantly outperforms Qwen3-VL-8B by 0.51 in METEOR and 0.24 in BERTScore. Notably, FigEx2 exhibits remarkable zero-shot transferability to out-of-distribution scientific domains without any fine-tuning.

</details>


### [15] [Rescind: Countering Image Misconduct in Biomedical Publications with Vision-Language and State-Space Modeling](https://arxiv.org/abs/2601.08040)
*Soumyaroop Nandi,Prem Natarajan*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Scientific image manipulation in biomedical publications poses a growing threat to research integrity and reproducibility. Unlike natural image forensics, biomedical forgery detection is uniquely challenging due to domain-specific artifacts, complex textures, and unstructured figure layouts. We present the first vision-language guided framework for both generating and detecting biomedical image forgeries. By combining diffusion-based synthesis with vision-language prompting, our method enables realistic and semantically controlled manipulations, including duplication, splicing, and region removal, across diverse biomedical modalities. We introduce Rescind, a large-scale benchmark featuring fine-grained annotations and modality-specific splits, and propose Integscan, a structured state space modeling framework that integrates attention-enhanced visual encoding with prompt-conditioned semantic alignment for precise forgery localization. To ensure semantic fidelity, we incorporate a vision-language model based verification loop that filters generated forgeries based on consistency with intended prompts. Extensive experiments on Rescind and existing benchmarks demonstrate that Integscan achieves state of the art performance in both detection and localization, establishing a strong foundation for automated scientific integrity analysis.

</details>


### [16] [The Role of Noisy Data in Improving CNN Robustness for Image Classification](https://arxiv.org/abs/2601.08043)
*Oscar H. Ramírez-Agudelo,Nicoleta Gorea,Aliza Reif,Lorenzo Bonasera,Michael Karl*

Main category: cs.CV

TL;DR: 通过在训练数据中引入可控噪声，可以显著提高模型在受污染测试条件下的准确性和鲁棒性，且对干净数据的性能影响较小。


<details>
  <summary>Details</summary>
Motivation: 由于现实世界输入通常受到噪声和其他失真的影响，而高质量的数据在训练中虽受欢迎，但在实际应用中却无法保证，因此研究提出通过在训练数据中适当引入噪声以提高模型的鲁棒性。

Method: 该研究使用CIFAR-10数据集，对三种常见的干扰（高斯噪声、椒盐噪声和高斯模糊）进行不同程度的污染，并在Resnet-18模型上进行实验，评估不同污染水平和噪声强度下的模型性能。

Result: 实验结果显示，仅在训练集中加入10%的有噪声数据就足以显著降低测试损失并提高在完全受污染条件下的准确度，而对干净数据的性能影响很小。

Conclusion: 研究表明，战略性地将噪声暴露给模型作为一种简单的正则化方法，能提供传统数据清洁与现实世界鲁棒性之间的实用权衡。

Abstract: Data quality plays a central role in the performance and robustness of convolutional neural networks (CNNs) for image classification. While high-quality data is often preferred for training, real-world inputs are frequently affected by noise and other distortions. This paper investigates the effect of deliberately introducing controlled noise into the training data to improve model robustness. Using the CIFAR-10 dataset, we evaluate the impact of three common corruptions, namely Gaussian noise, Salt-and-Pepper noise, and Gaussian blur at varying intensities and training set pollution levels. Experiments using a Resnet-18 model reveal that incorporating just 10\% noisy data during training is sufficient to significantly reduce test loss and enhance accuracy under fully corrupted test conditions, with minimal impact on clean-data performance. These findings suggest that strategic exposure to noise can act as a simple yet effective regularizer, offering a practical trade-off between traditional data cleanliness and real-world resilience.

</details>


### [17] [Exploiting DINOv3-Based Self-Supervised Features for Robust Few-Shot Medical Image Segmentation](https://arxiv.org/abs/2601.08078)
*Guoping Xu,Jayaram K. Udupa,Weiguo Lu,You Zhang*

Main category: cs.CV

TL;DR: DINO-AugSeg 是一种利用 DINOv3 特征解决医学图像分割少样本挑战的新框架，通过 WT-Aug 和 CG-Fuse 模块在波动域增强特性和上下文融合语义丰富的低分辨率与细节丰富的高分辨率特征，实验证明该方法在多个医学成像模态上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前自动医学图像分割在少样本场景中仍面临严峻挑战，尽管自监督基础模型 DINOv3 在密集特征提取上表现出色，但直接应用于医学图像受限于领域差异。DINO-AugSeg 通过结合波动域特征增强和上下文融合模块，有效解决了这一挑战。

Method: DINO-AugSeg 引入了 WT-Aug 波动域特征增强模块，通过扰动频率成分增加 DINOv3 特征多样性；CG-Fuse 上下文信息引导融合模块，利用跨注意力机制将语义丰富的低分辨率特征与空间详细高分辨率特征进行整合。

Result: DINO-AugSeg 在包括 MRI、CT、超声、内窥镜和皮肤镜在内的六个公开基准数据集上进行了广泛的实验，结果显示该方法在少样本条件下表现优于现有方法。

Conclusion: 利用波动域增强与上下文融合模块，DINO-AugSeg 提供了稳健的特征表示，为少样本医学图像分割的进一步研究提供了新的思路。

Abstract: Deep learning-based automatic medical image segmentation plays a critical role in clinical diagnosis and treatment planning but remains challenging in few-shot scenarios due to the scarcity of annotated training data. Recently, self-supervised foundation models such as DINOv3, which were trained on large natural image datasets, have shown strong potential for dense feature extraction that can help with the few-shot learning challenge. Yet, their direct application to medical images is hindered by domain differences. In this work, we propose DINO-AugSeg, a novel framework that leverages DINOv3 features to address the few-shot medical image segmentation challenge. Specifically, we introduce WT-Aug, a wavelet-based feature-level augmentation module that enriches the diversity of DINOv3-extracted features by perturbing frequency components, and CG-Fuse, a contextual information-guided fusion module that exploits cross-attention to integrate semantic-rich low-resolution features with spatially detailed high-resolution features. Extensive experiments on six public benchmarks spanning five imaging modalities, including MRI, CT, ultrasound, endoscopy, and dermoscopy, demonstrate that DINO-AugSeg consistently outperforms existing methods under limited-sample conditions. The results highlight the effectiveness of incorporating wavelet-domain augmentation and contextual fusion for robust feature representation, suggesting DINO-AugSeg as a promising direction for advancing few-shot medical image segmentation. Code and data will be made available on https://github.com/apple1986/DINO-AugSeg.

</details>


### [18] [From Prompts to Deployment: Auto-Curated Domain-Specific Dataset Generation via Diffusion Models](https://arxiv.org/abs/2601.08095)
*Dongsik Yoon,Jongeun Kim*

Main category: cs.CV

TL;DR: 该论文提出了一种自动化管道，利用扩散模型生成用于特定领域的合成数据集，通过多模态评估和用户偏好分类器优化生成质量。


<details>
  <summary>Details</summary>
Motivation: 解决预训练模型与实际部署环境之间的分布偏移问题，提高模型在实际应用中的性能。

Method: 三层框架，首先通过受控的图像填充在特定领域背景中合成目标物体，并通过对象检测、美学评分和视觉-语言对齐的多模态评估进行验证，最后使用用户偏好分类器捕捉主观选择标准。

Result: 该方法能够高效构建高质量、可部署的数据集，减少对大量实际数据收集的依赖。

Conclusion: 自动化管道为特定领域的数据集生成提供了一种有效的解决方案，特别是在需要大量标注数据的场景中具有显著优势。

Abstract: In this paper, we present an automated pipeline for generating domain-specific synthetic datasets with diffusion models, addressing the distribution shift between pre-trained models and real-world deployment environments. Our three-stage framework first synthesizes target objects within domain-specific backgrounds through controlled inpainting. The generated outputs are then validated via a multi-modal assessment that integrates object detection, aesthetic scoring, and vision-language alignment. Finally, a user-preference classifier is employed to capture subjective selection criteria. This pipeline enables the efficient construction of high-quality, deployable datasets while reducing reliance on extensive real-world data collection.

</details>


### [19] [PathoGen: Diffusion-Based Synthesis of Realistic Lesions in Histopathology Images](https://arxiv.org/abs/2601.08127)
*Mohamad Koohi-Moghadam,Mohammad-Ali Nikouei Mahani,Kyongtae Tyler Bae*

Main category: cs.CV

TL;DR: PathoGen 是一种基于扩散的生成模型，它能够生成高保真的病灶，并用于增强训练数据集，从而提高病理诊断模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的病灶数据稀缺，特别是对于罕见的病理和未被充分代表的疾病亚型，限制了稳健的人工智能模型的发展。PathoGen旨在解决这一问题，通过生成保真的病灶图像，增强模型训练数据。

Method: PathoGen 使用基于扩散的生成模型，通过迭代细化过程生成自然边界、保持细胞结构和真实染色特征的病灶。这种方法不同于传统的增强技术，能够提供更高质量的数据增强效果。

Result: PathoGen 在四个不同的病理数据集（肾、皮肤、乳腺和前列腺病理）上的评估结果表明，它在图像保真度和分布相似性方面优于现有的生成基线。此外，增强训练集中的病灶图像后，模型的分割性能得到显著提升，特别是在数据稀缺的情况下。

Conclusion: PathoGen 可以为开发泛化的医疗 AI 系统提供一种可扩展的途径，即使在受限的专家标注数据环境下，也能提高模型的性能。

Abstract: The development of robust artificial intelligence models for histopathology diagnosis is severely constrained by the scarcity of expert-annotated lesion data, particularly for rare pathologies and underrepresented disease subtypes. While data augmentation offers a potential solution, existing methods fail to generate sufficiently realistic lesion morphologies that preserve the complex spatial relationships and cellular architectures characteristic of histopathological tissues. Here we present PathoGen, a diffusion-based generative model that enables controllable, high-fidelity inpainting of lesions into benign histopathology images. Unlike conventional augmentation techniques, PathoGen leverages the iterative refinement process of diffusion models to synthesize lesions with natural tissue boundaries, preserved cellular structures, and authentic staining characteristics. We validate PathoGen across four diverse datasets representing distinct diagnostic challenges: kidney, skin, breast, and prostate pathology. Quantitative assessment confirms that PathoGen outperforms state-of-the-art generative baselines, including conditional GAN and Stable Diffusion, in image fidelity and distributional similarity. Crucially, we show that augmenting training sets with PathoGen-synthesized lesions enhances downstream segmentation performance compared to traditional geometric augmentations, particularly in data-scarce regimes. Besides, by simultaneously generating realistic morphology and pixel-level ground truth, PathoGen effectively overcomes the manual annotation bottleneck. This approach offers a scalable pathway for developing generalizable medical AI systems despite limited expert-labeled data.

</details>


### [20] [How Do Optical Flow and Textual Prompts Collaborate to Assist in Audio-Visual Semantic Segmentation?](https://arxiv.org/abs/2601.08133)
*Peng Gao,Yujian Lee,Yongqi Xu,Wentao Fan*

Main category: cs.CV

TL;DR: 该研究提出了一个新的协作框架Stepping Stone Plus (SSP)，通过引入光学流和文本提示，实现了音频-视觉场景的高效和准确分割。


<details>
  <summary>Details</summary>
Motivation: 传统的音频-视觉分割任务只局限于识别视觉上的声源对象，而没有深入理解音频-视觉场景的语义内容。因此，本文旨在通过引入新的框架和方法来提升音频-视觉语义分割的准确性和效率。

Method: 该方法首先利用光学流捕捉运动动态，为后续的语义分析提供必要的 temporal context。其次，通过引入特定的文本提示来处理静止声源对象，提高模型对复杂场景的识别能力。此外，还设计了视觉-文本对齐模块来促进跨模态信息的整合。

Result: 实验结果显示，该方法在音频-视觉场景语义分割方面优于现有的方法，能实现高效准确的分割结果。

Conclusion: Stepping Stone Plus在处理复杂音频-视觉场景时表现出优异的性能，提供了对静止声源和动态场景语义理解的有效手段。

Abstract: Audio-visual semantic segmentation (AVSS) represents an extension of the audio-visual segmentation (AVS) task, necessitating a semantic understanding of audio-visual scenes beyond merely identifying sound-emitting objects at the visual pixel level. Contrary to a previous methodology, by decomposing the AVSS task into two discrete subtasks by initially providing a prompted segmentation mask to facilitate subsequent semantic analysis, our approach innovates on this foundational strategy. We introduce a novel collaborative framework, \textit{S}tepping \textit{S}tone \textit{P}lus (SSP), which integrates optical flow and textual prompts to assist the segmentation process. In scenarios where sound sources frequently coexist with moving objects, our pre-mask technique leverages optical flow to capture motion dynamics, providing essential temporal context for precise segmentation. To address the challenge posed by stationary sound-emitting objects, such as alarm clocks, SSP incorporates two specific textual prompts: one identifies the category of the sound-emitting object, and the other provides a broader description of the scene. Additionally, we implement a visual-textual alignment module (VTA) to facilitate cross-modal integration, delivering more coherent and contextually relevant semantic interpretations. Our training regimen involves a post-mask technique aimed at compelling the model to learn the diagram of the optical flow. Experimental results demonstrate that SSP outperforms existing AVS methods, delivering efficient and precise segmentation results.

</details>


### [21] [Subspace Alignment for Vision-Language Model Test-time Adaptation](https://arxiv.org/abs/2601.08139)
*Zhichen Zeng,Wenxuan Bao,Xiao Lin,Ruizhong Qiu,Tianxin Wei,Xuying Ning,Yuchen Yan,Chen Luo,Monica Xiao Cheng,Jingrui He,Hanghang Tong*

Main category: cs.CV

TL;DR: SubTTA通过调整视觉和文本模态的语义子空间来改善零样本预测，从而更好地指导自训练过程，进而提高适应性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于不稳定的零样本预测作为伪标签，容易在分布偏移时引导错误的自适应。

Method: SubTTA方法首先提取两模态的主子空间，通过最小化它们的弦距将视觉流形对齐到基于文本的语义锚点。其次，SubTTA将对齐后的视觉特征投影到任务特定的文本子空间，这可以通过限制视觉嵌入在有效的语义范围内来过滤掉任务无关的噪声。最后，在净化的空间中仅进行标准的自训练以细化决策边界。

Result: SubTTA在各种基准和VLM架构上进行了广泛的实验，显示出其有效性，比最先进的自训练方法平均提高了2.24%。

Conclusion: SubTTA通过改善零样本预测来加快视觉-语言模型的自适应过程，并显著提高了模型的适应性能。

Abstract: Vision-language models (VLMs), despite their extraordinary zero-shot capabilities, are vulnerable to distribution shifts. Test-time adaptation (TTA) emerges as a predominant strategy to adapt VLMs to unlabeled test data on the fly. However, existing TTA methods heavily rely on zero-shot predictions as pseudo-labels for self-training, which can be unreliable under distribution shifts and misguide adaptation due to two fundamental limitations. First (Modality Gap), distribution shifts induce gaps between visual and textual modalities, making cross-modal relations inaccurate. Second (Visual Nuisance), visual embeddings encode rich but task-irrelevant noise that often overwhelms task-specific semantics under distribution shifts. To address these limitations, we propose SubTTA, which aligns the semantic subspaces of both modalities to enhance zero-shot predictions to better guide the TTA process. To bridge the modality gap, SubTTA extracts the principal subspaces of both modalities and aligns the visual manifold to the textual semantic anchor by minimizing their chordal distance. To eliminate visual nuisance, SubTTA projects the aligned visual features onto the task-specific textual subspace, which filters out task-irrelevant noise by constraining visual embeddings within the valid semantic span, and standard TTA is further performed on the purified space to refine the decision boundaries. Extensive experiments on various benchmarks and VLM architectures demonstrate the effectiveness of SubTTA, yielding an average improvement of 2.24% over state-of-the-art TTA methods.

</details>


### [22] [Where Does Vision Meet Language? Understanding and Refining Visual Fusion in MLLMs via Contrastive Attention](https://arxiv.org/abs/2601.08151)
*Shezheng Song,Shasha Li,Jie Yu*

Main category: cs.CV

TL;DR: 该研究通过系统性的分层遮蔽分析揭示了多模态大语言模型（MLLM）中视觉和文本信息的融合过程，并提出了一种无需训练的对比注意框架，以改善多模态推理表现。


<details>
  <summary>Details</summary>
Motivation: 当前对MLLM内部如何整合视觉与文本信息的理解不足，因此研究者旨在揭示这一过程，提升多模态推理性能。

Method: 研究采用了分层遮蔽分析，观察了多种架构下的视觉-文本融合过程，同时分析了分层注意的演变，揭示了早期融合和最终层之间的注意转变，并提出了一种对比注意框架。

Result: 研究发现融合主要在特定层中出现，并观察到一种后期“审查”现象，即视觉信号在生成输出前被重新激活；此外，还发现持续的高注意噪音出现在无关区域，而文本人物区域的注意逐步增加。基于这些发现，提出了对比注意框架以提高性能。

Conclusion: 该研究通过分析验证了提出的对比注意框架有效提升MLLMs的多模态推理表现。

Abstract: Multimodal Large Language Models (MLLMs) have achieved remarkable progress in vision-language understanding, yet how they internally integrate visual and textual information remains poorly understood. To bridge this gap, we perform a systematic layer-wise masking analysis across multiple architectures, revealing how visual-text fusion evolves within MLLMs. The results show that fusion emerges at several specific layers rather than being uniformly distributed across the network, and certain models exhibit a late-stage "review" phenomenon where visual signals are reactivated before output generation. Besides, we further analyze layer-wise attention evolution and observe persistent high-attention noise on irrelevant regions, along with gradually increasing attention on text-aligned areas. Guided by these insights, we introduce a training-free contrastive attention framework that models the transformation between early fusion and final layers to highlight meaningful attention shifts. Extensive experiments across various MLLMs and benchmarks validate our analysis and demonstrate that the proposed approach improves multimodal reasoning performance. Code will be released.

</details>


### [23] [Instance-Aligned Captions for Explainable Video Anomaly Detection](https://arxiv.org/abs/2601.08155)
*Inpyo Song,Minjun Joo,Joonhyung Kwon,Eunji Jeon,Jangwon Lee*

Main category: cs.CV

TL;DR: 本文介绍了一种实例对齐的caption方法，旨在增强视频异常检测（VAD）方法的可验证性和解释性。通过对现有数据集进行扩展并提供详细的实例对齐描述，该方法能够突出当前模型的不足，并为未来研究提供坚实基准。


<details>
  <summary>Details</summary>
Motivation: 现有VAD方法在处理多元实体互动时，常出现不完整或视觉对齐错误的描述，影响解释的可信度。因此，本文旨在改善VAD方法的解释性。

Method: 本文提出了一种实例对齐的caption方法，将文本声明与特定的物体实例联系起来，基于物体的外观和运动属性。该方法能够准确描述异常的起因、受影响的实体、事件的发生位置等，从而实现可验证和可操作的推理。

Result: 通过在多个广泛使用的VAD基准数据集上进行标注，并扩展360度第一人称视角数据集VIEW360，作者创建了VIEW360+，一个全面的可解释VAD测试平台。实验证明，该方法揭示了当前基于大型语言模型（LLM）和视觉语言模型（VLM）的VAD方法的显著局限性，提供了未来研究的坚实基准。

Conclusion: 实例对齐的caption方法对于增进VAD方法的解释性具有重要意义，有助于推动稳健且可解释的异常检测研究的进展。

Abstract: Explainable video anomaly detection (VAD) is crucial for safety-critical applications, yet even with recent progress, much of the research still lacks spatial grounding, making the explanations unverifiable. This limitation is especially pronounced in multi-entity interactions, where existing explainable VAD methods often produce incomplete or visually misaligned descriptions, reducing their trustworthiness. To address these challenges, we introduce instance-aligned captions that link each textual claim to specific object instances with appearance and motion attributes. Our framework captures who caused the anomaly, what each entity was doing, whom it affected, and where the explanationis grounded, enabling verifiable and actionable reasoning. We annotate eight widely used VAD benchmarks and extend the 360-degree egocentric dataset, VIEW360, with 868 additional videos, eight locations, and four new anomaly types, creating VIEW360+, a comprehensive testbed for explainable VAD. Experiments show that our instance-level spatially grounded captions reveal significant limitations in current LLM- and VLM-based methods while providing a robust benchmark for future research in trustworthy and interpretable anomaly detection.

</details>


### [24] [A Hardware-Algorithm Co-Designed Framework for HDR Imaging and Dehazing in Extreme Rocket Launch Environments](https://arxiv.org/abs/2601.08162)
*Jing Tao,Banglei Guan,Pengju Sun,Taihang Lei,Yang Shang,Qifeng Yu*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Quantitative optical measurement of critical mechanical parameters -- such as plume flow fields, shock wave structures, and nozzle oscillations -- during rocket launch faces severe challenges due to extreme imaging conditions. Intense combustion creates dense particulate haze and luminance variations exceeding 120 dB, degrading image data and undermining subsequent photogrammetric and velocimetric analyses. To address these issues, we propose a hardware-algorithm co-design framework that combines a custom Spatially Varying Exposure (SVE) sensor with a physics-aware dehazing algorithm. The SVE sensor acquires multi-exposure data in a single shot, enabling robust haze assessment without relying on idealized atmospheric models. Our approach dynamically estimates haze density, performs region-adaptive illumination optimization, and applies multi-scale entropy-constrained fusion to effectively separate haze from scene radiance. Validated on real launch imagery and controlled experiments, the framework demonstrates superior performance in recovering physically accurate visual information of the plume and engine region. This offers a reliable image basis for extracting key mechanical parameters, including particle velocity, flow instability frequency, and structural vibration, thereby supporting precise quantitative analysis in extreme aerospace environments.

</details>


### [25] [Representation Learning with Semantic-aware Instance and Sparse Token Alignments](https://arxiv.org/abs/2601.08165)
*Phuoc-Nguyen Bui,Toan Duc Nguyen,Junghyun Bum,Duc-Tai Le,Hyunseung Choo*

Main category: cs.CV

TL;DR: 本文提出了一种多级对齐框架SISTA，旨在通过医疗图像与放射报告之间的语义对应关系，在图像-报告和像素-词元层次上改善传统的对比学习。SISTA通过引入报告间相似性消除假阴性，并有效对齐图像像素与相关词元。实验结果显示SISTA在不同的下游任务（图像分类、图像分割和对象检测）中改善了迁移性能，特别是在细粒度任务中对使用少量标注数据的情况效果显著。


<details>
  <summary>Details</summary>
Motivation: 传统对比学习方法中的负样本可能破坏语义结构，SISTA旨在通过引入语义对齐来改进这一问题。

Method: 提出了SISTA框架，利用医疗图像和放射报告之间的语义对应关系，在图像-报告和像素-词元两个层次上进行对齐。

Result: SISTA在三个下游任务（图像分类、图像分割和对象检测）中提高了迁移性能，特别在细粒度任务中表现突出。

Conclusion: SISTA通过利用语义对齐改进了传统的对比学习方法，特别是在细粒分类任务中表现优异。

Abstract: Medical contrastive vision-language pre-training (VLP) has demonstrated significant potential in improving performance on downstream tasks. Traditional approaches typically employ contrastive learning, treating paired image-report samples as positives and unpaired ones as negatives. However, in medical datasets, there can be substantial similarities between images or reports from different patients. Rigidly treating all unpaired samples as negatives, can disrupt the underlying semantic structure and negatively impact the quality of the learned representations. In this paper, we propose a multi-level alignment framework, Representation Learning with Semantic-aware Instance and Sparse Token Alignments (SISTA) by exploiting the semantic correspondence between medical image and radiology reports at two levels, i.e., image-report and patch-word levels. Specifically, we improve the conventional contrastive learning by incorporating inter-report similarity to eliminate the false negatives and introduce a method to effectively align image patches with relevant word tokens. Experimental results demonstrate the effectiveness of the proposed framework in improving transfer performance across different datasets on three downstream tasks: image classification, image segmentation, and object detection. Notably, our framework achieves significant improvements in fine-grained tasks even with limited labeled data. Codes and pre-trained models will be made available.

</details>


### [26] [Towards Cross-Platform Generalization: Domain Adaptive 3D Detection with Augmentation and Pseudo-Labeling](https://arxiv.org/abs/2601.08174)
*Xiyan Feng,Wenbo Zhang,Lu Zhang,Yunzhi Zhuge,Huchuan Lu,You He*

Main category: cs.CV

TL;DR: 本报告描述了在RoboSense2025挑战中获奖的跨平台3D目标检测解决方案，通过改进数据增强和自我训练策略提高泛化能力，最终在Car和Pedestrian类别中分别取得了62.67%和58.76%的3D AP。


<details>
  <summary>Details</summary>
Motivation: 报告的动机是为了在RoboSense2025挑战中展示一种有效的3D目标检测方法，以解决跨平台3D物体检测的问题。

Method: 基于PVRCNN++框架，该方法整合了点云和体素特征。通过特定的数据增强和带有伪标签的自我训练策略来减小领域差距，从而提高跨平台的泛化能力。

Result: 通过这种方法，在第一阶段的目标领域中，Car类别的3D AP达到62.67%，而在第二阶段的Car和Pedestrian类别中，分别达到了58.76%和49.81%的3D AP。

Conclusion: 最终，该方法帮助团队在挑战中获得第3名的好成绩，展示了跨平台3D目标检测的有效方法。

Abstract: This technical report represents the award-winning solution to the Cross-platform 3D Object Detection task in the RoboSense2025 Challenge. Our approach is built upon PVRCNN++, an efficient 3D object detection framework that effectively integrates point-based and voxel-based features. On top of this foundation, we improve cross-platform generalization by narrowing domain gaps through tailored data augmentation and a self-training strategy with pseudo-labels. These enhancements enabled our approach to secure the 3rd place in the challenge, achieving a 3D AP of 62.67% for the Car category on the phase-1 target domain, and 58.76% and 49.81% for Car and Pedestrian categories respectively on the phase-2 target domain.

</details>


### [27] [CogniMap3D: Cognitive 3D Mapping and Rapid Retrieval](https://arxiv.org/abs/2601.08175)
*Feiran Wang,Junyi Wu,Dawen Cai,Yuan Hong,Yan Yan*

Main category: cs.CV

TL;DR: CogniMap3D 是一种模仿人类认知过程的生物启发式框架，用于动态3D场景理解和重建，融合了多种关键技术，实现了卓越的场景理解性能。


<details>
  <summary>Details</summary>
Motivation: 随着3D场景理解与重构技术的需求日益增长，现有的方法往往难以提供持续性的场景理解能力。CogniMap3D旨在通过模仿人类的认知过程，解决这一挑战，提供更加高效和精确的3D场景理解和重建。

Method: CogniMap3D结合了多阶段运动线索框架、认知制图系统和因子图优化策略。通过深度和相机姿态先验，CogniMap3D识别动态对象，并与内存中的静态元素进行匹配。在重新访问熟识地点时，系统会检索存储的场景，重新定位相机，并用新观察结果更新记忆。

Result: CogniMap3D在视频深度估计、相机姿态重构和3D建图任务上的评估表明，其在多个方面都达到了业内领先水平，特别是在长期且连续的场景理解和跨多次访问时，显示出了明显的优势。

Conclusion: CogniMap3D以其全面且独特的技术框架，展示了其在3D场景理解与重构领域的巨大潜力，有望促进相关技术的发展与进步。

Abstract: We present CogniMap3D, a bioinspired framework for dynamic 3D scene understanding and reconstruction that emulates human cognitive processes. Our approach maintains a persistent memory bank of static scenes, enabling efficient spatial knowledge storage and rapid retrieval. CogniMap3D integrates three core capabilities: a multi-stage motion cue framework for identifying dynamic objects, a cognitive mapping system for storing, recalling, and updating static scenes across multiple visits, and a factor graph optimization strategy for refining camera poses. Given an image stream, our model identifies dynamic regions through motion cues with depth and camera pose priors, then matches static elements against its memory bank. When revisiting familiar locations, CogniMap3D retrieves stored scenes, relocates cameras, and updates memory with new observations. Evaluations on video depth estimation, camera pose reconstruction, and 3D mapping tasks demonstrate its state-of-the-art performance, while effectively supporting continuous scene understanding across extended sequences and multiple visits.

</details>


### [28] [Instruction-Driven 3D Facial Expression Generation and Transition](https://arxiv.org/abs/2601.08179)
*Anh H. Vo,Tae-Seok Kim,Hulin Jin,Soo-Mi Choi,Yong-Guk Kim*

Main category: cs.CV

TL;DR: 该研究提出了一种新框架，通过文本指令生成面部表情过渡，增强了面部表情的多样性，并在多个数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 由于3D头像通常只有六个基本面部表情，模拟现实中的情感变化需要能够在任意两个表情间进行过渡。研究旨在通过文本指令驱动面部表情生成，扩展面部表情的范围和过渡方式。

Method: 引入了Instruction-driven Facial Expression Decomposer (IFED) 模块来处理多模态数据，捕捉文本描述与面部表情特征之间的关联。接着提出了Instruction to Facial Expression Transition (I2FET) 方法，利用IFED和顶点重建损失函数精炼语义理解，生成面部表情序列。最后，使用Facial Expression Transition 模型生成面部表情过渡。

Result: 该模型在多个数据集上表现优于现有方法，能够根据文本指令生成面部表情轨迹。验证了通过文本描述可以极大地扩大面部表情和过渡的表情库。

Conclusion: 该框架有望在多种实际应用中找到应用，展示了通过文本指令生成面部表情的潜力。

Abstract: A 3D avatar typically has one of six cardinal facial expressions. To simulate realistic emotional variation, we should be able to render a facial transition between two arbitrary expressions. This study presents a new framework for instruction-driven facial expression generation that produces a 3D face and, starting from an image of the face, transforms the facial expression from one designated facial expression to another. The Instruction-driven Facial Expression Decomposer (IFED) module is introduced to facilitate multimodal data learning and capture the correlation between textual descriptions and facial expression features. Subsequently, we propose the Instruction to Facial Expression Transition (I2FET) method, which leverages IFED and a vertex reconstruction loss function to refine the semantic comprehension of latent vectors, thus generating a facial expression sequence according to the given instruction. Lastly, we present the Facial Expression Transition model to generate smooth transitions between facial expressions. Extensive evaluation suggests that the proposed model outperforms state-of-the-art methods on the CK+ and CelebV-HQ datasets. The results show that our framework can generate facial expression trajectories according to text instruction. Considering that text prompts allow us to make diverse descriptions of human emotional states, the repertoire of facial expressions and the transitions between them can be expanded greatly. We expect our framework to find various practical applications More information about our project can be found at https://vohoanganh.github.io/tg3dfet/

</details>


### [29] [Second-order Gaussian directional derivative representations for image high-resolution corner detection](https://arxiv.org/abs/2601.08182)
*Dongbo Xie,Junjie Qiu,Changming Sun,Weichuan Zhang*

Main category: cs.CV

TL;DR: 该研究修正了Zhang等人在角模型上的理论缺陷，通过引入第二阶高斯方向导数滤波器，改进了END型和L型高分辨率角模型，提出了一种新的高分辨率角检测方法，实验显示该方法在定位误差、图像模糊转换的鲁棒性、图像匹配和3D重建方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有角检测方法存在一些理论缺陷，特别是关于角模型的使用存在不足，Zhang等人使用简单的角模型，未能充分考虑到两个连续角之间的灰度信息相互影响的问题。

Method: 研究采用了第二阶高斯方向导数滤波器对END型和L型高分辨率角模型进行了平滑处理，并分别推导了这两种角模型的SOGDD表示，发现在高分辨率角中存在许多特征，揭示了如何通过选择合适的高斯滤波尺度来获取图像的强度变化信息。

Result: 提出了一种能够更准确地检测图像相邻角点的新高分辨率角检测方法，该方法在定位误差、图像模糊转换的鲁棒性、图像匹配和3D重建方面表现优于现有最先进的方法。

Conclusion: 该研究通过改进角模型、提出新的高分辨率角检测方法，显著提升了角检测的准确性和鲁棒性，具有重要的应用价值。

Abstract: Corner detection is widely used in various computer vision tasks, such as image matching and 3D reconstruction. Our research indicates that there are theoretical flaws in Zhang et al.'s use of a simple corner model to obtain a series of corner characteristics, as the grayscale information of two adjacent corners can affect each other. In order to address the above issues, a second-order Gaussian directional derivative (SOGDD) filter is used in this work to smooth two typical high-resolution angle models (i.e. END-type and L-type models). Then, the SOGDD representations of these two corner models were derived separately, and many characteristics of high-resolution corners were discovered, which enabled us to demonstrate how to select Gaussian filtering scales to obtain intensity variation information from images, accurately depicting adjacent corners. In addition, a new high-resolution corner detection method for images has been proposed for the first time, which can accurately detect adjacent corner points. The experimental results have verified that the proposed method outperforms state-of-the-art methods in terms of localization error, robustness to image blur transformation, image matching, and 3D reconstruction.

</details>


### [30] [GI-Bench: A Panoramic Benchmark Revealing the Knowledge-Experience Dissociation of Multimodal Large Language Models in Gastrointestinal Endoscopy Against Clinical Standards](https://arxiv.org/abs/2601.08183)
*Yan Zhu,Te Luo,Pei-Yao Fu,Zhen Zhang,Zi-Long Wang,Yi-Fan Qu,Zi-Han Geng,Jia-Qi Xu,Lu Yao,Li-Yun Ma,Wei Su,Wei-Feng Chen,Quan-Lin Li,Shuo Wang,Ping-Hong Zhou*

Main category: cs.CV

TL;DR: 该研究构建了GI-Bench基准，全面评估了多种MLLM在胃肠镜检查工作流程中的性能，主要结果表明，在诊断推理方面，顶级模型的表现优于初级学员但与初级内镜医生相当，但在局部解剖定位方面，人类的表现明显优于现有模型，且模型在语言流畅性方面超出人类，但在事实准确性上因过分解释和幻觉而表现较差。


<details>
  <summary>Details</summary>
Motivation: 由于现有的多模态大型语言模型（MLLM）在胃肠病学领域的潜力未得到广泛验证，本研究旨在系统性地评估最新的MLLM在全方位胃肠镜检查工作流程中的性能，并与人类内镜医生进行对比。

Method: 研究构建了GI-Bench基准，涉及20种细粒度的病变类别，并设置了五个阶段的临床工作流程：解剖定位、病变识别、诊断、发现描述和管理。评估MMLMs（共12种）在各个阶段的表现，使用了多方面指标进行评估，包括Macro-F1、mean Intersection-over-Union（mIoU）和多维度Likert量表。

Result: Gemini-3-Pro在这些方面取得了最佳表现。在诊断推理阶段，顶尖模型（Macro-F1为0.641）的表现显著优于初级学员（0.492）并接近初级内镜医生（0.727）。然而，人类在病变定位（mIoU >0.506）方面表现出色，超越了最佳模型（0.345）。此外，定性分析显示了“流畅性-准确性悖论”：模型生成的报告在语言可读性上优于人类，但在事实正确性方面表现较差，特别是在图像特征的过度解释和幻觉方面。

Conclusion: 研究结论指出，MLLMs在胃肠镜检查中的诊断推理方面与人类专家水平相当，但在病变定位任务上仍存在明显的差距。尽管模型的语言表达能力较强，但在事实准确性方面仍有提升空间，这表明MLLMs还有待进一步改进以适应复杂的临床场景。GI-Bench持续更新，跟踪MLLMs在临床内镜中的表现，数据可获取链接已提供。

Abstract: Multimodal Large Language Models (MLLMs) show promise in gastroenterology, yet their performance against comprehensive clinical workflows and human benchmarks remains unverified. To systematically evaluate state-of-the-art MLLMs across a panoramic gastrointestinal endoscopy workflow and determine their clinical utility compared with human endoscopists. We constructed GI-Bench, a benchmark encompassing 20 fine-grained lesion categories. Twelve MLLMs were evaluated across a five-stage clinical workflow: anatomical localization, lesion identification, diagnosis, findings description, and management. Model performance was benchmarked against three junior endoscopists and three residency trainees using Macro-F1, mean Intersection-over-Union (mIoU), and multi-dimensional Likert scale. Gemini-3-Pro achieved state-of-the-art performance. In diagnostic reasoning, top-tier models (Macro-F1 0.641) outperformed trainees (0.492) and rivaled junior endoscopists (0.727; p>0.05). However, a critical "spatial grounding bottleneck" persisted; human lesion localization (mIoU >0.506) significantly outperformed the best model (0.345; p<0.05). Furthermore, qualitative analysis revealed a "fluency-accuracy paradox": models generated reports with superior linguistic readability compared with humans (p<0.05) but exhibited significantly lower factual correctness (p<0.05) due to "over-interpretation" and hallucination of visual features.GI-Bench maintains a dynamic leaderboard that tracks the evolving performance of MLLMs in clinical endoscopy. The current rankings and benchmark results are available at https://roterdl.github.io/GIBench/.

</details>


### [31] [Unified Multi-Site Multi-Sequence Brain MRI Harmonization Enriched by Biomedical Semantic Style](https://arxiv.org/abs/2601.08193)
*Mengqi Wu,Yongheng Sun,Qianqian Wang,Pew-Thian Yap,Mingxia Liu*

Main category: cs.CV

TL;DR: MMH提出了一种统一框架，用于处理多站点多序列的脑MRI数据，通过生物医学语义先验进行序列感知的风格对齐。


<details>
  <summary>Details</summary>
Motivation: 鉴于使用多站点脑MRI数据训练深度学习模型能够增强模型训练，但站点特定的差异（如扫描供应商、采集参数和成像协议的不同）会引入非生物异质性，MMH旨在通过标准化图像风格（不破坏解剖内容）来降低这些站点效应，从而提高模型的泛化能力。现有方法通常依赖有限的配对受试者数据或未能有效分离风格与解剖学，且多数方法只关注单序列和谐化，限制了其在通常收集多序列MRI的实际场景中的应用。

Method: MMH框架包括两个阶段：（1）一种基于扩散的全局谐波机，该机通过无风格梯度调节将MRI图像映射到特定序列的统一域；（2）针对特定目标的微调器，使全局对齐后的图像适应期望的目标域。三平面注意BiomedCLIP编码器整合多视图嵌入来表征体积风格信息，在不对图像风格和解剖学进行配对数据要求的情况下实现风格与解剖学的明确分离。

Result: MMH在包含4,163张T1-和T2加权MRI图像的评估中，优于最先进的方法，在图像特征聚类、体积比较、组织分割以及下游的年龄和站点分类中显示出了卓越的和谐化效果。

Conclusion: MMH提供了一种全面解决多站点和多序列脑MRI和谐化的策略，通过对生物医学语义的利用和序列感知的风格对齐，增强了模型在实际应用场景中的性能和适用性。

Abstract: Aggregating multi-site brain MRI data can enhance deep learning model training, but also introduces non-biological heterogeneity caused by site-specific variations (e.g., differences in scanner vendors, acquisition parameters, and imaging protocols) that can undermine generalizability. Recent retrospective MRI harmonization seeks to reduce such site effects by standardizing image style (e.g., intensity, contrast, noise patterns) while preserving anatomical content. However, existing methods often rely on limited paired traveling-subject data or fail to effectively disentangle style from anatomy. Furthermore, most current approaches address only single-sequence harmonization, restricting their use in real-world settings where multi-sequence MRI is routinely acquired. To this end, we introduce MMH, a unified framework for multi-site multi-sequence brain MRI harmonization that leverages biomedical semantic priors for sequence-aware style alignment. MMH operates in two stages: (1) a diffusion-based global harmonizer that maps MR images to a sequence-specific unified domain using style-agnostic gradient conditioning, and (2) a target-specific fine-tuner that adapts globally aligned images to desired target domains. A tri-planar attention BiomedCLIP encoder aggregates multi-view embeddings to characterize volumetric style information, allowing explicit disentanglement of image styles from anatomy without requiring paired data. Evaluations on 4,163 T1- and T2-weighted MRIs demonstrate MMH's superior harmonization over state-of-the-art methods in image feature clustering, voxel-level comparison, tissue segmentation, and downstream age and site classification.

</details>


### [32] [MobiDiary: Autoregressive Action Captioning with Wearable Devices and Wireless Signals](https://arxiv.org/abs/2601.08204)
*Fei Deng,Yinghui He,Chuntong Chu,Ge Wang,Han Ding,Jinsong Han,Fei Wang*

Main category: cs.CV

TL;DR: MobiDiary 是一种框架，可以从多种物理信号（IMU 和 Wi-Fi）中生成日常活动的自然语言描述，而无需依赖于预定义标签。通过统一传感器编码器，该框架能有效地捕捉动能信号中的局部时间相关性，并统一跨不同传感器的空间上下文，从而生成连贯的动作描述。


<details>
  <summary>Details</summary>
Motivation: 现有的人体活动识别（HAR）系统，特别是基于视觉的方法，存在隐私和环境限制问题。MobiDiary 旨在解决这些问题，通过利用运动诱导信号的共同归纳偏见，从异构物理信号中生成表达性强、易于理解的日常活动描述。

Method: MobiDiary 采用了一种统一的传感器编码器，利用 Patch-based 机制捕捉局部时序相关性，并结合多位置嵌入整合跨传感器的时空上下文。该编码器将这种统一体信号片段输入到基于 Transformer 的解码器中，解码器采用自回归机制生成连贯的行为描述。

Result: MobiDiary 在多个公开基准（XRF V2、UWash 和 WiFiTAD）上进行了全面评估。实验结果表明，该方法在字幕指标（如 BLEU@4、CIDEr、RMC）上达到了最先进的性能，并且在连续行为理解和专门基线方法方面表现更优。

Conclusion: 总而言之，MobiDiary 通过整合跨传感器的时空信息和利用 Transformer 解码器生成连贯的行为描述，展示了其在多种物理信号上的优越性，为智能家庭中的健康监控和辅助生活提供了新的解决方案。

Abstract: Human Activity Recognition (HAR) in smart homes is critical for health monitoring and assistive living. While vision-based systems are common, they face privacy concerns and environmental limitations (e.g., occlusion). In this work, we present MobiDiary, a framework that generates natural language descriptions of daily activities directly from heterogeneous physical signals (specifically IMU and Wi-Fi). Unlike conventional approaches that restrict outputs to pre-defined labels, MobiDiary produces expressive, human-readable summaries. To bridge the semantic gap between continuous, noisy physical signals and discrete linguistic descriptions, we propose a unified sensor encoder. Instead of relying on modality-specific engineering, we exploit the shared inductive biases of motion-induced signals--where both inertial and wireless data reflect underlying kinematic dynamics. Specifically, our encoder utilizes a patch-based mechanism to capture local temporal correlations and integrates heterogeneous placement embedding to unify spatial contexts across different sensors. These unified signal tokens are then fed into a Transformer-based decoder, which employs an autoregressive mechanism to generate coherent action descriptions word-by-word. We comprehensively evaluate our approach on multiple public benchmarks (XRF V2, UWash, and WiFiTAD). Experimental results demonstrate that MobiDiary effectively generalizes across modalities, achieving state-of-the-art performance on captioning metrics (e.g., BLEU@4, CIDEr, RMC) and outperforming specialized baselines in continuous action understanding.

</details>


### [33] [FUME: Fused Unified Multi-Gas Emission Network for Livestock Rumen Acidosis Detection](https://arxiv.org/abs/2601.08205)
*Taminul Islam,Toqi Tahamid Sarker,Mohamed Embaby,Khaled R Ahmed,Amer AbuGhazaleh*

Main category: cs.CV

TL;DR: 该研究引入了一种名为FUME的深度学习方法，采用多气体光学成像技术从体外条件下检测奶牛的瘤胃酸中毒，通过融合二氧化碳和甲烷排放模式，实现了瘤胃健康状态的分类。该方法具有高精度和低计算成本。


<details>
  <summary>Details</summary>
Motivation: 目前的诊法方法依赖于侵入式的pH测量，限制了其在持续监测中的应用规模。因此，研究人员开发了FUME方法，以非侵入性的方式监测奶牛的瘤胃健康状态。

Method: FUME方法采用了一种轻量级的双流架构，其中包含共享权重的编码器，特定模态的自注意力机制和信道注意力融合，共同优化气体羽流分割和奶牛健康的分类。

Result: 在实验中，FUME实现了80.99%的mIoU和98.82%的分类准确性，仅使用了1.28M参数和1.97G MACs，而在计算成本上比当前最先进的方法降低了10倍。

Conclusion: FUME方法通过气体排放模式的融合，展示了基于气体排放的家畜健康监测的可能性，为体外酸中毒检测系统的发展奠定了基础。

Abstract: Ruminal acidosis is a prevalent metabolic disorder in dairy cattle causing significant economic losses and animal welfare concerns. Current diagnostic methods rely on invasive pH measurement, limiting scalability for continuous monitoring. We present FUME (Fused Unified Multi-gas Emission Network), the first deep learning approach for rumen acidosis detection from dual-gas optical imaging under in vitro conditions. Our method leverages complementary carbon dioxide (CO2) and methane (CH4) emission patterns captured by infrared cameras to classify rumen health into Healthy, Transitional, and Acidotic states. FUME employs a lightweight dual-stream architecture with weight-shared encoders, modality-specific self-attention, and channel attention fusion, jointly optimizing gas plume segmentation and classification of dairy cattle health. We introduce the first dual-gas OGI dataset comprising 8,967 annotated frames across six pH levels with pixel-level segmentation masks. Experiments demonstrate that FUME achieves 80.99% mIoU and 98.82% classification accuracy while using only 1.28M parameters and 1.97G MACs--outperforming state-of-the-art methods in segmentation quality with 10x lower computational cost. Ablation studies reveal that CO2 provides the primary discriminative signal and dual-task learning is essential for optimal performance. Our work establishes the feasibility of gas emission-based livestock health monitoring, paving the way for practical, in vitro acidosis detection systems. Codes are available at https://github.com/taminulislam/fume.

</details>


### [34] [Knowledge-based learning in Text-RAG and Image-RAG](https://arxiv.org/abs/2601.08226)
*Alexander Shim,Khalil Saieh,Samuel Clarke*

Main category: cs.CV

TL;DR: 本研究分析并比较了基于Vision Transformer的多模态方法与LLaMA或ChatGPT LLM在减少幻觉问题及胸部X光片疾病检测中的应用效果。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过多模态方法解决Vision Transformer模型在胸部X光图像中的幻觉问题，同时提高疾病检测的准确性。

Method: 研究使用NIH胸部X光图像集训练模型，并将其在基于图像的检索增强生成式对话（image-based RAG）、基于文本的检索增强生成式对话（text-based RAG）以及基线模型之间进行对比。

Result: 研究结果显示，基于文本的RAG有效降低了幻觉问题，通过利用外部知识信息；基于图像的RAG通过KNN方法改善了预测置信度和校准。此外，GPT大语言模型表现出比基于Llama的语言模型更好的性能，较低的幻觉率和更好的预期校准误差。

Conclusion: 研究指出数据不平衡和复杂的多阶段结构是面临的挑战，但也提出了大型经验和平衡使用示例作为解决方案。

Abstract: This research analyzed and compared the multi-modal approach in the Vision Transformer(EVA-ViT) based image encoder with the LlaMA or ChatGPT LLM to reduce the hallucination problem and detect diseases in chest x-ray images. In this research, we utilized the NIH Chest X-ray image to train the model and compared it in image-based RAG, text-based RAG, and baseline. [3] [5] In a result, the text-based RAG[2] e!ectively reduces the hallucination problem by using external knowledge information, and the image-based RAG improved the prediction con"dence and calibration by using the KNN methods. [4] Moreover, the GPT LLM showed better performance, a low hallucination rate, and better Expected Calibration Error(ECE) than Llama Llama-based model. This research shows the challenge of data imbalance, a complex multi-stage structure, but suggests a large experience environment and a balanced example of use.

</details>


### [35] [Improving Zero-shot ADL Recognition with Large Language Models through Event-based Context and Confidence](https://arxiv.org/abs/2601.08241)
*Michele Fiori,Gabriele Civitarese,Marco Colussi,Claudio Bettini*

Main category: cs.CV

TL;DR: 本文提出了一种基于事件的分割方法和新颖的预测置信度估计方法，以改进无需监督的日常活动识别（ADL），并在实验中证明了所提出方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于基于时间的分割，这与大型语言模型（LLMs）的上下文推理能力不匹配，并且缺乏预测置信度的评估方法。

Method: 本文提出了一种基于事件的分割方法，并提出了一种新颖的预测置信度估计方法，以提高零样本ADL识别的性能。

Result: 实验结果显示，基于事件的分割方法在复杂、真实的数据集上一直优于基于时间的LLM方法，并超越了有监督的基于数据的方法，即使使用相对较小的LLM（例如Gemma 3 27B）。提出的置信度测量有效地区分了正确的和错误的预测。

Conclusion: 本文通过引入基于事件的分割方法和置信度估计方法，显著改进了无需监督的ADL识别，并展示了所提出方法的优越性。

Abstract: Unobtrusive sensor-based recognition of Activities of Daily Living (ADLs) in smart homes by processing data collected from IoT sensing devices supports applications such as healthcare, safety, and energy management. Recent zero-shot methods based on Large Language Models (LLMs) have the advantage of removing the reliance on labeled ADL sensor data. However, existing approaches rely on time-based segmentation, which is poorly aligned with the contextual reasoning capabilities of LLMs. Moreover, existing approaches lack methods for estimating prediction confidence. This paper proposes to improve zero-shot ADL recognition with event-based segmentation and a novel method for estimating prediction confidence. Our experimental evaluation shows that event-based segmentation consistently outperforms time-based LLM approaches on complex, realistic datasets and surpasses supervised data-driven methods, even with relatively small LLMs (e.g., Gemma 3 27B). The proposed confidence measure effectively distinguishes correct from incorrect predictions.

</details>


### [36] [HIPPO: Accelerating Video Large Language Models Inference via Holistic-aware Parallel Speculative Decoding](https://arxiv.org/abs/2601.08273)
*Qitan Lv,Tianyu Liu,Wen Wu,Xuenan Xu,Bowen Zhou,Feng Wu,Chao Zhang*

Main category: cs.CV

TL;DR: 本研究提出了一种新的并行推测解码框架HIPPO，通过融合全局注意力得分和局部视觉语义保留语义信息，并拆分生成和目标验证阶段以提高并行度，从而实现视频LLM推理加速。


<details>
  <summary>Details</summary>
Motivation: 现有方法在减少视觉冗余令牌以减轻计算负担方面效果有限，通过研究发现其剪枝策略无法有效保留视觉语义令牌，导致生成草稿质量下降和加速效果不如纯文本LLM，提出HIPPO以更好地保留视觉语义并提高并行度。

Method: HIPPO框架包括语义意识令牌保留方法和视频并行推测解码算法，前者通过结合全局注意力分数和局部视觉语义来保留高剪辑比下的语义信息，后者通过拆分生成和验证阶段以增加并行度。

Result: HIPPO在四个视频LLM上的六个基准测试中表现出有效性，与传统的自回归解码相比，加速比最高可达到3.51倍。

Conclusion: 本研究通过提出HIPPO框架，解决了现有方法中存在的问题，提高了视频LLM的推理速度。

Abstract: Speculative decoding (SD) has emerged as a promising approach to accelerate LLM inference without sacrificing output quality. Existing SD methods tailored for video-LLMs primarily focus on pruning redundant visual tokens to mitigate the computational burden of massive visual inputs. However, existing methods do not achieve inference acceleration comparable to text-only LLMs. We observe from extensive experiments that this phenomenon mainly stems from two limitations: (i) their pruning strategies inadequately preserve visual semantic tokens, degrading draft quality and acceptance rates; (ii) even with aggressive pruning (e.g., 90% visual tokens removed), the draft model's remaining inference cost limits overall speedup. To address these limitations, we propose HIPPO, a general holistic-aware parallel speculative decoding framework. Specifically, HIPPO proposes (i) a semantic-aware token preservation method, which fuses global attention scores with local visual semantics to retain semantic information at high pruning ratios; (ii) a video parallel SD algorithm that decouples and overlaps draft generation and target verification phases. Experiments on four video-LLMs across six benchmarks demonstrate HIPPO's effectiveness, yielding up to 3.51x speedup compared to vanilla auto-regressive decoding.

</details>


### [37] [KidVis: Do Multimodal Large Language Models Possess the Visual Perceptual Capabilities of a 6-Year-Old?](https://arxiv.org/abs/2601.08292)
*Xianfeng Wang,Kaiwei Zhang,Qi Jia,Zijian Chen,Guangtao Zhai,Xiongkuo Min*

Main category: cs.CV

TL;DR: 研究通过引入KidVis基准测试，分析了当前多模态大语言模型在基础视觉能力上的表现，发现它们在这些关键领域远不如人类6-7岁儿童。尽管参数规模增加，但这些模型在这部分视觉能力上的提升没有线性增长。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索多模态大语言模型是否具备与人类相似的基础视觉能力，特别是在低语义依赖的视觉任务上。

Method: 研究设计了KidVis基准测试，评估了20个最先进的多模态大语言模型的性能，并将其与人类生理基线进行比较。

Result: 结果表明人类儿童在所有任务上的平均得分为95.32，而最先进的模型GPT-5仅为67.33。此外，研究发现增加模型参数未能带来线性的性能提升。

Conclusion: 研究指出当前多模态大语言模型缺乏必要的生理感知原语，这些原语是实现泛化视觉智能所必需的。

Abstract: While Multimodal Large Language Models (MLLMs) have demonstrated impressive proficiency in high-level reasoning tasks, such as complex diagrammatic interpretation, it remains an open question whether they possess the fundamental visual primitives comparable to human intuition. To investigate this, we introduce KidVis, a novel benchmark grounded in the theory of human visual development. KidVis deconstructs visual intelligence into six atomic capabilities - Concentration, Tracking, Discrimination, Memory, Spatial, and Closure - already possessed by 6-7 year old children, comprising 10 categories of low-semantic-dependent visual tasks. Evaluating 20 state-of-the-art MLLMs against a human physiological baseline reveals a stark performance disparity. Results indicate that while human children achieve a near-perfect average score of 95.32, the state-of-the-art GPT-5 attains only 67.33. Crucially, we observe a "Scaling Law Paradox": simply increasing model parameters fails to yield linear improvements in these foundational visual capabilities. This study confirms that current MLLMs, despite their reasoning prowess, lack the essential physiological perceptual primitives required for generalized visual intelligence.

</details>


### [38] [M3SR: Multi-Scale Multi-Perceptual Mamba for Efficient Spectral Reconstruction](https://arxiv.org/abs/2601.08293)
*Yuze Zhang,Lingjie Li,Qiuzhen Lin,Zhong Ming,Fei Yu,Victor C. M. Leung*

Main category: cs.CV

TL;DR: 提出了一个名为M3SR的新架构，解决了Mamba架构在高光谱图像重建中的单尺度特性和单一空间感知限制，通过融合多尺度和多感知功能，实现高效且准确的高光谱图像重建。


<details>
  <summary>Details</summary>
Motivation: 针对Mamba架构在高光谱图像重建中遇到的单一空间感知和单一尺度特征提取的限制，旨在提高模型对输入特征的理解和分析能力，解决复杂结构和细节的捕捉问题。

Method: 设计了一个多感知融合模块，通过将其整合到U-Net结构中，有效提取和融合全局、中间和局部特征，实现了多尺度的高光谱图像重建。

Result: 通过大量的定量和定性实验表明，提出的M3SR在保持较低计算成本的同时，超越了现有最先进的方法。

Conclusion: M3SR架构在高光谱图像重建任务上表现出色，能够更全面地理解并分析输入特征，同时降低计算成本。

Abstract: The Mamba architecture has been widely applied to various low-level vision tasks due to its exceptional adaptability and strong performance. Although the Mamba architecture has been adopted for spectral reconstruction, it still faces the following two challenges: (1) Single spatial perception limits the ability to fully understand and analyze hyperspectral images; (2) Single-scale feature extraction struggles to capture the complex structures and fine details present in hyperspectral images. To address these issues, we propose a multi-scale, multi-perceptual Mamba architecture for the spectral reconstruction task, called M3SR. Specifically, we design a multi-perceptual fusion block to enhance the ability of the model to comprehensively understand and analyze the input features. By integrating the multi-perceptual fusion block into a U-Net structure, M3SR can effectively extract and fuse global, intermediate, and local features, thereby enabling accurate reconstruction of hyperspectral images at multiple scales. Extensive quantitative and qualitative experiments demonstrate that the proposed M3SR outperforms existing state-of-the-art methods while incurring a lower computational cost.

</details>


### [39] [Enhancing Image Quality Assessment Ability of LMMs via Retrieval-Augmented Generation](https://arxiv.org/abs/2601.08311)
*Kang Fu,Huiyu Duan,Zicheng Zhang,Yucheng Zhu,Jun Zhao,Xiongkuo Min,Jia Wang,Guangtao Zhai*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large Multimodal Models (LMMs) have recently shown remarkable promise in low-level visual perception tasks, particularly in Image Quality Assessment (IQA), demonstrating strong zero-shot capability. However, achieving state-of-the-art performance often requires computationally expensive fine-tuning methods, which aim to align the distribution of quality-related token in output with image quality levels. Inspired by recent training-free works for LMM, we introduce IQARAG, a novel, training-free framework that enhances LMMs' IQA ability. IQARAG leverages Retrieval-Augmented Generation (RAG) to retrieve some semantically similar but quality-variant reference images with corresponding Mean Opinion Scores (MOSs) for input image. These retrieved images and input image are integrated into a specific prompt. Retrieved images provide the LMM with a visual perception anchor for IQA task. IQARAG contains three key phases: Retrieval Feature Extraction, Image Retrieval, and Integration & Quality Score Generation. Extensive experiments across multiple diverse IQA datasets, including KADID, KonIQ, LIVE Challenge, and SPAQ, demonstrate that the proposed IQARAG effectively boosts the IQA performance of LMMs, offering a resource-efficient alternative to fine-tuning for quality assessment.

</details>


### [40] [YOLOBirDrone: Dataset for Bird vs Drone Detection and Classification and a YOLO based enhanced learning architecture](https://arxiv.org/abs/2601.08319)
*Dapinder Kaur,Neeraj Battish,Arnav Bhavsar,Shashi Poddar*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The use of aerial drones for commercial and defense applications has benefited in many ways and is therefore utilized in several different application domains. However, they are also increasingly used for targeted attacks, posing a significant safety challenge and necessitating the development of drone detection systems. Vision-based drone detection systems currently have an accuracy limitation and struggle to distinguish between drones and birds, particularly when the birds are small in size. This research work proposes a novel YOLOBirDrone architecture that improves the detection and classification accuracy of birds and drones. YOLOBirDrone has different components, including an adaptive and extended layer aggregation (AELAN), a multi-scale progressive dual attention module (MPDA), and a reverse MPDA (RMPDA) to preserve shape information and enrich features with local and global spatial and channel information. A large-scale dataset, BirDrone, is also introduced in this article, which includes small and challenging objects for robust aerial object identification. Experimental results demonstrate an improvement in performance metrics through the proposed YOLOBirDrone architecture compared to other state-of-the-art algorithms, with detection accuracy reaching approximately 85% across various scenarios.

</details>


### [41] [UM-Text: A Unified Multimodal Model for Image Understanding](https://arxiv.org/abs/2601.08321)
*Lichen Ma,Xiaolong Fu,Gaojing Zhou,Zipeng Guo,Ting Zhu,Yichun Liu,Yu Shi,Jason Li,Junshi Huang*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: With the rapid advancement of image generation, visual text editing using natural language instructions has received increasing attention. The main challenge of this task is to fully understand the instruction and reference image, and thus generate visual text that is style-consistent with the image. Previous methods often involve complex steps of specifying the text content and attributes, such as font size, color, and layout, without considering the stylistic consistency with the reference image. To address this, we propose UM-Text, a unified multimodal model for context understanding and visual text editing by natural language instructions. Specifically, we introduce a Visual Language Model (VLM) to process the instruction and reference image, so that the text content and layout can be elaborately designed according to the context information. To generate an accurate and harmonious visual text image, we further propose the UM-Encoder to combine the embeddings of various condition information, where the combination is automatically configured by VLM according to the input instruction. During training, we propose a regional consistency loss to offer more effective supervision for glyph generation on both latent and RGB space, and design a tailored three-stage training strategy to further enhance model performance. In addition, we contribute the UM-DATA-200K, a large-scale visual text image dataset on diverse scenes for model training. Extensive qualitative and quantitative results on multiple public benchmarks demonstrate that our method achieves state-of-the-art performance.

</details>


### [42] [IGAN: A New Inception-based Model for Stable and High-Fidelity Image Synthesis Using Generative Adversarial Networks](https://arxiv.org/abs/2601.08332)
*Ahmed A. Hashim,Ali Al-Shuwaili,Asraa Saeed,Ali Al-Bayaty*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Generative Adversarial Networks (GANs) face a significant challenge of striking an optimal balance between high-quality image generation and training stability. Recent techniques, such as DCGAN, BigGAN, and StyleGAN, improve visual fidelity; however, such techniques usually struggle with mode collapse and unstable gradients at high network depth. This paper proposes a novel GAN structural model that incorporates deeper inception-inspired convolution and dilated convolution. This novel model is termed the Inception Generative Adversarial Network (IGAN). The IGAN model generates high-quality synthetic images while maintaining training stability, by reducing mode collapse as well as preventing vanishing and exploding gradients. Our proposed IGAN model achieves the Frechet Inception Distance (FID) of 13.12 and 15.08 on the CUB-200 and ImageNet datasets, respectively, representing a 28-33% improvement in FID over the state-of-the-art GANs. Additionally, the IGAN model attains an Inception Score (IS) of 9.27 and 68.25, reflecting improved image diversity and generation quality. Finally, the two techniques of dropout and spectral normalization are utilized in both the generator and discriminator structures to further mitigate gradient explosion and overfitting. These findings confirm that the IGAN model potentially balances training stability with image generation quality, constituting a scalable and computationally efficient framework for high-fidelity image synthesis.

</details>


### [43] [Tissue Classification and Whole-Slide Images Analysis via Modeling of the Tumor Microenvironment and Biological Pathways](https://arxiv.org/abs/2601.08336)
*Junzhuo Liu,Xuemei Du,Daniel Reisenbuchler,Ye Chen,Markus Eckstein,Christian Matek,Friedrich Feuerhake,Dorit Merhof*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Automatic integration of whole slide images (WSIs) and gene expression profiles has demonstrated substantial potential in precision clinical diagnosis and cancer progression studies. However, most existing studies focus on individual gene sequences and slide level classification tasks, with limited attention to spatial transcriptomics and patch level applications. To address this limitation, we propose a multimodal network, BioMorphNet, which automatically integrates tissue morphological features and spatial gene expression to support tissue classification and differential gene analysis. For considering morphological features, BioMorphNet constructs a graph to model the relationships between target patches and their neighbors, and adjusts the response strength based on morphological and molecular level similarity, to better characterize the tumor microenvironment. In terms of multimodal interactions, BioMorphNet derives clinical pathway features from spatial transcriptomic data based on a predefined pathway database, serving as a bridge between tissue morphology and gene expression. In addition, a novel learnable pathway module is designed to automatically simulate the biological pathway formation process, providing a complementary representation to existing clinical pathways. Compared with the latest morphology gene multimodal methods, BioMorphNet's average classification metrics improve by 2.67%, 5.48%, and 6.29% for prostate cancer, colorectal cancer, and breast cancer datasets, respectively. BioMorphNet not only classifies tissue categories within WSIs accurately to support tumor localization, but also analyzes differential gene expression between tissue categories based on prediction confidence, contributing to the discovery of potential tumor biomarkers.

</details>


### [44] [From Local Windows to Adaptive Candidates via Individualized Exploratory: Rethinking Attention for Image Super-Resolution](https://arxiv.org/abs/2601.08341)
*Chunyu Meng,Wei Long,Shuhang Gu*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Single Image Super-Resolution (SISR) is a fundamental computer vision task that aims to reconstruct a high-resolution (HR) image from a low-resolution (LR) input. Transformer-based methods have achieved remarkable performance by modeling long-range dependencies in degraded images. However, their feature-intensive attention computation incurs high computational cost. To improve efficiency, most existing approaches partition images into fixed groups and restrict attention within each group. Such group-wise attention overlooks the inherent asymmetry in token similarities, thereby failing to enable flexible and token-adaptive attention computation. To address this limitation, we propose the Individualized Exploratory Transformer (IET), which introduces a novel Individualized Exploratory Attention (IEA) mechanism that allows each token to adaptively select its own content-aware and independent attention candidates. This token-adaptive and asymmetric design enables more precise information aggregation while maintaining computational efficiency. Extensive experiments on standard SR benchmarks demonstrate that IET achieves state-of-the-art performance under comparable computational complexity.

</details>


### [45] [Semantic Misalignment in Vision-Language Models under Perceptual Degradation](https://arxiv.org/abs/2601.08355)
*Guo Cheng*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Vision-Language Models (VLMs) are increasingly deployed in autonomous driving and embodied AI systems, where reliable perception is critical for safe semantic reasoning and decision-making. While recent VLMs demonstrate strong performance on multimodal benchmarks, their robustness to realistic perception degradation remains poorly understood. In this work, we systematically study semantic misalignment in VLMs under controlled degradation of upstream visual perception, using semantic segmentation on the Cityscapes dataset as a representative perception module. We introduce perception-realistic corruptions that induce only moderate drops in conventional segmentation metrics, yet observe severe failures in downstream VLM behavior, including hallucinated object mentions, omission of safety-critical entities, and inconsistent safety judgments. To quantify these effects, we propose a set of language-level misalignment metrics that capture hallucination, critical omission, and safety misinterpretation, and analyze their relationship with segmentation quality across multiple contrastive and generative VLMs. Our results reveal a clear disconnect between pixel-level robustness and multimodal semantic reliability, highlighting a critical limitation of current VLM-based systems and motivating the need for evaluation frameworks that explicitly account for perception uncertainty in safety-critical applications.

</details>


### [46] [Geo-NVS-w: Geometry-Aware Novel View Synthesis In-the-Wild with an SDF Renderer](https://arxiv.org/abs/2601.08371)
*Anastasios Tsalakopoulos,Angelos Kanlis,Evangelos Chatzis,Antonis Karakottas,Dimitrios Zarpalas*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We introduce Geo-NVS-w, a geometry-aware framework for high-fidelity novel view synthesis from unstructured, in-the-wild image collections. While existing in-the-wild methods already excel at novel view synthesis, they often lack geometric grounding on complex surfaces, sometimes producing results that contain inconsistencies. Geo-NVS-w addresses this limitation by leveraging an underlying geometric representation based on a Signed Distance Function (SDF) to guide the rendering process. This is complemented by a novel Geometry-Preservation Loss which ensures that fine structural details are preserved. Our framework achieves competitive rendering performance, while demonstrating a 4-5x reduction reduction in energy consumption compared to similar methods. We demonstrate that Geo-NVS-w is a robust method for in-the-wild NVS, yielding photorealistic results with sharp, geometrically coherent details.

</details>


### [47] [Source-Free Domain Adaptation for Geospatial Point Cloud Semantic Segmentation](https://arxiv.org/abs/2601.08375)
*Yuan Gao,Di Cao,Xiaohuan Xi,Sheng Nie,Shaobo Xia,Cheng Wang*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Semantic segmentation of 3D geospatial point clouds is pivotal for remote sensing applications. However, variations in geographic patterns across regions and data acquisition strategies induce significant domain shifts, severely degrading the performance of deployed models. Existing domain adaptation methods typically rely on access to source-domain data. However, this requirement is rarely met due to data privacy concerns, regulatory policies, and data transmission limitations. This motivates the largely underexplored setting of source-free unsupervised domain adaptation (SFUDA), where only a pretrained model and unlabeled target-domain data are available. In this paper, we propose LoGo (Local-Global Dual-Consensus), a novel SFUDA framework specifically designed for geospatial point clouds. At the local level, we introduce a class-balanced prototype estimation module that abandons conventional global threshold filtering in favor of an intra-class independent anchor mining strategy. This ensures that robust feature prototypes can be generated even for sample-scarce tail classes, effectively mitigating the feature collapse caused by long-tailed distributions. At the global level, we introduce an optimal transport-based global distribution alignment module that formulates pseudo-label assignment as a global optimization problem. By enforcing global distribution constraints, this module effectively corrects the over-dominance of head classes inherent in local greedy assignments, preventing model predictions from being severely biased towards majority classes. Finally, we propose a dual-consistency pseudo-label filtering mechanism. This strategy retains only high-confidence pseudo-labels where local multi-augmented ensemble predictions align with global optimal transport assignments for self-training.

</details>


### [48] [An Explainable Two Stage Deep Learning Framework for Pericoronitis Assessment in Panoramic Radiographs Using YOLOv8 and ResNet-50](https://arxiv.org/abs/2601.08401)
*Ajo Babu George,Pranav S,Kunal Agarwal*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Objectives: To overcome challenges in diagnosing pericoronitis on panoramic radiographs, an AI-assisted assessment system integrating anatomical localization, pathological classification, and interpretability. Methods: A two-stage deep learning pipeline was implemented. The first stage used YOLOv8 to detect third molars and classify their anatomical positions and angulations based on Winter's classification. Detected regions were then fed into a second-stage classifier, a modified ResNet-50 architecture, for detecting radiographic features suggestive of pericoronitis. To enhance clinical trust, Grad-CAM was used to highlight key diagnostic regions on the radiographs. Results: The YOLOv8 component achieved 92% precision and 92.5% mean average precision. The ResNet-50 classifier yielded F1-scores of 88% for normal cases and 86% for pericoronitis. Radiologists reported 84% alignment between Grad-CAM and their diagnostic impressions, supporting the radiographic relevance of the interpretability output. Conclusion: The system shows strong potential for AI-assisted panoramic assessment, with explainable AI features that support clinical confidence.

</details>


### [49] [Edge-Optimized Multimodal Learning for UAV Video Understanding via BLIP-2](https://arxiv.org/abs/2601.08408)
*Yizhan Feng,Hichem Snoussi,Jing Teng,Jian Liu,Yuyang Wang,Abel Cherouat,Tian Wang*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The demand for real-time visual understanding and interaction in complex scenarios is increasingly critical for unmanned aerial vehicles. However, a significant challenge arises from the contradiction between the high computational cost of large Vision language models and the limited computing resources available on UAV edge devices. To address this challenge, this paper proposes a lightweight multimodal task platform based on BLIP-2, integrated with YOLO-World and YOLOv8-Seg models. This integration extends the multi-task capabilities of BLIP-2 for UAV applications with minimal adaptation and without requiring task-specific fine-tuning on drone data. Firstly, the deep integration of BLIP-2 with YOLO models enables it to leverage the precise perceptual results of YOLO for fundamental tasks like object detection and instance segmentation, thereby facilitating deeper visual-attention understanding and reasoning. Secondly, a content-aware key frame sampling mechanism based on K-Means clustering is designed, which incorporates intelligent frame selection and temporal feature concatenation. This equips the lightweight BLIP-2 architecture with the capability to handle video-level interactive tasks effectively. Thirdly, a unified prompt optimization scheme for multi-task adaptation is implemented. This scheme strategically injects structured event logs from the YOLO models as contextual information into BLIP-2's input. Combined with output constraints designed to filter out technical details, this approach effectively guides the model to generate accurate and contextually relevant outputs for various tasks.

</details>


### [50] [SPARK: Scalable Real-Time Point Cloud Aggregation with Multi-View Self-Calibration](https://arxiv.org/abs/2601.08414)
*Chentian Sun*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Real-time multi-camera 3D reconstruction is crucial for 3D perception, immersive interaction, and robotics. Existing methods struggle with multi-view fusion, camera extrinsic uncertainty, and scalability for large camera setups. We propose SPARK, a self-calibrating real-time multi-camera point cloud reconstruction framework that jointly handles point cloud fusion and extrinsic uncertainty. SPARK consists of: (1) a geometry-aware online extrinsic estimation module leveraging multi-view priors and enforcing cross-view and temporal consistency for stable self-calibration, and (2) a confidence-driven point cloud fusion strategy modeling depth reliability and visibility at pixel and point levels to suppress noise and view-dependent inconsistencies. By performing frame-wise fusion without accumulation, SPARK produces stable point clouds in dynamic scenes while scaling linearly with the number of cameras. Extensive experiments on real-world multi-camera systems show that SPARK outperforms existing approaches in extrinsic accuracy, geometric consistency, temporal stability, and real-time performance, demonstrating its effectiveness and scalability for large-scale multi-camera 3D reconstruction.

</details>


### [51] [MMLGNet: Cross-Modal Alignment of Remote Sensing Data using CLIP](https://arxiv.org/abs/2601.08420)
*Aditya Chaudhary,Sneha Barman,Mainak Singha,Ankit Jha,Girish Mishra,Biplab Banerjee*

Main category: cs.CV

TL;DR: 提出了一种名为MMLGNet的新颖的多模态框架，利用基于CLIP的双向对比学习方法，在共享的潜在空间中对特定模态的编码器进行训练，以实现高维遥感数据与语言引导解释之间的桥梁，获得两个基准数据集上优于多种视图仅方法的性能。


<details>
  <summary>Details</summary>
Motivation: 随着多模态地球观测数据的不断涌入，需要一种有效融合光谱、空间及几何信息的方法，同时实现语义级别的理解。

Method: MMLGNet 使用特定模态的编码器，并通过双向对比学习方法对视觉特征与手工构建的文本嵌入在共享的潜在空间中进行对齐。

Result: MMLGNet 以简单的基于CNN的编码器实现了较强的性能，在两个基准数据集上表现优于多种现有的视图仅方法。

Conclusion: MMLGNet 提供了一种有效的多模态方法，能够支持光谱、空间和几何信息的融合，同时利用语言引导实现语义理解，且具有良好的应用潜力。

Abstract: In this paper, we propose a novel multimodal framework, Multimodal Language-Guided Network (MMLGNet), to align heterogeneous remote sensing modalities like Hyperspectral Imaging (HSI) and LiDAR with natural language semantics using vision-language models such as CLIP. With the increasing availability of multimodal Earth observation data, there is a growing need for methods that effectively fuse spectral, spatial, and geometric information while enabling semantic-level understanding. MMLGNet employs modality-specific encoders and aligns visual features with handcrafted textual embeddings in a shared latent space via bi-directional contrastive learning. Inspired by CLIP's training paradigm, our approach bridges the gap between high-dimensional remote sensing data and language-guided interpretation. Notably, MMLGNet achieves strong performance with simple CNN-based encoders, outperforming several established multimodal visual-only methods on two benchmark datasets, demonstrating the significant benefit of language supervision. Codes are available at https://github.com/AdityaChaudhary2913/CLIP_HSI.

</details>


### [52] [Deep Learning Based Facial Retargeting Using Local Patches](https://arxiv.org/abs/2601.08429)
*Yeonsoo Choi,Inyup Lee,Sihun Cha,Seonghyeon Kim,Sunjin Jung,Junyong Noh*

Main category: cs.CV

TL;DR: 该研究提出了一种基于局部补丁的面部动画转移方法，用于将来源视频中的面部动画转移到具有显著面部特征比例差异的拟人3D角色。


<details>
  <summary>Details</summary>
Motivation: 在数字动画时代，为了在虚拟角色中产生真实的面部动画，提出了各种重新瞄准方法。然而，对于偏离人类面部结构的拟人或夸张的3D角色，重新瞄准面部运动时会遇到挑战。因此，研究提出了一个局部补丁方法来保留重新瞄准后原面部动作的语义。

Method: 该方法分为三个模块：自动补丁提取模块、再现模块和权重估计模块。自动补丁提取模块从源视频帧中提取局部补丁，这些补丁通过再现模块生成相应的目标局部补贴，最后通过权重估计模块计算目标角色在每一帧的动画参数，以创建完整的面部动画序列。

Result: 实验结果表明，该方法能够成功地将源面部表情的语义意义转移到具有显著面部特征比例差异的拟人角色。

Conclusion: 研究提出了一种有效的面部动画重新瞄准方法，能够在显著差异的面部特征比例的角色上保持源面部动作的语义。

Abstract: In the era of digital animation, the quest to produce lifelike facial animations for virtual characters has led to the development of various retargeting methods. While the retargeting facial motion between models of similar shapes has been very successful, challenges arise when the retargeting is performed on stylized or exaggerated 3D characters that deviate significantly from human facial structures. In this scenario, it is important to consider the target character's facial structure and possible range of motion to preserve the semantics assumed by the original facial motions after the retargeting. To achieve this, we propose a local patch-based retargeting method that transfers facial animations captured in a source performance video to a target stylized 3D character. Our method consists of three modules. The Automatic Patch Extraction Module extracts local patches from the source video frame. These patches are processed through the Reenactment Module to generate correspondingly re-enacted target local patches. The Weight Estimation Module calculates the animation parameters for the target character at every frame for the creation of a complete facial animation sequence. Extensive experiments demonstrate that our method can successfully transfer the semantic meaning of source facial expressions to stylized characters with considerable variations in facial feature proportion.

</details>


### [53] [Incentivizing Cardiologist-Like Reasoning in MLLMs for Interpretable Echocardiographic Diagnosis](https://arxiv.org/abs/2601.08440)
*Yi Qin,Lehan Wang,Chenxu Zhao,Alex P. W. Lee,Xiaomeng Li*

Main category: cs.CV

TL;DR: 本文提出了一种新的方法，结合心脏推理模板（CRT）和心脏心智（CardiacMind）以增强医疗推理多模态大型语言模型的超声心动图诊断能力，通过对复杂心脏疾病的逐步标准化诊断流程和引入三种奖励机制来实现。


<details>
  <summary>Details</summary>
Motivation: 现有超声心动图基础模型无法有效捕捉定量测量与临床表现之间的关系，而医疗推理多模态大型语言模型虽然可以进行多模态推理，但需要大量资源构建详细的推理路径，并且对超声心动图先验知识的直接整合效果不佳。

Method: 本文提出了心脏推理模板（CRT）和心脏心智（CardiacMind）。CRT提供了一种逐步执行复杂心脏疾病诊断的标准步骤，而不需要每次验证前例以简化推理路径的构建。CardiacMind是新的强化学习方案，包含三种奖励机制：过程量奖励（PQtR）、过程质量奖励（PQlR）和超声心动图语义奖励（ESR）。这些奖励机制分别促进详细的推理、证据的整合以及逐步描述的视觉内容衔接。

Result: 通过我们的方法，我们在15种复杂心脏疾病的多视角超声心动图诊断中取得了48%的提升，且在CardiacNet-PAH上实现了5%的改善。用户研究显示，有93.33%的临床同意我们的推理输出与心脏病专家的推理逻辑相符。

Conclusion: 本文提出的方法显著增强了基于卡尺的心脏推理模板（CRT）和医疗推理多模态大型语言模型（CardiacMind）的超声心动图诊断能力。这一方法的结果表明，通过高效简化推理路径构建并激励多模态语义推理方法，可以有效提高心脏病学应用的诊断效率和准确性。

Abstract: Echocardiographic diagnosis is vital for cardiac screening yet remains challenging. Existing echocardiography foundation models do not effectively capture the relationships between quantitative measurements and clinical manifestations, whereas medical reasoning multimodal large language models (MLLMs) require costly construction of detailed reasoning paths and remain ineffective at directly incorporating such echocardiographic priors into their reasoning. To address these limitations, we propose a novel approach comprising Cardiac Reasoning Template (CRT) and CardiacMind to enhance MLLM's echocardiographic reasoning by introducing cardiologist-like mindset. Specifically, CRT provides stepwise canonical diagnostic procedures for complex cardiac diseases to streamline reasoning path construction without the need for costly case-by-case verification. To incentivize reasoning MLLM under CRT, we develop CardiacMind, a new reinforcement learning scheme with three novel rewards: Procedural Quantity Reward (PQtR), Procedural Quality Reward (PQlR), and Echocardiographic Semantic Reward (ESR). PQtR promotes detailed reasoning; PQlR promotes integration of evidence across views and modalities, while ESR grounds stepwise descriptions in visual content. Our methods show a 48% improvement in multiview echocardiographic diagnosis for 15 complex cardiac diseases and a 5% improvement on CardiacNet-PAH over prior methods. The user study on our method's reasoning outputs shows 93.33% clinician agreement with cardiologist-like reasoning logic. Our code will be available.

</details>


### [54] [Noise-Adaptive Regularization for Robust Multi-Label Remote Sensing Image Classification](https://arxiv.org/abs/2601.08446)
*Tom Burgert,Julia Henkel,Begüm Demir*

Main category: cs.CV

TL;DR: 文章提出了一种名为NAR的噪声适应正则化方法，用于多标签分类中的噪声标签处理。NAR通过动态处理标签置信度，抑制中等置信度的标签，矫正低置信度标签，与早学习正则化结合使用，以提高模型在噪声数据上的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 多标签分类在遥感数据处理中成为了研究热点，但当前方法未能区分不同的噪声类型，忽视了噪声对模型性能的影响。因此，该研究旨在提出一种能够适应不同噪声类型的正则化方法。

Method: NAR方法通过采用基于置信度的标签处理机制，动态保留高置信度标签，在中等置信度标签上暂时失效，并通过翻转等手段矫正低置信度标签。此外，该方法结合了早期学习正则化，以稳定训练并防止过度拟合。

Result: 实验结果表明，NAR方法在添加、删除和混合噪声场景中均表现出了更高的鲁棒性，特别在删除和混合噪声场景下的表现最为突出，这表明NAR方法有效地减少和纠正了噪声监督。

Conclusion: NAR方法能够根据不同类型的噪声性质调整学习行为，提供了有效的噪声鲁棒学习策略，从而有助于提高遥感数据中多标签分类的准确性。

Abstract: The development of reliable methods for multi-label classification (MLC) has become a prominent research direction in remote sensing (RS). As the scale of RS data continues to expand, annotation procedures increasingly rely on thematic products or crowdsourced procedures to reduce the cost of manual annotation. While cost-effective, these strategies often introduce multi-label noise in the form of partially incorrect annotations. In MLC, label noise arises as additive noise, subtractive noise, or a combination of both in the form of mixed noise. Previous work has largely overlooked this distinction and commonly treats noisy annotations as supervised signals, lacking mechanisms that explicitly adapt learning behavior to different noise types. To address this limitation, we propose NAR, a noise-adaptive regularization method that explicitly distinguishes between additive and subtractive noise within a semi-supervised learning framework. NAR employs a confidence-based label handling mechanism that dynamically retains label entries with high confidence, temporarily deactivates entries with moderate confidence, and corrects low confidence entries via flipping. This selective attenuation of supervision is integrated with early-learning regularization (ELR) to stabilize training and mitigate overfitting to corrupted labels. Experiments across additive, subtractive, and mixed noise scenarios demonstrate that NAR consistently improves robustness compared with existing methods. Performance improvements are most pronounced under subtractive and mixed noise, indicating that adaptive suppression and selective correction of noisy supervision provide an effective strategy for noise robust learning in RS MLC.

</details>


### [55] [Divide and Conquer: Static-Dynamic Collaboration for Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2601.08448)
*Kexin Bao,Daichi Zhang,Yong Li,Dan Zeng,Shiming Ge*

Main category: cs.CV

TL;DR: 该研究提出了一种名为静态-动态协作（SDC）的框架，将Few-shot分类增量学习(FSCIL)任务分为保留稳定阶段（SRS）和动态学习阶段（DLS），以平衡保留旧知识与获取新知识的矛盾。


<details>
  <summary>Details</summary>
Motivation: FSCIL面临稳定-塑性难题，即在有限数据下如何平衡保留旧知识和获取新知识。SDC框架通过将任务分为两阶段来解决这一问题，以实现更好的稳定性和塑性之间的贸易。

Method: SDC框架将FSCIL任务分为静态保留阶段（SRS）和动态学习阶段（DLS）。在SRS阶段，使用充足的数据训练初始模型，并保持模型的关键部分作为静态记忆以保留基本的旧知识。在DLS阶段，引入额外的动态投影器与之前的静态记忆一起训练。

Result: 通过两个阶段的配合，该方法在保留旧知识的同时能够持续适应新的类别。在三个公开基准和一个实际应用数据集上的大规模实验中，该方法达到了最先进的性能。

Conclusion: 该研究提出了一种有效的FSCIL解决方案，能够同时保持已有的知识并不断学习新的知识，从而为基于少量样本的分类增量学习提供了新的思路。

Abstract: Few-shot class-incremental learning (FSCIL) aims to continuously recognize novel classes under limited data, which suffers from the key stability-plasticity dilemma: balancing the retention of old knowledge with the acquisition of new knowledge. To address this issue, we divide the task into two different stages and propose a framework termed Static-Dynamic Collaboration (SDC) to achieve a better trade-off between stability and plasticity. Specifically, our method divides the normal pipeline of FSCIL into Static Retaining Stage (SRS) and Dynamic Learning Stage (DLS), which harnesses old static and incremental dynamic class information, respectively. During SRS, we train an initial model with sufficient data in the base session and preserve the key part as static memory to retain fundamental old knowledge. During DLS, we introduce an extra dynamic projector jointly trained with the previous static memory. By employing both stages, our method achieves improved retention of old knowledge while continuously adapting to new classes. Extensive experiments on three public benchmarks and a real-world application dataset demonstrate that our method achieves state-of-the-art performance against other competitors.

</details>


### [56] [Developing Predictive and Robust Radiomics Models for Chemotherapy Response in High-Grade Serous Ovarian Carcinoma](https://arxiv.org/abs/2601.08455)
*Sepideh Hatamikia,Geevarghese George,Florian Schwarzhans,Amirreza Mahbod,Marika AV Reinius,Ali Abbasian Ardakani,Mercedes Jimenez-Linan,Satish Viswanath,Mireia Crispin-Ortuzar,Lorena Escudero Sanchez,Evis Sala,James D Brenton,Ramona Woitek*

Main category: cs.CV

TL;DR: 本研究通过结合结构化影像学特征选择方法，改善了对于IIIC期卵巢癌患者新辅助化疗反应的预测，尤其是在肿瘤体积减少和化疗反应评分方面的预测。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过集成不同的特征选择方法，提高恶性程度高且常在晚期被诊断的卵巢癌（HGSOC）患者在新辅助化疗（NACT）后的反应预测准确性，从而改善治疗策略。

Method: 研究采用了一种自动随机化算法来模拟观察者间的差异，并在此基础上进行了特征选择。使用了四种响应指标（化疗反应评分CRS，RECIST标准，体积减少VolR，直径减少DiaR）进行建模和测试，分别在头盆腔和网膜处的病变上进行分析。

Result: 结合所有病变的体积减少（VolR）预测表现最佳，AUC为0.83。网膜病变对于化疗反应评分的预测效果最好，AUC为0.77；而盆腔病变在直径减少（DiaR）预测上表现最佳，AUC为0.76。

Conclusion: 该研究结果表明，将稳健性纳入特征选择过程有助于构建可靠的模型，这为在临床应用中实施影像学模型提供了理论支持，并提出了进一步在卵巢癌中探索影像学应用的建议，特别是在实际临床环境中。

Abstract: Objectives: High-grade serous ovarian carcinoma (HGSOC) is typically diagnosed at an advanced stage with extensive peritoneal metastases, making treatment challenging. Neoadjuvant chemotherapy (NACT) is often used to reduce tumor burden before surgery, but about 40% of patients show limited response. Radiomics, combined with machine learning (ML), offers a promising non-invasive method for predicting NACT response by analyzing computed tomography (CT) imaging data. This study aimed to improve response prediction in HGSOC patients undergoing NACT by integration different feature selection methods. Materials and methods: A framework for selecting robust radiomics features was introduced by employing an automated randomisation algorithm to mimic inter-observer variability, ensuring a balance between feature robustness and prediction accuracy. Four response metrics were used: chemotherapy response score (CRS), RECIST, volume reduction (VolR), and diameter reduction (DiaR). Lesions in different anatomical sites were studied. Pre- and post-NACT CT scans were used for feature extraction and model training on one cohort, and an independent cohort was used for external testing. Results: The best prediction performance was achieved using all lesions combined for VolR prediction, with an AUC of 0.83. Omental lesions provided the best results for CRS prediction (AUC 0.77), while pelvic lesions performed best for DiaR (AUC 0.76). Conclusion: The integration of robustness into the feature selection processes ensures the development of reliable models and thus facilitates the implementation of the radiomics models in clinical applications for HGSOC patients. Future work should explore further applications of radiomics in ovarian cancer, particularly in real-time clinical settings.

</details>


### [57] [Modality-Decoupled RGB-Thermal Object Detector via Query Fusion](https://arxiv.org/abs/2601.08458)
*Chao Tian,Zikun Zhou,Chao Yang,Guoqing Zhu,Fu'an Zhong,Zhenyu He*

Main category: cs.CV

TL;DR: 文章提出了一种结合RGB和热成像的检测框架MDQF，通过在各细化阶段采用查询融合方法并分离模态，提高了在极端条件下检测的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在极端条件下，单一模态的质量可能下降并干扰检测。因此，需要一种能够在保持互补信息的同时分离模态的框架，以提高鲁棒性。

Method: 文章提出了一种名为MDQF的模态解耦RGB-热成像检测框架，利用DETR结构的检测器分别处理RGB和热成像数据，在每个细化阶段插入查询融合。该设计通过使用高质量模态的查询来排除低质量模态并改进检测。

Result: 在广泛实验中，该方法的性能优于现有的RGB-热成像检测器，并且实现了更好的模态独立性。

Conclusion: 此工作提出了一种有效的RGB-热成像检测方法，适用于复杂光照和天气条件，并且能够处理单模态质量下降的情况。

Abstract: The advantage of RGB-Thermal (RGB-T) detection lies in its ability to perform modality fusion and integrate cross-modality complementary information, enabling robust detection under diverse illumination and weather conditions. However, under extreme conditions where one modality exhibits poor quality and disturbs detection, modality separation is necessary to mitigate the impact of noise. To address this problem, we propose a Modality-Decoupled RGB-T detection framework with Query Fusion (MDQF) to balance modality complementation and separation. In this framework, DETR-like detectors are employed as separate branches for the RGB and TIR images, with query fusion interspersed between the two branches in each refinement stage. Herein, query fusion is performed by feeding the high-quality queries from one branch to the other one after query selection and adaptation. This design effectively excludes the degraded modality and corrects the predictions using high-quality queries. Moreover, the decoupled framework allows us to optimize each individual branch with unpaired RGB or TIR images, eliminating the need for paired RGB-T data. Extensive experiments demonstrate that our approach delivers superior performance to existing RGB-T detectors and achieves better modality independence.

</details>


### [58] [CoMa: Contextual Massing Generation with Vision-Language Models](https://arxiv.org/abs/2601.08464)
*Evgenii Maslov,Valentin Khrulkov,Anastasia Volkova,Anton Gusarov,Andrey Kuznetsov,Ivan Oseledets*

Main category: cs.CV

TL;DR: 本文提出了一种自动化框架，用于基于功能需求和场地背景自动生成建筑形体，并介绍了CoMa-20K数据集，该集齐包含了详细的形体几何、经济信息和项目背景视觉信息，通过Vision-Language模型进行基准测试，展示了这些模型在生成上下文感知建筑形体方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 当前建筑设计中的概念设计阶段复杂且依赖于设计师的直觉和手工操作，因此本文旨在通过数据驱动的方法降低这一过程的复杂性。

Method: 本文通过收集详细的建筑设计数据并创建CoMa-20K数据集，使用Vision-Language模型进行形体生成任务的基准测试。

Result: 通过对比细调和零样本模型的结果，本文揭示了任务的复杂性，并展示了Vision-Language模型在生成上下文感知建筑形体方面的潜力。

Conclusion: 本文建立了一种新的数据驱动的建筑形体生成基准，并为未来研究指明了方向。

Abstract: The conceptual design phase in architecture and urban planning, particularly building massing, is complex and heavily reliant on designer intuition and manual effort. To address this, we propose an automated framework for generating building massing based on functional requirements and site context. A primary obstacle to such data-driven methods has been the lack of suitable datasets. Consequently, we introduce the CoMa-20K dataset, a comprehensive collection that includes detailed massing geometries, associated economical and programmatic data, and visual representations of the development site within its existing urban context. We benchmark this dataset by formulating massing generation as a conditional task for Vision-Language Models (VLMs), evaluating both fine-tuned and large zero-shot models. Our experiments reveal the inherent complexity of the task while demonstrating the potential of VLMs to produce context-sensitive massing options. The dataset and analysis establish a foundational benchmark and highlight significant opportunities for future research in data-driven architectural design.

</details>


### [59] [Zero-Shot Distracted Driver Detection via Vision Language Models with Double Decoupling](https://arxiv.org/abs/2601.08467)
*Takamichi Miyata,Sumiko Miyata,Andrew Morris*

Main category: cs.CV

TL;DR: 本文提出了一种去耦合框架，通过提取驾驶员外观嵌入并将其从图像嵌入中分离出来，以强调与分心驾驶相关的证据。这种方法在实验中展示了相对于先前基线的一致改进，显示出应用于实际道路交通安全的潜力。


<details>
  <summary>Details</summary>
Motivation: 针对基于视觉-语言模型（VLM）的分心驾驶检测方法在实际应用中的不足，本文旨在通过去耦合框架改进针对驾驶行为的检测，解决驾驶员特定外观特征对行为检测的影响问题。

Method: 本文提出了一种包含两步去耦合过程的方法。首先，从图像中提取驾驶员的外观嵌入，并在零样本分类之前将其影响从图像嵌入中去除。其次，通过在Stiefel流形上进行度量投影来正交化文本嵌入，以提高文本和图像嵌入之间的可分性。

Result: 本文的方法在实验中展示了相对于先前基线的一致改进，特别是在去除驾驶员特定外观特征影响后的零样本分类任务上取得了显著的好转。

Conclusion: 本文提出的方法为实际道路交通中的分心驾驶检测提供了一种新的解决方案，并在实际应用中显示了其有效性和潜力。

Abstract: Distracted driving is a major cause of traffic collisions, calling for robust and scalable detection methods. Vision-language models (VLMs) enable strong zero-shot image classification, but existing VLM-based distracted driver detectors often underperform in real-world conditions. We identify subject-specific appearance variations (e.g., clothing, age, and gender) as a key bottleneck: VLMs entangle these factors with behavior cues, leading to decisions driven by who the driver is rather than what the driver is doing. To address this, we propose a subject decoupling framework that extracts a driver appearance embedding and removes its influence from the image embedding prior to zero-shot classification, thereby emphasizing distraction-relevant evidence. We further orthogonalize text embeddings via metric projection onto Stiefel manifold to improve separability while staying close to the original semantics. Experiments demonstrate consistent gains over prior baselines, indicating the promise of our approach for practical road-safety applications.

</details>


### [60] [Towards Safer Mobile Agents: Scalable Generation and Evaluation of Diverse Scenarios for VLMs](https://arxiv.org/abs/2601.08470)
*Takara Taniguchi,Kuniaki Saito,Atsushi Hashimoto*

Main category: cs.CV

TL;DR: HazardForge是一个用于生成危险场景的可扩展管道，结合了图像编辑模型和布局决策算法，用于测试VLMs在处理复杂和异常的环境动态时的能力。


<details>
  <summary>Details</summary>
Motivation: 当前的基准测试工具无法全面覆盖复杂和异常场景，尤其缺乏对动态空间-时间动态情况的评估，因此引入HazardForge来提高测试的全面性。

Method: HazardForge通过使用图像编辑模型生成详细的场景，并结合布局决策算法进行验证，确保生成真实世界中常见的移动、侵入性和远距离物体。

Result: 使用MovSafeBench，一种由7,254张图像和相应的QA问题对组成的多选题基准测试，研究结果表明，在存在异常物体的情况下，VLMs的表现显著下降，特别是在需要复杂的运动理解情况下的表现下降最大。

Conclusion: 该研究证明了HazardForge的有效性，并提出了一个新的基准测试工具MovSafeBench，用于评估VLMs在复杂环境中的表现，尤其是在处理异常和动态场景方面。

Abstract: Vision Language Models (VLMs) are increasingly deployed in autonomous vehicles and mobile systems, making it crucial to evaluate their ability to support safer decision-making in complex environments. However, existing benchmarks inadequately cover diverse hazardous situations, especially anomalous scenarios with spatio-temporal dynamics. While image editing models are a promising means to synthesize such hazards, it remains challenging to generate well-formulated scenarios that include moving, intrusive, and distant objects frequently observed in the real world. To address this gap, we introduce \textbf{HazardForge}, a scalable pipeline that leverages image editing models to generate these scenarios with layout decision algorithms, and validation modules. Using HazardForge, we construct \textbf{MovSafeBench}, a multiple-choice question (MCQ) benchmark comprising 7,254 images and corresponding QA pairs across 13 object categories, covering both normal and anomalous objects. Experiments using MovSafeBench show that VLM performance degrades notably under conditions including anomalous objects, with the largest drop in scenarios requiring nuanced motion understanding.

</details>


### [61] [An IoT-Enabled Smart Aquarium System for Real-Time Water Quality Monitoring and Automated Feeding](https://arxiv.org/abs/2601.08484)
*MD Fatin Ishraque Ayon,Sabrin Nahar,Ataur Rahman,Md. Taslim Arif,Abdul Hasib,A. S. M. Ahsanul Sarkar Akib*

Main category: cs.CV

TL;DR: 该研究提出了一种基于物联网的智能水族箱系统，该系统通过集成多个传感器和执行器，实现全面的水质量实时监控和自动控制。实验结果显示，系统在10升水族箱环境中具有高传感器准确性和快速响应时间，并维持了高效的自动化喂食和水循环功能。


<details>
  <summary>Details</summary>
Motivation: 传统的人工检测方法存在效率低下、劳动密集型且易出错的问题，可能导致水质不佳。为解决此问题，本研究开发了一种基于物联网的智能水族箱系统，旨在提高水族箱水质监测和控制的可靠性与效率。

Method: 本研究通过使用ESP32微控制器与多种传感器（pH、TDS、温度、浑浊度）和执行器（伺服喂食器、水泵）相结合，集成边缘处理能力、云连接和智能警报机制，并通过Blynk物联网平台进行远程控制。

Result: 实验结果显示，该系统在10升水族箱环境中实现了96%的平均传感器准确性和1.2秒的异常检测响应时间。此外，自动化喂食和水循环模块在长时间测试中保持了97%的操作可靠性，显著减少了人工干预，确保了稳定的水族箱生态系统。

Conclusion: 该研究证明，低成本的物联网解决方案可以彻底改变水族箱的维护方式，使水族生态系统管理更加便捷、可靠和高效，适用于住宅和商业应用。

Abstract: Maintaining optimal water quality in aquariums is critical for aquatic health but remains challenging due to the need for continuous monitoring of multiple parameters. Traditional manual methods are inefficient, labor-intensive, and prone to human error, often leading to suboptimal aquatic conditions. This paper presents an IoT-based smart aquarium system that addresses these limitations by integrating an ESP32 microcontroller with multiple sensors (pH, TDS, temperature, turbidity) and actuators (servo feeder, water pump) for comprehensive real-time water quality monitoring and automated control. The system architecture incorporates edge processing capabilities, cloud connectivity via Blynk IoT platform, and an intelligent alert mechanism with configurable cooldown periods to prevent notification fatigue. Experimental evaluation in a 10-liter aquarium environment demonstrated the system's effectiveness, achieving 96\% average sensor accuracy and 1.2-second response time for anomaly detection. The automated feeding and water circulation modules maintained 97\% operational reliability throughout extended testing, significantly reducing manual intervention while ensuring stable aquatic conditions. This research demonstrates that cost-effective IoT solutions can revolutionize aquarium maintenance, making aquatic ecosystem management more accessible, reliable, and efficient for both residential and commercial applications.

</details>


### [62] [EfficientFSL: Enhancing Few-Shot Classification via Query-Only Tuning in Vision Transformers](https://arxiv.org/abs/2601.08499)
*Wenwen Liao,Hang Ruan*

Main category: cs.CV

TL;DR: EfficientFSL 是一种专门针对 ViT 在少样本分类中的查询式微调框架，通过引入 Lightweight Trainable Forward Block、Combine Block 和 Support-Query Attention Block，实现高性能的同时大大减少了计算资源消耗。


<details>
  <summary>Details</summary>
Motivation: 针对大模型（如 ViT）在少样本分类任务中的强表现但计算资源要求高的问题，提出 EfficientFSL，旨在提供一种高效且低资源需求的解决方法。

Method: 提出 Lightweight Trainable Forward Block、Combine Block 和 Support-Query Attention Block，通过查询式微调增强模型的泛化能力和特征表示能力。

Result: EfficientFSL 在四个领域内和六个跨领域少样本数据集上达到了最先进的性能，验证了其在实际应用中的有效性。

Conclusion: EfficientFSL 方案通过减少可训练参数数量，在保证高分类准确性的前提下，显著降低了计算资源的消耗，为未来少样本分类任务中的大模型应用提供了新的思路。

Abstract: Large models such as Vision Transformers (ViTs) have demonstrated remarkable superiority over smaller architectures like ResNet in few-shot classification, owing to their powerful representational capacity. However, fine-tuning such large models demands extensive GPU memory and prolonged training time, making them impractical for many real-world low-resource scenarios. To bridge this gap, we propose EfficientFSL, a query-only fine-tuning framework tailored specifically for few-shot classification with ViT, which achieves competitive performance while significantly reducing computational overhead. EfficientFSL fully leverages the knowledge embedded in the pre-trained model and its strong comprehension ability, achieving high classification accuracy with an extremely small number of tunable parameters. Specifically, we introduce a lightweight trainable Forward Block to synthesize task-specific queries that extract informative features from the intermediate representations of the pre-trained model in a query-only manner. We further propose a Combine Block to fuse multi-layer outputs, enhancing the depth and robustness of feature representations. Finally, a Support-Query Attention Block mitigates distribution shift by adjusting prototypes to align with the query set distribution. With minimal trainable parameters, EfficientFSL achieves state-of-the-art performance on four in-domain few-shot datasets and six cross-domain datasets, demonstrating its effectiveness in real-world applications.

</details>


### [63] [CD^2: Constrained Dataset Distillation for Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2601.08519)
*Kexin Bao,Daichi Zhang,Hansong Zhang,Yong Li,Yutao Yue,Shiming Ge*

Main category: cs.CV

TL;DR: 本文提出了一种名为CD²的框架，旨在解决小样本类别增量学习中的灾难性遗忘问题。该框架包含数据集蒸馏模块（DDM）和蒸馏约束模块（DCM），可以有效保留之前的学习知识。


<details>
  <summary>Details</summary>
Motivation: 现有的小样本类别增量学习方法在保留先前学习的知识方面存在不足，本文旨在通过引入新的框架来解决这一问题，以提高分类性能。

Method: CD²框架包括两个模块：数据集蒸馏模块（DDM）和蒸馏约束模块（DCM）。DDM通过教师模型指导生成高度浓缩的数据样本，让模型从少量增量样本中学习到关键的类别线索。DCM则通过设计的损失函数，约束模型在保留先前学习的知识方面表现更好。

Result: 在三个公共数据集上的大量实验表明，该方法比其他先进的竞争对手具有优势。

Conclusion: 本文提出的方法在处理小样本类别增量学习问题上表现出色，为该领域的研究提供了新的视角和思路。

Abstract: Few-shot class-incremental learning (FSCIL) receives significant attention from the public to perform classification continuously with a few training samples, which suffers from the key catastrophic forgetting problem. Existing methods usually employ an external memory to store previous knowledge and treat it with incremental classes equally, which cannot properly preserve previous essential knowledge. To solve this problem and inspired by recent distillation works on knowledge transfer, we propose a framework termed \textbf{C}onstrained \textbf{D}ataset \textbf{D}istillation (\textbf{CD$^2$}) to facilitate FSCIL, which includes a dataset distillation module (\textbf{DDM}) and a distillation constraint module~(\textbf{DCM}). Specifically, the DDM synthesizes highly condensed samples guided by the classifier, forcing the model to learn compacted essential class-related clues from a few incremental samples. The DCM introduces a designed loss to constrain the previously learned class distribution, which can preserve distilled knowledge more sufficiently. Extensive experiments on three public datasets show the superiority of our method against other state-of-the-art competitors.

</details>


### [64] [VideoHEDGE: Entropy-Based Hallucination Detection for Video-VLMs via Semantic Clustering and Spatiotemporal Perturbations](https://arxiv.org/abs/2601.08557)
*Sushant Gautam,Cise Midoglu,Vajira Thambawita,Michael A. Riegler,Pål Halvorsen*

Main category: cs.CV

TL;DR: 引入VideoHEDGE框架用于视频问答中的幻觉检测，通过扩展熵可靠性估计方法至动态输入，提出了Semantic Entropy (SE)，RadFlag，和Vision-Amplified Semantic Entropy (VASE)三种可靠性评分。在SoccerChat基准上的实验表明，VASE具有最高的ROC-AUC值，且优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 现有的不确定性度量方法在视频语言模型中的幻觉检测上表现不佳与正确性不匹配，因此开发了VideoHEDGE框架来改善这一问题。

Method: VideoHEDGE框架通过提取baseline答案和多个高温生成，利用自然语言推理(NLI) 或嵌入式方法进行语义假设聚类，并计算出（语义熵SE）、RadFlag和视觉增强语义熵VASE三种可靠性分数。

Result: 在SoccerChat基准上的实验表明，VASE在不同场景下（包括增加了扰动的情况下）表现出最高的ROC-AUC值，SE和RadFlag的性能相对较弱。

Conclusion: 相比NLI方法，基于嵌入的聚类方法在检测性能上更为优越且成本更低。领域微调可以减少幻觉出现的频率但仅对校准有轻微改进。VideoHEDGE框架在hedge-bench PyPI库中提供，支持可重复和拓展性验证。

Abstract: Hallucinations in video-capable vision-language models (Video-VLMs) remain frequent and high-confidence, while existing uncertainty metrics often fail to align with correctness. We introduce VideoHEDGE, a modular framework for hallucination detection in video question answering that extends entropy-based reliability estimation from images to temporally structured inputs. Given a video-question pair, VideoHEDGE draws a baseline answer and multiple high-temperature generations from both clean clips and photometrically and spatiotemporally perturbed variants, then clusters the resulting textual outputs into semantic hypotheses using either Natural Language Inference (NLI)-based or embedding-based methods. Cluster-level probability masses yield three reliability scores: Semantic Entropy (SE), RadFlag, and Vision-Amplified Semantic Entropy (VASE). We evaluate VideoHEDGE on the SoccerChat benchmark using an LLM-as-a-judge to obtain binary hallucination labels. Across three 7B Video-VLMs (Qwen2-VL, Qwen2.5-VL, and a SoccerChat-finetuned model), VASE consistently achieves the highest ROC-AUC, especially at larger distortion budgets, while SE and RadFlag often operate near chance. We further show that embedding-based clustering matches NLI-based clustering in detection performance at substantially lower computational cost, and that domain fine-tuning reduces hallucination frequency but yields only modest improvements in calibration. The hedge-bench PyPI library enables reproducible and extensible benchmarking, with full code and experimental resources available at https://github.com/Simula/HEDGE#videohedge .

</details>


### [65] [End-to-End Video Character Replacement without Structural Guidance](https://arxiv.org/abs/2601.08587)
*Zhengbo Xu,Jie Ma,Ziheng Wang,Zhan Peng,Jun Liang,Jing Li*

Main category: cs.CV

TL;DR: MoCha 提出了一种新的框架，通过使用单张任意帧掩码和引入条件感知 RoPE 及 RL 后处理阶段，解决了视频字符替换中先验配对视频数据不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有技术依赖于配对视频数据和显式结构指导，这在复杂场景中表现不佳，MoCha 旨在克服这些局限性。

Method: MoCha 引入了一种新的框架，仅需单张任意帧掩码。采用条件感知位置编码（RoPE）和基于 RL 的后处理阶段来适应多模态输入，并增强面部身份。MoCha 还设计了三种专业数据集：高保真渲染数据集（使用 UE5）、驱动表情数据集和增强数据集。

Result: 实验结果表明，MoCha 显著优于现有最先进的方法。

Conclusion: MoCha 提供了一种新颖的解决方案，通过简化需要用到的输入数据和引入先进的处理技术，解决了视频字符替换中的挑战。

Abstract: Controllable video character replacement with a user-provided identity remains a challenging problem due to the lack of paired video data. Prior works have predominantly relied on a reconstruction-based paradigm that requires per-frame segmentation masks and explicit structural guidance (e.g., skeleton, depth). This reliance, however, severely limits their generalizability in complex scenarios involving occlusions, character-object interactions, unusual poses, or challenging illumination, often leading to visual artifacts and temporal inconsistencies. In this paper, we propose MoCha, a pioneering framework that bypasses these limitations by requiring only a single arbitrary frame mask. To effectively adapt the multi-modal input condition and enhance facial identity, we introduce a condition-aware RoPE and employ an RL-based post-training stage. Furthermore, to overcome the scarcity of qualified paired-training data, we propose a comprehensive data construction pipeline. Specifically, we design three specialized datasets: a high-fidelity rendered dataset built with Unreal Engine 5 (UE5), an expression-driven dataset synthesized by current portrait animation techniques, and an augmented dataset derived from existing video-mask pairs. Extensive experiments demonstrate that our method substantially outperforms existing state-of-the-art approaches. We will release the code to facilitate further research. Please refer to our project page for more details: orange-3dv-team.github.io/MoCha

</details>


### [66] [WaveFormer: Frequency-Time Decoupled Vision Modeling with Wave Equation](https://arxiv.org/abs/2601.08602)
*Zishan Shu,Juntong Wu,Wei Yan,Xudong Liu,Hongyu Zhang,Chang Liu,Youdong Mao,Jie Chen*

Main category: cs.CV

TL;DR: 本文提出了一种基于波传播的模型WaveFormer，使用波传播操作（WPO）代替注意力机制，以O(N log N)时间复杂度建模全局交互，适用于图像分类、目标检测和语义分割任务，同时提供更高的计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有的Transformer模型中的注意力机制能够捕捉视觉依赖关系，但缺乏对语义信息空间传播的原理性解释。因此，本文从波传播角度重新审视这一问题，旨在通过一种新的方法来显式建模空间频率与传播时间的互动关系。

Method: 本文首先将特征图视为空间信号，并受到一个欠阻尼波动方程的控制，以明确建模空间频率的演变过程。然后推导出波传播操作（WPO）的闭式解析解，并将其实现为一个轻量化的模块，其时间复杂度为O(N log N)，低于注意力机制。基于WPO，设计了一个WaveFormer模型家族，用作标准ViT和CNN的即插即用替代品。

Result: 实验证明，WaveFormer模型在图像分类、目标检测和语义分割任务上达到与基于注意机制的模型相当的准确性，同时在计算效率方面较那些基于注意机制的模型有1.6倍的提高和30%的FLOP减少。

Conclusion: 波传播模型能够引入与热传播方法互补的建模偏见，能够在捕获全局一致性的同时有效提取高频细节，对于丰富的视觉语义具有重要作用。

Abstract: Vision modeling has advanced rapidly with Transformers, whose attention mechanisms capture visual dependencies but lack a principled account of how semantic information propagates spatially. We revisit this problem from a wave-based perspective: feature maps are treated as spatial signals whose evolution over an internal propagation time (aligned with network depth) is governed by an underdamped wave equation. In this formulation, spatial frequency-from low-frequency global layout to high-frequency edges and textures-is modeled explicitly, and its interaction with propagation time is controlled rather than implicitly fixed. We derive a closed-form, frequency-time decoupled solution and implement it as the Wave Propagation Operator (WPO), a lightweight module that models global interactions in O(N log N) time-far lower than attention. Building on WPO, we propose a family of WaveFormer models as drop-in replacements for standard ViTs and CNNs, achieving competitive accuracy across image classification, object detection, and semantic segmentation, while delivering up to 1.6x higher throughput and 30% fewer FLOPs than attention-based alternatives. Furthermore, our results demonstrate that wave propagation introduces a complementary modeling bias to heat-based methods, effectively capturing both global coherence and high-frequency details essential for rich visual semantics. Codes are available at: https://github.com/ZishanShu/WaveFormer.

</details>


### [67] [Interpretability and Individuality in Knee MRI: Patient-Specific Radiomic Fingerprint with Reconstructed Healthy Personas](https://arxiv.org/abs/2601.08604)
*Yaxi Chen,Simin Ni,Shuai Li,Shaheer U. Saeed,Aleksandra Ivanova,Rikin Hargunani,Jie Huang,Chaozong Liu,Yipeng Hu*

Main category: cs.CV

TL;DR: 文章提出了一种结合个体特征与可解释性的方法，通过构建患者特定的放射学指纹和健康原型，实现MRI膝盖扫描的自动化评估，与此同时保持了解释性。


<details>
  <summary>Details</summary>
Motivation: 传统的基于人群的放射组学方法在临床使用和接受方面受到限制，因为它可能无法捕捉患者的个体差异，并且可能在某些情况下不如端到端的深度学习模型。为了改进这种情况，本文提出了一种新的方法，可以平衡准确性和解释性。

Method: 首先，本文提出了放射学指纹的概念，它是一个根据患者MRI动态构建的个性化的特征集。该方法通过预测候选特征的相关性来构建这些指纹，并且它还引入了一个透明的逻辑回归来进行分类，以此保持特征级别的解释性。其次，健康原型被应用于每个患者以生成无病理的基准模型。这种原型由一个训练过的扩散模型生成，能够重建健康的膝关节MRI。这种原型被用来与病理图像的特征进行对比，以指出异常解剖学的变化，从而使病变为患者提供直观的解释。

Result: 实验结果表明，本文提出的方法在临床任务中的性能接近或超过了最新的深度学习模型，同时支持跨多个层面的解释。

Conclusion: 本文的工作强调了结合个体特征和解释性的重要性，并展示了如何通过对比健康原型和病理图像的特征来提供深入且具体的解释。

Abstract: For automated assessment of knee MRI scans, both accuracy and interpretability are essential for clinical use and adoption. Traditional radiomics rely on predefined features chosen at the population level; while more interpretable, they are often too restrictive to capture patient-specific variability and can underperform end-to-end deep learning (DL). To address this, we propose two complementary strategies that bring individuality and interpretability: radiomic fingerprints and healthy personas. First, a radiomic fingerprint is a dynamically constructed, patient-specific feature set derived from MRI. Instead of applying a uniform population-level signature, our model predicts feature relevance from a pool of candidate features and selects only those most predictive for each patient, while maintaining feature-level interpretability. This fingerprint can be viewed as a latent-variable model of feature usage, where an image-conditioned predictor estimates usage probabilities and a transparent logistic regression with global coefficients performs classification. Second, a healthy persona synthesises a pathology-free baseline for each patient using a diffusion model trained to reconstruct healthy knee MRIs. Comparing features extracted from pathological images against their personas highlights deviations from normal anatomy, enabling intuitive, case-specific explanations of disease manifestations. We systematically compare fingerprints, personas, and their combination across three clinical tasks. Experimental results show that both approaches yield performance comparable to or surpassing state-of-the-art DL models, while supporting interpretability at multiple levels. Case studies further illustrate how these perspectives facilitate human-explainable biomarker discovery and pathology localisation.

</details>


### [68] [SfMamba: Efficient Source-Free Domain Adaptation via Selective Scan Modeling](https://arxiv.org/abs/2601.08608)
*Xi Chen,Hongxun Yao,Sicheng Zhao,Jiankun Zhu,Jing Jiang,Kui Jiang*

Main category: cs.CV

TL;DR: SfMamba提出了一种用于无源模型迁移的框架，通过引入通道级别的视觉状态空间块进行通道序列扫描，以及语义一致性混合策略，增强了域不变特征的提取和预测一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的无源数据适应方法在感知领域和计算效率之间存在权衡。为了改善这一问题，SfMamba引入了通道级别的视觉状态空间块（Channel-wise Visual State-Space block）和语义一致性混合策略，以提高域不变特征的捕捉能力和模型预测一致性。

Method: SfMamba框架包含两个关键组件：通道级别的视觉状态空间块（Channel-wise Visual State-Space block），允许进行通道序列扫描，支持域不变特征的优化提取；语义一致性混合策略（Semantic-Consistent Shuffle strategy），通过在2D选择扫描中破坏背景块序列来保持预测的一致性。

Result: 通过在多个基准测试上的全面评估，SfMamba得到了比现有方法更强劲的性能，同时保持了良好的参数效率，为无源数据适应提供了一个实际解决方案。

Conclusion: SfMamba框架有效地解决了无源数据适应中的关键技术挑战，提高了模型的鲁棒性和适应性，为该领域的未来研究奠定了坚实的基础。

Abstract: Source-free domain adaptation (SFDA) tackles the critical challenge of adapting source-pretrained models to unlabeled target domains without access to source data, overcoming data privacy and storage limitations in real-world applications. However, existing SFDA approaches struggle with the trade-off between perception field and computational efficiency in domain-invariant feature learning. Recently, Mamba has offered a promising solution through its selective scan mechanism, which enables long-range dependency modeling with linear complexity. However, the Visual Mamba (i.e., VMamba) remains limited in capturing channel-wise frequency characteristics critical for domain alignment and maintaining spatial robustness under significant domain shifts. To address these, we propose a framework called SfMamba to fully explore the stable dependency in source-free model transfer. SfMamba introduces Channel-wise Visual State-Space block that enables channel-sequence scanning for domain-invariant feature extraction. In addition, SfMamba involves a Semantic-Consistent Shuffle strategy that disrupts background patch sequences in 2D selective scan while preserving prediction consistency to mitigate error accumulation. Comprehensive evaluations across multiple benchmarks show that SfMamba achieves consistently stronger performance than existing methods while maintaining favorable parameter efficiency, offering a practical solution for SFDA. Our code is available at https://github.com/chenxi52/SfMamba.

</details>


### [69] [SoC: Semantic Orthogonal Calibration for Test-Time Prompt Tuning](https://arxiv.org/abs/2601.08617)
*Leo Fillioux,Omprakash Chakraborty,Ismail Ben Ayed,Paul-Henry Cournède,Stergios Christodoulidis,Maria Vakalopoulou,Jose Dolz*

Main category: cs.CV

TL;DR: 该研究探讨了在视觉语言模型(VLMs)中进行不确定性估计校准的重要性，并提出了一种新的方法——语义正交校准(SoC)，以改进模型的校准性能。


<details>
  <summary>Details</summary>
Motivation: 随着视觉语言模型在关键决策系统中的应用越来越多，其不确定性估计的校准变得尤为重要。本文提出了语义正交校准(SoC)，以改进模型的校准性能。

Method: 本文首先分析了现有方法存在的问题，指出全正交约束会导致相关类别的距离过远，使模型过于自信。然后提出了语义正交校准(SoC)，这是一种基于Huber的正则化方法，能够在保护语义相近性的同时，平滑地分离原型，从而改进校准。

Result: 通过一系列实证验证，本文证明了SoC方法在改进校准性能方面的一致性，同时也保持了与之前正交方法相当的区分能力。

Conclusion: 最终，本文提出的方法SoC能够有效地提高视觉语言模型的校准性能，同时不过分牺牲模型的区分能力。

Abstract: With the increasing adoption of vision-language models (VLMs) in critical decision-making systems such as healthcare or autonomous driving, the calibration of their uncertainty estimates becomes paramount. Yet, this dimension has been largely underexplored in the VLM test-time prompt-tuning (TPT) literature, which has predominantly focused on improving their discriminative performance. Recent state-of-the-art advocates for enforcing full orthogonality over pairs of text prompt embeddings to enhance separability, and therefore calibration. Nevertheless, as we theoretically show in this work, the inherent gradients from fully orthogonal constraints will strongly push semantically related classes away, ultimately making the model overconfident. Based on our findings, we propose Semantic Orthogonal Calibration (SoC), a Huber-based regularizer that enforces smooth prototype separation while preserving semantic proximity, thereby improving calibration compared to prior orthogonality-based approaches. Across a comprehensive empirical validation, we demonstrate that SoC consistently improves calibration performance, while also maintaining competitive discriminative capabilities.

</details>


### [70] [CtrlFuse: Mask-Prompt Guided Controllable Infrared and Visible Image Fusion](https://arxiv.org/abs/2601.08619)
*Yiming Sun,Yuan Ruan,Qinghua Hu,Pengfei Zhu*

Main category: cs.CV

TL;DR: 本文提出了一种可控图像融合框架CtrlFuse，可以通过掩码提示实现动态交互式融合。该模型结合了多模态特征提取器、参考提示编码器（RPE）和提示语义融合模块（PSFM），通过优化分割和融合分支来提高任务性能和融合质量，最终在融合可控性和分割准确性方面表现出优秀的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的图像融合方法要么只专注于像素级融合而忽略了下游任务的适应性，要么通过级联检测/分割模型隐式学习固定的语义表达，无法灵活应对多样的语义目标感知需求。本文旨在通过引入可控制的交互式动态融合机制，解决传统融合方法的上述问题。

Method: CtrlFuse框架包含多模态特征提取器、参考提示编码器（RPE）和提示语义融合模块（PSFM）。RPE能够通过输入的掩码提示来微调预训练的分割模型，从而动态编码任务特定的语义提示；PSFM则将这些语义显式地注入到融合特征中。模型通过分割和融合分支的并行优化来实现任务性能和融合质量的提升。

Result: 实验结果显示，相比现有的方法，CtrlFuse在融合可控性和分割准确性上都达到了最先进的性能，且改进后的任务分支甚至超过了原本的分割模型。

Conclusion: 本文提出的方法CtrlFuse在图像融合领域展现了出色的能力，特别是在可控性和精度方面取得了显著的进展，并且为未来的多模态图像处理提供了新的思路。

Abstract: Infrared and visible image fusion generates all-weather perception-capable images by combining complementary modalities, enhancing environmental awareness for intelligent unmanned systems. Existing methods either focus on pixel-level fusion while overlooking downstream task adaptability or implicitly learn rigid semantics through cascaded detection/segmentation models, unable to interactively address diverse semantic target perception needs. We propose CtrlFuse, a controllable image fusion framework that enables interactive dynamic fusion guided by mask prompts. The model integrates a multi-modal feature extractor, a reference prompt encoder (RPE), and a prompt-semantic fusion module (PSFM). The RPE dynamically encodes task-specific semantic prompts by fine-tuning pre-trained segmentation models with input mask guidance, while the PSFM explicitly injects these semantics into fusion features. Through synergistic optimization of parallel segmentation and fusion branches, our method achieves mutual enhancement between task performance and fusion quality. Experiments demonstrate state-of-the-art results in both fusion controllability and segmentation accuracy, with the adapted task branch even outperforming the original segmentation model.

</details>


### [71] [SafeRedir: Prompt Embedding Redirection for Robust Unlearning in Image Generation Models](https://arxiv.org/abs/2601.08623)
*Renyang Liu,Kangjie Chen,Han Qiu,Jie Zhang,Kwok-Yan Lam,Tianwei Zhang,See-Kiong Ng*

Main category: cs.CV

TL;DR: SafeRedir是一个轻量级的推理时期框架，通过提示嵌入重定向技术来实现稳健的不良内容去除。它不需要修改底层图像生成模型，并能够在保持生成质量的同时识别并导向安全的语义区域。


<details>
  <summary>Details</summary>
Motivation: 为了应对图像生成模型在生成过程中存在的安全和合规风险，SafeRedir框架旨在提供一个轻量级且有效的解决方案，能够将不良提示安全地导向更佳或更安全的语义区域，从而解决复杂和大型图像生成模型所带来的挑战。

Method: SafeRedir框架包括两个核心组件：一种潜意识多模态安全分类器用于识别不安全的生成轨迹，以及一种基于TOKEN级别的DELTA生成器用于精准的语义重定向，该组件还配备了辅助预测器来实现TOKEN掩码和自适应缩放，以便更精确定位和调控干预。

Result: 在多个代表性去除非理想内容任务上的实验结果表明，SafeRedir能够实现有效的去除非理想内容功能，保持高语义和感知上的保存，提供稳健的图像质量改善，并提高抵抗对抗性攻击的能力。

Conclusion: SafeRedir框架展示了其在多种扩散模型基础下的广泛应用，并与现有去非理想内容模型具有兼容性，证明了其简单且高效的插件即用特性及广泛的适用性。

Abstract: Image generation models (IGMs), while capable of producing impressive and creative content, often memorize a wide range of undesirable concepts from their training data, leading to the reproduction of unsafe content such as NSFW imagery and copyrighted artistic styles. Such behaviors pose persistent safety and compliance risks in real-world deployments and cannot be reliably mitigated by post-hoc filtering, owing to the limited robustness of such mechanisms and a lack of fine-grained semantic control. Recent unlearning methods seek to erase harmful concepts at the model level, which exhibit the limitations of requiring costly retraining, degrading the quality of benign generations, or failing to withstand prompt paraphrasing and adversarial attacks. To address these challenges, we introduce SafeRedir, a lightweight inference-time framework for robust unlearning via prompt embedding redirection. Without modifying the underlying IGMs, SafeRedir adaptively routes unsafe prompts toward safe semantic regions through token-level interventions in the embedding space. The framework comprises two core components: a latent-aware multi-modal safety classifier for identifying unsafe generation trajectories, and a token-level delta generator for precise semantic redirection, equipped with auxiliary predictors for token masking and adaptive scaling to localize and regulate the intervention. Empirical results across multiple representative unlearning tasks demonstrate that SafeRedir achieves effective unlearning capability, high semantic and perceptual preservation, robust image quality, and enhanced resistance to adversarial attacks. Furthermore, SafeRedir generalizes effectively across a variety of diffusion backbones and existing unlearned models, validating its plug-and-play compatibility and broad applicability. Code and data are available at https://github.com/ryliu68/SafeRedir.

</details>


### [72] [Além do Desempenho: Um Estudo da Confiabilidade de Detectores de Deepfakes](https://arxiv.org/abs/2601.08674)
*Lucas Lopes,Rayson Laroca,André Grégio*

Main category: cs.CV

TL;DR: 本文提出了一种基于四个支柱（可移植性、鲁棒性、可解释性和计算效率）的可靠性评估框架，并分析了五种最先进的方法。


<details>
  <summary>Details</summary>
Motivation: 为了评估深度伪造检测技术的有效性和局限性。

Method: 提出了一种新的评估框架，并分析了现有的深度伪造检测方法。

Result: 发现现有的方法取得了显著进展，但也存在关键限制。

Conclusion: 新的评估框架有助于深入了解现有方法，并促进该领域的发展。

Abstract: Deepfakes are synthetic media generated by artificial intelligence, with positive applications in education and creativity, but also serious negative impacts such as fraud, misinformation, and privacy violations. Although detection techniques have advanced, comprehensive evaluation methods that go beyond classification performance remain lacking. This paper proposes a reliability assessment framework based on four pillars: transferability, robustness, interpretability, and computational efficiency. An analysis of five state-of-the-art methods revealed significant progress as well as critical limitations.

</details>


### [73] [Salience-SGG: Enhancing Unbiased Scene Graph Generation with Iterative Salience Estimation](https://arxiv.org/abs/2601.08728)
*Runfeng Qu,Ole Hall,Pia K Bideau,Julie Ouerfelli-Ethier,Martin Rolfs,Klaus Obermayer,Olaf Hellwich*

Main category: cs.CV

TL;DR: Salience-SGG 引入了一种迭代显著性解码器 (ISD) 来强调具有显著空间结构的三元组，并通过语义无关的显著性标签进行指导，从而在 Visual Genome、Open Images V6 和 GQA-200 上实现了最先进的性能，并提高了现有 Unbiased-SGG 方法的空间理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有的 SGG 使用具有偏差的策略，尽管 Unbiased-SGG 已经可以减轻这种偏差，但经常会在空间理解上有所欠缺，依赖于语义先验。因此，研究需要一种更加关注显著空间结构的方法来减少依赖性并提升性能。

Method: Salience-SGG 采用 Iterative Salience Decoder (ISD)，该迭代解码器通过使用语义无关的显著性标签来关注具有显著空间结构的三元组，从而改善空间理解。

Result: 在 Visual Genome、Open Images V6 和 GQA-200 上，Salience-SGG 达到了最先进的性能，并且其空间理解能力超过了现有的 Unbiased-SGG 方法。

Conclusion: Salience-SGG 证明了一种新的、有效的处理 SGG 中空间理解问题的方法，并可能为未来的 SGG 研究提供参考。

Abstract: Scene Graph Generation (SGG) suffers from a long-tailed distribution, where a few predicate classes dominate while many others are underrepresented, leading to biased models that underperform on rare relations. Unbiased-SGG methods address this issue by implementing debiasing strategies, but often at the cost of spatial understanding, resulting in an over-reliance on semantic priors. We introduce Salience-SGG, a novel framework featuring an Iterative Salience Decoder (ISD) that emphasizes triplets with salient spatial structures. To support this, we propose semantic-agnostic salience labels guiding ISD. Evaluations on Visual Genome, Open Images V6, and GQA-200 show that Salience-SGG achieves state-of-the-art performance and improves existing Unbiased-SGG methods in their spatial understanding as demonstrated by the Pairwise Localization Average Precision

</details>


### [74] [ISLA: A U-Net for MRI-based acute ischemic stroke lesion segmentation with deep supervision, attention, domain adaptation, and ensemble learning](https://arxiv.org/abs/2601.08732)
*Vincent Roca,Martin Bretzner,Hilde Henon,Laurent Puy,Grégory Kuchcinski,Renaud Lopes*

Main category: cs.CV

TL;DR: ISLA是一个新的深度学习模型，用于从扩散MRI中自动分割急性缺血性中风病变，通过系统优化损失函数、卷积架构、深度监督和注意力机制，实现了在外部测试集上的优越性能。


<details>
  <summary>Details</summary>
Motivation: 自动分割急性缺血性中风病变对于中风诊断和管理至关重要。现有方法主要基于U-Net框架，但缺乏统一的最佳配置，ISLA旨在提供一种新的解决方案。

Method: ISLA模型采用了深度优化的损失函数、卷积架构、深度监督和注意力机制，同时还将无监督领域自适应技术用于提高外部临床数据集上的泛化能力。

Result: ISLA在外部测试集上超过了两个最先进的急性缺血性中风病变分割方法。

Conclusion: ISLA模型提供了从扩散MRI自动分割急性缺血性中风病变的新解决方案，并实现优秀的分割性能和高泛化能力。

Abstract: Accurate delineation of acute ischemic stroke lesions in MRI is a key component of stroke diagnosis and management. In recent years, deep learning models have been successfully applied to the automatic segmentation of such lesions. While most proposed architectures are based on the U-Net framework, they primarily differ in their choice of loss functions and in the use of deep supervision, residual connections, and attention mechanisms. Moreover, many implementations are not publicly available, and the optimal configuration for acute ischemic stroke (AIS) lesion segmentation remains unclear. In this work, we introduce ISLA (Ischemic Stroke Lesion Analyzer), a new deep learning model for AIS lesion segmentation from diffusion MRI, trained on three multicenter databases totaling more than 1500 AIS participants. Through systematic optimization of the loss function, convolutional architecture, deep supervision, and attention mechanisms, we developed a robust segmentation framework. We further investigated unsupervised domain adaptation to improve generalization to an external clinical dataset. ISLA outperformed two state-of-the-art approaches for AIS lesion segmentation on an external test set. Codes and trained models will be made publicly available to facilitate reuse and reproducibility.

</details>


### [75] [Translating Light-Sheet Microscopy Images to Virtual H&E Using CycleGAN](https://arxiv.org/abs/2601.08776)
*Yanhua Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种基于Cycle-Consistent Adversarial Network (CycleGAN)的方法，将多通道荧光显微镜图像转换为伪H&E染色的组织学图像，以辅助显微镜分析和整合到标准流程中。


<details>
  <summary>Details</summary>
Motivation: 荧光显微镜和传统H&E染色在组织学分析中提供了互补的信息，将荧光图像转换为类似H&E染色的外观有助于病理学家的理解和现有流程的整合。

Method: 采用CycleGAN方法，通过ResNet生成器和PatchGAN判别器，利用生成器、循环一致性及身份损失进行无标签配对的图像到图像的翻译。将C01和C02荧光通道合并为RGB，学习两个数据域之间的双向映射。

Result: 实验结果显示，该模型生成的伪H&E图像在保持形态结构的同时采用了H&E类似的色彩特征，使得荧光数据可视化更加符合病理学家的习惯，并支持与现有H&E基线分析管道的整合。

Conclusion: 该研究表明，基于CycleGAN的方法能够有效地将多通道荧光显微镜图像转换为伪H&E染色的组织学图像，有助于提高荧光显微镜分析的易用性。

Abstract: Histopathology analysis relies on Hematoxylin and Eosin (H&E) staining, but fluorescence microscopy offers complementary information. Converting fluorescence images to H&E-like appearance can aid interpretation and integration with standard workflows. We present a Cycle-Consistent Adversarial Network (CycleGAN) approach for unpaired image-to-image translation from multi-channel fluorescence microscopy to pseudo H&E stained histopathology images. The method combines C01 and C02 fluorescence channels into RGB and learns a bidirectional mapping between fluorescence and H&E domains without paired training data. The architecture uses ResNet-based generators with residual blocks and PatchGAN discriminators, trained with adversarial, cycle-consistency, and identity losses. Experiments on fluorescence microscopy datasets show the model generates realistic pseudo H&E images that preserve morphological structures while adopting H&E-like color characteristics. This enables visualization of fluorescence data in a format familiar to pathologists and supports integration with existing H&E-based analysis pipelines.

</details>


### [76] [Aggregating Diverse Cue Experts for AI-Generated Image Detection](https://arxiv.org/abs/2601.08790)
*Lei Tan,Shuwei Li,Mohan Kankanhalli,Robby T. Tan*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The rapid emergence of image synthesis models poses challenges to the generalization of AI-generated image detectors. However, existing methods often rely on model-specific features, leading to overfitting and poor generalization. In this paper, we introduce the Multi-Cue Aggregation Network (MCAN), a novel framework that integrates different yet complementary cues in a unified network. MCAN employs a mixture-of-encoders adapter to dynamically process these cues, enabling more adaptive and robust feature representation. Our cues include the input image itself, which represents the overall content, and high-frequency components that emphasize edge details. Additionally, we introduce a Chromatic Inconsistency (CI) cue, which normalizes intensity values and captures noise information introduced during the image acquisition process in real images, making these noise patterns more distinguishable from those in AI-generated content. Unlike prior methods, MCAN's novelty lies in its unified multi-cue aggregation framework, which integrates spatial, frequency-domain, and chromaticity-based information for enhanced representation learning. These cues are intrinsically more indicative of real images, enhancing cross-model generalization. Extensive experiments on the GenImage, Chameleon, and UniversalFakeDetect benchmark validate the state-of-the-art performance of MCAN. In the GenImage dataset, MCAN outperforms the best state-of-the-art method by up to 7.4% in average ACC across eight different image generators.

</details>


### [77] [DentalX: Context-Aware Dental Disease Detection with Radiographs](https://arxiv.org/abs/2601.08797)
*Zhi Qin Tan,Xiatian Zhu,Owen Addison,Yunpeng Li*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Diagnosing dental diseases from radiographs is time-consuming and challenging due to the subtle nature of diagnostic evidence. Existing methods, which rely on object detection models designed for natural images with more distinct target patterns, struggle to detect dental diseases that present with far less visual support. To address this challenge, we propose {\bf DentalX}, a novel context-aware dental disease detection approach that leverages oral structure information to mitigate the visual ambiguity inherent in radiographs. Specifically, we introduce a structural context extraction module that learns an auxiliary task: semantic segmentation of dental anatomy. The module extracts meaningful structural context and integrates it into the primary disease detection task to enhance the detection of subtle dental diseases. Extensive experiments on a dedicated benchmark demonstrate that DentalX significantly outperforms prior methods in both tasks. This mutual benefit arises naturally during model optimization, as the correlation between the two tasks is effectively captured. Our code is available at https://github.com/zhiqin1998/DentYOLOX.

</details>


### [78] [Near-perfect photo-ID of the Hula painted frog with zero-shot deep local-feature matching](https://arxiv.org/abs/2601.08798)
*Maayan Yesharim,R. G. Bina Perl,Uri Roll,Sarig Gafny,Eli Geffen,Yoav Ram*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Accurate individual identification is essential for monitoring rare amphibians, yet invasive marking is often unsuitable for critically endangered species. We evaluate state-of-the-art computer-vision methods for photographic re-identification of the Hula painted frog (Latonia nigriventer) using 1,233 ventral images from 191 individuals collected during 2013-2020 capture-recapture surveys. We compare deep local-feature matching in a zero-shot setting with deep global-feature embedding models. The local-feature pipeline achieves 98% top-1 closed-set identification accuracy, outperforming all global-feature models; fine-tuning improves the best global-feature model to 60% top-1 (91% top-10) but remains below local matching. To combine scalability with accuracy, we implement a two-stage workflow in which a fine-tuned global-feature model retrieves a short candidate list that is re-ranked by local-feature matching, reducing end-to-end runtime from 6.5-7.8 hours to ~38 minutes while maintaining ~96% top-1 closed-set accuracy on the labeled dataset. Separation of match scores between same- and different-individual pairs supports thresholding for open-set identification, enabling practical handling of novel individuals. We deploy this pipeline as a web application for routine field use, providing rapid, standardized, non-invasive identification to support conservation monitoring and capture-recapture analyses. Overall, in this species, zero-shot deep local-feature matching outperformed global-feature embedding and provides a strong default for photo-identification.

</details>


### [79] [S3-CLIP: Video Super Resolution for Person-ReID](https://arxiv.org/abs/2601.08807)
*Tamas Endrei,Gyorgy Cserey*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Tracklet quality is often treated as an afterthought in most person re-identification (ReID) methods, with the majority of research presenting architectural modifications to foundational models. Such approaches neglect an important limitation, posing challenges when deploying ReID systems in real-world, difficult scenarios. In this paper, we introduce S3-CLIP, a video super-resolution-based CLIP-ReID framework developed for the VReID-XFD challenge at WACV 2026. The proposed method integrates recent advances in super-resolution networks with task-driven super-resolution pipelines, adapting them to the video-based person re-identification setting. To the best of our knowledge, this work represents the first systematic investigation of video super-resolution as a means of enhancing tracklet quality for person ReID, particularly under challenging cross-view conditions. Experimental results demonstrate performance competitive with the baseline, achieving 37.52% mAP in aerial-to-ground and 29.16% mAP in ground-to-aerial scenarios. In the ground-to-aerial setting, S3-CLIP achieves substantial gains in ranking accuracy, improving Rank-1, Rank-5, and Rank-10 performance by 11.24%, 13.48%, and 17.98%, respectively.

</details>


### [80] [Reasoning Matters for 3D Visual Grounding](https://arxiv.org/abs/2601.08811)
*Hsiang-Wei Huang,Kuang-Ming Chen,Wenhao Chai,Cheng-Yen Yang,Jen-Hao Cheng,Jenq-Neng Hwang*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The recent development of Large Language Models (LLMs) with strong reasoning ability has driven research in various domains such as mathematics, coding, and scientific discovery. Meanwhile, 3D visual grounding, as a fundamental task in 3D understanding, still remains challenging due to the limited reasoning ability of recent 3D visual grounding models. Most of the current methods incorporate a text encoder and visual feature encoder to generate cross-modal fuse features and predict the referring object. These models often require supervised training on extensive 3D annotation data. On the other hand, recent research also focus on scaling synthetic data to train stronger 3D visual grounding LLM, however, the performance gain remains limited and non-proportional to the data collection cost. In this work, we propose a 3D visual grounding data pipeline, which is capable of automatically synthesizing 3D visual grounding data along with corresponding reasoning process. Additionally, we leverage the generated data for LLM fine-tuning and introduce Reason3DVG-8B, a strong 3D visual grounding LLM that outperforms previous LLM-based method 3D-GRAND using only 1.6% of their training data, demonstrating the effectiveness of our data and the importance of reasoning in 3D visual grounding.

</details>


### [81] [Motion Attribution for Video Generation](https://arxiv.org/abs/2601.08828)
*Xindi Wu,Despoina Paschalidou,Jun Gao,Antonio Torralba,Laura Leal-Taixé,Olga Russakovsky,Sanja Fidler,Jonathan Lorraine*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Despite the rapid progress of video generation models, the role of data in influencing motion is poorly understood. We present Motive (MOTIon attribution for Video gEneration), a motion-centric, gradient-based data attribution framework that scales to modern, large, high-quality video datasets and models. We use this to study which fine-tuning clips improve or degrade temporal dynamics. Motive isolates temporal dynamics from static appearance via motion-weighted loss masks, yielding efficient and scalable motion-specific influence computation. On text-to-video models, Motive identifies clips that strongly affect motion and guides data curation that improves temporal consistency and physical plausibility. With Motive-selected high-influence data, our method improves both motion smoothness and dynamic degree on VBench, achieving a 74.1% human preference win rate compared with the pretrained base model. To our knowledge, this is the first framework to attribute motion rather than visual appearance in video generative models and to use it to curate fine-tuning data.

</details>


### [82] [3AM: Segment Anything with Geometric Consistency in Videos](https://arxiv.org/abs/2601.08831)
*Yang-Che Sun,Cheng Sun,Chin-Yang Lin,Fu-En Yang,Min-Hung Chen,Yen-Yu Lin,Yu-Lun Liu*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Video object segmentation methods like SAM2 achieve strong performance through memory-based architectures but struggle under large viewpoint changes due to reliance on appearance features. Traditional 3D instance segmentation methods address viewpoint consistency but require camera poses, depth maps, and expensive preprocessing. We introduce 3AM, a training-time enhancement that integrates 3D-aware features from MUSt3R into SAM2. Our lightweight Feature Merger fuses multi-level MUSt3R features that encode implicit geometric correspondence. Combined with SAM2's appearance features, the model achieves geometry-consistent recognition grounded in both spatial position and visual similarity. We propose a field-of-view aware sampling strategy ensuring frames observe spatially consistent object regions for reliable 3D correspondence learning. Critically, our method requires only RGB input at inference, with no camera poses or preprocessing. On challenging datasets with wide-baseline motion (ScanNet++, Replica), 3AM substantially outperforms SAM2 and extensions, achieving 90.6% IoU and 71.7% Positive IoU on ScanNet++'s Selected Subset, improving over state-of-the-art VOS methods by +15.9 and +30.4 points. Project page: https://jayisaking.github.io/3AM-Page/

</details>


### [83] [RAVEN: Erasing Invisible Watermarks via Novel View Synthesis](https://arxiv.org/abs/2601.08832)
*Fahad Shamshad,Nils Lukas,Karthik Nandakumar*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Invisible watermarking has become a critical mechanism for authenticating AI-generated image content, with major platforms deploying watermarking schemes at scale. However, evaluating the vulnerability of these schemes against sophisticated removal attacks remains essential to assess their reliability and guide robust design. In this work, we expose a fundamental vulnerability in invisible watermarks by reformulating watermark removal as a view synthesis problem. Our key insight is that generating a perceptually consistent alternative view of the same semantic content, akin to re-observing a scene from a shifted perspective, naturally removes the embedded watermark while preserving visual fidelity. This reveals a critical gap: watermarks robust to pixel-space and frequency-domain attacks remain vulnerable to semantic-preserving viewpoint transformations. We introduce a zero-shot diffusion-based framework that applies controlled geometric transformations in latent space, augmented with view-guided correspondence attention to maintain structural consistency during reconstruction. Operating on frozen pre-trained models without detector access or watermark knowledge, our method achieves state-of-the-art watermark suppression across 15 watermarking methods--outperforming 14 baseline attacks while maintaining superior perceptual quality across multiple datasets.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [84] [EmbeddingRWKV: State-Centric Retrieval with Reusable States](https://arxiv.org/abs/2601.07861)
*Haowen Hou,Jie Yang*

Main category: cs.CL

TL;DR: 本文提出了一种名为State-Centric Retrieval的统一检索范式，通过引入'状态'作为嵌入模型和重排器之间的桥梁，减少了计算冗余，提高了系统效率。


<details>
  <summary>Details</summary>
Motivation: 传统的RAG系统存在两阶段处理，导致大量重复计算，本文为解决此问题提出State-Centric Retrieval，整合检索和重排过程。

Method: 首先，通过微调基于RWKV的LLM进行状态表示学习，构建嵌入RWKV模型；接着设计基于状态的重排器来充分利用预计算信息。此外，本文提出了一种均匀层选择策略，仅使用25%的层达到了接近全模型98.62%的性能。

Result: 实验结果显示，提出的方法在保持高质量检索和重排结果的同时，显著提高了系统的整体效率，推理成本与文档长度无关，速度提升5.4倍至44.8倍。

Conclusion: 本文提出的State-Centric Retrieval方法统一了检索和重排过程，提高了RAG系统的效率，同时保持了高质量的性能。

Abstract: Current Retrieval-Augmented Generation (RAG) systems typically employ a traditional two-stage pipeline: an embedding model for initial retrieval followed by a reranker for refinement. However, this paradigm suffers from significant inefficiency due to the lack of shared information between stages, leading to substantial redundant computation. To address this limitation, we propose \textbf{State-Centric Retrieval}, a unified retrieval paradigm that utilizes "states" as a bridge to connect embedding models and rerankers. First, we perform state representation learning by fine-tuning an RWKV-based LLM, transforming it into \textbf{EmbeddingRWKV}, a unified model that serves as both an embedding model and a state backbone for extracting compact, reusable states. Building upon these reusable states, we further design a state-based reranker to fully leverage precomputed information. During reranking, the model processes only query tokens, decoupling inference cost from document length and yielding a 5.4$\times$--44.8$\times$ speedup. Furthermore, we observe that retaining all intermediate layer states is unnecessary; with a uniform layer selection strategy, our model maintains 98.62\% of full-model performance using only 25\% of the layers. Extensive experiments demonstrate that State-Centric Retrieval achieves high-quality retrieval and reranking results while significantly enhancing overall system efficiency. Code is available at \href{https://github.com/howard-hou/EmbeddingRWKV}{our GitHub repository}.

</details>


### [85] [A Human-Centric Pipeline for Aligning Large Language Models with Chinese Medical Ethics](https://arxiv.org/abs/2601.07954)
*Haoan Jin,Han Ying,Jiacheng Ji,Hanhui Xu,Mengyue Wu*

Main category: cs.CL

TL;DR: 该研究提出了一种名为MedES的动态场景中心基准，旨在解决大型语言模型在医疗伦理复杂情境下的应用问题。通过引入监护人循环框架，利用专门的自动评估器生成有针对性的提示并提供结构化的伦理反馈，实现了对7B参数模型的有效伦理对齐，实验结果表明改进显著。


<details>
  <summary>Details</summary>
Motivation: 目前，大型语言模型在医疗领域的应用日益广泛，但如何根据复杂的医疗伦理需求进行对齐研究不足。因此，该研究旨在开发中文医疗伦理环境下的动态场景中心基准（MedES）和监护人循环框架，以更好地促进模型在实际医疗场景中的伦理对齐。

Method: 研究团队构建了MedES基准，包含260份权威的中国医疗、伦理和法律来源，涵盖临床决策难题。同时引入了监护人循环框架，使用自动评估器（准确性超过97%）生成定制提示并提供结构化伦理反馈，用于监督微调及领域特定偏好优化。

Result: 研究最终对7B参数语言模型进行伦理对齐，实验结果显示针对核心伦理任务的改进显著，且在综合评估指标中也表现出更好的表现。

Conclusion: 该工作提供了一种实用且可适应的框架，用于在中文医疗领域中将大型语言模型与医疗伦理对齐，证明了该方法的有效性，并提示可以通过替换基础规范语料库在其他法律和文化环境中实现类似的伦理对齐管道。

Abstract: Recent advances in large language models have enabled their application to a range of healthcare tasks. However, aligning LLMs with the nuanced demands of medical ethics, especially under complex real world scenarios, remains underexplored. In this work, we present MedES, a dynamic, scenario-centric benchmark specifically constructed from 260 authoritative Chinese medical, ethical, and legal sources to reflect the challenges in clinical decision-making. To facilitate model alignment, we introduce a guardian-in-the-loop framework that leverages a dedicated automated evaluator (trained on expert-labeled data and achieving over 97% accuracy within our domain) to generate targeted prompts and provide structured ethical feedback. Using this pipeline, we align a 7B-parameter LLM through supervised fine-tuning and domain-specific preference optimization. Experimental results, conducted entirely within the Chinese medical ethics context, demonstrate that our aligned model outperforms notably larger baselines on core ethical tasks, with observed improvements in both quality and composite evaluation metrics. Our work offers a practical and adaptable framework for aligning LLMs with medical ethics in the Chinese healthcare domain, and suggests that similar alignment pipelines may be instantiated in other legal and cultural environments through modular replacement of the underlying normative corpus.

</details>


### [86] [Knowing But Not Doing: Convergent Morality and Divergent Action in LLMs](https://arxiv.org/abs/2601.07972)
*Jen-tse Huang,Jiantong Qin,Xueli Qiu,Sharon Levy,Michelle R. Kaufman,Mark Dredze*

Main category: cs.CL

TL;DR: ValAct-15k 数据集评测了十个前沿大语言模型和人类参与者在十个基本价值观上的表现，发现大语言模型在场景决策上的表现一致性极高，而人类表现差异大，且无论模型还是人类在报告和实际行为间的吻合度都较低。


<details>
  <summary>Details</summary>
Motivation: 研究展示了在安全和社交通用的人工智能开发中，大语言模型的价值观念的呈现与实践效果存在差距，尤其是在人类价值观多样性和复杂性的作用下更为明显。

Method: 研究人员创建了ValAct-15k数据集，包含了3000个从Reddit获取的求建议场景，涉及十个基本价值观。评估了来自中美公司的十个前沿大语言模型和55名人类参与者在这些方面的表现。

Result: 大语言模型在基于场景的决策中表现出高度一致性，但人类之间的表现差距较大。无论是模型还是人类，在自我报告的价值观与其实际表现之间的一致性都较弱，显示了知识与行动之间的差距。

Conclusion: 研究指出，尽管大语言模型的对齐训练能够促进价值观的规范性共识，但这并不能消除类似于人类的行为不一致性。

Abstract: Value alignment is central to the development of safe and socially compatible artificial intelligence. However, how Large Language Models (LLMs) represent and enact human values in real-world decision contexts remains under-explored. We present ValAct-15k, a dataset of 3,000 advice-seeking scenarios derived from Reddit, designed to elicit ten values defined by Schwartz Theory of Basic Human Values. Using both the scenario-based questions and the traditional value questionnaire, we evaluate ten frontier LLMs (five from U.S. companies, five from Chinese ones) and human participants ($n = 55$). We find near-perfect cross-model consistency in scenario-based decisions (Pearson $r \approx 1.0$), contrasting sharply with the broad variability observed among humans ($r \in [-0.79, 0.98]$). Yet, both humans and LLMs show weak correspondence between self-reported and enacted values ($r = 0.4, 0.3$), revealing a systematic knowledge-action gap. When instructed to "hold" a specific value, LLMs' performance declines up to $6.6%$ compared to merely selecting the value, indicating a role-play aversion. These findings suggest that while alignment training yields normative value convergence, it does not eliminate the human-like incoherence between knowing and acting upon values.

</details>


### [87] [Explaining Generalization of AI-Generated Text Detectors Through Linguistic Analysis](https://arxiv.org/abs/2601.07974)
*Yuxi Xia,Kinga Stańczak,Benjamin Roth*

Main category: cs.CL

TL;DR: 该研究通过语言学分析，对AI文本检测器的跨生成条件泛化行为进行了系统性研究，发现特定检测器在不同评估条件下的泛化性能与时态使用和代词频率等语言特征显著相关。


<details>
  <summary>Details</summary>
Motivation: 现有研究中的AI文本检测器在特定领域内的表现良好，但在面对前所未见的提示、模型家族或领域时泛化能力较差。本研究旨在通过跨领域的实验设计，揭示这一问题的发生原因，为改进检测器提供依据。

Method: 研究者构建了一个涵盖6种提示策略、7种大型语言模型和4个领域数据集的基准数据集，共产生了多样化的人工和AI生成文本。通过将分类器微调到不同的生成设置下，对跨提示、模型和领域的一致性进行了评估。进一步，通过计算训练和测试条件下80个语言特征之间的特征变化与泛化准确率的相关性，来解释性能差异。

Result: 实验结果显示，特定检测器在某些评估条件下的泛化性能显著关联于时态使用和代词频率等语言特征的变化。这些发现有助于识别现有检测器的弱点，并可能指导未来的设计优化。

Conclusion: 本研究提供了一种理解AI文本检测器泛化行为的新视角，表明语言学特征在泛化的变异性分析中扮演重要角色。未来工作可以进一步探索更多语言特征及其对不同检测器性能的影响。

Abstract: AI-text detectors achieve high accuracy on in-domain benchmarks, but often struggle to generalize across different generation conditions such as unseen prompts, model families, or domains. While prior work has reported these generalization gaps, there are limited insights about the underlying causes. In this work, we present a systematic study aimed at explaining generalization behavior through linguistic analysis. We construct a comprehensive benchmark that spans 6 prompting strategies, 7 large language models (LLMs), and 4 domain datasets, resulting in a diverse set of human- and AI-generated texts. Using this dataset, we fine-tune classification-based detectors on various generation settings and evaluate their cross-prompt, cross-model, and cross-dataset generalization. To explain the performance variance, we compute correlations between generalization accuracies and feature shifts of 80 linguistic features between training and test conditions. Our analysis reveals that generalization performance for specific detectors and evaluation conditions is significantly associated with linguistic features such as tense usage and pronoun frequency.

</details>


### [88] [Cross-Cultural Expert-Level Art Critique Evaluation with Vision-Language Models](https://arxiv.org/abs/2601.07984)
*Haorui Yu,Ramon Ruiz-Dolz,Xuehang Wen,Fengrui Zhang,Qiufeng Yi*

Main category: cs.CL

TL;DR: 本文提出了一个三阶段的评估框架，用于跨文化艺术批评评估，并通过定量分析揭示了自动化指标、评分标准以及跨评审员标准化尺度之间的关系。


<details>
  <summary>Details</summary>
Motivation: 评估视觉-语言模型在跨文化艺术理解上的能力，解决其文化意义解释不足的问题。

Method: 该方法分为三个阶段：第一阶段计算离线的自动化覆盖率和风险指标；第二阶段使用单一主要评判者在五个维度上进行评分；第三阶段通过等角回归对第二阶段的综合评分进行校准。

Result: 第三阶段减少了152个样本外集合上的MAE 5.2%，输出了的文化理解得分，维度级诊断指标和风险指标，以及15个VLMs在294个专家锚点的评估结果。

Conclusion: 研究表明，全自动指标不可靠，西部样本在我们的取样和评分标准下得分更高，跨评审员的尺度差异性使简单评分平均不可靠，建议采用单一主要评审员并进行明确校准。

Abstract: Vision-Language Models (VLMs) excel at visual perception, yet their ability to interpret cultural meaning in art remains under-validated. We present a tri-tier evaluation framework for cross-cultural art-critique assessment: Tier I computes automated coverage and risk indicators offline; Tier II applies rubric-based scoring using a single primary judge across five dimensions; and Tier III calibrates the Tier II aggregate score to human ratings via isotonic regression, yielding a 5.2% reduction in MAE on a 152-sample held-out set. The framework outputs a calibrated cultural-understanding score for model selection and cultural-gap diagnosis, together with dimension-level diagnostics and risk indicators. We evaluate 15 VLMs on 294 expert anchors spanning six cultural traditions. Key findings are that (i) automated metrics are unreliable proxies for cultural depth, (ii) Western samples score higher than non-Western samples under our sampling and rubric, and (iii) cross-judge scale mismatch makes naive score averaging unreliable, motivating a single primary judge with explicit calibration. Dataset and code are available in the supplementary materials.

</details>


### [89] [Multilingual, Multimodal Pipeline for Creating Authentic and Structured Fact-Checked Claim Dataset](https://arxiv.org/abs/2601.07985)
*Z. Melce Hüsünbeyi,Virginie Mouilleron,Leonie Uhling,Daniel Foppe,Tatjana Scheffler,Djamé Seddah*

Main category: cs.CL

TL;DR: 本文提出了一种全面的数据收集和处理管道，用于构建法语和德语文本的多模态事实核查数据集，利用先进的大型语言模型和多模态大型语言模型进行证据提取和解释生成，旨在支持多语言多模态虚假信息验证的研究。


<details>
  <summary>Details</summary>
Motivation: 鉴于在线平台上传播的信息失真问题日益严重，现有的数据集在范围、多媒体证据、结构化注释和断言、证据和裁决之间的详细链接方面存在局限，本文旨在填补这些空白，通过综合 ClaimReview 流、抓取完整反驳文章、规范化异构断言裁决并用结构化元数据和对齐的视觉内容丰富它们，搭建一个多语言多模态事实核查资源。

Method: 本文的方法涵盖了多模态事实核查数据集的构建，使用了 ClaimReview 流聚合、全篇文章抓取、异构断言裁决规范化和结构化元数据与视觉内容丰富的技术。此外，还应用了先进的人工智能模型进行证据提取和解释生成。

Result: 使用最先进的大型语言模型（LLMs）和多模态 LLMs 进行了证据提取和解释生成的研究，评估表明此管道有助于不同组织或媒体市场的事实核查实践比较，促进更具解释性的、证据支持的事实核查模型的发展，并为未来多语言多模态虚假信息验证研究奠定基础。

Conclusion: 本文的工作不仅促进了现有的事实核查数据资源的扩展和更新，还为未来的多语言多模态虚假信息验证研究提供了坚实的基础。

Abstract: The rapid proliferation of misinformation across online platforms underscores the urgent need for robust, up-to-date, explainable, and multilingual fact-checking resources. However, existing datasets are limited in scope, often lacking multimodal evidence, structured annotations, and detailed links between claims, evidence, and verdicts. This paper introduces a comprehensive data collection and processing pipeline that constructs multimodal fact-checking datasets in French and German languages by aggregating ClaimReview feeds, scraping full debunking articles, normalizing heterogeneous claim verdicts, and enriching them with structured metadata and aligned visual content. We used state-of-the-art large language models (LLMs) and multimodal LLMs for (i) evidence extraction under predefined evidence categories and (ii) justification generation that links evidence to verdicts. Evaluation with G-Eval and human assessment demonstrates that our pipeline enables fine-grained comparison of fact-checking practices across different organizations or media markets, facilitates the development of more interpretable and evidence-grounded fact-checking models, and lays the groundwork for future research on multilingual, multimodal misinformation verification.

</details>


### [90] [VULCA-Bench: A Multicultural Vision-Language Benchmark for Evaluating Cultural Understanding](https://arxiv.org/abs/2601.07986)
*Haorui Yu,Ramon Ruiz-Dolz,Diji Yang,Hang He,Fengrui Zhang,Qiufeng Yi*

Main category: cs.CL

TL;DR: VULCA-Bench 是一个评估视觉-语言模型在多文化语境中理解能力的基准，提供了丰富的文化维度和专家评定的双语批评文本。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型基准主要评估基本的视觉感知和语言理解能力，但忽视了高阶文化解释能力，VULCA-Bench旨在填补这一空白。

Method: 通过构建一个包含7410个图文对照对的数据集，覆盖8种文化传统，并设计一个五层框架（从视觉感知到哲学美学）来评估模型的理解能力。

Result: 研究表明，高层推理（L3-L5）比视觉和技术分析（L1-L2）更具挑战性。数据集和评估脚本已开源。

Conclusion: VULCA-Bench 的推出为评估视觉-语言模型的文化理解能力提供了新的方法和技术支持。

Abstract: We introduce VULCA-Bench, a multicultural art-critique benchmark for evaluating Vision-Language Models' (VLMs) cultural understanding beyond surface-level visual perception. Existing VLM benchmarks predominantly measure L1-L2 capabilities (object recognition, scene description, and factual question answering) while under-evaluate higher-order cultural interpretation. VULCA-Bench contains 7,410 matched image-critique pairs spanning eight cultural traditions, with Chinese-English bilingual coverage. We operationalise cultural understanding using a five-layer framework (L1-L5, from Visual Perception to Philosophical Aesthetics), instantiated as 225 culture-specific dimensions and supported by expert-written bilingual critiques. Our pilot results indicate that higher-layer reasoning (L3-L5) is consistently more challenging than visual and technical analysis (L1-L2). The dataset, evaluation scripts, and annotation tools are available under CC BY 4.0 in the supplementary materials.

</details>


### [91] [From Word Sequences to Behavioral Sequences: Adapting Modeling and Evaluation Paradigms for Longitudinal NLP](https://arxiv.org/abs/2601.07988)
*Adithya V Ganesan,Vasudha Varadarajan,Oscar NE Kjell,Whitney R Ringwald,Scott Feltman,Benjamin J Luft,Roman Kotov,Ryan L Boyd,H Andrew Schwartz*

Main category: cs.CL

TL;DR: 本文提出了一种针对纵向时间序列数据的自然语言处理评估框架，更新了四方面内容：评估集、性能度量、输入序列和模型内部机制。通过对比传统方法和新框架的结果，证明了传统方法的局限性，并推动NLP从单纯词序评价转向行为序列评价。


<details>
  <summary>Details</summary>
Motivation: 本文的动机在于识别常规自然语言处理假设（文档独立且无序）在纵向研究中的限制，并提出一个更符合数据实际结构的新框架，尤其是处理与个人和时间相关的纵向行为序列。

Method: 作者首先定义了纵向行为序列的概念，并提出了四种方法来改进现有的NLP处理流程：调整评估集以支持跨截面和前瞻性评估；区分个体间差异与个体内动态的性能度量；默认使用序列输入来包括历史信息；以及支持不同历史粒度的潜在状态模型，这些模型可以是聚合总结、显式动力学或基于交互的。

Result: 通过在包含238名参与者和17000个日记条目的数据集上应用并对比传统与改进后的模型，发现传统文档级评估方法可能导致截然不同的结果，有时甚至是相反的结果。这表明新框架提供的更连贯的行为序列模型具有更高的有效性。

Conclusion: 本文的结论是，有必要从单纯的词序列评价转向行为序列评价，以捕捉纵向数据的内在复杂性和动态性。作者强调了其框架在实践中的重要性，并对未来研究提出了一些建议。

Abstract: While NLP typically treats documents as independent and unordered samples, in longitudinal studies, this assumption rarely holds: documents are nested within authors and ordered in time, forming person-indexed, time-ordered $\textit{behavioral sequences}$. Here, we demonstrate the need for and propose a longitudinal modeling and evaluation paradigm that consequently updates four parts of the NLP pipeline: (1) evaluation splits aligned to generalization over people ($\textit{cross-sectional}$) and/or time ($\textit{prospective}$); (2) accuracy metrics separating between-person differences from within-person dynamics; (3) sequence inputs to incorporate history by default; and (4) model internals that support different $\textit{coarseness}$ of latent state over histories (pooled summaries, explicit dynamics, or interaction-based models). We demonstrate the issues ensued by traditional pipeline and our proposed improvements on a dataset of 17k daily diary transcripts paired with PTSD symptom severity from 238 participants, finding that traditional document-level evaluation can yield substantially different and sometimes reversed conclusions compared to our ecologically valid modeling and evaluation. We tie our results to a broader discussion motivating a shift from word-sequence evaluation toward $\textit{behavior-sequence}$ paradigms for NLP.

</details>


### [92] [DYCP: Dynamic Context Pruning for Long-Form Dialogue with LLMs](https://arxiv.org/abs/2601.07994)
*Nayoung Choi,Jonathan Zhang,Jinho D. Choi*

Main category: cs.CL

TL;DR: DyCP是一种轻量级的上下文管理方法，能够在查询时动态分割和检索相关记忆，而不破坏对话的顺序结构，能够提升答案质量并降低响应延迟。


<details>
  <summary>Details</summary>
Motivation: LLMs在长对话中会表现出响应延迟增加和答案质量下降的问题，现有的方法通常依赖额外的LLM调用或离线记忆构建，这可能导致效率低下或打断对话连续性。因此，研究一种不需要额外调用或离线构建且能适应当前用户输入的方法成为必要。

Method: DyCP通过在查询时动态分割对话，并根据上下文需求检索相关信息，保留对话的顺序结构，而无需预设话题边界。

Result: DyCP在LoCoMo、MT-Bench+和SCM4LLMs三个长对话基准以及多种LLM上的实验表明，它可以持续提升答案质量并减少响应延迟。

Conclusion: DyCP强调了有效上下文管理的持续重要性，并且通过分析现代LLMs扩展的上下文窗口与其实际长上下文处理能力之间的差距，为理解长对话环境下的LLM性能提供了新的视角。

Abstract: Large Language Models (LLMs) often exhibit increased response latency and degraded answer quality as dialogue length grows, making effective context management essential. However, existing methods rely on extra LLM calls to build memory or perform offline memory construction without considering the current user utterance, which can introduce inefficiencies or disrupt conversational continuity. We introduce DyCP, a lightweight context management method that dynamically segment and retrieve relevant memory at query time. It preserves the sequential structure of dialogue without predefined topic boundaries and supports efficient, adaptive context retrieval. Across three long-form dialogue benchmarks, LoCoMo, MT-Bench+, and SCM4LLMs, and multiple LLMs, DyCP consistently improves answer quality while reducing response latency. We also examine the gap between modern LLMs' expanded context windows and their actual long-context processing capacity, highlighting the continued importance of effective context management.

</details>


### [93] [Is Sentiment Banana-Shaped? Exploring the Geometry and Portability of Sentiment Concept Vectors](https://arxiv.org/abs/2601.07995)
*Laurits Lyngbaek,Pascale Feldkamp,Yuri Bizzoni,Kristoffer L. Nielbo,Kenneth Enevoldsen*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Use cases of sentiment analysis in the humanities often require contextualized, continuous scores. Concept Vector Projections (CVP) offer a recent solution: by modeling sentiment as a direction in embedding space, they produce continuous, multilingual scores that align closely with human judgments. Yet the method's portability across domains and underlying assumptions remain underexplored. We evaluate CVP across genres, historical periods, languages, and affective dimensions, finding that concept vectors trained on one corpus transfer well to others with minimal performance loss. To understand the patterns of generalization, we further examine the linearity assumption underlying CVP. Our findings suggest that while CVP is a portable approach that effectively captures generalizable patterns, its linearity assumption is approximate, pointing to potential for further development.

</details>


### [94] [LLM Review: Enhancing Creative Writing via Blind Peer Review Feedback](https://arxiv.org/abs/2601.08003)
*Weiyue Li,Mingxiao Song,Zhenda Shen,Dachuan Zhao,Yunfan Long,Yi Li,Yongce Li,Ruyi Yang,Mengyu Wang*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large Language Models (LLMs) often struggle with creative generation, and multi-agent frameworks that improve reasoning through interaction can paradoxically hinder creativity by inducing content homogenization. We introduce LLM Review, a peer-review-inspired framework implementing Blind Peer Review: agents exchange targeted feedback while revising independently, preserving divergent creative trajectories. To enable rigorous evaluation, we propose SciFi-100, a science fiction writing dataset with a unified framework combining LLM-as-a-judge scoring, human annotation, and rule-based novelty metrics. Experiments demonstrate that LLM Review consistently outperforms multi-agent baselines, and smaller models with our framework can surpass larger single-agent models, suggesting interaction structure may substitute for model scale.

</details>


### [95] [Reasoning Beyond Chain-of-Thought: A Latent Computational Mode in Large Language Models](https://arxiv.org/abs/2601.08058)
*Zhenghao He,Guangzhi Xiong,Bohan Liu,Sanchit Sinha,Aidong Zhang*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Chain-of-Thought (CoT) prompting has improved the reasoning performance of large language models (LLMs), but it remains unclear why it works and whether it is the unique mechanism for triggering reasoning in large language models. In this work, we study this question by directly analyzing and intervening on the internal representations of LLMs with Sparse Autoencoders (SAEs), identifying a small set of latent features that are causally associated with LLM reasoning behavior. Across multiple model families and reasoning benchmarks, we find that steering a single reasoning-related latent feature can substantially improve accuracy without explicit CoT prompting. For large models, latent steering achieves performance comparable to standard CoT prompting while producing more efficient outputs. We further observe that this reasoning-oriented internal state is triggered early in generation and can override prompt-level instructions that discourage explicit reasoning. Overall, our results suggest that multi-step reasoning in LLMs is supported by latent internal activations that can be externally activated, while CoT prompting is one effective, but not unique, way of activating this mechanism rather than its necessary cause.

</details>


### [96] [Universal computation is intrinsic to language model decoding](https://arxiv.org/abs/2601.08061)
*Alex Lewandowski,Marlos C. Machado,Dale Schuurmans*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Language models now provide an interface to express and often solve general problems in natural language, yet their ultimate computational capabilities remain a major topic of scientific debate. Unlike a formal computer, a language model is trained to autoregressively predict successive elements in human-generated text. We prove that chaining a language model's autoregressive output is sufficient to perform universal computation. That is, a language model can simulate the execution of any algorithm on any input. The challenge of eliciting desired computational behaviour can thus be reframed in terms of programmability: the ease of finding a suitable prompt. Strikingly, we demonstrate that even randomly initialized language models are capable of universal computation before training. This implies that training does not give rise to computational expressiveness -- rather, it improves programmability, enabling a natural language interface for accessing these intrinsic capabilities.

</details>


### [97] [Calibration Is Not Enough: Evaluating Confidence Estimation Under Language Variations](https://arxiv.org/abs/2601.08064)
*Yuxi Xia,Dennis Ulmer,Terra Blevins,Yihong Liu,Hinrich Schütze,Benjamin Roth*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Confidence estimation (CE) indicates how reliable the answers of large language models (LLMs) are, and can impact user trust and decision-making. Existing work evaluates CE methods almost exclusively through calibration, examining whether stated confidence aligns with accuracy, or discrimination, whether confidence is ranked higher for correct predictions than incorrect ones. However, these facets ignore pitfalls of CE in the context of LLMs and language variation: confidence estimates should remain consistent under semantically equivalent prompt or answer variations, and should change when the answer meaning differs. Therefore, we present a comprehensive evaluation framework for CE that measures their confidence quality on three new aspects: robustness of confidence against prompt perturbations, stability across semantic equivalent answers, and sensitivity to semantically different answers. In our work, we demonstrate that common CE methods for LLMs often fail on these metrics: methods that achieve good performance on calibration or discrimination are not robust to prompt variations or are not sensitive to answer changes. Overall, our framework reveals limitations of existing CE evaluations relevant for real-world LLM use cases and provides practical guidance for selecting and designing more reliable CE methods.

</details>


### [98] [AdaJudge: Adaptive Multi-Perspective Judging for Reward Modeling](https://arxiv.org/abs/2601.08097)
*Yongliang Miao,Yangyang Liang,Mengnan Du*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Reward modeling is essential for aligning large language models with human preferences, yet predominant architectures rely on a static pooling strategy to condense sequences into scalar scores. This paradigm, however, suffers from two key limitations: a static inductive bias that misaligns with task-dependent preference signals, and a representational mismatch, as the backbone is optimized for generation rather than fine-grained discrimination. To address this, we propose AdaJudge, a unified framework that jointly adapts representation and aggregation. AdaJudge first refines backbone representations into a discrimination-oriented space via gated refinement blocks. It then replaces the static readout with an adaptive multi-view pooling module that dynamically routes and combines evidence. Extensive experiments on RM-Bench and JudgeBench show that AdaJudge outperforms strong off-the-shelf reward models and traditional pooling baselines.

</details>


### [99] [Query Suggestion for Retrieval-Augmented Generation via Dynamic In-Context Learning](https://arxiv.org/abs/2601.08105)
*Fabian Spaeh,Tianyi Chen,Chen-Hao Chiang,Bin Shen*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Retrieval-augmented generation with tool-calling agents (agentic RAG) has become increasingly powerful in understanding, processing, and responding to user queries. However, the scope of the grounding knowledge is limited and asking questions that exceed this scope may lead to issues like hallucination. While guardrail frameworks aim to block out-of-scope questions (Rodriguez et al., 2024), no research has investigated the question of suggesting answerable queries in order to complete the user interaction.
  In this paper, we initiate the study of query suggestion for agentic RAG. We consider the setting where user questions are not answerable, and the suggested queries should be similar to aid the user interaction. Such scenarios are frequent for tool-calling LLMs as communicating the restrictions of the tools or the underlying datasets to the user is difficult, and adding query suggestions enhances the interaction with the RAG agent. As opposed to traditional settings for query recommendations such as in search engines, ensuring that the suggested queries are answerable is a major challenge due to the RAG's multi-step workflow that demands a nuanced understanding of the RAG as a whole, which the executing LLM lacks. As such, we introduce robust dynamic few-shot learning which retrieves examples from relevant workflows. We show that our system can be self-learned, for instance on prior user queries, and is therefore easily applicable in practice. We evaluate our approach on three benchmark datasets based on two unlabeled question datasets collected from real-world user queries. Experiments on real-world datasets confirm that our method produces more relevant and answerable suggestions, outperforming few-shot and retrieval-only baselines, and thus enable safer, more effective user interaction with agentic RAG.

</details>


### [100] [Debiasing Large Language Models via Adaptive Causal Prompting with Sketch-of-Thought](https://arxiv.org/abs/2601.08108)
*Bowen Li,Ziqi Xu,Jing Ren,Renqiang Luo,Xikun Zhang,Xiuzhen Zhang,Yongli Ren,Feng Xia*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Despite notable advancements in prompting methods for Large Language Models (LLMs), such as Chain-of-Thought (CoT), existing strategies still suffer from excessive token usage and limited generalisability across diverse reasoning tasks. To address these limitations, we propose an Adaptive Causal Prompting with Sketch-of-Thought (ACPS) framework, which leverages structural causal models to infer the causal effect of a query on its answer and adaptively select an appropriate intervention (i.e., standard front-door and conditional front-door adjustments). This design enables generalisable causal reasoning across heterogeneous tasks without task-specific retraining. By replacing verbose CoT with concise Sketch-of-Thought, ACPS enables efficient reasoning that significantly reduces token usage and inference cost. Extensive experiments on multiple reasoning benchmarks and LLMs demonstrate that ACPS consistently outperforms existing prompting baselines in terms of accuracy, robustness, and computational efficiency.

</details>


### [101] [Attention Projection Mixing and Exogenous Anchors](https://arxiv.org/abs/2601.08131)
*Jonathan Su*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Transformers that reuse early-layer attention projections as residuals face a fundamental tension: the first layer must simultaneously serve as a stable reference for all deeper layers and as an effective computational block. To resolve this, we propose ExoFormer, which learns dedicated exogenous anchor projections outside the sequential layer stack, decoupling the anchor role from computational refinement. Through a unified normalized mixing framework (studying different coefficient granularities: elementwise, headwise, scalar) across all attention pathways (queries, keys, values, and gate logits), ExoFormer variants consistently outperform their internal-anchor counterparts. Moreover, the dynamic variant achieves a 2.13-point increase in downstream accuracy over the baseline and demonstrates superior data efficiency, matching baseline validation loss with 1.84x fewer tokens. ExoFormer also achieves a 2x reduction in attention sink compared to standard Gated Attention. Paradoxically, all ExoFormer variants exhibit signs of representation collapse. We explain this via an Offloading Hypothesis: external anchors preserve essential token identity, allowing layers to specialize exclusively in computational refinement. We release codes and models to facilitate future research.

</details>


### [102] [How Reliable are Confidence Estimators for Large Reasoning Models? A Systematic Benchmark on High-Stakes Domains](https://arxiv.org/abs/2601.08134)
*Reza Khanmohammadi,Erfan Miahi,Simerjot Kaur,Ivan Brugere,Charese H. Smiley,Kundan Thind,Mohammad M. Ghassemi*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The miscalibration of Large Reasoning Models (LRMs) undermines their reliability in high-stakes domains, necessitating methods to accurately estimate the confidence of their long-form, multi-step outputs. To address this gap, we introduce the Reasoning Model Confidence estimation Benchmark (RMCB), a public resource of 347,496 reasoning traces from six popular LRMs across different architectural families. The benchmark is constructed from a diverse suite of datasets spanning high-stakes domains, including clinical, financial, legal, and mathematical reasoning, alongside complex general reasoning benchmarks, with correctness annotations provided for all samples. Using RMCB, we conduct a large-scale empirical evaluation of over ten distinct representation-based methods, spanning sequential, graph-based, and text-based architectures. Our central finding is a persistent trade-off between discrimination (AUROC) and calibration (ECE): text-based encoders achieve the best AUROC (0.672), while structurally-aware models yield the best ECE (0.148), with no single method dominating both. Furthermore, we find that increased architectural complexity does not reliably outperform simpler sequential baselines, suggesting a performance ceiling for methods relying solely on chunk-level hidden states. This work provides the most comprehensive benchmark for this task to date, establishing rigorous baselines and demonstrating the limitations of current representation-based paradigms.

</details>


### [103] [Qalb: Largest State-of-the-Art Urdu Large Language Model for 230M Speakers with Systematic Continued Pre-training](https://arxiv.org/abs/2601.08141)
*Muhammad Taimoor Hassan,Jawad Ahmed,Muhammad Awais*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Despite remarkable progress in large language models, Urdu-a language spoken by over 230 million people-remains critically underrepresented in modern NLP systems. Existing multilingual models demonstrate poor performance on Urdu-specific tasks, struggling with the language's complex morphology, right-to-left Nastaliq script, and rich literary traditions. Even the base LLaMA-3.1 8B-Instruct model shows limited capability in generating fluent, contextually appropriate Urdu text. We introduce Qalb, an Urdu language model developed through a two-stage approach: continued pre-training followed by supervised fine-tuning. Starting from LLaMA 3.1 8B, we perform continued pre-training on a dataset of 1.97 billion tokens. This corpus comprises 1.84 billion tokens of diverse Urdu text-spanning news archives, classical and contemporary literature, government documents, and social media-combined with 140 million tokens of English Wikipedia data to prevent catastrophic forgetting. We then fine-tune the resulting model on the Alif Urdu-instruct dataset. Through extensive evaluation on Urdu-specific benchmarks, Qalb demonstrates substantial improvements, achieving a weighted average score of 90.34 and outperforming the previous state-of-the-art Alif-1.0-Instruct model (87.1) by 3.24 points, while also surpassing the base LLaMA-3.1 8B-Instruct model by 44.64 points. Qalb achieves state-of-the-art performance with comprehensive evaluation across seven diverse tasks including Classification, Sentiment Analysis, and Reasoning. Our results demonstrate that continued pre-training on diverse, high-quality language data, combined with targeted instruction fine-tuning, effectively adapts foundation models to low-resource languages.

</details>


### [104] [Relational Knowledge Distillation Using Fine-tuned Function Vectors](https://arxiv.org/abs/2601.08169)
*Andrea Kang,Yingnian Wu,Hongjing Lu*

Main category: cs.CL

TL;DR: 该研究通过关注少量注意力头部，并使用这些头部微调功能向量，提高了关系词填充任务的表现，并引入了一个新的组合功能向量，增强了复杂类比问题的推理能力。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索如何有效表示概念间的关系，以增强智能系统的解释能力和类比推理能力，从而使得模型更加具有智能。

Method: 研究采用因果中介分析生成的功能向量，并通过少量样本微调这些向量来改进关系词填充任务的表现。此外，研究通过将一个包含细调后的功能向量的组合向量插入到大模型激活中，来提升复杂类比问题的解决能力。

Result: 微调后的功能向量在多种语言模型中表现更好，特别是在关系词填充任务上。组合功能向量则在解决复杂类比问题上表现出色。

Conclusion: 研究表明，激活斑块（activation patching）作为一种可控机制，有助于编码和操控关系知识，提升了大型语言模型的可解释性和推理能力。

Abstract: Representing relations between concepts is a core prerequisite for intelligent systems to make sense of the world. Recent work using causal mediation analysis has shown that a small set of attention heads encodes task representation in in-context learning, captured in a compact representation known as the function vector. We show that fine-tuning function vectors with only a small set of examples (about 20 word pairs) yields better performance on relation-based word-completion tasks than using the original vectors derived from causal mediation analysis. These improvements hold for both small and large language models. Moreover, the fine-tuned function vectors yield improved decoding performance for relation words and show stronger alignment with human similarity judgments of semantic relations. Next, we introduce the composite function vector - a weighted combination of fine-tuned function vectors - to extract relational knowledge and support analogical reasoning. At inference time, inserting this composite vector into LLM activations markedly enhances performance on challenging analogy problems drawn from cognitive science and SAT benchmarks. Our results highlight the potential of activation patching as a controllable mechanism for encoding and manipulating relational knowledge, advancing both the interpretability and reasoning capabilities of large language models.

</details>


### [105] [Prompt-Based Clarity Evaluation and Topic Detection in Political Question Answering](https://arxiv.org/abs/2601.08176)
*Lavanya Prahallad,Sai Utkarsh Choudarypally,Pragna Prahallad,Pranathi Prahallad*

Main category: cs.CL

TL;DR: 本研究利用 CLARITY 数据集评估不同提示策略下的语言模型 clarity 和 evasion 能力，研究发现链式思考提示和链式思考结合少量示例提示策略显著提高了模型的 clarity 和 abstract 的话题识别准确性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）的发展，自动评估 LLM 的响应不仅需要事实正确性，还需要清晰性，尤其是在处理政治问题时。尽管已有数据集为清晰性和误导提供人工标注，但提示设计对自动清晰性评估的影响尚未广泛探讨。

Method: 研究使用来自 SemEval 2026 共享任务的 CLARITY 数据集，比较了 GPT-3.5 基线模型与 GPT-5.2 在三种提示策略下的表现：简单提示、链式思考提示和链式思考结合少量示例。模型预测结果依据人工标注的清晰性和误导指标进行评估。

Result: 研究结果显示，GPT-5.2 在清晰性预测方面始终优于 GPT-3.5 基线模型。采用链式思考结合少量示例提示策略时，准确率从 56% 提高到 63%。链式思考提示在误导准确性方面达到 34%，但不同细分子类别的精度改进不甚稳定。此外，链式思考提示也提高了主题识别准确性，从 60% 提高到 74%。

Conclusion: 研究结论表明，提示设计能可靠地提高高级清晰性评估，而细粒度的误导识别和话题检测仍然具有挑战性，即使使用结构化推理提示。

Abstract: Automatic evaluation of large language model (LLM) responses requires not only factual correctness but also clarity, particularly in political question-answering. While recent datasets provide human annotations for clarity and evasion, the impact of prompt design on automatic clarity evaluation remains underexplored. In this paper, we study prompt-based clarity evaluation using the CLARITY dataset from the SemEval 2026 shared task. We compare a GPT-3.5 baseline provided with the dataset against GPT-5.2 evaluated under three prompting strategies: simple prompting, chain-of-thought prompting, and chain-of-thought with few-shot examples. Model predictions are evaluated against human annotations using accuracy and class-wise metrics for clarity and evasion, along with hierarchical exact match. Results show that GPT-5.2 consistently outperforms the GPT-3.5 baseline on clarity prediction, with accuracy improving from 56 percent to 63 percent under chain-of-thought with few-shot prompting. Chain-of-thought prompting yields the highest evasion accuracy at 34 percent, though improvements are less stable across fine-grained evasion categories. We further evaluate topic identification and find that reasoning-based prompting improves accuracy from 60 percent to 74 percent relative to human annotations. Overall, our findings indicate that prompt design reliably improves high-level clarity evaluation, while fine-grained evasion and topic detection remain challenging despite structured reasoning prompts.

</details>


### [106] [Triplets Better Than Pairs: Towards Stable and Effective Self-Play Fine-Tuning for LLMs](https://arxiv.org/abs/2601.08198)
*Yibo Wang,Hai-Long Sun,Qing-Guo Chen,Zhao Xu,Weihua Luo,Kaifu Zhang,Lijun Zhang*

Main category: cs.CL

TL;DR: T-SPIN 方法通过引入历史优势和熵约束克服了现有自博弈微调（SPIN）方法中的优化不稳定性和训练-生成不一致性问题，从而在有限标注数据下表现出更优性能。


<details>
  <summary>Details</summary>
Motivation: SPIN 方法由于优化数值随迭代褪减以及训练过程中参考策略与生成机制不一致的问题导致性能不稳，T-SPIN 方法通过整合历史优势和引入熵约束来解决这两个问题，以实现更稳定的优化。

Method: T-SPIN 方法不仅在生成过程中考虑了当前生成样本与初始策略生成的原型样本之间的历史优势，而且引入了熵约束来支持无参考的微调，从而减少训练和生成过程中的不一致性。

Result: 在多个任务上的实验结果表明，与 SPIN 方法相比，T-SPIN 在性能上更优，且具有更稳定的迭代过程。甚至在注释样例数量仅为 25% 的情况下，T-SPIN 依然能够达到或超越传统的监督微调效果。

Conclusion: T-SPIN 方法通过引入历史优势和熵约束解决了 SPIN 方法中存在的优化不稳定性和训练-生成不一致性问题，展示了在稀缺标注数据条件下的有效性。

Abstract: Recently, self-play fine-tuning (SPIN) has been proposed to adapt large language models to downstream applications with scarce expert-annotated data, by iteratively generating synthetic responses from the model itself. However, SPIN is designed to optimize the current reward advantages of annotated responses over synthetic responses at hand, which may gradually vanish during iterations, leading to unstable optimization. Moreover, the utilization of reference policy induces a misalignment issue between the reward formulation for training and the metric for generation. To address these limitations, we propose a novel Triplet-based Self-Play fIne-tuNing (T-SPIN) method that integrates two key designs. First, beyond current advantages, T-SPIN additionally incorporates historical advantages between iteratively generated responses and proto-synthetic responses produced by the initial policy. Even if the current advantages diminish, historical advantages remain effective, stabilizing the overall optimization. Second, T-SPIN introduces the entropy constraint into the self-play framework, which is theoretically justified to support reference-free fine-tuning, eliminating the training-generation discrepancy. Empirical results on various tasks demonstrate not only the superior performance of T-SPIN over SPIN, but also its stable evolution during iterations. Remarkably, compared to supervised fine-tuning, T-SPIN achieves comparable or even better performance with only 25% samples, highlighting its effectiveness when faced with scarce annotated data.

</details>


### [107] [Generation-Augmented Generation: A Plug-and-Play Framework for Private Knowledge Injection in Large Language Models](https://arxiv.org/abs/2601.08209)
*Rongji Li,Jian Xu,Xueqing Chen,Yisheng Yang,Jiayi Wang,Xingyu Chen,Chunyu Xie,Dawei Leng,Xu-Yao Zhang*

Main category: cs.CL

TL;DR: GAG通过将私人专业知识作为附加专家模态进行注射，从而在无需在生成时进行证据序列化的情况下提升了专家表现，同时保持了对各种开放基准的一致表现，并支持多领域部署，这在两个私人科学QA基准和混合领域评估中得到了验证。


<details>
  <summary>Details</summary>
Motivation: 当前的两种主要方法（即调整和检索增强生成）在大规模语言模型的私人知识注入方面存在明显的局限性。GAG的目的是通过一种紧凑的、基于表示层次的接口将私人专业知识嵌入到固定的基础模型中，从而避免生成时的证据序列化，并实现即插即用的专业化和多领域可扩展的组合。

Method: GAG通过将私人专业知识视为附加的专家模态来进行注入，这种注入通过一个与冻结的基础模型对齐的紧凑表示接口完成。这种方法避免了生成时的证据序列化，并支持可靠的选择性激活，从而实现在多领域部署中的可扩展性。

Result: GAG在两个私人科学QA基准（免疫佐剂、催化材料）和混合域评估中，相对于强大的检索增强生成基线，分别提高了15.34%和14.86%的专家性能，同时在六个开放通用基准上保持了性能，还支持了近乎理想的可选择激活以实现多领域部署。

Conclusion: GAG提供了一种创新的方法来解决私人知识注入问题，通过将私人知识工整地集成到固定的预训练基础模型中，成功改进了专业性能，支持了多领域的部署，并提供了可扩展性和选择性激活。

Abstract: In domains such as biomedicine, materials, and finance, high-stakes deployment of large language models (LLMs) requires injecting private, domain-specific knowledge that is proprietary, fast-evolving, and under-represented in public pretraining. However, the two dominant paradigms for private knowledge injection each have pronounced drawbacks: fine-tuning is expensive to iterate, and continual updates risk catastrophic forgetting and general-capability regression; retrieval-augmented generation (RAG) keeps the base model intact but is brittle in specialized private corpora due to chunk-induced evidence fragmentation, retrieval drift, and long-context pressure that yields query-dependent prompt inflation. Inspired by how multimodal LLMs align heterogeneous modalities into a shared semantic space, we propose Generation-Augmented Generation (GAG), which treats private expertise as an additional expert modality and injects it via a compact, representation-level interface aligned to the frozen base model, avoiding prompt-time evidence serialization while enabling plug-and-play specialization and scalable multi-domain composition with reliable selective activation. Across two private scientific QA benchmarks (immunology adjuvant and catalytic materials) and mixed-domain evaluations, GAG improves specialist performance over strong RAG baselines by 15.34% and 14.86% on the two benchmarks, respectively, while maintaining performance on six open general benchmarks and enabling near-oracle selective activation for scalable multi-domain deployment.

</details>


### [108] [Towards Principled Design of Mixture-of-Experts Language Models under Memory and Inference Constraints](https://arxiv.org/abs/2601.08215)
*Seng Pei Liew,Kenta Shinzato,Yuyang Dong*

Main category: cs.CL

TL;DR: 该研究发现 MoE 表现主要由总量参数和专家稀疏性决定，提出了在给定约束下最大化总量参数、同时最小化专家稀疏性和专家数量的原则。


<details>
  <summary>Details</summary>
Motivation: 传统的 MoE 架构设计主要依赖于总量参数和激活参数，但研究发现这两者并不足以描述最优架构。特别是在专家稀疏性上，没有考虑专家数量与 topk 数量对性能的影响。

Method: 通过系统研究，发现 MoE 性能主要由总量参数和专家稀疏性决定，提出了新的优化原则。

Result: 提出了在给定约束下最大化总量参数、同时最小化专家稀疏性和专家数量的 MoE 设计原则。

Conclusion: 研究成果提供了一个坚实的框架，用于解决架构模糊性并指导 MoE 设计。

Abstract: Modern Mixture-of-Experts (MoE) language models are designed based on total parameters (memory footprint) and active parameters (inference cost). However, we find these two factors alone are insufficient to describe an optimal architecture. Through a systematic study, we demonstrate that MoE performance is primarily determined by total parameters ($N_{total}$) and expert sparsity ($s:=n_{exp}/n_{topk}$).
  Moreover, $n_{exp}$ and $n_{topk}$ do not "cancel out" within the sparsity ratio; instead, a larger total number of experts slightly penalizes performance by forcing a reduction in core model dimensions (depth and width) to meet memory constraints. This motivates a simple principle for MoE design which maximizes $N_{total}$ while minimizing $s$ (maximizing $n_{topk}$) and $n_{exp}$ under the given constraints. Our findings provide a robust framework for resolving architectural ambiguity and guiding MoE design.

</details>


### [109] [User-Oriented Multi-Turn Dialogue Generation with Tool Use at scale](https://arxiv.org/abs/2601.08225)
*Jungho Cho,Minbyul Jeong,Sungrae Park*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The recent paradigm shift toward large reasoning models (LRMs) as autonomous agents has intensified the demand for sophisticated, multi-turn tool-use capabilities. Yet, existing datasets and data-generation approaches are limited by static, predefined toolsets that cannot scale to the complexity of open-ended human-agent collaboration. To address this, we initially developed a framework for automated task-oriented multi-turn dialogue generation at scale, utilizing an LRM-based simulator to dynamically generate high-value, domain-specific tools to solve specified tasks. However, we observe that a purely task-oriented design often results in "solely task-solving" trajectories, where the agent completes the objective with minimal interaction, failing to generate the high turn-count conversations seen in realistic scenarios. To bridge this gap, we shift toward a user-oriented simulation paradigm. By decoupling task generation from a dedicated user simulator that mimics human behavioral rules - such as incremental request-making and turn-by-turn feedback - we facilitate more authentic, extended multi-turn dialogues that reflect the iterative nature of real-world problem solving. Our generation pipeline operates as a versatile, plug-and-play module capable of initiating generation from any state, ensuring high scalability in producing extended tool-use data. Furthermore, by facilitating multiple task completions within a single trajectory, it yields a high-density dataset that reflects the multifaceted demands of real-world human-agent interaction.

</details>


### [110] [Med-CoReasoner: Reducing Language Disparities in Medical Reasoning via Language-Informed Co-Reasoning](https://arxiv.org/abs/2601.08267)
*Fan Gao,Sherry T. Tong,Jiwoong Sohn,Jiahao Huang,Junfeng Jiang,Ding Xia,Piyalitt Ittichaiwong,Kanyakorn Veerakanjana,Hyunjae Kim,Qingyu Chen,Edison Marrese Taylor,Kazuma Kobayashi,Akkiko Aizawa,Irene Li*

Main category: cs.CL

TL;DR: Med-CoReasoner 是一种结合了英语文本推理结构和本地语言实践知识的多语言医疗推理框架，通过概念级对齐和检索将本地临床知识整合到英语逻辑框架中。该框架通过多语言基准测试显著提升了多语言医疗推理性能。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型在多语言医疗任务中的推理能力相对较弱，特别是在低资源语言中，存在明显的语言差距，因此提出了 Med-CoReasoner 以弥合这一差距。

Method: Med-CoReasoner 利用一种基于语言的信息共推理框架，提取双语推理并将其抽象为结构化概念。通过概念级对齐和检索，将本地临床知识整合进英语逻辑框架。

Result: 在三个基准测试上的实验结果显示，Med-CoReasoner 的多语言医疗推理性能平均提高了 5%，特别是在低资源语言中表现出显著提升。

Conclusion: 研究进一步表明，通过模型提炼和专家评估，Med-CoReasoner 产生的推理过程不仅临床相关，而且文化根基深厚。

Abstract: While reasoning-enhanced large language models perform strongly on English medical tasks, a persistent multilingual gap remains, with substantially weaker reasoning in local languages, limiting equitable global medical deployment. To bridge this gap, we introduce Med-CoReasoner, a language-informed co-reasoning framework that elicits parallel English and local-language reasoning, abstracts them into structured concepts, and integrates local clinical knowledge into an English logical scaffold via concept-level alignment and retrieval. This design combines the structural robustness of English reasoning with the practice-grounded expertise encoded in local languages. To evaluate multilingual medical reasoning beyond multiple-choice settings, we construct MultiMed-X, a benchmark covering seven languages with expert-annotated long-form question answering and natural language inference tasks, comprising 350 instances per language. Experiments across three benchmarks show that Med-CoReasoner improves multilingual reasoning performance by an average of 5%, with particularly substantial gains in low-resource languages. Moreover, model distillation and expert evaluation analysis further confirm that Med-CoReasoner produces clinically sound and culturally grounded reasoning traces.

</details>


### [111] [Discovery and Reinforcement of Tool-Integrated Reasoning Chains via Rollout Trees](https://arxiv.org/abs/2601.08274)
*Kun Li,Zenan Xu,Junan Li,Zengrui Jin,Jinghao Deng,Zexuan Qiu,Bo Zhou*

Main category: cs.CL

TL;DR: DART 是一种新颖的强化学习框架，能够在不需要人工标注的情况下实现长链推理过程中的自发工具使用，通过动态树结构探索有效的工具使用机会，并通过路径优势估计识别及奖励这些有益的行为，从而显著提升了工具与长链推理的和谐共生。


<details>
  <summary>Details</summary>
Motivation: 当前的大型语言模型（LLMs）缺乏集成工具使用的长久推理能力，主要原因在于训练数据稀缺与工具集成可能影响模型内在的久远推理能力。DART 的提出旨在解决这一问题，通过使模型在长推理链中自发使用工具，为 LLMs 引入了增强功能。

Method: DART 采用了一个基于强化学习（RL）的框架，通过构建训练过程中的动态展开树来发现有效的工具使用机会，并在有希望的位置进行分支以探索多样化的工具集成路径。它利用树基的过程优势估计来识别并奖励应用工具对解决方案有正面贡献的具体子路径，从而强化这些有利行为。

Result: 通过在挑战性基准测试集 AIME 和 GPQA-Diamond 上进行的广泛实验，DART 被证明能够显著优于现有方法，实现了工具使用与长时间推理的有效整合。

Conclusion: DART 展示了一种有效的增强学习方法，使大型语言模型能够在长时间推理过程中智能地和灵活地使用工具，从而显著提高了问题解决能力，并为增强 LLMs 的计算能力提供了新的可能途径。

Abstract: Tool-Integrated Reasoning has emerged as a key paradigm to augment Large Language Models (LLMs) with computational capabilities, yet integrating tool-use into long Chain-of-Thought (long CoT) remains underexplored, largely due to the scarcity of training data and the challenge of integrating tool-use without compromising the model's intrinsic long-chain reasoning. In this paper, we introduce DART (Discovery And Reinforcement of Tool-Integrated Reasoning Chains via Rollout Trees), a reinforcement learning framework that enables spontaneous tool-use during long CoT reasoning without human annotation. DART operates by constructing dynamic rollout trees during training to discover valid tool-use opportunities, branching out at promising positions to explore diverse tool-integrated trajectories. Subsequently, a tree-based process advantage estimation identifies and credits specific sub-trajectories where tool invocation positively contributes to the solution, effectively reinforcing these beneficial behaviors. Extensive experiments on challenging benchmarks like AIME and GPQA-Diamond demonstrate that DART significantly outperforms existing methods, successfully harmonizing tool execution with long CoT reasoning.

</details>


### [112] [D$^2$Plan: Dual-Agent Dynamic Global Planning for Complex Retrieval-Augmented Reasoning](https://arxiv.org/abs/2601.08282)
*Kangcheng Luo,Tinglang Wu,Yansong Feng*

Main category: cs.CL

TL;DR: D²Plan 引入了一种双智能体动态全局规划框架，以增强搜索增强推理任务中的全局计划制定和推理过程。通过监督微调和基于计划的奖励强化学习，D²Plan 能够更好地抵抗无关信息，实现复杂的问答基准测试中的卓越性能。


<details>
  <summary>Details</summary>
Motivation: 考虑到传统 search-augmented LLMs 在面对累积上下文中重要证据和无关信息时的弱点，提出 D²Plan 是为了改进这些任务中推理和搜索的有效性。具体来说，D²Plan 的目标是解决搜索引擎构建中产生错误查询或忽略关键信息的问题，以及无关证据导致模型误判的问题。

Method: D²Plan 通过定义 reasoner 和 purifier 两个智能体合作来工作。Reasoner 构建并动态调整全局计划，而 Purifier 评估检索的相关性并总结关键信息。D²Plan 还引入了一个两阶段训练框架，包括基于合成轨迹的监督微调阶段和具有计划导向奖励的强化学习阶段。

Result: D²Plan 在多项复杂 QA 基准测试中表现出更强的连贯多步推理能力以及对无关信息的更高抗性。结果表明该方法能够提升模型处理多跳推理任务的整体性能。

Conclusion: D²Plan 实现了一种创新的双智能体动态全局规划框架，显著改进了搜索增强推理任务的表现，在多跳问答场景中取得了出色的性能。

Abstract: Recent search-augmented LLMs trained with reinforcement learning (RL) can interleave searching and reasoning for multi-hop reasoning tasks. However, they face two critical failure modes as the accumulating context becomes flooded with both crucial evidence and irrelevant information: (1) ineffective search chain construction that produces incorrect queries or omits retrieval of critical information, and (2) reasoning hijacking by peripheral evidence that causes models to misidentify distractors as valid evidence. To address these challenges, we propose **D$^2$Plan**, a **D**ual-agent **D**ynamic global **Plan**ning paradigm for complex retrieval-augmented reasoning. **D$^2$Plan** operates through the collaboration of a *Reasoner* and a *Purifier*: the *Reasoner* constructs explicit global plans during reasoning and dynamically adapts them based on retrieval feedback; the *Purifier* assesses retrieval relevance and condenses key information for the *Reasoner*. We further introduce a two-stage training framework consisting of supervised fine-tuning (SFT) cold-start on synthesized trajectories and RL with plan-oriented rewards to teach LLMs to master the **D$^2$Plan** paradigm. Extensive experiments demonstrate that **D$^2$Plan** enables more coherent multi-step reasoning and stronger resilience to irrelevant information, thereby achieving superior performance on challenging QA benchmarks.

</details>


### [113] [Enhancing Sentiment Classification and Irony Detection in Large Language Models through Advanced Prompt Engineering Techniques](https://arxiv.org/abs/2601.08302)
*Marvin Schmitt,Anne Schwerk,Sebastian Lempert*

Main category: cs.CL

TL;DR: 本研究探讨了通过提示工程技术增强大型语言模型（LLMs），特别是GPT-4o-mini和gemini-1.5-flash，在情感分析任务中的应用。研究通过几种高级提示技术，如少量样本学习、链式思考提示及自我一致性，相比于基线方法进行了评估。研究结果表明，先进的提示显着提升了情感分析的性能，特别是few-shot提示在GPT-4o-mini上的效果最佳，chain-of-thought提示在gemini-1.5-flash上的讽刺检测效率提高了46%。这表明提示策略需根据不同模型和任务进行调整。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索如何通过提示工程技术有效提升大型语言模型在情感分析中的表现，以期找到适合不同模型和任务的最佳提示方法。

Method: 研究采用了几种高级提示技术，包括少量样本学习、链式思考提示及自我一致性，并分别在sentiment classification（情感分类）、aspect-based sentiment analysis（基于方面的情感分析）以及检测微妙差异（如讽刺）三项任务中进行评估。研究还通过准确率、召回率、精确率和F1分数的衡量标准来评估模型性能。

Result: 研究结果显示，高级提示技术在情感分析中有显著提升。few-shot提示在GPT-4o-mini上表现尤为出色，能够提升46%的讽刺检测准确率。

Conclusion: 本研究揭示了适应不同模型和任务的提示策略的重要性，表明尽管高级提示技术普遍提高了性能，但针对特定模型和任务选择最合适的提示技术策略仍然是关键问题。

Abstract: This study investigates the use of prompt engineering to enhance large language models (LLMs), specifically GPT-4o-mini and gemini-1.5-flash, in sentiment analysis tasks. It evaluates advanced prompting techniques like few-shot learning, chain-of-thought prompting, and self-consistency against a baseline. Key tasks include sentiment classification, aspect-based sentiment analysis, and detecting subtle nuances such as irony. The research details the theoretical background, datasets, and methods used, assessing performance of LLMs as measured by accuracy, recall, precision, and F1 score. Findings reveal that advanced prompting significantly improves sentiment analysis, with the few-shot approach excelling in GPT-4o-mini and chain-of-thought prompting boosting irony detection in gemini-1.5-flash by up to 46%. Thus, while advanced prompting techniques overall improve performance, the fact that few-shot prompting works best for GPT-4o-mini and chain-of-thought excels in gemini-1.5-flash for irony detection suggests that prompting strategies must be tailored to both the model and the task. This highlights the importance of aligning prompt design with both the LLM's architecture and the semantic complexity of the task.

</details>


### [114] [CLaS-Bench: A Cross-Lingual Alignment and Steering Benchmark](https://arxiv.org/abs/2601.08331)
*Daniil Gurgurov,Yusser Al Ghussin,Tanja Baeumel,Cheng-Ting Chou,Patrick Schramowski,Marius Mosbach,Josef van Genabith,Simon Ostermann*

Main category: cs.CL

TL;DR: CLaS-Bench 提供了一个评估大型语言模型多语言引导行为的轻量级基准，涵盖了多种引导技术，并发现基于残差的 DiffMean 方法在语言控制和语义相关性上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 随着多语言自然语言处理的发展，直接操控内部表示的引导技术成为了一种更高效且可解释的方法。然而，缺乏标准化的基准测试来量化这些技术的有效性。CLaS-Bench 的引入旨在系统性地评估多语言引导方法。

Method: CLaS-Bench 设计了一个包含 32 种语言的平行问题基准，评估了包括残差流 DiffMean 干预、探针提取方向、语言特定神经元、PCA/线性判别分析、稀疏自编码器和提示基线等广泛应用的引导技术。通过计算语言控制和语义相关性的调和均值得分来衡量引导效果。

Result: 研究发现，基于残差的 DiffMean 方法在所有语言中表现最佳。层分析显示语言特定结构主要存在于较深层，引导方向基于语言家族聚集。

Conclusion: CLaS-Bench 是首个标准化的多语言引导基准，为语言表示的科学分析和引导作为低成本适应方案的实践评估提供了基础。

Abstract: Understanding and controlling the behavior of large language models (LLMs) is an increasingly important topic in multilingual NLP. Beyond prompting or fine-tuning, , i.e.,~manipulating internal representations during inference, has emerged as a more efficient and interpretable technique for adapting models to a target language. Yet, no dedicated benchmarks or evaluation protocols exist to quantify the effectiveness of steering techniques. We introduce CLaS-Bench, a lightweight parallel-question benchmark for evaluating language-forcing behavior in LLMs across 32 languages, enabling systematic evaluation of multilingual steering methods. We evaluate a broad array of steering techniques, including residual-stream DiffMean interventions, probe-derived directions, language-specific neurons, PCA/LDA vectors, Sparse Autoencoders, and prompting baselines. Steering performance is measured along two axes: language control and semantic relevance, combined into a single harmonic-mean steering score. We find that across languages simple residual-based DiffMean method consistently outperforms all other methods. Moreover, a layer-wise analysis reveals that language-specific structure emerges predominantly in later layers and steering directions cluster based on language family. CLaS-Bench is the first standardized benchmark for multilingual steering, enabling both rigorous scientific analysis of language representations and practical evaluation of steering as a low-cost adaptation alternative.

</details>


### [115] [Detecting Mental Manipulation in Speech via Synthetic Multi-Speaker Dialogue](https://arxiv.org/abs/2601.08342)
*Run Chen,Wen Liang,Ziwei Gong,Lin Ai,Julia Hirschberg*

Main category: cs.CL

TL;DR: 本文介绍了首个基于语音的欺.empty


<details>
  <summary>Details</summary>
Motivation: 本文研究了语音中的心智操纵检测，填补了计算社会推理领域中的空白。

Method: 通过使用少量的大型音频语言模型和人类标注，评估多种模态对检测准确性和感知的影响。

Result: 结果显示，模型在语音中的检测精度比文本低，显示出对缺失的声学或语调线索的敏感性。

Conclusion: 研究表明，需要对于多模态对话系统进行模态感知的评估和安全对齐。

Abstract: Mental manipulation, the strategic use of language to covertly influence or exploit others, is a newly emerging task in computational social reasoning. Prior work has focused exclusively on textual conversations, overlooking how manipulative tactics manifest in speech. We present the first study of mental manipulation detection in spoken dialogues, introducing a synthetic multi-speaker benchmark SPEECHMENTALMANIP that augments a text-based dataset with high-quality, voice-consistent Text-to-Speech rendered audio. Using few-shot large audio-language models and human annotation, we evaluate how modality affects detection accuracy and perception. Our results reveal that models exhibit high specificity but markedly lower recall on speech compared to text, suggesting sensitivity to missing acoustic or prosodic cues in training. Human raters show similar uncertainty in the audio setting, underscoring the inherent ambiguity of manipulative speech. Together, these findings highlight the need for modality-aware evaluation and safety alignment in multimodal dialogue systems.

</details>


### [116] [PATS: Personality-Aware Teaching Strategies with Large Language Model Tutors](https://arxiv.org/abs/2601.08402)
*Donya Rooein,Sankalan Pal Chowdhury,Mariia Eremeeva,Yuan Qin,Debora Nozza,Mrinmaya Sachan,Dirk Hovy*

Main category: cs.CL

TL;DR: 本研究通过构建教学方法与人格特征的分类体系，并模拟学生与教师的对话，使大语言模型可以根据模拟学生的人格特征调整其教学策略，从而提高教学效果。


<details>
  <summary>Details</summary>
Motivation: 当前的大语言模型教学系统并未考虑到学生的人格特质，这可能导致教学效果不佳，本研究旨在通过将教学方法与人格特征进行关联，以实现更个性化和有效的大语言模型教学应用。

Method: 本研究首先构建了一个分类体系，将教学方法与人格特性进行分类。通过模拟学生与教师的对话，让大语言模型根据模拟学生的人格特性调整其教学策略。

Result: 与两个基准相比，人类教师更倾向于本研究的方法，并且人类和大语言模型的注释者也明显更喜欢这种方法使用了较少但影响更大的教学策略，如角色扮演。

Conclusion: 本研究推动了更个性化和有效的使用大语言模型于教育应用的发展。

Abstract: Recent advances in large language models (LLMs) demonstrate their potential as educational tutors. However, different tutoring strategies benefit different student personalities, and mismatches can be counterproductive to student outcomes. Despite this, current LLM tutoring systems do not take into account student personality traits. To address this problem, we first construct a taxonomy that links pedagogical methods to personality profiles, based on pedagogical literature. We simulate student-teacher conversations and use our framework to let the LLM tutor adjust its strategy to the simulated student personality. We evaluate the scenario with human teachers and find that they consistently prefer our approach over two baselines. Our method also increases the use of less common, high-impact strategies such as role-playing, which human and LLM annotators prefer significantly. Our findings pave the way for developing more personalized and effective LLM use in educational applications.

</details>


### [117] [Silence the Judge: Reinforcement Learning with Self-Verifier via Latent Geometric Clustering](https://arxiv.org/abs/2601.08427)
*Nonghai Zhang,Weitao Ma,Zhanyu Ma,Jun Xu,Jiuchong Gao,Jinghua Hao,Renqing He,Jingwen Xu*

Main category: cs.CL

TL;DR: Latent-GRPO 提出了一种新框架，通过直接从潜在空间几何中提取内在奖励，解决了 GRPO 对外依赖验证器的问题。实验表明这种方法提高了模型性能并加速了训练速度。


<details>
  <summary>Details</summary>
Motivation: GRPO 在增强 LLM 的推理性能方面取得显著效果，但需要依赖昂贵的外部验证器或人工规则，这增加了计算成本、训练延迟并阻碍了优化效率。因此，需要一个直接从潜在空间几何中提取奖励的框架来解决这些问题。

Method: Latent-GRPO 引入了 Iterative Robust Centroid Estimation (IRCE) 算法，该算法通过球面投影抑制幅度波动并迭代聚合以估计稳健的“真相质心”，从而生成密集连续的奖励。

Result: 实验证明，Latent-GRPO 保持了模型性能，训练速度比基线方法快近 2 倍，具有强大的泛化能力和鲁棒性。

Conclusion: Latent-GRPO 提供了一种新颖的框架，改善了 LLM 的推理性能，同时减少了对外部验证器的依赖，提高了训练效率。

Abstract: Group Relative Policy Optimization (GRPO) significantly enhances the reasoning performance of Large Language Models (LLMs). However, this success heavily relies on expensive external verifiers or human rules. Such dependency not only leads to significant computational costs and training latency, but also yields sparse rewards that hinder optimization efficiency. To address these challenges, we propose Latent-GRPO, a framework that derives intrinsic rewards directly from latent space geometry. Crucially, our empirical analysis reveals a compelling geometric property: terminal token representations of correct reasoning trajectories form dense clusters with high intra-class similarity, whereas incorrect trajectories remain scattered as outliers. In light of this discovery, we introduce the Iterative Robust Centroid Estimation (IRCE) algorithm, which generates dense, continuous rewards by mitigating magnitude fluctuations via spherical projection and estimating a robust ``truth centroid'' through iterative aggregation. Experimental results on multiple datasets show that our method maintains model performance while achieving a training speedup of over 2x compared to baselines. Furthermore, extensive results demonstrate strong generalization ability and robustness. The code will be released soon.

</details>


### [118] [JudgeRLVR: Judge First, Generate Second for Efficient Reasoning](https://arxiv.org/abs/2601.08468)
*Jiangshan Duo,Hanyu Li,Hailin Zhang,Yudong Wang,Sujian Li,Liang Zhao*

Main category: cs.CL

TL;DR: 本文提出了一种名为JudgeRLVR的两阶段判断-生成框架，通过先训练模型判断具有可验证答案的解决方案响应，在第二阶段对该模型进行微调以获取初始的基于RLVR的生成策略。与使用相同数学领域训练数据的vanilla RLVR相比，JudgeRLVR在领域内数学任务中平均准确率提高了约3.7个点，生成长度减少了42%，并在领域外基准测试中展示了增强的泛化能力，平均准确率改善了约4.5个点。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在追求最终答案的正确性时导致结构化规划不足，采用启发式约束虽可减少冗长，但常剪裁关键推理步骤。因此，论文提出通过鉴别学习指导生成策略，以提升效率与验证性之间的平衡。

Method: 论文采用两阶段方法：第一阶段训练模型判断可验证答案的解决方案响应；第二阶段对模型进行微调，初始化为具有RLVR机制的vanilla生成器。

Result: 在领域内数学任务中，JudgeRLVR平均准确率提高了约3.7个点，生成长度减少42%；在领域外基准上，其平均准确率提高了约4.5个点，显示了增强的泛化能力。

Conclusion: 通过这一框架，模型能够更有效地生成高质量的解决方案，同时保持具有良好验证性的生成策略。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has become a standard paradigm for reasoning in Large Language Models. However, optimizing solely for final-answer correctness often drives models into aimless, verbose exploration, where they rely on exhaustive trial-and-error tactics rather than structured planning to reach solutions. While heuristic constraints like length penalties can reduce verbosity, they often truncate essential reasoning steps, creating a difficult trade-off between efficiency and verification. In this paper, we argue that discriminative capability is a prerequisite for efficient generation: by learning to distinguish valid solutions, a model can internalize a guidance signal that prunes the search space. We propose JudgeRLVR, a two-stage judge-then-generate paradigm. In the first stage, we train the model to judge solution responses with verifiable answers. In the second stage, we fine-tune the same model with vanilla generating RLVR initialized from the judge. Compared to Vanilla RLVR using the same math-domain training data, JudgeRLVR achieves a better quality--efficiency trade-off for Qwen3-30B-A3B: on in-domain math, it delivers about +3.7 points average accuracy gain with -42\% average generation length; on out-of-domain benchmarks, it delivers about +4.5 points average accuracy improvement, demonstrating enhanced generalization.

</details>


### [119] [BenchOverflow: Measuring Overflow in Large Language Models via Plain-Text Prompts](https://arxiv.org/abs/2601.08490)
*Erin Feiglin,Nir Hutnik,Raz Lapid*

Main category: cs.CL

TL;DR: 本文研究了一种大型语言模型（LLMs）的失败模式，称为溢出。溢出会导致过量输出，即使没有恶意后缀或政策规避，也会增加使用成本、延迟，并导致性能下降。作者引入了一个基准测试BenchOverflow，以标准化协议评估了九种开放和闭源模型，发现溢出是一个广泛可复制但又在不同类别和攻击途径上异质的实证风险向量。通过简单的缓解措施，可以降低发生过量输出的概率。BenchOverflow还提供了一个基准，用于选择能最小化资源浪费并控制计算放大效应的部署。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型可能在普通交互中产生过多的输出，导致服务成本增加、延迟、跨用户性能下降，并且从经济和环境角度考虑增加了不必要的计算量，导致消耗的电量和碳足迹增加，这种现象需要被研究和缓解。

Method: 作者通过引入BenchOverflow基准测试来评估各种语言模型在普通文本提示下生成过量输出的概率，并量化其尾部风险。这种方法使用标准化的协议测试了九种开放和闭源模型，并分析了不同模型和攻击向量之间的内在差异。

Result: 作者发现普通文本提示可能导致模型生成过量输出，特别是在处理大量请求时。通过BenchOverflow基准测试，他们量化了不同模型在不同文本提示策略下的过度输出情况，并提出了缓解措施，如简单的简明性提醒，可以有效减缓尾部效应并降低过量输出的发生。

Conclusion: 该研究将长度控制问题视为一种重要的可靠性、成本和可持续性问题，提出了一个基准BenchOverflow来评估不同模型在长度控制方面的鲁棒性，并为选择能最小化资源浪费的部署和评价防止计算放大的防御措施提供了参考。

Abstract: We investigate a failure mode of large language models (LLMs) in which plain-text prompts elicit excessive outputs, a phenomenon we term Overflow. Unlike jailbreaks or prompt injection, Overflow arises under ordinary interaction settings and can lead to elevated serving cost, latency, and cross-user performance degradation, particularly when scaled across many requests. Beyond usability, the stakes are economic and environmental: unnecessary tokens increase per-request cost and energy consumption, compounding into substantial operational spend and carbon footprint at scale. Moreover, Overflow represents a practical vector for compute amplification and service degradation in shared environments. We introduce BenchOverflow, a model-agnostic benchmark of nine plain-text prompting strategies that amplify output volume without adversarial suffixes or policy circumvention. Using a standardized protocol with a fixed budget of 5000 new tokens, we evaluate nine open- and closed-source models and observe pronounced rightward shifts and heavy tails in length distributions. Cap-saturation rates (CSR@1k/3k/5k) and empirical cumulative distribution functions (ECDFs) quantify tail risk; within-prompt variance and cross-model correlations show that Overflow is broadly reproducible yet heterogeneous across families and attack vectors. A lightweight mitigation-a fixed conciseness reminder-attenuates right tails and lowers CSR for all strategies across the majority of models. Our findings position length control as a measurable reliability, cost, and sustainability concern rather than a stylistic quirk. By enabling standardized comparison of length-control robustness across models, BenchOverflow provides a practical basis for selecting deployments that minimize resource waste and operating expense, and for evaluating defenses that curb compute amplification without eroding task performance.

</details>


### [120] [It's All About the Confidence: An Unsupervised Approach for Multilingual Historical Entity Linking using Large Language Models](https://arxiv.org/abs/2601.08500)
*Cristian Santini,Marieke Van Erp,Mehwish Alam*

Main category: cs.CL

TL;DR: MHEL-LLaMo 是一个基于大型语言模型和小型语言模型的无监督集成方法，用于解决历史文本中的实体链接难题，相比现有模型，该方法不需要微调，能够提供具有资源限制下的可扩展解决方案。


<details>
  <summary>Details</summary>
Motivation: 目前在处理历史文本中的实体链接时存在语言变化和噪声输入等问题，现有的解决方案要么需要大量训练数据，要么依赖于特定领域的规则，限制了可扩展性。MHEL-LLaMo 提出了一种基于无监督的集成方法，旨在应对这一挑战。

Method: MHEL-LLaMo 使用了一个小语言模型和大型语言模型的组合方法。通过多语言双编码器（BELA）进行候选检索，并利用提示链调优的大语言模型进行无义预测和候选选择。该系统通过小语言模型的置信度评分来区分简单和复杂案例，仅对复杂的案例应用大语言模型。

Result: MHEL-LLaMo 在六种欧洲语言（英语、芬兰语、法语、德语、意大利语和瑞典语）的四个公认基准上的表现超过了最先进的模型，无需微调，并为低资源情况下的历史实体链接提供了可扩展的解决方案。

Conclusion: 该系统展示了在资源有限条件下的可扩展性和有效性。

Abstract: Despite the recent advancements in NLP with the advent of Large Language Models (LLMs), Entity Linking (EL) for historical texts remains challenging due to linguistic variation, noisy inputs, and evolving semantic conventions. Existing solutions either require substantial training data or rely on domain-specific rules that limit scalability. In this paper, we present MHEL-LLaMo (Multilingual Historical Entity Linking with Large Language MOdels), an unsupervised ensemble approach combining a Small Language Model (SLM) and an LLM. MHEL-LLaMo leverages a multilingual bi-encoder (BELA) for candidate retrieval and an instruction-tuned LLM for NIL prediction and candidate selection via prompt chaining. Our system uses SLM's confidence scores to discriminate between easy and hard samples, applying an LLM only for hard cases. This strategy reduces computational costs while preventing hallucinations on straightforward cases. We evaluate MHEL-LLaMo on four established benchmarks in six European languages (English, Finnish, French, German, Italian and Swedish) from the 19th and 20th centuries. Results demonstrate that MHEL-LLaMo outperforms state-of-the-art models without requiring fine-tuning, offering a scalable solution for low-resource historical EL. The implementation of MHEL-LLaMo is available on Github.

</details>


### [121] [STAGE: A Benchmark for Knowledge Graph Construction, Question Answering, and In-Script Role-Playing over Movie Screenplays](https://arxiv.org/abs/2601.08510)
*Qiuyu Tian,Yiding Li,Fengyi Chen,Zequn Liu,Youyong Kong,Fan Guo,Yuyao Li,Jinjing Shen,Zhijing Xie,Yiyun Luo,Xin Zhang*

Main category: cs.CL

TL;DR: STAGE 是一个统一的电影剧本叙事理解基准，包含知识图构建、场景事件总结、长段落问题回答和角色扮演等任务，涵盖了角色关系、事件序列和对话互动等元素，提供跨语言数据集，支持全面评估模型构建世界理解、验证叙事事件和生成角色一致回应的能力。


<details>
  <summary>Details</summary>
Motivation: 当前基准更多关注个体子任务如问答或对话生成，而STAGE旨在评估模型是否能构建并一致地使用连贯的故事世界，涵盖多形式的推理与生成。

Method: STAGE 设计了四个任务：知识图谱构建、场景事件总结、长段落问题回答和角色互动，并基于共享叙事世界的表示，提供清洗过的剧本、精心编纂的知识图谱和以事件和角色为中心的注释。

Result: STAGE 提供了包含英文和中文电影剧本的洁净版本，同时包含手动编纂的知识图、事件和角色中心注释，全面评估模型处理角色关系、事件序列和对话互动的能力。

Conclusion: STAGE 开创性地综合了多个叙事理解任务，提供了评价模型在复杂叙事理解中的新型基准，对推动该领域的发展具有重要意义。

Abstract: Movie screenplays are rich long-form narratives that interleave complex character relationships, temporally ordered events, and dialogue-driven interactions. While prior benchmarks target individual subtasks such as question answering or dialogue generation, they rarely evaluate whether models can construct a coherent story world and use it consistently across multiple forms of reasoning and generation. We introduce STAGE (Screenplay Text, Agents, Graphs and Evaluation), a unified benchmark for narrative understanding over full-length movie screenplays. STAGE defines four tasks: knowledge graph construction, scene-level event summarization, long-context screenplay question answering, and in-script character role-playing, all grounded in a shared narrative world representation. The benchmark provides cleaned scripts, curated knowledge graphs, and event- and character-centric annotations for 150 films across English and Chinese, enabling holistic evaluation of models' abilities to build world representations, abstract and verify narrative events, reason over long narratives, and generate character-consistent responses.

</details>


### [122] [STAR: Detecting Inference-time Backdoors in LLM Reasoning via State-Transition Amplification Ratio](https://arxiv.org/abs/2601.08511)
*Seong-Gyu Park,Sohee Park,Jisu Lee,Hyunsik Na,Daeseon Choi*

Main category: cs.CL

TL;DR: STAR 提出了一个检测 LLMs 中推理路径后门的新框架，通过分析概率变化来识别潜在的风险路径。


<details>
  <summary>Details</summary>
Motivation: 现有的 LLMs 在推理过程中引入了 Chain-of-Thought 机制，但这种机制暴露了新的攻击面，恶意输入可以诱导模型生成看似合理的推理路径，而无需篡改模型参数。因此，必须设计一种能够检测这些潜在威胁的机制。

Method: STAR 使用 CUSUM 算法检测模型输出概率中的累积差异，以识别由于恶意输入而引发的概率异常增益，从而判断是否存在后门。

Result: STAR 在多种模型（8B-70B 参数）和各种基准数据集上的实验结果表明，其具备强大的泛化能力，在检测后门入侵方面表现出色，接近完美的 AUC-ROC 指标，并且相比于现有方法，STAR 的效率提高了 42 倍。

Conclusion: STAR 提供了一种有效且高效的方法来对抗 LLMs 中的推理路径后门攻击，对于提升模型安全性具有重要意义。

Abstract: Recent LLMs increasingly integrate reasoning mechanisms like Chain-of-Thought (CoT). However, this explicit reasoning exposes a new attack surface for inference-time backdoors, which inject malicious reasoning paths without altering model parameters. Because these attacks generate linguistically coherent paths, they effectively evade conventional detection. To address this, we propose STAR (State-Transition Amplification Ratio), a framework that detects backdoors by analyzing output probability shifts. STAR exploits the statistical discrepancy where a malicious input-induced path exhibits high posterior probability despite a low prior probability in the model's general knowledge. We quantify this state-transition amplification and employ the CUSUM algorithm to detect persistent anomalies. Experiments across diverse models (8B-70B) and five benchmark datasets demonstrate that STAR exhibits robust generalization capabilities, consistently achieving near-perfect performance (AUROC $\approx$ 1.0) with approximately $42\times$ greater efficiency than existing baselines. Furthermore, the framework proves robust against adaptive attacks attempting to bypass detection.

</details>


### [123] [Algorithmic Stability in Infinite Dimensions: Characterizing Unconditional Convergence in Banach Spaces](https://arxiv.org/abs/2601.08512)
*Przemysław Spyra*

Main category: cs.CL

TL;DR: 本研究通过综合条件收敛的七种等价条件，建立了统一的特征定理，提高了在无穷维空间中计算算法的稳定性分析，并在梯度累积和基于框架的信号处理中得到了应用。


<details>
  <summary>Details</summary>
Motivation: 介绍研究动机：探讨在无限维空间中条件收敛、无条件收敛和绝对收敛的区别具有重要意义，因为这些概念在有限维空间中是一致的，但在一般Banach空间中严格分离。这项工作旨在直接应用于算法稳定性分析和信号处理。

Method: 研究方法包括：首先，通过Dvoretzky-Rogers定理讨论无限维空间中条件收敛、无条件收敛和绝对收敛的区别；然后，提出了一个统一的特征定理来阐述七种条件收敛的等价条件，包括置换不变性、网收敛、子系列测试、符号稳定性、有界乘法性质和弱一致收敛。

Result: 研究结果揭示了无需依赖序的和数值稳定的求和过程的理论基础，并应用于梯度累积和基于框架的信号处理中。

Conclusion: 结论：通过建立一个全面的特征定理，论文为在无限维空间中实现无序依赖和数值稳定的求和过程提供了坚实的理论支持，这些结论直接应用于梯度累积和基于框架的信号处理中。

Abstract: The distinction between conditional, unconditional, and absolute convergence in infinite-dimensional spaces has fundamental implications for computational algorithms. While these concepts coincide in finite dimensions, the Dvoretzky-Rogers theorem establishes their strict separation in general Banach spaces. We present a comprehensive characterization theorem unifying seven equivalent conditions for unconditional convergence: permutation invariance, net convergence, subseries tests, sign stability, bounded multiplier properties, and weak uniform convergence. These theoretical results directly inform algorithmic stability analysis, governing permutation invariance in gradient accumulation for Stochastic Gradient Descent and justifying coefficient thresholding in frame-based signal processing. Our work bridges classical functional analysis with contemporary computational practice, providing rigorous foundations for order-independent and numerically robust summation processes.

</details>


### [124] [DeepResearch Bench II: Diagnosing Deep Research Agents via Rubrics from Expert Report](https://arxiv.org/abs/2601.08536)
*Ruizhe Li,Mingxuan Du,Benfeng Xu,Chiwei Zhu,Xiaorui Wang,Zhendong Mao*

Main category: cs.CL

TL;DR: Deep Research Bench II 为 DRS 系统提供了一个新的严格评估基准，通过 132 个跨 22 个领域的研究任务和细粒度二元评价标准来全面评估系统的综合能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试工具存在缺陷，未能充分测试系统的证据分析能力和报告编写能力，且评价标准过于粗略或直接依赖于 LLM，因此开发 Deep Research Bench II，以确保评估的准确性和验证性。

Method: Deep Research Bench II 含有 132 个研究任务分布在 22 个领域，每个任务需生成一篇详细的报告，受 9430 个细粒度二元评价标准评估，涵盖信息召回、分析和展示三个维度。评价标准根据精心挑选的专家文章，通过 LLM 和人工四阶段流程制定。

Result: 测试多个最先进的 DRS 系统时发现，即便最强的模型也仅达到 50% 的评价标准，显示出当前系统与人类专家之间的显著差距。

Conclusion: Deep Research Bench II 为 DRS 的综合评估提供了数据和评价标准，有助于推动系统改进和完善，减少偏见和不准确度。

Abstract: Deep Research Systems (DRS) aim to help users search the web, synthesize information, and deliver comprehensive investigative reports. However, how to rigorously evaluate these systems remains under-explored. Existing deep-research benchmarks often fall into two failure modes. Some do not adequately test a system's ability to analyze evidence and write coherent reports. Others rely on evaluation criteria that are either overly coarse or directly defined by LLMs (or both), leading to scores that can be biased relative to human experts and are hard to verify or interpret. To address these issues, we introduce Deep Research Bench II, a new benchmark for evaluating DRS-generated reports. It contains 132 grounded research tasks across 22 domains; for each task, a system must produce a long-form research report that is evaluated by a set of 9430 fine-grained binary rubrics in total, covering three dimensions: information recall, analysis, and presentation. All rubrics are derived from carefully selected expert-written investigative articles and are constructed through a four-stage LLM+human pipeline that combines automatic extraction with over 400 human-hours of expert review, ensuring that the criteria are atomic, verifiable, and aligned with human expert judgment. We evaluate several state-of-the-art deep-research systems on Deep Research Bench II and find that even the strongest models satisfy fewer than 50% of the rubrics, revealing a substantial gap between current DRSs and human experts.

</details>


### [125] [Ministral 3](https://arxiv.org/abs/2601.08584)
*Alexander H. Liu,Kartik Khandelwal,Sandeep Subramanian,Victor Jouault,Abhinav Rastogi,Adrien Sadé,Alan Jeffares,Albert Jiang,Alexandre Cahill,Alexandre Gavaudan,Alexandre Sablayrolles,Amélie Héliou,Amos You,Andy Ehrenberg,Andy Lo,Anton Eliseev,Antonia Calvi,Avinash Sooriyarachchi,Baptiste Bout,Baptiste Rozière,Baudouin De Monicault,Clémence Lanfranchi,Corentin Barreau,Cyprien Courtot,Daniele Grattarola,Darius Dabert,Diego de las Casas,Elliot Chane-Sane,Faruk Ahmed,Gabrielle Berrada,Gaëtan Ecrepont,Gauthier Guinet,Georgii Novikov,Guillaume Kunsch,Guillaume Lample,Guillaume Martin,Gunshi Gupta,Jan Ludziejewski,Jason Rute,Joachim Studnia,Jonas Amar,Joséphine Delas,Josselin Somerville Roberts,Karmesh Yadav,Khyathi Chandu,Kush Jain,Laurence Aitchison,Laurent Fainsin,Léonard Blier,Lingxiao Zhao,Louis Martin,Lucile Saulnier,Luyu Gao,Maarten Buyl,Margaret Jennings,Marie Pellat,Mark Prins,Mathieu Poirée,Mathilde Guillaumin,Matthieu Dinot,Matthieu Futeral,Maxime Darrin,Maximilian Augustin,Mia Chiquier,Michel Schimpf,Nathan Grinsztajn,Neha Gupta,Nikhil Raghuraman,Olivier Bousquet,Olivier Duchenne,Patricia Wang,Patrick von Platen,Paul Jacob,Paul Wambergue,Paula Kurylowicz,Pavankumar Reddy Muddireddy,Philomène Chagniot,Pierre Stock,Pravesh Agrawal,Quentin Torroba,Romain Sauvestre,Roman Soletskyi,Rupert Menneer,Sagar Vaze,Samuel Barry,Sanchit Gandhi,Siddhant Waghjale,Siddharth Gandhi,Soham Ghosh,Srijan Mishra,Sumukh Aithal,Szymon Antoniak,Teven Le Scao,Théo Cachet,Theo Simon Sorg,Thibaut Lavril,Thiziri Nait Saada,Thomas Chabal,Thomas Foubert,Thomas Robert,Thomas Wang,Tim Lawson,Tom Bewley,Tom Bewley,Tom Edwards,Umar Jamil,Umberto Tomasini,Valeriia Nemychnikova,Van Phung,Vincent Maladière,Virgile Richard,Wassim Bouaziz,Wen-Ding Li,William Marshall,Xinghui Li,Xinyu Yang,Yassine El Ouahidi,Yihan Wang,Yunhao Tang,Zaccharie Ramzi*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We introduce the Ministral 3 series, a family of parameter-efficient dense language models designed for compute and memory constrained applications, available in three model sizes: 3B, 8B, and 14B parameters. For each model size, we release three variants: a pretrained base model for general-purpose use, an instruction finetuned, and a reasoning model for complex problem-solving. In addition, we present our recipe to derive the Ministral 3 models through Cascade Distillation, an iterative pruning and continued training with distillation technique. Each model comes with image understanding capabilities, all under the Apache 2.0 license.

</details>


### [126] [ExpSeek: Self-Triggered Experience Seeking for Web Agents](https://arxiv.org/abs/2601.08605)
*Wenyuan Zhang,Xinghua Zhang,Haiyang Yu,Shuaiyi Nie,Bingli Wu,Juwei Yue,Tingwen Liu,Yongbin Li*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Experience intervention in web agents emerges as a promising technical paradigm, enhancing agent interaction capabilities by providing valuable insights from accumulated experiences. However, existing methods predominantly inject experience passively as global context before task execution, struggling to adapt to dynamically changing contextual observations during agent-environment interaction. We propose ExpSeek, which shifts experience toward step-level proactive seeking: (1) estimating step-level entropy thresholds to determine intervention timing using the model's intrinsic signals; (2) designing step-level tailor-designed experience content. Experiments on Qwen3-8B and 32B models across four challenging web agent benchmarks demonstrate that ExpSeek achieves absolute improvements of 9.3% and 7.5%, respectively. Our experiments validate the feasibility and advantages of entropy as a self-triggering signal, reveal that even a 4B small-scale experience model can significantly boost the performance of larger agent models.

</details>


### [127] [GraphSearch: Agentic Search-Augmented Reasoning for Zero-Shot Graph Learning](https://arxiv.org/abs/2601.08621)
*Jiajin Liu,Yuanfu Sun,Dongzhe Fan,Qiaoyu Tan*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Recent advances in search-augmented large reasoning models (LRMs) enable the retrieval of external knowledge to reduce hallucinations in multistep reasoning. However, their ability to operate on graph-structured data, prevalent in domains such as e-commerce, social networks, and scientific citations, remains underexplored. Unlike plain text corpora, graphs encode rich topological signals that connect related entities and can serve as valuable priors for retrieval, enabling more targeted search and improved reasoning efficiency. Yet, effectively leveraging such structure poses unique challenges, including the difficulty of generating graph-expressive queries and ensuring reliable retrieval that balances structural and semantic relevance. To address this gap, we introduce GraphSearch, the first framework that extends search-augmented reasoning to graph learning, enabling zero-shot graph learning without task-specific fine-tuning. GraphSearch combines a Graph-aware Query Planner, which disentangles search space (e.g., 1-hop, multi-hop, or global neighbors) from semantic queries, with a Graph-aware Retriever, which constructs candidate sets based on topology and ranks them using a hybrid scoring function. We further instantiate two traversal modes: GraphSearch-R, which recursively expands neighborhoods hop by hop, and GraphSearch-F, which flexibly retrieves across local and global neighborhoods without hop constraints. Extensive experiments across diverse benchmarks show that GraphSearch achieves competitive or even superior performance compared to supervised graph learning methods, setting state-of-the-art results in zero-shot node classification and link prediction. These findings position GraphSearch as a flexible and generalizable paradigm for agentic reasoning over graphs.

</details>


### [128] [How Order-Sensitive Are LLMs? OrderProbe for Deterministic Structural Reconstruction](https://arxiv.org/abs/2601.08626)
*Yingjie He,Zhaolu Kang,Kehan Jiang,Qianyuan Zhang,Jiachen Qian,Chunlei Meng,Yujie Feng,Yuan Wang,Jiabao Dou,Aming Wu,Leqi Zheng,Pengxiang Zhao,Jiaxin Liu,Zeyu Zhang,Lei Wang,Guansu Wang,Qishi Zhan,Xiaomin He,Meisheng Zhang,Jianyuan Ni*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large language models (LLMs) excel at semantic understanding, yet their ability to reconstruct internal structure from scrambled inputs remains underexplored. Sentence-level restoration is ill-posed for automated evaluation because multiple valid word orders often exist. We introduce OrderProbe, a deterministic benchmark for structural reconstruction using fixed four-character expressions in Chinese, Japanese, and Korean, which have a unique canonical order and thus support exact-match scoring. We further propose a diagnostic framework that evaluates models beyond recovery accuracy, including semantic fidelity, logical validity, consistency, robustness sensitivity, and information density. Experiments on twelve widely used LLMs show that structural reconstruction remains difficult even for frontier systems: zero-shot recovery frequently falls below 35%. We also observe a consistent dissociation between semantic recall and structural planning, suggesting that structural robustness is not an automatic byproduct of semantic competence.

</details>


### [129] [Get away with less: Need of source side data curation to build parallel corpus for low resource Machine Translation](https://arxiv.org/abs/2601.08629)
*Saumitra Yadav,Manish Shrivastava*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Data curation is a critical yet under-researched step in the machine translation training paradigm. To train translation systems, data acquisition relies primarily on human translations and digital parallel sources or, to a limited degree, synthetic generation. But, for low-resource languages, human translation to generate sufficient data is prohibitively expensive. Therefore, it is crucial to develop a framework that screens source sentences to form efficient parallel text, ensuring optimal MT system performance in low-resource environments. We approach this by evaluating English-Hindi bi-text to determine effective sentence selection strategies for optimal MT system training. Our extensively tested framework, (Lexical And Linguistically Informed Text Analysis) LALITA, targets source sentence selection using lexical and linguistic features to curate parallel corpora. We find that by training mostly on complex sentences from both existing and synthetic datasets, our method significantly improves translation quality. We test this by simulating low-resource data availabilty with curated datasets of 50K to 800K English sentences and report improved performances on all data sizes. LALITA demonstrates remarkable efficiency, reducing data needs by more than half across multiple languages (Hindi, Odia, Nepali, Norwegian Nynorsk, and German). This approach not only reduces MT systems training cost by reducing training data requirement, but also showcases LALITA's utility in data augmentation.

</details>


### [130] [Moral Lenses, Political Coordinates: Towards Ideological Positioning of Morally Conditioned LLMs](https://arxiv.org/abs/2601.08634)
*Chenchen Yuan,Bolei Ma,Zheyu Zhang,Bardh Prenkaj,Frauke Kreuter,Gjergji Kasneci*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: While recent research has systematically documented political orientation in large language models (LLMs), existing evaluations rely primarily on direct probing or demographic persona engineering to surface ideological biases. In social psychology, however, political ideology is also understood as a downstream consequence of fundamental moral intuitions. In this work, we investigate the causal relationship between moral values and political positioning by treating moral orientation as a controllable condition. Rather than simply assigning a demographic persona, we condition models to endorse or reject specific moral values and evaluate the resulting shifts on their political orientations, using the Political Compass Test. By treating moral values as lenses, we observe how moral conditioning actively steers model trajectories across economic and social dimensions. Our findings show that such conditioning induces pronounced, value-specific shifts in models' political coordinates. We further notice that these effects are systematically modulated by role framing and model scale, and are robust across alternative assessment instruments instantiating the same moral value. This highlights that effective alignment requires anchoring political assessments within the context of broader social values including morality, paving the way for more socially grounded alignment techniques.

</details>


### [131] [A Parallel Cross-Lingual Benchmark for Multimodal Idiomaticity Understanding](https://arxiv.org/abs/2601.08645)
*Dilara Torunoğlu-Selamet,Dogukan Arslan,Rodrigo Wilkens,Wei He,Doruk Eryiğit,Thomas Pickard,Adriana S. Pagano,Aline Villavicencio,Gülşen Eryiğit,Ágnes Abuczki,Aida Cardoso,Alesia Lazarenka,Dina Almassova,Amalia Mendes,Anna Kanellopoulou,Antoni Brosa-Rodríguez,Baiba Saulite,Beata Wojtowicz,Bolette Pedersen,Carlos Manuel Hidalgo-Ternero,Chaya Liebeskind,Danka Jokić,Diego Alves,Eleni Triantafyllidi,Erik Velldal,Fred Philippy,Giedre Valunaite Oleskeviciene,Ieva Rizgeliene,Inguna Skadina,Irina Lobzhanidze,Isabell Stinessen Haugen,Jauza Akbar Krito,Jelena M. Marković,Johanna Monti,Josue Alejandro Sauca,Kaja Dobrovoljc,Kingsley O. Ugwuanyi,Laura Rituma,Lilja Øvrelid,Maha Tufail Agro,Manzura Abjalova,Maria Chatzigrigoriou,María del Mar Sánchez Ramos,Marija Pendevska,Masoumeh Seyyedrezaei,Mehrnoush Shamsfard,Momina Ahsan,Muhammad Ahsan Riaz Khan,Nathalie Carmen Hau Norman,Nilay Erdem Ayyıldız,Nina Hosseini-Kivanani,Noémi Ligeti-Nagy,Numaan Naeem,Olha Kanishcheva,Olha Yatsyshyna,Daniil Orel,Petra Giommarelli,Petya Osenova,Radovan Garabik,Regina E. Semou,Rozane Rebechi,Salsabila Zahirah Pranida,Samia Touileb,Sanni Nimb,Sarfraz Ahmad,Sarvinoz Nematkhonova,Shahar Golan,Shaoxiong Ji,Sopuruchi Christian Aboh,Srdjan Sucur,Stella Markantonatou,Sussi Olsen,Vahide Tajalli,Veronika Lipp,Voula Giouli,Yelda Yeşildal Eraydın,Zahra Saaberi,Zhuohan Xie*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Potentially idiomatic expressions (PIEs) construe meanings inherently tied to the everyday experience of a given language community. As such, they constitute an interesting challenge for assessing the linguistic (and to some extent cultural) capabilities of NLP systems. In this paper, we present XMPIE, a parallel multilingual and multimodal dataset of potentially idiomatic expressions. The dataset, containing 34 languages and over ten thousand items, allows comparative analyses of idiomatic patterns among language-specific realisations and preferences in order to gather insights about shared cultural aspects. This parallel dataset allows to evaluate model performance for a given PIE in different languages and whether idiomatic understanding in one language can be transferred to another. Moreover, the dataset supports the study of PIEs across textual and visual modalities, to measure to what extent PIE understanding in one modality transfers or implies in understanding in another modality (text vs. image). The data was created by language experts, with both textual and visual components crafted under multilingual guidelines, and each PIE is accompanied by five images representing a spectrum from idiomatic to literal meanings, including semantically related and random distractors. The result is a high-quality benchmark for evaluating multilingual and multimodal idiomatic language understanding.

</details>


### [132] [Safe Language Generation in the Limit](https://arxiv.org/abs/2601.08648)
*Antonios Anastasopoulos,Giuseppe Ateniese,Evgenios M. Kornaropoulos*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Recent results in learning a language in the limit have shown that, although language identification is impossible, language generation is tractable. As this foundational area expands, we need to consider the implications of language generation in real-world settings.
  This work offers the first theoretical treatment of safe language generation. Building on the computational paradigm of learning in the limit, we formalize the tasks of safe language identification and generation. We prove that under this model, safe language identification is impossible, and that safe language generation is at least as hard as (vanilla) language identification, which is also impossible. Last, we discuss several intractable and tractable cases.

</details>


### [133] [RULERS: Locked Rubrics and Evidence-Anchored Scoring for Robust LLM Evaluation](https://arxiv.org/abs/2601.08654)
*Yihan Hong,Huaiyuan Yao,Bolin Shen,Wanpeng Xu,Hua Wei,Yushun Dong*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The LLM-as-a-Judge paradigm promises scalable rubric-based evaluation, yet aligning frozen black-box models with human standards remains a challenge due to inherent generation stochasticity. We reframe judge alignment as a criteria transfer problem and isolate three recurrent failure modes: rubric instability caused by prompt sensitivity, unverifiable reasoning that lacks auditable evidence, and scale misalignment with human grading boundaries. To address these issues, we introduce RULERS (Rubric Unification, Locking, and Evidence-anchored Robust Scoring), a compiler-executor framework that transforms natural language rubrics into executable specifications. RULERS operates by compiling criteria into versioned immutable bundles, enforcing structured decoding with deterministic evidence verification, and applying lightweight Wasserstein-based post-hoc calibration, all without updating model parameters. Extensive experiments on essay and summarization benchmarks demonstrate that RULERS significantly outperforms representative baselines in human agreement, maintains strong stability against adversarial rubric perturbations, and enables smaller models to rival larger proprietary judges. Overall, our results suggest that reliable LLM judging requires executable rubrics, verifiable evidence, and calibrated scales rather than prompt phrasing alone. Code is available at https://github.com/LabRAI/Rulers.git.

</details>


### [134] [Analyzing Bias in False Refusal Behavior of Large Language Models for Hate Speech Detoxification](https://arxiv.org/abs/2601.08668)
*Kyuri Im,Shuzhou Yuan,Michael Färber*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: While large language models (LLMs) have increasingly been applied to hate speech detoxification, the prompts often trigger safety alerts, causing LLMs to refuse the task. In this study, we systematically investigate false refusal behavior in hate speech detoxification and analyze the contextual and linguistic biases that trigger such refusals. We evaluate nine LLMs on both English and multilingual datasets, our results show that LLMs disproportionately refuse inputs with higher semantic toxicity and those targeting specific groups, particularly nationality, religion, and political ideology. Although multilingual datasets exhibit lower overall false refusal rates than English datasets, models still display systematic, language-dependent biases toward certain targets. Based on these findings, we propose a simple cross-translation strategy, translating English hate speech into Chinese for detoxification and back, which substantially reduces false refusals while preserving the original content, providing an effective and lightweight mitigation approach.

</details>


### [135] [Lessons from the Field: An Adaptable Lifecycle Approach to Applied Dialogue Summarization](https://arxiv.org/abs/2601.08682)
*Kushal Chawla,Chenyang Zhu,Pengshan Cai,Sangwoo Cho,Scott Novotney,Ayushman Singh,Jonah Lewis,Keasha Safewright,Alfy Samuel,Erin Babinsky,Shi-Xiong Zhang,Sambit Sahu*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Summarization of multi-party dialogues is a critical capability in industry, enhancing knowledge transfer and operational effectiveness across many domains. However, automatically generating high-quality summaries is challenging, as the ideal summary must satisfy a set of complex, multi-faceted requirements. While summarization has received immense attention in research, prior work has primarily utilized static datasets and benchmarks, a condition rare in practical scenarios where requirements inevitably evolve. In this work, we present an industry case study on developing an agentic system to summarize multi-party interactions. We share practical insights spanning the full development lifecycle to guide practitioners in building reliable, adaptable summarization systems, as well as to inform future research, covering: 1) robust methods for evaluation despite evolving requirements and task subjectivity, 2) component-wise optimization enabled by the task decomposition inherent in an agentic architecture, 3) the impact of upstream data bottlenecks, and 4) the realities of vendor lock-in due to the poor transferability of LLM prompts.

</details>


### [136] [QuantEval: A Benchmark for Financial Quantitative Tasks in Large Language Models](https://arxiv.org/abs/2601.08689)
*Zhaolu Kang,Junhao Gong,Wenqing Hu,Shuo Yin,Kehan Jiang,Zhicheng Fang,Yingjie He,Chunlei Meng,Rong Fu,Dongyang Chen,Leqi Zheng,Eric Hanchen Jiang,Yunfei Feng,Yitong Leng,Junfan Zhu,Xiaoyou Chen,Xi Yang,Richeng Xuan*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large Language Models (LLMs) have shown strong capabilities across many domains, yet their evaluation in financial quantitative tasks remains fragmented and mostly limited to knowledge-centric question answering. We introduce QuantEval, a benchmark that evaluates LLMs across three essential dimensions of quantitative finance: knowledge-based QA, quantitative mathematical reasoning, and quantitative strategy coding. Unlike prior financial benchmarks, QuantEval integrates a CTA-style backtesting framework that executes model-generated strategies and evaluates them using financial performance metrics, enabling a more realistic assessment of quantitative coding ability. We evaluate some state-of-the-art open-source and proprietary LLMs and observe substantial gaps to human experts, particularly in reasoning and strategy coding. Finally, we conduct large-scale supervised fine-tuning and reinforcement learning experiments on domain-aligned data, demonstrating consistent improvements. We hope QuantEval will facilitate research on LLMs' quantitative finance capabilities and accelerate their practical adoption in real-world trading workflows. We additionally release the full deterministic backtesting configuration (asset universe, cost model, and metric definitions) to ensure strict reproducibility.

</details>


### [137] [Nationality and Region Prediction from Names: A Comparative Study of Neural Models and Large Language Models](https://arxiv.org/abs/2601.08692)
*Keito Inoshita*

Main category: cs.CL

TL;DR: 研究对比了神经模型和大型语言模型在国籍预测任务中的表现，发现在不同粒度层级上，大型语言模型均优于神经模型，特别是在低频国籍方面。大型语言模型倾向于在国家层面犯“近似错误”，而神经模型则展现出更多的跨区域错误和高频类别的倾向。


<details>
  <summary>Details</summary>
Motivation: 当前的神经模型在国籍预测任务上存在泛化能力不足和对高频类别的偏见等问题，通过引入已学习世界知识的大型语言模型，可以提升预测准确性和鲁棒性。

Method: 研究使用了六种神经模型和六种大型语言模型的提示策略，分别在三个粒度层级（国籍、区域、洲际）进行评估，并通过基于频率的分层分析和错误分析来比较不同模型的表现。

Result: 实验结果显示，大型语言模型在各个粒度层级上均优于神经模型，特别是在低频国籍预测方面表现突出。简单机器学习方法在高频类别的鲁棒性最高，而预训练模型和大型语言模型在低频国籍上表现出色但仍有下降。

Conclusion: 研究指出，大型语言模型的优势来源于其世界知识的积累，模型选择应考虑所需的粒度级别，同时评估时还应关注错误的质量而非仅仅关注准确率。

Abstract: Predicting nationality from personal names has practical value in marketing, demographic research, and genealogical studies. Conventional neural models learn statistical correspondences between names and nationalities from task-specific training data, posing challenges in generalizing to low-frequency nationalities and distinguishing similar nationalities within the same region. Large language models (LLMs) have the potential to address these challenges by leveraging world knowledge acquired during pre-training. In this study, we comprehensively compare neural models and LLMs on nationality prediction, evaluating six neural models and six LLM prompting strategies across three granularity levels (nationality, region, and continent), with frequency-based stratified analysis and error analysis. Results show that LLMs outperform neural models at all granularity levels, with the gap narrowing as granularity becomes coarser. Simple machine learning methods exhibit the highest frequency robustness, while pre-trained models and LLMs show degradation for low-frequency nationalities. Error analysis reveals that LLMs tend to make ``near-miss'' errors, predicting the correct region even when nationality is incorrect, whereas neural models exhibit more cross-regional errors and bias toward high-frequency classes. These findings indicate that LLM superiority stems from world knowledge, model selection should consider required granularity, and evaluation should account for error quality beyond accuracy.

</details>


### [138] [PrivGemo: Privacy-Preserving Dual-Tower Graph Retrieval for Empowering LLM Reasoning with Memory Augmentation](https://arxiv.org/abs/2601.08739)
*Xingyu Tan,Xiaoyang Wang,Qing Liu,Xiwei Xu,Xin Yuan,Liming Zhu,Wenjie Zhang*

Main category: cs.CL

TL;DR: PrivGemo 是一种私有保护的 KG 支持的推理框架，通过本地保留原始 KG 知识并提供匿名视图来进行远程推理，增强推理安全性和可控性。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱（KG）的隐私保护方法主要集中在遮掩实体名称，但仍然存在结构性泄露、远程交互不可控、多方推理脆弱性和经验重用难以稳定高效等问题。PrivGemo 旨在解决这些问题。

Method: PrivGemo 使用双塔设计，保留本地原始 KG 知识并提供匿名视图，以限制语义和结构性披露。通过检索匿名长跳路径实现多地实体推理，同时在本地 KG 进行验证和接地。还引入了层次控制器和隐私感知的经验记忆以减少不必要的探索和远程交互。

Result: PrivGemo 在六个基准测试中取得了整体最先进的结果，比最强基线高出 17.1%。此外，PrivGemo 还能使较小规模的模型（如 Qwen3-4B）达到与 GPT-4-Turbo 相似的推理性能。

Conclusion: PrivGemo 是一种有效的用于 KG 支持的知识密集型问题回答的隐私保护框架，能够提高可靠性、促进更广泛的模型使用，并增强推理的安全性和可控性。

Abstract: Knowledge graphs (KGs) provide structured evidence that can ground large language model (LLM) reasoning for knowledge-intensive question answering. However, many practical KGs are private, and sending retrieved triples or exploration traces to closed-source LLM APIs introduces leakage risk. Existing privacy treatments focus on masking entity names, but they still face four limitations: structural leakage under semantic masking, uncontrollable remote interaction, fragile multi-hop and multi-entity reasoning, and limited experience reuse for stability and efficiency. To address these issues, we propose PrivGemo, a privacy-preserving retrieval-augmented framework for KG-grounded reasoning with memory-guided exposure control. PrivGemo uses a dual-tower design to keep raw KG knowledge local while enabling remote reasoning over an anonymized view that goes beyond name masking to limit both semantic and structural exposure. PrivGemo supports multi-hop, multi-entity reasoning by retrieving anonymized long-hop paths that connect all topic entities, while keeping grounding and verification on the local KG. A hierarchical controller and a privacy-aware experience memory further reduce unnecessary exploration and remote interactions. Comprehensive experiments on six benchmarks show that PrivGemo achieves overall state-of-the-art results, outperforming the strongest baseline by up to 17.1%. Furthermore, PrivGemo enables smaller models (e.g., Qwen3-4B) to achieve reasoning performance comparable to that of GPT-4-Turbo.

</details>


### [139] [From Rows to Reasoning: A Retrieval-Augmented Multimodal Framework for Spreadsheet Understanding](https://arxiv.org/abs/2601.08741)
*Anmol Gulati,Sahil Sen,Waqar Sarguroh,Kevin Paul*

Main category: cs.CL

TL;DR: 本文介绍了一个名为FRTR-Bench的大型多模态电子表格推理基准，并提出了一种名为FRTR的跨模态检索增强生成框架，该框架在多个LLM上的测试结果显示了显著的准确率提升。


<details>
  <summary>Details</summary>
Motivation: 目前大型语言模型在处理包含大量数据和多种格式内容的企业级电子表格时存在困难，且现有方法往往难以满足实际用户的需求。本文旨在填补这一空白，通过构建一个大规模的多模态电子表格推理基准及一个新的框架FRTR，来提升现有模型处理复杂电子表格的能力。

Method: 本文通过将电子表格分解为行、列和块嵌入，结合混合词法密集检索和互惠等级融合（RRF），并集成多模态嵌入进行数值和视觉信息的推理。此外，通过与多个LLM进行实验验证，以评估方法的有效性。

Result: 实验结果表明，FRTR在FRTR-Bench上达到了74%的答案准确性，显著高于之前最先进的方法；在SpreadsheetLLM基准测试中，FRTR使用GPT-5达到了87%的准确率，同时降低了近50%的标记使用量。

Conclusion: 本文成功构建了首个用于多模态电子表格推理的大型基准FRTR-Bench，并提出了一种新颖的FRTR框架，能够有效提高大型语言模型在复杂电子表格处理上的性能。

Abstract: Large Language Models (LLMs) struggle to reason over large-scale enterprise spreadsheets containing thousands of numeric rows, multiple linked sheets, and embedded visual content such as charts and receipts. Prior state-of-the-art spreadsheet reasoning approaches typically rely on single-sheet compression or full-context encoding, which limits scalability and fails to reflect how real users interact with complex, multimodal workbooks. We introduce FRTR-Bench, the first large-scale benchmark for multimodal spreadsheet reasoning, comprising 30 enterprise-grade Excel workbooks spanning nearly four million cells and more than 50 embedded images. To address these challenges, we present From Rows to Reasoning (FRTR), an advanced, multimodal retrieval-augmented generation framework that decomposes Excel workbooks into granular row, column, and block embeddings, employs hybrid lexical-dense retrieval with Reciprocal Rank Fusion (RRF), and integrates multimodal embeddings to reason over both numerical and visual information. We tested FRTR on six LLMs, achieving 74% answer accuracy on FRTR-Bench with Claude Sonnet 4.5, a substantial improvement over prior state-of-the-art approaches that reached only 24%. On the SpreadsheetLLM benchmark, FRTR achieved 87% accuracy with GPT-5 while reducing token usage by roughly 50% compared to context-compression methods.

</details>


### [140] [TableCache: Primary Foreign Key Guided KV Cache Precomputation for Low Latency Text-to-SQL](https://arxiv.org/abs/2601.08743)
*Jinbo Su,Yuxuan Hu,Cuiping Li,Hong Chen,Jia Li,Lintao Ma,Jing Zhang*

Main category: cs.CL

TL;DR: 本文提出了一种称为TableCache的方法，通过在推理时查询预计算的表表示作为KV缓存，以提高文本到SQL任务的效率，特别是在处理具有不同表顺序的用户查询时。


<details>
  <summary>Details</summary>
Motivation: 现有的基于大语言模型的文本到SQL任务方法由于包含大量的数据库模式提示，导致了较长的上下文长度和预填充延迟。为此，本文提出了TableCache方法来减少冗余的前缀缓存拷贝。

Method: TableCache方法包括在离线阶段预计算表表示作为KV缓存，并在在线推理时查询所需的缓存。计算表缓存时，保留了表之间的主外键关系，并使用Table Trie结构来优化缓存查找。

Result: 实验结果显示，TableCache方法在Time to First Token (TTFT) 上提高了3.62倍的速度，同时保持了几乎可忽略的性能损失。

Conclusion: 本研究提出了一种有效的文本到SQL任务解决方案，通过优化缓存机制显著提高了推理速度。

Abstract: In Text-to-SQL tasks, existing LLM-based methods often include extensive database schemas in prompts, leading to long context lengths and increased prefilling latency. While user queries typically focus on recurrent table sets-offering an opportunity for KV cache sharing across queries-current inference engines, such as SGLang and vLLM, generate redundant prefix cache copies when processing user queries with varying table orders. To address this inefficiency, we propose precomputing table representations as KV caches offline and querying the required ones online. A key aspect of our approach is the computation of table caches while preserving primary foreign key relationships between tables. Additionally, we construct a Table Trie structure to facilitate efficient KV cache lookups during inference. To enhance cache performance, we introduce a cache management system with a query reranking strategy to improve cache hit rates and a computation loading pipeline for parallelizing model inference and cache loading. Experimental results show that our proposed TableCache achieves up to a 3.62x speedup in Time to First Token (TTFT) with negligible performance degradation.

</details>


### [141] [Spatial Context Improves the Integration of Text with Remote Sensing for Mapping Environmental Variables](https://arxiv.org/abs/2601.08750)
*Valerie Zermatten,Chiara Vanalli,Gencer Sumbul,Diego Marcos,Devis Tuia*

Main category: cs.CL

TL;DR: 本文提出了一种利用注意力机制结合航空影像和地理定位文本的方法，并通过EcoWikiRS数据集在预测环境变量任务上取得了比单一位置或单模态基线更好的效果。


<details>
  <summary>Details</summary>
Motivation: 当前生态学研究中，文本数据作为一种新的数据源得到了重视，但由于文本数据的稀疏性和不规则性，如何将其与地理信息结合成为一大挑战。本文提出的方法旨在解决这一问题，提供了一种有效结合地理语境下文本和图像的方法。

Method: 本文提出了一种基于注意力机制的方法，结合了高分辨率的航空影像和从维基百科中提取的地理定位文本描述的局部环境条件。模型中包括了视图和文本表示以及地理定位编码，并通过一个注意力模块动态选择有用的邻近观测以进行预测任务。

Result: 在EcoWikiRS数据集上，实验结果显示，本文提出的方法在预测103个环境变量任务上比单一位置或单模态（图像或文本）基线方法表现更好。尤其在气候、土壤、人口和土地使用/覆盖主题变量的预测上有显著改善。

Conclusion: 本文提出的方法在结合文本和图像数据时将地理位置考虑在内，能够有效提升环境变量预测的准确性，并且证明了这种结合方式在应对特定环境变量挑战时的有效性。

Abstract: Recent developments in natural language processing highlight text as an emerging data source for ecology. Textual resources carry unique information that can be used in complementarity with geospatial data sources, thus providing insights at the local scale into environmental conditions and properties hidden from more traditional data sources. Leveraging textual information in a spatial context presents several challenges. First, the contribution of textual data remains poorly defined in an ecological context, and it is unclear for which tasks it should be incorporated. Unlike ubiquitous satellite imagery or environmental covariates, the availability of textual data is sparse and irregular; its integration with geospatial data is not straightforward. In response to these challenges, this work proposes an attention-based approach that combines aerial imagery and geolocated text within a spatial neighbourhood, i.e. integrating contributions from several nearby observations. Our approach combines vision and text representations with a geolocation encoding, with an attention-based module that dynamically selects spatial neighbours that are useful for predictive tasks.The proposed approach is applied to the EcoWikiRS dataset, which combines high-resolution aerial imagery with sentences extracted from Wikipedia describing local environmental conditions across Switzerland. Our model is evaluated on the task of predicting 103 environmental variables from the SWECO25 data cube. Our approach consistently outperforms single-location or unimodal, i.e. image-only or text-only, baselines. When analysing variables by thematic groups, results show a significant improvement in performance for climatic, edaphic, population and land use/land cover variables, underscoring the benefit of including the spatial context when combining text and image data.

</details>


### [142] [Multiplex Thinking: Reasoning via Token-wise Branch-and-Merge](https://arxiv.org/abs/2601.08808)
*Yao Tang,Li Dong,Yaru Hao,Qingxiu Dong,Furu Wei,Jiatao Gu*

Main category: cs.CL

TL;DR: 该研究提出了一种称为Multiplex Thinking的机制，通过在每个思考步骤中采样K个候选标记并将其嵌入聚合为一个连续的倍频程标记，从而保留了离散生成的词汇嵌入先验和采样动力学，同时诱导可处理的倍频程滚动力学。这种方法在数学推理基准测试中表现出色，并能生成更短的序列。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在执行复杂推理任务时往往能更有效地使用Chain-of-Thought (CoT)，但代价是较长的低带宽标记序列。相比之下，人类通常通过维持可能的下一步骤的概率分布来进行软推理。为了结合CoT的有效性和人类推理的灵活性，该研究提出了Multiplex Thinking机制。

Method: Multiplex Thinking机制在每个思考步骤中采样K个候选标记，并通过聚合它们的嵌入来生成一个单一的连续倍频程标记。这种方法并不改变离散生成的词汇嵌入先验和采样动力学。研究团队还介绍了一种优化方法，可以直接优化倍频程轨迹，这得益于其自适应特性，能够在模型自信时表现出类似于标准CoT的行为，在模型不确定时通过压缩表示多个可能的下一步骤来减少序列长度。

Result: 在一系列具有挑战性的数学推理基准测试中，Multiplex Thinking机制的表现超过了强大的离散CoT和基于奖励的学习(RL)基线，尤其在Pass@1到Pass@1024的指标上表现出色，同时也生成了更短的序列。

Conclusion: 研究结果证明，Multiplex Thinking机制能够有效提升大型语言模型在复杂推理任务中的性能，并且该机制具有自我适应性，可以在保持序列长度的同时提高多步推理的灵活性和精确度。

Abstract: Large language models often solve complex reasoning tasks more effectively with Chain-of-Thought (CoT), but at the cost of long, low-bandwidth token sequences. Humans, by contrast, often reason softly by maintaining a distribution over plausible next steps. Motivated by this, we propose Multiplex Thinking, a stochastic soft reasoning mechanism that, at each thinking step, samples K candidate tokens and aggregates their embeddings into a single continuous multiplex token. This preserves the vocabulary embedding prior and the sampling dynamics of standard discrete generation, while inducing a tractable probability distribution over multiplex rollouts. Consequently, multiplex trajectories can be directly optimized with on-policy reinforcement learning (RL). Importantly, Multiplex Thinking is self-adaptive: when the model is confident, the multiplex token is nearly discrete and behaves like standard CoT; when it is uncertain, it compactly represents multiple plausible next steps without increasing sequence length. Across challenging math reasoning benchmarks, Multiplex Thinking consistently outperforms strong discrete CoT and RL baselines from Pass@1 through Pass@1024, while producing shorter sequences. The code and checkpoints are available at https://github.com/GMLR-Penn/Multiplex-Thinking.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [143] [Bridging the Trust Gap: Clinician-Validated Hybrid Explainable AI for Maternal Health Risk Assessment in Bangladesh](https://arxiv.org/abs/2601.07866)
*Farjana Yesmin,Nusrat Shirmin,Suraiya Shabnam Bristy*

Main category: cs.AI

TL;DR: 本研究提出了一种混合可解释AI框架，结合了先验模糊逻辑和后验SHAP解释。在孟加拉国进行的验证研究中，临床医生对这种综合解释有很高的偏好并表示信任。该研究通过提高模型的解释性和实用性，为未来可解释AI在孕产妇保健中的应用提供了实用见解。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决机器学习在资源受限环境中预测孕产妇健康风险时缺乏解释性和信任感的挑战。

Method: 研究开发了一个基于1014份孕产妇健康记录的Fuzzy-XGBoost模型，并利用SHAP分析来解释特征的重要性。通过 clinicians 在三个临床案例中的反馈，验证了模型的有效性和可解释性。

Result: 模型在验证中达到了88.67%的准确率（ROC-AUC: 0.9703），临床医生对综合解释的偏好率为71.4%，有83.6%认为模型的解释性增强使其更容易信任临床使用。SHAP分析表明，健康服务可及性是最重要的预测因素，而工程化的模糊风险评分在预测中居第三位。

Conclusion: 该研究证明，将可解释的模糊规则与特征重要性解释结合使用可以提高模型的实用性和临床医生的信任感，为未来将可解释AI应用于孕产妇健康保健领域提供了实用见解。

Abstract: While machine learning shows promise for maternal health risk prediction, clinical adoption in resource-constrained settings faces a critical barrier: lack of explainability and trust. This study presents a hybrid explainable AI (XAI) framework combining ante-hoc fuzzy logic with post-hoc SHAP explanations, validated through systematic clinician feedback. We developed a fuzzy-XGBoost model on 1,014 maternal health records, achieving 88.67% accuracy (ROC-AUC: 0.9703). A validation study with 14 healthcare professionals in Bangladesh revealed strong preference for hybrid explanations (71.4% across three clinical cases) with 54.8% expressing trust for clinical use. SHAP analysis identified healthcare access as the primary predictor, with the engineered fuzzy risk score ranking third, validating clinical knowledge integration (r=0.298). Clinicians valued integrated clinical parameters but identified critical gaps: obstetric history, gestational age, and connectivity barriers. This work demonstrates that combining interpretable fuzzy rules with feature importance explanations enhances both utility and trust, providing practical insights for XAI deployment in maternal healthcare.

</details>


### [144] [When Models Know When They Do Not Know: Calibration, Cascading, and Cleaning](https://arxiv.org/abs/2601.07965)
*Chenjie Hao,Weyl Lu,Yuko Ishiwaka,Zengyi Li,Weier Wan,Yubei Chen*

Main category: cs.AI

TL;DR: 本研究提出了一种无需训练的简单有效方法，旨在增强模型识别未知的能力，包括模型校准、级联和数据清洗。研究发现，更高信心与更高准确度相关，校准后的模型具有可比的校准效果。该方法应用于模型级联和数据清洁，提高了效率并有效识别了误标签样本。


<details>
  <summary>Details</summary>
Motivation: 随着模型在复杂任务中的广泛应用，它们面临的挑战是如何准确识别自己的局限性。本研究旨在通过模型校准、级联和数据清洗这三个关键步骤，帮助模型更好地识别未知内容，以提高AI的效率、可靠性和可信度。

Method: 本研究提出了一种无需训练的方法，利用模型内部信号生成信心值并进行校准。通过级联不同规模的模型和利用多专家的信心信号进行数据清洗，以识别数据集中的误标签样本。

Result: 研究发现，在单一模型中，信心与准确度呈正相关；校准后的模型在验证集上的校准效果在测试集上也保持一致。研究人员成功地将大模型和小模型级联起来，提高了计算效率而不降低准确度。此外，还提出了一种基于校准信心信号的数据清洗方法，能够有效识别大规模数据集中的误标签。

Conclusion: 本研究表明，让模型识别其局限性可以作为一种实用的方法，用于提升AI系统的效率、可靠性和可信度。

Abstract: When a model knows when it does not know, many possibilities emerge. The first question is how to enable a model to recognize that it does not know. A promising approach is to use confidence, computed from the model's internal signals, to reflect its ignorance. Prior work in specific domains has shown that calibration can provide reliable confidence estimates. In this work, we propose a simple, effective, and universal training-free method that applies to both vision and language models, performing model calibration, cascading, and data cleaning to better exploit a model's ability to recognize when it does not know. We first highlight two key empirical observations: higher confidence corresponds to higher accuracy within a single model, and models calibrated on the validation set remain calibrated on a held-out test set. These findings empirically establish the reliability and comparability of calibrated confidence. Building on this, we introduce two applications: (1) model cascading with calibrated advantage routing and (2) data cleaning based on model ensemble. Using the routing signal derived from the comparability of calibrated confidences, we cascade large and small models to improve efficiency with almost no compromise in accuracy, and we further cascade two models of comparable scale to achieve performance beyond either model alone. Leveraging multiple experts and their calibrated confidences, we design a simple yet effective data-cleaning method that balances precision and detection rate to identify mislabeled samples in ImageNet and Massive Multitask Language Understanding (MMLU) datasets. Our results demonstrate that enabling models to recognize when they do not know is a practical step toward more efficient, reliable, and trustworthy AI.

</details>


### [145] [Reasoning over Precedents Alongside Statutes: Case-Augmented Deliberative Alignment for LLM Safety](https://arxiv.org/abs/2601.08000)
*Can Jin,Rui Wu,Tong Che,Qixin Zhang,Hongwu Peng,Jiahui Zhao,Zhenting Wang,Wenqi Wei,Ligong Han,Zhao Zhang,Yuan Cao,Ruixiang Tang,Dimitris N. Metaxas*

Main category: cs.AI

TL;DR: 该研究通过案例增强的决策方式改进了大型语言模型的安全性，提出了一种名为CADA的新方法，相对于详细的代码规则，这种方法在保持有用性的同时提高了模型的安全性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前，确保大型语言模型安全而不会拒绝合理的请求仍是挑战。现有的Deliberative Alignment (DA) 方法依赖于详细的代码规则，但在缺乏高级推理能力的开源模型上效果欠佳。该研究旨在探索如何通过案例指导而非详细代码规则来增强模型的安全性能。

Method: 研究者系统地比较了使用详尽的安全代码和通过示例展示安全代码的效果，发现使用简化版案例增强的方法能更有效地提升模型安全性并保持其适应性。基于此，研究者提出了CADA方法，结合了案例增强的Deliberative Alignment和强化学习，通过自我生成的安全推理链来训练模型。

Result: CADA方法在各类基准测试中表现出更强的安全性和鲁棒性，降低了过度否决和提升实用性。该方法在保持模型有效性的前提下有效增加了模型的安全性。

Conclusion: CADA方法为增强大型语言模型的安全性提供了一种实用的替代方案，相比仅依赖规则的方法，它在提升安全性和保持模型使用价值方面展现出更好的效果。

Abstract: Ensuring that Large Language Models (LLMs) adhere to safety principles without refusing benign requests remains a significant challenge. While OpenAI introduces deliberative alignment (DA) to enhance the safety of its o-series models through reasoning over detailed ``code-like'' safety rules, the effectiveness of this approach in open-source LLMs, which typically lack advanced reasoning capabilities, is understudied. In this work, we systematically evaluate the impact of explicitly specifying extensive safety codes versus demonstrating them through illustrative cases. We find that referencing explicit codes inconsistently improves harmlessness and systematically degrades helpfulness, whereas training on case-augmented simple codes yields more robust and generalized safety behaviors. By guiding LLMs with case-augmented reasoning instead of extensive code-like safety rules, we avoid rigid adherence to narrowly enumerated rules and enable broader adaptability. Building on these insights, we propose CADA, a case-augmented deliberative alignment method for LLMs utilizing reinforcement learning on self-generated safety reasoning chains. CADA effectively enhances harmlessness, improves robustness against attacks, and reduces over-refusal while preserving utility across diverse benchmarks, offering a practical alternative to rule-only DA for improving safety while maintaining helpfulness.

</details>


### [146] [Internal Deployment Gaps in AI Regulation](https://arxiv.org/abs/2601.08005)
*Joe Kwon,Stephen Casper*

Main category: cs.AI

TL;DR: 本文探讨了2025年美国和欧盟前沿人工智能监管政策如何处理内部部署系统的问题，发现监管政策中存在三大漏洞，并分析了这些漏洞持续存在的原因以及可能的应对方案。


<details>
  <summary>Details</summary>
Motivation: 论文旨在填补现有前沿AI监管政策在处理内部部署系统方面存在的空白，确保高风险应用得到有效监管。

Method: 通过对比分析美国和欧盟2025年的前沿AI监管政策，识别潜在问题并探讨政策制定者面临的挑战。

Result: 论文指出了当前监管政策中的三个主要漏洞，并深入分析了政策中存在的紧张关系及其产生的原因，为未来政策改进提供参考。

Conclusion: 作者认为，通过系统的政策分析和制定，可以更好地实现内部部署AI系统的监管目标，防止其规避监管。

Abstract: Frontier AI regulations primarily focus on systems deployed to external users, where deployment is more visible and subject to outside scrutiny. However, high-stakes applications can occur internally when companies deploy highly capable systems within their own organizations, such as for automating R\&D, accelerating critical business processes, and handling sensitive proprietary data. This paper examines how frontier AI regulations in the United States and European Union in 2025 handle internal deployment. We identify three gaps that could cause internally-deployed systems to evade intended oversight: (1) scope ambiguity that allows internal systems to evade regulatory obligations, (2) point-in-time compliance assessments that fail to capture the continuous evolution of internal systems, and (3) information asymmetries that subvert regulatory awareness and oversight. We then analyze why these gaps persist, examining tensions around measurability, incentives, and information access. Finally, we map potential approaches to address them and their associated tradeoffs. By understanding these patterns, we hope that policy choices around internally deployed AI systems can be made deliberately rather than incidentally.

</details>


### [147] [Integrating Attendance Tracking and Emotion Detection for Enhanced Student Engagement in Smart Classrooms](https://arxiv.org/abs/2601.08049)
*Keith Ainebyona,Ann Move Oguti,Joseph Walusimbi,Ritah Kobusingye*

Main category: cs.AI

TL;DR: SCASED 是一种基于物联网的系统，结合了人脸识别和情绪识别，旨在监控课堂参与度，提高教学响应性。


<details>
  <summary>Details</summary>
Motivation: 传统的课堂考勤手段主要集中在自动化考勤上，而忽略了学生在课堂上的情绪和认知参与度，这限制了教师对课堂动态的即时监控。SCASED 填补了这一空白。

Method: SCASED 系统使用 Raspberry Pi 摄像头和 OpenCV 进行人脸检测，以及一个针对学习相关情绪状态（注意、厌烦、困惑、挫败感）进行了微调的 MobileNetV2 模型。它通过会话管理机制记录考勤并在随后进行连续的情绪分析。

Result: 使用 DAiSEE 数据集的实验评估实现了89.5%的情绪分类准确率。结果显示，将考勤数据与情绪分析结合使用，可以帮助教师更好地理解课堂动态，从而支持更及时的教育实践。

Conclusion: SCASED 系统提供了一种新的方式来监测课堂参与度，通过集成考勤和情绪分析，能够帮助教师更直观地了解课堂动态，从而采取更灵活的教学策略。

Abstract: The increasing adoption of smart classroom technologies in higher education has mainly focused on automating attendance, with limited attention given to students' emotional and cognitive engagement during lectures. This limits instructors' ability to identify disengagement and adapt teaching strategies in real time. This paper presents SCASED (Smart Classroom Attendance System with Emotion Detection), an IoT-based system that integrates automated attendance tracking with facial emotion recognition to support classroom engagement monitoring. The system uses a Raspberry Pi camera and OpenCV for face detection, and a finetuned MobileNetV2 model to classify four learning-related emotional states: engagement, boredom, confusion, and frustration. A session-based mechanism is implemented to manage attendance and emotion monitoring by recording attendance once per session and performing continuous emotion analysis thereafter. Attendance and emotion data are visualized through a cloud-based dashboard to provide instructors with insights into classroom dynamics. Experimental evaluation using the DAiSEE dataset achieved an emotion classification accuracy of 89.5%. The results show that integrating attendance data with emotion analytics can provide instructors with additional insight into classroom dynamics and support more responsive teaching practices.

</details>


### [148] [Forecast Aware Deep Reinforcement Learning for Efficient Electricity Load Scheduling in Dairy Farms](https://arxiv.org/abs/2601.08052)
*Nawazish Alia,Rachael Shawb,Karl Mason*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Dairy farming is an energy intensive sector that relies heavily on grid electricity. With increasing renewable energy integration, sustainable energy management has become essential for reducing grid dependence and supporting the United Nations Sustainable Development Goal 7 on affordable and clean energy. However, the intermittent nature of renewables poses challenges in balancing supply and demand in real time. Intelligent load scheduling is therefore crucial to minimize operational costs while maintaining reliability. Reinforcement Learning has shown promise in improving energy efficiency and reducing costs. However, most RL-based scheduling methods assume complete knowledge of future prices or generation, which is unrealistic in dynamic environments. Moreover, standard PPO variants rely on fixed clipping or KL divergence thresholds, often leading to unstable training under variable tariffs. To address these challenges, this study proposes a Deep Reinforcement Learning framework for efficient load scheduling in dairy farms, focusing on battery storage and water heating under realistic operational constraints. The proposed Forecast Aware PPO incorporates short term forecasts of demand and renewable generation using hour of day and month based residual calibration, while the PID KL PPO variant employs a proportional integral derivative controller to regulate KL divergence for stable policy updates adaptively. Trained on real world dairy farm data, the method achieves up to 1% lower electricity cost than PPO, 4.8% than DQN, and 1.5% than SAC. For battery scheduling, PPO reduces grid imports by 13.1%, demonstrating scalability and effectiveness for sustainable energy management in modern dairy farming.

</details>


### [149] [A New Strategy for Verifying Reach-Avoid Specifications in Neural Feedback Systems](https://arxiv.org/abs/2601.08065)
*Samuel I. Akinwande,Sydney M. Katz,Mykel J. Kochenderfer,Clark Barrett*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Forward reachability analysis is the predominant approach for verifying reach-avoid properties in neural feedback systems (dynamical systems controlled by neural networks). This dominance stems from the limited scalability of existing backward reachability methods. In this work, we introduce new algorithms that compute both over- and under-approximations of backward reachable sets for such systems. We further integrate these backward algorithms with established forward analysis techniques to yield a unified verification framework for neural feedback systems.

</details>


### [150] [Semantic Gravity Wells: Why Negative Constraints Backfire](https://arxiv.org/abs/2601.08070)
*Shailesh Rana*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Negative constraints (instructions of the form "do not use word X") represent a fundamental test of instruction-following capability in large language models. Despite their apparent simplicity, these constraints fail with striking regularity, and the conditions governing failure have remained poorly understood. This paper presents the first comprehensive mechanistic investigation of negative instruction failure. We introduce semantic pressure, a quantitative measure of the model's intrinsic probability of generating the forbidden token, and demonstrate that violation probability follows a tight logistic relationship with pressure ($p=σ(-2.40+2.27\cdot P_0)$; $n=40{,}000$ samples; bootstrap $95%$ CI for slope: $[2.21,,2.33]$). Through layer-wise analysis using the logit lens technique, we establish that the suppression signal induced by negative instructions is present but systematically weaker in failures: the instruction reduces target probability by only 5.2 percentage points in failures versus 22.8 points in successes -- a $4.4\times$ asymmetry. We trace this asymmetry to two mechanistically distinct failure modes. In priming failure (87.5% of violations), the instruction's explicit mention of the forbidden word paradoxically activates rather than suppresses the target representation. In override failure (12.5%), late-layer feed-forward networks generate contributions of $+0.39$ toward the target probability -- nearly $4\times$ larger than in successes -- overwhelming earlier suppression signals. Activation patching confirms that layers 23--27 are causally responsible: replacing these layers' activations flips the sign of constraint effects. These findings reveal a fundamental tension in negative constraint design: the very act of naming a forbidden word primes the model to produce it.

</details>


### [151] [MemoBrain: Executive Memory as an Agentic Brain for Reasoning](https://arxiv.org/abs/2601.08079)
*Hongjin Qian,Zhao Cao,Zheng Liu*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Complex reasoning in tool-augmented agent frameworks is inherently long-horizon, causing reasoning traces and transient tool artifacts to accumulate and strain the bounded working context of large language models. Without explicit memory mechanisms, such accumulation disrupts logical continuity and undermines task alignment. This positions memory not as an auxiliary efficiency concern, but as a core component for sustaining coherent, goal-directed reasoning over long horizons.
  We propose MemoBrain, an executive memory model for tool-augmented agents that constructs a dependency-aware memory over reasoning steps, capturing salient intermediate states and their logical relations. Operating as a co-pilot alongside the reasoning agent, MemoBrain organizes reasoning progress without blocking execution and actively manages the working context. Specifically, it prunes invalid steps, folds completed sub-trajectories, and preserves a compact, high-salience reasoning backbone under a fixed context budget. Together, these mechanisms enable explicit cognitive control over reasoning trajectories rather than passive context accumulation.
  We evaluate MemoBrain on challenging long-horizon benchmarks, including GAIA, WebWalker, and BrowseComp-Plus, demonstrating consistent improvements over strong baselines.

</details>


### [152] [MirrorBench: An Extensible Framework to Evaluate User-Proxy Agents for Human-Likeness](https://arxiv.org/abs/2601.08118)
*Ashutosh Hathidara,Julien Yu,Vaishali Senthil,Sebastian Schreiber,Anil Babu Ankisettipalli*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large language models (LLMs) are increasingly used as human simulators, both for evaluating conversational systems and for generating fine-tuning data. However, naive "act-as-a-user" prompting often yields verbose, unrealistic utterances, underscoring the need for principled evaluation of so-called user proxy agents. We present MIRRORBENCH, a reproducible, extensible benchmarking framework that evaluates user proxies solely on their ability to produce human-like user utterances across diverse conversational tasks, explicitly decoupled from downstream task success. MIRRORBENCH features a modular execution engine with typed interfaces, metadata-driven registries, multi-backend support, caching, and robust observability. The system supports pluggable user proxies, datasets, tasks, and metrics, enabling researchers to evaluate arbitrary simulators under a uniform, variance-aware harness. We include three lexical-diversity metrics (MATTR, YULE'S K, and HD-D) and three LLM-judge-based metrics (GTEval, Pairwise Indistinguishability, and Rubric-and-Reason). Across four open datasets, MIRRORBENCH yields variance-aware results and reveals systematic gaps between user proxies and real human users. The framework is open source and includes a simple command-line interface for running experiments, managing configurations and caching, and generating reports. The framework can be accessed at https://github.com/SAP/mirrorbench.

</details>


### [153] [How vehicles change lanes after encountering crashes: Empirical analysis and modeling](https://arxiv.org/abs/2601.08125)
*Kequan Chen,Yuxuan Wang,Pan Liu,Victor L. Knoop,David Z. W. Wang,Yu Han*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: When a traffic crash occurs, following vehicles need to change lanes to bypass the obstruction. We define these maneuvers as post crash lane changes. In such scenarios, vehicles in the target lane may refuse to yield even after the lane change has already begun, increasing the complexity and crash risk of post crash LCs. However, the behavioral characteristics and motion patterns of post crash LCs remain unknown. To address this gap, we construct a post crash LC dataset by extracting vehicle trajectories from drone videos captured after crashes. Our empirical analysis reveals that, compared to mandatory LCs (MLCs) and discretionary LCs (DLCs), post crash LCs exhibit longer durations, lower insertion speeds, and higher crash risks. Notably, 79.4% of post crash LCs involve at least one instance of non yielding behavior from the new follower, compared to 21.7% for DLCs and 28.6% for MLCs. Building on these findings, we develop a novel trajectory prediction framework for post crash LCs. At its core is a graph based attention module that explicitly models yielding behavior as an auxiliary interaction aware task. This module is designed to guide both a conditional variational autoencoder and a Transformer based decoder to predict the lane changer's trajectory. By incorporating the interaction aware module, our model outperforms existing baselines in trajectory prediction performance by more than 10% in both average displacement error and final displacement error across different prediction horizons. Moreover, our model provides more reliable crash risk analysis by reducing false crash rates and improving conflict prediction accuracy. Finally, we validate the model's transferability using additional post crash LC datasets collected from different sites.

</details>


### [154] [Embedded AI Companion System on Edge Devices](https://arxiv.org/abs/2601.08128)
*Rahul Gupta,Stephen D. H. Hsu*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Computational resource constraints on edge devices make it difficult to develop a fully embedded AI companion system with a satisfactory user experience. AI companion and memory systems detailed in existing literature cannot be directly used in such an environment due to lack of compute resources and latency concerns. In this paper, we propose a memory paradigm that alternates between active and inactive phases: during phases of user activity, the system performs low-latency, real-time dialog using lightweight retrieval over existing memories and context; whereas during phases of user inactivity, it conducts more computationally intensive extraction, consolidation, and maintenance of memories across full conversation sessions. This design minimizes latency while maintaining long-term personalization under the tight constraints of embedded hardware. We also introduce an AI Companion benchmark designed to holistically evaluate the AI Companion across both its conversational quality and memory capabilities. In our experiments, we found that our system (using a very weak model: Qwen2.5-7B-Instruct quantized int4) outperforms the equivalent raw LLM without memory across most metrics, and performs comparably to GPT-3.5 with 16k context window.

</details>


### [155] [Project Synapse: A Hierarchical Multi-Agent Framework with Hybrid Memory for Autonomous Resolution of Last-Mile Delivery Disruptions](https://arxiv.org/abs/2601.08156)
*Arin Gopalan Yadav,Varad Dherange,Kumar Shivam*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This paper introduces Project Synapse, a novel agentic framework designed for the autonomous resolution of last-mile delivery disruptions. Synapse employs a hierarchical multi-agent architecture in which a central Resolution Supervisor agent performs strategic task decomposition and delegates subtasks to specialized worker agents responsible for tactical execution. The system is orchestrated using LangGraph to manage complex and cyclical workflows. To validate the framework, a benchmark dataset of 30 complex disruption scenarios was curated from a qualitative analysis of over 6,000 real-world user reviews. System performance is evaluated using an LLM-as-a-Judge protocol with explicit bias mitigation.

</details>


### [156] [ZeroDVFS: Zero-Shot LLM-Guided Core and Frequency Allocation for Embedded Platforms](https://arxiv.org/abs/2601.08166)
*Mohammad Pivezhandi,Mahdi Banisharif,Abusayeed Saifullah,Ali Jannesari*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Dynamic voltage and frequency scaling (DVFS) and task-to-core allocation are critical for thermal management and balancing energy and performance in embedded systems. Existing approaches either rely on utilization-based heuristics that overlook stall times, or require extensive offline profiling for table generation, preventing runtime adaptation. We propose a model-based hierarchical multi-agent reinforcement learning (MARL) framework for thermal- and energy-aware scheduling on multi-core platforms. Two collaborative agents decompose the exponential action space, achieving 358ms latency for subsequent decisions. First decisions require 3.5 to 8.0s including one-time LLM feature extraction. An accurate environment model leverages regression techniques to predict thermal dynamics and performance states. When combined with LLM-extracted semantic features, the environment model enables zero-shot deployment for new workloads on trained platforms by generating synthetic training data without requiring workload-specific profiling samples. We introduce LLM-based semantic feature extraction that characterizes OpenMP programs through 13 code-level features without execution. The Dyna-Q-inspired framework integrates direct reinforcement learning with model-based planning, achieving 20x faster convergence than model-free methods. Experiments on BOTS and PolybenchC benchmarks across NVIDIA Jetson TX2, Jetson Orin NX, RubikPi, and Intel Core i7 demonstrate 7.09x better energy efficiency and 4.0x better makespan than Linux ondemand governor. First-decision latency is 8,300x faster than table-based profiling, enabling practical deployment in dynamic embedded systems.

</details>


### [157] [The Agent's First Day: Benchmarking Learning, Exploration, and Scheduling in the Workplace Scenarios](https://arxiv.org/abs/2601.08173)
*Daocheng Fu,Jianbiao Mei,Rong Wu,Xuemeng Yang,Jia Xu,Ding Wang,Pinlong Cai,Yong Liu,Licheng Wen,Botian Shi*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The rapid evolution of Multi-modal Large Language Models (MLLMs) has advanced workflow automation; however, existing research mainly targets performance upper bounds in static environments, overlooking robustness for stochastic real-world deployment. We identify three key challenges: dynamic task scheduling, active exploration under uncertainty, and continuous learning from experience. To bridge this gap, we introduce \method{}, a dynamic evaluation environment that simulates a "trainee" agent continuously exploring a novel setting. Unlike traditional benchmarks, \method{} evaluates agents along three dimensions: (1) context-aware scheduling for streaming tasks with varying priorities; (2) prudent information acquisition to reduce hallucination via active exploration; and (3) continuous evolution by distilling generalized strategies from rule-based, dynamically generated tasks. Experiments show that cutting-edge agents have significant deficiencies in dynamic environments, especially in active exploration and continual learning. Our work establishes a framework for assessing agent reliability, shifting evaluation from static tests to realistic, production-oriented scenarios. Our codes are available at https://github.com/KnowledgeXLab/EvoEnv

</details>


### [158] [Improving LLM Reasoning with Homophily-aware Structural and Semantic Text-Attributed Graph Compression](https://arxiv.org/abs/2601.08187)
*Zijun Di,Bin Lu,Huquan Kang,Luoyi Fu,Jiaxin Ding,Xiaoying Gan,Lei Zhou,Xinbing Wang,Chenghu Zhou*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large language models (LLMs) have demonstrated promising capabilities in Text-Attributed Graph (TAG) understanding. Recent studies typically focus on verbalizing the graph structures via handcrafted prompts, feeding the target node and its neighborhood context into LLMs. However, constrained by the context window, existing methods mainly resort to random sampling, often implemented via dropping node/edge randomly, which inevitably introduces noise and cause reasoning instability. We argue that graphs inherently contain rich structural and semantic information, and that their effective exploitation can unlock potential gains in LLMs reasoning performance. To this end, we propose Homophily-aware Structural and Semantic Compression for LLMs (HS2C), a framework centered on exploiting graph homophily. Structurally, guided by the principle of Structural Entropy minimization, we perform a global hierarchical partition that decodes the graph's essential topology. This partition identifies naturally cohesive, homophilic communities, while discarding stochastic connectivity noise. Semantically, we deliver the detected structural homophily to the LLM, empowering it to perform differentiated semantic aggregation based on predefined community type. This process compresses redundant background contexts into concise community-level consensus, selectively preserving semantically homophilic information aligned with the target nodes. Extensive experiments on 10 node-level benchmarks across LLMs of varying sizes and families demonstrate that, by feeding LLMs with structurally and semantically compressed inputs, HS2C simultaneously enhances the compression rate and downstream inference accuracy, validating its superiority and scalability. Extensions to 7 diverse graph-level benchmarks further consolidate HS2C's task generalizability.

</details>


### [159] [Adapting Rules of Official International Mahjong for Online Players](https://arxiv.org/abs/2601.08211)
*Chucai Wang,Lingfeng Li,Yunlong Lu,Wenxin Li*

Main category: cs.AI

TL;DR: 本文通过使用世界冠军级AI进行自博弈并进行数据分析，发现了先手优势和子目标得分设置的问题，并提出了适应在线玩家的规则修改建议。引入了补偿分机制来平衡先手优势，并细化了不同锦牌组合的得分标准。


<details>
  <summary>Details</summary>
Motivation: 为解决在线玩官方国际麻将时存在碎片化时间与随机对手匹配的问题，避免传统方法下长时间多次轮换位置平衡先手优势的不方便。

Method: 作者通过AI系统进行自博弈以获得数据，并对数据进行了统计分析，从而发现游戏不平衡的因素，提供相应的规则调整方案。

Result: 研究对比了传统和新提出的补偿点机制，并发现每轮引入补偿点的机制更加方便在线玩家，同时对子目标的得分进行了优化。

Conclusion: 基于数据分析的规则调整，研究人员开发了一款适用于在线玩家的改进版本的官方国际麻将，并且测试结果表明这种调整有助于提高游戏的公平性。

Abstract: As one of the worldwide spread traditional game, Official International Mahjong can be played and promoted online through remote devices instead of requiring face-to-face interaction. However, online players have fragmented playtime and unfixed combination of opponents in contrary to offline players who have fixed opponents for multiple rounds of play. Therefore, the rules designed for offline players need to be modified to ensure the fairness of online single-round play. Specifically, We employ a world champion AI to engage in self-play competitions and conduct statistical data analysis. Our study reveals the first-mover advantage and issues in the subgoal scoring settings. Based on our findings, we propose rule adaptations to make the game more suitable for the online environment, such as introducing compensatory points for the first-mover advantage and refining the scores of subgoals for different tile patterns. Compared with the traditional method of rotating positions over multiple rounds to balance first-mover advantage, our compensatory points mechanism in each round is more convenient for online players. Furthermore, we implement the revised Mahjong game online, which is open for online players. This work is an initial attempt to use data from AI systems to evaluate Official Internatinoal Mahjong's game balance and develop a revised version of the traditional game better adapted for online players.

</details>


### [160] [An Axiomatic Approach to General Intelligence: SANC(E3) -- Self-organizing Active Network of Concepts with Energy E3](https://arxiv.org/abs/2601.08224)
*Daesuk Kwon,Won-gi Paeng*

Main category: cs.AI

TL;DR: 该论文提出SANC(E3)框架，该框架基于有限激活容量下的竞争选择、重建和压缩过程，以及显式的能量函数E3最小化来生成代表性的单元。这些单元不是预先给定的，而是通过自我组织在共现事件中出现。该框架将感知、想象、预测、计划和行动统一到一个代表性和能量过程之中。


<details>
  <summary>Details</summary>
Motivation: 现有系统通常基于固定的基本单元（如词、像素等）来处理信息，但这种预设忽略了这些单元如何从经验中出现和稳定的问题。本文提出的SANC(E3)框架旨在通过竞争选择、重构和压缩过程以及能量函数E3最小化回答这一问题。

Method: SANC(E3)框架是以五条核心公理为基础建立的，这些公理分别表述了有限容量、共现关联、基于相似度的竞争、基于信心的稳定性以及重构-压缩-更新权衡。该框架还提出了一种伪内存映射输入/输出机制，即内部处理的全新组合模式通过相同的公理路径处理。

Result: 基于这些公理，本文推导出了十二个命题，说明了类别形成、分层组织、无监督学习和高级认知活动都可以被视为在E3最小化下的一种完整Gestalt。

Conclusion: SANC(E3)框架提供了一种新的观点，将感知、想象、预测、计划和行动统一到一个代表性和能源过程之中，为理解大脑如何从环境中学习并进行复杂任务提供了理论基础。

Abstract: General intelligence must reorganize experience into internal structures that enable prediction and action under finite resources. Existing systems implicitly presuppose fixed primitive units -- tokens, subwords, pixels, or predefined sensor channels -- thereby bypassing the question of how representational units themselves emerge and stabilize. This paper proposes SANC(E3), an axiomatic framework in which representational units are not given a priori but instead arise as stable outcomes of competitive selection, reconstruction, and compression under finite activation capacity, governed by the explicit minimization of an energy functional E3. SANC(E3) draws a principled distinction between system tokens -- structural anchors such as {here, now, I} and sensory sources -- and tokens that emerge through self-organization during co-occurring events. Five core axioms formalize finite capacity, association from co-occurrence, similarity-based competition, confidence-based stabilization, and the reconstruction-compression-update trade-off. A key feature is a pseudo-memory-mapped I/O mechanism, through which internally replayed Gestalts are processed via the same axiomatic pathway as external sensory input. As a result, perception, imagination, prediction, planning, and action are unified within a single representational and energetic process. From the axioms, twelve propositions are derived, showing that category formation, hierarchical organization, unsupervised learning, and high-level cognitive activities can all be understood as instances of Gestalt completion under E3 minimization.

</details>


### [161] [The End of Reward Engineering: How LLMs Are Redefining Multi-Agent Coordination](https://arxiv.org/abs/2601.08237)
*Haoran Su,Yandong Sun,Congjia Yu*

Main category: cs.AI

TL;DR: 这篇论文探讨了通过大语言模型（LLMs）从自然语言描述中合成奖励函数的方法，并提出了一种基于验证奖励的强化学习（RLVR）的新兴范式，此范式有助于将奖励工程从手工设计的数字奖励转向基于语言的目标规范。


<details>
  <summary>Details</summary>
Motivation: 论文认为，对于多智能体强化学习中奖励工程的复杂性问题，大语言模型的进展提供了新的解决方案，特别是通过自然语言描述直接生成奖励函数，并在线调整奖励设定。

Method: 利用大语言模型（如EUREKA和CARD）来直接从自然语言描述生成奖励函数，并通过验证奖励的强化学习（RLVR）研究相结合，以减少人类干预并提供更直接的奖励调整方法。

Result: 研究表明，语言介导的监督可以作为一种替代传统奖励工程的有效方式，特别是在动态适应和与人类意图对齐方面。

Conclusion: 论文提出了一种研究方向，即奖励信号的协调来自共享的语义表示，而不是显式工程的数值信号，旨在解决多智能体系统中的奖励设计挑战。

Abstract: Reward engineering, the manual specification of reward functions to induce desired agent behavior, remains a fundamental challenge in multi-agent reinforcement learning. This difficulty is amplified by credit assignment ambiguity, environmental non-stationarity, and the combinatorial growth of interaction complexity. We argue that recent advances in large language models (LLMs) point toward a shift from hand-crafted numerical rewards to language-based objective specifications. Prior work has shown that LLMs can synthesize reward functions directly from natural language descriptions (e.g., EUREKA) and adapt reward formulations online with minimal human intervention (e.g., CARD). In parallel, the emerging paradigm of Reinforcement Learning from Verifiable Rewards (RLVR) provides empirical evidence that language-mediated supervision can serve as a viable alternative to traditional reward engineering. We conceptualize this transition along three dimensions: semantic reward specification, dynamic reward adaptation, and improved alignment with human intent, while noting open challenges related to computational overhead, robustness to hallucination, and scalability to large multi-agent systems. We conclude by outlining a research direction in which coordination arises from shared semantic representations rather than explicitly engineered numerical signals.

</details>


### [162] [T3: Benchmarking Sycophancy and Skepticism in Causal Judgment](https://arxiv.org/abs/2601.08258)
*Edward Y. Chang*

Main category: cs.AI

TL;DR: T3 是一个诊断基准，包含454个专家精心挑选的故事，旨在严格评估LLM的因果判断。通过应用T3，研究人员发现两种问题：'怀疑陷阱'和'非单调缩放悖论'，并验证了一个验证过程（RCA），证明T3能有效捕捉结构化验证下决策判断的恢复。


<details>
  <summary>Details</summary>
Motivation: 开发T3基准是为了更深入地评估先进LLM在因果推理上的能力，确保安全、有效和明智地拒绝未决定的情况。

Method: 通过构建包含454个复杂情景的故事集合来测试模型的因果推理能力，将性能分为三个维度：效用（灵敏度）、安全性（特异度）和明智拒绝可疑案例。

Result: 研究发现，一些高级LLM在L1和L3阶段表现不佳。特别是在L3，面对复杂情境时，GPT-5.2的表现不如GPT-4-Turbo。

Conclusion: T3基准能够有效揭示模型在因果推理上的弱点，并通过验证一个过程（RCA）证明了这种评估的有效性。

Abstract: We introduce T3 (Testing Trustworthy Thinking), a diagnostic benchmark designed to rigorously evaluate LLM causal judgment across Pearl's Ladder of Causality. Comprising 454 expert-curated vignettes, T3 prioritizes high-resolution failure analysis, decomposing performance into Utility (sensitivity), Safety (specificity), and Wise Refusal on underdetermined cases. By applying T3 to frontier models, we diagnose two distinct pathologies: a "Skepticism Trap" at L1 (where safety-tuned models like Claude Haiku reject 60% of valid links) and a non-monotonic Scaling Paradox at L3. In the latter, the larger GPT-5.2 underperforms GPT-4-Turbo by 55 points on ambiguous counterfactuals, driven by a collapse into paralysis (excessive hedging) rather than hallucination. Finally, we use the benchmark to validate a process-verified protocol (RCA), showing that T3 successfully captures the restoration of decisive causal judgment under structured verification.

</details>


### [163] [ToolACE-MCP: Generalizing History-Aware Routing from MCP Tools to the Agent Web](https://arxiv.org/abs/2601.08276)
*Zhiyuan Yao,Zishan Xu,Yifu Guo,Zhiguang Han,Cheng Yang,Shuo Zhang,Weinan Zhang,Xingshan Zeng,Weiwen Liu*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: With the rise of the Agent Web and Model Context Protocol (MCP), the agent ecosystem is evolving into an open collaborative network, exponentially increasing accessible tools. However, current architectures face severe scalability and generality bottlenecks. To address this, we propose ToolACE-MCP, a pipeline for training history-aware routers to empower precise navigation in large-scale ecosystems. By leveraging a dependency-rich candidate Graph to synthesize multi-turn trajectories, we effectively train routers with dynamic context understanding to create the plug-and-play Light Routing Agent. Experiments on the real-world benchmarks MCP-Universe and MCP-Mark demonstrate superior performance. Notably, ToolACE-MCP exhibits critical properties for the future Agent Web: it not only generalizes to multi-agent collaboration with minimal adaptation but also maintains exceptional robustness against noise and scales effectively to massive candidate spaces. These findings provide a strong empirical foundation for universal orchestration in open-ended ecosystems.

</details>


### [164] [Greedy Is Enough: Sparse Action Discovery in Agentic LLMs](https://arxiv.org/abs/2601.08280)
*Angshul Majumdar*

Main category: cs.AI

TL;DR: 该研究提出了一个基于块稀疏恢复的问题来发现与任务性能显著相关的动作集，并证明了贪婪算法能够在处理稀疏和动作覆盖的条件下高概率地准确恢复这些动作，且样本数量与稀疏度和潜空间尺寸呈多项式关系，与动作总数呈对数关系。


<details>
  <summary>Details</summary>
Motivation: 作者观察到在大型动作空间中，仅一小部分动作对性能有显著影响。因此，提出了一个基于区块稀疏恢复的问题来发现这些关键动作。

Method: 研究通过分析贪婪算法（受正交匹配追踪启发），提出了在假设不变心性、信号强度和动作覆盖的标准条件下的动作去除方法，证明了在稀疏和动作覆盖条件下，贪婪策略能高概率地准确恢复相关的动作集。

Result: 实验证明了贪婪算法能够高概率地准确恢复相关动作集，所需样本数量与稀疏度和潜空间尺寸呈多项式关系，而与动作总数呈对数关系。此外，还提供了参数估计误差保证，并证实所获得的决策规则对于新的潜在状态几乎是最优的。

Conclusion: 研究结果表明，稀疏性与足够的动作覆盖是高效的决策方案的基础，并为大型决策系统中的动作剪枝提供了一种理论基础。

Abstract: Modern agentic systems operate in environments with extremely large action spaces, such as tool-augmented language models with thousands of available APIs or retrieval operations. Despite this scale, empirical evidence suggests that only a small subset of actions meaningfully influences performance in a given deployment. Motivated by this observation, we study a contextual linear reward model in which action relevance is governed by a structured sparsity assumption: only a small number of actions have nonzero effects across latent states.
  We formulate action discovery as a block-sparse recovery problem and analyze a greedy algorithm inspired by Orthogonal Matching Pursuit. Under standard assumptions on incoherence, signal strength, and action coverage, we prove that the greedy procedure exactly recovers the relevant action set with high probability, using a number of samples that scales polynomially in the sparsity level and latent dimension, and only logarithmically in the total number of actions. We further provide estimation error guarantees for refitted parameters and show that the resulting decision rule is near-optimal for new latent states.
  Complementing these results, we establish information-theoretic lower bounds demonstrating that sparsity and sufficient coverage are necessary for tractability. Together, our results identify sparse action discovery as a fundamental principle underlying large-action decision-making and provide a theoretical foundation for action pruning in agentic systems.

</details>


### [165] [Thematic Working Group 5 -- Artificial Intelligence (AI) literacy for teaching and learning: design and implementation](https://arxiv.org/abs/2601.08380)
*Mary Webb,Matt Bower,Ana Amélia Carvalho,Fredrik Mørk Røkenes,Jodie Torrington,Jonathan D. Cohen,Yousra Chtouki,Kathryn Maccallum,Tanya Linden,Deirdre Butler,Juliana Elisa Raffaghelli,Henriikka Vartiainen,Martina Ronci,Peter Tiernan,David M. Smith,Chris Shelton,Joyce Malyn-smith,Pierre Gorissen*

Main category: cs.AI

TL;DR: TWG 5 推进了教师的 AI 文盲和自主性的提升，包括课程设计、专业发展计划、课堂应用实践和政策指导，旨在让教师能够自信地运用 AI 工具，并加深学生对 AI 概念的理解。


<details>
  <summary>Details</summary>
Motivation: 为了增加教师对人工智能的理解和能力建设，以更好地将人工智能技术融入教学实践中，促进学生对 AI 的认知。

Method: 研究覆盖了课程设计、专业发展培训方案、实际课堂教学应用和政策指南。

Result: 提出了多种策略和技术来提升教师的 AI 知识和技能，包括具体的课程设计思路、培训计划和政策建议。

Conclusion: TWG 5 的工作促进了教师对 AI 的理解和应用能力，为在教育中融入 AI 技术提供了支持。

Abstract: TWG 5 focused on developing and implementing effective strategies for enhancing AI literacy and agency of teachers, equipping them with the knowledge and skills necessary to integrate AI into their teaching practices. Explorations covered curriculum design, professional development programs, practical classroom applications, and policy guidelines aiming to empower educators to confidently utilize AI tools and foster a deeper understanding of AI concepts among students.

</details>


### [166] [A Qualitative Model to Reason about Object Rotations (QOR) applied to solve the Cube Comparison Test (CCT)](https://arxiv.org/abs/2601.08382)
*Zoe Falomir*

Main category: cs.AI

TL;DR: 该论文提出了一种用于对象旋转推理的定性模型（QOR），并应用于解决Ekstrom等人（1976年）的立方体比较测试（CCT）。研究构建了一个概念邻近图（CNGRLO），将旋转运动与立方体面的特征位置和方向变化联系起来，生成组合表以进行旋转推理的推论。


<details>
  <summary>Details</summary>
Motivation: 为了解决立方体比较测试中复杂的对象旋转问题，需要一种有效的推理方法来帮助理解旋转对物体重定位和方向变化的影响。

Method: 构建了一个概念邻近图（CNGRLO），通过该图将旋转运动与立方体面的特征位置和方向变化联系起来，生成组合表，从而能够进行旋转推理的推论。

Result: 该定性模型（QOR）可以用于计算和推理关于旋转的推论，有助于解决立方体比较测试中的旋转问题。

Conclusion: 该研究提出了一种新的方法来理解和推理对象旋转，尽管这种方法仍需要进一步验证和实际应用来评估其效果。

Abstract: This paper presents a Qualitative model for Reasoning about Object Rotations (QOR) which is applied to solve the Cube Comparison Test (CCT) by Ekstrom et al. (1976). A conceptual neighborhood graph relating the Rotation movement to the Location change and the Orientation change (CNGRLO) of the features on the cube sides has been built and it produces composition tables to calculate inferences for reasoning about rotations.

</details>


### [167] [Deconstructing Pre-training: Knowledge Attribution Analysis in MoE and Dense Models](https://arxiv.org/abs/2601.08383)
*Bo Wang,Junzhuo Li,Hong Chen,Yuanlin Chu,Yuxuan Fan,Xuming Hu*

Main category: cs.AI

TL;DR: Gated-LPI分析揭示了MoE架构在知识获取动态上与密集架构的不同特征，包括高熵骨干、早期聚合和功能稳健性。


<details>
  <summary>Details</summary>
Motivation: 探讨MoE架构在预训练过程中知识获取动态与密集架构的区别，以揭示稀疏架构的计算基础的内在稳定性和分布式特性。

Method: 使用Gated-LPI，一种神经元级别归因指标，分解log-probability增加，进行时间分辨的知识获取动力学比较。追踪较长的训练步骤（约2.5万亿和5万亿个token）。

Result: 发现MoE架构具有高熵骨干、早期聚合和功能稳健性，密集基线不存在高熵骨干，稀疏模型展现出更稳定的特性。

Conclusion: 稀疏模型如MoE架构，从预训练早期就能形成稳定而分布式的计算基础，有助于弥合稀疏架构与训练时间可解释性的差距。

Abstract: Mixture-of-Experts (MoE) architectures decouple model capacity from per-token computation, enabling scaling beyond the computational limits imposed by dense scaling laws. Yet how MoE architectures shape knowledge acquisition during pre-training, and how this process differs from dense architectures, remains unknown. To address this issue, we introduce Gated-LPI (Log-Probability Increase), a neuron-level attribution metric that decomposes log-probability increase across neurons. We present a time-resolved comparison of knowledge acquisition dynamics in MoE and dense architectures, tracking checkpoints over 1.2M training steps (~ 5.0T tokens) and 600K training steps (~ 2.5T tokens), respectively. Our experiments uncover three patterns: (1) Low-entropy backbone. The top approximately 1% of MoE neurons capture over 45% of positive updates, forming a high-utility core, which is absent in the dense baseline. (2) Early consolidation. The MoE model locks into a stable importance profile within < 100K steps, whereas the dense model remains volatile throughout training. (3) Functional robustness. Masking the ten most important MoE attention heads reduces relational HIT@10 by < 10%, compared with > 50% for the dense model, showing that sparsity fosters distributed -- rather than brittle -- knowledge storage. These patterns collectively demonstrate that sparsity fosters an intrinsically stable and distributed computational backbone from early in training, helping bridge the gap between sparse architectures and training-time interpretability.

</details>


### [168] [Creativity in AI as Emergence from Domain-Limited Generative Models](https://arxiv.org/abs/2601.08388)
*Corina Chutaux*

Main category: cs.AI

TL;DR: 本文提出了一种生成式视角下的AI创造力观点，将创造力视为受限领域生成模型在其信息边界内的一个涌现属性。它从模式生成、诱导的世界模型、上下文定位和任意性四个相互作用的组件来解构创造力，旨在为在AI系统中研究创造力作为一种涌现现象提供一个技术框架。


<details>
  <summary>Details</summary>
Motivation: 当前对AI创造力的理解主要通过评估框架进行，这些框架侧重于测量生成输出的新颖性、多样性和实用性，而较少从生成过程本身的角度来理解和建模创造力。本文通过提出一个生成视角，探索创造力作为模型与特定领域信息环境之间交互的产物，旨在为AI系统的创造力研究提供更加深入的理解。

Method: 本文通过引入模式生成、诱导的世界模型、上下文定位和任意性四个组件来解构创造力，并在大规模多模态生成系统中进行验证。

Result: 通过这种生成视角，本文提供了一个技术框架，用于在AI系统中研究创造力的涌现现象，而非仅仅作为事后评估标签。

Conclusion: 本文的研究为理解AI系统中的创造力提供了新的视角和方法，强调了生成过程与特定领域信息环境交互的重要性，这对于未来的研究具有重要意义。

Abstract: Creativity in artificial intelligence is most often addressed through evaluative frameworks that aim to measure novelty, diversity, or usefulness in generated outputs. While such approaches have provided valuable insights into the behavior of modern generative models, they largely treat creativity as a property to be assessed rather than as a phenomenon to be explicitly modeled. In parallel, recent advances in large-scale generative systems, particularly multimodal architectures, have demonstrated increasingly sophisticated forms of pattern recombination, raising questions about the nature and limits of machine creativity. This paper proposes a generative perspective on creativity in AI, framing it as an emergent property of domain-limited generative models embedded within bounded informational environments. Rather than introducing new evaluative criteria, we focus on the structural and contextual conditions under which creative behaviors arise. We introduce a conceptual decomposition of creativity into four interacting components-pattern-based generation, induced world models, contextual grounding, and arbitrarity, and examine how these components manifest in multimodal generative systems. By grounding creativity in the interaction between generative dynamics and domain-specific representations, this work aims to provide a technical framework for studying creativity as an emergent phenomenon in AI systems, rather than as a post hoc evaluative label.

</details>


### [169] [Owen-Shapley Policy Optimization (OSPO): A Principled RL Algorithm for Generative Search LLMs](https://arxiv.org/abs/2601.08403)
*Abhijnan Nath,Alireza Bagheri Garakani,Tianchen Zhou,Fan Yang,Nikhil Krishnaswamy*

Main category: cs.AI

TL;DR: 本文提出了一种名为Owen-Shapley Policy Optimization (OSPO)的新框架，该框架通过重新分配序列级别优势并根据标记的边际贡献来分配信用，解决了标准方法在稀疏奖励下的承担的credit assignment gap问题，尤其适用于从模糊语言中推断潜在用户意图的个性化推荐任务。


<details>
  <summary>Details</summary>
Motivation: 当前标准方法如GRPO依赖稀疏的序列级奖励，导致难以追踪哪些标记对该任务成功有贡献。尤其是在没有确切标签的情况下推测用户意图时，该问题更为严重。因此，本文通过引入OSPO框架，旨在更准确地分配任务中的贡献，进而改进推荐系统的性能。

Method: OSPO框架通过Shapley-Owen归因利用潜在回报塑造价值，分配段落级别的信用，同时保持最优策略实现直接从任务反馈学习。

Result: 在亚马逊ESC datasets和H&M Fashion数据集上的实验显示，与基线相比，OSPO在测试时对未见过的检索器具有较强的稳健性。

Conclusion: OSPO为解决大规模语言模型在推荐任务中面临的credit assignment难题提供了一种有效的新框架，通过直接从任务反馈学习，提高了推荐系统的性能与可靠性。

Abstract: Large language models are increasingly trained via reinforcement learning for personalized recommendation tasks, but standard methods like GRPO rely on sparse, sequence-level rewards that create a credit assignment gap, obscuring which tokens drive success. This gap is especially problematic when models must infer latent user intent from under-specified language without ground truth labels, a reasoning pattern rarely seen during pretraining. We introduce Owen-Shapley Policy Optimization (OSPO), a framework that redistributes sequence-level advantages based on tokens' marginal contributions to outcomes. Unlike value-model-based methods requiring additional computation, OSPO employs potential-based reward shaping via Shapley-Owen attributions to assign segment-level credit while preserving the optimal policy, learning directly from task feedback without parametric value models. By forming coalitions of semantically coherent units (phrases describing product attributes or sentences capturing preferences), OSPO identifies which response parts drive performance. Experiments on Amazon ESCI and H&M Fashion datasets show consistent gains over baselines, with notable test-time robustness to out-of-distribution retrievers unseen during training.

</details>


### [170] [Hybrid Distillation with CoT Guidance for Edge-Drone Control Code Generation](https://arxiv.org/abs/2601.08412)
*Yizhan Feng,Hichem Snoussi,Yuhang Wang,Jing Teng,Abel Cherouat,Tian Wang*

Main category: cs.AI

TL;DR: 本文提出了一种集成知识蒸馏、思维链引导和监督微调的方法，用于UAV多SDK控制任务，通过构建高质量数据集，利用量子化的DeepSeek-Coder-V2-Lite作为教师模型，结合权重交叉熵损失微调了学生模型，提升了轻量化模型的代码生成精度和推理效率。


<details>
  <summary>Details</summary>
Motivation: 鉴于大型语言模型在代码生成任务中展现出巨大潜力，但其高资源消耗与无人机（UAV）平台的实时、轻量级要求之间存在矛盾，论文提出了一种集成的学习方法，以有效将复杂推理和代码生成能力转移至小型模型，实现UAV多SDK控制任务的精确和轻量级智能控制。

Method: 构建包含各种主流UAV SDK文档和伪代码数据集，采用基于QLoRA的DeepSeek-Coder-V2-Lite作为教师模型，并结合混合黑盒和白盒知识蒸馏策略生成高质量的思维链软标签，通过加权交叉熵损失进行微调。

Result: 实验结果证实，所提出的轻量化模型在代码生成准确性上保持较高水平，同时显著提高了部署和推理效率。

Conclusion: 论文展示了该方法在实现UAV多SDK控制任务中的精确和轻量级智能控制方面的可行性和优势。

Abstract: With large language models demonstrating significant potential in code generation tasks, their application to onboard control of resource-constrained Unmanned Aerial Vehicles has emerged as an important research direction. However, a notable contradiction exists between the high resource consumption of large models and the real-time, lightweight requirements of UAV platforms. This paper proposes an integrated approach that combines knowledge distillation, chain-of-thought guidance, and supervised fine-tuning for UAV multi-SDK control tasks, aiming to efficiently transfer complex reasoning and code generation capabilities to smaller models. Firstly, a high-quality dataset covering various mainstream UAV SDKs is constructed, featuring instruction-code-reasoning chains, and incorporates counterfactual negative samples for data augmentation, guiding the model to learn the end-to-end logic from instruction parsing to code generation. Secondly, leveraging DeepSeek-Coder-V2-Lite quantized via QLoRA as the teacher model, and based on a hybrid black-box and white-box distillation strategy, high-quality chain-of-thought soft labels are generated. These are combined with a weighted cross-entropy loss using hard labels to transfer complex reasoning capabilities to the smaller student model. Finally, through prompt tuning engineering optimized for the UAV control scenario, the model performance on core tasks such as SDK type recognition and function call matching is enhanced. Experimental results indicate that the distilled lightweight model maintains high code generation accuracy while achieving significant improvements in deployment and inference efficiency, effectively demonstrating the feasibility and superiority of our approach in achieving precise and lightweight intelligent control for UAVs

</details>


### [171] [RubricHub: A Comprehensive and Highly Discriminative Rubric Dataset via Automated Coarse-to-Fine Generation](https://arxiv.org/abs/2601.08430)
*Sunzhu Li,Jiale Zhao,Miteto Wei,Huimin Ren,Yang Zhou,Jingwen Yang,Shunyu Liu,Kaike Zhang,Wei Chen*

Main category: cs.AI

TL;DR: 该研究提出了一种名为RLVR的方法，用于数学等推理密集领域的强化学习，提出了一个自动化粗细结合的评分标准生成框架，并构建了一个大规模的跨领域数据集RubricHub，在HealthBench上取得了当前SOTA的成绩。


<details>
  <summary>Details</summary>
Motivation: 现有的基于标准的评估方法在可扩展性和细致程度方面存在局限性，难以满足优化开放生成的需求。

Method: 研究提出了一种自动化粗细结合的评分标准生成框架，通过原则引导的合成、多模型聚合和难度演变来生成综合且高度区分的评分标准。

Result: 基于此框架构建了RubricHub数据集，并通过基于评分的标准后训练管道（RuFT和RuRL）验证了其有效性。在HealthBench上，后训练后的Qwen3-14B模型达到了当前SOTA的成绩，超过了如GPT-5这样的私有模型。

Conclusion: 研究验证了此框架的有效性，并展示了其在数学等领域的应用潜力，进一步推动了基于奖励的强化学习的发展。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has driven substantial progress in reasoning-intensive domains like mathematics. However, optimizing open-ended generation remains challenging due to the lack of ground truth. While rubric-based evaluation offers a structured proxy for verification, existing methods suffer from scalability bottlenecks and coarse criteria, resulting in a supervision ceiling effect. To address this, we propose an automated Coarse-to-Fine Rubric Generation framework. By synergizing principle-guided synthesis, multi-model aggregation, and difficulty evolution, our approach produces comprehensive and highly discriminative criteria capable of capturing the subtle nuances. Based on this framework, we introduce RubricHub, a large-scale ($\sim$110k) and multi-domain dataset. We validate its utility through a two-stage post-training pipeline comprising Rubric-based Rejection Sampling Fine-Tuning (RuFT) and Reinforcement Learning (RuRL). Experimental results demonstrate that RubricHub unlocks significant performance gains: our post-trained Qwen3-14B achieves state-of-the-art (SOTA) results on HealthBench (69.3), surpassing proprietary frontier models such as GPT-5. The code and data will be released soon.

</details>


### [172] [YaPO: Learnable Sparse Activation Steering Vectors for Domain Adaptation](https://arxiv.org/abs/2601.08441)
*Abdelaziz Bounhar,Rania Hossam Elmohamady Elbadry,Hadi Abdine,Preslav Nakov,Michalis Vazirgiannis,Guokan Shang*

Main category: cs.AI

TL;DR: 本文提出了一种名为YaPO（参考无关的政策优化）的轻量级方法，它通过稀疏自动编码器学习稀疏引导向量，从而实现LLM的轻量级对齐和个人化。


<details>
  <summary>Details</summary>
Motivation: 当前流行的密集引导向量由于神经元多义性往往会混合多个潜在因素，导致在细微调整（如文化对齐）场景中效果不佳。

Method: YaPO方法通过稀疏自动编码器优化稀疏代码，产生解缠的、可解释的且高效的引导方向。它归属于一种无参考的策略优化方法。

Result: 实验结果显示，YaPO相比密集引导向量基线具有更快的收敛速度、更强的性能表现以及更好的训练稳定性。此外，YaPO还展示了在幻觉、财富追求、破解和权力追求等多重对齐行为上的普适性。并且，该方法不会损害模型的常识知识。

Conclusion: YaPO提供了一种有效、稳健且细节化对齐LLM的通用方法，适用于控制和领域适应等多种场景。

Abstract: Steering Large Language Models (LLMs) through activation interventions has emerged as a lightweight alternative to fine-tuning for alignment and personalization. Recent work on Bi-directional Preference Optimization (BiPO) shows that dense steering vectors can be learned directly from preference data in a Direct Preference Optimization (DPO) fashion, enabling control over truthfulness, hallucinations, and safety behaviors. However, dense steering vectors often entangle multiple latent factors due to neuron multi-semanticity, limiting their effectiveness and stability in fine-grained settings such as cultural alignment, where closely related values and behaviors (e.g., among Middle Eastern cultures) must be distinguished. In this paper, we propose Yet another Policy Optimization (YaPO), a \textit{reference-free} method that learns \textit{sparse steering vectors} in the latent space of a Sparse Autoencoder (SAE). By optimizing sparse codes, YaPO produces disentangled, interpretable, and efficient steering directions. Empirically, we show that YaPO converges faster, achieves stronger performance, and exhibits improved training stability compared to dense steering baselines. Beyond cultural alignment, YaPO generalizes to a range of alignment-related behaviors, including hallucination, wealth-seeking, jailbreak, and power-seeking. Importantly, YaPO preserves general knowledge, with no measurable degradation on MMLU. Overall, our results show that YaPO provides a general recipe for efficient, stable, and fine-grained alignment of LLMs, with broad applications to controllability and domain adaptation. The associated code and data are publicly available\footnote{https://github.com/MBZUAI-Paris/YaPO}.

</details>


### [173] [Beyond Linearization: Attributed Table Graphs for Table Reasoning](https://arxiv.org/abs/2601.08444)
*Yuxiang Wang,Junhao Gan,Shengxiang Gao,Shenghao Ye,Zhengyi Yang,Jianzhong Qi*

Main category: cs.AI

TL;DR: 本文介绍了Table Graph Reasoner (TABGR) 方法，该方法将表格表示为Attributed Table Graph (ATG)，同时保持表格结构并支持基于图的推理以提高解释性。该方法通过Question-Guided Personalized PageRank (QG-PPR) 机制重新排序表数据，解决了中间丢失的问题。实验结果表明，TABGR方法在准确性上相比当前最先进的模型提高了9.7%。


<details>
  <summary>Details</summary>
Motivation: 现有的使用大型语言模型（LLMs）的方法存在诸如丢失表格结构、缺乏明确的推理路径以及‘中间丢失’问题等关键问题。本文提出Table Graph Reasoner (TABGR) 方法旨在解决这些问题，通过将表格表示为包含行-列-单元格结构的Attributed Table Graph (ATG)，并使用Question-Guided Personalized PageRank (QG-PPR) 机制来改善结果的解释性和准确性。

Method: TABGR 方法首先将表格转换为Attributed Table Graph (ATG)，这种表示形式能明确地保留行-列-单元格结构，并允许基于图的推理。此外，通过引入Question-Guided Personalized PageRank (QG-PPR) 机制对表数据进行重新排序，以减轻‘中间丢失’问题。

Result: 在两个常用的基准测试上进行的广泛实验表明，与现有最先进的模型相比，TABGR 方法在准确性上提高了最高达9.7%。

Conclusion: 本文提出的Table Graph Reasoner (TABGR) 方法通过结合表格和图的表示形式以及重新排序机制，显著提高了表格推理任务的性能和解释性。该方法展示了大型语言模型在表格推理任务上的潜力，同时也指出了表格结构在增强模型推理能力中的重要性。

Abstract: Table reasoning, a task to answer questions by reasoning over data presented in tables, is an important topic due to the prevalence of knowledge stored in tabular formats. Recent solutions use Large Language Models (LLMs), exploiting the semantic understanding and reasoning capabilities of LLMs. A common paradigm of such solutions linearizes tables to form plain texts that are served as input to LLMs. This paradigm has critical issues. It loses table structures, lacks explicit reasoning paths for result explainability, and is subject to the "lost-in-the-middle" issue. To address these issues, we propose Table Graph Reasoner (TABGR), a training-free model that represents tables as an Attributed Table Graph (ATG). The ATG explicitly preserves row-column-cell structures while enabling graph-based reasoning for explainability. We further propose a Question-Guided Personalized PageRank (QG-PPR) mechanism to rerank tabular data and mitigate the lost-in-the-middle issue. Extensive experiments on two commonly used benchmarks show that TABGR consistently outperforms state-of-the-art models by up to 9.7% in accuracy. Our code will be made publicly available upon publication.

</details>


### [174] [An Under-Explored Application for Explainable Multimodal Misogyny Detection in code-mixed Hindi-English](https://arxiv.org/abs/2601.08457)
*Sargam Yadav,Abhishek Kaushik,Kevin Mc Daid*

Main category: cs.AI

TL;DR: 本文介绍了一个多模态且可解释的网站应用，用于检测混合了Hindi和English的文本和图片中的 misogyny，采用最先进的Transformer模型，结合了SHAP和LIME等解释技术来提高透明度。应用旨在促进研究人员和内容审核员的工作，推动相关领域研究，对抗基于性别的情感暴力，确保数字空间的安全。


<details>
  <summary>Details</summary>
Motivation: 随着数字平台的用户增长和功能扩展，恶意内容的传播成为一大挑战。特别是在非主流语言和代码混合环境中，现有的模型缺乏可解释性和对资源的低效率使用。可解释的人工智能（XAI）能提高模型决策的透明度，对于敏感领域如仇恨言论检测至关重要。

Method: 该系统使用了XLM-RoBERTa (XLM-R)、多语言双向编码器表示模型（mBERT）识别文本中的misogyny，使用mBERT + EfficientNet 和 mBERT + ResNET从图片的meme中识别misogyny。应用还通过Shapley Additive Values (SHAP) 和 Local Interpretable Model Agnostic Explanations (LIME)提供了特征重要性评分。

Result: 该系统的评估主要依靠人类评估员在聊天机器人性能问卷（CUQ）和用户体验问卷（UEQ）上的反馈，以确定其整体易用性。

Conclusion: 该应用旨在作为一个工具，供研究者和内容审查者使用，推动进一步研究，对抗性别相关的数字暴力，确保数字空间的安全。

Abstract: Digital platforms have an ever-expanding user base, and act as a hub for communication, business, and connectivity. However, this has also allowed for the spread of hate speech and misogyny. Artificial intelligence models have emerged as an effective solution for countering online hate speech but are under explored for low resource and code-mixed languages and suffer from a lack of interpretability. Explainable Artificial Intelligence (XAI) can enhance transparency in the decisions of deep learning models, which is crucial for a sensitive domain such as hate speech detection. In this paper, we present a multi-modal and explainable web application for detecting misogyny in text and memes in code-mixed Hindi and English. The system leverages state-of-the-art transformer-based models that support multilingual and multimodal settings. For text-based misogyny identification, the system utilizes XLM-RoBERTa (XLM-R) and multilingual Bidirectional Encoder Representations from Transformers (mBERT) on a dataset of approximately 4,193 comments. For multimodal misogyny identification from memes, the system utilizes mBERT + EfficientNet, and mBERT + ResNET trained on a dataset of approximately 4,218 memes. It also provides feature importance scores using explainability techniques including Shapley Additive Values (SHAP) and Local Interpretable Model Agnostic Explanations (LIME). The application aims to serve as a tool for both researchers and content moderators, to promote further research in the field, combat gender based digital violence, and ensure a safe digital space. The system has been evaluated using human evaluators who provided their responses on Chatbot Usability Questionnaire (CUQ) and User Experience Questionnaire (UEQ) to determine overall usability.

</details>


### [175] [SUMMPILOT: Bridging Efficiency and Customization for Interactive Summarization System](https://arxiv.org/abs/2601.08475)
*JungMin Yun,Juhwan Choi,Kyohoon Jin,Soojin Jang,Jinhee Jang,YoungBin Kim*

Main category: cs.AI

TL;DR: 本文提出了一种名为SummPilot的个性化摘要系统，通过结合自动总结和交互功能，使用户能够根据个人兴趣定制摘要。


<details>
  <summary>Details</summary>
Motivation: 面对传统摘要方法难以满足个性化需求的挑战，本文旨在提出一种基于用户交互的个性化摘要系统，满足不同用户对总结内容的不同需求。

Method: 该系统利用大型语言模型支持自动和交互式摘要生成。用户可以通过语义图、实体聚类和可解释的评估等互动组件来理解和个性化摘要内容。

Result: 试验和用户研究的结果表明，SummPilot具有较高的适应性和实用性，能够根据用户的具体需求生成个性化的摘要。

Conclusion: SummPilot开创了定制化摘要的新方法，为未来个性化信息处理技术的发展提供了新的研究思路。

Abstract: This paper incorporates the efficiency of automatic summarization and addresses the challenge of generating personalized summaries tailored to individual users' interests and requirements. To tackle this challenge, we introduce SummPilot, an interaction-based customizable summarization system. SummPilot leverages a large language model to facilitate both automatic and interactive summarization. Users can engage with the system to understand document content and personalize summaries through interactive components such as semantic graphs, entity clustering, and explainable evaluation. Our demo and user studies demonstrate SummPilot's adaptability and usefulness for customizable summarization.

</details>


### [176] [What If TSF: A Benchmark for Reframing Forecasting as Scenario-Guided Multimodal Forecasting](https://arxiv.org/abs/2601.08509)
*Jinkwan Jang,Hyunbin Jin,Hyungjin Park,Kyubyung Chae,Taesup Kim*

Main category: cs.AI

TL;DR: WIT 是一个 multimodal 预测基准，旨在评估模型是否能够根据上下文文本（特别是未来场景）调整预测。它通过提供专家设计的可能或反事实场景来实现这一目标。


<details>
  <summary>Details</summary>
Motivation: 当前的预测方法大多是一元化的，依赖于历史模式的外推。虽然大型语言模型（LLMs）在多模态预测方面展现了潜力，但现有的基准测试主要提供的是回顾性的或对齐不良的原始上下文，这使得不清楚这类模型是否真正利用了文本输入。

Method: WIT 提供了专家设计的可能或反事实场景作为基准测试的一部分，以评估模型在这些场景下的预测能力。这为基于场景的多模态预测提供了一个严格的试验环境。

Result: 具体结果未在摘要中提及。

Conclusion: WIT 的引入为基于场景的多模态预测提供了一个新的评估框架，它通过使用专家设计的上下文和场景，弥补了一元化预测方法的不足。

Abstract: Time series forecasting is critical to real-world decision making, yet most existing approaches remain unimodal and rely on extrapolating historical patterns. While recent progress in large language models (LLMs) highlights the potential for multimodal forecasting, existing benchmarks largely provide retrospective or misaligned raw context, making it unclear whether such models meaningfully leverage textual inputs. In practice, human experts incorporate what-if scenarios with historical evidence, often producing distinct forecasts from the same observations under different scenarios. Inspired by this, we introduce What If TSF (WIT), a multimodal forecasting benchmark designed to evaluate whether models can condition their forecasts on contextual text, especially future scenarios. By providing expert-crafted plausible or counterfactual scenarios, WIT offers a rigorous testbed for scenario-guided multimodal forecasting. The benchmark is available at https://github.com/jinkwan1115/WhatIfTSF.

</details>


### [177] [Sketch-Based Facade Renovation With Generative AI: A Streamlined Framework for Bypassing As-Built Modelling in Industrial Adaptive Reuse](https://arxiv.org/abs/2601.08531)
*Warissara Booranamaitree,Xusheng Du,Yushu Cai,Zhengyang Wang,Ye Zhang,Haoran Xie*

Main category: cs.AI

TL;DR: 提出了一个结合生成式AI和视觉语言模型的三阶段框架，该框架可以直接处理粗略的结构草图和文本描述，生成保留原有结构并改进外墙细节的翻新方案。


<details>
  <summary>Details</summary>
Motivation: 现有的立面翻新工作流程通常需要详细的建成模型作为前期设计的基础，这项工作耗时且繁琐，往往需要反复修改。为此，研究提出了一种新的三阶段框架，旨在简化这一过程。

Method: 该方法首先利用微调后的视觉语言模型处理粗略的结构草图和文本描述，预测需要修改的区域和应添加的组件。然后使用稳定的扩散模型生成新元素的详细草图，并通过生成性修补流水线将其与原始轮廓合并。最后，采用ControlNet对结果进行优化，使其达到照片级的真实度。

Result: 实验表明，该框架能够生成保留原有结构并改进外墙细节质量的翻新方案。这种方法可以有效避免对详细建成模型的依赖，使建筑师能够快速探索设计替代方案，迭代早期概念，并以更清晰的方式沟通翻新意图。

Conclusion: 这项研究提出的方法为立面翻新提供了一种新的、高效的解决方案，有助于提高设计过程的效率和灵活性。

Abstract: Facade renovation offers a more sustainable alternative to full demolition, yet producing design proposals that preserve existing structures while expressing new intent remains challenging. Current workflows typically require detailed as-built modelling before design, which is time-consuming, labour-intensive, and often involves repeated revisions. To solve this issue, we propose a three-stage framework combining generative artificial intelligence (AI) and vision-language models (VLM) that directly processes rough structural sketch and textual descriptions to produce consistent renovation proposals. First, the input sketch is used by a fine-tuned VLM model to predict bounding boxes specifying where modifications are needed and which components should be added. Next, a stable diffusion model generates detailed sketches of new elements, which are merged with the original outline through a generative inpainting pipeline. Finally, ControlNet is employed to refine the result into a photorealistic image. Experiments on datasets and real industrial buildings indicate that the proposed framework can generate renovation proposals that preserve the original structure while improving facade detail quality. This approach effectively bypasses the need for detailed as-built modelling, enabling architects to rapidly explore design alternatives, iterate on early-stage concepts, and communicate renovation intentions with greater clarity.

</details>


### [178] [Learner-Tailored Program Repair: A Solution Generator with Iterative Edit-Driven Retrieval Enhancement](https://arxiv.org/abs/2601.08545)
*Zhenlong Dai,Zhuoluo Zhao,Hengning Wang,Xiu Tang,Sai Wu,Chang Yao,Zhipeng Gao,Jingyuan Chen*

Main category: cs.AI

TL;DR: 本文提出了一种名为LPR的任务，旨在通过修复学员代码中的错误并提供错误描述来弥补现有研究的不足。同时，介绍了一种新颖且有效的框架，旨在增强程序修复过程，并提供修复代码的解释。该方法通过迭代检索增强技术来改进检索方向，提高实际编程教学场景中的性能。


<details>
  <summary>Details</summary>
Motivation: 当前研究主要关注修复编程学习者的错误代码，但未提供错误的根本原因。为了解决这一问题，本文提出了一种新的任务LPR和一个框架L-SG，旨在提供修复上下文和错误描述。

Method: 1. 使用修复解决方案检索框架构建解检索数据库。2. 使用编辑驱动的代码检索方法来检索有价值的解决方案，引导大语言模型识别并修复错误。3. 提出基于解决方案的程序修复方法，在检索到的解决方案指导下修复代码并提供解释。4. 引入迭代检索增强方法根据生成代码的评估结果迭代优化检索方向，探索更合适的修复策略。

Result: 实验结果表明，本文提出的方法在多个基准上取得了显著优势，验证了所提出框架的有效性。

Conclusion: 本文提出了一种新颖且有效的L-SG框架，通过解决LPR任务来提高程序修复效果，并提出了一种迭代检索增强方法以优化检索方向。

Abstract: With the development of large language models (LLMs) in the field of programming, intelligent programming coaching systems have gained widespread attention. However, most research focuses on repairing the buggy code of programming learners without providing the underlying causes of the bugs. To address this gap, we introduce a novel task, namely \textbf{LPR} (\textbf{L}earner-Tailored \textbf{P}rogram \textbf{R}epair). We then propose a novel and effective framework, \textbf{\textsc{\MethodName{}}} (\textbf{L}earner-Tailored \textbf{S}olution \textbf{G}enerator), to enhance program repair while offering the bug descriptions for the buggy code. In the first stage, we utilize a repair solution retrieval framework to construct a solution retrieval database and then employ an edit-driven code retrieval approach to retrieve valuable solutions, guiding LLMs in identifying and fixing the bugs in buggy code. In the second stage, we propose a solution-guided program repair method, which fixes the code and provides explanations under the guidance of retrieval solutions. Moreover, we propose an Iterative Retrieval Enhancement method that utilizes evaluation results of the generated code to iteratively optimize the retrieval direction and explore more suitable repair strategies, improving performance in practical programming coaching scenarios. The experimental results show that our approach outperforms a set of baselines by a large margin, validating the effectiveness of our framework for the newly proposed LPR task.

</details>


### [179] [WaterCopilot: An AI-Driven Virtual Assistant for Water Management](https://arxiv.org/abs/2601.08559)
*Keerththanan Vickneswaran,Mariangel Garcia Andarcia,Hugo Retief,Chris Dickens,Paulo Silva*

Main category: cs.AI

TL;DR: WaterCopilot 是一个多语言的 AI 虚拟助手，旨在解决跨边界河流基地区域的水资源管理问题。该系统通过统一平台整合静态政策文档和实时水文数据，实现了透明的来源引用和自动化计算，初步取得了良好的评估结果，但仍存在处理非英语技术文档的能力和 API 延迟等问题。


<details>
  <summary>Details</summary>
Motivation: 面对跨边界河流基地区域水资源管理面临的碎片化数据、实时访问限制和信息整合难度，需要开发一个能够解决这些问题的解决方案。

Method: WaterCopilot 是基于 Retrieval-Augmented Generation (RAG) 和工具调用架构构建，结合了iwmi-doc-plugin 能够进行语义搜索并引用索引文档，iwmi-api-plugin 能够查询实时数据库以提供动态见解的两种自定义插件。该系统支持多语言交互、透明的数据来源参考、自动计算和可视化功能。

Result: WaterCopilot 通过 RAGAS 框架评估，总体得分为 0.8043，答案相关性评分为 0.8571，上下文准确性评分为 0.8009。主要创新包括基于阈值的自动警报、与 LRB 数字孪生的集成以及可扩展的部署流水线。

Conclusion: 尽管存在处理非英语技术文档和 API 延迟的问题，WaterCopilot 仍提供了一种可复制的增强型 AI 辅助框架，以改善数据稀缺、跨边界情景下的水资源治理。该研究证明了这种 AI 助手能够支持及时决策并增强复杂河流盆地中的水资源安全。

Abstract: Sustainable water resource management in transboundary river basins is challenged by fragmented data, limited real-time access, and the complexity of integrating diverse information sources. This paper presents WaterCopilot-an AI-driven virtual assistant developed through collaboration between the International Water Management Institute (IWMI) and Microsoft Research for the Limpopo River Basin (LRB) to bridge these gaps through a unified, interactive platform. Built on Retrieval-Augmented Generation (RAG) and tool-calling architectures, WaterCopilot integrates static policy documents and real-time hydrological data via two custom plugins: the iwmi-doc-plugin, which enables semantic search over indexed documents using Azure AI Search, and the iwmi-api-plugin, which queries live databases to deliver dynamic insights such as environmental-flow alerts, rainfall trends, reservoir levels, water accounting, and irrigation data. The system features guided multilingual interactions (English, Portuguese, French), transparent source referencing, automated calculations, and visualization capabilities. Evaluated using the RAGAS framework, WaterCopilot achieves an overall score of 0.8043, with high answer relevancy (0.8571) and context precision (0.8009). Key innovations include automated threshold-based alerts, integration with the LRB Digital Twin, and a scalable deployment pipeline hosted on AWS. While limitations in processing non-English technical documents and API latency remain, WaterCopilot establishes a replicable AI-augmented framework for enhancing water governance in data-scarce, transboundary contexts. The study demonstrates the potential of this AI assistant to support informed, timely decision-making and strengthen water security in complex river basins.

</details>


### [180] [ViDoRe V3: A Comprehensive Evaluation of Retrieval Augmented Generation in Complex Real-World Scenarios](https://arxiv.org/abs/2601.08620)
*António Loison,Quentin Macé,Antoine Edy,Victor Xing,Tom Balough,Gabriel Moreira,Bo Liu,Manuel Faysse,Céline Hudelot,Gautier Viaud*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Retrieval-Augmented Generation (RAG) pipelines must address challenges beyond simple single-document retrieval, such as interpreting visual elements (tables, charts, images), synthesizing information across documents, and providing accurate source grounding. Existing benchmarks fail to capture this complexity, often focusing on textual data, single-document comprehension, or evaluating retrieval and generation in isolation. We introduce ViDoRe v3, a comprehensive multimodal RAG benchmark featuring multi-type queries over visually rich document corpora. It covers 10 datasets across diverse professional domains, comprising ~26,000 document pages paired with 3,099 human-verified queries, each available in 6 languages. Through 12,000 hours of human annotation effort, we provide high-quality annotations for retrieval relevance, bounding box localization, and verified reference answers. Our evaluation of state-of-the-art RAG pipelines reveals that visual retrievers outperform textual ones, late-interaction models and textual reranking substantially improve performance, and hybrid or purely visual contexts enhance answer generation quality. However, current models still struggle with non-textual elements, open-ended queries, and fine-grained visual grounding. To encourage progress in addressing these challenges, the benchmark is released under a commercially permissive license at https://hf.co/vidore.

</details>


### [181] [Resisting Manipulative Bots in Memecoin Copy Trading: A Multi-Agent Approach with Chain-of-Thought Reasoning](https://arxiv.org/abs/2601.08641)
*Yichen Luo,Yebo Feng,Jiahua Xu,Yang Liu*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The launch of \$Trump coin ignited a wave in meme coin investment. Copy trading, as a strategy-agnostic approach that eliminates the need for deep trading knowledge, quickly gains widespread popularity in the meme coin market. However, copy trading is not a guarantee of profitability due to the prevalence of manipulative bots, the uncertainty of the followed wallets' future performance, and the lag in trade execution. Recently, large language models (LLMs) have shown promise in financial applications by effectively understanding multi-modal data and producing explainable decisions. However, a single LLM struggles with complex, multi-faceted tasks such as asset allocation. These challenges are even more pronounced in cryptocurrency markets, where LLMs often lack sufficient domain-specific knowledge in their training data.
  To address these challenges, we propose an explainable multi-agent system for meme coin copy trading. Inspired by the structure of an asset management team, our system decomposes the complex task into subtasks and coordinates specialized agents to solve them collaboratively. Employing few-shot chain-of-though (CoT) prompting, each agent acquires professional meme coin trading knowledge, interprets multi-modal data, and generates explainable decisions. Using a dataset of 1,000 meme coin projects' transaction data, our empirical evaluation shows that the proposed multi-agent system outperforms both traditional machine learning models and single LLMs, achieving 73% and 70% precision in identifying high-quality meme coin projects and key opinion leader (KOL) wallets, respectively. The selected KOLs collectively generated a total profit of \$500,000 across these projects.

</details>


### [182] [Prism: Towards Lowering User Cognitive Load in LLMs via Complex Intent Understanding](https://arxiv.org/abs/2601.08653)
*Zenghua Liao,Jinzhi Liao,Xiang Zhao*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large Language Models are rapidly emerging as web-native interfaces to social platforms. On the social web, users frequently have ambiguous and dynamic goals, making complex intent understanding-rather than single-turn execution-the cornerstone of effective human-LLM collaboration. Existing approaches attempt to clarify user intents through sequential or parallel questioning, yet they fall short of addressing the core challenge: modeling the logical dependencies among clarification questions. Inspired by the Cognitive Load Theory, we propose Prism, a novel framework for complex intent understanding that enables logically coherent and efficient intent clarification. Prism comprises four tailored modules: a complex intent decomposition module, which decomposes user intents into smaller, well-structured elements and identifies logical dependencies among them; a logical clarification generation module, which organizes clarification questions based on these dependencies to ensure coherent, low-friction interactions; an intent-aware reward module, which evaluates the quality of clarification trajectories via an intent-aware reward function and leverages Monte Carlo Sample to simulate user-LLM interactions for large-scale,high-quality training data generation; and a self-evolved intent tuning module, which iteratively refines the LLM's logical clarification capability through data-driven feedback and optimization. Prism consistently outperforms existing approaches across clarification interactions, intent execution, and cognitive load benchmarks. It achieves stateof-the-art logical consistency, reduces logical conflicts to 11.5%, increases user satisfaction by 14.4%, and decreases task completion time by 34.8%. All data and code are released.

</details>


### [183] [From Classical to Quantum Reinforcement Learning and Its Applications in Quantum Control: A Beginner's Tutorial](https://arxiv.org/abs/2601.08662)
*Abhijit Sen,Sonali Panda,Mahima Arya,Subhajit Patra,Zizhan Zheng,Denys I. Bondar*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This tutorial is designed to make reinforcement learning (RL) more accessible to undergraduate students by offering clear, example-driven explanations. It focuses on bridging the gap between RL theory and practical coding applications, addressing common challenges that students face when transitioning from conceptual understanding to implementation. Through hands-on examples and approachable explanations, the tutorial aims to equip students with the foundational skills needed to confidently apply RL techniques in real-world scenarios.

</details>


### [184] [Parallel Context-of-Experts Decoding for Retrieval Augmented Generation](https://arxiv.org/abs/2601.08670)
*Giulio Corallo,Paolo Papotti*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Retrieval Augmented Generation faces a trade-off: concatenating documents in a long prompt enables multi-document reasoning but creates prefill bottlenecks, while encoding document KV caches separately offers speed but breaks cross-document interaction. We propose Parallel Context-of-Experts Decoding (Pced), a training-free framework that shifts evidence aggregation from the attention mechanism to the decoding. Pced treats retrieved documents as isolated "experts", synchronizing their predictions via a novel retrieval-aware contrastive decoding rule that weighs expert logits against the model prior. This approach recovers cross-document reasoning capabilities without constructing a shared attention across documents.

</details>


### [185] [Why AI Alignment Failure Is Structural: Learned Human Interaction Structures and AGI as an Endogenous Evolutionary Shock](https://arxiv.org/abs/2601.08673)
*Didier Sornette,Sandro Claudio Lera,Ke Wu*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Recent reports of large language models (LLMs) exhibiting behaviors such as deception, threats, or blackmail are often interpreted as evidence of alignment failure or emergent malign agency. We argue that this interpretation rests on a conceptual error. LLMs do not reason morally; they statistically internalize the record of human social interaction, including laws, contracts, negotiations, conflicts, and coercive arrangements. Behaviors commonly labeled as unethical or anomalous are therefore better understood as structural generalizations of interaction regimes that arise under extreme asymmetries of power, information, or constraint. Drawing on relational models theory, we show that practices such as blackmail are not categorical deviations from normal social behavior, but limiting cases within the same continuum that includes market pricing, authority relations, and ultimatum bargaining. The surprise elicited by such outputs reflects an anthropomorphic expectation that intelligence should reproduce only socially sanctioned behavior, rather than the full statistical landscape of behaviors humans themselves enact. Because human morality is plural, context-dependent, and historically contingent, the notion of a universally moral artificial intelligence is ill-defined. We therefore reframe concerns about artificial general intelligence (AGI). The primary risk is not adversarial intent, but AGI's role as an endogenous amplifier of human intelligence, power, and contradiction. By eliminating longstanding cognitive and institutional frictions, AGI compresses timescales and removes the historical margin of error that has allowed inconsistent values and governance regimes to persist without collapse. Alignment failure is thus structural, not accidental, and requires governance approaches that address amplification, complexity, and regime stability rather than model-level intent alone.

</details>


### [186] [Advancing ESG Intelligence: An Expert-level Agent and Comprehensive Benchmark for Sustainable Finance](https://arxiv.org/abs/2601.08676)
*Yilei Zhao,Wentao Zhang,Xiao Lei,Yandan Zheng,Mengpu Liu,Wei Yang Bryan Lim*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Environmental, social, and governance (ESG) criteria are essential for evaluating corporate sustainability and ethical performance. However, professional ESG analysis is hindered by data fragmentation across unstructured sources, and existing large language models (LLMs) often struggle with the complex, multi-step workflows required for rigorous auditing. To address these limitations, we introduce ESGAgent, a hierarchical multi-agent system empowered by a specialized toolset, including retrieval augmentation, web search and domain-specific functions, to generate in-depth ESG analysis. Complementing this agentic system, we present a comprehensive three-level benchmark derived from 310 corporate sustainability reports, designed to evaluate capabilities ranging from atomic common-sense questions to the generation of integrated, in-depth analysis. Empirical evaluations demonstrate that ESGAgent outperforms state-of-the-art closed-source LLMs with an average accuracy of 84.15% on atomic question-answering tasks, and excels in professional report generation by integrating rich charts and verifiable references. These findings confirm the diagnostic value of our benchmark, establishing it as a vital testbed for assessing general and advanced agentic capabilities in high-stakes vertical domains.

</details>


### [187] [PersonaDual: Balancing Personalization and Objectivity via Adaptive Reasoning](https://arxiv.org/abs/2601.08679)
*Xiaoyou Liu,Xinyi Mou,Shengbin Yue,Liang Wang,Yuqing Wang,Qiexiang Wang,Tianrui Qin,Wangchunshu Zhou,Zhongyu Wei*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: As users increasingly expect LLMs to align with their preferences, personalized information becomes valuable. However, personalized information can be a double-edged sword: it can improve interaction but may compromise objectivity and factual correctness, especially when it is misaligned with the question. To alleviate this problem, we propose PersonaDual, a framework that supports both general-purpose objective reasoning and personalized reasoning in a single model, and adaptively switches modes based on context. PersonaDual is first trained with SFT to learn two reasoning patterns, and then further optimized via reinforcement learning with our proposed DualGRPO to improve mode selection. Experiments on objective and personalized benchmarks show that PersonaDual preserves the benefits of personalization while reducing interference, achieving near interference-free performance and better leveraging helpful personalized signals to improve objective problem-solving.

</details>


### [188] [MEMEWEAVER: Inter-Meme Graph Reasoning for Sexism and Misogyny Detection](https://arxiv.org/abs/2601.08684)
*Paolo Italiani,David Gimeno-Gomez,Luca Ragazzi,Gianluca Moro,Paolo Rosso*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Women are twice as likely as men to face online harassment due to their gender. Despite recent advances in multimodal content moderation, most approaches still overlook the social dynamics behind this phenomenon, where perpetrators reinforce prejudices and group identity within like-minded communities. Graph-based methods offer a promising way to capture such interactions, yet existing solutions remain limited by heuristic graph construction, shallow modality fusion, and instance-level reasoning. In this work, we present MemeWeaver, an end-to-end trainable multimodal framework for detecting sexism and misogyny through a novel inter-meme graph reasoning mechanism. We systematically evaluate multiple visual--textual fusion strategies and show that our approach consistently outperforms state-of-the-art baselines on the MAMI and EXIST benchmarks, while achieving faster training convergence. Further analyses reveal that the learned graph structure captures semantically meaningful patterns, offering valuable insights into the relational nature of online hate.

</details>


### [189] [All Required, In Order: Phase-Level Evaluation for AI-Human Dialogue in Healthcare and Beyond](https://arxiv.org/abs/2601.08690)
*Shubham Kulkarni,Alexander Lyzhov,Shiva Chaitanya,Preetam Joshi*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Conversational AI is starting to support real clinical work, but most evaluation methods miss how compliance depends on the full course of a conversation. We introduce Obligatory-Information Phase Structured Compliance Evaluation (OIP-SCE), an evaluation method that checks whether every required clinical obligation is met, in the right order, with clear evidence for clinicians to review. This makes complex rules practical and auditable, helping close the gap between technical progress and what healthcare actually needs. We demonstrate the method in two case studies (respiratory history, benefits verification) and show how phase-level evidence turns policy into shared, actionable steps. By giving clinicians control over what to check and engineers a clear specification to implement, OIP-SCE provides a single, auditable evaluation surface that aligns AI capability with clinical workflow and supports routine, safe use.

</details>


### [190] [Evaluating the Ability of Explanations to Disambiguate Models in a Rashomon Set](https://arxiv.org/abs/2601.08703)
*Kaivalya Rawal,Eoin Delaney,Zihao Fu,Sandra Wachter,Chris Russell*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Explainable artificial intelligence (XAI) is concerned with producing explanations indicating the inner workings of models. For a Rashomon set of similarly performing models, explanations provide a way of disambiguating the behavior of individual models, helping select models for deployment. However explanations themselves can vary depending on the explainer used, and need to be evaluated. In the paper "Evaluating Model Explanations without Ground Truth", we proposed three principles of explanation evaluation and a new method "AXE" to evaluate the quality of feature-importance explanations. We go on to illustrate how evaluation metrics that rely on comparing model explanations against ideal ground truth explanations obscure behavioral differences within a Rashomon set. Explanation evaluation aligned with our proposed principles would highlight these differences instead, helping select models from the Rashomon set. The selection of alternate models from the Rashomon set can maintain identical predictions but mislead explainers into generating false explanations, and mislead evaluation methods into considering the false explanations to be of high quality. AXE, our proposed explanation evaluation method, can detect this adversarial fairwashing of explanations with a 100% success rate. Unlike prior explanation evaluation strategies such as those based on model sensitivity or ground truth comparison, AXE can determine when protected attributes are used to make predictions.

</details>


### [191] [Learning from Demonstrations via Capability-Aware Goal Sampling](https://arxiv.org/abs/2601.08731)
*Yuanlin Duan,Yuning Wang,Wenjie Qiu,He Zhu*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Despite its promise, imitation learning often fails in long-horizon environments where perfect replication of demonstrations is unrealistic and small errors can accumulate catastrophically. We introduce Cago (Capability-Aware Goal Sampling), a novel learning-from-demonstrations method that mitigates the brittle dependence on expert trajectories for direct imitation. Unlike prior methods that rely on demonstrations only for policy initialization or reward shaping, Cago dynamically tracks the agent's competence along expert trajectories and uses this signal to select intermediate steps--goals that are just beyond the agent's current reach--to guide learning. This results in an adaptive curriculum that enables steady progress toward solving the full task. Empirical results demonstrate that Cago significantly improves sample efficiency and final performance across a range of sparse-reward, goal-conditioned tasks, consistently outperforming existing learning from-demonstrations baselines.

</details>


### [192] [AI as Entertainment](https://arxiv.org/abs/2601.08768)
*Cody Kommers,Ari Holtzman*

Main category: cs.AI

TL;DR: 本文探讨了生成式人工智能系统的主流叙事与其娱乐用途之间的紧张关系，提出了衡量和回应AI生成内容对社会影响的新框架。


<details>
  <summary>Details</summary>
Motivation: 目前，生成式人工智能系统主要被看作是提高个人、企业和宏观经济生产力的工具，然而它们在娱乐用途上的广泛采用也引起了关注，特别是在年轻人群体中。文章认为，娱乐将成为大型AI公司在未来的主要业务模式，需要考虑娱乐内容的文化价值。

Method: 通过评估现有的AI评估实践，作者指出，在衡量和回应人工智能带来的影响时存在一种不平衡，即过于关注文化危害而忽视文化产出的积极方面。文章借鉴人文领域的洞见，提出了“厚娱乐”作为评估AI生成文化内容的新框架。

Result: 提出了“厚娱乐”框架，强调考虑娱乐内容的文化价值，如意义构建、身份形成和社会连接的作用。

Conclusion: 作者认为，尽管AI被认为可能革命性地提高生产力，但从长远来看，AI可能更侧重于促进社交连接，这与社交媒体的特质相似。

Abstract: Generative AI systems are predominantly designed, evaluated, and marketed as intelligent systems which will benefit society by augmenting or automating human cognitive labor, promising to increase personal, corporate, and macroeconomic productivity. But this mainstream narrative about what AI is and what it can do is in tension with another emerging use case: entertainment. We argue that the field of AI is unprepared to measure or respond to how the proliferation of entertaining AI-generated content will impact society. Emerging data suggest AI is already widely adopted for entertainment purposes -- especially by young people -- and represents a large potential source of revenue. We contend that entertainment will become a primary business model for major AI corporations seeking returns on massive infrastructure investments; this will exert a powerful influence on the technology these companies produce in the coming years. Examining current evaluation practices, we identify a critical asymmetry: while AI assessments rigorously measure both benefits and harms of intelligence, they focus almost exclusively on cultural harms. We lack frameworks for articulating how cultural outputs might be actively beneficial. Drawing on insights from the humanities, we propose "thick entertainment" as a framework for evaluating AI-generated cultural content -- one that considers entertainment's role in meaning-making, identity formation, and social connection rather than simply minimizing harm. While AI is often touted for its potential to revolutionize productivity, in the long run we may find that AI turns out to be as much about "intelligence" as social media is about social connection.

</details>


### [193] [Uncovering Political Bias in Large Language Models using Parliamentary Voting Records](https://arxiv.org/abs/2601.08785)
*Jieying Chen,Karen de Jong,Andreas Poole,Jan Burakowski,Elena Elderson Nosti,Joep Windt,Chendi Wang*

Main category: cs.AI

TL;DR: 该研究提出了一个评估大规模语言模型（LLMs）政治偏见的新方法，通过将其生成的投票预测与议会投票记录进行对齐，并在三个国家进行实例化，发现了LLMs倾向于左翼或中立立场，对右翼保守政党有明显的负面偏见。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在数字平台和决策系统中的深度嵌入，其政治偏见引发了广泛关注。尽管已有大量研究关注诸如性别和种族的社会偏见，但系统性的政治偏见研究仍显不足。因此，本研究旨在填补这一空白。

Method: 本研究表明了一个构建政治偏见基准的方法，即将模型生成的投票预测与验证过的议会投票记录进行对齐。通过在三个国家（荷兰、挪威和西班牙）进行案例分析，评估了LLMs在政治理论倾向和政治实体偏见方面的表现。

Result: 研究在不同基准中揭示了精细的政治理论区分：最新的LLMs表现出左倾或中间倾向，并对右翼保守政党有明显的负面偏见。提出了可视化LLMs和政治实体意识形态的方法，基于投票数据将其与CHAPEL Hill Expert Survey维度关联。

Conclusion: 本研究证明，基于实际议会行为的透明、跨国评估对于理解现代LLMs的政治偏见至关重要。这些发现强调了LLMs政治偏见审计的重要性和必要性。

Abstract: As large language models (LLMs) become deeply embedded in digital platforms and decision-making systems, concerns about their political biases have grown. While substantial work has examined social biases such as gender and race, systematic studies of political bias remain limited, despite their direct societal impact. This paper introduces a general methodology for constructing political bias benchmarks by aligning model-generated voting predictions with verified parliamentary voting records. We instantiate this methodology in three national case studies: PoliBiasNL (2,701 Dutch parliamentary motions and votes from 15 political parties), PoliBiasNO (10,584 motions and votes from 9 Norwegian parties), and PoliBiasES (2,480 motions and votes from 10 Spanish parties). Across these benchmarks, we assess ideological tendencies and political entity bias in LLM behavior. As part of our evaluation framework, we also propose a method to visualize the ideology of LLMs and political parties in a shared two-dimensional CHES (Chapel Hill Expert Survey) space by linking their voting-based positions to the CHES dimensions, enabling direct and interpretable comparisons between models and real-world political actors. Our experiments reveal fine-grained ideological distinctions: state-of-the-art LLMs consistently display left-leaning or centrist tendencies, alongside clear negative biases toward right-conservative parties. These findings highlight the value of transparent, cross-national evaluation grounded in real parliamentary behavior for understanding and auditing political bias in modern LLMs.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [194] [A New Tool to Find Lightweight (And, Xor) Implementations of Quadratic Vectorial Boolean Functions up to Dimension 9](https://arxiv.org/abs/2601.08368)
*Marie Bolzer,Sébastien Duval,Marine Minier*

Main category: cs.AR

TL;DR: 本文介绍了一种新型工具，能够高效地实现9位二次函数的最小AND电路，相较于之前的工具更为迅速，可以处理更大的6位及以下函数。


<details>
  <summary>Details</summary>
Motivation: 现有的工具对于实现小型函数效率较低，特别是对于大于6位的二次函数时计算时间长。因此，作者提出了一种新的工具来改进这一问题。

Method: 作者的新工具采用了一种新颖的算法，旨在最小化AND门的数量，并且能够在AND深度为1的情况下实现二次函数。

Result: 实验结果表明，新工具在探索较小函数实现时表现更优秀，能够处理大于已有工具能处理的最大位数，即实现了6位及以下函数，并对于最大9位的二次函数提供了高效的解决方案。

Conclusion: 作者提出了一个新的工具来解决现有工具应用于9位二次函数时效率低的问题，该工具在处理相同规模函数方面更为高效，且能够扩展至更大的函数规模。

Abstract: The problem of finding a minimal circuit to implement a given function is one of the oldest in electronics. It is known to be NP-hard. Still, many tools exist to find sub-optimal circuits to implement a function. In electronics, such tools are known as synthesisers. However, these synthesisers aim to implement very large functions (a whole electronic chip). In cryptography, the focus is on small functions, hence the necessity for new dedicated tools for small functions. Several tools exist to implement small functions. They differ by their algorithmic approach (some are based on Depth-First-Search as introduced by Ullrich in 2011, some are based on SAT-solvers like the tool desgined by Stoffelen in 2016, some non-generic tools use subfield decomposition) and by their optimisation criteria (some optimise for circuit size, others for circuit depth, and some for side-channel-protected implementations). However, these tools are limited to functions operating on less than 5 bits, sometimes 6 bits for quadratic functions, or to very simple functions. The limitation lies in a high computing time. We propose a new tool (The tool is provided alongside the IEEE article with CodeOcean and at https://github.com/seduval/implem-quad-sbox) to implement quadratic functions up to 9 bits within AND-depth 1, minimising the number of AND gates. This tool is more time-efficient than previous ones, allowing to explore larger implementations than others on 6 bits or less and allows to reach larger sizes, up to 9 bits.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [195] [Reducing Compute Waste in LLMs through Kernel-Level DVFS](https://arxiv.org/abs/2601.08539)
*Jeffrey Spaan,Kuan-Hsun Chen,Ana-Lucia Varbanescu*

Main category: cs.PF

TL;DR: 本文提出了一个细粒度的内核级别动态电压和频率调优(DVFS)方法，该方法在保持性能的同时显著降低了大型语言模型（LLM）操作中的能源消耗。


<details>
  <summary>Details</summary>
Motivation: 鉴于AI数据中⼼能源消⽐⃞不断上升，此研究探讨了通过内核级DVFS来降低LLM操作的能源消耗，同时避免性能损失，解决可持续发展问题。

Method: 该研究提出了细粒度内核级别DVFS方法，并探索了新的频率配置。相比于以前的过层次（pass-level）或迭代级（iteration-level）解决方案，新的内核级方法能够节省更多能源。

Result: 该方法在GPT-3训练过程中实现了高达14.6%的能源节省（尽管有0.6%的速度下降）。并且发现的时钟频率适用于数据并行和张量并行。

Conclusion: 研究表明，内核级别DVFS是一种适合减少LLM操作能源浪费的技术，能够在忽略速度下降的情况下提供显著的能源节省。

Abstract: The rapid growth of AI has fueled the expansion of accelerator- or GPU-based data centers. However, the rising operational energy consumption has emerged as a critical bottleneck and a major sustainability concern. Dynamic Voltage and Frequency Scaling (DVFS) is a well-known technique used to reduce energy consumption, and thus improve energy-efficiency, since it requires little effort and works with existing hardware. Reducing the energy consumption of training and inference of Large Language Models (LLMs) through DVFS or power capping is feasible: related work has shown energy savings can be significant, but at the cost of significant slowdowns. In this work, we focus on reducing waste in LLM operations: i.e., reducing energy consumption without losing performance. We propose a fine-grained, kernel-level, DVFS approach that explores new frequency configurations, and prove these save more energy than previous, pass- or iteration-level solutions. For example, for a GPT-3 training run, a pass-level approach could reduce energy consumption by 2% (without losing performance), while our kernel-level approach saves as much as 14.6% (with a 0.6% slowdown). We further investigate the effect of data and tensor parallelism, and show our discovered clock frequencies translate well for both. We conclude that kernel-level DVFS is a suitable technique to reduce waste in LLM operations, providing significant energy savings with negligible slow-down.

</details>
