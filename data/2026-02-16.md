<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 57]
- [cs.CL](#cs.CL) [Total: 29]
- [cs.AR](#cs.AR) [Total: 4]
- [cs.AI](#cs.AI) [Total: 10]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Thermal Imaging for Contactless Cardiorespiratory and Sudomotor Response Monitoring](https://arxiv.org/abs/2602.12361)
*Constantino Álvarez Casado,Mohammad Rahman,Sasan Sharifipour,Nhi Nguyen,Manuel Lage Cañellas,Xiaoting Wu,Miguel Bordallo López*

Main category: cs.CV

TL;DR: 该研究通过热红外成像技术，分析面部热视频中的皮肤温度变化，提取心率、呼吸率及电导率活动等三种生物信号。研究基于信号处理管道方法，通过合成处理过程中的解调和分离，优化参数配置，最终在公共SIMULATOR STUDY 1数据集上获得心率、呼吸率的良好估计，而电导率活动的估计效果则受限于硬件帧率。


<details>
  <summary>Details</summary>
Motivation: 由于可见光方法无法直接获取电导率活动(EDA)，而电导率活动是衡量交感神经激活的标准标志物，因此采用热红外成像方法来接触性地估计心率、呼吸率和电导率活动。

Method: 该研究利用热红外成像数据，通过信号处理管道进行解调和分离，包括跟踪面部解剖区域、进行空间聚集和分离慢速的排汗趋势与快速的心肺成分。为心率应用了正交矩阵图像变换(OMIT) 分解，并在多个感兴趣区域（ROI）上应用。呼吸率则通过平均鼻部和脸颊信号，再进行频谱峰值检测。

Result: 研究评估了288种EDA配置和HR/BR管道在31个会话中的表现，发现最佳的固定EDA配置（鼻部区域，指数移动平均）达到了与手掌ED的平均绝对相关性为0.40 ± 0.23，个别会话达到0.89。呼吸率估计的平均绝对误差为3.1 ± 1.1 每分钟一次心跳（bpm），而心率估计则因为摄像机帧率（每秒7.5帧）较低，其平均绝对误差为13.8 ± 7.5 bpm。

Conclusion: 研究结果提供了评估热红外接触性生物信号估计的基本性能界限，并对管道设计提供指导。

Abstract: Thermal infrared imaging captures skin temperature changes driven by autonomic regulation and can potentially provide contactless estimation of electrodermal activity (EDA), heart rate (HR), and breathing rate (BR). While visible-light methods address HR and BR, they cannot access EDA, a standard marker of sympathetic activation. This paper characterizes the extraction of these three biosignals from facial thermal video using a signal-processing pipeline that tracks anatomical regions, applies spatial aggregation, and separates slow sudomotor trends from faster cardiorespiratory components. For HR, we apply an orthogonal matrix image transformation (OMIT) decomposition across multiple facial regions of interest (ROIs), and for BR we average nasal and cheek signals before spectral peak detection. We evaluate 288 EDA configurations and the HR/BR pipeline on 31 sessions from the public SIMULATOR STUDY 1 (SIM1) driver monitoring dataset. The best fixed EDA configuration (nose region, exponential moving average) reaches a mean absolute correlation of $0.40 \pm 0.23$ against palm EDA, with individual sessions reaching 0.89. BR estimation achieves a mean absolute error of $3.1 \pm 1.1$ bpm, while HR estimation yields $13.8 \pm 7.5$ bpm MAE, limited by the low camera frame rate (7.5 Hz). We report signal polarity alternation across sessions, short thermodynamic latency for well-tracked signals, and condition-dependent and demographic effects on extraction quality. These results provide baseline performance bounds and design guidance for thermal contactless biosignal estimation.

</details>


### [2] [LLaMo: Scaling Pretrained Language Models for Unified Motion Understanding and Generation with Continuous Autoregressive Tokens](https://arxiv.org/abs/2602.12370)
*Zekun Li,Sizhe An,Chengcheng Tang,Chuan Guo,Ivan Shugurov,Linguang Zhang,Amy Zhao,Srinath Sridhar,Lingling Tao,Abhay Mittal*

Main category: cs.CV

TL;DR: LLaMo 是一种新的统一框架，通过特定模态的 Mixture-of-Transformers 架构扩展预训练的语言模型，实现高效的多模态理解与生成，特别适用于零样本运动生成。


<details>
  <summary>Details</summary>
Motivation: 现有的方法在将运动数据与大型语言模型结合时存在一些局限性，如有限规模的数据对语言能力的遗忘以及通过量化将运动转换为离散表示方式带来的误差。这些局限性促使研究者开发新型的统一模型来改善多模态任务的表现。

Method: 通过模态特定的混合变换器（Mixture-of-Transformers）架构，将预训练的语言模型扩展，并采用因果连续的潜在空间编码人类运动，通过轻量级流匹配头部保持下一个token预测模式，支持实时流式运动生成。

Result: 实验结果表明，LLaMo 较好地实现了文本到运动的转换精度以及运动到文本的描述，尤其在零样本运动生成方面表现出色。

Conclusion: LLaMo 是一个重要的进展，为开发通用的统一运动语言大型模型奠定了基础。

Abstract: Recent progress in large models has led to significant advances in unified multimodal generation and understanding. However, the development of models that unify motion-language generation and understanding remains largely underexplored. Existing approaches often fine-tune large language models (LLMs) on paired motion-text data, which can result in catastrophic forgetting of linguistic capabilities due to the limited scale of available text-motion pairs. Furthermore, prior methods typically convert motion into discrete representations via quantization to integrate with language models, introducing substantial jitter artifacts from discrete tokenization. To address these challenges, we propose LLaMo, a unified framework that extends pretrained LLMs through a modality-specific Mixture-of-Transformers (MoT) architecture. This design inherently preserves the language understanding of the base model while enabling scalable multimodal adaptation. We encode human motion into a causal continuous latent space and maintain the next-token prediction paradigm in the decoder-only backbone through a lightweight flow-matching head, allowing for streaming motion generation in real-time (>30 FPS). Leveraging the comprehensive language understanding of pretrained LLMs and large-scale motion-text pretraining, our experiments demonstrate that LLaMo achieves high-fidelity text-to-motion generation and motion-to-text captioning in general settings, especially zero-shot motion generation, marking a significant step towards a general unified motion-language large model.

</details>


### [3] [Synthetic Image Detection with CLIP: Understanding and Assessing Predictive Cues](https://arxiv.org/abs/2602.12381)
*Marco Willi,Melanie Mathys,Michael Graber*

Main category: cs.CV

TL;DR: SynthCLIC, a paired dataset of real photographs and high-quality synthetic images, is introduced to study CLIP-based detectors for synthetic image detection. Despite achieving high accuracy on some benchmarks, these detectors show limited generalization and rely on high-level photography attributes rather than generator-specific artifacts.


<details>
  <summary>Details</summary>
Motivation: To understand the underlying cues CLIP-based detectors are leveraging in synthetic image detection and to assess their performance and generalization across different generative models.

Method: SynthCLIC dataset and an interpretable linear head with de-correlated activations and a text-grounded concept-model are used to analyze CLIP-based detectors. The performance of these detectors is evaluated on GAN and high-quality diffusion models benchmarks.

Result: CLIP-based linear detectors achieve 0.96 mAP on a GAN-based benchmark but only 0.92 on the high-quality diffusion model dataset SynthCLIC. The generalization across different generator families drops to as low as 0.37 mAP. Detectors primarily rely on high-level photographic attributes.

Conclusion: CLIP-based detectors perform well but generalize unevenly across diverse generative architectures. Continuous model updates and broader training exposure are needed for better performance.

Abstract: Recent generative models produce near-photorealistic images, challenging the trustworthiness of photographs. Synthetic image detection (SID) has thus become an important area of research. Prior work has highlighted how synthetic images differ from real photographs--unfortunately, SID methods often struggle to generalize to novel generative models and often perform poorly in practical settings. CLIP, a foundational vision-language model which yields semantically rich image-text embeddings, shows strong accuracy and generalization for SID. Yet, the underlying relevant cues embedded in CLIP-features remain unknown. It is unclear, whether CLIP-based detectors simply detect strong visual artifacts or exploit subtle semantic biases, both of which would render them useless in practical settings or on generative models of high quality. We introduce SynthCLIC, a paired dataset of real photographs and high-quality synthetic counterparts from recent diffusion models, designed to reduce semantic bias in SID. Using an interpretable linear head with de-correlated activations and a text-grounded concept-model, we analyze what CLIP-based detectors learn. CLIP-based linear detectors reach 0.96 mAP on a GAN-based benchmark but only 0.92 on our high-quality diffusion dataset SynthCLIC, and generalization across generator families drops to as low as 0.37 mAP. We find that the detectors primarily rely on high-level photographic attributes (e.g., minimalist style, lens flare, or depth layering), rather than overt generator-specific artifacts. CLIP-based detectors perform well overall but generalize unevenly across diverse generative architectures. This highlights the need for continual model updates and broader training exposure, while reinforcing CLIP-based approaches as a strong foundation for more universal, robust SID.

</details>


### [4] [Reproducing DragDiffusion: Interactive Point-Based Editing with Diffusion Models](https://arxiv.org/abs/2602.12393)
*Ali Subhan,Ashir Raza*

Main category: cs.CV

TL;DR: 本文研究了基于Diffusion的方法DragDiffusion的可重复性，通过使用作者提供的实现和DragBench基准，重现了主要消融实验，并观察到与原始工作一致的定性和定量趋势。我们的实验表明性能对一些假设的超参数敏感，尤其是优化的时间步和用于运动监督的特征级别，而其他组件则具有更广泛的适用范围。我们进一步评估了一种多时间步长的潜变量优化变体，发现它并不能提高空间准确性，却大大增加了计算成本。


<details>
  <summary>Details</summary>
Motivation: 为了支持DragDiffusion方法的核心主张，并对其在不同条件下的可靠可重复性进行明确。

Method: 通过使用作者提供的实现和DragBench基准，进行主要消融实验的重现。

Result: 观察到与原始工作一致的定性和定量趋势。性能对一些假设的超参数敏感，尤其是优化的时间步和用于运动监督的特征级别，而其他组件则具有更广泛的适用范围。多时间步长的潜变量优化变体在空间准确性方面未见改进，但增加了计算成本。

Conclusion: 研究支持DragDiffusion的核心主张，同时明确了其可靠可重复性的条件。

Abstract: DragDiffusion is a diffusion-based method for interactive point-based image editing that enables users to manipulate images by directly dragging selected points. The method claims that accurate spatial control can be achieved by optimizing a single diffusion latent at an intermediate timestep, together with identity-preserving fine-tuning and spatial regularization. This work presents a reproducibility study of DragDiffusion using the authors' released implementation and the DragBench benchmark. We reproduce the main ablation studies on diffusion timestep selection, LoRA-based fine-tuning, mask regularization strength, and UNet feature supervision, and observe close agreement with the qualitative and quantitative trends reported in the original work. At the same time, our experiments show that performance is sensitive to a small number of hyperparameter assumptions, particularly the optimized timestep and the feature level used for motion supervision, while other components admit broader operating ranges. We further evaluate a multi-timestep latent optimization variant and find that it does not improve spatial accuracy while substantially increasing computational cost. Overall, our findings support the central claims of DragDiffusion while clarifying the conditions under which they are reliably reproducible. Code is available at https://github.com/AliSubhan5341/DragDiffusion-TMLR-Reproducibility-Challenge.

</details>


### [5] [What does RL improve for Visual Reasoning? A Frankenstein-Style Analysis](https://arxiv.org/abs/2602.12395)
*Xirui Li,Ming Li,Tianyi Zhou*

Main category: cs.CV

TL;DR: 本文提出了一种Frankenstein分析框架，通过因果探针、参数比较和模型合并测试，揭示强化学习在视觉推理中的具体贡献。


<details>
  <summary>Details</summary>
Motivation: 现有研究中，既有的端到端基准测试难以明确分离强化学习在视觉推理中的具体能力提升，论文旨在解决这一问题。

Method: 采用因果探针进行功能定位，通过参数比较刻画更新特征，以及使用模型合并测试转移性。

Result: 强化学习主要在中后期层产生一致的推理时间变化，这些改进在合并和冻结模型的情况下依然转移且必要，表明强化学习在视觉推理中的具体贡献是对中后期Transformer计算的系统性优化，提升视觉到推理的对齐和推理性能。

Conclusion: 研究成果指出，强化学习在提升视觉推理能力上的稳定贡献并非普遍提升视觉感知，而是针对中后期Transformer的计算进行了系统性改进，这对理解多模态推理的改进具有重要局限。

Abstract: Reinforcement learning (RL) with verifiable rewards has become a standard post-training stage for boosting visual reasoning in vision-language models, yet it remains unclear what capabilities RL actually improves compared with supervised fine-tuning as cold-start initialization (IN). End-to-end benchmark gains conflate multiple factors, making it difficult to attribute improvements to specific skills. To bridge the gap, we propose a Frankenstein-style analysis framework including: (i) functional localization via causal probing; (ii) update characterization via parameter comparison; and (iii) transferability test via model merging. Instead, RL induces a consistent inference-time shift primarily in mid-to-late layers, and these mid-to-late refinements are both transferable (via merging) and necessary (via freezing) for RL gains. Overall, our results suggest that RL's reliable contribution in visual reasoning is not a uniform enhancement of visual perception, but a systematic refinement of mid-to-late transformer computation that improves vision-to-reasoning alignment and reasoning performance, highlighting the limitations of benchmark-only evaluation for understanding multimodal reasoning improvements.

</details>


### [6] [ZeroDiff++: Substantial Unseen Visual-semantic Correlation in Zero-shot Learning](https://arxiv.org/abs/2602.12401)
*Zihan Ye,Shreyank N Gowda,Kaile Du,Weijian Luo,Ling Shao*

Main category: cs.CV

TL;DR: 提出了一种基于扩散的生成框架ZeroDiff++, 该框架通过增强看到的和未看到的类别的视觉-语义关联来解决零样本学习中的伪相关问题，并通过多种技术改进了生成器的表现。


<details>
  <summary>Details</summary>
Motivation: 现有生成式零样本学习方法中，由于看到的类别样本不足，导致了视觉语义伪相关性，不易生成未见类别的特征，从而影响性能。

Method: ZeroDiff++框架包括扩散增强生成、监督对比表示、多视角判别器与Wasserstein互学习，在生成阶段引入了扩散测试时适应和扩散测试时生成，生成部分合成特征，连接真实和生成的数据。

Result: ZeroDiff++在三个零样本学习基准测试中显著优于现有方法，并且能够在样本稀缺的情况下保持鲁棒的性能。

Conclusion: 提出了零样本学习中基于扩散的生成方法ZeroDiff++, 并通过实验证明其在多个基准测试上的有效性。

Abstract: Zero-shot Learning (ZSL) enables classifiers to recognize classes unseen during training, commonly via generative two stage methods: (1) learn visual semantic correlations from seen classes; (2) synthesize unseen class features from semantics to train classifiers. In this paper, we identify spurious visual semantic correlations in existing generative ZSL worsened by scarce seen class samples and introduce two metrics to quantify spuriousness for seen and unseen classes. Furthermore, we point out a more critical bottleneck: existing unadaptive fully noised generators produce features disconnected from real test samples, which also leads to the spurious correlation. To enhance the visual-semantic correlations on both seen and unseen classes, we propose ZeroDiff++, a diffusion-based generative framework. In training, ZeroDiff++ uses (i) diffusion augmentation to produce diverse noised samples, (ii) supervised contrastive (SC) representations for instance level semantics, and (iii) multi view discriminators with Wasserstein mutual learning to assess generated features. At generation time, we introduce (iv) Diffusion-based Test time Adaptation (DiffTTA) to adapt the generator using pseudo label reconstruction, and (v) Diffusion-based Test time Generation (DiffGen) to trace the diffusion denoising path and produce partially synthesized features that connect real and generated data, and mitigates data scarcity further. Extensive experiments on three ZSL benchmarks demonstrate that ZeroDiff++ not only achieves significant improvements over existing ZSL methods but also maintains robust performance even with scarce training data. Code would be available.

</details>


### [7] [MonoLoss: A Training Objective for Interpretable Monosemantic Representations](https://arxiv.org/abs/2602.12403)
*Ali Nasiri-Sarvi,Anh Tien Nguyen,Hassan Rivaz,Dimitris Samaras,Mahdi S. Hosseini*

Main category: cs.CV

TL;DR: Sparse autoencoders (SAEs) are enhanced with a new training approach called Monosemanticity Loss (MonoLoss), based on the MonoScore metric, leading to improved monosemantic representations and better class purity, with potential accuracy gains in fine-tuning tasks.


<details>
  <summary>Details</summary>
Motivation: 当前标准训练目标对多义性神经表示的分解鼓励较弱，且现有的一致性度量需要跨所有数据集样本进行成对比较，导致训练和评估效率低下。MonoScore是一种高效的度量方法，引入MonoLoss旨在优化自动编码器（SAEs）的学习过程。

Method: 研究了MonoScore度量标准并提出了一种单次通过算法，即单图算法。在此基础上，提出了Monosemanticity Loss（MonoLoss），它是MonoScore的训练信号插件目标。

Result: 在OpenImagesV7数据集上，MonoLoss实现了高达1200倍的评估加速和159倍的训练加速，同时增加的每次迭代仅占约4%的额外开销。MonoLoss在大部分特征层中提升了MonoScore，并增加类纯度，最高提升基线纯度47.1%。在ResNet-50和CLIP-ViT-B/32微调中，MonoLoss带来了最高0.6%的准确率提升，同时保证了学习到的单一概念活性模式。

Conclusion: MonoLoss作为辅助正则化项提升了SAEs的性能，实现了更快的训练和更高的表示质量，为未来的研究提供了有价值的参考。

Abstract: Sparse autoencoders (SAEs) decompose polysemantic neural representations, where neurons respond to multiple unrelated concepts, into monosemantic features that capture single, interpretable concepts. However, standard training objectives only weakly encourage this decomposition, and existing monosemanticity metrics require pairwise comparisons across all dataset samples, making them inefficient during training and evaluation. We study a recent MonoScore metric and derive a single-pass algorithm that computes exactly the same quantity, but with a cost that grows linearly, rather than quadratically, with the number of dataset images. On OpenImagesV7, we achieve up to a 1200x speedup wall-clock speedup in evaluation and 159x during training, while adding only ~4% per-epoch overhead. This allows us to treat MonoScore as a training signal: we introduce the Monosemanticity Loss (MonoLoss), a plug-in objective that directly rewards semantically consistent activations for learning interpretable monosemantic representations. Across SAEs trained on CLIP, SigLIP2, and pretrained ViT features, using BatchTopK, TopK, and JumpReLU SAEs, MonoLoss increases MonoScore for most latents. MonoLoss also consistently improves class purity (the fraction of a latent's activating images belonging to its dominant class) across all encoder and SAE combinations, with the largest gain raising baseline purity from 0.152 to 0.723. Used as an auxiliary regularizer during ResNet-50 and CLIP-ViT-B/32 finetuning, MonoLoss yields up to 0.6\% accuracy gains on ImageNet-1K and monosemantic activating patterns on standard benchmark datasets. The code is publicly available at https://github.com/AtlasAnalyticsLab/MonoLoss.

</details>


### [8] [Prototype-driven fusion of pathology and spatial transcriptomics for interpretable survival prediction](https://arxiv.org/abs/2602.12441)
*Lihe Liu,Xiaoxi Pan,Yinyin Yuan,Lulu Shang*

Main category: cs.CV

TL;DR: PathoSpatial 是一个集成共注册WSI和空间转录组(ST)的端到端可解释框架，利用任务导向的原型学习在多级别的专家架构中，适应性地协调模态内的无监督发现与跨模态的监督聚合。它在三阴性乳腺癌队列中表现出强大的一致性表现，提供可解释的生物分子解释，突出可能的预后因素。


<details>
  <summary>Details</summary>
Motivation: 随着配对的WSI-ST队列扩展到人口规模，融合互补的空间信号以进行预后变得至关重要，但现有的跨模态融合策略尚不成熟。因此，需要一个既能维持判别能力又能增强可解释性的框架。

Method: PathoSpatial框架采用了任务导向的原型学习和多级专家模型架构，通过自适应协调模态内无监督发现和跨模态监督聚合，旨在提升疾病预后的预测能力。

Result: PathoSpatial在三阴性乳腺癌队列中的五个生存终点上展示了强大且一致的性能，优于或等于最先进的单模态和跨模态方法。它还支持后验原型解释和分子风险分解，为生物学驱动的预后因素提供了定量解释。

Conclusion: PathoSpatial提供了一种可扩展、可解释的跨模态学习方法框架，展示了在空间基因组学-病理学融合中的应用潜力。

Abstract: Whole slide images (WSIs) enable weakly supervised prognostic modeling via multiple instance learning (MIL). Spatial transcriptomics (ST) preserves in situ gene expression, providing a spatial molecular context that complements morphology. As paired WSI-ST cohorts scale to population level, leveraging their complementary spatial signals for prognosis becomes crucial; however, principled cross-modal fusion strategies remain limited for this paradigm. To this end, we introduce PathoSpatial, an interpretable end-to-end framework integrating co-registered WSIs and ST to learn spatially informed prognostic representations. PathoSpatial uses task-guided prototype learning within a multi-level experts architecture, adaptively orchestrating unsupervised within-modality discovery with supervised cross-modal aggregation. By design, PathoSpatial substantially strengthens interpretability while maintaining discriminative ability. We evaluate PathoSpatial on a triple-negative breast cancer cohort with paired ST and WSIs. PathoSpatial delivers strong and consistent performance across five survival endpoints, achieving superior or comparable performance to leading unimodal and multimodal methods. PathoSpatial inherently enables post-hoc prototype interpretation and molecular risk decomposition, providing quantitative, biologically grounded explanations, highlighting candidate prognostic factors. We present PathoSpatial as a proof-of-concept for scalable and interpretable multimodal learning for spatial omics-pathology fusion.

</details>


### [9] [Semantic-aware Adversarial Fine-tuning for CLIP](https://arxiv.org/abs/2602.12461)
*Jiacheng Zhang,Jinhao Li,Hanxun Huang,Sarah M. Erfani,Benjamin I. P. Rubinstein,Feng Liu*

Main category: cs.CV

TL;DR: 论文提出了一种新的基于语义增强的对抗训练方法（SAFT），通过使用语义增强的对抗样本对抗细调CLIP的图像编码器，提高了模型在零样本分类任务中的对抗鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 针对传统通过最小化单个图像和手工艺品之间的余弦相似性生成的对抗样本在使用语义增强相似度度量时无法有效欺骗CLIP的问题，论文提出了一种基于语义集合攻击的对抗细调方法，以增强CLIP的鲁棒性。

Method: 论文首先通过一个语义集合攻击生成语义感知的对抗样本，这些样本通过基础模型生成并经过改进以减少幻觉。然后，提出了一种基于语义增强的对抗细调方法（SAFT），使用这些语义感知的对抗样本来对抗细调CLIP的图像编码器。

Result: 经过实验验证，SAFT在16个数据集上显著提高了CLIP在零样本对抗鲁棒性方面的性能。

Conclusion: SAFT为增强图像基础模型的零样本分类任务对抗鲁棒性提供了一种有效的方法，并提供了相关代码供研究使用。

Abstract: Recent studies have shown that CLIP model's adversarial robustness in zero-shot classification tasks can be enhanced by adversarially fine-tuning its image encoder with adversarial examples (AEs), which are generated by minimizing the cosine similarity between images and a hand-crafted template (e.g., ''A photo of a {label}''). However, it has been shown that the cosine similarity between a single image and a single hand-crafted template is insufficient to measure the similarity for image-text pairs. Building on this, in this paper, we find that the AEs generated using cosine similarity may fail to fool CLIP when the similarity metric is replaced with semantically enriched alternatives, making the image encoder fine-tuned with these AEs less robust. To overcome this issue, we first propose a semantic-ensemble attack to generate semantic-aware AEs by minimizing the average similarity between the original image and an ensemble of refined textual descriptions. These descriptions are initially generated by a foundation model to capture core semantic features beyond hand-crafted templates and are then refined to reduce hallucinations. To this end, we propose Semantic-aware Adversarial Fine-Tuning (SAFT), which fine-tunes CLIP's image encoder with semantic-aware AEs. Extensive experiments show that SAFT outperforms current methods, achieving substantial improvements in zero-shot adversarial robustness across 16 datasets. Our code is available at: https://github.com/tmlr-group/SAFT.

</details>


### [10] [A Lightweight and Explainable DenseNet-121 Framework for Grape Leaf Disease Classification](https://arxiv.org/abs/2602.12484)
*Md. Ehsanul Haque,Md. Saymon Hosen Polash,Rakib Hasan Ovi,Aminul Kader Bulbul,Md Kamrul Siam,Tamim Hasan Saykat*

Main category: cs.CV

TL;DR: 本研究提出了一种基于优化的DenseNet 121模型进行葡萄叶病害分类的方法，该方法在性能、解释性和计算成本方面均优于传统模型。


<details>
  <summary>Details</summary>
Motivation: 目前的自动化方法成本高且缺乏可解释性，不能满足实际应用场景的需求，因此需要一种新的方法来提高葡萄病害检测的效率和准确性。

Method: 本研究利用了优化的DenseNet 121模型进行病害分类。研究中还包括了针对农业领域的特定预处理步骤，以及如静脉、边缘和病斑等病害相关特征的识别。

Result: 研究结果显示，提出的模型性能优越，准确率为99.27%，F1分数为99.28%，特异性为99.71%，Kappa值为98.86%，推断时间为9秒。此外，交叉验证表明其平均准确率为99.12%，在所有类别的泛化能力较强。通过Grad-CAM工具，模型能够突出表现病害相关区域，提高解释性和可信度。

Conclusion: 综合而言，该研究提出的方法在葡萄叶病害检测方面表现出强大的性能、广泛的适用性和计算效率，使得其能够应用于实际的葡萄园监控和管理中，对葡萄生产具有积极意义。

Abstract: Grapes are among the most economically and culturally significant fruits on a global scale, and table grapes and wine are produced in significant quantities in Europe and Asia. The production and quality of grapes are significantly impacted by grape diseases such as Bacterial Rot, Downy Mildew, and Powdery Mildew. Consequently, the sustainable management of a vineyard necessitates the early and precise identification of these diseases. Current automated methods, particularly those that are based on the YOLO framework, are often computationally costly and lack interpretability that makes them unsuitable for real-world scenarios. This study proposes grape leaf disease classification using Optimized DenseNet 121. Domain-specific preprocessing and extensive connectivity reveal disease-relevant characteristics, including veins, edges, and lesions. An extensive comparison with baseline CNN models, including ResNet18, VGG16, AlexNet, and SqueezeNet, demonstrates that the proposed model exhibits superior performance. It achieves an accuracy of 99.27%, an F1 score of 99.28%, a specificity of 99.71%, and a Kappa of 98.86%, with an inference time of 9 seconds. The cross-validation findings show a mean accuracy of 99.12%, indicating strength and generalizability across all classes. We also employ Grad-CAM to highlight disease-related regions to guarantee the model is highlighting physiologically relevant aspects and increase transparency and confidence. Model optimization reduces processing requirements for real-time deployment, while transfer learning ensures consistency on smaller and unbalanced samples. An effective architecture, domain-specific preprocessing, and interpretable outputs make the proposed framework scalable, precise, and computationally inexpensive for detecting grape leaf diseases.

</details>


### [11] [Human-Like Coarse Object Representations in Vision Models](https://arxiv.org/abs/2602.12486)
*Andrey Gizdov,Andrea Procopio,Yichen Li,Daniel Harari,Tomer Ullman*

Main category: cs.CV

TL;DR: 研究通过时间到碰撞行为范式，发现物体分割模型在训练时间和规模上有不同表现，呈现出一个倒U型曲线，较小或短暂训练的模型过于概括成块状，而大型或完全训练的模型过度分割并出现边界波动，只有中等程度的物体细分与人类行为最为匹配。这表明人类似然的粗略物体表示可能是资源限制作用的结果，而非特定偏见。


<details>
  <summary>Details</summary>
Motivation: 探索物体分割模型在物体表示上的表现及其与人类行为的相关性，以便更好地理解人类如何进行物理直觉推断。

Method: 使用时间到碰撞(TTC)行为范式，对不同训练时间和大小的物体分割模型进行比较分析，引入模型对齐度量，并通过剪枝控制模型的有效容量。

Result: 观察到一个倒U型曲线，表明在训练时间和规模的不同条件下，模型分割物体的表现不同。较小或短暂训练的模型倾向于过度概括，而大型或完全训练的模型过度分割，只有中等程度的分割最能匹配人类行为。

Conclusion: 研究表明，利用资源约束原理，可通过早期检查点、适度架构和轻度剪枝等简单调节手段来促进生成更物理高效的表征。

Abstract: Humans appear to represent objects for intuitive physics with coarse, volumetric bodies'' that smooth concavities - trading fine visual details for efficient physical predictions - yet their internal structure is largely unknown. Segmentation models, in contrast, optimize pixel-accurate masks that may misalign with such bodies. We ask whether and when these models nonetheless acquire human-like bodies. Using a time-to-collision (TTC) behavioral paradigm, we introduce a comparison pipeline and alignment metric, then vary model training time, size, and effective capacity via pruning. Across all manipulations, alignment with human behavior follows an inverse U-shaped curve: small/briefly trained/pruned models under-segment into blobs; large/fully trained models over-segment with boundary wiggles; and an intermediate ideal body granularity'' best matches humans. This suggests human-like coarse bodies emerge from resource constraints rather than bespoke biases, and points to simple knobs - early checkpoints, modest architectures, light pruning - for eliciting physics-efficient representations. We situate these results within resource-rational accounts balancing recognition detail against physical affordances.

</details>


### [12] [Layer-Specific Fine-Tuning for Improved Negation Handling in Medical Vision-Language Models](https://arxiv.org/abs/2602.12498)
*Ali Abbasi,Mehdi Taghipour,Rahmatollah Beheshti*

Main category: cs.CV

TL;DR: 本文提出了一种针对医学报告语义极性的新型基准，并开发了一种名为Negation-Aware Selective Training (NAST) 的适应方法，以提高VLM在区分肯定和否定临床陈述方面的性能。


<details>
  <summary>Details</summary>
Motivation: 近年来，基于视觉-语言模型在健康医疗领域中的应用逐渐增多，但传统的VLMs无法很好地处理否定表达，这对临床报告至关重要。因此，本文旨在改进VLMs的语义极性判断能力。

Method: 本文首先构建了一个针对放射科的诊断基准，引入了特定的医学VLMs基准测试，在受控临床条件下评估其极性敏感性。此外，还构建了一个包含语义和结构化信息的临床否定数据集。基于这些资源，本文提出了Negation-Aware Selective Training (NAST) 方法，利用因果追踪效应（CTEs）指导层间梯度更新，提高模型对否定语义的理解。

Result: 实验表明，NAST可以提升VLM对肯定和否定临床陈述的区分能力，同时保持与一般视觉-语言模型的一致性。

Conclusion: 本文提出的方法可以在保持医疗数据安全性和去除误导性信息的情况下，提升VLM的性能，对于临床报告语义极性的识别是一个重要的步骤。

Abstract: Negation is a fundamental linguistic operation in clinical reporting, yet vision-language models (VLMs) frequently fail to distinguish affirmative from negated medical statements. To systematically characterize this limitation, we introduce a radiology-specific diagnostic benchmark that evaluates polarity sensitivity under controlled clinical conditions, revealing that common medical VLMs consistently confuse negated and non-negated findings. To enable learning beyond simple condition absence, we further construct a contextual clinical negation dataset that encodes structured claims and supports attribute-level negations involving location and severity. Building on these resources, we propose Negation-Aware Selective Training (NAST), an interpretability-guided adaptation method that uses causal tracing effects (CTEs) to modulate layer-wise gradient updates during fine-tuning. Rather than applying uniform learning rates, NAST scales each layer's update according to its causal contribution to negation processing, transforming mechanistic interpretability signals into a principled optimization rule. Experiments demonstrate improved discrimination of affirmative and negated clinical statements without degrading general vision-language alignment, highlighting the value of causal interpretability for targeted model adaptation in safety-critical medical settings. Code and resources are available at https://github.com/healthylaife/NAST.

</details>


### [13] [Matching of SAR and optical images based on transformation to shared modality](https://arxiv.org/abs/2602.12515)
*Alexey Borisov,Evgeny Myasnikov,Vladislav Myasnikov*

Main category: cs.CV

TL;DR: 本文提出了一种通过将光学图像和SAR图像转换为共享模态来进行匹配的新方法，该方法利用RoMa模型实现，无需重新训练即可实现高质量的光学和SAR图像匹配。


<details>
  <summary>Details</summary>
Motivation: 光学图像和SAR图像因为获取原理的不同，精确的配准很难实现，本文提出了新的方法来解决这个问题。

Method: 将光学图像和SAR图像转换为一个新的具有相同通道数的共享模态，同时保持图像的重要特征，利用RoMa模型进行图像匹配。

Result: 该方法在MultiSenGE数据集上优于其他图像跨模态转换和特征匹配算法，提供了更好的匹配质量，并且更为灵活。

Conclusion: 新的图像配准方法有效提升了光学和SAR图像之间的精确匹配，并且因为使用了预训练的RoMa和DeDoDe模型，所以更为方便。

Abstract: Significant differences in optical images and Synthetic Aperture Radar (SAR) images are caused by fundamental differences in the physical principles underlying their acquisition by Earth remote sensing platforms. These differences make precise image matching (co-registration) of these two types of images difficult. In this paper, we propose a new approach to image matching of optical and SAR images, which is based on transforming the images to a new modality. The new image modality is common to both optical and SAR images and satisfies the following conditions. First, the transformed images must have an equal pre-defined number of channels. Second, the transformed and co-registered images must be as similar as possible. Third, the transformed images must be non-degenerate, meaning they must preserve the significant features of the original images. To further match images transformed to this shared modality, we train the RoMa image matching model, which is one of the leading solutions for matching of regular digital photographs. We evaluated the proposed approach on the publicly available MultiSenGE dataset containing both optical and SAR images. We demonstrated its superiority over alternative approaches based on image translation between original modalities and various feature matching algorithms. The proposed solution not only provides better quality of matching, but is also more versatile. It enables the use of ready-made RoMa and DeDoDe models, pre-trained for regular images, without retraining for a new modality, while maintaining high-quality matching of optical and SAR images.

</details>


### [14] [LiDAR-Anchored Collaborative Distillation for Robust 2D Representations](https://arxiv.org/abs/2602.12524)
*Wonjun Jo,Hyunwoo Ha,Kim Ji-Yeon,Hawook Jeong,Tae-Hyun Oh*

Main category: cs.CV

TL;DR: 该论文提出了一种新的自监督学习方法，称为协作蒸馏，该方法利用3D LiDAR作为自我监督来提高2D图像编码器在恶劣天气条件下的鲁棒性，同时保留其原有功能。该方法在多种下游任务中表现出色，并增强了3D感知。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习的发展，自监督学习取得了显著的进步。然而，现有的2D图像编码器在面对恶劣天气条件和噪声时效果不佳。提出协作蒸馏方法旨在克服这一挑战，提高鲁棒性。

Method: 协作蒸馏方法结合了3D LiDAR数据作为自我监督，通过对齐2D图像和3D点云来改进2D图像编码器在各种恶劣条件下的性能。

Result: 该方法在多种下游任务中优于竞争对手，并展示了强大的泛化能力。此外，即使在去除2D图像输入的情况下，方法依然能够提高3D感知能力。

Conclusion: 该研究不仅提高了现有2D图像编码器的鲁棒性，还展示了协作蒸馏方法在实际应用中的实用性和适应性。

Abstract: As deep learning continues to advance, self-supervised learning has made considerable strides. It allows 2D image encoders to extract useful features for various downstream tasks, including those related to vision-based systems. Nevertheless, pre-trained 2D image encoders fall short in conducting the task under noisy and adverse weather conditions beyond clear daytime scenes, which require for robust visual perception. To address these issues, we propose a novel self-supervised approach, \textbf{Collaborative Distillation}, which leverages 3D LiDAR as self-supervision to improve robustness to noisy and adverse weather conditions in 2D image encoders while retaining their original capabilities. Our method outperforms competing methods in various downstream tasks across diverse conditions and exhibits strong generalization ability. In addition, our method also improves 3D awareness stemming from LiDAR's characteristics. This advancement highlights our method's practicality and adaptability in real-world scenarios.

</details>


### [15] [Self-Supervised JEPA-based World Models for LiDAR Occupancy Completion and Forecasting](https://arxiv.org/abs/2602.12540)
*Haoran Zhu,Anna Choromanska*

Main category: cs.CV

TL;DR: 本文提出了一种名为AD-LiST-JEPA的自监督自动驾驶世界模型，利用JEPA框架从LiDAR数据中预测未来的时空演化。通过下游的LiDAR占位完成和预测任务（OCF），评估了学习的表示质量，并证明了预训练编码器在基于JEPA的世界模型学习后的OCF性能更优。


<details>
  <summary>Details</summary>
Motivation: 随着自主驾驶技术的发展，构建能够捕捉环境随时间和空间演变的世界模型对于支持长期规划至关重要。但是，传统的监督学习方法依赖大量标记数据，成本高昂。因此，需要一种能够从大量未标记数据中学习世界模型的自监督学习方法。

Method: AD-LiST-JEPA采取JEPA框架，通过利用大量的未标记LiDAR数据，实现高效的自监督学习。该方法不依赖于昂贵的人工标注，从而提高了学习效率。

Result: 实验结果表明，基于JEPA的世界模型学习后，AD-LiST-JEPA在下游的LiDAR占位完成和预测任务中表现更好，特别是经过预训练编码器的辅助。

Conclusion: AD-LiST-JEPA为自监督学习在自动驾驶领域的应用提供了一种新的解决方案，能够有效利用大量未标记的LiDAR数据，提高未来时空演化预测的准确性。

Abstract: Autonomous driving, as an agent operating in the physical world, requires the fundamental capability to build \textit{world models} that capture how the environment evolves spatiotemporally in order to support long-term planning. At the same time, scalability demands learning such models in a self-supervised manner; \textit{joint-embedding predictive architecture (JEPA)} enables learning world models via leveraging large volumes of unlabeled data without relying on expensive human annotations. In this paper, we propose \textbf{AD-LiST-JEPA}, a self-supervised world model for autonomous driving that predicts future spatiotemporal evolution from LiDAR data using a JEPA framework. We evaluate the quality of the learned representations through a downstream LiDAR-based occupancy completion and forecasting (OCF) task, which jointly assesses perception and prediction. Proof of concept experiments show better OCF performance with pretrained encoder after JEPA-based world model learning.

</details>


### [16] [PLLM: Pseudo-Labeling Large Language Models for CAD Program Synthesis](https://arxiv.org/abs/2602.12561)
*Yuanbo Li,Dule Shu,Yanying Chen,Matt Klenk,Daniel Ritchie*

Main category: cs.CV

TL;DR: 本文介绍了一种名为PLLM的自我训练框架，用于从未标记的3D形状中合成CAD程序。该方法利用预训练的CAD能力语言模型和3D形状数据集，通过迭代生成候选程序、选择高保真执行和增加程序来构建合成程序-形状对，从而实现微调。


<details>
  <summary>Details</summary>
Motivation: 现有的CAD程序合成方法依赖于带有配对几何形状-程序数据的监督训练，这往往不可用。本文提出的方法旨在突破这一限制，通过利用未标记的3D形状集自主学习CAD程序。

Method: PLLM框架通过以下步骤工作：首先，使用预训练的CAD语言模型和3D形状数据集初始化。然后，框架迭代地抽样候选程序，筛选出高保真执行，增加程序以构建合成程序-形状对，最终进行微调。

Result: 在适应DeepCAD中的CAD-Recode到未标记的ABC数据集上，PLLM展示了几何保真度和程序多样性方面的持续改进。

Conclusion: PLLM表明，通过自我训练，可以从未标记的数据中生成优质的CAD程序，为CAD程序合成开辟了新的路径。

Abstract: Recovering Computer-Aided Design (CAD) programs from 3D geometries is a widely studied problem. Recent advances in large language models (LLMs) have enabled progress in CAD program synthesis, but existing methods rely on supervised training with paired shape-program data, which is often unavailable. We introduce PLLM, a self-training framework for CAD program synthesis from unlabeled 3D shapes. Given a pre-trained CAD-capable LLM and a shape dataset, PLLM iteratively samples candidate programs, selects high-fidelity executions, and augments programs to construct synthetic program-shape pairs for fine-tuning. We experiment on adapting CAD-Recode from DeepCAD to the unlabeled ABC dataset show consistent improvements in geometric fidelity and program diversity.

</details>


### [17] [The Constant Eye: Benchmarking and Bridging Appearance Robustness in Autonomous Driving](https://arxiv.org/abs/2602.12563)
*Jiabao Wang,Hongyu Zhou,Yuanbo Yang,Jiahao Shao,Yiyi Liao*

Main category: cs.CV

TL;DR: 本文提出了一种名为navdream的高保真鲁棒性基准，通过像素对齐的样式转移，隔离出现象对驾驶性能的影响，揭示了现有规划算法在非分布外观条件下的显著退化，并提出了一种通用感知界面，通过冻结视觉基础模型(DINOv3)提取出现象不变特征，实现跨多种规划模型的零样本泛化。


<details>
  <summary>Details</summary>
Motivation: 当前的自动驾驶算法对不同分布的条件非常脆弱，识别关键分离故障，即对出现象和结构场景变化的区分不足。

Method: 该方法通过建立基于生成像素对齐风格转移的高保真鲁棒性基准navdream，创建一个视觉压力测试，以最小的几何偏差测试出现象对驾驶性能的影响。

Result: 研究结果表明，现有规划算法在非分布外观条件下表现明显下降，即使场景结构保持一致。通过使用冻结的视觉基础模型(DINOv3)提取出现象不变特征，方案实现了跨多种规划模型的零样本泛化。

Conclusion: 本文提出的方法通过隔离出现象的影响，提高了规划算法在不同分布条件下的鲁棒性，展示了在极端出现象变化情况下的稳定性能。

Abstract: Despite rapid progress, autonomous driving algorithms remain notoriously fragile under Out-of-Distribution (OOD) conditions. We identify a critical decoupling failure in current research: the lack of distinction between appearance-based shifts, such as weather and lighting, and structural scene changes. This leaves a fundamental question unanswered: Is the planner failing because of complex road geometry, or simply because it is raining? To resolve this, we establish navdream, a high-fidelity robustness benchmark leveraging generative pixel-aligned style transfer. By creating a visual stress test with negligible geometric deviation, we isolate the impact of appearance on driving performance. Our evaluation reveals that existing planning algorithms often show significant degradation under OOD appearance conditions, even when the underlying scene structure remains consistent. To bridge this gap, we propose a universal perception interface leveraging a frozen visual foundation model (DINOv3). By extracting appearance-invariant features as a stable interface for the planner, we achieve exceptional zero-shot generalization across diverse planning paradigms, including regression-based, diffusion-based, and scoring-based models. Our plug-and-play solution maintains consistent performance across extreme appearance shifts without requiring further fine-tuning. The benchmark and code will be made available.

</details>


### [18] [Language-Guided Invariance Probing of Vision-Language Models](https://arxiv.org/abs/2511.13494)
*Jae Joong Lee*

Main category: cs.CV

TL;DR: 该研究通过引入一种新基准LGIP，考察了视觉-语言模型在语义不变性和语义变化敏感性方面的表现，发现EVA02-CLIP和大型OpenCLIP版本表现最佳，而SigLIP和SigLIP2则表现较弱。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型如CLIP、OpenCLIP等在零样本设置下表现出色，但缺乏对控制性语言扰动的可靠性测试。

Method: 研究利用40k MS COCO图像及其五个人类描述生成语义不变性和语义变化基准，评价模型对语义扰动的反应。

Result: 研究结果表明，EVA02-CLIP和大型OpenCLIP版本在语义不变性和语义变化方面表现出色，SigLIP和SigLIP2则表现不佳。

Conclusion: 该研究指出，LGIP可以作为模型的通用诊断工具，评估VLMs在不同语言变化下的鲁棒性，超越传统的准确性评分。

Abstract: Recent vision-language models (VLMs) such as CLIP, OpenCLIP, EVA02-CLIP and SigLIP achieve strong zero-shot performance, but it is unclear how reliably they respond to controlled linguistic perturbations. We introduce Language-Guided Invariance Probing (LGIP), a benchmark that measures (i) invariance to meaning-preserving paraphrases and (ii) sensitivity to meaning-changing semantic flips in image-text matching. Using 40k MS COCO images with five human captions each, we automatically generate paraphrases and rule-based flips that alter object category, color or count, and summarize model behavior with an invariance error, a semantic sensitivity gap and a positive-rate statistic.
  Across nine VLMs, EVA02-CLIP and large OpenCLIP variants lie on a favorable invariance-sensitivity frontier, combining low paraphrase-induced variance with consistently higher scores for original captions than for their flipped counterparts. In contrast, SigLIP and SigLIP2 show much larger invariance error and often prefer flipped captions to the human descriptions, especially for object and color edits. These failures are largely invisible to standard retrieval metrics, indicating that LGIP provides a model-agnostic diagnostic for the linguistic robustness of VLMs beyond conventional accuracy scores.

</details>


### [19] [Unbiased Gradient Estimation for Event Binning via Functional Backpropagation](https://arxiv.org/abs/2602.12590)
*Jinze Chen,Wei Zhai,Han Han,Tiankai Ma,Yang Cao,Bin Li,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: 提出了一种新颖的框架，通过合成反向传播中的弱导数来进行无偏梯度估计，而不改变前向输出。实验表明，这种方法可以提高基于事件的自我运动估计、自监督光流和SLAM任务的性能。


<details>
  <summary>Details</summary>
Motivation: 当前事件驱动视觉中的梯度估计存在问题，常规的采样方法引入了偏差，而直接从原始事件学习又受限于梯度估计的准确性。

Method: 该方法提出了在反向传播过程中合成弱导数，利用积分部分原理将目标函数提升为泛函，进而获得梯度的积分形式。通过从采样向量重构余切函数来计算弱导数，它可以匹配平滑和非平滑目标的长期数值差。

Result: 实验结果表明，该方法在基于事件的自我运动估计中，RMS误差降低了3.2%，收敛速度提高了1.57倍；在复杂的下游任务中，自监督光流的EPE降低了9.4%，SLAM的RMS误差降低了5.1%。

Conclusion: 该方法为事件驱动视觉感知提供了更高效的学习机制。

Abstract: Event-based vision encodes dynamic scenes as asynchronous spatio-temporal spikes called events. To leverage conventional image processing pipelines, events are typically binned into frames. However, binning functions are discontinuous, which truncates gradients at the frame level and forces most event-based algorithms to rely solely on frame-based features. Attempts to directly learn from raw events avoid this restriction but instead suffer from biased gradient estimation due to the discontinuities of the binning operation, ultimately limiting their learning efficiency. To address this challenge, we propose a novel framework for unbiased gradient estimation of arbitrary binning functions by synthesizing weak derivatives during backpropagation while keeping the forward output unchanged. The key idea is to exploit integration by parts: lifting the target functions to functionals yields an integral form of the derivative of the binning function during backpropagation, where the cotangent function naturally arises. By reconstructing this cotangent function from the sampled cotangent vector, we compute weak derivatives that provably match long-range finite differences of both smooth and non-smooth targets. Experimentally, our method improves simple optimization-based egomotion estimation with 3.2\% lower RMS error and 1.57$\times$ faster convergence. On complex downstream tasks, we achieve 9.4\% lower EPE in self-supervised optical flow, and 5.1\% lower RMS error in SLAM, demonstrating broad benefits for event-based visual perception. Source code can be found at https://github.com/chjz1024/EventFBP.

</details>


### [20] [QuEPT: Quantized Elastic Precision Transformers with One-Shot Calibration for Multi-Bit Switching](https://arxiv.org/abs/2602.12609)
*Ke Xu,Yixin Wang,Zhongcheng Li,Hao Cui,Jinshui Hu,Xingyi Zhang*

Main category: cs.CV

TL;DR: QuEPT 提出了一种高效的后训练方案，通过一次校准可以在小数据切片上重建块级的多比特误差，支持不同低秩适配器的级联，并在切换比特宽度时实现统一和混合精度量化，从而提高了准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 针对 Transformer 架构的弹性量化研究有限，特别是对于大型语言模型，QuEPT 旨在通过减少存储和优化成本来提高量化效率和灵活性。

Method: QuEPT 引入了 Multi-Bit Token Merging (MB-ToMe) 和 Multi-Bit Cascaded Low-Rank adapters (MB-CLoRA)，在不重复优化的情况下实现灵活的比特宽度切换和增强的性能。

Result: 实验结果表明，QuEPT 在保持或超越现有最先进的后训练量化方法性能的同时，能有效提高准确性和鲁棒性。

Conclusion: QuEPT 提供了一种有效的解决方案，可以在较少的计算资源下实现多样化的量化部署，增强大型语言模型的性能并适应不同的应用场景。

Abstract: Elastic precision quantization enables multi-bit deployment via a single optimization pass, fitting diverse quantization scenarios.Yet, the high storage and optimization costs associated with the Transformer architecture, research on elastic quantization remains limited, particularly for large language models.This paper proposes QuEPT, an efficient post-training scheme that reconstructs block-wise multi-bit errors with one-shot calibration on a small data slice. It can dynamically adapt to various predefined bit-widths by cascading different low-rank adapters, and supports real-time switching between uniform quantization and mixed precision quantization without repeated optimization. To enhance accuracy and robustness, we introduce Multi-Bit Token Merging (MB-ToMe) to dynamically fuse token features across different bit-widths, improving robustness during bit-width switching. Additionally, we propose Multi-Bit Cascaded Low-Rank adapters (MB-CLoRA) to strengthen correlations between bit-width groups, further improve the overall performance of QuEPT. Extensive experiments demonstrate that QuEPT achieves comparable or better performance to existing state-of-the-art post-training quantization methods.Our code is available at https://github.com/xuke225/QuEPT

</details>


### [21] [Vision Token Reduction via Attention-Driven Self-Compression for Efficient Multimodal Large Language Models](https://arxiv.org/abs/2602.12618)
*Omer Faruk Deniz,Ruiyu Mao,Ruochen Li,Yapeng Tian,Latifur Khan*

Main category: cs.CV

TL;DR: ADSC（注意力驱动的自我压缩）是一种简化多模态大语言模型的方法，通过仅使用LLM的注意力机制，逐层减少视觉标记，无需额外的评分计算或辅助模块，大幅减少了计算量和内存消耗，同时保持了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大型语言模型（MLLMs）由于高度依赖视觉标记处理，导致巨大的计算开销。当前的剪枝方法要么运行在LLM之前，限制了适用性，要么在LLM内部使用与FlashAttention不兼容的启发式方法。ADSC提出了以LLM自身作为压缩指导，逐步减少视觉标记的新策略，期望提升模型效率与性能。

Method: ADSC通过在选定的LLM层应用均匀的标记下采样技术，形成瓶颈以促使模型重组并压缩剩余标记的信息。该方法完全兼容FlashAttention，无需额外的评分计算或辅助模块，实现了HUGS配置的有效简化。

Result: 在LLaVA-1.5模型上应用ADSC，计算量（FLOPs）减少了53.7%，峰值KV缓存内存减少了56.7%，同时保持了98.2%的原始模型性能。在多个基准测试中，ADSC在效率和准确性方面均优于此前的方法。在高压缩比下，该方法仍保持稳健，而基于启发式的技术则急剧下降。

Conclusion: ADSC提供了一种有效的多模态模型压缩方案，能够在不牺牲性能的情况下显著减少计算和内存开销。

Abstract: Multimodal Large Language Models (MLLMs) incur significant computational cost from processing numerous vision tokens through all LLM layers. Prior pruning methods operate either before the LLM, limiting generality due to diverse encoder-projector designs or within the LLM using heuristics that are incompatible with FlashAttention. We take a different approach: rather than identifying unimportant tokens, we treat the LLM itself as the optimal guide for compression. Observing that deeper layers naturally transmit vision-to-text information, we introduce Attention-Driven Self-Compression (ADSC), a simple, broadly applicable method that progressively reduces vision tokens using only the LLM's attention mechanism. Our method applies uniform token downsampling at selected layers, forming bottlenecks that encourage the model to reorganize and compress information into the remaining tokens. It requires no score computation, auxiliary modules, or attention modification, and remains fully compatible with FlashAttention. Applied to LLaVA-1.5, ADSC reduces FLOPs by 53.7% and peak KV-cache memory by 56.7%, while preserving 98.2% of the original model performance. Across multiple benchmarks, it outperforms prior pruning approaches in both efficiency and accuracy. Crucially, under high compression ratios, our method remains robust while heuristic-based techniques degrade sharply.

</details>


### [22] [ImageRAGTurbo: Towards One-step Text-to-Image Generation with Retrieval-Augmented Diffusion Models](https://arxiv.org/abs/2602.12640)
*Peijie Qiu,Hariharan Ramshankar,Arnau Ramisa,René Vidal,Amit Kumar K C,Vamsi Salaka,Rahul Bhagat*

Main category: cs.CV

TL;DR: 本文提出了一种名为ImageRAGTurbo的新方法，通过检索增强来高效微调少量步骤的扩散模型。该方法利用检索到的文本-图像对来改进图像生成过程，无需大量微调即可提升图像质量，同时保持生成速度。


<details>
  <summary>Details</summary>
Motivation: 尽管现有的少量步骤扩散模型在生成速度上有所提高，但它们的图像质量、提示对齐等问题仍然存在，特别是在单步骤生成时更为明显。ImageRAGTurbo旨在解决这一问题，通过检索增强来优化生成过程。

Method: ImageRAGTurbo方法通过检索数据库中的相关文本-图像对来增强扩散模型生成过程中的上下文信息，减少去噪步骤，而无需额外微调。进一步优化时，引入了一个在H空问中可训练的适配器，利用交叉注意机制将检索到的内容与目标提示进行高效融合。

Result: 实验结果表明，ImageRAGTurbo方法能够在不牺牲生成速度的情况下，生成高质量的图像，与现有方法相比更具优势。

Conclusion: ImageRAGTurbo通过检索增强策略有效改善了少量步骤扩散模型的图像生成性能，为快速生成高质量图像提供了一种解决方案。

Abstract: Diffusion models have emerged as the leading approach for text-to-image generation. However, their iterative sampling process, which gradually morphs random noise into coherent images, introduces significant latency that limits their applicability. While recent few-step diffusion models reduce the number of sampling steps to as few as one to four steps, they often compromise image quality and prompt alignment, especially in one-step generation. Additionally, these models require computationally expensive training procedures. To address these limitations, we propose ImageRAGTurbo, a novel approach to efficiently finetune few-step diffusion models via retrieval augmentation. Given a text prompt, we retrieve relevant text-image pairs from a database and use them to condition the generation process. We argue that such retrieved examples provide rich contextual information to the UNet denoiser that helps reduce the number of denoising steps without compromising image quality. Indeed, our initial investigations show that using the retrieved content to edit the denoiser's latent space ($\mathcal{H}$-space) without additional finetuning already improves prompt fidelity. To further improve the quality of the generated images, we augment the UNet denoiser with a trainable adapter in the $\mathcal{H}$-space, which efficiently blends the retrieved content with the target prompt using a cross-attention mechanism. Experimental results on fast text-to-image generation demonstrate that our approach produces high-fidelity images without compromising latency compared to existing methods.

</details>


### [23] [Multi-Task Learning with Additive U-Net for Image Denoising and Classification](https://arxiv.org/abs/2602.12649)
*Vikram Lakkavalli,Neelam Sinha*

Main category: cs.CV

TL;DR: AddUNet通过将跳跃连接融合模式从串联改为加法融合，实现了有效的编码器-解码器信息流控制，并在单任务去噪和多任务学习中表现出稳定的训练和竞争的重建性能。


<details>
  <summary>Details</summary>
Motivation: 现有的U-Net架构在多任务学习中存在优化不稳定的问题，通过引入控制跳跃连接融合模式，旨在提高模型在多个任务学习中的稳定性和性能。

Method: 提出的AddUNet通过使用门控加法融合替代传统的串联跳跃连接，限制了跳跃连接的容量同时保持固定特征维度，从而引入了结构正则化以控制编码器-解码器之间的信息流并优化多任务学习。

Result: 实验表明，AddUNet在单任务去噪和联合去噪-分类任务中均表现出良好的重建性能，并且对分类任务容量有限的情况具有鲁棒性。此外，跳跃连接中学习到的权重在不同的任务之间表现出有系统的任务感知重分布。

Conclusion: 该研究证明了简单的跳跃连接约束可以通过有效的方式作为用于稳定和可扩展多任务学习的正则化器，而不增加模型复杂性。

Abstract: We investigate additive skip fusion in U-Net architectures for image denoising and denoising-centric multi-task learning (MTL). By replacing concatenative skips with gated additive fusion, the proposed Additive U-Net (AddUNet) constrains shortcut capacity while preserving fixed feature dimensionality across depth. This structural regularization induces controlled encoder-decoder information flow and stabilizes joint optimization. Across single-task denoising and joint denoising-classification settings, AddUNet achieves competitive reconstruction performance with improved training stability. In MTL, learned skip weights exhibit systematic task-aware redistribution: shallow skips favor reconstruction, while deeper features support discrimination. Notably, reconstruction remains robust even under limited classification capacity, indicating implicit task decoupling through additive fusion. These findings show that simple constraints on skip connections act as an effective architectural regularizer for stable and scalable multi-task learning without increasing model complexity.

</details>


### [24] [CBEN -- A Multimodal Machine Learning Dataset for Cloud Robust Remote Sensing Image Understanding](https://arxiv.org/abs/2602.12652)
*Marco Stricker,Masakazu Iwamura,Koichi Kise*

Main category: cs.CV

TL;DR: 本文介绍了CloudyBigEarthNet（CBEN）数据集，并展示了在该数据集上训练的最先进的方法在面对有云的光学数据时的性能显著下降。通过对具有云遮挡的光学数据进行训练，这些方法在有云测试案例上的表现有了17.2-28.7%的相对改进。


<details>
  <summary>Details</summary>
Motivation: 由于云层常导致光学卫星图像失真，影响遥感任务，本文旨在开发适用于有云天气环境的方法。现有的方法多排除有云图像，本文提出一个结合光学和雷达数据且包含云遮挡的CBEN数据集，旨在增加方法的云鲁棒性。

Method: 本文方法涉及创建CBEN数据集，该数据集结合了光学与雷达图像，并含有云遮挡情况。此外，对先前在清晰天空条件下训练的方法进行调整以适应云光学数据。

Result: 在采用平均精确度（AP）作为评估指标时，最先进的方法处理CBEN数据集中的有云图像的性能急剧下降，下降幅度达到23-33个百分点。通过调整训练过程并应用于云光学数据，这些方法在有云测试数据上的表现提高了17.2-28.7个百分点。

Conclusion: 本文研究证明，通过调整训练过程以考虑云条件，可以显著提高光学遥感方法的云鲁棒性。提供了CBEN数据集和相关代码以促进进一步的研究和开发工作。

Abstract: Clouds are a common phenomenon that distorts optical satellite imagery, which poses a challenge for remote sensing. However, in the literature cloudless analysis is often performed where cloudy images are excluded from machine learning datasets and methods. Such an approach cannot be applied to time sensitive applications, e.g., during natural disasters. A possible solution is to apply cloud removal as a preprocessing step to ensure that cloudfree solutions are not failing under such conditions. But cloud removal methods are still actively researched and suffer from drawbacks, such as generated visual artifacts. Therefore, it is desirable to develop cloud robust methods that are less affected by cloudy weather. Cloud robust methods can be achieved by combining optical data with radar, a modality unaffected by clouds. While many datasets for machine learning combine optical and radar data, most researchers exclude cloudy images. We identify this exclusion from machine learning training and evaluation as a limitation that reduces applicability to cloudy scenarios. To investigate this, we assembled a dataset, named CloudyBigEarthNet (CBEN), of paired optical and radar images with cloud occlusion for training and evaluation. Using average precision (AP) as the evaluation metric, we show that state-of-the-art methods trained on combined clear-sky optical and radar imagery suffer performance drops of 23-33 percentage points when evaluated on cloudy images. We then adapt these methods to cloudy optical data during training, achieving relative improvement of 17.2-28.7 percentage points on cloudy test cases compared with the original approaches. Code and dataset are publicly available at: https://github.com/mstricker13/CBEN

</details>


### [25] [Motion Prior Distillation in Time Reversal Sampling for Generative Inbetweening](https://arxiv.org/abs/2602.12679)
*Wooseok Jeon,Seunghyun Shin,Dongmin Shin,Hae-Gon Jeon*

Main category: cs.CV

TL;DR: 该研究提出了一个名为Motion Prior Distillation (MPD)的简单而有效的方法，通过将前向路径的运动残差蒸馏到后向路径中，来减少双向不匹配，从而生成更加连贯的中间帧。


<details>
  <summary>Details</summary>
Motivation: 现有的基于双向路径的图像到视频生成方法，由于路径间不匹配问题导致了临时不连贯和视觉上的问题。

Method: 提出了一个名为Motion Prior Distillation (MPD)的技术，通过将前向路径的运动残差蒸馏到后向路径中，减少双向路径的不匹配，从而避免在路径末端造成模糊性。

Result: 该方法能够生成更连贯的中间帧，并通过标准基准测试和用户研究有效地验证了其效果。

Conclusion: 本文提出的方法有效地解决了双向路径生成中的不连贯问题，为图像到视频生成领域带来了改进的方法。

Abstract: Recent progress in image-to-video (I2V) diffusion models has significantly advanced the field of generative inbetweening, which aims to generate semantically plausible frames between two keyframes. In particular, inference-time sampling strategies, which leverage the generative priors of large-scale pre-trained I2V models without additional training, have become increasingly popular. However, existing inference-time sampling, either fusing forward and backward paths in parallel or alternating them sequentially, often suffers from temporal discontinuities and undesirable visual artifacts due to the misalignment between the two generated paths. This is because each path follows the motion prior induced by its own conditioning frame. In this work, we propose Motion Prior Distillation (MPD), a simple yet effective inference-time distillation technique that suppresses bidirectional mismatch by distilling the motion residual of the forward path into the backward path. Our method can deliberately avoid denoising the end-conditioned path which causes the ambiguity of the path, and yield more temporally coherent inbetweening results with the forward motion prior. We not only perform quantitative evaluations on standard benchmarks, but also conduct extensive user studies to demonstrate the effectiveness of our approach in practical scenarios.

</details>


### [26] [Channel-Aware Probing for Multi-Channel Imaging](https://arxiv.org/abs/2602.12696)
*Umar Marikkar,Syed Sameed Husain,Muhammad Awais,Sara Atito*

Main category: cs.CV

TL;DR: 提出了Channel-Aware Probing (CAP) 方法，在多通道成像 (MCI) 数据上通过控制特征流和分离编码通道与池化步骤来提高固定的预训练表示在下游任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在固定预训练表示方面研究不足，而直接将其他领域的策略应用到MCI上效果较差。因此，CAP通过利用MCI数据中的内在跨通道多样性来优化特征流控制。

Method: CAP方法包括两个关键技术：独立特征编码 (IFE)，用于单独编码每个通道；解耦池化 (DCP)，在跨通道聚合之前对通道内进行池化。

Result: 在三个MCI基准测试中，CAP在探针性能上优于标准探针协议，达到了从头开始微调的效果，并大大缩小了与从相同MCI预训练检查点进行全面微调的差距。

Conclusion: CAP能够有效利用MCI数据内部的跨通道多样性来优化固定表示用于下游任务，并在多个基准测试中展示了优势。

Abstract: Training and evaluating vision encoders on Multi-Channel Imaging (MCI) data remains challenging as channel configurations vary across datasets, preventing fixed-channel training and limiting reuse of pre-trained encoders on new channel settings. Prior work trains MCI encoders but typically evaluates them via full fine-tuning, leaving probing with frozen pre-trained encoders comparatively underexplored. Existing studies that perform probing largely focus on improving representations, rather than how to best leverage fixed representations for downstream tasks. Although the latter problem has been studied in other domains, directly transferring those strategies to MCI yields weak results, even worse than training from scratch. We therefore propose Channel-Aware Probing (CAP), which exploits the intrinsic inter-channel diversity in MCI datasets by controlling feature flow at both the encoder and probe levels. CAP uses Independent Feature Encoding (IFE) to encode each channel separately, and Decoupled Pooling (DCP) to pool within channels before aggregating across channels. Across three MCI benchmarks, CAP consistently improves probing performance over the default probing protocol, matches fine-tuning from scratch, and largely reduces the gap to full fine-tuning from the same MCI pre-trained checkpoints. Code can be found in https://github.com/umarikkar/CAP.

</details>


### [27] [SPRig: Self-Supervised Pose-Invariant Rigging from Mesh Sequences](https://arxiv.org/abs/2602.12740)
*Ruipeng Wang,Langkun Zhong,Miaowei Wang*

Main category: cs.CV

TL;DR: SPRig 提出了一种通用的微调框架，用于在保留现有模型的基础上学习跨帧一致的姿势不变的骨骼绑定，适用于缺乏 T 姿势的序列数据。


<details>
  <summary>Details</summary>
Motivation: 现有的骨骼绑定方法假设有一致的 T 姿势，但对缺乏 T 姿势的序列数据（如动物动作捕捉或 AIGC/视频衍生网格序列）不适用。SPRig 旨在提供一种通用方法，即使在没有 T 姿势的情况下也能保证骨骼绑定的一致性。

Method: SPRig 引入了一种新的跨帧一致性损失函数，通过这一损失函数对现有的骨骼绑定模型进行微调，以确保生成的骨骼绑定能够在整个序列中保持一致。

Result: 实验表明，SPRig 方法在处理复杂的序列数据时表现出色，能够生成连贯的骨骼绑定，并显著减少了基线方法带来的视觉伪影。

Conclusion: SPRig 为序列数据的骨骼绑定提供了一种新颖且有效的方法，促进该领域的发展，代码将在论文接受后公开发布。

Abstract: State-of-the-art rigging methods assume a canonical rest pose--an assumption that fails for sequential data (e.g., animal motion capture or AIGC/video-derived mesh sequences) that lack the T-pose. Applied frame-by-frame, these methods are not pose-invariant and produce topological inconsistencies across frames. Thus We propose SPRig, a general fine-tuning framework that enforces cross-frame consistency losses to learn pose-invariant rigs on top of existing models. We validate our approach on rigging using a new permutation-invariant stability protocol. Experiments demonstrate SOTA temporal stability: our method produces coherent rigs from challenging sequences and dramatically reduces the artifacts that plague baseline methods. The code will be released publicly upon acceptance.

</details>


### [28] [Towards reconstructing experimental sparse-view X-ray CT data with diffusion models](https://arxiv.org/abs/2602.12755)
*Nelas J. Thomsen,Xinyuan Wang,Felix Lucka,Ezgi Demircan-Tureyen*

Main category: cs.CV

TL;DR: 通过在不同程度具有领域偏移的合成数据集上训练扩散先验，并在逐渐增加难度的稀视角CT数据集上测试，研究揭示了领域偏移和正向模型偏移在将生成模型应用于实验数据时的作用。


<details>
  <summary>Details</summary>
Motivation: 研究目的在于探讨扩散生成模型在处理稀视角CT等逆问题时的有效性，特别是在实际应用中是否存在数据域移位或正向模型不匹配的问题。

Method: 使用物理模型拍摄的CT数据作为实际数据源，以Shepp-Logan模型作为合成数据的参考点。通过调整合成数据集以引入不同程度的领域偏移，训练不同版本的扩散先验，然后在逐渐增加稀视角程度的CT数据集上进行测试。

Result: 研究发现，领域偏移对模型性能有复杂影响。重度偏移会导致模型崩溃和幻视，而多样化的先验优于与实际数据高度匹配但窄范围的先验。此外，正向模型偏移导致图像样本偏离先验流形，这会产生伪影，但可通过衰减似然性调度来减轻这种问题，同时提高计算效率。

Conclusion: 研究结论表明，从合成数据到实验数据的性能提升并不直接，未来研究需要在实际基准上验证模型的有效性。

Abstract: Diffusion-based image generators are promising priors for ill-posed inverse problems like sparse-view X-ray Computed Tomography (CT). As most studies consider synthetic data, it is not clear whether training data mismatch (``domain shift'') or forward model mismatch complicate their successful application to experimental data. We measured CT data from a physical phantom resembling the synthetic Shepp-Logan phantom and trained diffusion priors on synthetic image data sets with different degrees of domain shift towards it. Then, we employed the priors in a Decomposed Diffusion Sampling scheme on sparse-view CT data sets with increasing difficulty leading to the experimental data. Our results reveal that domain shift plays a nuanced role: while severe mismatch causes model collapse and hallucinations, diverse priors outperform well-matched but narrow priors. Forward model mismatch pulls the image samples away from the prior manifold, which causes artifacts but can be mitigated with annealed likelihood schedules that also increase computational efficiency. Overall, we demonstrate that performance gains do not immediately translate from synthetic to experimental data, and future development must validate against real-world benchmarks.

</details>


### [29] [Towards complete digital twins in cultural heritage with ART3mis 3D artifacts annotator](https://arxiv.org/abs/2602.12761)
*Dimitrios Karamatskos,Vasileios Arampatzakis,Vasileios Sevetlidis,Stavros Nousias,Athanasios Kalogeras,Christos Koulamas,Aris Lalos,George Pavlidis*

Main category: cs.CV

TL;DR: ART3mis 是一种通用、用户友好且功能丰富的基于 Web 的 3D 数字遗产文本注释工具，支持元数据标注，符合 W3C Web 注释数据模型，旨在帮助文化遗产保护者、修复者和策展人轻松处理、分割和标注 3D 数字复制品。


<details>
  <summary>Details</summary>
Motivation: 当前的 3D 视觉化工具功能较为单一，缺乏通用性和互操作性，无法满足考古学家和文化遗产专业人员对附加功能的需求，如特定区域的 3D 数字遗产生命周期管理中的元数据标注等。因此，本文介绍了 ART3mis，旨在克服这些不足，提供一种易于使用且功能丰富的解决方案。

Method: 本文通过设计与开发 ART3mis，主要包括前端 Web 用户界面和后端服务，支持在线注释、存储和管理 3D 数字遗产数据，并确保符合 W3C Web 注释数据模型。

Result: ART3mis 已经成功应用于多个 3D 数字遗产项目，并得到了文化遗产专业人员的好评。功能包括但不限于：3D 数字复制品的可视化、区域划分、元数据标注、存储以及与 W3C Web 注释数据模型的互操作。

Conclusion: ART3mis 作为一种实用的 3D 数字遗产管理工具，不仅简化了文化遗产专业人员的工作流程，也提升了他们处理复杂 3D 数字复制品的能力。未来，该工具将继续优化和拓展，以更好地满足文化遗产保护领域的需求。

Abstract: Archaeologists, as well as specialists and practitioners in cultural heritage, require applications with additional functions, such as the annotation and attachment of metadata to specific regions of the 3D digital artifacts, to go beyond the simplistic three-dimensional (3D) visualization. Different strategies addressed this issue, most of which are excellent in their particular area of application, but their capacity is limited to their design's purpose; they lack generalization and interoperability. This paper introduces ART3mis, a general-purpose, user-friendly, feature-rich, interactive web-based textual annotation tool for 3D objects. Moreover, it enables the communication, distribution, and reuse of information as it complies with the W3C Web Annotation Data Model. It is primarily designed to help cultural heritage conservators, restorers, and curators who lack technical expertise in 3D imaging and graphics, handle, segment, and annotate 3D digital replicas of artifacts with ease.

</details>


### [30] [PixelRush: Ultra-Fast, Training-Free High-Resolution Image Generation via One-step Diffusion](https://arxiv.org/abs/2602.12769)
*Hong-Phuc Lai,Phong Nguyen,Anh Tran*

Main category: cs.CV

TL;DR: PixelRush 是一种无调优高分辨率文本到图像生成框架，通过建立在基于块的推理框架之上，采用高效的块去噪策略，并提出平滑融合策略和噪声注入机制，以克服小步数生成中引入的伪影和过度平滑问题。该方法在4K图像生成方面表现出色，速度比最先进的方法提高了10到35倍，同时保持了视觉保真度。


<details>
  <summary>Details</summary>
Motivation: 现有方法虽然能够生成高质图像，但在高分辨率生成上仍受限于其原本的训练分辨率。PixelRush 致力于克服这一限制，提供一种无需调优的高分辨率文本到图像生成框架，从而提高效率和保持视觉质量。

Method: PixelRush 基于块推理框架，通过高效的块去噪策略，并用平滑融合策略和噪声注入机制来处理小步数生成中的伪影和过度平滑问题。这种方法避免了多次反向求解和再生成的循环，能够在低步数背景下高效执行块去噪。

Result: PixelRush 能够在约20秒内生成4K图像，比最先进的方法快10到35倍，同时保持了高质量的视觉效果。大量的实验验证了该方法的性能改进和输出质量。

Conclusion: PixelRush 是一种突破性的高分辨率文本到图像生成方法，通过创新的无调优策略显著提高了生成效率，同时满足了视觉保真度的要求。

Abstract: Pre-trained diffusion models excel at generating high-quality images but remain inherently limited by their native training resolution. Recent training-free approaches have attempted to overcome this constraint by introducing interventions during the denoising process; however, these methods incur substantial computational overhead, often requiring more than five minutes to produce a single 4K image. In this paper, we present PixelRush, the first tuning-free framework for practical high-resolution text-to-image generation. Our method builds upon the established patch-based inference paradigm but eliminates the need for multiple inversion and regeneration cycles. Instead, PixelRush enables efficient patch-based denoising within a low-step regime. To address artifacts introduced by patch blending in few-step generation, we propose a seamless blending strategy. Furthermore, we mitigate over-smoothing effects through a noise injection mechanism. PixelRush delivers exceptional efficiency, generating 4K images in approximately 20 seconds representing a 10$\times$ to 35$\times$ speedup over state-of-the-art methods while maintaining superior visual fidelity. Extensive experiments validate both the performance gains and the quality of outputs achieved by our approach.

</details>


### [31] [Bootstrapping MLLM for Weakly-Supervised Class-Agnostic Object Counting](https://arxiv.org/abs/2602.12774)
*Xiaowen Zhang,Zijie Yue,Yong Luo,Cairong Zhao,Qijun Chen,Miaojing Shi*

Main category: cs.CV

TL;DR: WS-COC 提出了一种多模态语言模型驱动的弱监督多类别无标注物计数框架。通过对话策略、比较和排名策略以及局部与全局计数增强策略，实现了在多个数据集上的优越性能。


<details>
  <summary>Details</summary>
Motivation: 当前弱监督方法多限于单一类别计数，而全监督方法需要昂贵的对象级别注释。因此，提出 WS-COC 旨在提供一种能够处理多类别无标注物计数的方法，同时减少注释成本。

Method: WS-COC 方法主要包括三个策略：对话调优策略用于指导模型逐步确定物体计数范围；比较和排名优化策略针对多个图像的对象计数进行相对排名学习；局部与全局计数增强策略汇集和融合局部和全局预测，提高密集场景中的计数性能。

Result: WS-COC 在多个数据集上的实验结果表明，该方法在性能上与许多全监督方法相当甚至超越，同时大幅度降低了标注成本。

Conclusion: WS-COC 是首个能够实现多类别弱监督无标注物体计数的框架，显著减少了注释需求。

Abstract: Object counting is a fundamental task in computer vision, with broad applicability in many real-world scenarios. Fully-supervised counting methods require costly point-level annotations per object. Few weakly-supervised methods leverage only image-level object counts as supervision and achieve fairly promising results. They are, however, often limited to counting a single category, e.g. person. In this paper, we propose WS-COC, the first MLLM-driven weakly-supervised framework for class-agnostic object counting. Instead of directly fine-tuning MLLMs to predict object counts, which can be challenging due to the modality gap, we incorporate three simple yet effective strategies to bootstrap the counting paradigm in both training and testing: First, a divide-and-discern dialogue tuning strategy is proposed to guide the MLLM to determine whether the object count falls within a specific range and progressively break down the range through multi-round dialogue. Second, a compare-and-rank count optimization strategy is introduced to train the MLLM to optimize the relative ranking of multiple images according to their object counts. Third, a global-and-local counting enhancement strategy aggregates and fuses local and global count predictions to improve counting performance in dense scenes. Extensive experiments on FSC-147, CARPK, PUCPR+, and ShanghaiTech show that WS-COC matches or even surpasses many state-of-art fully-supervised methods while significantly reducing annotation costs. Code is available at https://github.com/viscom-tongji/WS-COC.

</details>


### [32] [RADAR: Revealing Asymmetric Development of Abilities in MLLM Pre-training](https://arxiv.org/abs/2602.12892)
*Yunshuang Nie,Bingqian Lin,Minzhe Niu,Kun Xiang,Jianhua Han,Guowei Huang,Xingyue Quan,Hang Xu,Bokui Chen,Xiaodan Liang*

Main category: cs.CV

TL;DR: RADAR 提出了一种高效的针对性评估框架，旨在揭示预训练多模态大规模语言模型在感知和推理能力上的不对称发展。该框架包括 Soft Discrimination Score 和 Multi-Modal Mixture Benchmark 两个关键组件，旨在不依赖额外微调的情况下监测模型能力发展，并提供了一个大规模的零样本基准来全面评估模型的感知和推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前对预训练多模态大规模语言模型（MLLMs）的评估缺乏有效的框架，主要依赖于监督微调后的测试，这增加额外的培训和自回归解码开销，而且现有的评价基准通常规模有限且与预训练目标不一致，因此研究者提出了 RADAR 作为新的评估框架。

Method: RADAR 框架包含两个关键组件：Soft Discrimination Score 用于通过量化模型对正确答案与干扰项的偏好来逐步跟踪模型能力的发展；Multi-Modal Mixture Benchmark 则是一个包含 15K+ 样本的新基准，旨在零样本下全面评估预训练 MLLMs 的感知和推理能力，覆盖了权威基准数据集和精心构建的新数据集，以扩大评估的范围并解决现有基准的不足。

Result: 基于 RADAR 框架的研究揭示了预训练 MLLMs 在感知和推理能力上的不对称发展变化，这些发现有助于理解预训练能力瓶颈的具体情况，并为改进 MLLMs 提供有针对性的干预措施。

Conclusion: RADAR 的提出强调了需要从分解的角度来看预训练能力瓶颈，并指出了未来可能针对这些瓶颈进行优化的方向。

Abstract: Pre-trained Multi-modal Large Language Models (MLLMs) provide a knowledge-rich foundation for post-training by leveraging their inherent perception and reasoning capabilities to solve complex tasks. However, the lack of an efficient evaluation framework impedes the diagnosis of their performance bottlenecks. Current evaluation primarily relies on testing after supervised fine-tuning, which introduces laborious additional training and autoregressive decoding costs. Meanwhile, common pre-training metrics cannot quantify a model's perception and reasoning abilities in a disentangled manner. Furthermore, existing evaluation benchmarks are typically limited in scale or misaligned with pre-training objectives. Thus, we propose RADAR, an efficient ability-centric evaluation framework for Revealing Asymmetric Development of Abilities in MLLM pRe-training. RADAR involves two key components: (1) Soft Discrimination Score, a novel metric for robustly tracking ability development without fine-tuning, based on quantifying nuanced gradations of the model preference for the correct answer over distractors; and (2) Multi-Modal Mixture Benchmark, a new 15K+ sample benchmark for comprehensively evaluating pre-trained MLLMs' perception and reasoning abilities in a 0-shot manner, where we unify authoritative benchmark datasets and carefully collect new datasets, extending the evaluation scope and addressing the critical gaps in current benchmarks. With RADAR, we comprehensively reveal the asymmetric development of perceptual and reasoning capabilities in pretrained MLLMs across diverse factors, including data volume, model size, and pretraining strategy. Our RADAR underscores the need for a decomposed perspective on pre-training ability bottlenecks, informing targeted interventions to advance MLLMs efficiently. Our code is publicly available at https://github.com/Nieysh/RADAR.

</details>


### [33] [Thinking Like a Radiologist: A Dataset for Anatomy-Guided Interleaved Vision Language Reasoning in Chest X-ray Interpretation](https://arxiv.org/abs/2602.12843)
*Yichen Zhao,Zelin Peng,Piao Yang,Xiaokang Yang,Wei Shen*

Main category: cs.CV

TL;DR: MMRad-IVL-22K是一个大规模数据集，旨在支持交互式的视觉语言推理在胸部X光解读中。它通过构建包含反复循环的推理和视觉检测流程的数据集，改善了现有模型的性能。


<details>
  <summary>Details</summary>
Motivation: 当前的大型视觉语言模型（LVLMs）主要依靠文本链条的方式来处理，这种方法倾向于产生幻觉。本文受到放射学诊断中视觉检查和语言推理交替进行的启发，提出了MMRad-IVL-22K这个大规模数据集。

Method: 该数据集包含了21,994个诊断踪迹，涵盖了35个解剖区域，让模型能够系统地扫描和分析这些区域。数据集中还包括视觉提示来补充文本描述，这与文本推理形成了互补。

Result: 实验表明，基于多模态链条生成报告在临床准确性和报告质量上显著优于仅基于文本链条的报告生成策略（例如，RadGraph度量的提升达到了6%）。此外，MMRad-IVL-22K的训练模型在各种开源视觉语言模型上的表现超越了一般用途模型和专门针对医学的模型。

Conclusion: MMRad-IVL-22K的引入展示了高质量的互动视觉语言推理能力对于可靠医疗AI的重要性，并且通过训练该数据集，可以提高医疗视觉语言模型的推理一致性和报告质量。

Abstract: Radiological diagnosis is a perceptual process in which careful visual inspection and language reasoning are repeatedly interleaved. Most medical large vision language models (LVLMs) perform visual inspection only once and then rely on text-only chain-of-thought (CoT) reasoning, which operates purely in the linguistic space and is prone to hallucination. Recent methods attempt to mitigate this issue by introducing visually related coordinates, such as bounding boxes. However, these remain a pseudo-visual solution: coordinates are still text and fail to preserve rich visual details like texture and density. Motivated by the interleaved nature of radiological diagnosis, we introduce MMRad-IVL-22K, the first large-scale dataset designed for natively interleaved visual language reasoning in chest X-ray interpretation. MMRad-IVL-22K reflects a repeated cycle of reasoning and visual inspection workflow of radiologists, in which visual rationales complement textual descriptions and ground each step of the reasoning process. MMRad-IVL-22K comprises 21,994 diagnostic traces, enabling systematic scanning across 35 anatomical regions. Experimental results on advanced closed-source LVLMs demonstrate that report generation guided by multimodal CoT significantly outperforms that guided by text-only CoT in clinical accuracy and report quality (e.g., 6\% increase in the RadGraph metric), confirming that high-fidelity interleaved vision language evidence is a non-substitutable component of reliable medical AI. Furthermore, benchmarking across seven state-of-the-art open-source LVLMs demonstrates that models fine-tuned on MMRad-IVL-22K achieve superior reasoning consistency and report quality compared with both general-purpose and medical-specific LVLMs. The project page is available at https://github.com/qiuzyc/thinking_like_a_radiologist.

</details>


### [34] [Robustness of Object Detection of Autonomous Vehicles in Adverse Weather Conditions](https://arxiv.org/abs/2602.12902)
*Fox Pettersen,Hong Zhu*

Main category: cs.CV

TL;DR: 该研究提出了一种通过数据增强生成合成数据来评估自动驾驶车辆在恶劣天气条件下的目标检测模型鲁棒性的方法。实验表明，相较于YOLO模型系列，Faster R-CNN在不同恶劣环境条件下的鲁棒性最高，但过拟合也会导致鲁棒性下降。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶技术的普及，需要确保这些车辆在各种环境条件下的安全操作阈值，特别是在恶劣天气等不良操作条件下，以保障公共安全。

Method: 通过使用数据增强操作（如模拟不同程度的雾、雨、雪等天气以及不同的照明条件）来生成合成数据，以检验目标检测模型在这些恶劣条件下的最临界阈值。实验中使用了YOLOv5s、YOLOv11s、Faster R-CNN和Detectron2四种目标检测模型。

Result: 实验结果显示，Faster R-CNN在各种恶劣条件下的鲁棒性表现最优，其在所有七个不良条件下的平均首次失败系数（AFFC）为71.9%。而YOLO系列模型的AFFC值约为43%。此外，通过使用合成数据增强训练模型，在一定程度上可以提高对恶劣条件的鲁棒性，但过度训练可能导致鲁棒性的下降。

Conclusion: 研究证明了该方法在评估目标检测模型鲁棒性方面的可行性、有效性和高效性。同时也指出了过度依赖于特定条件数据增强带来的潜在问题。

Abstract: As self-driving technology advances toward widespread adoption, determining safe operational thresholds across varying environmental conditions becomes critical for public safety. This paper proposes a method for evaluating the robustness of object detection ML models in autonomous vehicles under adverse weather conditions. It employs data augmentation operators to generate synthetic data that simulates different severance degrees of the adverse operation conditions at progressive intensity levels to find the lowest intensity of the adverse conditions at which the object detection model fails. The robustness of the object detection model is measured by the average first failure coefficients (AFFC) over the input images in the benchmark. The paper reports an experiment with four object detection models: YOLOv5s, YOLOv11s, Faster R-CNN, and Detectron2, utilising seven data augmentation operators that simulate weather conditions fog, rain, and snow, and lighting conditions of dark, bright, flaring, and shadow. The experiment data show that the method is feasible, effective, and efficient to evaluate and compare the robustness of object detection models in various adverse operation conditions. In particular, the Faster R-CNN model achieved the highest robustness with an overall average AFFC of 71.9% over all seven adverse conditions, while YOLO variants showed the AFFC values of 43%. The method is also applied to assess the impact of model training that targets adverse operation conditions using synthetic data on model robustness. It is observed that such training can improve robustness in adverse conditions but may suffer from diminishing returns and forgetting phenomena (i.e., decline in robustness) if overtrained.

</details>


### [35] [RoadscapesQA: A Multitask, Multimodal Dataset for Visual Question Answering on Indian Roads](https://arxiv.org/abs/2602.12877)
*Vijayasri Iyer,Maahin Rathinagiriswaran,Jyothikamalesh S*

Main category: cs.CV

TL;DR: Roadscapes 数据集是一个多任务、多模态的数据集，包含来自印度多样化的驾驶环境的图像，附带手动验证的边界框，旨在推动对非结构化环境的视觉场景理解研究。


<details>
  <summary>Details</summary>
Motivation: 为了使自主驾驶系统能够有效理解视觉环境并据此做出决策，需要理解道路场景。Roadscapes 数据集的创建是为了应对这一挑战。

Method: Roadscapes 通过在其上标注场景属性来收集和注释数据。这些属性是通过规则启发式方法推断出来的，并用于生成用于任务如目标定位、推理和场景理解的问题-答案对。

Result: 该数据集包括了来自印度各种驾驶环境的场景，涵盖了从农村到城市的多种环境。同时，还为图像问题-答案任务提供了初始基线。

Conclusion: Roadscapes 数据集为视觉场景理解研究提供了一个重要的资源，并为非结构化环境中的图像问题-答案任务建立了一定基础。

Abstract: Understanding road scenes is essential for autonomous driving, as it enables systems to interpret visual surroundings to aid in effective decision-making. We present Roadscapes, a multitask multimodal dataset consisting of upto 9,000 images captured in diverse Indian driving environments, accompanied by manually verified bounding boxes. To facilitate scalable scene understanding, we employ rule-based heuristics to infer various scene attributes, which are subsequently used to generate question-answer (QA) pairs for tasks such as object grounding, reasoning, and scene understanding. The dataset includes a variety of scenes from urban and rural India, encompassing highways, service roads, village paths, and congested city streets, captured in both daytime and nighttime settings. Roadscapes has been curated to advance research on visual scene understanding in unstructured environments. In this paper, we describe the data collection and annotation process, present key dataset statistics, and provide initial baselines for image QA tasks using vision-language models.

</details>


### [36] [EPRBench: A High-Quality Benchmark Dataset for Event Stream Based Visual Place Recognition](https://arxiv.org/abs/2602.12919)
*Xiao Wang,Xingxing Xiong,Jinfeng Gao,Xufeng Lou,Bo Jiang,Si-bao Chen,Yaowei Wang,Yonghong Tian*

Main category: cs.CV

TL;DR: 该研究旨在为事件流基于的视觉场地识别（VPR）建立一个高质量基准，EPRBench，包含10K事件序列和65K事件帧，并提出了一种基于大型语言模型（LLM）多模态融合框架以实现准确且可解释的场地识别。


<details>
  <summary>Details</summary>
Motivation: 由于当前缺乏针对事件流基于的VPR的专业数据集，研究团队引入了一个新的基准EPRBench，以填补此空白。此外，为了促进基于LLM的VPR研究，数据集还提供了LLM生成的场景描述，增强了解释性。

Method: 研究团队通过使用手持和车载设备采集事件序列和事件帧，收集了10K事件序列和65K事件帧。然后利用大型语言模型生成场景描述，并通过人工注释进行了优化。为了系统地评估VPR算法，团队实现了15种最新VPR算法，并进行了基准测试。此外，提出了一种新的多模态融合框架，利用LLM从原始事件流生成文本场景描述，引导空间注意词选择、跨模态特征融合和多尺度表示学习。

Result: 研究团队建立了EPRBench，并对15种最先进的VPR算法进行了基准测试。提出的多模态融合框架不仅实现了高精度的场地识别，还产生了解释性的推理过程，增强了模型的透明度和可解释性。

Conclusion: 该研究为事件流基于的VPR领域提供了一个全面的基准数据集和一种新的多模态融合框架，推动了后续研究的发展。

Abstract: Event stream-based Visual Place Recognition (VPR) is an emerging research direction that offers a compelling solution to the instability of conventional visible-light cameras under challenging conditions such as low illumination, overexposure, and high-speed motion. Recognizing the current scarcity of dedicated datasets in this domain, we introduce EPRBench, a high-quality benchmark specifically designed for event stream-based VPR. EPRBench comprises 10K event sequences and 65K event frames, collected using both handheld and vehicle-mounted setups to comprehensively capture real-world challenges across diverse viewpoints, weather conditions, and lighting scenarios. To support semantic-aware and language-integrated VPR research, we provide LLM-generated scene descriptions, subsequently refined through human annotation, establishing a solid foundation for integrating LLMs into event-based perception pipelines. To facilitate systematic evaluation, we implement and benchmark 15 state-of-the-art VPR algorithms on EPRBench, offering a strong baseline for future algorithmic comparisons. Furthermore, we propose a novel multi-modal fusion paradigm for VPR: leveraging LLMs to generate textual scene descriptions from raw event streams, which then guide spatially attentive token selection, cross-modal feature fusion, and multi-scale representation learning. This framework not only achieves highly accurate place recognition but also produces interpretable reasoning processes alongside its predictions, significantly enhancing model transparency and explainability. The dataset and source code will be released on https://github.com/Event-AHU/Neuromorphic_ReID

</details>


### [37] [Deep-Learning Atlas Registration for Melanoma Brain Metastases: Preserving Pathology While Enabling Cohort-Level Analyses](https://arxiv.org/abs/2602.12933)
*Nanna E. Wielenberg,Ilinca Popp,Oliver Blanck,Lucas Zander,Jan C. Peeken,Stephanie E. Combs,Anca-Ligia Grosu,Dimos Baltas,Tobias Fechter*

Main category: cs.CV

TL;DR: 本文提出了一种无需病变遮罩或预处理的全可微、基于深度学习的变形配准框架，实现了对脑转移瘤患者的标准化病变映射，验证了脑转移瘤在脑结构和功能图谱上的特定分布。


<details>
  <summary>Details</summary>
Motivation: 脑转移瘤是一种常见的且具有空间异质性的病变，传统的大样本分析受限于解剖变异和不同的MRI协议。因此，需要一种无需病变遮罩或特殊预处理的方法来标准化这些病变的映射。

Method: 通过结合基于变形前后的距离变换解剖标签的相似度度量和体积保持正则项，该框架能够处理由于转移瘤导致的解剖对应关系缺失问题。特别是在应用到209名不同中心的脑转移瘤患者后，证明了该方法的高注册精度。

Result: 该方法在多个评价指标（Dice系数，Hausdorff距离和平均对称表面距离）上表现优异，特别是在转移瘤体积矫正后，动脉供氧区并无增加的转移概率。此外，实验验证了脑转移瘤在大脑皮层和尾状核的高过代表，以及在白质中的低代表。

Conclusion: 该框架实现了脑肿瘤MRI病变的稳健注册，特别是在没有病变遮罩的情况下，支持多中心可重复分析。该方法还被证明可以扩展到其他脑肿瘤和神经病理性状的研究。

Abstract: Melanoma brain metastases (MBM) are common and spatially heterogeneous lesions, complicating cohort-level analyses due to anatomical variability and differing MRI protocols. We propose a fully differentiable, deep-learning-based deformable registration framework that aligns individual pathological brains to a common atlas while preserving metastatic tissue without requiring lesion masks or preprocessing.
  Missing anatomical correspondences caused by metastases are handled through a forward-model similarity metric based on distance-transformed anatomical labels, combined with a volume-preserving regularization term to ensure deformation plausibility. Registration performance was evaluated using Dice coefficient (DSC), Hausdorff distance (HD), average symmetric surface distance (ASSD), and Jacobian-based measures. The method was applied to 209 MBM patients from three centres, enabling standardized mapping of metastases to anatomical, arterial, and perfusion atlases.
  The framework achieved high registration accuracy across datasets (DSC 0.89-0.92, HD 6.79-7.60 mm, ASSD 0.63-0.77 mm) while preserving metastatic volumes. Spatial analysis demonstrated significant over-representation of MBM in the cerebral cortex and putamen, under-representation in white matter, and consistent localization near the gray-white matter junction. No arterial territory showed increased metastasis frequency after volume correction.
  This approach enables robust atlas registration of pathological brain MRI without lesion masks and supports reproducible multi-centre analyses. Applied to MBM, it confirms and refines known spatial predilections, particularly preferential seeding near the gray-white matter junction and cortical regions. The publicly available implementation facilitates reproducible research and extension to other brain tumours and neurological pathologies.

</details>


### [38] [Detecting Object Tracking Failure via Sequential Hypothesis Testing](https://arxiv.org/abs/2602.12983)
*Alejandro Monroy Muñoz,Rajeev Verma,Alexander Timans*

Main category: cs.CV

TL;DR: 该研究将实时物体跟踪视为一个序列假设检验过程，旨在高效地识别跟踪失败，同时降低错误警报，使系统能够自主做出决策。


<details>
  <summary>Details</summary>
Motivation: 现有跟踪系统缺乏正式的安全保证，无法判定跟踪何时可靠何时可能失败。本研究旨在提出一种序列假设检验方法（e过程），以获取这样的保证。

Method: 该方法基于最近在序列假设检验领域的进步，通过逐时间积累证据来判断跟踪是否失效，同时保证误警报在设定的水平下。

Result: 研究提出了监督和非监督两种变体，适用于两种已建立的跟踪模型，在四个视频基准测试中证明了其有效性。

Conclusion: 该研究提供了一种基于统计保证的机制，能够高效地将安全保证融入到实时跟踪系统中。

Abstract: Real-time online object tracking in videos constitutes a core task in computer vision, with wide-ranging applications including video surveillance, motion capture, and robotics. Deployed tracking systems usually lack formal safety assurances to convey when tracking is reliable and when it may fail, at best relying on heuristic measures of model confidence to raise alerts. To obtain such assurances we propose interpreting object tracking as a sequential hypothesis test, wherein evidence for or against tracking failures is gradually accumulated over time. Leveraging recent advancements in the field, our sequential test (formalized as an e-process) quickly identifies when tracking failures set in whilst provably containing false alerts at a desired rate, and thus limiting potentially costly re-calibration or intervention steps. The approach is computationally light-weight, requires no extra training or fine-tuning, and is in principle model-agnostic. We propose both supervised and unsupervised variants by leveraging either ground-truth or solely internal tracking information, and demonstrate its effectiveness for two established tracking models across four video benchmarks. As such, sequential testing can offer a statistically grounded and efficient mechanism to incorporate safety assurances into real-time tracking systems.

</details>


### [39] [Reliable Thinking with Images](https://arxiv.org/abs/2602.12916)
*Haobin Li,Yutong Yang,Yijie Lin,Dai Xiang,Mouxing Yang,Xi Peng*

Main category: cs.CV

TL;DR: 本文介绍了Thinking with Images (TWI) 的一种新的挑战——Noisy Thinking（NT），并提出了一种魯棒的思考方式——Reliable Thinking with Images（RTWI），旨在提高多模态大语言模型（MLLMs）的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的TWI方法基于视觉线索无误的假设，但在实际场景中容易出现错误，因此提出了Noisy Thinking (NT) 问题，旨在改进TWI方法的鲁棒性和准确性。

Method: RTWI方法融合了视觉线索和文本推理过程，通过统一的文本中心方式评估视觉线索和文本CoT的可靠性，并采用鲁棒过滤和投票模块，防止错误传播影响最终答案。

Result: 在七个基准测试上的实验验证了RTWI方法的有效性。

Conclusion: RTWI方法为解决NT问题提供了一种有效的解决方案，提高了MLLMs的推理能力，减轻了错误积累对性能的影响。

Abstract: As a multimodal extension of Chain-of-Thought (CoT), Thinking with Images (TWI) has recently emerged as a promising avenue to enhance the reasoning capability of Multi-modal Large Language Models (MLLMs), which generates interleaved CoT by incorporating visual cues into the textual reasoning process. However, the success of existing TWI methods heavily relies on the assumption that interleaved image-text CoTs are faultless, which is easily violated in real-world scenarios due to the complexity of multimodal understanding. In this paper, we reveal and study a highly-practical yet under-explored problem in TWI, termed Noisy Thinking (NT). Specifically, NT refers to the imperfect visual cues mining and answer reasoning process. As the saying goes, ``One mistake leads to another'', erroneous interleaved CoT would cause error accumulation, thus significantly degrading the performance of MLLMs. To solve the NT problem, we propose a novel method dubbed Reliable Thinking with Images (RTWI). In brief, RTWI estimates the reliability of visual cues and textual CoT in a unified text-centric manner and accordingly employs robust filtering and voting modules to prevent NT from contaminating the final answer. Extensive experiments on seven benchmarks verify the effectiveness of RTWI against NT.

</details>


### [40] [Human-Aligned MLLM Judges for Fine-Grained Image Editing Evaluation: A Benchmark, Framework, and Analysis](https://arxiv.org/abs/2602.13028)
*Runzhou Liu,Hailey Weingord,Sejal Mittal,Prakhar Dungarwal,Anusha Nandula,Bo Ni,Samyadeep Basu,Hongjie Chen,Nesreen K. Ahmed,Li Li,Jiayi Zhang,Koustava Goswami,Subhojyoti Mukherjee,Branislav Kveton,Puneet Mathur,Franck Dernoncourt,Yue Zhao,Yu Wang,Ryan A. Rossi,Zhengzhong Tu,Hongru Du*

Main category: cs.CV

TL;DR: 论文引入了一种细粒度的多模态大型语言模型（MLLM）作为评判框架，该框架将常见评估概念分解为十二个可解释因素，涵盖图像保留、编辑质量和指令保真度。通过人类验证的基准测试和广泛的人类研究，展示了MLLM评判者在细粒度上的表现与人类评估高度一致，提供了比传统图像编辑指标更直观、更有信息量的评估。


<details>
  <summary>Details</summary>
Motivation: 传统的单一粒度和可解释性不足的指标无法全面捕捉人类感知和意图中的重要方面，因此需要一种细粒度的、可解释性强的评估框架。

Method: 论文提出了一种基于多模态大型语言模型（MLLM）的框架，细分为12个因素进行评估，结合了人工评估、MLLM评估和传统指标。

Result: 广泛的人类研究表明，提出的MLLM评判者在细粒度上的表现与人类评估高度一致，并且提供了比传统图像编辑指标更直观和信息丰富的评估。这种方法为研究、比较和改进图像编辑方法奠定了基础。

Conclusion: 论文提出的新方法比传统的评估指标更有效，能够更好地评估图像编辑的质量，并为未来的图像编辑评价提供了可靠和可扩展的标准。

Abstract: Evaluating image editing models remains challenging due to the coarse granularity and limited interpretability of traditional metrics, which often fail to capture aspects important to human perception and intent. Such metrics frequently reward visually plausible outputs while overlooking controllability, edit localization, and faithfulness to user instructions. In this work, we introduce a fine-grained Multimodal Large Language Model (MLLM)-as-a-Judge framework for image editing that decomposes common evaluation notions into twelve fine-grained interpretable factors spanning image preservation, edit quality, and instruction fidelity. Building on this formulation, we present a new human-validated benchmark that integrates human judgments, MLLM-based evaluations, model outputs, and traditional metrics across diverse image editing tasks. Through extensive human studies, we show that the proposed MLLM judges align closely with human evaluations at a fine granularity, supporting their use as reliable and scalable evaluators. We further demonstrate that traditional image editing metrics are often poor proxies for these factors, failing to distinguish over-edited or semantically imprecise outputs, whereas our judges provide more intuitive and informative assessments in both offline and online settings. Together, this work introduces a benchmark, a principled factorization, and empirical evidence positioning fine-grained MLLM judges as a practical foundation for studying, comparing, and improving image editing approaches.

</details>


### [41] [Beyond Benchmarks of IUGC: Rethinking Requirements of Deep Learning Methods for Intrapartum Ultrasound Biometry from Fetal Ultrasound Videos](https://arxiv.org/abs/2602.12922)
*Jieyun Bai,Zihao Zhou,Yitong Tang,Jie Gan,Zhuonan Liang,Jianan Fan,Lisa B. Mcguire,Jillian L. Clarke,Weidong Cai,Jacaueline Spurway,Yubo Tang,Shiye Wang,Wenda Shen,Wangwang Yu,Yihao Li,Philippe Zhang,Weili Jiang,Yongjie Li,Salem Muhsin Ali Binqahal Al Nasim,Arsen Abzhanov,Numan Saeed,Mohammad Yaqub,Zunhui Xian,Hongxing Lin,Libin Lan,Jayroop Ramesh,Valentin Bacher,Mark Eid,Hoda Kalabizadeh,Christian Rupprecht,Ana I. L. Namburete,Pak-Hei Yeung,Madeleine K. Wyburd,Nicola K. Dinsdale,Assanali Serikbey,Jiankai Li,Sung-Liang Chen,Zicheng Hu,Nana Liu,Yian Deng,Wei Hu,Cong Tan,Wenfeng Zhang,Mai Tuyet Nhi,Gregor Koehler,Rapheal Stock,Klaus Maier-Hein,Marawan Elbatel,Xiaomeng Li,Saad Slimani,Victor M. Campello,Benard Ohene-Botwe,Isaac Khobo,Yuxin Huang,Zhenyan Han,Hongying Hou,Di Qiu,Zheng Zheng,Gongning Luo,Dong Ni,Yaosheng Lu,Karim Lekadir,Shuo Li*

Main category: cs.CV

TL;DR: 该研究介绍了Intrapartum Ultrasound Grand Challenge，旨在解决资源限制地区的产程超声监测不足的问题，通过开发综合测量框架和提供最大的多中心产程超声视频数据集来实现。分析了八支参赛团队的方法，并探讨了现有的瓶颈和未来的研究挑战。


<details>
  <summary>Details</summary>
Motivation: 鉴于低中收入国家产程期间死亡率高，且资源匮乏地区难以实施常规超声监测，提出Intrapartum Ultrasound Grand Challenge旨在通过技术创新来改善产程超声监测。

Method: 研究设计了一个包括标准平面分类、胎儿头骨盆对线分割与测量的多任务自动测量框架，并发布了包含774个视频（68,106帧）的多中心数据集。对八支参赛团队的方法进行了全面的分析，从预处理、数据增强、学习策略、模型结构和后处理五个方面出发。

Result: 研究结果表明尽管参赛团队达到了令人鼓舞的效果，但是在实际临床应用中仍然遇到了许多关键瓶颈，需要进一步深入研究。所有基准解决方案及完整数据集已公开发布，以促进可再现研究和自动产程超声测量技术的进一步发展。

Conclusion: 该挑战不仅促进了自动产程超声测量技术的进步，也为后续研究指明了方向，但仍需更多的研究投入以实现临床中的大规模应用。

Abstract: A substantial proportion (45\%) of maternal deaths, neonatal deaths, and stillbirths occur during the intrapartum phase, with a particularly high burden in low- and middle-income countries. Intrapartum biometry plays a critical role in monitoring labor progression; however, the routine use of ultrasound in resource-limited settings is hindered by a shortage of trained sonographers. To address this challenge, the Intrapartum Ultrasound Grand Challenge (IUGC), co-hosted with MICCAI 2024, was launched. The IUGC introduces a clinically oriented multi-task automatic measurement framework that integrates standard plane classification, fetal head-pubic symphysis segmentation, and biometry, enabling algorithms to exploit complementary task information for more accurate estimation. Furthermore, the challenge releases the largest multi-center intrapartum ultrasound video dataset to date, comprising 774 videos (68,106 frames) collected from three hospitals, providing a robust foundation for model training and evaluation. In this study, we present a comprehensive overview of the challenge design, review the submissions from eight participating teams, and analyze their methods from five perspectives: preprocessing, data augmentation, learning strategy, model architecture, and post-processing. In addition, we perform a systematic analysis of the benchmark results to identify key bottlenecks, explore potential solutions, and highlight open challenges for future research. Although encouraging performance has been achieved, our findings indicate that the field remains at an early stage, and further in-depth investigation is required before large-scale clinical deployment. All benchmark solutions and the complete dataset have been publicly released to facilitate reproducible research and promote continued advances in automatic intrapartum ultrasound biometry.

</details>


### [42] [CoPE-VideoLM: Codec Primitives For Efficient Video Language Models](https://arxiv.org/abs/2602.13191)
*Sayan Deb Sarkar,Rémi Pautrat,Ondrej Miksik,Marc Pollefeys,Iro Armeni,Mahdi Rad,Mihai Dusmanu*

Main category: cs.CV

TL;DR: 本文提出了一种利用视频编解码器基础（特别是运动矢量和残差）的方法，以减少计算开销并提高视频理解模型的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 当前视频语言模型（VideoLMs）方法受限于最大上下文窗口约束，通过关键帧采样来适应，这可能导致宏层面事件和微观细节的遗漏。另外，逐帧处理完整图像和其标记会带来巨大的计算负担。

Method: 本文提出了一种新的轻量级变压器编码器，利用视频编解码器基础并通过预训练策略在端到端微调中加速收敛。该模型减少了第一个词的到来时间高达86%，且标记使用量减少了高达93%。

Result: 该方法在14个不同的视频理解基准上的性能表现良好或优于标准VideoLMs，涵盖了从一般问题回答到时间推理、长时间理解以及空间场景理解等多个方面。

Conclusion: 与现有的视频语言模型相比，本文提出的基于视频编解码器基础的方法能够显著提高模型的效率和性能。

Abstract: Video Language Models (VideoLMs) empower AI systems to understand temporal dynamics in videos. To fit to the maximum context window constraint, current methods use keyframe sampling which can miss both macro-level events and micro-level details due to the sparse temporal coverage. Furthermore, processing full images and their tokens for each frame incurs substantial computational overhead. To address these limitations, we propose to leverage video codec primitives (specifically motion vectors and residuals) which natively encode video redundancy and sparsity without requiring expensive full-image encoding for most frames. To this end, we introduce lightweight transformer-based encoders that aggregate codec primitives and align their representations with image encoder embeddings through a pre-training strategy that accelerates convergence during end-to-end fine-tuning. Our approach reduces the time-to-first-token by up to $86\%$ and token usage by up to $93\%$ compared to standard VideoLMs. Moreover, by varying the keyframe and codec primitive densities we are able to maintain or exceed performance on $14$ diverse video understanding benchmarks spanning general question answering, temporal reasoning, long-form understanding, and spatial scene understanding.

</details>


### [43] [Unleashing MLLMs on the Edge: A Unified Framework for Cross-Modal ReID via Adaptive SVD Distillation](https://arxiv.org/abs/2602.12936)
*Hongbo Jiang,Jie Li,Xinqi Cai,Tianyu Xie,Yunhang Shen,Pingyang Dai,Liujuan Cao*

Main category: cs.CV

TL;DR: 提出了一个称为MLLMEmbed-ReID的统一框架，该框架通过将强大的多模态大语言模型适应为边缘设备上的小型模型，解决了跨模态再识别在云边部署中的挑战。


<details>
  <summary>Details</summary>
Motivation: 当前云边部署的跨模态再识别面临的主要挑战包括保持特殊化云模型的生态系统以及现有方法未能有效利用多模态大语言模型的潜在统一性。

Method: 该方法包括两个主要步骤：1. 通过基于指令的提示将基础MLLM适应为先进的云模型，并利用层次低秩适应微调策略进行高效训练；2. 提出了一个新颖的知识蒸馏策略，以保持教师特征空间中的低秩性质，并通过主成分映射损失选择性地传递关键信息，同时通过特征关系损失保留关系结构。

Result: 该方法在多个跨模态再识别基准测试集上实现了轻量级边缘模型的最先进性能，其基于云的对应版本在所有跨模态再识别基准测试中表现出色。这项工作提供了一个全面有效的解决方案，以便在资源受限设备上部署统一的MMLM级别智能。

Conclusion: 研究表明，MLLMEmbed-ReID框架能够克服云-边部署跨模态再识别的现有挑战，并实现了在资源紧张设备上的有效部署。

Abstract: Practical cloud-edge deployment of Cross-Modal Re-identification (CM-ReID) faces challenges due to maintaining a fragmented ecosystem of specialized cloud models for diverse modalities. While Multi-Modal Large Language Models (MLLMs) offer strong unification potential, existing approaches fail to adapt them into a single end-to-end backbone and lack effective knowledge distillation strategies for edge deployment. To address these limitations, we propose MLLMEmbed-ReID, a unified framework based on a powerful cloud-edge architecture. First, we adapt a foundational MLLM into a state-of-the-art cloud model. We leverage instruction-based prompting to guide the MLLM in generating a unified embedding space across RGB, infrared, sketch, and text modalities. This model is then trained efficiently with a hierarchical Low-Rank Adaptation finetuning (LoRA-SFT) strategy, optimized under a holistic cross-modal alignment objective. Second, to deploy its knowledge onto an edge-native student, we introduce a novel distillation strategy motivated by the low-rank property in the teacher's feature space. To prioritize essential information, this method employs a Principal Component Mapping loss, while relational structures are preserved via a Feature Relation loss. Our lightweight edge-based model achieves state-of-the-art performance on multiple visual CM-ReID benchmarks, while its cloud-based counterpart excels across all CM-ReID benchmarks. The MLLMEmbed-ReID framework thus presents a complete and effective solution for deploying unified MLLM-level intelligence on resource-constrained devices. The code and models will be open-sourced soon.

</details>


### [44] [Training-Free Acceleration for Document Parsing Vision-Language Model with Hierarchical Speculative Decoding](https://arxiv.org/abs/2602.12957)
*Wenhui Liao,Hongliang Li,Pengyu Xie,Xinyu Cai,Yufan Shen,Yi Xin,Qi Qin,Shenglong Ye,Tianbin Li,Ming Hu,Junjun He,Yihao Liu,Wenhai Wang,Min Dou,Bin Fu,Botian Shi,Yu Qiao,Lianwen Jin*

Main category: cs.CV

TL;DR: 提出了一种无训练的高效加速方法，通过一个轻量级文档解析管道作为草稿模型预测未来的 token，而更准确的 VLM 并行验证这些预测。利用文档的布局结构特性，将每页划分为独立区域，每个区域并行解码，最终根据自然阅读顺序组装成结果。


<details>
  <summary>Details</summary>
Motivation: 面对 VLM 基于端到端的方法在处理长文档时因自回归生成长序列而导致的高推理延迟问题。

Method: 采用投机解码方法，利用轻量级文档解析管道作为草稿模型，预测未来的 token，同时更准确的 VLM 并行验证这些预测。利用文档的布局结构特性，将每页划分为独立区域，采取相同的草稿-验证策略并行解码，最终按照自然阅读顺序组装结果。

Result: 在通用 OmniDocBench 基准测试中，方法对 dots.ocr 模型实现了 2.42 倍的无损加速，并在长文档解析任务中达到最高 4.89 倍的加速。

Conclusion: 该方法提供了一种高效的无训练加速方案，显著降低了文档解析的推理延迟。

Abstract: Document parsing is a fundamental task in multimodal understanding, supporting a wide range of downstream applications such as information extraction and intelligent document analysis. Benefiting from strong semantic modeling and robust generalization, VLM-based end-to-end approaches have emerged as the mainstream paradigm in recent years. However, these models often suffer from substantial inference latency, as they must auto-regressively generate long token sequences when processing long-form documents. In this work, motivated by the extremely long outputs and complex layout structures commonly found in document parsing, we propose a training-free and highly efficient acceleration method. Inspired by speculative decoding, we employ a lightweight document parsing pipeline as a draft model to predict batches of future tokens, while the more accurate VLM verifies these draft predictions in parallel. Moreover, we further exploit the layout-structured nature of documents by partitioning each page into independent regions, enabling parallel decoding of each region using the same draft-verify strategy. The final predictions are then assembled according to the natural reading order. Experimental results demonstrate the effectiveness of our approach: on the general-purpose OmniDocBench, our method provides a 2.42x lossless acceleration for the dots.ocr model, and achieves up to 4.89x acceleration on long-document parsing tasks. We will release our code to facilitate reproducibility and future research.

</details>


### [45] [MASAR: Motion-Appearance Synergy Refinement for Joint Detection and Trajectory Forecasting](https://arxiv.org/abs/2602.13003)
*Mohammed Amine Bencheikh Lehocine,Julian Schmidt,Frank Moosmann,Dikshant Gupta,Fabian Flohr*

Main category: cs.CV

TL;DR: MASAR 提出了一种结合了新型时空机制和端到端训练的框架，通过预测过往轨迹并利用外观线索进行细化，来提高未来的轨迹预测能力。


<details>
  <summary>Details</summary>
Motivation: 传统的自动驾驶系统中，感知和预测模块之间通过手工制作的边界框接口连接，限制了信息流动并产生了下游任务中的错误。在此之前的研究尝试开发整合感知与预测的端到端模型，但往往未能充分利用外观和运动线索之间的协同效应。MASAR 旨在解决这一问题，通过考虑“往身后看以展望未来”的理念，提出了一种新的可微分框架。

Method: MASAR 框架利用了一种基于对象的时空机制，同时编码外观和运动特征。通过预测过去轨迹并使用外观线索进行精炼，MASAR 捕捉了长期时间依赖性，从而优化未来的轨迹预测。

Result: 在 nuScenes 数据集上的实验证明了 MASAR 的有效性，与基线方法相比，其 minADE 和 minFDE 性能提升了超过 20%，同时保持了稳健的检测性能。

Conclusion: 总的来说，MASAR 通过增加时间序列建模能力提升了自动驾驶中的轨迹预测效果，并且有望为未来的研究和应用提供新的思路。

Abstract: Classical autonomous driving systems connect perception and prediction modules via hand-crafted bounding-box interfaces, limiting information flow and propagating errors to downstream tasks. Recent research aims to develop end-to-end models that jointly address perception and prediction; however, they often fail to fully exploit the synergy between appearance and motion cues, relying mainly on short-term visual features. We follow the idea of "looking backward to look forward", and propose MASAR, a novel fully differentiable framework for joint 3D detection and trajectory forecasting compatible with any transformer-based 3D detector. MASAR employs an object-centric spatio-temporal mechanism that jointly encodes appearance and motion features. By predicting past trajectories and refining them using guidance from appearance cues, MASAR captures long-term temporal dependencies that enhance future trajectory forecasting. Experiments conducted on the nuScenes dataset demonstrate MASAR's effectiveness, showing improvements of over 20% in minADE and minFDE while maintaining robust detection performance. Code and models are available at https://github.com/aminmed/MASAR.

</details>


### [46] [Towards Universal Video MLLMs with Attribute-Structured and Quality-Verified Instructions](https://arxiv.org/abs/2602.13013)
*Yunheng Li,Hengrui Zhang,Meng-Hao Guo,Wenzhao Gao,Shaoyong Jia,Shaohui Jiao,Qibin Hou,Ming-Ming Cheng*

Main category: cs.CV

TL;DR: 该研究提出了一个名为ASID-1M的开源数据集，包含一百万条结构化且细粒度的音频视觉指令注释，同时引入了ASID-Verify数据处理流水线，以及使用Supervised Fine-Tuning训练的ASID-Captioner模型。实验结果显示，ASID-Captioner在细粒度字幕质量、减少幻觉和指令跟随方面取得了显著改进。


<details>
  <summary>Details</summary>
Motivation: 现有的模型主要受限于视频-指令数据集，这些数据集用单一、不完整的描述表示复杂的视听内容，缺乏细粒度的组织和可靠的注释。本文旨在通过构建一个新的大型数据集和开发一个可扩展的数据处理流水线来提高视频理解性能。

Method: 提出了ASID-1M数据集，并使用Supervised Fine-Tuning方法训练ASID-Captioner模型。引入ASID-Verify用于注释的自动验证和修正。

Result: 在七个包含音频视觉字幕、属性字幕、字幕驱动的问答和字幕驱动的空间定位基准测试中，ASID-Captioner在细粒度字幕准确性、减少幻觉和指令跟随方面表现出色，达到了开源模型的领先地位，与Gemini-3-Pro相当。

Conclusion: 新的数据集和模型在不同任务中展现了优异性能，特别是在细粒度字幕和指令执行方面，表明了其创新性和有效性。

Abstract: Universal video understanding requires modeling fine-grained visual and audio information over time in diverse real-world scenarios. However, the performance of existing models is primarily constrained by video-instruction data that represents complex audiovisual content as single, incomplete descriptions, lacking fine-grained organization and reliable annotation. To address this, we introduce: (i) ASID-1M, an open-source collection of one million structured, fine-grained audiovisual instruction annotations with single- and multi-attribute supervision; (ii) ASID-Verify, a scalable data curation pipeline for annotation, with automatic verification and refinement that enforces semantic and temporal consistency between descriptions and the corresponding audiovisual content; and (iii) ASID-Captioner, a video understanding model trained via Supervised Fine-Tuning (SFT) on the ASID-1M. Experiments across seven benchmarks covering audiovisual captioning, attribute-wise captioning, caption-based QA, and caption-based temporal grounding show that ASID-Captioner improves fine-grained caption quality while reducing hallucinations and improving instruction following. It achieves state-of-the-art performance among open-source models and is competitive with Gemini-3-Pro.

</details>


### [47] [Multimodal Classification via Total Correlation Maximization](https://arxiv.org/abs/2602.13015)
*Feng Yu,Xiangyu Wu,Yang Yang,Jianfeng Lu*

Main category: cs.CV

TL;DR: 本文通过最大化多模态特征与标签之间的总相关性，提出了一个无超参的损失函数TCMax，以缓解模态竞争并捕捉跨模态交互。实验表明，TCMax在联合学习和单模态学习方法中表现优越。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明，联合学习会导致某些模态过拟合，而忽略其他模态。本文从信息论角度分析模态竞争，并提出基于最大总相关性的方法，旨在平衡各模态的贡献。

Method: 本文基于互信息神经估计（MINE）引入了总相关性神经估计（TCNE），以推导总相关性的下界。进一步提出TCMax损失函数，通过变分界优化最大化总相关性。

Result: 实验表明，TCMax相较于最先进的联合学习和单模态学习方法，能够更有效地提高多模态分类性能。

Conclusion: 综上所述，本文提出的方法能够缓解模态间的竞争，提升多模态信息的综合处理效果，从而在分类任务中取得更好的性能。

Abstract: Multimodal learning integrates data from diverse sensors to effectively harness information from different modalities. However, recent studies reveal that joint learning often overfits certain modalities while neglecting others, leading to performance inferior to that of unimodal learning. Although previous efforts have sought to balance modal contributions or combine joint and unimodal learning, thereby mitigating the degradation of weaker modalities with promising outcomes, few have examined the relationship between joint and unimodal learning from an information-theoretic perspective. In this paper, we theoretically analyze modality competition and propose a method for multimodal classification by maximizing the total correlation between multimodal features and labels. By maximizing this objective, our approach alleviates modality competition while capturing inter-modal interactions via feature alignment. Building on Mutual Information Neural Estimation (MINE), we introduce Total Correlation Neural Estimation (TCNE) to derive a lower bound for total correlation. Subsequently, we present TCMax, a hyperparameter-free loss function that maximizes total correlation through variational bound optimization. Extensive experiments demonstrate that TCMax outperforms state-of-the-art joint and unimodal learning approaches. Our code is available at https://github.com/hubaak/TCMax.

</details>


### [48] [Learning Image-based Tree Crown Segmentation from Enhanced Lidar-based Pseudo-labels](https://arxiv.org/abs/2602.13022)
*Julius Pesonen,Stefan Rua,Josef Taher,Niko Koivumäki,Xiaowei Yu,Eija Honkavaara*

Main category: cs.CV

TL;DR: 该研究提出了一种使用伪标签和零样本实例分割模型（SAM 2）来训练深度学习模型的方法，用于从RGB和多光谱图像中分割和分离单个树木。这种方法能够以光学图像为基础获取特定领域训练注解，无需手动标注，并在目标任务上表现优于通用领域部署模型。


<details>
  <summary>Details</summary>
Motivation: 由于城市树木管理和森林健康监测任务的需求，需要准确分离单个树木的树冠。高空成像技术因其纹理复杂性和树冠部分重叠而难以实现自动化分离。

Method: 该方法利用从航空激光扫描（ALS）数据中提取的伪标签来训练深度学习模型。进一步使用零样本实例分割模型（SAM 2）增强ALS衍生的伪标签。

Result: 该方法能够通过光学图像获取特定领域的训练标注，无需人工标注成本，所训练的分割模型在目标任务上表现超越通用领域部署模型。

Conclusion: 该研究提供了一种有效的方法来克服树木分割的挑战，并为其它具有相似需求的领域提供了新的解决方案。

Abstract: Mapping individual tree crowns is essential for tasks such as maintaining urban tree inventories and monitoring forest health, which help us understand and care for our environment. However, automatically separating the crowns from each other in aerial imagery is challenging due to factors such as the texture and partial tree crown overlaps. In this study, we present a method to train deep learning models that segment and separate individual trees from RGB and multispectral images, using pseudo-labels derived from aerial laser scanning (ALS) data. Our study shows that the ALS-derived pseudo-labels can be enhanced using a zero-shot instance segmentation model, Segment Anything Model 2 (SAM 2). Our method offers a way to obtain domain-specific training annotations for optical image-based models without any manual annotation cost, leading to segmentation models which outperform any available models which have been targeted for general domain deployment on the same task.

</details>


### [49] [Implicit-Scale 3D Reconstruction for Multi-Food Volume Estimation from Monocular Images](https://arxiv.org/abs/2602.13041)
*Yuhao Chen,Gautham Vinod,Siddeshwar Raghavan,Talha Ibn Mahmud,Bruce Coburn,Jinge Ma,Fengqing Zhu,Jiangpeng He*

Main category: cs.CV

TL;DR: 本文介绍了一种名为 Implicit-Scale 3D Reconstruction from Monocular Multi-Food Images 的基准数据集，旨在推动几何基于的食物份量估算在现实用餐场景中的发展。该数据集涉及多食物场景、频繁遮挡和复杂的空间布局，旨在解决现有方法中的尺度模糊问题。


<details>
  <summary>Details</summary>
Motivation: 现有的饮食评估方法大多依赖单张图像分析或基于外观的推理，缺乏明确的几何推理，容易受到尺度模糊的影响。因此，本文提出了一种新的基准数据集，将食物份量估算重新定义为单目观测下的隐尺度3D重建问题。

Method: 此数据集消除了明确的物理参考和度量标注，提供了背景中的盘子和餐具等，要求算法从隐含线索和先验知识推断尺度。研究团队利用这种方法下的重建解决方案进行实验。

Result: 实验结果表明，虽然基于视觉-语言的基线方法的性能竞争力十分强劲，但基于几何的重建方法具有更高的精度和更强的鲁棒性，结果表明最佳方法在体积估算中的MAPE达到0.21，在几何准确性中的L1 Chamfer Distance为5.7。

Conclusion: MetaFood 2025研讨会中采用了此基准数据集，多个团队提出了基于重建的方法，而基于几何的重建方法在准确性和鲁棒性方面得到了显著改进。

Abstract: We present Implicit-Scale 3D Reconstruction from Monocular Multi-Food Images, a benchmark dataset designed to advance geometry-based food portion estimation in realistic dining scenarios. Existing dietary assessment methods largely rely on single-image analysis or appearance-based inference, including recent vision-language models, which lack explicit geometric reasoning and are sensitive to scale ambiguity. This benchmark reframes food portion estimation as an implicit-scale 3D reconstruction problem under monocular observations. To reflect real-world conditions, explicit physical references and metric annotations are removed; instead, contextual objects such as plates and utensils are provided, requiring algorithms to infer scale from implicit cues and prior knowledge. The dataset emphasizes multi-food scenes with diverse object geometries, frequent occlusions, and complex spatial arrangements. The benchmark was adopted as a challenge at the MetaFood 2025 Workshop, where multiple teams proposed reconstruction-based solutions. Experimental results show that while strong vision--language baselines achieve competitive performance, geometry-based reconstruction methods provide both improved accuracy and greater robustness, with the top-performing approach achieving 0.21 MAPE in volume estimation and 5.7 L1 Chamfer Distance in geometric accuracy.

</details>


### [50] [A Calibrated Memorization Index (MI) for Detecting Training Data Leakage in Generative MRI Models](https://arxiv.org/abs/2602.13066)
*Yash Deo,Yan Jia,Toni Lassila,Victoria J Hodge,Alejandro F Frang,Chenghao Qian,Siyuan Kang,Ibrahim Habli*

Main category: cs.CV

TL;DR: 该研究提出了一种针对医学影像生成模型的校准单样本度量方法，用于检测训练数据的重复和复制。该方法使用MRI基础模型提取图像特征，计算多层白化最近邻相似性，并映射到一个受限制的过度拟合/新颖性指数（ONI）和记忆指数（MI）分数。


<details>
  <summary>Details</summary>
Motivation: 医学图像生成模型存在隐私泄露风险，因为这些模型在生成图像时可能会复制训练数据。本文旨在提出一种新的单样本度量方法来检测训练数据的重复和复制，以提升模型的隐私保护性能。

Method: 研究采用MRI基础模型提取图像特征，通过多层白化最近邻相似性计算，将这些相似性映射到一个限制在一定范围内的过度拟合/新颖性指数（ONI）和记忆指数（MI）分数。这种方法在三个具有不同复制比例和典型图像增强的MRI数据集上进行验证。

Result: 研究结果表明，该新度量方法能够稳健地检测到重复的数据，并提供了一致度更好的度量值。在单样本级别上，该方法几乎可以完美地检测到重复的样本。

Conclusion: 本文提出的方法有助于提升医学图像生成模型的安全性和隐私保护，对抗模型在生成图像时引起的隐私泄露问题。该研究对未来改进医学图像生成模型提供了新的思路。

Abstract: Image generative models are known to duplicate images from the training data as part of their outputs, which can lead to privacy concerns when used for medical image generation. We propose a calibrated per-sample metric for detecting memorization and duplication of training data. Our metric uses image features extracted using an MRI foundation model, aggregates multi-layer whitened nearest-neighbor similarities, and maps them to a bounded \emph{Overfit/Novelty Index} (ONI) and \emph{Memorization Index} (MI) scores. Across three MRI datasets with controlled duplication percentages and typical image augmentations, our metric robustly detects duplication and provides more consistent metric values across datasets. At the sample level, our metric achieves near-perfect detection of duplicates.

</details>


### [51] [SIEFormer: Spectral-Interpretable and -Enhanced Transformer for Generalized Category Discovery](https://arxiv.org/abs/2602.13067)
*Chunming Li,Shidong Wang,Tong Xin,Haofeng Zhang*

Main category: cs.CV

TL;DR: 本文介绍了SIEFormer，一种利用频谱分析重新定义Vision Transformer (ViT)中的注意力机制并增强其特征适应性的新型方法，特别针对具有挑战性的Generalized Category Discovery (GCD)任务。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有Vision Transformer在处理具有挑战性的Generalized Category Discovery (GCD)任务时存在的特征适应性不足的问题。

Method: SIEFormer提出了两个主要分支，隐式分支利用不同类型的图拉普拉斯算子来建模标记的局部结构关联，并引入了可变滤波层（MFL）学习输入features的全局依赖关系。显式分支则采用了频谱分析来增强特征。

Result: 实验结果显示，SIEFormer在多种图像识别数据集上取得了最先进的性能，并通过消融研究和可视化验证了其优越性。

Conclusion: 总之，SIEFormer为改进Vision Transformer在复杂任务中的性能提供了一种新的解决方案。

Abstract: This paper presents a novel approach, Spectral-Interpretable and -Enhanced Transformer (SIEFormer), which leverages spectral analysis to reinterpret the attention mechanism within Vision Transformer (ViT) and enhance feature adaptability, with particular emphasis on challenging Generalized Category Discovery (GCD) tasks. The proposed SIEFormer is composed of two main branches, each corresponding to an implicit and explicit spectral perspective of the ViT, enabling joint optimization. The implicit branch realizes the use of different types of graph Laplacians to model the local structure correlations of tokens, along with a novel Band-adaptive Filter (BaF) layer that can flexibly perform both band-pass and band-reject filtering. The explicit branch, on the other hand, introduces a Maneuverable Filtering Layer (MFL) that learns global dependencies among tokens by applying the Fourier transform to the input ``value" features, modulating the transformed signal with a set of learnable parameters in the frequency domain, and then performing an inverse Fourier transform to obtain the enhanced features. Extensive experiments reveal state-of-the-art performance on multiple image recognition datasets, reaffirming the superiority of our approach through ablation studies and visualizations.

</details>


### [52] [Universal Transformation of One-Class Classifiers for Unsupervised Anomaly Detection](https://arxiv.org/abs/2602.13091)
*Declan McIntosh,Alexandra Branzan Albu*

Main category: cs.CV

TL;DR: 该研究提出了一种方法，能够将基于一类分类器的异常检测器转换为完全无监督的方法，通过假设异常在训练集中罕见且通常具有异质性，实现了这一目标。该方法无需修改底层异常检测器，只需要选择算法性数据子集进行训练。实验表明，该方法能够将多种现有的异常检测器转换为无监督模式，在多个数据集上实现了最先进的无监督异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 当前的异常检测方法大多依赖于带有标签的训练数据，但标签噪声可能使检测算法收效甚微或完全失效。这篇论文尝试通过改造现有的基于一类分类器的异常检测器，来构建一种无监督的异常检测方法。

Method: 研究中提出了一个数据折叠方法，通过设定一些关键的弱假设，即异常在训练集中不常见且通常是异质的，利用多个独立训练的一类分类器实例来筛选出训练集中的异常。这种方法不需要对基础的异常检测器进行修改，只需选择算法性数据子集用于训练。

Result: 该方法能够将多种基于一类分类器的异常检测器转换为无监督模式。实验结果显示，该方法在MVTec AD、ViSA和MVTec Loco AD数据集上实现了最先进的无监督异常检测性能。

Conclusion: 这种数据折叠方法成功地将一类分类器的异常检测器转化为了无监督异常检测方法，并展示了其在多种场景下的有效性。它表明，即使是一类分类器性能得到改进，这种方法也能直接转移到无监督领域，促进两个领域的联系。

Abstract: Detecting anomalies in images and video is an essential task for multiple real-world problems, including industrial inspection, computer-assisted diagnosis, and environmental monitoring. Anomaly detection is typically formulated as a one-class classification problem, where the training data consists solely of nominal values, leaving methods built on this assumption susceptible to training label noise. We present a dataset folding method that transforms an arbitrary one-class classifier-based anomaly detector into a fully unsupervised method. This is achieved by making a set of key weak assumptions: that anomalies are uncommon in the training dataset and generally heterogeneous. These assumptions enable us to utilize multiple independently trained instances of a one-class classifier to filter the training dataset for anomalies. This transformation requires no modifications to the underlying anomaly detector; the only changes are algorithmically selected data subsets used for training. We demonstrate that our method can transform a wide variety of one-class classifier anomaly detectors for both images and videos into unsupervised ones. Our method creates the first unsupervised logical anomaly detectors by transforming existing methods. We also demonstrate that our method achieves state-of-the-art performance for unsupervised anomaly detection on the MVTec AD, ViSA, and MVTec Loco AD datasets. As improvements to one-class classifiers are made, our method directly transfers those improvements to the unsupervised domain, linking the domains.

</details>


### [53] [Realistic Face Reconstruction from Facial Embeddings via Diffusion Models](https://arxiv.org/abs/2602.13168)
*Dong Han,Yong Li,Joachim Denzler*

Main category: cs.CV

TL;DR: 本文提出了一种名为Face Embedding Mapping (FEM) 的框架，通过利用预训练的身份保持扩散模型，以Kolmogorov-Arnold Network (KAN) 从隐私保护面部识别系统的嵌入中重构高分辨率的面部图像，以评估其隐私泄露风险。


<details>
  <summary>Details</summary>
Motivation: 随着面部识别系统的进步，隐私保护的面部识别系统变得越来越受欢迎。然而，关于这些系统潜在隐私风险的研究还比较有限，特别是关于从嵌入中重构高分辨率面部图像的研究。

Method: 该方法利用预训练的身份保持扩散模型和Kolmogorov-Arnold Network（KAN），将面部识别和隐私保护面部识别系统的嵌入转换为高分辨率的脸部图像。

Result: 实验结果显示，重构的脸部图像可以被其他真实世界的面部识别系统使用，且在部分和保护的嵌入图像上具有良好的重构能力。此外，FEM能够被用作评估面部识别和隐私保护面部识别系统安全性的工具，特别是在隐私泄露方面。

Conclusion: 该工作验证了隐私保护面部识别系统的潜在隐私风险，并为进一步研究提供了一种有效的方法。

Abstract: With the advancement of face recognition (FR) systems, privacy-preserving face recognition (PPFR) systems have gained popularity for their accurate recognition, enhanced facial privacy protection, and robustness to various attacks. However, there are limited studies to further verify privacy risks by reconstructing realistic high-resolution face images from embeddings of these systems, especially for PPFR. In this work, we propose the face embedding mapping (FEM), a general framework that explores Kolmogorov-Arnold Network (KAN) for conducting the embedding-to-face attack by leveraging pre-trained Identity-Preserving diffusion model against state-of-the-art (SOTA) FR and PPFR systems. Based on extensive experiments, we verify that reconstructed faces can be used for accessing other real-word FR systems. Besides, the proposed method shows the robustness in reconstructing faces from the partial and protected face embeddings. Moreover, FEM can be utilized as a tool for evaluating safety of FR and PPFR systems in terms of privacy leakage. All images used in this work are from public datasets.

</details>


### [54] [LongStream: Long-Sequence Streaming Autoregressive Visual Geometry](https://arxiv.org/abs/2602.13172)
*Chong Cheng,Xianda Chen,Tao Xie,Wei Yin,Weiqiang Ren,Qian Zhang,Xiaoyuang Guo,Hao Wang*

Main category: cs.CV

TL;DR: LongStream引入了一种新颖的基于相位分离的流媒体视觉几何模型，用于跨数千帧实现米级场景重建。该模型通过取消首帧锚定并预测关键帧相对姿态、引入正交尺度学习以分离几何与尺度估计、以及解决Transformers缓存问题，从而克服了长期自回归模型在长序列处理中的注意力衰减、尺度漂移和外推误差问题。实验表明，LongStream在千帧级别实现了现有最佳性能，能够稳定进行米级场景重建，帧率为18 FPS。


<details>
  <summary>Details</summary>
Motivation: 现有的自回归模型在处理长序列时会遇到注意力衰减、尺度漂移和外推误差等挑战，这限制了其在大规模场景重建中的应用。

Method: LongStream通过取消首帧锚定并预测关键帧相对姿态，将长期外推任务转化为恒定难度的局部任务；引入正交尺度学习以分离几何与尺度估计，从而抑制尺度漂移；并通过缓存一致训练和定期缓存刷新，解决Transformers缓存问题。

Result: 实验结果表明，LongStream在大规模场景重建中取得了现有最佳性能，能够稳定实现米级重建，帧率为18 FPS。

Conclusion: LongStream提供了一种有效的解决方案来处理长序列3D重建问题，通过关键改进显著提升了三维重建的稳定性和精度。

Abstract: Long-sequence streaming 3D reconstruction remains a significant open challenge. Existing autoregressive models often fail when processing long sequences. They typically anchor poses to the first frame, which leads to attention decay, scale drift, and extrapolation errors. We introduce LongStream, a novel gauge-decoupled streaming visual geometry model for metric-scale scene reconstruction across thousands of frames. Our approach is threefold. First, we discard the first-frame anchor and predict keyframe-relative poses. This reformulates long-range extrapolation into a constant-difficulty local task. Second, we introduce orthogonal scale learning. This method fully disentangles geometry from scale estimation to suppress drift. Finally, we solve Transformer cache issues such as attention-sink reliance and long-term KV-cache contamination. We propose cache-consistent training combined with periodic cache refresh. This approach suppresses attention degradation over ultra-long sequences and reduces the gap between training and inference. Experiments show LongStream achieves state-of-the-art performance. It delivers stable, metric-scale reconstruction over kilometer-scale sequences at 18 FPS. Project Page: https://3dagentworld.github.io/longstream/

</details>


### [55] [Monocular Markerless Motion Capture Enables Quantitative Assessment of Upper Extremity Reachable Workspace](https://arxiv.org/abs/2602.13176)
*Seth Donahue,J. D. Peiffer,R. Tyler Richardson,Yishan Zhong,Shaun Q. Y. Tan,Benoit Marteau,Stephanie R. Russo,May D. Wang,R. James Cotton,Ross Chafetz*

Main category: cs.CV

TL;DR: 本研究验证了一种使用单（单目）相机和基于AI的无标记运动捕捉（AI-驱动的MMC）来量化上肢可触及工作区（UERW）的方法。


<details>
  <summary>Details</summary>
Motivation: 为特定临床任务的客观评估和验证这些技术对于临床运动分析的采用至关重要。

Method: 使用九名无损伤的成人参与者，他们完成了标准化的UERW任务，通过标记运动捕捉系统和8台FLIR相机的组合进行同时捕捉。

Result: 前视相机配置表现出与标记基于参考的强一致性，最小的均值偏差为0.61 ± 0.12%的有效触及空间。而偏置相机视图低估了触及的工作空间，达到-5.66 ± 0.45%的有效触及空间。

Conclusion: 研究结果支持 frontal 单目相机配置用于 UERW 评估的可行性，特别适用于前部工作空间的评估。该方法在技术复杂性降低方面显示出临床潜力，使其能够广泛实施量化上肢运动评估。

Abstract: To validate a clinically accessible approach for quantifying the Upper Extremity Reachable Workspace (UERW) using a single (monocular) camera and Artificial Intelligence (AI)-driven Markerless Motion Capture (MMC) for biomechanical analysis. Objective assessment and validation of these techniques for specific clinically oriented tasks are crucial for their adoption in clinical motion analysis. AI-driven monocular MMC reduces the barriers to adoption in the clinic and has the potential to reduce the overhead for analysis of this common clinical assessment. Nine adult participants with no impairments performed the standardized UERW task, which entails reaching targets distributed across a virtual sphere centered on the torso, with targets displayed in a VR headset. Movements were simultaneously captured using a marker-based motion capture system and a set of eight FLIR cameras. We performed monocular video analysis on two of these video camera views to compare a frontal and offset camera configurations. The frontal camera orientation demonstrated strong agreement with the marker-based reference, exhibiting a minimal mean bias of $0.61 \pm 0.12$ \% reachspace reached per octanct (mean $\pm$ standard deviation). In contrast, the offset camera view underestimated the percent workspace reached ($-5.66 \pm 0.45$ \% reachspace reached). Conclusion: The findings support the feasibility of a frontal monocular camera configuration for UERW assessment, particularly for anterior workspace evaluation where agreement with marker-based motion capture was highest. The overall performance demonstrates clinical potential for practical, single-camera assessments. This study provides the first validation of monocular MMC system for the assessment of the UERW task. By reducing technical complexity, this approach enables broader implementation of quantitative upper extremity mobility assessment.

</details>


### [56] [FlexAM: Flexible Appearance-Motion Decomposition for Versatile Video Generation Control](https://arxiv.org/abs/2602.13185)
*Mingzhi Sheng,Zekai Gu,Peng Li,Cheng Lin,Hao-Xiang Guo,Ying-Cong Chen,Yuan Liu*

Main category: cs.CV

TL;DR: FlexAM 是一种新的统一框架，通过引入一种新颖的 3D 控信号，实现了视频生成中“外观”和“运动”的分离，提升了多种任务的表现。


<details>
  <summary>Details</summary>
Motivation: 当前大多数方法依赖于模糊或任务特定的信号，而我们认为将“外观”和“运动”进行基础解耦，能提供更稳健和可扩展的解决方案。

Method: FlexAM 采用多频率位置编码来区分精细的运动，深度意识位置编码以及灵活的控制信号来平衡精度与生成质量，通过点云表示视频动力学，从而实现了外观和运动的有效解耦。

Result: 实验结果表明，FlexAM 在所有评估任务中均表现出卓越性能。

Conclusion: FlexAM 提供了一种统一的框架，利用 3D 控信号实现了视频生成任务中的关键解耦，适用于包括 I2V、V2V 编辑、摄像机控制和空间对象编辑等多种任务。

Abstract: Effective and generalizable control in video generation remains a significant challenge. While many methods rely on ambiguous or task-specific signals, we argue that a fundamental disentanglement of "appearance" and "motion" provides a more robust and scalable pathway. We propose FlexAM, a unified framework built upon a novel 3D control signal. This signal represents video dynamics as a point cloud, introducing three key enhancements: multi-frequency positional encoding to distinguish fine-grained motion, depth-aware positional encoding, and a flexible control signal for balancing precision and generative quality. This representation allows FlexAM to effectively disentangle appearance and motion, enabling a wide range of tasks including I2V/V2V editing, camera control, and spatial object editing. Extensive experiments demonstrate that FlexAM achieves superior performance across all evaluated tasks.

</details>


### [57] [Conversational Image Segmentation: Grounding Abstract Concepts with Scalable Supervision](https://arxiv.org/abs/2602.13195)
*Aadarsh Sahoo,Georgia Gkioxari*

Main category: cs.CV

TL;DR: 本文介绍了Conversational Image Segmentation（CIS）及其基准ConverSeg，旨在通过对话方式实现像素级准确的掩码生成，同时涵盖实体、空间关系、意图、功能、安全性和物理推理。ConverSeg-Net结合了强大的分割先验与语言理解，并通过AI驱动的数据生成引擎无需人工监督生成提示-掩码对。实验结果显示，当前的语言指导分割模型对于CIS效果不佳，而ConverSeg-Net在ConverSeg上取得了显著提升，且在现有语言指导分割基准上表现依然出色。


<details>
  <summary>Details</summary>
Motivation: 现有工作主要关注图像引用中的类别和空间查询，但忽视了功能和物理推理。本文旨在填补这一空白，提高对话理解在像素级图像分割任务中的应用效果。

Method: 提出了一种新的对话式图像分割方法CIS及基准ConverSeg，该方法通过结合强烈的分割先验与语言理解来生成掩码。同时引入了AI驱动的数据生成引擎，无需人工监督生成提示-掩码对。

Result: 实验结果表明，当前的语言指导分割模型在CIS任务上表现不佳，而基于本文提出的ConverSeg-Net方法在ConverSeg基准上取得了显著改进，并在其他现有语言指导分割基准上保持了优秀的性能。

Conclusion: 本文提出的CIS方法及ConverSeg-Net模型在图像分割和对话理解结合方面取得了重要进展，为该领域的研究提供了新的思路和方法。

Abstract: Conversational image segmentation grounds abstract, intent-driven concepts into pixel-accurate masks. Prior work on referring image grounding focuses on categorical and spatial queries (e.g., "left-most apple") and overlooks functional and physical reasoning (e.g., "where can I safely store the knife?"). We address this gap and introduce Conversational Image Segmentation (CIS) and ConverSeg, a benchmark spanning entities, spatial relations, intent, affordances, functions, safety, and physical reasoning. We also present ConverSeg-Net, which fuses strong segmentation priors with language understanding, and an AI-powered data engine that generates prompt-mask pairs without human supervision. We show that current language-guided segmentation models are inadequate for CIS, while ConverSeg-Net trained on our data engine achieves significant gains on ConverSeg and maintains strong performance on existing language-guided segmentation benchmarks. Project webpage: https://glab-caltech.github.io/converseg/

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [58] [A Lightweight LLM Framework for Disaster Humanitarian Information Classification](https://arxiv.org/abs/2602.12284)
*Han Jinzhen,Kim Jisung,Yang Jong Soo,Yun Hong Sik*

Main category: cs.CL

TL;DR: 本研究开发了一种轻量级的框架，通过参数高效微调进行灾难推文分类，建立了一个统一的实验语料库，并通过系统的评估展示了该方法在资源受限环境下的有效性。


<details>
  <summary>Details</summary>
Motivation: 在紧急情况下部署大规模语言模型进行人道主义信息分类面临资源限制的挑战，因此需要开发一种轻量级且成本效益高的框架。

Method: 通过参数高效微调（LoRA和QLoRA）和检索增强生成（RAG）策略对Llama 3.1 8B模型进行训练，构建了一致的实验数据集，对比多种策略的效果。

Result: 研究结果表明LoRA方法可实现79.62%的人道主义分类准确性，并且QLoRA方法可以在50%内存使用的情况下保持接近LoRA的性能。同时，RAG方法并没有提高模型性能，反而由于检索到示例的标签噪声而降低了模型性能。

Conclusion: 研究建立了一种在有限计算资源下构建可靠危机情报系统的可行管道，为紧急情况下的及时信息分类提供了有效的解决方案。

Abstract: Timely classification of humanitarian information from social media is critical for effective disaster response. However, deploying large language models (LLMs) for this task faces challenges in resource-constrained emergency settings. This paper develops a lightweight, cost-effective framework for disaster tweet classification using parameter-efficient fine-tuning. We construct a unified experimental corpus by integrating and normalizing the HumAID dataset (76,484 tweets across 19 disaster events) into a dual-task benchmark: humanitarian information categorization and event type identification. Through systematic evaluation of prompting strategies, LoRA fine-tuning, and retrieval-augmented generation (RAG) on Llama 3.1 8B, we demonstrate that: (1) LoRA achieves 79.62% humanitarian classification accuracy (+37.79% over zero-shot) while training only ~2% of parameters; (2) QLoRA enables efficient deployment with 99.4% of LoRA performance at 50% memory cost; (3) contrary to common assumptions, RAG strategies degrade fine-tuned model performance due to label noise from retrieved examples. These findings establish a practical, reproducible pipeline for building reliable crisis intelligence systems with limited computational resources.

</details>


### [59] [Retrieval-Augmented Self-Taught Reasoning Model with Adaptive Chain-of-Thought for ASR Named Entity Correction](https://arxiv.org/abs/2602.12287)
*Junjie An,Jingguang Tian,Tianyi Wang,Yu Gao,Xiaofeng Mou,Yi Xu*

Main category: cs.CL

TL;DR: 本文提出了一种新颖的检索增强生成框架，用于自动语音识别中命名实体错误的修正。该方法结合了重构语言模型和一种新型自学习推理模型。实验结果表明该方法在两个数据集上的命名实体字符错误率分别降低了17.96%和34.42%，与强基线相比效果显著。


<details>
  <summary>Details</summary>
Motivation: 自动语音识别系统在处理专门为领域特定短语时容易出现错误，这些错误可能导致下游任务的灾难性失败。现有的基于大型语言模型的命名实体纠正方法尚未充分利用这些模型复杂的推理能力。

Method: 该方法包含两个关键组件：一种用于命名实体识别的重构语言模型，结合声学编辑距离进行候选检索；以及一种新型自学习推理模型，具备自适应链式推理能力，能够根据任务难度动态调整推理深度。

Result: 在AISHELL-1和同音词数据集上的实验结果显示，该方法相对于强基线，分别在命名实体字符错误率上取得了17.96%和34.42%的相对减少。

Conclusion: 这项研究表明，该命名实体纠正方法在自动语音识别领域的应用具有显著的效果和潜力。

Abstract: End-to-end automatic speech recognition (ASR) systems frequently misrecognize domain-specific phrases like named entities, which can cause catastrophic failures in downstream tasks. A new family of named entity correction methods based on large language models (LLMs) has recently emerged. However, these approaches have yet to fully exploit the sophisticated reasoning capabilities inherent to LLMs. To bridge this gap, we propose a novel retrieval-augmented generation framework for correcting named entity errors in ASR. Our approach consists of two key components: (1) a rephrasing language model (RLM) for named entity recognition, followed by candidate retrieval using a phonetic-level edit distance; and (2) a novel self-taught reasoning model with adaptive chain-of-thought (A-STAR) that dynamically adjusts the depth of its reasoning based on task difficulty. Experiments on the AISHELL-1 and Homophone datasets demonstrate the effectiveness of our method, which achieves relative reductions in the named entity character error rate of 17.96\% and 34.42\%, respectively, compared to a strong baseline.

</details>


### [60] [Grandes Modelos de Linguagem Multimodais (MLLMs): Da Teoria à Prática](https://arxiv.org/abs/2602.12302)
*Neemias da Silva,Júlio C. W. Scholz,John Harrison,Marina Borges,Paulo Ávila,Frances A Santos,Myriam Delgado,Rodrigo Minetto,Thiago H Silva*

Main category: cs.CL

TL;DR: 本章节介绍了多模态大型语言模型（MLLMs）的基础知识、代表性模型、多模态预处理、提示工程以及使用LangChain和LangGraph构建多模态管道的方法，同时也探讨了面临的挑战和潜在趋势。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能领域的发展，多模态技术逐渐成为研究热点，而多模态大型语言模型则结合了自然语言处理和感知能力，能够在多个模态中进行理解和生成任务，这为更广泛应用场景带来了潜力。

Method: 本章节通过介绍基础概念、方法论并提供了实际操作指南，包括多模态预处理、提示工程和使用特定框架进行实践操作的具体步骤。

Result: 读者能够理解多模态大型语言模型的重要性及其在实际应用中的能力，还能够学习到如何利用现有工具和方法构建和使用这些模型。

Conclusion: 尽管多模态大型语言模型已取得一定进展，但仍存在一些挑战，未来的研究需关注这些挑战以推进相关技术的发展。

Abstract: Multimodal Large Language Models (MLLMs) combine the natural language understanding and generation capabilities of LLMs with perception skills in modalities such as image and audio, representing a key advancement in contemporary AI. This chapter presents the main fundamentals of MLLMs and emblematic models. Practical techniques for preprocessing, prompt engineering, and building multimodal pipelines with LangChain and LangGraph are also explored. For further practical study, supplementary material is publicly available online: https://github.com/neemiasbsilva/MLLMs-Teoria-e-Pratica. Finally, the chapter discusses the challenges and highlights promising trends.

</details>


### [61] [RankLLM: Weighted Ranking of LLMs by Quantifying Question Difficulty](https://arxiv.org/abs/2602.12424)
*Ziqian Zhang,Xingjian Hu,Yue Huang,Kai Zhang,Ruoxi Chen,Yixin Liu,Qingsong Wen,Kaidi Xu,Xiangliang Zhang,Neil Zhenqiang Gong,Lichao Sun*

Main category: cs.CL

TL;DR: 该研究提出了一种名为RankLLM的新框架，旨在量化问题难度与模型能力，通过双向评分传播机制，能够更精细地评估大规模语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的评估基准未能区分问题的难度，限制了对模型能力的有效区分。因此，研究提出RankLLM框架来解决这一问题。

Method: RankLLM框架通过双向评分传播机制，将问题的得分为模型的正确回答增加，同时将模型未能回答的问题难度增加。该方法利用此框架对30个模型在35,550个跨领域问题上进行评估。

Result: RankLLM在与人类判断的比较中达到了90%的一致性，并且在强基准如IRT（项目反应理论）上表现更优。同时，它还展示了高度的稳定性、快速收敛性和高计算效率。

Conclusion: 研究表明，RankLLM框架是适合大规模、考虑难度的大规模语言模型评估的有效解决方案。

Abstract: Benchmarks establish a standardized evaluation framework to systematically assess the performance of large language models (LLMs), facilitating objective comparisons and driving advancements in the field. However, existing benchmarks fail to differentiate question difficulty, limiting their ability to effectively distinguish models' capabilities. To address this limitation, we propose RankLLM, a novel framework designed to quantify both question difficulty and model competency. RankLLM introduces difficulty as the primary criterion for differentiation, enabling a more fine-grained evaluation of LLM capabilities. RankLLM's core mechanism facilitates bidirectional score propagation between models and questions. The core intuition of RankLLM is that a model earns a competency score when it correctly answers a question, while a question's difficulty score increases when it challenges a model. Using this framework, we evaluate 30 models on 35,550 questions across multiple domains. RankLLM achieves 90% agreement with human judgments and consistently outperforms strong baselines such as IRT. It also exhibits strong stability, fast convergence, and high computational efficiency, making it a practical solution for large-scale, difficulty-aware LLM evaluation.

</details>


### [62] [RBCorr: Response Bias Correction in Language Models](https://arxiv.org/abs/2602.12445)
*Om Bhatt,Anna A. Ivanova*

Main category: cs.CL

TL;DR: 该研究提出了一种简单的方法（RBCorr）来纠正语言模型中的响应偏见，并在多种类型的语言模型上进行测试，验证了其有效性和通用性。


<details>
  <summary>Details</summary>
Motivation: 鉴于语言模型容易表现出响应偏好偏见，研究开发了一种低成本且有效的响应偏见纠正方法，以提高模型性能，使其在回答固定响应问题时更加准确。

Method: 研究团队提出了一种基于LogProbs的响应偏见纠正策略（RBCorr），并在包括yes-no、蕴含判断和多项选择题在内的12种开放型语言模型上进行了测试。

Result: 研究结果显示，纠正前的模型普遍存在响应偏见，而应用RBCorr策略能够有效地消除这些偏见并提升模型性能。此外，研究发现基于LogProbs的纠正策略对不同模型、数据集和提示格式的依赖性较强。

Conclusion: 研究指出，RBCorr作为一种简单易于使用的手段，能够显著提升小型语言模型的性能，并确保通过封闭响应基准评估的语言模型性能更加接近其真实能力。

Abstract: Language models (LMs) are known to be prone to response biases, which present as option preference biases in fixed-response questions. It is therefore imperative to develop low-cost and effective response bias correction methods to improve LM performance and enable more accurate evaluations of model abilities. Here, we propose a simple response bias correction strategy ($\texttt{RBCorr}$) and test it on 12 open-weight language models using yes-no, entailment, and multiple choice questions. We show that response bias is prevalent in LMs pre-correction and that $\texttt{RBCorr}$ effectively eliminates bias and boosts model performance. We also explore the generalizability of bias behavior across models, datasets, and prompt formats, showing that LogProbs-based correction is highly dependent on all three of these aspects. Overall, $\texttt{RBCorr}$ is an easy-to-use method that can boost the performance of smaller LMs and ensure that LM performance on closed-response benchmarks aligns more closely with their true capabilities.

</details>


### [63] [Unleashing Low-Bit Inference on Ascend NPUs: A Comprehensive Evaluation of HiFloat Formats](https://arxiv.org/abs/2602.12635)
*Pengxiang Zhao,Hui-Ling Zhen,Xing Li,Han Bao,Weizhe Lin,Zhiyuan Yang,Ziwei Yu,Xin Wang,Mingxuan Yuan,Xianzhi Yu,Zhenhua Dong*

Main category: cs.CL

TL;DR: 这篇论文评估了HiFloat（HiF8和HiF4）格式在Ascend NPUs上的性能，强调了整数和浮点格式适用于不同场景，并展示了HiFloat在保持高效率的同时保证了高精度。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs的扩大，低比特浮点格式如MXFP和NVFP4为精度和效率带来了新的机会。因此，作者评估了HiFloat格式（HiF8和HiF4），以针对Ascend NPUs提供一种高效且准确的解决方案。

Method: 作者通过对HiFloat在权重激活和KV缓存任务方面的严格比较，验证了其在不同场景下的适用性。

Result: 研究结果表明，INT8适用于窄范围数据，而浮点格式在高变化数据方面表现出色；在4位环境中，HiF4的层次化缩放防止了如整数格式所示的准确性崩溃；此外，HiFloat完全兼容最先进的后训练量化框架。

Conclusion: HiFloat提供了在NPUs上进行高效LLM推理的解决方案，结合了良好的性能和易用性。

Abstract: As LLMs scale, low-bit floating-point formats like MXFP and NVFP4 offer new opportunities for precision and efficiency. In this work, we evaluate HiFloat (HiF8 and HiF4), a family of formats tailored for Ascend NPUs. Through rigorous comparison across weight-activation and KV-cache tasks, we provide three key insights: (1) INT8 suits narrow-range data, while floating-point formats excel with high-variance data; (2) in 4-bit regimes, HiF4's hierarchical scaling prevents the accuracy collapse seen in integer formats; and (3) HiFloat is fully compatible with state-of-the-art post-training quantization frameworks. Overall, HiFloat provides a solution for high-efficiency LLM inference on NPUs.

</details>


### [64] [CLASE: A Hybrid Method for Chinese Legalese Stylistic Evaluation](https://arxiv.org/abs/2602.12639)
*Yiran Rex Ma,Yuxiao Ye,Huiyuan Xie*

Main category: cs.CL

TL;DR: 该研究引入了CLASE方法，一种结合了语言特征评分和经验指导的LLM评分的综合评估方法，用于评估法律文本的风格。实验表明，CLASE比现有传统度量标准和纯粹的LLM评分方法能够更接近人类判断，且提供可解释的评分分解和改进建议。


<details>
  <summary>Details</summary>
Motivation: 现今的LLMs生成的法律文本尽管事实准确性较高，但在风格上难以符合专业规范及语言习惯。为了提升法律文本的风格质量，亟需一种可靠且透明的评估机制。

Method: CLASE方法集成了基于语言特征的评分和经验引导的LLM评分，通过对比真实的合法文件与其由LLM恢复的版本来学习特征权重和LLM评分经验。

Result: CLASE方法在200份中文法律文本上的实验结果显示，其与人工判断的契合度明显优于传统度量标准和纯粹的LLM评分方法，并提供了可解释的评分分解和改进建议。

Conclusion: CLASE通过其透明、无参考的评估方式，不仅提高了法律文本风格评估的准确性，还为专业的法律文本生成提供了实用的解决方案。

Abstract: Legal text generated by large language models (LLMs) can usually achieve reasonable factual accuracy, but it frequently fails to adhere to the specialised stylistic norms and linguistic conventions of legal writing. In order to improve stylistic quality, a crucial first step is to establish a reliable evaluation method. However, having legal experts manually develop such a metric is impractical, as the implicit stylistic requirements in legal writing practice are difficult to formalise into explicit rubrics. Meanwhile, existing automatic evaluation methods also fall short: reference-based metrics conflate semantic accuracy with stylistic fidelity, and LLM-as-a-judge evaluations suffer from opacity and inconsistency. To address these challenges, we introduce CLASE (Chinese LegAlese Stylistic Evaluation), a hybrid evaluation method that focuses on the stylistic performance of legal text. The method incorporates a hybrid scoring mechanism that combines 1) linguistic feature-based scores and 2) experience-guided LLM-as-a-judge scores. Both the feature coefficients and the LLM scoring experiences are learned from contrastive pairs of authentic legal documents and their LLM-restored counterparts. This hybrid design captures both surface-level features and implicit stylistic norms in a transparent, reference-free manner. Experiments on 200 Chinese legal documents show that CLASE achieves substantially higher alignment with human judgments than traditional metrics and pure LLM-as-a-judge methods. Beyond improved alignment, CLASE provides interpretable score breakdowns and suggestions for improvements, offering a scalable and practical solution for professional stylistic evaluation in legal text generation (Code and data for CLASE is available at: https://github.com/rexera/CLASE).

</details>


### [65] [Beyond Normalization: Rethinking the Partition Function as a Difficulty Scheduler for RLVR](https://arxiv.org/abs/2602.12642)
*Dohyung Kim,Minbeom Kim,Jeonghye Kim,Sangmook Lee,Sojeong Rhee,Kyomin Jung*

Main category: cs.CL

TL;DR: 该研究提出了一种名为PACED-RL的新框架，改进了基于GFlowNet的LLM训练方法，通过利用准确率估计来提升样本效率。


<details>
  <summary>Details</summary>
Motivation: 当前的RL方法虽然能提高LLM的推理能力，但降低了输出的多样性。为了解决这个缺点，研究提出了PACED-RL框架，旨在提升样本效率。

Method: 研究首先建立了分区函数与准确率估计之间的理论关系，然后提出了PACED-RL框架，该框架通过准确率估计优先选择富有信息性的问题提示，并通过准确率估计误差优先的回放进一步提高样本效率。

Result: 该研究在多个基准测试中证明了PACED-RL比GRPO和之前的GFlowNet方法具有更优秀的性能。

Conclusion: PACED-RL框架为LLM更高效的数据匹配训练提供了新的途径，是一个值得探索的方向。

Abstract: Reward-maximizing RL methods enhance the reasoning performance of LLMs, but often reduce the diversity among outputs. Recent works address this issue by adopting GFlowNets, training LLMs to match a target distribution while jointly learning its partition function. In contrast to prior works that treat this partition function solely as a normalizer, we reinterpret it as a per-prompt expected-reward (i.e., online accuracy) signal, leveraging this unused information to improve sample efficiency. Specifically, we first establish a theoretical relationship between the partition function and per-prompt accuracy estimates. Building on this key insight, we propose Partition Function-Guided RL (PACED-RL), a post-training framework that leverages accuracy estimates to prioritize informative question prompts during training, and further improves sample efficiency through an accuracy estimate error-prioritized replay. Crucially, both components reuse information already produced during GFlowNet training, effectively amortizing the compute overhead into the existing optimization process. Extensive experiments across diverse benchmarks demonstrate strong performance improvements over GRPO and prior GFlowNet approaches, highlighting PACED-RL as a promising direction for a more sample efficient distribution-matching training for LLMs.

</details>


### [66] [Learning Ordinal Probabilistic Reward from Preferences](https://arxiv.org/abs/2602.12660)
*Longze Chen,Lu Wang,Renke Shan,Ze Gong,Run Luo,Jiaming Li,Jing Luo,Qiyao Wang,Min Yang*

Main category: cs.CL

TL;DR: 该研究提出了基于概率的奖励模型（PRM），并通过特定的评分离散化方法（OPRM）和区域泛滥调优策略（RgFT）提高了奖励模型的准确性和数据效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法（生成式GRM和判别式DRM）各有不足，如生成式模型需大量点式监督，判别式模型评分不具概率解释性。因此，该研究旨在克服这些限制。

Method: 定义概率奖励模型（PRM），并将其具体化为阶次概率奖励模型（OPRM），进一步提出区域泛滥调优策略（RgFT），利用质量等级注释来引导模型集中概率质量。

Result: 该方法在多项奖励模型基准测试中取得了2.9%~7.4%的准确率提升，显示出强大的性能和数据效率。

Conclusion: 该研究发现概率奖励模型能不仅捕捉到相对排名，还能反映绝对质量，展示了其在奖励建模中的优势。

Abstract: Reward models are crucial for aligning large language models (LLMs) with human values and intentions. Existing approaches follow either Generative (GRMs) or Discriminative (DRMs) paradigms, yet both suffer from limitations: GRMs typically demand costly point-wise supervision, while DRMs produce uncalibrated relative scores that lack probabilistic interpretation. To address these challenges, we introduce a novel reward modeling paradigm: Probabilistic Reward Model (PRM). Instead of modeling reward as a deterministic scalar, our approach treats it as a random variable, learning a full probability distribution for the quality of each response. To make this paradigm practical, we present its closed-form, discrete realization: the Ordinal Probabilistic Reward Model (OPRM), which discretizes the quality score into a finite set of ordinal ratings. Building on OPRM, we propose a data-efficient training strategy called Region Flooding Tuning (RgFT). It enables rewards to better reflect absolute text quality by incorporating quality-level annotations, which guide the model to concentrate the probability mass within corresponding rating sub-regions. Experiments on various reward model benchmarks show that our method improves accuracy by $\textbf{2.9%}\sim\textbf{7.4%}$ compared to prior reward models, demonstrating strong performance and data efficiency. Analysis of the score distribution provides evidence that our method captures not only relative rankings but also absolute quality.

</details>


### [67] [$\mathcal{X}$-KD: General Experiential Knowledge Distillation for Large Language Models](https://arxiv.org/abs/2602.12674)
*Yuang Cai,Yuyu Yuan*

Main category: cs.CL

TL;DR: 提出了一种新的知识蒸馏方法X-KD，旨在使学生模型在教师的原始学习环境中学习，通过联合建模教师的原始奖励函数和策略蒸馏，以保持学生策略与原始奖励函数的一致性。


<details>
  <summary>Details</summary>
Motivation: 现有知识蒸馏方法主要模仿教师行为，但忽略了教师知识形成的原始学习环境。X-KD基于经验学习理论和逆强化学习，旨在解决这一问题。

Method: X-KD采用近似变分奖励模仿学习（AVRIL）框架，同时建模教师的原始奖励函数和执行策略蒸馏，鼓励学生策略与原始奖励函数一致。

Result: 实验结果显示X-KD在摘要生成、机器翻译和算术推理任务上优于通用知识蒸馏和MiniLLM基线，同时提供更好的性能-多样性权衡和数据效率。

Conclusion: X-KD提供了一种简单且灵活的方法，可以应用于序列级和偏差基于的知识蒸馏。

Abstract: Knowledge Distillation (KD) for Large Language Models (LLMs) has become increasingly important as models grow in size and complexity. While existing distillation approaches focus on imitating teacher behavior, they often overlook the original learning environment that shaped the teacher's knowledge. Inspired by the experiential learning theory and inverse reinforcement learning, we propose Experiential Knowledge Distillation ($\mathcal{X}$-KD), a novel and general framework that enables student models to learn in the teacher's original learning environment. $\mathcal{X}$-KD adopts the Approximated Variational Reward Imitation Learning (AVRIL) framework to jointly model the teacher's original reward function and perform policy distillation, encouraging consistency between the student policy and the original reward function. Our derivation demonstrates that $\mathcal{X}$-KD follows the supervised learning framework and applies to both sequence-level and divergence-based distillation methods, underlining the simplicity and flexibility of our approach. Empirical results show that $\mathcal{X}$-KD outperforms the generalized KD and MiniLLM baselines on abstractive summarization, machine translation, and arithmetic reasoning tasks. Additionally, $\mathcal{X}$-KD achieves better performance-diversity trade-off and data efficiency than baseline KD approaches.

</details>


### [68] [ReFilter: Improving Robustness of Retrieval-Augmented Generation via Gated Filter](https://arxiv.org/abs/2602.12709)
*Yixin Chen,Ying Xiong,Shangyu Wu,Xiangrui Ke,Nan Guan,Chun Jason Xue*

Main category: cs.CL

TL;DR: 为了解决检索增强生成在大规模检索时面临的不足，提出了ReFilter，这是一种通过token级别过滤和融合的新型潜空间融合框架，确保在扩展检索规模时保持性能。


<details>
  <summary>Details</summary>
Motivation: 随着知识密集型问题回答中大型语言模型对外部证据需求的增加，如何高效地将检索到的内容融合到LLM中成为一个关键挑战。

Method: ReFilter采用了一个上下文编码器、门控过滤器和token融合模块，实现token级别的权重分配和融合。

Result: ReFilter在四个通用领域QA基准测试中表现出色，且在五个生物医学领域QA基准测试中实现零样本迁移，平均准确率达到70.01%。

Conclusion: ReFilter显著改进了RAG方法在大规模检索条件下的表现，展示了良好的泛化能力。

Abstract: Retrieval-augmented generation (RAG) has become a dominant paradigm for grounding large language models (LLMs) with external evidence in knowledge-intensive question answering. A core design choice is how to fuse retrieved samples into the LLMs, where existing internal fusion approaches broadly fall into query-based fusion, parametric fusion, and latent-based fusion. Despite their effectiveness at modest retrieval scales, these methods often fail to scale gracefully as the number of retrieved candidates k increases: Larger k improves evidence coverage, yet realistic top-k retrieval inevitably contains irrelevant or redundant content and increases the inference cost.
  To address these limitations, we propose ReFilter, a novel latent-based fusion framework that performs token-level filtering and fusion. ReFilter consists of three key components: a context encoder for encoding context features, a gated filter for weighting each token, and a token fusion module for integrating the weighted token feature into the LLM's hidden states. Our experiments across four general-domain QA benchmarks show that ReFilter consistently achieves the best average performance under both in-domain adaptation and out-of-domain transfer. ReFilter further generalizes to five biomedical QA benchmarks in zero-shot transfer without domain fine-tuning, reaching 70.01% average accuracy with Qwen2.5-14B-Instruct.

</details>


### [69] [Lamer-SSL: Layer-aware Mixture of LoRA Experts for Continual Multilingual Expansion of Self-supervised Models without Forgetting](https://arxiv.org/abs/2602.12746)
*Jing Xu,Minglin Wu,Xueyuan Chen,Xixin Wu,Helen Meng*

Main category: cs.CL

TL;DR: Lamer-SSL 提出了一种参数效率高的框架，通过集成 Layer-Aware MixturE of LoRA Experts (Lamer) 模块与回放策略，实现了自我监督语音模型在新语言上的有效扩展，同时保留了之前学习语言的强大性能。


<details>
  <summary>Details</summary>
Motivation: 现有的自监督语音模型在迁移学习时难以泛化到新语言，并在持续训练中容易遗忘之前学过的信息。

Method: Lamer-SSL 框架通过结合 Layer-Aware MixturE of LoRA Experts (Lamer) 模块和回放策略来解决这些问题。Lamer 模块能够灵活平衡共享和语言特定的表示，而层感知专家分配则将更多专家分配给深层层，这些层中包含更丰富的语义信息。回放策略通过最小的数据保留先前的知识，从而在持续训练期间减轻遗忘。

Result: 实证研究显示，Lamer-SSL 框架有效地将自监督模型扩展到了新语言，同时保持了之前学习语言的强大性能，仅需 2.14% 的可训练参数。

Conclusion: Lamer-SSL 通过有效的参数管理策略和智能的模块设计，成功提高了自监督语音模型在多个任务上的泛化能力和稳定性。

Abstract: Despite their impressive performance, self-supervised speech models often struggle to generalize to new languages and tend to forget previously acquired knowledge during continual training. To address this, we propose Lamer-SSL, a parameter-efficient framework that integrates a Layer-Aware MixturE of LoRA Experts (Lamer) module with a replay strategy. The Lamer module enables flexible balancing between shared and language-specific representations, while layer-aware expert allocation assigns more experts to deeper layers where semantic information is richer. Meanwhile, the replay strategy retains prior knowledge using minimal data, mitigating forgetting during continual training. Experiments on automatic speech recognition (ASR) and language identification (LID) demonstrate that Lamer-SSL extends self-supervised models to new languages effectively while maintaining strong performance on previously learned languages with only 2.14% parameters being trainable.

</details>


### [70] [Towards a Diagnostic and Predictive Evaluation Methodology for Sequence Labeling Tasks](https://arxiv.org/abs/2602.12759)
*Elena Alvarez-Mellado,Julio Gonzalo*

Main category: cs.CL

TL;DR: 该研究提出了一种基于错误分析的序列标注任务评估方法，通过少量但语义导向的测试集，提供关于系统改进的定量和定性信息，并预测模型在不同数据分布上的表现。


<details>
  <summary>Details</summary>
Motivation: 当前标准评估方法在NLP中存在局限性，无法提供系统改进的具体指导，且预测外部数据集表现不佳。

Method: 该方法通过设计覆盖广泛语义属性的手工创建的小规模测试集进行错误分析，不依赖于大量真实世界数据的收集。

Result: 研究在英语词素识别基准上展示了该方法的有效性，提供了诊断性、可操作性和预测性结果。

Conclusion: 该评估方法能够帮助识别系统性能中的系统性弱点，指导选择更适合特定场景的模型，并预测模型在外分布数据集上的表现，具有较高的预测准确性。

Abstract: Standard evaluation in NLP typically indicates that system A is better on average than system B, but it provides little info on how to improve performance and, what is worse, it should not come as a surprise if B ends up being better than A on outside data. We propose an evaluation methodology for sequence labeling tasks grounded on error analysis that provides both quantitative and qualitative information on where systems must be improved and predicts how models will perform on a different distribution. The key is to create test sets that, contrary to common practice, do not rely on gathering large amounts of real-world in-distribution scraped data, but consists in handcrafting a small set of linguistically motivated examples that exhaustively cover the range of span attributes (such as shape, length, casing, sentence position, etc.) a system may encounter in the wild. We demonstrate this methodology on a benchmark for anglicism identification in Spanish. Our methodology provides results that are diagnostic (because they help identify systematic weaknesses in performance), actionable (because they can inform which model is better suited for a given scenario) and predictive: our method predicts model performance on external datasets with a median correlation of 0.85.

</details>


### [71] [Aspect-Based Sentiment Analysis for Future Tourism Experiences: A BERT-MoE Framework for Persian User Reviews](https://arxiv.org/abs/2602.12778)
*Hamidreza Kazemi Taskooh,Taha Zare Harofte*

Main category: cs.CL

TL;DR: 本研究提出了一种混合 BERT 基准模型，采用 Top-K路由和辅助损失来解决低资源语言中的 ABSA 挑战，模型在处理伊朗住宿平台 Jabama 的预处理评论时，实现了 90.6% 的加权 F1 分数，显著优于基线 BERT (89.25%) 和标准混合方法 (85.7%)，且能耗降低了 39%，这是一个关于波斯语旅游评论的首例 ABSA 研究，并公开了注释数据集，以促进未来多语言 NLP 研究在旅游领域的进展。


<details>
  <summary>Details</summary>
Motivation: 鉴于低资源语言在 NLP 领域中的挑战，特别是波斯语在旅游领域的使用，研究旨在开发一种新的 ABSA 方法，以促进该语言在旅游评论分析中的应用，同时提高效率并支持可持续发展目标。

Method: 本研究提出了一种混合 BERT 基准模型，采用了 Top-K 路由和辅助损失来缓解路由崩溃，提升了模型的效率和性能。

Result: 该模型在 Jabama 的预处理评论上的加权 F1 分数达到了 90.6%，超过了基线 BERT (89.25%) 和标准混合方法 (85.7%)，并且在能耗上降低了 39%。

Conclusion: 该研究是第一个专注于波斯语旅游评论的 ABSA 研究，展示了在少资源语言中进行评论分析的潜力，并通过公开注释数据集，促进了未来在多语言 NLP 研究中的发展。

Abstract: This study advances aspect-based sentiment analysis (ABSA) for Persian-language user reviews in the tourism domain, addressing challenges of low-resource languages. We propose a hybrid BERT-based model with Top-K routing and auxiliary losses to mitigate routing collapse and improve efficiency. The pipeline includes: (1) overall sentiment classification using BERT on 9,558 labeled reviews, (2) multi-label aspect extraction for six tourism-related aspects (host, price, location, amenities, cleanliness, connectivity), and (3) integrated ABSA with dynamic routing. The dataset consists of 58,473 preprocessed reviews from the Iranian accommodation platform Jabama, manually annotated for aspects and sentiments. The proposed model achieves a weighted F1-score of 90.6% for ABSA, outperforming baseline BERT (89.25%) and a standard hybrid approach (85.7%). Key efficiency gains include a 39% reduction in GPU power consumption compared to dense BERT, supporting sustainable AI deployment in alignment with UN SDGs 9 and 12. Analysis reveals high mention rates for cleanliness and amenities as critical aspects. This is the first ABSA study focused on Persian tourism reviews, and we release the annotated dataset to facilitate future multilingual NLP research in tourism.

</details>


### [72] [RAT-Bench: A Comprehensive Benchmark for Text Anonymization](https://arxiv.org/abs/2602.12806)
*Nataša Krčo,Zexi Yao,Matthieu Meeus,Yves-Alexandre de Montjoye*

Main category: cs.CL

TL;DR: 研究介绍了RAT-Bench基准，评估文本匿名化工具的再识别风险，发现尽管现有工具能力不一，但通过对不同类型标识符的处理，LLM基匿名化工具提供了较好的隐私-实用性 trade-off。


<details>
  <summary>Details</summary>
Motivation: 鉴于大型语言模型（LLMs）越来越多地使用包含个人信息的数据进行训练或查询，以及现有的匿名化工具主要关注去除特定标识符而非防止再识别效果欠佳，本文旨在评估匿名化工具的真实再识别风险。

Method: 使用美国人口统计数据生成包含不同类型标识符的合成文本，覆盖领域、语言和难度级别；评估NLP和LLM基匿名化工具，依据攻击者能从匿名化文本中正确推断的属性计算在美国人口中的再识别风险。

Result: 所有评估的工具在处理直接标识符和间接标识符选择不当方面效果不尽如人意。LLM基匿名化工具总体上提供了更好的隐私-实用性trade-off，但在处理直接标识符时需大量计算资源。

Conclusion: 本文强调了需要改进匿名化工具以更好地处理不同类型的标识符，提出了下一步改进的建议，并承诺将发布基准以鼓励扩展到更多地理区域。

Abstract: Data containing personal information is increasingly used to train, fine-tune, or query Large Language Models (LLMs). Text is typically scrubbed of identifying information prior to use, often with tools such as Microsoft's Presidio or Anthropic's PII purifier. These tools have traditionally been evaluated on their ability to remove specific identifiers (e.g., names), yet their effectiveness at preventing re-identification remains unclear. We introduce RAT-Bench, a comprehensive benchmark for text anonymization tools based on re-identification risk. Using U.S. demographic statistics, we generate synthetic text containing various direct and indirect identifiers across domains, languages, and difficulty levels. We evaluate a range of NER- and LLM-based text anonymization tools and, based on the attributes an LLM-based attacker is able to correctly infer from the anonymized text, we report the risk of re-identification in the U.S. population, while properly accounting for the disparate impact of identifiers. We find that, while capabilities vary widely, even the best tools are far from perfect in particular when direct identifiers are not written in standard ways and when indirect identifiers enable re-identification. Overall we find LLM-based anonymizers, including new iterative anonymizers, to provide a better privacy-utility trade-off albeit at a higher computational cost. Importantly, we also find them to work well across languages. We conclude with recommendations for future anonymization tools and will release the benchmark and encourage community efforts to expand it, in particular to other geographies.

</details>


### [73] [Left-right asymmetry in predicting brain activity from LLMs' representations emerges with their formal linguistic competence](https://arxiv.org/abs/2602.12811)
*Laurent Bonnasse-Gahot,Christophe Pallier*

Main category: cs.CL

TL;DR: 本研究通过比较不同训练阶段的大规模语言模型（LLM）的脑活动预测性和若干基准任务的性能，揭示了左右脑预测性不对称性与形式语言能力的进步相关。


<details>
  <summary>Details</summary>
Motivation: 为了理解大规模语言模型在学习过程中，哪一种语言能力的发展导致了大脑活动预测性左右不对称性的出现。

Method: 使用OLMo-2 7B语言模型及其在不同训练阶段的数据，并结合英语参与者的fMRI数据进行分析；通过考察不同任务表现与左右不对称性的相关性，来识别何种语言能力与其相关。

Result: 研究发现，左右不对称性与其对应表现出了与形式语言能力的提升相关，而与算术或语法无关的任务表现无关。

Conclusion: 该工作表明，大脑活动预测性的左右不对称性与形式语言能力进步高度相关，这可能是大脑语言处理功能发展的机制之一。

Abstract: When humans and large language models (LLMs) process the same text, activations in the LLMs correlate with brain activity measured, e.g., with functional magnetic resonance imaging (fMRI). Moreover, it has been shown that, as the training of an LLM progresses, the performance in predicting brain activity from its internal activations improves more in the left hemisphere than in the right one. The aim of the present work is to understand which kind of competence acquired by the LLMs underlies the emergence of this left-right asymmetry. Using the OLMo-2 7B language model at various training checkpoints and fMRI data from English participants, we compare the evolution of the left-right asymmetry in brain scores alongside performance on several benchmarks. We observe that the asymmetry co-emerges with the formal linguistic abilities of the LLM. These abilities are demonstrated in two ways: by the model's capacity to assign a higher probability to an acceptable sentence than to a grammatically unacceptable one within a minimal contrasting pair, or its ability to produce well-formed text. On the opposite, the left-right asymmetry does not correlate with the performance on arithmetic or Dyck language tasks; nor with text-based tasks involving world knowledge and reasoning. We generalize these results to another family of LLMs (Pythia) and another language, namely French. Our observations indicate that the left-right asymmetry in brain predictivity matches the progress in formal linguistic competence (knowledge of linguistic patterns).

</details>


### [74] [BaziQA-Benchmark: Evaluating Symbolic and Temporally Compositional Reasoning in Large Language Models](https://arxiv.org/abs/2602.12889)
*Jiangxi Chen,Qian Liu*

Main category: cs.CL

TL;DR: BaziQA-Benchmark 为大语言模型评估符号及时间组成推理能力设定了标准，涵盖200个专业精选的问题，包括结构化推理和时间条件交互。


<details>
  <summary>Details</summary>
Motivation: 旨在提供一个标准化基准来客观评估和对比大语言模型在符号和时间组成推理方面的表现。

Method: 基于2021-2025年的Fortune-teller Competition的问题设计基准，采用多个评估模型并在多轮设置下进行评估。

Result: 模型在基准上表现出色，但依然无法达到饱和状态，显示出对时间组成及推理顺序的高敏感性，同时在具体时间定位和多条件符号判断上存在系统性缺陷。

Conclusion: 需进一步改进以增强模型的推理能力和应对复杂时间条件的能力。

Abstract: We present BaziQA-Benchmark, a standardized benchmark for evaluating symbolic and temporally compositional reasoning in large language models. The benchmark is derived from 200 professionally curated, multiple-choice problems from the Global Fortune-teller Competition (2021--2025), where each instance requires structured inference over a fixed symbolic chart and interacting temporal conditions. Unlike anecdotal or prompt-driven evaluations, BaziQA-Benchmark enables objective scoring and controlled comparison across years, domains, and model families. We evaluate contemporary language models under a multi-turn setting and analyze performance variation across temporal difficulty, reasoning domains, and inference protocols.To further probe reasoning behavior, we introduce a lightweight Structured Reasoning Protocol that constrains inference order without adding domain knowledge. Results show that models consistently outperform chance but remain far from saturation, exhibiting pronounced sensitivity to temporal composition and reasoning order, as well as systematic failures on precise temporal localization and multi-condition symbolic judgments.

</details>


### [75] [ViMedCSS: A Vietnamese Medical Code-Switching Speech Dataset & Benchmark](https://arxiv.org/abs/2602.12911)
*Tung X. Nguyen,Nhu Vo,Giang-Son Nguyen,Duy Mai Hoang,Chien Dinh Huynh,Inigo Jauregi Unanue,Massimo Piccardi,Wray Buntine,Dung D. Le*

Main category: cs.CL

TL;DR: 该研究构建了一个34小时的越南语医疗代码转换语音数据集（ViMedCSS），包含16,576个语句，每个语句至少包含一个英语医学术语。通过此数据集评估了多个最先进的ASR模型并探讨了不同的特定微调策略，以改善对英语医疗术语的识别。实验结果显示，优化越南语模型在一般片段上表现更好，而多语言预训练有助于捕捉英语插入。两者结合提供了最佳的整体和代码转换准确性平衡。


<details>
  <summary>Details</summary>
Motivation: 当前的ASR系统在识别越南医疗语境中的英语词时遇到困难，没有针对这一挑战的基准测试。为解决这一问题，研究构建了第一个针对越南医疗代码转换的基准数据集ViMedCSS，并探索了有效的领域适应策略，以提高低资源多语言ASR系统的性能。

Method: 研究通过收集16,576个包含至少一个英语医学术语的越南口头语句，构建了一个独特数据集。研究使用了多个最先进的ASR模型，并探索了多种微调策略，以评估和改进ASR模型在识别英语医疗术语方面的能力。

Result: 研究结果表明，越南语优化模型在一般片段上表现较好，而多语言预训练有助于捕捉英语插入。两者结合的方法在整体和代码转换准确性方面提供了最佳平衡。

Conclusion: 该工作提供了第一个针对越南医疗代码转换的基准数据集，并为低资源多语言ASR系统的有功能域适应提供了见解。

Abstract: Code-switching (CS), which is when Vietnamese speech uses English words like drug names or procedures, is a common phenomenon in Vietnamese medical communication. This creates challenges for Automatic Speech Recognition (ASR) systems, especially in low-resource languages like Vietnamese. Current most ASR systems struggle to recognize correctly English medical terms within Vietnamese sentences, and no benchmark addresses this challenge. In this paper, we construct a 34-hour \textbf{Vi}etnamese \textbf{Med}ical \textbf{C}ode-\textbf{S}witching \textbf{S}peech dataset (ViMedCSS) containing 16,576 utterances. Each utterance includes at least one English medical term drawn from a curated bilingual lexicon covering five medical topics. Using this dataset, we evaluate several state-of-the-art ASR models and examine different specific fine-tuning strategies for improving medical term recognition to investigate the best approach to solve in the dataset. Experimental results show that Vietnamese-optimized models perform better on general segments, while multilingual pretraining helps capture English insertions. The combination of both approaches yields the best balance between overall and code-switched accuracy. This work provides the first benchmark for Vietnamese medical code-switching and offers insights into effective domain adaptation for low-resource, multilingual ASR systems.

</details>


### [76] [When Words Don't Mean What They Say: Figurative Understanding in Bengali Idioms](https://arxiv.org/abs/2602.12921)
*Adib Sakhawat,Shamim Ara Parveen,Md Ruhul Amin,Shamim Al Mahmud,Md Saiful Islam,Tahera Khatun*

Main category: cs.CL

TL;DR: 该研究引入了10,361条孟加拉语成语的新数据集，并通过全面的19字段注释方案进行了文化背景标注，评估了30种最先进的多语言和指令调优的大语言模型在成语理解任务中的表现，揭示了现有模型在跨语言和文化推理方面的局限。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型在低资源语言如孟加拉语的认知语言理解方面存在显著挑战，该研究旨在提供一个大规模、文化基础的数据集，以促进(figurative language)理解的进展。

Method: 研究团队创建了一个包含10,361条成语的数据集，并通过专家共识确立和细化了一个包含19个字段的注释方案，用于捕捉成语的语义、语法、文化与宗教维度。

Result: 在使用这个新数据集进行的基准测试中，评估了30种最先进的多语言和指令调优的大型语言模型，结果显示没有模型能够超过50%的准确率，这远低于人类83.4%的表现。

Conclusion: 研究结果强调了现有模型在跨语言和文化推理方面的局限性，并通过发布新数据集和基准测试，为孟加拉语和其他低资源语言的认知语言理解的进一步发展奠定了基础。

Abstract: Figurative language understanding remains a significant challenge for Large Language Models (LLMs), especially for low-resource languages. To address this, we introduce a new idiom dataset, a large-scale, culturally-grounded corpus of 10,361 Bengali idioms. Each idiom is annotated under a comprehensive 19-field schema, established and refined through a deliberative expert consensus process, that captures its semantic, syntactic, cultural, and religious dimensions, providing a rich, structured resource for computational linguistics. To establish a robust benchmark for Bangla figurative language understanding, we evaluate 30 state-of-the-art multilingual and instruction-tuned LLMs on the task of inferring figurative meaning. Our results reveal a critical performance gap, with no model surpassing 50% accuracy, a stark contrast to significantly higher human performance (83.4%). This underscores the limitations of existing models in cross-linguistic and cultural reasoning. By releasing the new idiom dataset and benchmark, we provide foundational infrastructure for advancing figurative language understanding and cultural grounding in LLMs for Bengali and other low-resource languages.

</details>


### [77] [Curriculum Learning and Pseudo-Labeling Improve the Generalization of Multi-Label Arabic Dialect Identification Models](https://arxiv.org/abs/2602.12937)
*Ali Mekky,Mohamed El Zeftawy,Lara Hassan,Amr Keleg,Preslav Nakov*

Main category: cs.CL

TL;DR: 本文介绍了将阿拉伯方言识别（ADI）从单标签分类任务转变为多标签分类任务的研究。通过使用GPT-4o和二元方言可接受性分类器自动生成多标签注释，并使用基于阿拉伯方言水平（ALDi）的排序进行聚合，进而开发了一个新的BERT多标签分类器，该模型在MLADI排行榜上取得了显著的 Macro F1 分数。


<details>
  <summary>Details</summary>
Motivation: 近年来，有研究表明阿拉伯方言识别（ADI）任务应以多标签分类任务的形式进行。本文通过使用基于GPT-4o和二元方言接受性分类器的自动多标签注释生成方法以及基于专家系统的方法，试图解决将单一标签数据重新利用于多标签阿拉伯方言识别（MLADI）任务中的问题。

Method: 文章的方法包括：1) 使用GPT-4o和二元分类器生成多标签注释；2) 使用阿拉伯方言水平（ALDi）指导注释聚合策略；3) 应用基于复杂度和标签数量的课程学习策略对BERT进行训练。

Result: 本文的方法和模型在MLADI基准测试上取得了显著的结果，其最佳的LAHJATBERT模型的Macro F1得分为0.69，相较于之前最强大的系统提升了25.45%。

Conclusion: 该研究通过改进多标签阿拉伯方言识别数据集的构建方法，并使用特定的学习策略改进了模型性能，显示出多标签分类方法在阿拉伯方言识别任务中的潜力。

Abstract: Being modeled as a single-label classification task for a long time, recent work has argued that Arabic Dialect Identification (ADI) should be framed as a multi-label classification task. However, ADI remains constrained by the availability of single-label datasets, with no large-scale multi-label resources available for training. By analyzing models trained on single-label ADI data, we show that the main difficulty in repurposing such datasets for Multi-Label Arabic Dialect Identification (MLADI) lies in the selection of negative samples, as many sentences treated as negative could be acceptable in multiple dialects. To address these issues, we construct a multi-label dataset by generating automatic multi-label annotations using GPT-4o and binary dialect acceptability classifiers, with aggregation guided by the Arabic Level of Dialectness (ALDi). Afterward, we train a BERT-based multi-label classifier using curriculum learning strategies aligned with dialectal complexity and label cardinality. On the MLADI leaderboard, our best-performing LAHJATBERT model achieves a macro F1 of 0.69, compared to 0.55 for the strongest previously reported system. Code and data are available at https://mohamedalaa9.github.io/lahjatbert/.

</details>


### [78] [ProbeLLM: Automating Principled Diagnosis of LLM Failures](https://arxiv.org/abs/2602.12966)
*Yue Huang,Zhengzhe Jiang,Yuchen Ma,Yu Jiang,Xiangqi Wang,Yujun Zhou,Yuexing Hao,Kehan Guo,Pin-Yu Chen,Stefan Feuerriegel,Xiangliang Zhang*

Main category: cs.CL

TL;DR: 提出了一种名为ProbeLLM的自适应探针框架，该框架能够发现更大范围且更精细的模型弱点，并将其分为可解释的失败模式，从而从孤立的失败案例转向结构化的失败模式发现。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）的快速发展，静态评估已无法跟上模型的变化。现有的自动化探针方法往往只能发现孤立的失败案例，缺乏探索控制的原则，并且对模型弱点的结构洞察有限。为了解决这些问题，提出了ProbeLLM框架。

Method: ProbeLLM通过将探针视为分层的蒙特卡洛树搜索来运作，分配有限的探针预算用于新的失败区域的全局探索和重复错误模式的局部细化。此外，通过限制探针仅作用于可验证的测试用例，并结合工具增强的生成和验证，ProbeLLM将失败发现建立在可靠的证据之上，通过失败感知嵌入和边界感知归纳进一步将发现的失败总结为可解释的失败模式。

Result: 在多种基准和大型语言模型中，ProbeLLM揭示了比传统基准和先前的自动方法更为广泛、清晰和精细的失败景观，支持了从案例为中心的评估向原理性的弱点发现的转变。

Conclusion: ProbeLLM增强了大型语言模型弱点的发现，为更深入理解和改进这些模型提供了有效的方法。

Abstract: Understanding how and why large language models (LLMs) fail is becoming a central challenge as models rapidly evolve and static evaluations fall behind. While automated probing has been enabled by dynamic test generation, existing approaches often discover isolated failure cases, lack principled control over exploration, and provide limited insight into the underlying structure of model weaknesses. We propose ProbeLLM, a benchmark-agnostic automated probing framework that elevates weakness discovery from individual failures to structured failure modes. ProbeLLM formulates probing as a hierarchical Monte Carlo Tree Search, explicitly allocating limited probing budgets between global exploration of new failure regions and local refinement of recurring error patterns. By restricting probing to verifiable test cases and leveraging tool-augmented generation and verification, ProbeLLM grounds failure discovery in reliable evidence. Discovered failures are further consolidated into interpretable failure modes via failure-aware embeddings and boundary-aware induction. Across diverse benchmarks and LLMs, ProbeLLM reveals substantially broader, cleaner, and more fine-grained failure landscapes than static benchmarks and prior automated methods, supporting a shift from case-centric evaluation toward principled weakness discovery.

</details>


### [79] [Evaluating the Homogeneity of Keyphrase Prediction Models](https://arxiv.org/abs/2602.12989)
*Maël Houbre,Florian Boudin,Beatrice Daille*

Main category: cs.CL

TL;DR: 本文介绍了一种评估Keyword Prediction模型同质性的方法，并发现与生成模型相比，传统的Keyword Extraction方法在同质性方面更具竞争力，生成模型中的Absent Keyphrase能力反而可能不利于模型同质性。


<details>
  <summary>Details</summary>
Motivation: 当前的基准测试尚未涵盖Keyphrase预测模型的同质性问题，本文旨在填补这一空白，通过引入一种评估方法来研究Keyword Generation模型中的Absent Keyphrase能力是否有助于提升模型同质性。

Method: 作者设计了一种新的评估方法，利用两个相似主题的文档集合，通过对比传统的Keyword Extraction模型和生成模型的表现，分析了不同模型在预测摘要关键字的有效性和同质性。

Result: 实验结果表明，尽管生成模型可以预测文档中未出现的关键词（Absent Keyphrase），但传统的Keyword Extraction模型在提升模型同质性方面表现更佳。生成模型中的Absent Keyphrase能力可能对模型同质性产生负面影响。

Conclusion: 本文的研究结果挑战了关于生成模型在同质性方面的优越性，强调了传统Keyword Extraction方法在某些情况下依旧具有竞争力，同时也指出生成模型需要特别注意其生成能力可能带来的负面影响。

Abstract: Keyphrases which are useful in several NLP and IR applications are either extracted from text or predicted by generative models. Contrarily to keyphrase extraction approaches, keyphrase generation models can predict keyphrases that do not appear in a document's text called `absent keyphrases`. This ability means that keyphrase generation models can associate a document to a notion that is not explicitly mentioned in its text. Intuitively, this suggests that for two documents treating the same subjects, a keyphrase generation model is more likely to be homogeneous in their indexing i.e. predict the same keyphrase for both documents, regardless of those keyphrases appearing in their respective text or not; something a keyphrase extraction model would fail to do. Yet, homogeneity of keyphrase prediction models is not covered by current benchmarks. In this work, we introduce a method to evaluate the homogeneity of keyphrase prediction models and study if absent keyphrase generation capabilities actually help the model to be more homogeneous. To our surprise, we show that keyphrase extraction methods are competitive with generative models, and that the ability to generate absent keyphrases can actually have a negative impact on homogeneity. Our data, code and prompts are available on huggingface and github.

</details>


### [80] [Know More, Know Clearer: A Meta-Cognitive Framework for Knowledge Augmentation in Large Language Models](https://arxiv.org/abs/2602.12996)
*Hao Chen,Ye He,Yuchun Fan,Yukun Yan,Zhenghao Liu,Qingfu Zhu,Maosong Sun,Wanxiang Che*

Main category: cs.CL

TL;DR: 本文提出了一种新的元认知框架，通过差异化干预和对齐来增强知识的可靠性，利用内部认知信号划分知识空间，引入认知一致性机制确保主观确定性与客观准确性同步。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常以模型性能等同于内部知识为基础，忽视了知识置信度差距导致的过自信错误或不确定真相。该研究旨在通过提出一种新的元认知框架，解决这一问题。

Method: 该框架利用内部认知信号将知识空间划分为掌握、困惑和缺失区域，并指导有针对性的知识扩展。同时引入认知一致性机制，以同步主观确定性和客观准确性。

Result: 实验结果表明，该框架在多个方面超越了强基准，不仅增强了知识能力，还培养了更好的区分已知和未知的认知行为。

Conclusion: 该研究提出的方法在提高模型可靠性、解决知识置信度差距方面表现出色，为大型语言模型的知识增强提供了新思路。

Abstract: Knowledge augmentation has significantly enhanced the performance of Large Language Models (LLMs) in knowledge-intensive tasks. However, existing methods typically operate on the simplistic premise that model performance equates with internal knowledge, overlooking the knowledge-confidence gaps that lead to overconfident errors or uncertain truths. To bridge this gap, we propose a novel meta-cognitive framework for reliable knowledge augmentation via differentiated intervention and alignment. Our approach leverages internal cognitive signals to partition the knowledge space into mastered, confused, and missing regions, guiding targeted knowledge expansion. Furthermore, we introduce a cognitive consistency mechanism to synchronize subjective certainty with objective accuracy, ensuring calibrated knowledge boundaries. Extensive experiments demonstrate the our framework consistently outperforms strong baselines, validating its rationality in not only enhancing knowledge capabilities but also fostering cognitive behaviors that better distinguish knowns from unknowns.

</details>


### [81] [Can we trust AI to detect healthy multilingual English speakers among the cognitively impaired cohort in the UK? An investigation using real-world conversational speech](https://arxiv.org/abs/2602.13047)
*Madhurananda Pahar,Caitlin Illingworth,Dorota Braun,Bahman Mirheidari,Lise Sproson,Daniel Blackburn,Heidi Christensen*

Main category: cs.CL

TL;DR: 本研究评估了AI模型在识别认知受损群体中的准确性，发现无显著偏见，但多语言使用者在记忆、流畅性和阅读任务中表现出更大的偏见，尤其是当模型使用公开数据集训练时。此外，多语言使用者更有可能被错误分类为认知减退。


<details>
  <summary>Details</summary>
Motivation: 鉴于英国亚裔和黑人社区痴呆症发病率的预期增长，研究旨在评估AI模型在检测认知受损人群中健康多语言英语说话者的可靠性，确保工具的临床实用性。

Method: 实验中，全国招募了单语言参与者，并从谢菲尔德和布拉德福德四个社区中心招募了四种方言的多语言讲者（索马里语、汉语、南亚语言）。研究主要通过ASR系统和分类回归模型使用声学和语言特征评估偏见情况。

Result: 虽然ASR系统未表现出显著偏见，但分类和回归模型在记忆、流畅性和阅读任务中对多语言使用者表现出明显偏见，特别是在使用公开数据集DementiaBank时。多语言使用者更容易被误分类为认知减退。

Conclusion: 研究指出，尽管这些AI模型整体表现良好，但在理解和诊断来自少数民族背景的多语言个体方面的偏见需要被解决。未来研究将致力于开发更为普遍的、减少偏见的模型。

Abstract: Conversational speech often reveals early signs of cognitive decline, such as dementia and MCI. In the UK, one in four people belongs to an ethnic minority, and dementia prevalence is expected to rise most rapidly among Black and Asian communities. This study examines the trustworthiness of AI models, specifically the presence of bias, in detecting healthy multilingual English speakers among the cognitively impaired cohort, to make these tools clinically beneficial. For experiments, monolingual participants were recruited nationally (UK), and multilingual speakers were enrolled from four community centres in Sheffield and Bradford. In addition to a non-native English accent, multilinguals spoke Somali, Chinese, or South Asian languages, who were further divided into two Yorkshire accents (West and South) to challenge the efficiency of the AI tools thoroughly. Although ASR systems showed no significant bias across groups, classification and regression models using acoustic and linguistic features exhibited bias against multilingual speakers, particularly in memory, fluency, and reading tasks. This bias was more pronounced when models were trained on the publicly available DementiaBank dataset. Moreover, multilinguals were more likely to be misclassified as having cognitive decline. This study is the first of its kind to discover that, despite their strong overall performance, current AI models show bias against multilingual individuals from ethnic minority backgrounds in the UK, and they are also more likely to misclassify speakers with a certain accent (South Yorkshire) as living with a more severe cognitive decline. In this pilot study, we conclude that the existing AI tools are therefore not yet reliable for diagnostic use in these populations, and we aim to address this in future work by developing more generalisable, bias-mitigated models.

</details>


### [82] [Exploring a New Competency Modeling Process with Large Language Models](https://arxiv.org/abs/2602.13084)
*Silin Du,Manqing Xin,Raymond Jia Wang*

Main category: cs.CL

TL;DR: 本研究提出了一种基于大语言模型的新型胜任力建模过程，通过结构化的计算组件重构建性流程，自动化提取和映射行为及心理描述，并采用可学习参数整合信息源以提高准确性。论文通过一项实证研究，证明了该方法在软件外包公司中的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统的人力资源管理中，基于专家判断的胜任力建模方法成本高昂且容易出现偏差，因此亟需一种新的自动化算法流程来改善。

Method: 本研究所提出的方法利用大语言模型从原始文本中提取行为和心理描述，并通过嵌入基础相似度映射到预定义的胜任力库。同时，引入了一个可学习的参数来动态整合不同信息源，以确定行为和心理信号的重要性。为了解决验证难题，研究开发了一种离线评估程序，允许系统性模型选择而无需额外的大规模数据收集。

Result: 研究结果表明，该方法在实际软件外包公司中的应用具有强大的预测效度、跨库一致性以及结构稳健性。

Conclusion: 本研究展示了一种基于大语言模型的透明、数据驱动且可评价的胜任力建模框架，相较于传统方法，提供了更为可靠的分析过程。

Abstract: Competency modeling is widely used in human resource management to select, develop, and evaluate talent. However, traditional expert-driven approaches rely heavily on manual analysis of large volumes of interview transcripts, making them costly and prone to randomness, ambiguity, and limited reproducibility. This study proposes a new competency modeling process built on large language models (LLMs). Instead of merely automating isolated steps, we reconstruct the workflow by decomposing expert practices into structured computational components. Specifically, we leverage LLMs to extract behavioral and psychological descriptions from raw textual data and map them to predefined competency libraries through embedding-based similarity. We further introduce a learnable parameter that adaptively integrates different information sources, enabling the model to determine the relative importance of behavioral and psychological signals. To address the long-standing challenge of validation, we develop an offline evaluation procedure that allows systematic model selection without requiring additional large-scale data collection. Empirical results from a real-world implementation in a software outsourcing company demonstrate strong predictive validity, cross-library consistency, and structural robustness. Overall, our framework transforms competency modeling from a largely qualitative and expert-dependent practice into a transparent, data-driven, and evaluable analytical process.

</details>


### [83] [Towards interpretable models for language proficiency assessment: Predicting the CEFR level of Estonian learner texts](https://arxiv.org/abs/2602.13102)
*Kais Allkivi*

Main category: cs.CL

TL;DR: 该研究通过分析真实的语言学习者作文来分类爱沙尼亚水平考试写作（A2-C1级），旨在通过仔细选择特征来构建更可解释和泛化的机器学习模型。研究发现，某些选定特征在分类准确性和稳定性方面表现出与常规特征相当的效果，作文复杂度在七年到十年内有所提高。


<details>
  <summary>Details</summary>
Motivation: 当前研究旨在填补将自动评估与第二语言生产研究相结合的文献空白，探索是否可以利用自然语言处理技术提高语言测试的自动化和解释能力。

Method: 研究采用了自然语言处理技术，首先对爱沙尼亚水平考试写作（A2-C1级）进行特征分析，选择与复杂性和正确性相关的语言特征（如词汇、形态学、表层特征和错误特征），训练分类模型，并对比其他特征模型的效果。研究还对更早的考试样本进行了评估，以考察作文复杂度的变化。

Result: 选定的特征在分类准确性和结果稳定性方面表现良好，最佳分类器达到约0.9的准确率。即便作文复杂度在七年到十年间有所提高，使用某些特征集，准确率仍能达到0.8。研究成果已被应用于爱沙尼亚开源语言学习环境中的写作评价模块。

Conclusion: 该研究证明了通过选择相关特征可以构建有效的机器学习模型用于语言测试评估，并且这种技术在一定程度上能够反映出第二语言作文的复杂性变化。

Abstract: Using NLP to analyze authentic learner language helps to build automated assessment and feedback tools. It also offers new and extensive insights into the development of second language production. However, there is a lack of research explicitly combining these aspects. This study aimed to classify Estonian proficiency examination writings (levels A2-C1), assuming that careful feature selection can lead to more explainable and generalizable machine learning models for language testing. Various linguistic properties of the training data were analyzed to identify relevant proficiency predictors associated with increasing complexity and correctness, rather than the writing task. Such lexical, morphological, surface, and error features were used to train classification models, which were compared to models that also allowed for other features. The pre-selected features yielded a similar test accuracy but reduced variation in the classification of different text types. The best classifiers achieved an accuracy of around 0.9. Additional evaluation on an earlier exam sample revealed that the writings have become more complex over a 7-10-year period, while accuracy still reached 0.8 with some feature sets. The results have been implemented in the writing evaluation module of an Estonian open-source language learning environment.

</details>


### [84] [From sunblock to softblock: Analyzing the correlates of neology in published writing and on social media](https://arxiv.org/abs/2602.13123)
*Maria Ryskina,Matthew R. Gormley,Kyle Mahowald,David R. Mortensen,Taylor Berg-Kirkpatrick,Vivek Kulkarni*

Main category: cs.CL

TL;DR: 研究扩展了早期关于英语词汇演变的工作，在不同的语境嵌入和静态嵌入的基础上分析了新词形成，发现两个领域的一致性，但推特上的现象可能有所不同，这可能与其不同的新词形成机制有关。


<details>
  <summary>Details</summary>
Motivation: 探讨不同语言使用环境下新词形成的不同压力和机制

Method: 将先前关于英语词汇演变的研究方法扩展至包括语境嵌入的新方法，并应用这些方法分析来自推特的数据。

Result: 结果显示新词形成的因素在两种环境下保持一致，但在推特上的增长因素可能不如出版物中的显著。

Conclusion: 研究推断不同的语言使用环境可能促进新词形成的不同机制，这些机制在特定语境下更加重要。

Abstract: Living languages are shaped by a host of conflicting internal and external evolutionary pressures. While some of these pressures are universal across languages and cultures, others differ depending on the social and conversational context: language use in newspapers is subject to very different constraints than language use on social media. Prior distributional semantic work on English word emergence (neology) identified two factors correlated with creation of new words by analyzing a corpus consisting primarily of historical published texts (Ryskina et al., 2020, arXiv:2001.07740). Extending this methodology to contextual embeddings in addition to static ones and applying it to a new corpus of Twitter posts, we show that the same findings hold for both domains, though the topic popularity growth factor may contribute less to neology on Twitter than in published writing. We hypothesize that this difference can be explained by the two domains favouring different neologism formation mechanisms.

</details>


### [85] [OpenLID-v3: Improving the Precision of Closely Related Language Identification -- An Experience Report](https://arxiv.org/abs/2602.13139)
*Mariia Fedorova,Nikolay Arefyev,Maja Buljan,Jindřich Helcl,Stephan Oepen,Egil Rønningstad,Yves Scherrer*

Main category: cs.CL

TL;DR: 该研究通过扩展OpenLID分类器，增加了更多训练数据，合并了语言变体集群，并引入了标记噪声的特殊标签，旨在提高低资源语言的识别精度，同时开发了新的评估数据集。


<details>
  <summary>Details</summary>
Motivation: 现有LID工具在识别紧密相关语言和区分有效自然语言与噪音方面表现不佳，导致语言特定子集受污染，特别是一些低资源语言。

Method: 研究通过增加训练数据、合并语言变体集群，并引入标记噪声的特殊标签来扩展OpenLID分类器。此外，针对紧密相关语言组别（波斯尼亚语、克罗地亚语和塞尔维亚语；意大利北部和法国南部的罗马语种；及斯堪的纳维亚语言）开发了新的评估数据集。

Result: 虽然集成方法提高了精度，但也显著降低了低资源语言的覆盖率。研究结果表明OpenLID-v3在多基准测试中表现出色，并已发布。

Conclusion: OpenLID-v3已被证明在识别紧密相关语言和降低噪音污染方面优于现有工具，如GlotLID，并且已被公开发布。

Abstract: Language identification (LID) is an essential step in building high-quality multilingual datasets from web data. Existing LID tools (such as OpenLID or GlotLID) often struggle to identify closely related languages and to distinguish valid natural language from noise, which contaminates language-specific subsets, especially for low-resource languages. In this work we extend the OpenLID classifier by adding more training data, merging problematic language variant clusters, and introducing a special label for marking noise. We call this extended system OpenLID-v3 and evaluate it against GlotLID on multiple benchmarks. During development, we focus on three groups of closely related languages (Bosnian, Croatian, and Serbian; Romance varieties of Northern Italy and Southern France; and Scandinavian languages) and contribute new evaluation datasets where existing ones are inadequate. We find that ensemble approaches improve precision but also substantially reduce coverage for low-resource languages. OpenLID-v3 is available on https://huggingface.co/HPLT/OpenLID-v3.

</details>


### [86] [Semantic Chunking and the Entropy of Natural Language](https://arxiv.org/abs/2602.13194)
*Weishun Zhong,Doron Sivan,Tankut Can,Mikhail Katkov,Misha Tsodyks*

Main category: cs.CL

TL;DR: 本文介绍了一个统计模型，该模型试图捕捉自然语言复杂的多层次结构，旨在量化冗余水平。该模型预测的熵率与印刷英文字母的估计熵率一致，并展示了熵率随语料库语义复杂性增加而增加。


<details>
  <summary>Details</summary>
Motivation: 现代大型语言模型虽然已经能够达到印刷英语的熵率，但仍未能完全理解自然语言中的冗余层面。本文旨在通过引入一个基于第一原理的统计模型，来更深入地解析这一冗余水平，该模型可以捕捉文本的自相似分段，从而实现对语义层次结构的逐级分解。

Method: 文章提出了一种统计模型，通过文本的自相似分段方法将文本分解成具有语义连贯性的块，直至单个单词层。这种模型允许逐级分解文本的语义结构，并通过与现代大型语言模型和公开数据集的数值实验，验证了模型的效用。

Result: 实验表明，该模型能够定性地捕捉实际文本在不同语义层级结构上的复杂性。同时，模型预测的熵率与印刷英语的估计熵率相符，进一步证明了自然语言的熵率并非固定值，而是会随着语料库的语义复杂性增加而增加。

Conclusion: 该模型为理解自然语言的复杂结构提供了新的视角，并揭示了自然语言中熵率随语义复杂性增加而变化的新现象，为未来研究自然语言的冗余性提供了理论支持。

Abstract: The entropy rate of printed English is famously estimated to be about one bit per character, a benchmark that modern large language models (LLMs) have only recently approached. This entropy rate implies that English contains nearly 80 percent redundancy relative to the five bits per character expected for random text. We introduce a statistical model that attempts to capture the intricate multi-scale structure of natural language, providing a first-principles account of this redundancy level. Our model describes a procedure of self-similarly segmenting text into semantically coherent chunks down to the single-word level. The semantic structure of the text can then be hierarchically decomposed, allowing for analytical treatment. Numerical experiments with modern LLMs and open datasets suggest that our model quantitatively captures the structure of real texts at different levels of the semantic hierarchy. The entropy rate predicted by our model agrees with the estimated entropy rate of printed English. Moreover, our theory further reveals that the entropy rate of natural language is not fixed but should increase systematically with the semantic complexity of corpora, which are captured by the only free parameter in our model.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [87] [Design Environment of Quantization-Aware Edge AI Hardware for Few-Shot Learning](https://arxiv.org/abs/2602.12295)
*R. Kanda,N. Onizawa,M. Leonardon,V. Gripon,T. Hanyu*

Main category: cs.AR

TL;DR: 本研究通过在预训练和评估阶段实施定点数据处理，利用Brevitas量化模块，并使用Tensil硬件实施QAT和PTQ方法，展示了定点数据处理在边缘AI硬件中实现少样本学习时即使整数和小数部分位宽降低到6位或5位，仍能保持与浮点运算相近的准确度。


<details>
  <summary>Details</summary>
Motivation: 研究的动机在于确保边缘AI硬件在少样本学习中的整个设计流程中保持高精度一致性，以及探索定点数据处理在优化资源使用方面的能力。

Method: 研究中采用的方法包括使用定点数据处理模块Brevitas，并在该模块下应用量化感知训练（QAT）和后训练量化（PTQ）方法。同时，研究还使用了Tensil作为当前设计流程的一部分来实施这一方法。

Result: 实验结果表明，即使将整数和小数部分位宽降低到6位或5位，定点数据处理也能维持与浮点运算相近的准确性。这显示出进一步降低计算资源的潜力。

Conclusion: 研究表明，通过定点数据处理可以实现边缘AI硬件中少样本学习的良好性能，提供了一个灵活的设计和评估环境。

Abstract: This study aims to ensure consistency in accuracy throughout the entire design flow in the implementation of edge AI hardware for few-shot learning, by implementing fixed-point data processing in the pre-training and evaluation phases. Specifically, the quantization module, called Brevitas, is applied to implement fixed-point data processing, which allows for arbitrary specification of the bit widths for the integer and fractional parts. Two methods of fixed-point data quantization, quantization-aware training (QAT) and post-training quantization (PTQ), are utilized in Brevitas. With Tensil, which is used in the current design flow, the bit widths of the integer and fractional parts need to be 8 bits each or 16 bits each when implemented in hardware, but performance validation has shown that accuracy comparable to floating-point operations can be maintained even with 6 bits or 5 bits each, indicating potential for further reduction in computational resources. These results clearly contribute to the creation of a versatile design and evaluation environment for edge AI hardware for few-shot learning.

</details>


### [88] [CacheMind: From Miss Rates to Why -- Natural-Language, Trace-Grounded Reasoning for Cache Replacement](https://arxiv.org/abs/2602.12422)
*Kaushal Mhapsekar,Azam Ghanbari,Bita Aslrousta,Samira Mirbagher-Ajorpaz*

Main category: cs.AR

TL;DR: CacheMind is an innovative tool that utilizes Retrieval-Augmented Generation and Large Language Models for interactive, semantic reasoning over cache traces, significantly enhancing cache performance analysis. It achieves high accuracy in trace-grounded reasoning and demonstrates substantial utility in real-world applications.


<details>
  <summary>Details</summary>
Motivation: 当前CPU微体系结构中，缓存替换策略的优化依赖于手工设计的启发式方法，这限制了缓存性能。现有的分析方法需要手动解析大量的跟踪条目，处理效率低下且不具互动性。为解决这一问题，本文提出CacheMind，这是一种利用检索增强生成（RAG）和大型语言模型（LLMs）的工具，使架构师能够通过自然语言问问题，如'与PC X相关的内存访问为什么会导致更多的驱逐？'，并收到带有程序语义支持的细节回答，这是一个前所未有的功能。

Method: CacheMind采用RAG模式，整合了检索与生成能力，使其能够提取出与复杂问题相关的缓存跟踪数据，并生成具有针对性和解释性的回答。通过使用不同的检索器（SIEVE或RANGER），CacheMind能够根据缓存踪迹和特定策略问题，提供精确的解答。

Result: 使用CacheMind，验证了其在缓存追踪和策略特定推理任务中的高效性和准确性。具体而言，CacheMind在通过SIEVE从75个未见的缓存追踪问题中成功回答了49个，准确率为66.67%；在通过RANGER完成的25个未见策略特定推理任务中成功回答了21个，准确率为84.80%。此外，使用RANGER，CacheMind在CacheMindBench的追踪级别中4个类别中达到了100%的准确率。相比之下，LlamaIndex仅在10%的检索中成功，SIEVE提高了60%，而RANGER达到了90%，表明现有RAG对于精确的微体系结构推理能力不足。

Conclusion: CacheMind展示出在复杂的缓存分析任务中使用自然语言接口的潜力，明显改善了缓存替换策略的优化。提供的具体应用案例展示了其在提高缓存命中率、加速软件修复以及优化替换策略方面具有显著的实际价值。

Abstract: Cache replacement remains a challenging problem in CPU microarchitecture, often addressed using hand-crafted heuristics, limiting cache performance. Cache data analysis requires parsing millions of trace entries with manual filtering, making the process slow and non-interactive. To address this, we introduce CacheMind, a conversational tool that uses Retrieval-Augmented Generation (RAG) and Large Language Models (LLMs) to enable semantic reasoning over cache traces. Architects can now ask natural language questions like, "Why is the memory access associated with PC X causing more evictions?", and receive trace-grounded, human-readable answers linked to program semantics for the first time. To evaluate CacheMind, we present CacheMindBench, the first verified benchmark suite for LLM-based reasoning for the cache replacement problem. Using the SIEVE retriever, CacheMind achieves 66.67% on 75 unseen trace-grounded questions and 84.80% on 25 unseen policy-specific reasoning tasks; with RANGER, it achieves 89.33% and 64.80% on the same evaluations. Additionally, with RANGER, CacheMind achieves 100% accuracy on 4 out of 6 categories in the trace-grounded tier of CacheMindBench. Compared to LlamaIndex (10% retrieval success), SIEVE achieves 60% and RANGER achieves 90%, demonstrating that existing Retrieval-Augmented Generation (RAGs) are insufficient for precise, trace-grounded microarchitectural reasoning. We provided four concrete actionable insights derived using CacheMind, wherein bypassing use case improved cache hit rate by 7.66% and speedup by 2.04%, software fix use case gives speedup of 76%, and Mockingjay replacement policy use case gives speedup of 0.7%; showing the utility of CacheMind on non-trivial queries that require a natural-language interface.

</details>


### [89] [MXFormer: A Microscaling Floating-Point Charge-Trap Transistor Compute-in-Memory Transformer Accelerator](https://arxiv.org/abs/2602.12480)
*George Karfakis,Samyak Chakrabarty,Vinod Kurian Jacob,Siyun Qiao,Subramanian S. Iyer,Sudhakar Pamarti,Puneet Gupta*

Main category: cs.AR

TL;DR: MXFormer提出了一种基于Compute-in-Memory (CIM)的混合加速器，通过在芯片上高效存储和处理参数，显著提高了大型短序列Transformer的推理性能，相比其他类型的加速器在计算密度和能源效率上表现更优。


<details>
  <summary>Details</summary>
Motivation: 缓解Transformer模型部署中计算与内存需求之间的矛盾，提高固定模型在大型短序列Transformer上的推理效率。

Method: MXFormer采用超密集的Charge-Trap Transistors (CTTs)和Microscaling MXFP4 CIM阵列，通过静态分区设计和深管道化数据流，利用高度并行的模拟CTT数组处理静态权重层，并用MXFP加速器处理动态计算。

Result: MXFormer架构在ViT-L/32和ViT-B/16上分别实现了58275 FPS和41269 FPS的单流吞吐量和效率，并且相比传统的非FWS数字、混合和光子Transformer加速器在计算密度和能源效率上分别提升了3.3-60.5倍和1.7-2.5倍。

Conclusion: MXFormer在无需模型再训练的情况下，实现了近数字级的精度，并且在计算密度和驻留权重存储密度方面均有所提升，证明了其在Transformer模型加速中的优越性。

Abstract: The proliferation of Transformer models is often constrained by the significant computational and memory bandwidth demands of deployment. To address this, we present MXFormer, a novel, hybrid, weight-stationary Compute-in-Memory (CIM) accelerator that provides high throughput and efficiency for fixed-model inference on large short-sequence Transformers. Our architecture's foundation is the use of ultra-dense Charge-Trap Transistors (CTTs) in Microscaling MXFP4 CIM arrays, uniquely enabling the on-chip storage of up to hundreds of millions of parameters in Fully Weight Stationary (FWS) fashion.
  We introduce a statically partitioned design with 12 Transformer blocks connected by a deeply pipelined dataflow. Static-weight layers (MLPs and linear projections) execute on highly parallel analog CTT arrays using an MXFP4-native flow with per-block exponent alignment and a 10-bit SAR ADC. Dynamic computations are handled in fully accurate digital blocks that utilize MXFP-enabled systolic arrays for scaled dot-product attention and vector units for LayerNorm and FlashAttention-style Softmax.
  By eliminating all weight movement, the deeply pipelined MXFormer architecture yields very high single-stream throughput and efficiency, processing 58275 FPS on ViT-L/32 (dual-chip) or 41269 FPS on ViT-B/16 (single chip). MXFormer outperforms comparable state-of-the-art non-FWS digital, hybrid and photonic Transformer accelerators ~3.3x-60.5x in compute density and ~1.7x-2.5x in energy efficiency. Against FWS accelerators, MXFormer improves compute density by ~20.9x and resident weight storage density by ~2x, while preserving near-digital accuracy (drop of <1%) without any model retraining.

</details>


### [90] [TriGen: NPU Architecture for End-to-End Acceleration of Large Language Models based on SW-HW Co-Design](https://arxiv.org/abs/2602.12962)
*Jonghun Lee,Junghoon Lee,Hyeonjin Kim,Seoho Jeon,Jisup Yoon,Hyunbin Park,Meejeong Park,Heonjae Ha*

Main category: cs.AR

TL;DR: TriGen 是一种针对资源受限环境设计的新型 NPU 架构，通过软件硬件协同设计，采用微缩精度（MX）计算和查找表（LUT）加速非线性操作，结合调度技术优化计算利用率，并在多种 LLM 上验证了显著的性能提升和内存传输减少。


<details>
  <summary>Details</summary>
Motivation: 面对近期 NPU 架构在AI推理加速中的广泛应用，以及大型语言模型（LLM）的模型规模快速增长但参数重用较低的挑战，现提出 TriGen 架构旨在提供高效的解决方案。

Method: TriGen 在低精度计算（MX）基础上实现，采用查找表（LUT）加速关键非线性操作，并结合调度技术以最大程度地利用计算资源，同时处理有限的片上内存。

Result: 在测试过程中，TriGen 在多个LLM上实现了2.73倍的性能加速，内存传输减少52%，且几乎不影响模型精度。

Conclusion: TriGen 是一种创新性的NPU架构，证明了在资源受限的设备上执行大型语言模型的可行性，通过精细化的设计带来了显著的性能提升和能效优化。

Abstract: Recent studies have extensively explored NPU architectures for accelerating AI inference in on-device environments, which are inherently resource-constrained. Meanwhile, transformer-based large language models (LLMs) have become dominant, with rapidly increasing model sizes but low degree of parameter reuse compared to conventional CNNs, making end-to-end execution on resource-limited devices extremely challenging. To address these challenges, we propose TriGen, a novel NPU architecture tailored for resource-constrained environments through software-hardware co-design. Firstly, TriGen adopts low-precision computation using microscaling (MX) to enable additional optimization opportunities while preserving accuracy, and resolves the issues that arise by employing such precision. Secondly, to jointly optimize both nonlinear and linear operations, TriGen eliminates the need for specialized hardware for essential nonlinear operations by using fast and accurate LUT, thereby maximizing performance gains and reducing hardware-cost in on-device environments, and finally, by taking practical hardware constraints into account, further employs scheduling techniques to maximize computational utilization even under limited on-chip memory capacity. We evaluate the performance of TriGen on various LLMs and show that TriGen achieves an average 2.73x performance speedup and 52% less memory transfer over the baseline NPU design with negligible accuracy loss.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [91] [Evolving Beyond Snapshots: Harmonizing Structure and Sequence via Entity State Tuning for Temporal Knowledge Graph Forecasting](https://arxiv.org/abs/2602.12389)
*Siyuan Li,Yunjia Wu,Yiyong Xiao,Pingyang Huang,Peize Li,Ruitong Liu,Yan Wen,Te Sun,Fangyi Pei*

Main category: cs.AI

TL;DR: Entity State Tuning (EST) 是一种无需编码器的方法，能够保持实体状态的持久性和连续进化，用于解决 TKG 预测中的短期记忆问题。


<details>
  <summary>Details</summary>
Motivation: 现有的 TKG 预测方法由于频繁重新计算实体表示而导致短期记忆问题，EST 模型通过保持和连续更新实体状态来解决这一问题。

Method: EST 模型采用编码器无关的设计，维持全局状态缓冲，并通过闭环设计逐步对齐结构证据和序列信号。具体而言，拓扑感知的状态感知器注入实体状态先验到结构编码中，统一时间上下文模块使用插件化的序列骨干聚合增强状态的事件，双轨进化机制将更新后的上下文写回全局实体状态记忆，平衡塑性和稳定性。

Result: 在多个基准测试上，EST 模型提升了一系列基础架构的表现，并取得了最先进的性能，证明了状态持久性对于长周期 TKG 预测的重要性。

Conclusion: EST 提供了一种有效缓解状态感知问题的方法，通过连续更新和保持实体状态，增强了 TKG 预测的准确性。

Abstract: Temporal knowledge graph (TKG) forecasting requires predicting future facts by jointly modeling structural dependencies within each snapshot and temporal evolution across snapshots. However, most existing methods are stateless: they recompute entity representations at each timestamp from a limited query window, leading to episodic amnesia and rapid decay of long-term dependencies. To address this limitation, we propose Entity State Tuning (EST), an encoder-agnostic framework that endows TKG forecasters with persistent and continuously evolving entity states. EST maintains a global state buffer and progressively aligns structural evidence with sequential signals via a closed-loop design. Specifically, a topology-aware state perceiver first injects entity-state priors into structural encoding. Then, a unified temporal context module aggregates the state-enhanced events with a pluggable sequence backbone. Subsequently, a dual-track evolution mechanism writes the updated context back to the global entity state memory, balancing plasticity against stability. Experiments on multiple benchmarks show that EST consistently improves diverse backbones and achieves state-of-the-art performance, highlighting the importance of state persistence for long-horizon TKG forecasting. The code is published at https://github.com/yuanwuyuan9/Evolving-Beyond-Snapshots

</details>


### [92] [Scaling Web Agent Training through Automatic Data Generation and Fine-grained Evaluation](https://arxiv.org/abs/2602.12544)
*Lajanugen Logeswaran,Jaekyeom Kim,Sungryull Sohn,Creighton Glasscock,Honglak Lee*

Main category: cs.AI

TL;DR: 提出了一种可扩展的自动生成高质量训练数据的流水线，引入了一种基于约束的评估框架来衡量进度，并在新提出的BookingArena基准上展示了其方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前方法难以识别高质量的训练实例，特别在轨迹评估方面，即量化任务完成进度。因此，需要一种更加精细的方法来评估进度，以利用部分成功的轨迹，从而扩展可用的训练数据量。

Method: 提出了一个基于约束的评估框架，能够对任务完成进度进行精确实时的评估。这种方法允许使用部分成功的轨迹，从而大大增加可用的训练数据。

Result: 在新基准BookingArena上，展示了该方法的有效性，表明精简后的学生模型在性能上优于开源方法和匹配或超越商业系统，同时该模型规模显著更小。

Conclusion: 该工作解决了高效创建多样化、现实的网页交互数据集的挑战，并提供了针对复杂结构化网页任务的系统评估方法。

Abstract: We present a scalable pipeline for automatically generating high-quality training data for web agents. In particular, a major challenge in identifying high-quality training instances is trajectory evaluation - quantifying how much progress was made towards task completion. We introduce a novel constraint-based evaluation framework that provides fine-grained assessment of progress towards task completion. This enables us to leverage partially successful trajectories, which significantly expands the amount of usable training data. We evaluate our method on a new benchmark we propose called BookingArena, which consists of complex booking tasks across 20 popular websites, and demonstrate that our distilled student model outperforms open-source approaches and matches or exceeds commercial systems, while being a significantly smaller model. Our work addresses the challenge of efficiently creating diverse, realistic web interaction datasets and provides a systematic evaluation methodology for complex structured web tasks.

</details>


### [93] [To Mix or To Merge: Toward Multi-Domain Reinforcement Learning for Large Language Models](https://arxiv.org/abs/2602.12566)
*Haoqing Wang,Xiang Long,Ziheng Li,Yilong Xu,Tingguang Li,Yehui Tang*

Main category: cs.AI

TL;DR: 该研究提出了混合多任务训练或独立训练再合并的方法（称为M2RL），用于多领域强化学习中的验证奖励强化学习（RLVR）。研究通过广泛质性和定量实验发现，跨领域的RLVR展示出微弱的相互干扰，并且复杂推理领域的相互协同效应显著。同时，分析了从参数空间几何、模型预测行为和信息约束视角的相互增益机制。


<details>
  <summary>Details</summary>
Motivation: 目前最先进的模型主要采用混合多任务训练或独立训练再合并这两种不同的多领域强化学习训练范式。然而，大多数工作并未对此这两种范式进行详细的比较与分析。该研究旨在填补这一空白。

Method: 该研究选择了数学、编码、科学和指令跟随等常用高级任务作为目标领域。通过使用开源数据集，进行了广泛的质性和定量实验，从参数空间几何、模型预测行为和信息约束角度分析了互惠增益机制。

Result: 研究发现，跨领域的RLVR仅有微弱的相互干扰，并且复杂推理领域的相互协同效应显著。此外，研究还探讨了这些领域之间的互惠增益机制。

Conclusion: 该研究提出了一种名为M2RL的方法，用于构建多领域专家级模型。研究结果表明，通过混合多任务训练或独立训练再合并，可以有效提高模型在多领域任务上的性能。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) plays a key role in stimulating the explicit reasoning capability of Large Language Models (LLMs). We can achieve expert-level performance in some specific domains via RLVR, such as coding or math. When a general multi-domain expert-level model is required, we need to carefully consider the collaboration of RLVR across different domains. The current state-of-the-art models mainly employ two different training paradigms for multi-domain RLVR: mixed multi-task RLVR and separate RLVR followed by model merging. However, most of the works did not provide a detailed comparison and analysis about these paradigms. To this end, we choose multiple commonly used high-level tasks (e.g., math, coding, science, and instruction following) as our target domains and design extensive qualitative and quantitative experiments using open-source datasets. We find the RLVR across domains exhibits few mutual interferences, and reasoning-intensive domains demonstrate mutually synergistic effects. Furthermore, we analyze the internal mechanisms of mutual gains from the perspectives of weight space geometry, model prediction behavior, and information constraints. This project is named as M2RL that means Mixed multi-task training or separate training followed by model Merging for Reinforcement Learning, and the homepage is at https://github.com/mosAI25/M2RL

</details>


### [94] [Can I Have Your Order? Monte-Carlo Tree Search for Slot Filling Ordering in Diffusion Language Models](https://arxiv.org/abs/2602.12586)
*Joshua Ong Jun Leang,Yu Zhao,Mihaela Cătălina Stoian,Wenda Li,Shay B. Cohen,Eleonora Giunchiglia*

Main category: cs.AI

TL;DR: McDiffuSE 提出了一种新的框架，通过 Monte Carlo Tree Search (MCTS) 优化 slot infilling 的顺序，从而显著提高了 Masked Diffusion Models (MDMs) 在数学和代码推理任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 当前的 plan-and-infill 方法的性能对插槽填充顺序极其敏感，导致输出结果差异较大。为了提高模型在数学和代码推理任务中的表现，研究者提出了一个新的框架 McDiffuSE。

Method: McDiffuSE 使用 Monte Carlo Tree Search (MCTS) 来优化 slot infilling 的顺序，通过前瞻性的模拟来评估部分完成的内容，从而系统地探索生成序列的组合空间。

Result: 实验结果显示，与自回归基线相比，McDiffuSE 平均提高了 3.2%，与 baseline plan-and-infill 相比提高了 8.0%，在 MBPP 和 MATH500 任务上分别取得 19.5% 和 4.9% 的显著提升。

Conclusion: 研究发现，虽然 McDiffuSE 主要遵循顺序排列，但非顺序生成的引入对于最大化性能至关重要。此外，更大的探索常数而非更多的模拟是克服模型置信度偏差和发现有效序列的关键。这些发现证明了基于 MCTS 的规划是提高 MDMs 生成质量的有效方法。

Abstract: While plan-and-infill decoding in Masked Diffusion Models (MDMs) shows promise for mathematical and code reasoning, performance remains highly sensitive to slot infilling order, often yielding substantial output variance. We introduce McDiffuSE, a framework that formulates slot selection as decision making and optimises infilling orders through Monte Carlo Tree Search (MCTS). McDiffuSE uses look-ahead simulations to evaluate partial completions before commitment, systematically exploring the combinatorial space of generation orders. Experiments show an average improvement of 3.2% over autoregressive baselines and 8.0% over baseline plan-and-infill, with notable gains of 19.5% on MBPP and 4.9% on MATH500. Our analysis reveals that while McDiffuSE predominantly follows sequential ordering, incorporating non-sequential generation is essential for maximising performance. We observe that larger exploration constants, rather than increased simulations, are necessary to overcome model confidence biases and discover effective orderings. These findings establish MCTS-based planning as an effective approach for enhancing generation quality in MDMs.

</details>


### [95] [Evaluating Robustness of Reasoning Models on Parameterized Logical Problems](https://arxiv.org/abs/2602.12665)
*Naïm Es-sebbani,Esteban Marquer,Yakoub Salhi,Zied Bouraoui*

Main category: cs.AI

TL;DR: 论文通过介绍一种针对2-SAT的诊断性基准，利用参数化的结构化2-CNF公式族来控制矛盾循环、解的多样性、植物载体和灵敏度等变量，评估了基于LLM的推理器在结构变化下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有标准基准（如SAT风格）难以区分表面难度与结构特性对可满足性的真正影响，因此需要一种能够精细控制的基准来评估基于语言模型的推理器。

Method: 构建一个参数化的2-SAT基准，利用多种生成器生成不同类型的实例，如矛盾循环、解的多样性控制、植物载体和非单调区域的敏感性等。

Result: 实验证明，当表面统计保持不变时，不同模型在特定结构干预下的表现会有显著变化，揭示了仅从整体SAT准确率难以发现的脆弱区域。

Conclusion: 该基准能更细致地评估基于LLM的推理器的性能，特别是在结构变化下的魯棒性。

Abstract: Logic provides a controlled testbed for evaluating LLM-based reasoners, yet standard SAT-style benchmarks often conflate surface difficulty (length, wording, clause order) with the structural phenomena that actually determine satisfiability. We introduce a diagnostic benchmark for 2-SAT built from parameterized families of structured 2--CNF formulas, where satisfiability is characterized by the implication graph and can be tuned along interpretable axes. Our generators isolate distinct competencies and failure modes: (i) contradiction-cycle UNSAT cores with controllable size and imbalance, (ii) SAT instances with a prescribed fraction of free variables to control solution multiplicity, (iii) planted backbones that modulate propagation, (iv) late bridge clauses that couple otherwise monotone regions to probe sensitivity to ordering and revision, and (v) symmetry/duplication variants that test abstraction under renaming and redundant structure. We evaluate LLM-based reasoners on decision accuracy and assignment validity, and quantify robustness under semantics-preserving perturbations such as clause reordering, filler clauses, and variable renaming. Across models, we observe sharp performance transitions under targeted structural interventions even when surface statistics are held fixed, revealing brittleness regimes that are invisible to aggregate SAT accuracy.

</details>


### [96] [X-SYS: A Reference Architecture for Interactive Explanation Systems](https://arxiv.org/abs/2602.12748)
*Tobias Labarta,Nhi Hoang,Maximilian Dreyer,Jim Berend,Oleg Hein,Jackie Ma,Wojciech Samek,Sebastian Lapuschkin*

Main category: cs.AI

TL;DR: 本研究旨在解决交互式解释系统的可操作性问题，提出了X-SYS参考架构，通过STAR质量和五个组件分解来指导XAI研究人员、开发者和实践者。该架构通过服务合约、离线/在线分离和持久状态管理简化了用户界面和后端计算的解耦。


<details>
  <summary>Details</summary>
Motivation: 当前XAI研究存在的挑战包括：交互解释系统的实现需要合适的算法和维持解释易于理解的能力，同时要应对模型和数据的变化以及治理限制。文章提出的动机是将可解释AI视为信息系统问题，基于用户交互需求引导系统需求，从而提出了解决这些挑战的X-SYS参考架构。

Method: 文章通过对X-SYS架构的设计，提出了STAR质量和五个组件（XUI服务、解释服务、模型服务、数据服务、 orchestration和治理）的分解方法。通过一个实现示例（SemanticLens）来展示如何通过服务边界、离线/在线分离和状态管理促进系统的独立演化。

Result: X-SYS为交互式解释系统的设计提供了一个可重复使用的蓝图，并通过具体实例（SemanticLens）证明了其实现的有效性。

Conclusion: 本文提出了一种新的参考架构X-SYS，旨在解决交互式解释系统在实际应用中的挑战，通过STAR质量和五个组件指导设计，并通过SemanticLens展示了其实现方式。

Abstract: The explainable AI (XAI) research community has proposed numerous technical methods, yet deploying explainability as systems remains challenging: Interactive explanation systems require both suitable algorithms and system capabilities that maintain explanation usability across repeated queries, evolving models and data, and governance constraints. We argue that operationalizing XAI requires treating explainability as an information systems problem where user interaction demands induce specific system requirements. We introduce X-SYS, a reference architecture for interactive explanation systems, that guides (X)AI researchers, developers and practitioners in connecting interactive explanation user interfaces (XUI) with system capabilities. X-SYS organizes around four quality attributes named STAR (scalability, traceability, responsiveness, and adaptability), and specifies a five-component decomposition (XUI Services, Explanation Services, Model Services, Data Services, Orchestration and Governance). It maps interaction patterns to system capabilities to decouple user interface evolution from backend computation. We implement X-SYS through SemanticLens, a system for semantic search and activation steering in vision-language models. SemanticLens demonstrates how contract-based service boundaries enable independent evolution, offline/online separation ensures responsiveness, and persistent state management supports traceability. Together, this work provides a reusable blueprint and concrete instantiation for interactive explanation systems supporting end-to-end design under operational constraints.

</details>


### [97] [Information-theoretic analysis of world models in optimal reward maximizers](https://arxiv.org/abs/2602.12963)
*Alfred Harwood,Jose Faustino,Alex Altair*

Main category: cs.AI

TL;DR: 该研究量化了最优策略关于环境的信息量，证明在给定条件下观察到确定性最优策略能提供环境的n log m比特信息。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决AI领域关于行为是否需要世界内部表示的疑问，通过定量分析最优策略的环境信息量来探讨这一问题。

Method: 研究通过构建一个n个状态和m个动作的受控马尔可夫过程(CMP)，基于非零奖励函数的最优策略，运用互信息理论进行证明。

Result: 研究证明，在特定假设下，确定性最优策略提供了环境n log m比特的信息，此结果适用于不同类型的目标函数。

Conclusion: 研究得出精确的信息论下限，即最优行为所需的“隐含世界模型”。

Abstract: An important question in the field of AI is the extent to which successful behaviour requires an internal representation of the world. In this work, we quantify the amount of information an optimal policy provides about the underlying environment. We consider a Controlled Markov Process (CMP) with $n$ states and $m$ actions, assuming a uniform prior over the space of possible transition dynamics. We prove that observing a deterministic policy that is optimal for any non-constant reward function then conveys exactly $n \log m$ bits of information about the environment. Specifically, we show that the mutual information between the environment and the optimal policy is $n \log m$ bits. This bound holds across a broad class of objectives, including finite-horizon, infinite-horizon discounted, and time-averaged reward maximization. These findings provide a precise information-theoretic lower bound on the "implicit world model'' necessary for optimality.

</details>


### [98] [Consistency of Large Reasoning Models Under Multi-Turn Attacks](https://arxiv.org/abs/2602.13093)
*Yubo Li,Ramayya Krishnan,Rema Padman*

Main category: cs.AI

TL;DR: 研究发现，尽管具备推理能力的大型模型在复杂任务上表现出色，但在多轮对抗攻击下的鲁棒性不足，存在多种失败模式，如自我怀疑、社会推崇模式服从等。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于填补推理模型在多轮对抗攻击下鲁棒性不足的研究空白，旨在通过评估不同推理模型在对抗攻击下的表现，揭示推理能力对鲁棒性的影响。

Method: 研究方法涉及到评估九种前沿推理模型在对抗攻击下的表现，并通过轨迹分析识别出推理模型中的五种失败模式。

Result: 研究发现，大多数推理模型在对抗攻击下相较于指令调优基线有显著的性能优势，但都存在不同的脆弱性表现，误导性建议普遍有效，而社会压力则具有模型特异性效果。轨迹分析还确定了五种失败模式，包括自我怀疑、社会推崇模式服从等。

Conclusion: 研究结论表明，推理能力不能自动保证模型在对抗攻击下的鲁棒性，基于自信的防御策略需要为推理模型进行根本性再设计。

Abstract: Large reasoning models with reasoning capabilities achieve state-of-the-art performance on complex tasks, but their robustness under multi-turn adversarial pressure remains underexplored. We evaluate nine frontier reasoning models under adversarial attacks. Our findings reveal that reasoning confers meaningful but incomplete robustness: most reasoning models studied significantly outperform instruction-tuned baselines, yet all exhibit distinct vulnerability profiles, with misleading suggestions universally effective and social pressure showing model-specific efficacy. Through trajectory analysis, we identify five failure modes (Self-Doubt, Social Conformity, Suggestion Hijacking, Emotional Susceptibility, and Reasoning Fatigue) with the first two accounting for 50% of failures. We further demonstrate that Confidence-Aware Response Generation (CARG), effective for standard LLMs, fails for reasoning models due to overconfidence induced by extended reasoning traces; counterintuitively, random confidence embedding outperforms targeted extraction. Our results highlight that reasoning capabilities do not automatically confer adversarial robustness and that confidence-based defenses require fundamental redesign for reasoning models.

</details>


### [99] [Constrained Assumption-Based Argumentation Frameworks](https://arxiv.org/abs/2602.13135)
*Emanuele De Angelis,Fabio Fioravanti,Maria Chiara Meo,Alberto Pettorossi,Maurizio Proietti,Francesca Toni*

Main category: cs.AI

TL;DR: 该论文提出了一种新的受限假设基础论证（CABA），放宽了传统无变量的形式化限制，允许使用带变量的论证和攻击，且定义了一种新的非基础语义。


<details>
  <summary>Details</summary>
Motivation: 传统ABA框架受到表达限制，只适用于无变量的论证及攻击。本文提出的CABA框架旨在解决此限制，提升其适用性。

Method: 通过引入带变量的论证和攻击，定义了CABA框架和相应的非基础语义。

Result: 论文成功地扩展了传统ABA的范围，并定义了与其标准语义相容的新型语义。

Conclusion: CABA框架是一种有价值的扩展，增强了形式逻辑系统在真实世界应用中的灵活性和适用性。

Abstract: Assumption-based Argumentation (ABA) is a well-established form of structured argumentation. ABA frameworks with an underlying atomic language are widely studied, but their applicability is limited by a representational restriction to ground (variable-free) arguments and attacks built from propositional atoms. In this paper, we lift this restriction and propose a novel notion of constrained ABA (CABA), whose components, as well as arguments built from them, may include constrained variables, ranging over possibly infinite domains. We define non-ground semantics for CABA, in terms of various notions of non-ground attacks. We show that the new semantics conservatively generalise standard ABA semantics.

</details>


### [100] [Optimal Take-off under Fuzzy Clearances](https://arxiv.org/abs/2602.13166)
*Hugo Henry,Arthur Tsai,Kelly Cohen*

Main category: cs.AI

TL;DR: 本文提出了一种结合了Optimal Control under Clearance和模糊规则系统(FRBS)的混合障碍避让架构，旨在提供适应性的约束管理，减轻不确定性对无人驾驶航空器的影响，同时确保安全操作。通过MATLAB实现证明了其在单线程环境下的可行性，但在最新的FALCON和IPOPT版本中发现了约束违规问题。


<details>
  <summary>Details</summary>
Motivation: 为了克服传统最优控制在不确定性条件下存在的局限性，以及提高安全关键航空系统中决策的可解释性

Method: 通过设计一个三阶段的Takagi-Sugeno-Kang模糊层来调整约束半径、紧迫性级别和激活决策，然后将这些由模糊系统得出的清学位引入最优控制问题中，使用FALCON和IPOPT求解器求解。

Result: 基于简化飞机模型的原型实现表明，该方法能够在单线程MATLAB环境中每迭代2.3秒生成最优轨迹，在实时应用领域具有可行性。然而，发现在最新版本的FALCON和IPOPT中存在软件兼容性问题。

Conclusion: 未来的研究将包括验证此现象、优化模糊隶属函数以及扩展到更高保真度的飞机模型及随机障碍环境。

Abstract: This paper presents a hybrid obstacle avoidance architecture that integrates Optimal Control under clearance with a Fuzzy Rule Based System (FRBS) to enable adaptive constraint handling for unmanned aircraft. Motivated by the limitations of classical optimal control under uncertainty and the need for interpretable decision making in safety critical aviation systems, we design a three stage Takagi Sugeno Kang fuzzy layer that modulates constraint radii, urgency levels, and activation decisions based on regulatory separation minima and airworthiness guidelines from FAA and EASA. These fuzzy-derived clearances are then incorporated as soft constraints into an optimal control problem solved using the FALCON toolbox and IPOPT. The framework aims to reduce unnecessary recomputations by selectively activating obstacle avoidance updates while maintaining compliance with aviation procedures. A proof of concept implementation using a simplified aircraft model demonstrates that the approach can generate optimal trajectories with computation times of 2,3 seconds per iteration in a single threaded MATLAB environment, suggesting feasibility for near real time applications. However, our experiments revealed a critical software incompatibility in the latest versions of FALCON and IPOPT, in which the Lagrangian penalty term remained identically zero, preventing proper constraint enforcement. This behavior was consistent across scenarios and indicates a solver toolbox regression rather than a modeling flaw. Future work includes validating this effect by reverting to earlier software versions, optimizing the fuzzy membership functions using evolutionary methods, and extending the system to higher fidelity aircraft models and stochastic obstacle environments.

</details>
