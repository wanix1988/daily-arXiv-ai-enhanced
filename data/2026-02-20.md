<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 44]
- [cs.CL](#cs.CL) [Total: 36]
- [cs.AI](#cs.AI) [Total: 43]
- [cs.AR](#cs.AR) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Three-dimensional Damage Visualization of Civil Structures via Gaussian Splatting-enabled Digital Twins](https://arxiv.org/abs/2602.16713)
*Shuo Wang,Shuo Wang,Xin Nie,Yasutaka Narazaki,Thomas Matiki,Billie F. Spencer*

Main category: cs.CV

TL;DR: 该研究提出了一种基于Gaussian Splatting (GS) 的数字孪生方法，用于精准的3D损伤可视化，涵盖了多种优化策略，并在地震后检查中展示了其在全面3D损伤可视化的潜力。


<details>
  <summary>Details</summary>
Motivation: 近年来，基础设施检查的精准3D损伤可视化需求增加，特别是对于超越传统2D图像识别方法的现代方法的需求。

Method: 方法基于Gaussian Splatting (GS)，通过多尺度重建策略平衡效率与损伤细节，并将2D损伤分割结果转化为3D可视化。

Result: 在开源合成数据集上展示了该方法在地震后检查中的潜力。

Conclusion: 该研究提出了一个有效的3D损伤可视化方法，可用于更新随时间演变的数字孪生，为全面的3D损伤可视化提供了新的思路。

Abstract: Recent advancements in civil infrastructure inspections underscore the need for precise three-dimensional (3D) damage visualization on digital twins, transcending traditional 2D image-based damage identifications. Compared to conventional photogrammetric 3D reconstruction techniques, modern approaches such as Neural Radiance Field (NeRF) and Gaussian Splatting (GS) excel in scene representation, rendering quality, and handling featureless regions. Among them, GS stands out for its efficiency, leveraging discrete anisotropic 3D Gaussians to represent radiance fields, unlike NeRF's continuous implicit model. This study introduces a GS-enabled digital twin method tailored for effective 3D damage visualization. The method's key contributions include: 1) utilizing GS-based 3D reconstruction to visualize 2D damage segmentation results while reducing segmentation errors; 2) developing a multi-scale reconstruction strategy to balance efficiency and damage detail; 3) enabling digital twin updates as damage evolves over time. Demonstrated on an open-source synthetic dataset for post-earthquake inspections, the proposed approach offers a promising solution for comprehensive 3D damage visualization in civil infrastructure digital twins.

</details>


### [2] [Analytic Score Optimization for Multi Dimension Video Quality Assessment](https://arxiv.org/abs/2602.16856)
*Boda Lin,Yongjie Zhu,Wenyu Qin,Meng Wang,Pengfei Wan*

Main category: cs.CV

TL;DR: 本文介绍了UltraVQA多维度视频质量评估数据集和Analytic Score Optimization方法，通过理论支持的后训练目标优化多维度视频质量评估。


<details>
  <summary>Details</summary>
Motivation: 随着视频质量评估（VQA）从单一的平均满意度评分转向更丰富、多维度的视频内容评估，需要引入能够更好利用丰富注释并提升离散质量评分评估的方法。

Method: 本文提出了UltraVQA数据集，包含超过3个评价者基于五个关键质量维度（运动质量、运动幅值、美学质量、内容质量和清晰度质量）对每个视频进行细粒度子属性标签分级，并附带基于集体人类判断生成的解释性理由。进一步引入了Analytic Score Optimization (ASO) 方法，将质量评估重新定义为正则化的决策过程，通过理论支持的后训练目标获得封闭形式的解决方案。

Result: 实验结果表明，本文方法在质量预测的平均绝对误差（MAE）上优于大多数基线模型，包括闭源API和开源模型。

Conclusion: 本文的工作突显了多维度、可解释的注释和基于强化的学习对提升视频质量评估的重要性。

Abstract: Video Quality Assessment (VQA) is evolving beyond single-number mean opinion score toward richer, multi-faceted evaluations of video content. In this paper, we present a large-scale multi-dimensional VQA dataset UltraVQA that encompasses diverse User-Generated Content~(UGC) annotated across five key quality dimensions: Motion Quality, Motion Amplitude, Aesthetic Quality, Content Quality, and Clarity Quality. Each video in our dataset is scored by over 3 human raters on these dimensions, with fine-grained sub-attribute labels, and accompanied by an explanatory rationale generated by GPT based on the collective human judgments. To better leverage these rich annotations and improve discrete quality score assessment, we introduce Analytic Score Optimization (ASO), a theoretically grounded post-training objective derived for multi-dimensional VQA. By reframing quality assessment as a regularized decision-making process, we obtain a closed-form solution that naturally captures the ordinal nature of human ratings, ensuring alignment with human ranking preferences. In experiments, our method outperforms most baselines including closed-source APIs and open-source models, while also reducing mean absolute error (MAE) in quality prediction. Our work highlights the importance of multi-dimensional, interpretable annotations and reinforcement-based alignment in advancing video quality assessment.

</details>


### [3] [DODO: Discrete OCR Diffusion Models](https://arxiv.org/abs/2602.16872)
*Sean Man,Roy Ganz,Roi Ronen,Shahar Tsiper,Shai Mazor,Niv Nayman*

Main category: cs.CV

TL;DR: 提出了一种名为DODO的新VLM方法，通过将生成过程分解为块级，实现了OCR任务的大规模并行解码，在保持高精度的同时提高了推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有的Vision-Language模型（VLM）虽然在OCR任务中取得了高精度，但依赖于自回归解码，导致长文档处理时计算成本高且速度慢。因此，研究旨在通过利用OCR的高度确定性，提出一种能够并行解码的VLM方法。

Method: 通过构建块离散扩散模型DODO，将生成过程分解为多个块，来减少全局扩散模型同步错误，从而实现高效并行解码。

Result: DODO方法在保持接近最新技术水平准确性的前提下，相比自回归基线可加快3倍的推理速度。

Conclusion: DODO方法为OCR任务提供了一种新的有效解决方案，通过并行处理提高了处理效率。

Abstract: Optical Character Recognition (OCR) is a fundamental task for digitizing information, serving as a critical bridge between visual data and textual understanding. While modern Vision-Language Models (VLM) have achieved high accuracy in this domain, they predominantly rely on autoregressive decoding, which becomes computationally expensive and slow for long documents as it requires a sequential forward pass for every generated token. We identify a key opportunity to overcome this bottleneck: unlike open-ended generation, OCR is a highly deterministic task where the visual input strictly dictates a unique output sequence, theoretically enabling efficient, parallel decoding via diffusion models. However, we show that existing masked diffusion models fail to harness this potential; those introduce structural instabilities that are benign in flexible tasks, like captioning, but catastrophic for the rigid, exact-match requirements of OCR. To bridge this gap, we introduce DODO, the first VLM to utilize block discrete diffusion and unlock its speedup potential for OCR. By decomposing generation into blocks, DODO mitigates the synchronization errors of global diffusion. Empirically, our method achieves near state-of-the-art accuracy while enabling up to 3x faster inference compared to autoregressive baselines.

</details>


### [4] [StereoAdapter-2: Globally Structure-Consistent Underwater Stereo Depth Estimation](https://arxiv.org/abs/2602.16915)
*Zeyu Ren,Xiang Li,Yiran Wang,Zeyu Zhang,Hao Tang*

Main category: cs.CV

TL;DR: 本文提出了一种名为StereoAdapter-2的新型技术，它通过一种新型的ConvSS2D操作替换传统的ConvGRU更新器，改进了水下立体深度估计的性能。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决水下环境中的立体深度估计面临的挑战，包括光譜依赖的光衰减、散射和折射导致的重大域变化。

Method: 提出了一种名为ConvSS2D的新操作，它结合了四向扫描策略，充分利用了epipolar几何结构，并使用了选择性状态空间模型来高效地在单个更新步骤中完成长距离的空间传播。

Result: 该框架在水下基准测试中的零样本性能达到了领先水平，在TartanAir-UW上提高了17%，在SQUID上提高了7.2%。

Conclusion: 该技术通过实验验证，在BlueROV2平台上展示了其在现实世界中的鲁棒性。

Abstract: Stereo depth estimation is fundamental to underwater robotic perception, yet suffers from severe domain shifts caused by wavelength-dependent light attenuation, scattering, and refraction. Recent approaches leverage monocular foundation models with GRU-based iterative refinement for underwater adaptation; however, the sequential gating and local convolutional kernels in GRUs necessitate multiple iterations for long-range disparity propagation, limiting performance in large-disparity and textureless underwater regions. In this paper, we propose StereoAdapter-2, which replaces the conventional ConvGRU updater with a novel ConvSS2D operator based on selective state space models. The proposed operator employs a four-directional scanning strategy that naturally aligns with epipolar geometry while capturing vertical structural consistency, enabling efficient long-range spatial propagation within a single update step at linear computational complexity. Furthermore, we construct UW-StereoDepth-80K, a large-scale synthetic underwater stereo dataset featuring diverse baselines, attenuation coefficients, and scattering parameters through a two-stage generative pipeline combining semantic-aware style transfer and geometry-consistent novel view synthesis. Combined with dynamic LoRA adaptation inherited from StereoAdapter, our framework achieves state-of-the-art zero-shot performance on underwater benchmarks with 17% improvement on TartanAir-UW and 7.2% improvment on SQUID, with real-world validation on the BlueROV2 platform demonstrates the robustness of our approach. Code: https://github.com/AIGeeksGroup/StereoAdapter-2. Website: https://aigeeksgroup.github.io/StereoAdapter-2.

</details>


### [5] [SemCovNet: Towards Fair and Semantic Coverage-Aware Learning for Underrepresented Visual Concepts](https://arxiv.org/abs/2602.16917)
*Sakib Ahammed,Xia Cui,Xinqi Fan,Wenqi Lu,Moi Hoon Yap*

Main category: cs.CV

TL;DR: 该研究提出了SemCovNet模型，旨在解决视觉模型在学习稀有但有意义的语义概念时面临的语义覆盖不平衡问题，通过引入语义描述图谱（SDM）、描述注意调制（DAM）模块和描述-视觉对齐（DVA）损失函数来实现公平性。


<details>
  <summary>Details</summary>
Motivation: 现有数据集中存在语义覆盖不平衡问题，影响了模型对稀有但有意义的语义概念的理解和推理。

Method: SemCovNet模型结合了语义描述图谱（SDM）、描述注意调制（DAM）模块和描述-视觉对齐（DVA）损失函数来纠正语义覆盖差异。

Result: SemCovNet模型显著提高了模型的可靠性并减少了语义偏差指数（CDI），实现了更公平和更平等的性能。

Conclusion: 该研究将语义覆盖不平衡视为可度量且可纠正的偏差，为进一步推进语义公平性和可解释视觉学习奠定了基础。

Abstract: Modern vision models increasingly rely on rich semantic representations that extend beyond class labels to include descriptive concepts and contextual attributes. However, existing datasets exhibit Semantic Coverage Imbalance (SCI), a previously overlooked bias arising from the long-tailed semantic representations. Unlike class imbalance, SCI occurs at the semantic level, affecting how models learn and reason about rare yet meaningful semantics. To mitigate SCI, we propose Semantic Coverage-Aware Network (SemCovNet), a novel model that explicitly learns to correct semantic coverage disparities. SemCovNet integrates a Semantic Descriptor Map (SDM) for learning semantic representations, a Descriptor Attention Modulation (DAM) module that dynamically weights visual and concept features, and a Descriptor-Visual Alignment (DVA) loss that aligns visual features with descriptor semantics. We quantify semantic fairness using a Coverage Disparity Index (CDI), which measures the alignment between coverage and error. Extensive experiments across multiple datasets demonstrate that SemCovNet enhances model reliability and substantially reduces CDI, achieving fairer and more equitable performance. This work establishes SCI as a measurable and correctable bias, providing a foundation for advancing semantic fairness and interpretable vision learning.

</details>


### [6] [Xray-Visual Models: Scaling Vision models on Industry Scale Data](https://arxiv.org/abs/2602.16918)
*Shlok Mishra,Tsung-Yu Lin,Linda Wang,Hongli Xu,Yimin Liu,Michael Hsu,Chaitanya Ahuja,Hao Yuan,Jianpeng Cheng,Hong-You Chen,Haoyuan Xu,Chao Li,Abhijeet Awasthi,Jihye Moon,Don Husa,Michael Ge,Sumedha Singla,Arkabandhu Chowdhury,Phong Dingh,Satya Narayan Shukla,Yonghuan Yang,David Jacobs,Qi Guo,Jun Xiao,Xiangjun Fan,Aashu Singh*

Main category: cs.CV

TL;DR: Xray-Visual 是一种大规模图像和视频理解的统一视觉模型，训练于工业规模的社交媒体数据，包含自监督学习、半监督学习和对比学习等多个训练阶段，展示了出色的跨模态检索性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前视觉模型缺乏统一架构处理大规模图像和视频的同时保持高精度和计算效率。

Method: 该模型基于Vision Transformer，配有高效标记重组（EViT），采用包含自监督掩模自编码、半监督Hashtag分类以及CLIP风格对比学习的三阶段训练策略。

Result: Xray-Visual 在多个基准测试中（如ImageNet图像分类、Kinetics和HMDB51视频理解、MSCOCO跨模态检索）获得领先表现，并且具有优秀的域迁移鲁棒性和对抗扰动鲁棒性。

Conclusion: Xray-Visual 为大规模、跨模态视觉模型设定了新基准，同时保持了较高的准确性和计算效率。

Abstract: We present Xray-Visual, a unified vision model architecture for large-scale image and video understanding trained on industry-scale social media data. Our model leverages over 15 billion curated image-text pairs and 10 billion video-hashtag pairs from Facebook and Instagram, employing robust data curation pipelines that incorporate balancing and noise suppression strategies to maximize semantic diversity while minimizing label noise. We introduce a three-stage training pipeline that combines self-supervised MAE, semi-supervised hashtag classification, and CLIP-style contrastive learning to jointly optimize image and video modalities. Our architecture builds on a Vision Transformer backbone enhanced with efficient token reorganization (EViT) for improved computational efficiency. Extensive experiments demonstrate that Xray-Visual achieves state-of-the-art performance across diverse benchmarks, including ImageNet for image classification, Kinetics and HMDB51 for video understanding, and MSCOCO for cross-modal retrieval. The model exhibits strong robustness to domain shift and adversarial perturbations. We further demonstrate that integrating large language models as text encoders (LLM2CLIP) significantly enhances retrieval performance and generalization capabilities, particularly in real-world environments. Xray-Visual establishes new benchmarks for scalable, multimodal vision models, while maintaining superior accuracy and computational efficiency.

</details>


### [7] [HS-3D-NeRF: 3D Surface and Hyperspectral Reconstruction From Stationary Hyperspectral Images Using Multi-Channel NeRFs](https://arxiv.org/abs/2602.16950)
*Kibon Ku,Talukder Z. Jubery,Adarsh Krishnamurthy,Baskar Ganapathysubramanian*

Main category: cs.CV

TL;DR: HSI-SC-NeRF是一种用于农产品后收获检测的高通量高光谱3D重建框架，通过静止摄像头和多视角高光谱数据实现了高效准确的3D重建。


<details>
  <summary>Details</summary>
Motivation: 当前的高光谱成像和3D重建方法在农业领域应用时存在硬件兼容性和自动化程度不足的问题，本文旨在通过神经辐射场（NeRF）技术，开发一种适用于室内农业环境的高通量高光谱3D重建方法，以解决这一问题。

Method: HSI-SC-NeRF框架采用静止摄像头捕获多视角高光谱数据，通过ArUco标记估算目标姿态，并通过模拟姿态转换，使数据适用于标准NeRF训练。框架提出了一种多通道NeRF模型，利用复合光谱损失联合优化各高光谱波段，采用两阶段训练方法分别进行几何初始化和辐射度细化。

Result: 在三种农产品样本上的实验验证了HSI-SC-NeRF在空间重建精度和光谱保真度方面的优越性能，该方法能有效应用于自动化农业工作流程中。

Conclusion: HSI-SC-NeRF为农业领域的高通量高光谱3D重建提供了解决方案，其多通道NeRF模型和两步训练流程为未来类似技术的发展奠定了基础。

Abstract: Advances in hyperspectral imaging (HSI) and 3D reconstruction have enabled accurate, high-throughput characterization of agricultural produce quality and plant phenotypes, both essential for advancing agricultural sustainability and breeding programs. HSI captures detailed biochemical features of produce, while 3D geometric data substantially improves morphological analysis. However, integrating these two modalities at scale remains challenging, as conventional approaches involve complex hardware setups incompatible with automated phenotyping systems. Recent advances in neural radiance fields (NeRF) offer computationally efficient 3D reconstruction but typically require moving-camera setups, limiting throughput and reproducibility in standard indoor agricultural environments. To address these challenges, we introduce HSI-SC-NeRF, a stationary-camera multi-channel NeRF framework for high-throughput hyperspectral 3D reconstruction targeting postharvest inspection of agricultural produce. Multi-view hyperspectral data is captured using a stationary camera while the object rotates within a custom-built Teflon imaging chamber providing diffuse, uniform illumination. Object poses are estimated via ArUco calibration markers and transformed to the camera frame of reference through simulated pose transformations, enabling standard NeRF training on stationary-camera data. A multi-channel NeRF formulation optimizes reconstruction across all hyperspectral bands jointly using a composite spectral loss, supported by a two-stage training protocol that decouples geometric initialization from radiometric refinement. Experiments on three agricultural produce samples demonstrate high spatial reconstruction accuracy and strong spectral fidelity across the visible and near-infrared spectrum, confirming the suitability of HSI-SC-NeRF for integration into automated agricultural workflows.

</details>


### [8] [DDiT: Dynamic Patch Scheduling for Efficient Diffusion Transformers](https://arxiv.org/abs/2602.16968)
*Dahye Kim,Deepti Ghadiyaram,Raghudeep Gadde*

Main category: cs.CV

TL;DR: 本文提出了一种动态分词策略，在保持生成质量和提示遵循性的前提下，相较于固定分词方法，在FLUX-1.Dev和Wan 2.1数据集上分别实现了3.52倍和3.2倍的加速。


<details>
  <summary>Details</summary>
Motivation: 当前的扩散变换器（DiTs）在图像和视频生成任务中表现优异，但由于固定的分词过程，导致了计算资源的大量消耗。

Method: 该方法通过根据内容复杂度和去噪时间步长动态调整分词大小，早期时间步仅需粗略的分词来捕捉全局结构，后期步骤则使用更精细的分词来精细建模局部细节。

Result: 在推理阶段，该方法通过跨去噪步骤动态重新分配分词大小，显著减少了计算成本，同时保持了感知生成质量。实验结果表明，该方法在FLUX-1.Dev和Wan 2.1数据集上的加速比分别为3.52倍和3.2倍，且不牺牲生成质量和提示遵循性。

Conclusion: 动态分词策略有效地提高了扩散模型在图像和视频生成任务中的效率，而无需牺牲生成质量。

Abstract: Diffusion Transformers (DiTs) have achieved state-of-the-art performance in image and video generation, but their success comes at the cost of heavy computation. This inefficiency is largely due to the fixed tokenization process, which uses constant-sized patches throughout the entire denoising phase, regardless of the content's complexity. We propose dynamic tokenization, an efficient test-time strategy that varies patch sizes based on content complexity and the denoising timestep. Our key insight is that early timesteps only require coarser patches to model global structure, while later iterations demand finer (smaller-sized) patches to refine local details. During inference, our method dynamically reallocates patch sizes across denoising steps for image and video generation and substantially reduces cost while preserving perceptual generation quality. Extensive experiments demonstrate the effectiveness of our approach: it achieves up to $3.52\times$ and $3.2\times$ speedup on FLUX-1.Dev and Wan $2.1$, respectively, without compromising the generation quality and prompt adherence.

</details>


### [9] [Characterizing the Predictive Impact of Modalities with Supervised Latent-Variable Modeling](https://arxiv.org/abs/2602.16979)
*Divyam Madaan,Sumit Chopra,Kyunghyun Cho*

Main category: cs.CV

TL;DR: PRIMO 提出了一种新方法，旨在解决多模态数据不完整的问题，通过填充缺失的模态来实现多模态学习，并在合成 XOR 数据集、Audio-Vision MNIST 和 MIMIC-III 数据集上进行了评估。


<details>
  <summary>Details</summary>
Motivation: 现有方法大多假设多模态数据在训练和推理阶段都完整。然而，在实际应用中，多模态数据往往不完整。PRIMO 被设计用来填补缺失的模态以应对这种不完整的问题。

Method: PRIMO 采用监督学习的潜在变量插补模型，在预测上下文中通过潜在变量捕获缺失模态与观测模态的关系。在推断阶段，PRIMO 从缺失模态的学习分布中抽样，以生成预测的边际分布，并评估每个实例中缺失模态的影响。

Result: PRIMO 在合成 XOR 数据集、Audio-Vision MNIST 和 MIMIC-III 数据集上表现良好，与单一模态基线在模态完全缺失时表现出色，与多模态基线在所有模态都可用时具有相当的性能。同时，PRIMO 可以量化每个实例中模态预测的影响，并可视化不同的模态补全。

Conclusion: PRIMO 在处理多模态数据不完整的问题上有显著改进，且在多个数据集上的实验结果证实了其有效性。

Abstract: Despite the recent success of Multimodal Large Language Models (MLLMs), existing approaches predominantly assume the availability of multiple modalities during training and inference. In practice, multimodal data is often incomplete because modalities may be missing, collected asynchronously, or available only for a subset of examples. In this work, we propose PRIMO, a supervised latent-variable imputation model that quantifies the predictive impact of any missing modality within the multimodal learning setting. PRIMO enables the use of all available training examples, whether modalities are complete or partial. Specifically, it models the missing modality through a latent variable that captures its relationship with the observed modality in the context of prediction. During inference, we draw many samples from the learned distribution over the missing modality to both obtain the marginal predictive distribution (for the purpose of prediction) and analyze the impact of the missing modalities on the prediction for each instance. We evaluate PRIMO on a synthetic XOR dataset, Audio-Vision MNIST, and MIMIC-III for mortality and ICD-9 prediction. Across all datasets, PRIMO obtains performance comparable to unimodal baselines when a modality is fully missing and to multimodal baselines when all modalities are available. PRIMO quantifies the predictive impact of a modality at the instance level using a variance-based metric computed from predictions across latent completions. We visually demonstrate how varying completions of the missing modality result in a set of plausible labels.

</details>


### [10] [Patch-Based Spatial Authorship Attribution in Human-Robot Collaborative Paintings](https://arxiv.org/abs/2602.17030)
*Eric Chen,Patricia Alves-Oliveira*

Main category: cs.CV

TL;DR: 该研究提出了一种用于人类-机器人协作绘画实践中空间作者归属的贴图框架，通过来自一名人机艺术家和机器人系统的一系列抽象画进行案例研究验证，其准确率高于先前的纹理基线和预训练特征基准。


<details>
  <summary>Details</summary>
Motivation: 当前人工智能在创造性生产中的参与越来越广泛，艺术家、收藏家和法律语境下的作者归属记录变得尤为重要。这项研究旨在通过对人类和机器人的协作绘画进行分析，开发了一种空间作者归属的方法，以解决创作背景下作者权证的记录难题。

Method: 该研究采用了一种基于贴图的方法，使用商品级平板扫描仪和逐次交叉验证的方法，开发了一种斑点级别的作者归属模型，该模型通过最大投票法提高了绘画级别的准确率，特别是在模糊的协作作品中使用了条件香农熵来量化风格的重叠。

Result: 采用该方法，研究在15幅抽象画中实现了88.8%的斑点级别准确率，86.7%的绘画级别准确率，优于先前的纹理基线和预训练特征基准。在协作艺术品中，该模型能够判断混合作品的斑点区域，从而进一步明确模型识别的有效性。

Conclusion: 该研究方法不仅针对特定的人机艺术家-机器人对，也提供了一种在数据稀缺的人工智能与人类创造领域中进行样本高效归属的方法，并为进一步将作者归属扩展到任何人类-机器人合作绘画提供了基础。

Abstract: As agentic AI becomes increasingly involved in creative production, documenting authorship has become critical for artists, collectors, and legal contexts. We present a patch-based framework for spatial authorship attribution within human-robot collaborative painting practice, demonstrated through a forensic case study of one human artist and one robotic system across 15 abstract paintings. Using commodity flatbed scanners and leave-one-painting-out cross-validation, the approach achieves 88.8% patch-level accuracy (86.7% painting-level via majority vote), outperforming texture-based and pretrained-feature baselines (68.0%-84.7%). For collaborative artworks, where ground truth is inherently ambiguous, we use conditional Shannon entropy to quantify stylistic overlap; manually annotated hybrid regions exhibit 64% higher uncertainty than pure paintings (p=0.003), suggesting the model detects mixed authorship rather than classification failure. The trained model is specific to this human-robot pair but provides a methodological grounding for sample-efficient attribution in data-scarce human-AI creative workflows that, in the future, has the potential to extend authorship attribution to any human-robot collaborative painting.

</details>


### [11] [PartRAG: Retrieval-Augmented Part-Level 3D Generation and Editing](https://arxiv.org/abs/2602.17033)
*Peize Li,Zeyu Zhang,Hao Tang*

Main category: cs.CV

TL;DR: PartRAG 提出了一种检索增强框架，结合外部部件数据库和扩散变压器来耦合生成过程与可编辑表示，解决了单图像 3D 生成中的结构层面挑战，包括部件几何的长尾覆盖不足和多视图一致性问题。该框架利用层次对比检索模块注入多样且物理上合理的部件实例，以及通过部分级别的掩码编辑模块实现局部修改。


<details>
  <summary>Details</summary>
Motivation: 现有的单图像3D生成方法在部件几何形状的长尾覆盖不足和多视图一致性方面遇到困难，PartRAG旨在通过引入检索模块和部分编辑方法来解决这些问题。

Method: PartRAG 的方法是结合了一个扩散变压器模型和一个层次对比检索模块，以及一个部分级别的掩码编辑模块。该模块能够在不重新生成整个对象的情况下，实现局部修改，同时保持非目标部分和多视图一致性。

Result: PartRAG 在 Objaverse、ShapeNet 和 ABO 数据集上取得了竞争性的结果。在 Objaverse 数据集上，PartRAG 将 Chamfer 距离从 0.1726 降低到 0.1528，将 F-Score 提高到 0.844。同时，PartRAG 的推断时间为 38s，交互式编辑时间为 5-8s。此外，该方法在部件边界锐化、薄结构保真度和复合对象上的鲁棒性方面表现出更优秀的性能。

Conclusion: PartRAG 提出的检索增强框架与可编辑表示结合方案在单图像3D生成领域提供了新的研究角度，成功处理了关键的技术挑战，并且在多个基准数据集上展现出了优越的性能。

Abstract: Single-image 3D generation with part-level structure remains challenging: learned priors struggle to cover the long tail of part geometries and maintain multi-view consistency, and existing systems provide limited support for precise, localized edits. We present PartRAG, a retrieval-augmented framework that integrates an external part database with a diffusion transformer to couple generation with an editable representation. To overcome the first challenge, we introduce a Hierarchical Contrastive Retrieval module that aligns dense image patches with 3D part latents at both part and object granularity, retrieving from a curated bank of 1,236 part-annotated assets to inject diverse, physically plausible exemplars into denoising. To overcome the second challenge, we add a masked, part-level editor that operates in a shared canonical space, enabling swaps, attribute refinements, and compositional updates without regenerating the whole object while preserving non-target parts and multi-view consistency. PartRAG achieves competitive results on Objaverse, ShapeNet, and ABO-reducing Chamfer Distance from 0.1726 to 0.1528 and raising F-Score from 0.7472 to 0.844 on Objaverse-with inference of 38s and interactive edits in 5-8s. Qualitatively, PartRAG produces sharper part boundaries, better thin-structure fidelity, and robust behavior on articulated objects. Code: https://github.com/AIGeeksGroup/PartRAG. Website: https://aigeeksgroup.github.io/PartRAG.

</details>


### [12] [Amber-Image: Efficient Compression of Large-Scale Diffusion Transformers](https://arxiv.org/abs/2602.17047)
*Chaojie Yang,Tian Li,Yue Zhang,Jun Gao*

Main category: cs.CV

TL;DR: 一种高效的压缩框架被提出，用于将60层的MMDiT基Qwen-Image模型压缩为轻量级的Amber-Image模型，同时保持高质量的文本到图像生成效果。


<details>
  <summary>Details</summary>
Motivation: 目前的Diffusion Transformer（DiT）架构在文本到图像生成方面取得了显著进步，但存在计算成本高昂和部署限制的问题。因此，提出了一种压缩框架，旨在降低计算成本并改善模型的部署性。

Method: 该方法采用了时间步敏感的深度剪枝策略，以及局部权重平均、分层蒸馏和全参数微调，从10B版本开始，逐步发展出6B版本的Amber-Image模型。

Result: 此方法显著减少了参数数量，实现了参数减少了70%，而且整个压缩和训练过程在不到2000个GPU小时内完成。与从头训练相比，这显示出极佳的成本效益。评估结果显示，Amber-Image在生成质量和文本呈现方面与较大模型相当，甚至在某些方面优于较大的模型。

Conclusion: 该研究提出的方法和模型对于文本到图像生成任务来说是有效的，特别是在需要高效率和成本节约的应用场景中。

Abstract: Diffusion Transformer (DiT) architectures have significantly advanced Text-to-Image (T2I) generation but suffer from prohibitive computational costs and deployment barriers. To address these challenges, we propose an efficient compression framework that transforms the 60-layer dual-stream MMDiT-based Qwen-Image into lightweight models without training from scratch. Leveraging this framework, we introduce Amber-Image, a series of streamlined T2I models. We first derive Amber-Image-10B using a timestep-sensitive depth pruning strategy, where retained layers are reinitialized via local weight averaging and optimized through layer-wise distillation and full-parameter fine-tuning. Building on this, we develop Amber-Image-6B by introducing a hybrid-stream architecture that converts deep-layer dual streams into a single stream initialized from the image branch, further refined via progressive distillation and lightweight fine-tuning. Our approach reduces parameters by 70% and eliminates the need for large-scale data engineering. Notably, the entire compression and training pipeline-from the 10B to the 6B variant-requires fewer than 2,000 GPU hours, demonstrating exceptional cost-efficiency compared to training from scratch. Extensive evaluations on benchmarks like DPG-Bench and LongText-Bench show that Amber-Image achieves high-fidelity synthesis and superior text rendering, matching much larger models.

</details>


### [13] [StructCore: Structure-Aware Image-Level Scoring for Training-Free Unsupervised Anomaly Detection](https://arxiv.org/abs/2602.17048)
*Joongwon Chae,Lihui Luo,Yang Liu,Runming Wang,Dongmei Yu,Zeming Liang,Xi Yuan,Dayan Zhang,Zhenglin Chen,Peiwu Qin,Ilmoon Chae*

Main category: cs.CV

TL;DR: StructCore 是一个无需训练、增强结构意识的图像级评分方法，能有效避免 max pooling 丢弃的图像中异常证据的分布和结构信息，从而提供了更高效的图像级异常检测能力。


<details>
  <summary>Details</summary>
Motivation: 现有的异常检测方法，特别是基于记忆库的无监督异常检测(UAD)，往往通过 max pooling 将异常分数图转化为图像级别的决策。但这种方法过度依赖单一的极值响应，导致丢失了异常证据在图像中的分布和结构信息，从而导致正常和异常分数的重叠。

Method: StructCore 通过计算低维结构描述符 phi(S)，提取分布性和空间性特征，并利用从训练好的正常样本中估计出的对角 Mahalanobis 校准来优化图像级评分，而不改变像素级的定位。

Result: 在 MVTec AD 和 VisA 数据集上，StructCore 获得了 99.6% 和 98.4% 的图像级别 AUROC 分数，证明了通过发掘 max pooling 未能捕捉的结构特征，可以实现更为稳健的图像级别异常检测。

Conclusion: StructCore 提供了一种无需训练的、结构意识强的图像级评分方法，提高了异常检测的准确性。

Abstract: Max pooling is the de facto standard for converting anomaly score maps into image-level decisions in memory-bank-based unsupervised anomaly detection (UAD). However, because it relies on a single extreme response, it discards most information about how anomaly evidence is distributed and structured across the image, often causing normal and anomalous scores to overlap.
  We propose StructCore, a training-free, structure-aware image-level scoring method that goes beyond max pooling. Given an anomaly score map, StructCore computes a low-dimensional structural descriptor phi(S) that captures distributional and spatial characteristics, and refines image-level scoring via a diagonal Mahalanobis calibration estimated from train-good samples, without modifying pixel-level localization.
  StructCore achieves image-level AUROC scores of 99.6% on MVTec AD and 98.4% on VisA, demonstrating robust image-level anomaly detection by exploiting structural signatures missed by max pooling.

</details>


### [14] [Cholec80-port: A Geometrically Consistent Trocar Port Segmentation Dataset for Robust Surgical Scene Understanding](https://arxiv.org/abs/2602.17060)
*Shunsuke Kikuchi,Atsushi Kouno,Hiroki Matsuzaki*

Main category: cs.CV

TL;DR: 介绍了Cholec80-port数据集，这是一个来自Cholec80的高保真度Trocar端口分割数据集，它包括一个严格的SOP（标准操作程序），定义了不包括中央开口的端口套筒掩码。还对现有公共数据集进行了清洁和统一，使用相同SOP。实验表明，几何上一致的注释在跨数据集的鲁棒性方面比单独的数据集大小所能提供的要好。


<details>
  <summary>Details</summary>
Motivation: Trocar端口的存在会导致内窥镜手术图像中的遮挡和特征点偏移，这对依赖于精确几何和动态重建的下游应用程序如图像拼接、3D重建和视觉SLAM特别不利。现有的公开数据集中的端口标注往往难以保持几何一致性，这限制了它们的实际应用价值。

Method: 基于Cholec80数据集，提出Cholec80-port，通过定义一个不包括中央开口的端口套筒掩码的严格SOP来生成几何上一致的端口分割数据集。此外，它还对其他公共数据集进行了清理和统一，以确保它们符合相同的SOP。

Result: Cholec80-port数据集展示了几何上一致的端口标注能显著提高跨数据集的鲁棒性，优于仅依靠数据集大小的改进。

Conclusion: 本研究通过提供一个高保真度的Trocar端口分割数据集及其严格的SOP，有助于提高依赖于精确几何和动态重建的内窥镜手术下游应用的鲁棒性和一致性，促进相关领域的发展。

Abstract: Trocar ports are camera-fixed, pseudo-static structures that can persistently occlude laparoscopic views and attract disproportionate feature points due to specular, textured surfaces. This makes ports particularly detrimental to geometry-based downstream pipelines such as image stitching, 3D reconstruction, and visual SLAM, where dynamic or non-anatomical outliers degrade alignment and tracking stability. Despite this practical importance, explicit port labels are rare in public surgical datasets, and existing annotations often violate geometric consistency by masking the central lumen (opening), even when anatomical regions are visible through it. We present Cholec80-port, a high-fidelity trocar port segmentation dataset derived from Cholec80, together with a rigorous standard operating procedure (SOP) that defines a port-sleeve mask excluding the central opening. We additionally cleanse and unify existing public datasets under the same SOP. Experiments demonstrate that geometrically consistent annotations substantially improve cross-dataset robustness beyond what dataset size alone provides.

</details>


### [15] [Cross Pseudo Labeling For Weakly Supervised Video Anomaly Detection](https://arxiv.org/abs/2602.17077)
*Lee Dayeon,Kim Dongheyong,Park Chaewon,Woo Sungmin,Lee Sangyoun*

Main category: cs.CV

TL;DR: CPL-VAD 提出了一种双分支框架，通过跨分支伪标记传递互补优势，实现视频异常检测和异常类别识别的先进性能。


<details>
  <summary>Details</summary>
Motivation: 当前视频异常检测方法需要详细的标注，投入成本高，而只利用视频级别的标签，难以精确识别异常类别和定位异常片段。此研究旨在解决这一问题。

Method: CPL-VAD 引入了双分支结构，其中一个分支专注于片段级的异常定位，另一个分支通过视觉-语言对齐来识别异常事件类别。两个分支交互交换伪标签，以互补的方式增强彼此性能。

Result: 在 XD-Violence 和 UCF-Crime 数据集上，CPL-VAD 在异常检测和异常类别识别方面达到了最先进的性能。

Conclusion: 该研究提出了一种有效的弱监督视频异常检测方法，展示了该方法在实际应用中的潜力。

Abstract: Weakly supervised video anomaly detection aims to detect anomalies and identify abnormal categories with only video-level labels. We propose CPL-VAD, a dual-branch framework with cross pseudo labeling. The binary anomaly detection branch focuses on snippet-level anomaly localization, while the category classification branch leverages vision-language alignment to recognize abnormal event categories. By exchanging pseudo labels, the two branches transfer complementary strengths, combining temporal precision with semantic discrimination. Experiments on XD-Violence and UCF-Crime demonstrate that CPL-VAD achieves state-of-the-art performance in both anomaly detection and abnormal category classification.

</details>


### [16] [ComptonUNet: A Deep Learning Model for GRB Localization with Compton Cameras under Noisy and Low-Statistic Conditions](https://arxiv.org/abs/2602.17085)
*Shogo Sato,Kazuo Tanaka,Shojun Ogasawara,Kazuki Yamamoto,Kazuhiko Murasaki,Ryuichi Tanida,Jun Kataoka*

Main category: cs.CV

TL;DR: ComptonUNet 是一种能有效处理有限光子统计和强烈背景污染情况的混合深度学习框架，相较于现有方法，它在低统计量和高背景环境中实现了更准确的 GRB 定位。


<details>
  <summary>Details</summary>
Motivation: 探测和定位来自遥远宇宙的微弱伽马射线爆发（GRBs），用于深入研究早期恒星形成阶段，面临低光子统计和强背景噪声的挑战，为解决这一问题提出了 ComptonUNet 混合深度学习框架。

Method: ComptonUNet 融合了直接重构模型的统计高效性和基于图像的架构的降噪能力，通过模拟低地球轨道任务背景环境中的 GRB 类事件来进行性能评估。

Result: ComptonUNet 在低光子统计和高背景环境下的 GRB 定位中优于现有方法，展现出更好的定位准确性。

Conclusion: 研究结果表明，ComptonUNet 有望在伽马射线暴探测和定位方面提供更有效的方法。

Abstract: Gamma-ray bursts (GRBs) are among the most energetic transient phenomena in the universe and serve as powerful probes for high-energy astrophysical processes. In particular, faint GRBs originating from a distant universe may provide unique insights into the early stages of star formation. However, detecting and localizing such weak sources remains challenging owing to low photon statistics and substantial background noise. Although recent machine learning models address individual aspects of these challenges, they often struggle to balance the trade-off between statistical robustness and noise suppression. Consequently, we propose ComptonUNet, a hybrid deep learning framework that jointly processes raw data and reconstructs images for robust GRB localization. ComptonUNet was designed to operate effectively under conditions of limited photon statistics and strong background contamination by combining the statistical efficiency of direct reconstruction models with the denoising capabilities of image-based architectures. We perform realistic simulations of GRB-like events embedded in background environments representative of low-Earth orbit missions to evaluate the performance of ComptonUNet. Our results demonstrate that ComptonUNet significantly outperforms existing approaches, achieving improved localization accuracy across a wide range of low-statistic and high-background scenarios.

</details>


### [17] [3D Scene Rendering with Multimodal Gaussian Splatting](https://arxiv.org/abs/2602.17124)
*Chi-Shiang Gau,Konstantinos D. Polyzos,Athanasios Bacharis,Saketh Madhuvarasu,Tara Javidi*

Main category: cs.CV

TL;DR: 该研究提出了一种结合RF感知和GS（3D高斯洒点）技术的多模态框架，旨在提高3D场景重建与渲染在恶劣天气、低光照和部分遮挡条件下的鲁棒性和效率。


<details>
  <summary>Details</summary>
Motivation: 当前基于视觉的GS管线在光照条件欠佳或存在遮挡时表现不佳，而RF信号对天气、光照和遮挡具有较强抗性，因此将RF感知与GS技术结合以提高渲染质量。

Method: 研究构建了一个多模态框架，该框架利用汽车雷达等RF设备获取稀疏深度测量数据，用于GS函数的深度预测与初始化。

Result: 实验表明，该框架能够有效地生成高质量的3D点云，实现由RF洞察驱动的结构精度，从而提高在不同环境下的3D场景渲染保真度。

Conclusion: 该研究提出的方法为在复杂环境中实现高效且鲁棒的3D场景重建与渲染提供了一种新的解决方案。

Abstract: 3D scene reconstruction and rendering are core tasks in computer vision, with applications spanning industrial monitoring, robotics, and autonomous driving. Recent advances in 3D Gaussian Splatting (GS) and its variants have achieved impressive rendering fidelity while maintaining high computational and memory efficiency. However, conventional vision-based GS pipelines typically rely on a sufficient number of camera views to initialize the Gaussian primitives and train their parameters, typically incurring additional processing cost during initialization while falling short in conditions where visual cues are unreliable, such as adverse weather, low illumination, or partial occlusions. To cope with these challenges, and motivated by the robustness of radio-frequency (RF) signals to weather, lighting, and occlusions, we introduce a multimodal framework that integrates RF sensing, such as automotive radar, with GS-based rendering as a more efficient and robust alternative to vision-only GS rendering. The proposed approach enables efficient depth prediction from only sparse RF-based depth measurements, yielding a high-quality 3D point cloud for initializing Gaussian functions across diverse GS architectures. Numerical tests demonstrate the merits of judiciously incorporating RF sensing into GS pipelines, achieving high-fidelity 3D scene rendering driven by RF-informed structural accuracy.

</details>


### [18] [B$^3$-Seg: Camera-Free, Training-Free 3DGS Segmentation via Analytic EIG and Beta-Bernoulli Bayesian Updates](https://arxiv.org/abs/2602.17134)
*Hiromichi Kamata,Samuel Arthur Munro,Fuminori Homma*

Main category: cs.CV

TL;DR: B^3-Seg 提出了一个基于 Beta-Bernoulli 贝叶斯更新并利用分析化的期望信息增益（EIG）来选择下一个视图的快速且理论上可靠的方法，适用于无相机和无训练数据的 3D 螺旋体分割。


<details>
  <summary>Details</summary>
Motivation: 现有的 3D 螺旋体分割方法依赖于预定义的相机视角、地面真实标签或昂贵的重新训练，这使得它们在低延迟使用时不可行。B^3-Seg 提供了一种无需相机和无需训练数据的端到端方法，以实现快速高效的 3D 螺旋体分割。

Method: B^3-Seg 的方法包括将分割问题重新表述为基于贝塔-0-1 贝叶斯更新的序列模型，并通过计算分析化的期望信息增益（EIG）来动态选择下一个视图。这种方法保证了 EIG 的自适应单调性和次模性，从而产生了对最优视图采样策略的近似（1-1/e）。

Result: 在多个数据集上的实验表明，B^3-Seg 在几秒内实现端到端分割，其结果与高成本的监督方法相当，证明了其信息效率。

Conclusion: B^3-Seg 通过提供无相机和无训练数据的快速 3D 螺旋体分割方法，展示了其在实际应用中的潜力，实现了一个有效的、相互作用的 3D 螺旋体分割系统。

Abstract: Interactive 3D Gaussian Splatting (3DGS) segmentation is essential for real-time editing of pre-reconstructed assets in film and game production. However, existing methods rely on predefined camera viewpoints, ground-truth labels, or costly retraining, making them impractical for low-latency use. We propose B$^3$-Seg (Beta-Bernoulli Bayesian Segmentation for 3DGS), a fast and theoretically grounded method for open-vocabulary 3DGS segmentation under camera-free and training-free conditions. Our approach reformulates segmentation as sequential Beta-Bernoulli Bayesian updates and actively selects the next view via analytic Expected Information Gain (EIG). This Bayesian formulation guarantees the adaptive monotonicity and submodularity of EIG, which produces a greedy $(1{-}1/e)$ approximation to the optimal view sampling policy. Experiments on multiple datasets show that B$^3$-Seg achieves competitive results to high-cost supervised methods while operating end-to-end segmentation within a few seconds. The results demonstrate that B$^3$-Seg enables practical, interactive 3DGS segmentation with provable information efficiency.

</details>


### [19] [BadCLIP++: Stealthy and Persistent Backdoors in Multimodal Contrastive Learning](https://arxiv.org/abs/2602.17168)
*Siyuan Liang,Yongcheng Jing,Yingjie Wang,Jiaxing Huang,Ee-chien Chang,Dacheng Tao*

Main category: cs.CV

TL;DR: BadCLIP++ 提出了一种增强型框架，通过引入隐秘的 QRM 启发式触发器和正则化策略，解决了 multimodal contrastive learning 模型中的欺骗性和持久性攻击挑战，即使在重度检测或连续微调下也能实现高攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 目前针对多模态对比学习模型的后门攻击方法在面对强烈检测或持续微调时往往会失效，主要是因为跨模态的一致性问题和低投毒率下的梯度稀释导致的后门遗忘，因此，BadCLIP++ 致力于解决这些问题，提出了一种兼具隐秘性（stealthiness）和持久性（persistence）的框架以应对上述挑战。

Method: BadCLIP++ 采用的办法包括：1) 引入语义融合的 QRM 微触发器，该触发器可接近任务相关区域并嵌入不可感知的模式，同时保持干净数据的统计特征。2) 应用目标对齐的数据子集选择以在低注入率下增强信号。3) 稳定触发嵌入通过半径收缩和质心对齐，以及通过曲率控制和弹性权重概化稳定模型参数，确保模型参数处于低曲率的宽盆地，以抵御进一步的微调。

Result: 实验结果显示，使用仅 0.3% 的投毒速率，BadCLIP++ 可以在数字攻击环境中实现 99.99% 的攻击成功率（ASR），超过基线 11.4 个百分点。在十九种防御措施下，尽管 ASR 下降了不到 0.8% 的干净准确率，但仍保持在 99.90% 以上。此外，该方法也在物理攻击中表现出 65.03% 的成功概率，并且对水印去除的防御措施具有鲁棒性。

Conclusion: BadCLIP++ 为解决 multimodal contrastive learning 模型中遇到的欺骗性和持久性攻击的挑战提供了一个创新的解决方案，该方案能够在对抗更严格的检测和连续微调等复杂环境的同时保持高攻击成功率。

Abstract: Research on backdoor attacks against multimodal contrastive learning models faces two key challenges: stealthiness and persistence. Existing methods often fail under strong detection or continuous fine-tuning, largely due to (1) cross-modal inconsistency that exposes trigger patterns and (2) gradient dilution at low poisoning rates that accelerates backdoor forgetting. These coupled causes remain insufficiently modeled and addressed. We propose BadCLIP++, a unified framework that tackles both challenges. For stealthiness, we introduce a semantic-fusion QR micro-trigger that embeds imperceptible patterns near task-relevant regions, preserving clean-data statistics while producing compact trigger distributions. We further apply target-aligned subset selection to strengthen signals at low injection rates. For persistence, we stabilize trigger embeddings via radius shrinkage and centroid alignment, and stabilize model parameters through curvature control and elastic weight consolidation, maintaining solutions within a low-curvature wide basin resistant to fine-tuning. We also provide the first theoretical analysis showing that, within a trust region, gradients from clean fine-tuning and backdoor objectives are co-directional, yielding a non-increasing upper bound on attack success degradation. Experiments demonstrate that with only 0.3% poisoning, BadCLIP++ achieves 99.99% attack success rate (ASR) in digital settings, surpassing baselines by 11.4 points. Across nineteen defenses, ASR remains above 99.90% with less than 0.8% drop in clean accuracy. The method further attains 65.03% success in physical attacks and shows robustness against watermark removal defenses.

</details>


### [20] [NRGS-SLAM: Monocular Non-Rigid SLAM for Endoscopy via Deformation-Aware 3D Gaussian Splatting](https://arxiv.org/abs/2602.17182)
*Jiwei Shan,Zeyu Cai,Yirui Li,Yongbo Chen,Lijun Han,Yun-hui Liu,Hesheng Wang,Shing Shin Cheng*

Main category: cs.CV

TL;DR: NRGS-SLAM 提出了一种基于 3D 高斯散射的端镜非刚性 SLAM 系统，通过引入变形感知的 3D 高斯图优化，设计了鲁棒粗到细的位姿估计模块和变形可扩展的地图生成模块，实现了更准确的相机位姿估计和高质量的图像重建效果。


<details>
  <summary>Details</summary>
Motivation: 端镜场景中存在持续的软组织变形，打破传统刚性假设下的 SLAM 系统，传统方法难以解决这种变形带来的耦合不确定性问题，导致跟踪漂移和重建质量下降。

Method: NRGS-SLAM 引入了变形感知的 3D 高斯图，每个高斯原语附加了一个可以学习的变形概率，通过贝叶斯自监督策略优化。变形跟踪模块优先估计低变形区域，然后进行高效的框格帧变形更新。变形映射模块逐步扩展和细化地图，平衡表示能力和计算效率。同时，引入了统一的鲁棒几何损失，整合外部几何先验以克服单目非刚性 SLAM 的固有欠定性。

Result: 在多个公开的端镜数据集上进行的大量实验表明，NRGS-SLAM 较现有方法获得了更准确的相机位姿估计（RMSE 减少 50%）和更高的质量的逼真重建。

Conclusion: 该论文提出了一种创新的非刚性 SLAM 方法，成功解决了端镜场景下的变形问题，并通过实验证明了其优越性。

Abstract: Visual simultaneous localization and mapping (V-SLAM) is a fundamental capability for autonomous perception and navigation. However, endoscopic scenes violate the rigidity assumption due to persistent soft-tissue deformations, creating a strong coupling ambiguity between camera ego-motion and intrinsic deformation. Although recent monocular non-rigid SLAM methods have made notable progress, they often lack effective decoupling mechanisms and rely on sparse or low-fidelity scene representations, which leads to tracking drift and limited reconstruction quality. To address these limitations, we propose NRGS-SLAM, a monocular non-rigid SLAM system for endoscopy based on 3D Gaussian Splatting. To resolve the coupling ambiguity, we introduce a deformation-aware 3D Gaussian map that augments each Gaussian primitive with a learnable deformation probability, optimized via a Bayesian self-supervision strategy without requiring external non-rigidity labels. Building on this representation, we design a deformable tracking module that performs robust coarse-to-fine pose estimation by prioritizing low-deformation regions, followed by efficient per-frame deformation updates. A carefully designed deformable mapping module progressively expands and refines the map, balancing representational capacity and computational efficiency. In addition, a unified robust geometric loss incorporates external geometric priors to mitigate the inherent ill-posedness of monocular non-rigid SLAM. Extensive experiments on multiple public endoscopic datasets demonstrate that NRGS-SLAM achieves more accurate camera pose estimation (up to 50\% reduction in RMSE) and higher-quality photo-realistic reconstructions than state-of-the-art methods. Comprehensive ablation studies further validate the effectiveness of our key design choices. Source code will be publicly available upon paper acceptance.

</details>


### [21] [Selective Training for Large Vision Language Models via Visual Information Gain](https://arxiv.org/abs/2602.17186)
*Seulbi Lee,Sangheum Hwang*

Main category: cs.CV

TL;DR: 提出了一种名为视觉信息增益（VIG）的度量标准，通过这一标准实现了一种新的训练策略，专注于视觉信息丰富的样本和标记，从而提高了视觉接地并减少了语言偏见。


<details>
  <summary>Details</summary>
Motivation: 目前的大型视觉语言模型存在语言偏见问题，过度依赖语言信息而忽视视觉证据。以往为了解决这个问题，尝试了不同的方法，但缺乏量化每个训练样本或标记从视觉输入中受益程度的指标。

Method: 引入了视觉信息增益（VIG）作为度量标准，计算视觉输入对预测不确定性减少的大小。基于VIG，提出了一种新的选择性训练方案，优先训练高VIG的样本和标记。

Result: 新方法能够更精细地分析样本和标记级别的视觉信息增益，因此能够更好地实现视觉接地，减少语言偏见。通过这种方法，即使在减少了监督的情况下，模型也能达到更好的性能。

Conclusion: 通过视觉信息增益作为导向的训练方案，能够在保持模型性能的同时减少监督需求，解决大型视觉语言模型的语言偏见问题。

Abstract: Large Vision Language Models (LVLMs) have achieved remarkable progress, yet they often suffer from language bias, producing answers without relying on visual evidence. While prior work attempts to mitigate this issue through decoding strategies, architectural modifications, or curated instruction data, they typically lack a quantitative measure of how much individual training samples or tokens actually benefit from the image. In this work, we introduce Visual Information Gain (VIG), a perplexity-based metric that measures the reduction in prediction uncertainty provided by visual input. VIG enables fine-grained analysis at both sample and token levels, effectively highlighting visually grounded elements such as colors, spatial relations, and attributes. Leveraging this, we propose a VIG-guided selective training scheme that prioritizes high-VIG samples and tokens. This approach improves visual grounding and mitigates language bias, achieving superior performance with significantly reduced supervision by focusing exclusively on visually informative samples and tokens.

</details>


### [22] [EntropyPrune: Matrix Entropy Guided Visual Token Pruning for Multimodal Large Language Models](https://arxiv.org/abs/2602.17196)
*Yahong Wang,Juncheng Wu,Zhangkai Ni,Chengmei Yang,Yihang Liu,Longzhen Yang,Yuyin Zhou,Ying Wen,Lianghua He*

Main category: cs.CV

TL;DR: 提出了EntropyPrune框架，这是一种基于矩阵熵的文本-视觉标记剪枝方法，可以通过计算视觉标记的信息价值来选择剪枝阶段，并通过该过程实现铰链式加速。该方法解决了传统剪枝方法中依赖于注意力图和人工选择层等问题，适用于不同分辨率和基于视频的模型，展现出优越的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的标记剪枝方法大多依赖于静态选择和注意力图，这限制了它们在不同模型间的解释性和迁移性。

Method: 引入了基于矩阵熵的视角来识别'熵崩溃层'(ECL)，提出了EntropyPrune框架，使用剪枝矩阵来量化视觉标记的信息价值，实现无注意力图依赖的剪枝。

Result: 在多个多媒体基准上进行了实验，并展示了EntropyPrune在准确性和效率上的优越性。在LLaVA-1.5-7B上，与现有最佳剪枝方法相比，FLOPs减少了68.2%，同时保留了96.0%的原始性能。

Conclusion: EntropyPrune方法为多媒体大型语言模型的加速提供了一种有效的方法，能够在保持高效率的同时提高模型性能。

Abstract: Multimodal large language models (MLLMs) incur substantial inference cost due to the processing of hundreds of visual tokens per image. Although token pruning has proven effective for accelerating inference, determining when and where to prune remains largely heuristic. Existing approaches typically rely on static, empirically selected layers, which limit interpretability and transferability across models. In this work, we introduce a matrix-entropy perspective and identify an "Entropy Collapse Layer" (ECL), where the information content of visual representations exhibits a sharp and consistent drop, which provides a principled criterion for selecting the pruning stage. Building on this observation, we propose EntropyPrune, a novel matrix-entropy-guided token pruning framework that quantifies the information value of individual visual tokens and prunes redundant ones without relying on attention maps. Moreover, to enable efficient computation, we exploit the spectral equivalence of dual Gram matrices, reducing the complexity of entropy computation and yielding up to a 64x theoretical speedup. Extensive experiments on diverse multimodal benchmarks demonstrate that EntropyPrune consistently outperforms state-of-the-art pruning methods in both accuracy and efficiency. On LLaVA-1.5-7B, our method achieves a 68.2% reduction in FLOPs while preserving 96.0% of the original performance. Furthermore, EntropyPrune generalizes effectively to high-resolution and video-based models, highlighting the strong robustness and scalability in practical MLLM acceleration. The code will be publicly available at https://github.com/YahongWang1/EntropyPrune.

</details>


### [23] [GASS: Geometry-Aware Spherical Sampling for Disentangled Diversity Enhancement in Text-to-Image Generation](https://arxiv.org/abs/2602.17200)
*Ye Zhu,Kaleb S. Newman,Johannes F. Lutzeyer,Adriana Romero-Soriano,Michal Drozdzal,Olga Russakovsky*

Main category: cs.CV

TL;DR: 该研究通过几何视角引入了一种几何感知球面采样（GASS）方法，提高了文本到图像生成中图像的多样性。


<details>
  <summary>Details</summary>
Motivation: 现代文本到图像生成模型在处理给定提示生成图像多样性方面表现不佳，限制了用户选择并可能放大社会偏见。

Method: 研究引入了一种新的几何感知球面采样（GASS）方法，通过解耦文本嵌入和背景嵌入来增加生成图像嵌入的几何投影差异，并指导生成过程。

Result: 在不同的冻结T2I基础模型和基准测试上，实验显示GASS方法在保持图像保真度和语义一致性的同时，提高了图像多样性。

Conclusion: 研究表明，通过GASS方法，可以在不牺牲图像质量和语义匹配度的基础上，显著提高文本到图像生成的多样性。

Abstract: Despite high semantic alignment, modern text-to-image (T2I) generative models still struggle to synthesize diverse images from a given prompt. This lack of diversity not only restricts user choice, but also risks amplifying societal biases. In this work, we enhance the T2I diversity through a geometric lens. Unlike most existing methods that rely primarily on entropy-based guidance to increase sample dissimilarity, we introduce Geometry-Aware Spherical Sampling (GASS) to enhance diversity by explicitly controlling both prompt-dependent and prompt-independent sources of variation. Specifically, we decompose the diversity measure in CLIP embeddings using two orthogonal directions: the text embedding, which captures semantic variation related to the prompt, and an identified orthogonal direction that captures prompt-independent variation (e.g., backgrounds). Based on this decomposition, GASS increases the geometric projection spread of generated image embeddings along both axes and guides the T2I sampling process via expanded predictions along the generation trajectory. Our experiments on different frozen T2I backbones (U-Net and DiT, diffusion and flow) and benchmarks demonstrate the effectiveness of disentangled diversity enhancement with minimal impact on image fidelity and semantic alignment.

</details>


### [24] [A Multi-modal Detection System for Infrastructure-based Freight Signal Priority](https://arxiv.org/abs/2602.17252)
*Ziyan Zhang,Chuheng Wei,Xuanpeng Zhao,Siyan Li,Will Snyder,Mike Stas,Peng Hao,Kanok Boriboonsomsin,Guoyuan Wu*

Main category: cs.CV

TL;DR: 本文介绍了一种基于基础设施的多模态货车检测系统，该系统结合了LiDAR和相机传感器，通过无线通信实现同步数据传输，并采用聚类和深度学习检测方法结合卡尔曼滤波跟踪，以实现实时稳定感知。实验证明，该系统能够以高时空分辨率可靠地监测货车移动。


<details>
  <summary>Details</summary>
Motivation: 随着交通流量的增长，货车信号优先（FSP）的需求增加，但现有技术无法提供准确和及时的感知。本文旨在设计并实现一种基于基础设施的多模态货车检测系统，以提供准确的车辆类型、位置和速度感知，以便实现有效的优先控制策略。

Method: 本文采用了结合了LiDAR和相机传感器的混合传感架构，通过无线通信进行同步数据传输，并结合了基于聚类和深度学习的检测方法，以及卡尔曼滤波跟踪，实现稳定实时的感知。同时，采用地心参考框架进行LiDAR测量注册，以支持车道级定位并实现一致的车辆跟踪。

Result: 实地测试表明，该系统能够在高时空分辨率下可靠地监控货车移动，为支持FSP应用提供了基础设施感知系统的实用见解。

Conclusion: 本文设计并实现了基于基础设施的多模态货车检测系统，该系统将为FSP应用提供准确、可靠的感知技术。

Abstract: Freight vehicles approaching signalized intersections require reliable detection and motion estimation to support infrastructure-based Freight Signal Priority (FSP). Accurate and timely perception of vehicle type, position, and speed is essential for enabling effective priority control strategies. This paper presents the design, deployment, and evaluation of an infrastructure-based multi-modal freight vehicle detection system integrating LiDAR and camera sensors. A hybrid sensing architecture is adopted, consisting of an intersection-mounted subsystem and a midblock subsystem, connected via wireless communication for synchronized data transmission. The perception pipeline incorporates both clustering-based and deep learning-based detection methods with Kalman filter tracking to achieve stable real-time performance. LiDAR measurements are registered into geodetic reference frames to support lane-level localization and consistent vehicle tracking. Field evaluations demonstrate that the system can reliably monitor freight vehicle movements at high spatio-temporal resolution. The design and deployment provide practical insights for developing infrastructure-based sensing systems to support FSP applications.

</details>


### [25] [EA-Swin: An Embedding-Agnostic Swin Transformer for AI-Generated Video Detection](https://arxiv.org/abs/2602.17260)
*Hung Mai,Loi Dinh,Duc Hai Nguyen,Dat Do,Luong Doan,Khanh Nguyen Quoc,Huan Vu,Phong Ho,Naeem Ul Islam,Tuan Do*

Main category: cs.CV

TL;DR: EA-Swin通过在预训练的视频嵌入上直接建模时空依赖关系，实现了对现代AI生成视频的高精度检测，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成技术的快速发展使得现有依赖浅层特征或计算密集型模型的检测方法暴露出不足，因此需提出更为有效的检测方法。

Method: EA-Swin采用Embedding-Agnostic Swin Transformer，通过因子化窗口注意力设计来直接建模预训练视频嵌入中的时空依赖关系，适用于通用的基于块的编码器。

Result: EA-Swin在主要视频生成器上的检测准确率达到0.97-0.99，比现有最佳方法高出5-20%；同时保持了良好的跨分布泛化能力。

Conclusion: EA-Swin为现代AI生成视频的检测提供了一种可扩展且稳健的解决方案，展示了在不同生成器分布下的优越性能。

Abstract: Recent advances in foundation video generators such as Sora2, Veo3, and other commercial systems have produced highly realistic synthetic videos, exposing the limitations of existing detection methods that rely on shallow embedding trajectories, image-based adaptation, or computationally heavy MLLMs. We propose EA-Swin, an Embedding-Agnostic Swin Transformer that models spatiotemporal dependencies directly on pretrained video embeddings via a factorized windowed attention design, making it compatible with generic ViT-style patch-based encoders. Alongside the model, we construct the EA-Video dataset, a benchmark dataset comprising 130K videos that integrates newly collected samples with curated existing datasets, covering diverse commercial and open-source generators and including unseen-generator splits for rigorous cross-distribution evaluation. Extensive experiments show that EA-Swin achieves 0.97-0.99 accuracy across major generators, outperforming prior SoTA methods (typically 0.8-0.9) by a margin of 5-20%, while maintaining strong generalization to unseen distributions, establishing a scalable and robust solution for modern AI-generated video detection.

</details>


### [26] [Physics Encoded Spatial and Temporal Generative Adversarial Network for Tropical Cyclone Image Super-resolution](https://arxiv.org/abs/2602.17277)
*Ruoyi Zhang,Jiawei Yuan,Lujia Ye,Runling Yu,Liling Zhao*

Main category: cs.CV

TL;DR: 文章提出了一种名为PESTGAN的新方法，旨在利用物理约束提高热带气旋卫星图像的解析度，在结构保真度和感知质量上表现出色，且物理一致性更强。


<details>
  <summary>Details</summary>
Motivation: 现有的基于深度学习的超分辨率方法在处理卫星图像序列时忽视了支配云运动的物理规律，本研究旨在通过结合物理约束提高热带气旋图像的解析度。

Method: PESTGAN通过设计一个分离生成器架构整合PhyCell模块，使用受约束的卷积近似涡旋方程，并编码得到的近似物理动力学作为隐式潜在表示，以分离物理动态和视 觉纹理。同时引入了双重判别器框架，通过时间判别器保证运动一致性和空间现实性。

Result: 在数字台风数据集上的4倍放大实验中，PESTGAN在结构保真度和感知质量上表现出色，且与现有方法相比，在重建气象上可验证的云结构方面具有更高的物理保真度。

Conclusion: PESTGAN方法不仅在像素级准确性上具有竞争力，还在重建气象上可验证的云结构方面显著优于现有方法，是提高热带气旋图像解析度的有效方法。

Abstract: High-resolution satellite imagery is indispensable for tracking the genesis, intensification, and trajectory of tropical cyclones (TCs). However, existing deep learning-based super-resolution (SR) methods often treat satellite image sequences as generic videos, neglecting the underlying atmospheric physical laws governing cloud motion. To address this, we propose a Physics Encoded Spatial and Temporal Generative Adversarial Network (PESTGAN) for TC image super-resolution. Specifically, we design a disentangled generator architecture incorporating a PhyCell module, which approximates the vorticity equation via constrained convolutions and encodes the resulting approximate physical dynamics as implicit latent representations to separate physical dynamics from visual textures. Furthermore, a dual-discriminator framework is introduced, employing a temporal discriminator to enforce motion consistency alongside spatial realism. Experiments on the Digital Typhoon dataset for 4$\times$ upscaling demonstrate that PESTGAN establishes a better performance in structural fidelity and perceptual quality. While maintaining competitive pixel-wise accuracy compared to existing approaches, our method significantly excels in reconstructing meteorologically plausible cloud structures with superior physical fidelity.

</details>


### [27] [Attachment Anchors: A Novel Framework for Laparoscopic Grasping Point Prediction in Colorectal Surgery](https://arxiv.org/abs/2602.17310)
*Dennis N. Schneider,Lars Wagner,Daniel Rueckert,Dirk Wilhelm*

Main category: cs.CV

TL;DR: 本研究提出了一种结构化的表示——附着锚点，通过编码结肠手术中组织与其解剖附着物之间的局部几何和机械关系，提高钳夹点预测的准确性，特别是在未见手术和未见器械操作者的情境下。


<details>
  <summary>Details</summary>
Motivation: 在复杂程度高且持续时间长的结肠手术中准确预测钳夹点是一个挑战。鉴于其重复的组织操作和未充分利用的学习环境，该研究旨在通过引入附着锚点作为中介表示，实现基于机器学习的组织操作支持。

Method: 该方法包括开发一种结构化表示算法，来表示组织与解剖附着物之间的局部几何和机械关系，并使用从腹腔镜图像中预测附着锚点的方法将其整合到基于机器学习的抓取框架中。

Result: 研究在90例结肠手术数据集上验证了附着锚点的有效性，其在未见手术和未见手术者的设置下，显著提高了钳夹点预测的准确性。

Conclusion: 这些结果表明，附着锚点成为一种有效的基于学习的组织操纵中在结肠手术中的中间表示形式。

Abstract: Accurate grasping point prediction is a key challenge for autonomous tissue manipulation in minimally invasive surgery, particularly in complex and variable procedures such as colorectal interventions. Due to their complexity and prolonged duration, colorectal procedures have been underrepresented in current research. At the same time, they pose a particularly interesting learning environment due to repetitive tissue manipulation, making them a promising entry point for autonomous, machine learning-driven support. Therefore, in this work, we introduce attachment anchors, a structured representation that encodes the local geometric and mechanical relationships between tissue and its anatomical attachments in colorectal surgery. This representation reduces uncertainty in grasping point prediction by normalizing surgical scenes into a consistent local reference frame. We demonstrate that attachment anchors can be predicted from laparoscopic images and incorporated into a grasping framework based on machine learning. Experiments on a dataset of 90 colorectal surgeries demonstrate that attachment anchors improve grasping point prediction compared to image-only baselines. There are particularly strong gains in out-of-distribution settings, including unseen procedures and operating surgeons. These results suggest that attachment anchors are an effective intermediate representation for learning-based tissue manipulation in colorectal surgery.

</details>


### [28] [Polaffini: A feature-based approach for robust affine and polyaffine image registration](https://arxiv.org/abs/2602.17337)
*Antoine Legouhy,Cosimo Campo,Ross Callaghan,Hojjat Azadbakht,Hui Zhang*

Main category: cs.CV

TL;DR: Polaffini 是一个基于解剖学的稳健且多功能的框架，使用深学习预训练分割模型自动生成一致的解剖特征点，实现高效的全局和局部仿射配准，最终生成从仿射到多项仿射的平滑变换，广泛应用于独立配准和后续非线性配准预对齐。


<details>
  <summary>Details</summary>
Motivation: 当前的医学图像配准方法多依赖于强度基线方法，而基于特征的方法由于特征提取的挑战而较少使用。基于近年来在深度学习中的进展，可以使用预训练的分割模型进行快速可靠的解剖分割，这激发了创建新的基于解剖的图像配准算法的想法。

Method: Polaffini 获取由这些分割区域各个位置中的解剖特征点，并通过计算这些特征点的质心来提取。通过使用具有闭式解的全局局部仿射配准，实现从仿射到多项仿射变换的生成，并且这种变换具有变形形变特性，因此比常规的仿射变换具有更多的自由度。这种方法被应用于独立的配准和后续非线性配准的预对齐。

Result: 实验结果表明，Polaffini 在结构对齐方面优于现有方法，并且为后续非线性配准提供了更好的初始化。

Conclusion: 总体而言，Polaffini 提供了一种高效、稳健且准确的配准框架，特别适合集成到医学图像处理流水线中。

Abstract: In this work we present Polaffini, a robust and versatile framework for anatomically grounded registration. Medical image registration is dominated by intensity-based registration methods that rely on surrogate measures of alignment quality. In contrast, feature-based approaches that operate by identifying explicit anatomical correspondences, while more desirable in theory, have largely fallen out of favor due to the challenges of reliably extracting features. However, such challenges are now significantly overcome thanks to recent advances in deep learning, which provide pre-trained segmentation models capable of instantly delivering reliable, fine-grained anatomical delineations. We aim to demonstrate that these advances can be leveraged to create new anatomically-grounded image registration algorithms. To this end, we propose Polaffini, which obtains, from these segmented regions, anatomically grounded feature points with 1-to-1 correspondence in a particularly simple way: extracting their centroids. These enable efficient global and local affine matching via closed-form solutions. Those are used to produce an overall transformation ranging from affine to polyaffine with tunable smoothness. Polyaffine transformations can have many more degrees of freedom than affine ones allowing for finer alignment, and their embedding in the log-Euclidean framework ensures diffeomorphic properties. Polaffini has applications both for standalone registration and as pre-alignment for subsequent non-linear registration, and we evaluate it against popular intensity-based registration techniques. Results demonstrate that Polaffini outperforms competing methods in terms of structural alignment and provides improved initialisation for downstream non-linear registration. Polaffini is fast, robust, and accurate, making it particularly well-suited for integration into medical image processing pipelines.

</details>


### [29] [Tree crop mapping of South America reveals links to deforestation and conservation](https://arxiv.org/abs/2602.17372)
*Yuchang Jiang,Anton Raichuk,Xiaoye Tong,Vivien Sainte Fare Garnot,Daniel Ortiz-Gonzalo,Dan Morris,Konrad Schindler,Jan Dirk Wegner,Maxim Neumann*

Main category: cs.CV

TL;DR: 该研究利用_sentinel-1_和_sentinel-2_卫星图像时间序列建立了南美洲首个10米分辨率的树木作物地图，识别出约1100万公顷的树木作物，占2000-2020年间森林覆盖率下降的23%，旨在解决现有监管地图对小型农业尤其是小农林业的误分类问题，提供高分辨率基线以支持有效的、包容性和公平性的保护政策。


<details>
  <summary>Details</summary>
Motivation: 现有监管地图对大型和小型农业的分类错误可能导致小规模农民受到虚假的砍伐森林警报和不公平的处罚。这项研究旨在提供一个高分辨率的基线地图，以纠正这些问题，促进公平和包容性政策。

Method: 研究使用了多模态、时空深度学习模型，基于Sentinel-1和Sentinel-2卫星图像时间序列来进行树木作物的地图生成。

Result: 研究生成了南美洲首个10米分辨率的树木作物地图，识别了约1100万公顷的树木作物，其中23%的区域与2000-2020年的森林覆盖率下降有关。

Conclusion: 这项研究提供了对树木作物的高分辨率基线地图，有助于制定有效的、包容性的和公平的保护政策，为零森林砍伐政策提供支持。

Abstract: Monitoring tree crop expansion is vital for zero-deforestation policies like the European Union's Regulation on Deforestation-free Products (EUDR). However, these efforts are hindered by a lack of highresolution data distinguishing diverse agricultural systems from forests. Here, we present the first 10m-resolution tree crop map for South America, generated using a multi-modal, spatio-temporal deep learning model trained on Sentinel-1 and Sentinel-2 satellite imagery time series. The map identifies approximately 11 million hectares of tree crops, 23% of which is linked to 2000-2020 forest cover loss. Critically, our analysis reveals that existing regulatory maps supporting the EUDR often classify established agriculture, particularly smallholder agroforestry, as "forest". This discrepancy risks false deforestation alerts and unfair penalties for small-scale farmers. Our work mitigates this risk by providing a high-resolution baseline, supporting conservation policies that are effective, inclusive, and equitable.

</details>


### [30] [DRetHTR: Linear-Time Decoder-Only Retentive Network for Handwritten Text Recognition](https://arxiv.org/abs/2602.17387)
*Changhun Kim,Martin Mayr,Thomas Gorges,Fei Wu,Mathias Seuret,Andreas Maier,Vincent Christlein*

Main category: cs.CV

TL;DR: DRetHTR 使用 RetNet 架构在手写文本识别中实现与标准解码器 Transformer 相当的性能，同时提高了解码速度和内存效率。


<details>
  <summary>Details</summary>
Motivation: 现有的 Transformer 手写文本识别系统因为不断增长的关键值缓存而变得解码时速度慢和内存使用量大。为了改善这一问题，作者提出 DRetHTR 模型。

Method: DRetHTR 使用 RetNet 架构，通过使用 softmax-free retention 和多尺度序列先验，避免了 KV 缓存的增长，同时引入了逐层伽马缩放，以恢复局部到全局的归纳偏置。

Result: DRetHTR 的推理速度比同等大小的解码器 Transformer 快 1.6 到 1.9 倍，并且内存使用量减少了 38-42%，同时没有损失准确性。DRetHTR 达到了最好的测试字符错误率，实现了在 IAM-A、RIMES 和 Bentham 数据集上的 2.26%、1.81% 和 3.46% 的表现。

Conclusion: DRetHTR 证明了解码器独立试用 RetNet 架构可以实现与标准 Transformer 相当的手写文本识别性能，同时极大地提高了解码速度和内存效率。

Abstract: State-of-the-art handwritten text recognition (HTR) systems commonly use Transformers, whose growing key-value (KV) cache makes decoding slow and memory-intensive. We introduce DRetHTR, a decoder-only model built on Retentive Networks (RetNet). Compared to an equally sized decoder-only Transformer baseline, DRetHTR delivers 1.6-1.9x faster inference with 38-42% less memory usage, without loss of accuracy. By replacing softmax attention with softmax-free retention and injecting multi-scale sequential priors, DRetHTR avoids a growing KV cache: decoding is linear in output length in both time and memory. To recover the local-to-global inductive bias of attention, we propose layer-wise gamma scaling, which progressively enlarges the effective retention horizon in deeper layers. This encourages early layers to model short-range dependencies and later layers to capture broader context, mitigating the flexibility gap introduced by removing softmax. Consequently, DRetHTR achieves best reported test character error rates of 2.26% (IAM-A, en), 1.81% (RIMES, fr), and 3.46% (Bentham, en), and is competitive on READ-2016 (de) with 4.21%. This demonstrates that decoder-only RetNet enables Transformer-level HTR accuracy with substantially improved decoding speed and memory efficiency.

</details>


### [31] [SpectralGCD: Spectral Concept Selection and Cross-modal Representation Learning for Generalized Category Discovery](https://arxiv.org/abs/2602.17395)
*Lorenzo Caselli,Marco Mistretta,Simone Magistri,Andrew D. Bagdanov*

Main category: cs.CV

TL;DR: SpectralGCD是通过使用CLIP实现的新型的高效多模态方法，专为解决泛化类别发现问题。方法利用强教师模型测量的跨模态相似性构建统一的跨模态表示，并通过Spectral Filtering筛选相关概念，确保学习到的表示保留足够的语义质量。这种方法在6个基准测试中实现了接近或显著优于现有方法的准确性，同时计算成本较低。


<details>
  <summary>Details</summary>
Motivation: 传统的方法在仅使用图像特征进行参数分类训练时容易过拟合到旧类别，而现有的多模态方法虽然能提升性能，但通常受限于独立处理模态的方式和高昂的计算成本。SpectralGCD旨在解决这些问题。

Method: SpectralGCD采用CLIP跨模态图像概念相似性构建统一的跨模态表示。每张图像被表示为一个来自大型任务无关字典的语义概念混合，这有助于锚定学习过程，减少对无关视觉线索的依赖。为了保持由高效学生模型学习的表示的语义质量，引入了Spectral Filtering技术，该技术利用强教师模型测量的跨模态协方差矩阵来自动挑选相关概念。通过来自同一个教师的前向和反向知识蒸馏，确保学生的跨模态表示既具有足够的语义又能良好对齐。

Result: SpectralGCD在六个基准测试中实现了接近或显著优于当前最先进的方法的准确性，且计算成本较低。此外，已经开源了该方法的代码。

Conclusion: SpectralGCD方法通过利用CLIP模型实现高效多模态表示，提供了一种改进泛化类别发现任务性能的新途径，同时保持了较低的计算成本。

Abstract: Generalized Category Discovery (GCD) aims to identify novel categories in unlabeled data while leveraging a small labeled subset of known classes. Training a parametric classifier solely on image features often leads to overfitting to old classes, and recent multimodal approaches improve performance by incorporating textual information. However, they treat modalities independently and incur high computational cost. We propose SpectralGCD, an efficient and effective multimodal approach to GCD that uses CLIP cross-modal image-concept similarities as a unified cross-modal representation. Each image is expressed as a mixture over semantic concepts from a large task-agnostic dictionary, which anchors learning to explicit semantics and reduces reliance on spurious visual cues. To maintain the semantic quality of representations learned by an efficient student, we introduce Spectral Filtering which exploits a cross-modal covariance matrix over the softmaxed similarities measured by a strong teacher model to automatically retain only relevant concepts from the dictionary. Forward and reverse knowledge distillation from the same teacher ensures that the cross-modal representations of the student remain both semantically sufficient and well-aligned. Across six benchmarks, SpectralGCD delivers accuracy comparable to or significantly superior to state-of-the-art methods at a fraction of the computational cost. The code is publicly available at: https://github.com/miccunifi/SpectralGCD.

</details>


### [32] [A High-Level Survey of Optical Remote Sensing](https://arxiv.org/abs/2602.17397)
*Panagiotis Koletsis,Vasilis Efthymiou,Maria Vakalopoulou,Nikos Komodakis,Anastasios Doulamis,Georgios Th. Papadopoulos*

Main category: cs.CV

TL;DR: 本文提供了遥感领域综合概述，涵盖了各种任务、能力和方法，同时提供了关键信息如数据集和见解，旨在为新进入该领域的研究人员提供指南。


<details>
  <summary>Details</summary>
Motivation: 由于无人机在遥感领域的广泛应用，现有的光学遥感文献非常庞大且涉及多样化的任务和方法。当前，尚缺乏一个综合性的文献概述和指南。

Method: 本文主要采用文献综述的方法，综合分析了各类光学遥感任务、能力和方法，并提供了大量数据集和见解作为支撑。

Result: 本文提供了光学遥感领域的全面视角，包括任务、能力和方法的综合概述，以及关键的资源信息和见解，有助于研究人员更好地了解该领域。

Conclusion: 本文填补了该领域文章综述方面的空白，为新进入光学遥感领域的研究人员提供了一本指南手册。

Abstract: In recent years, significant advances in computer vision have also propelled progress in remote sensing. Concurrently, the use of drones has expanded, with many organizations incorporating them into their operations. Most drones are equipped by default with RGB cameras, which are both robust and among the easiest sensors to use and interpret. The body of literature on optical remote sensing is vast, encompassing diverse tasks, capabilities, and methodologies. Each task or methodology could warrant a dedicated survey. This work provides a comprehensive overview of the capabilities of the field, while also presenting key information, such as datasets and insights. It aims to serve as a guide for researchers entering the field, offering high-level insights and helping them focus on areas most relevant to their interests. To the best of our knowledge, no existing survey addresses this holistic perspective.

</details>


### [33] [EAGLE: Expert-Augmented Attention Guidance for Tuning-Free Industrial Anomaly Detection in Multimodal Large Language Models](https://arxiv.org/abs/2602.17419)
*Xiaomeng Peng,Xilang Huang,Seon Han Choi*

Main category: cs.CV

TL;DR: EAGLE 提出了一种无需微调的框架，通过集成专家模型来指导 MLLMs 进行工业异常检测，增强了检测的准确性与解释性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在异常检测上往往仅提供二进制决策且缺乏语义解释，而 MLLMs 虽然有潜力产生细粒度的、基于语言的分析，但现有方法往往需要昂贵的微调且并未能显著提升异常检测准确率。

Method: EAGLE 通过一种参数更新方式，直接利用专家模型来引导 MLLMs，减少传统微调步骤，同时利用中间层的注意力分布研究和验证了其效果。

Result: 在 MVTec-AD 和 VisA 数据集上的实验表明，EAGLE 能在不进行参数更新的情况下显著提升 MLLMs 的异常检测性能，结果堪比基于微调的方法。

Conclusion: EAGLE 提供了一种有效的方法来提高 MLLMs 的异常检测性能，同时也增强了异常描述的解释性，有望在智能制造中发挥重要作用。

Abstract: Industrial anomaly detection is important for smart manufacturing, but many deep learning approaches produce only binary decisions and provide limited semantic explanations. Multimodal large language models (MLLMs) can potentially generate fine-grained, language-based analyses, yet existing methods often require costly fine-tuning and do not consistently improve anomaly detection accuracy compared to lightweight specialist detectors. We propose expert-augmented attention guidance for industrial anomaly detection in MLLMs (EAGLE), a tuning-free framework that integrates outputs from expert model to guide MLLMs toward both accurate detection and interpretable anomaly descriptions. We further study how EAGLE affects MLLMs internals by examining the attention distribution of MLLMs to the anomalous image regions in the intermediate layers. We observe that successful anomaly detection is associated with increased attention concentration on anomalous regions, and EAGLE tends to encourage this alignment. Experiments on MVTec-AD and VisA show that EAGLE improves anomaly detection performance across multiple MLLMs without any parameter updates, achieving results comparable to fine-tuning based methods. Code is available at \href{https://github.com/shengtun/Eagle}{https://github.com/shengtun/Eagle}

</details>


### [34] [4D Monocular Surgical Reconstruction under Arbitrary Camera Motions](https://arxiv.org/abs/2602.17473)
*Jiwei Shan,Zeyu Cai,Cheng-Tai Hsieh,Yirui Li,Hao Liu,Lijun Han,Hesheng Wang,Shing Shin Cheng*

Main category: cs.CV

TL;DR: 本文提出了一种针对具有任意相机运动的单目内窥镜序列的高质量4D重建框架Local-EndoGS，该框架引入了基于窗口的全局表示，采用粗到细策略增强了初始化处理，并结合2D像素轨迹和物理运动先验提高了形变合理性。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要针对固定内窥镜视角和要求深度先验或结构光的变形场景，无法处理真实临床环境中单目序列的大范围运动。因此，需要一种新的框架来适应任意相机运动的内窥镜序列。

Method: Local-EndoGS框架采用局部变形场景模型，基于窗口的全局表示，结合多视图几何、窗口间信息和单目深度先验的粗到细策略进行优化，同时加入长时间2D像素轨迹约束和物理运动先验以改善形变合理性。

Result: 实验表明，Local-EndoGS在多个公开的具有不同变形场景和相机运动的内窥镜数据集上，重建结果在视觉质量和几何准确度方面均优于现有方法。

Conclusion: 本工作提出了一种适用于具有任意相机运动的内窥镜序列的4D重建框架Local-EndoGS，证明了其在现实临床场景下的适用性和优越性。

Abstract: Reconstructing deformable surgical scenes from endoscopic videos is challenging and clinically important. Recent state-of-the-art methods based on implicit neural representations or 3D Gaussian splatting have made notable progress. However, most are designed for deformable scenes with fixed endoscope viewpoints and rely on stereo depth priors or accurate structure-from-motion for initialization and optimization, limiting their ability to handle monocular sequences with large camera motion in real clinical settings. To address this, we propose Local-EndoGS, a high-quality 4D reconstruction framework for monocular endoscopic sequences with arbitrary camera motion. Local-EndoGS introduces a progressive, window-based global representation that allocates local deformable scene models to each observed window, enabling scalability to long sequences with substantial motion. To overcome unreliable initialization without stereo depth or accurate structure-from-motion, we design a coarse-to-fine strategy integrating multi-view geometry, cross-window information, and monocular depth priors, providing a robust foundation for optimization. We further incorporate long-range 2D pixel trajectory constraints and physical motion priors to improve deformation plausibility. Experiments on three public endoscopic datasets with deformable scenes and varying camera motions show that Local-EndoGS consistently outperforms state-of-the-art methods in appearance quality and geometry. Ablation studies validate the effectiveness of our key designs. Code will be released upon acceptance at: https://github.com/IRMVLab/Local-EndoGS.

</details>


### [35] [QuPAINT: Physics-Aware Instruction Tuning Approach to Quantum Material Discovery](https://arxiv.org/abs/2602.17478)
*Xuan-Bac Nguyen,Hoang-Quan Nguyen,Sankalp Pandey,Tim Faltermeier,Nicholas Borys,Hugh Churchill,Khoa Luu*

Main category: cs.CV

TL;DR: 本文提出了一种新的基于物理的多模态框架，该框架包括一个物理合成数据生成器（Synthia）和第一份大规模指令数据集（QMat-Instruct），并提出了一种物理感知的指令调整方法（QuPAINT），以提高量子材料图像特征的鲁棒性和区分性。


<details>
  <summary>Details</summary>
Motivation: 现有光学显微镜图像中的二维量子材料分类存在挑战，因为缺乏物理先验知识，现有模型难以推广到新材料或硬件条件下。本文为了克服这些限制，提出了新的基于物理的多模态框架。

Method: 该方法包括三个主要部分：1. 物理合成数据生成器（Synthia）模拟量子材料在薄层干涉下的光学响应；2. 大规模物理感知指令数据集（QMat-Instruct），用于多功能大规模语言模型的学习；3. 物理感知指令调整方法（QuPAINT），将物理先验知识与视觉嵌入融合。

Result: 该框架生成了高质量的物理真实数据，并建立了一个涵盖多种材料、基板和成像设置的基准测试集（QF-Bench），为量子材料的光学显微镜图像特征提供了新的理解和方法。

Conclusion: 本文通过结合物理感知技术和大规模数据集，提出了一种新的方法，旨在改善和评估光学显微镜图像中二维量子材料的特征表示和理解能力。

Abstract: Characterizing two-dimensional quantum materials from optical microscopy images is challenging due to the subtle layer-dependent contrast, limited labeled data, and significant variation across laboratories and imaging setups. Existing vision models struggle in this domain since they lack physical priors and cannot generalize to new materials or hardware conditions. This work presents a new physics-aware multimodal framework that addresses these limitations from both the data and model perspectives. We first present Synthia, a physics-based synthetic data generator that simulates realistic optical responses of quantum material flakes under thin-film interference. Synthia produces diverse and high-quality samples, helping reduce the dependence on expert manual annotation. We introduce QMat-Instruct, the first large-scale instruction dataset for quantum materials, comprising multimodal, physics-informed question-answer pairs designed to teach Multimodal Large Language Models (MLLMs) to understand the appearance and thickness of flakes. Then, we propose Physics-Aware Instruction Tuning (QuPAINT), a multimodal architecture that incorporates a Physics-Informed Attention module to fuse visual embeddings with optical priors, enabling more robust and discriminative flake representations. Finally, we establish QF-Bench, a comprehensive benchmark spanning multiple materials, substrates, and imaging settings, offering standardized protocols for fair and reproducible evaluation.

</details>


### [36] [Tracing Copied Pixels and Regularizing Patch Affinity in Copy Detection](https://arxiv.org/abs/2602.17484)
*Yichen Lu,Siwei Nie,Minlong Lu,Xudong Yang,Xiaobo Zhang,Peng Zhang*

Main category: cs.CV

TL;DR: 该研究通过提出PixTrace模块和CopyNCE损失函数，解决了现有视图级对比方法在处理复杂编辑时的不足，实现了对图像编辑内容的像素级可追踪性与片段级相似学习的融合，显著提高了图像复制检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视图级对比方法在处理复杂的图像编辑时无法实现细粒度的对应学习，本研究旨在通过像素级跟踪模块和几何引导的对比损失函数，提升图像复制检测的性能。

Method: 该方法包含两个创新点：1) PixTrace模块，用于维护编辑过程中像素坐标的显式空间映射；2) CopyNCE损失函数，利用PixTrace验证的映射信息指导片段间的几何相似性学习。整体制定了一个从像素级跟踪到片段级相似性的学习框架。

Result: 实验结果表明，该方法在DISC21数据集上达到了SOTA性能，匹配器的表现为88.7% uAP和83.9% RP90，描述符的表现为72.6% uAP和68.4% RP90，并且在解释性方面也优于现有方法。

Conclusion: 该研究提供了一种有效的图像复制检测方法，通过结合像素级跟踪和片段级相似性学习，显著提高了图像编辑内容的检测性能，并展示了其在实际应用中的潜力。

Abstract: Image Copy Detection (ICD) aims to identify manipulated content between image pairs through robust feature representation learning. While self-supervised learning (SSL) has advanced ICD systems, existing view-level contrastive methods struggle with sophisticated edits due to insufficient fine-grained correspondence learning. We address this limitation by exploiting the inherent geometric traceability in edited content through two key innovations. First, we propose PixTrace - a pixel coordinate tracking module that maintains explicit spatial mappings across editing transformations. Second, we introduce CopyNCE, a geometrically-guided contrastive loss that regularizes patch affinity using overlap ratios derived from PixTrace's verified mappings. Our method bridges pixel-level traceability with patch-level similarity learning, suppressing supervision noise in SSL training. Extensive experiments demonstrate not only state-of-the-art performance (88.7% uAP / 83.9% RP90 for matcher, 72.6% uAP / 68.4% RP90 for descriptor on DISC21 dataset) but also better interpretability over existing methods.

</details>


### [37] [FoundationPose-Initialized 3D-2D Liver Registration for Surgical Augmented Reality](https://arxiv.org/abs/2602.17517)
*Hanyuan Zhang,Lucas He,Runlong He,Abdolrahim Kadkhodamohammadi,Danail Stoyanov,Brian R. Davidson,Evangelos B. Mazomenos,Matthew J. Clarkson*

Main category: cs.CV

TL;DR: 本文提出了一种通过整合内窥镜深度图和基于基础姿态估计器的方法来改进腹腔镜肝脏手术中肿瘤定位的增强现实技术。该方法通过使用非刚性迭代最近点算法（NICP）代替基于有限元的变形，降低了工程和建模复杂性。在实际患者数据上的测试表明，该方法能够达到临床相关精度，为有限元基变形提供了一个轻量级、工程友好型的替代方案。


<details>
  <summary>Details</summary>
Motivation: 现有的注册管道通常依赖于器官轮廓，柔型（非刚性）对齐通常采用与降维或机器学习组件耦合的有限元模型。本文旨在通过提出一种替代，降低模型构建和工程实现的复杂度。

Method: 本文通过结合内窥镜深度图与基础姿态估计器来实现摄像头-肝脏姿态估计，并使用非刚性迭代最近点算法（NICP）替换基于有限元的变形。

Result: 在实际患者数据上，深度增强的基础姿态方法达到了9.91毫米的平均注册误差，在3个试验案例中。结合刚性-NICP注册方法比刚性注册方法表现更优，说明了NICP作为一种替代有限元变形模型的有效性。

Conclusion: 该注册管道实现了临床相关精度，提供了一个替代有限元变形的轻量级、工程友好的方案。

Abstract: Augmented reality can improve tumor localization in laparoscopic liver surgery. Existing registration pipelines typically depend on organ contours; deformable (non-rigid) alignment is often handled with finite-element (FE) models coupled to dimensionality-reduction or machine-learning components. We integrate laparoscopic depth maps with a foundation pose estimator for camera-liver pose estimation and replace FE-based deformation with non-rigid iterative closest point (NICP) to lower engineering/modeling complexity and expertise requirements. On real patient data, the depth-augmented foundation pose approach achieved 9.91 mm mean registration error in 3 cases. Combined rigid-NICP registration outperformed rigid-only registration, demonstrating NICP as an efficient substitute for finite-element deformable models. This pipeline achieves clinically relevant accuracy while offering a lightweight, engineering-friendly alternative to FE-based deformation.

</details>


### [38] [LATA: Laplacian-Assisted Transductive Adaptation for Conformal Uncertainty in Medical VLMs](https://arxiv.org/abs/2602.17535)
*Behzad Bozorgtabar,Dwarikanath Mahapatra,Sudipta Roy,Muzammal Naseer,Imran Razzak,Zongyuan Ge*

Main category: cs.CV

TL;DR: LATA（拉普拉斯辅助交互适应）是一种无训练数据和标签的校准方法，通过平滑似然估计和改进的分数计算，提高了医学影像识别的效率和准确度，同时保持了预测集的覆盖率，并在多种医疗视觉语言模型和下游任务中表现出优胜。


<details>
  <summary>Details</summary>
Motivation: 当前的校准方法在医学领域存在预测集大小过大、类别间覆盖率不平衡等问题，尤其是在样本量较少的情况下。此外，直接将校准标签应用于预测可能破坏数据的随机性。因此，提出LATA方法以解决这些问题。

Method: LATA方法首先通过拉普拉斯辅助KNN图来平滑零样本概率，并利用CCC（闭合类外部验证）更新来改进预测。其次，提出了一种失败感知的校准分数，用于增强视觉语言不确定性框架下的预测可靠性。LATA是黑盒的，计算量小，可控可选地使用先验信息。

Result: 在三种医学视觉语言模型和九个下游任务上，LATA方法能够减小预测集大小并减少类别间的差距，同时保持目标覆盖率，优于之前的交互校准基线方法，并与使用标签的方法的差距缩小，同时计算资源消耗更少。

Conclusion: LATA方法在多种医疗视觉语言模型和下游任务中展现出优越的能力，能够在不牺牲预测集覆盖率的基础上改进零样本预测，提供了一种低计算开销、鲁棒性强的校准策略。

Abstract: Medical vision-language models (VLMs) are strong zero-shot recognizers for medical imaging, but their reliability under domain shift hinges on calibrated uncertainty with guarantees. Split conformal prediction (SCP) offers finite-sample coverage, yet prediction sets often become large (low efficiency) and class-wise coverage unbalanced-high class-conditioned coverage gap (CCV), especially in few-shot, imbalanced regimes; moreover, naively adapting to calibration labels breaks exchangeability and voids guarantees. We propose \texttt{\textbf{LATA}} (Laplacian-Assisted Transductive Adaptation), a \textit{training- and label-free} refinement that operates on the joint calibration and test pool by smoothing zero-shot probabilities over an image-image k-NN graph using a small number of CCCP mean-field updates, preserving SCP validity via a deterministic transform. We further introduce a \textit{failure-aware} conformal score that plugs into the vision-language uncertainty (ViLU) framework, providing instance-level difficulty and label plausibility to improve prediction set efficiency and class-wise balance at fixed coverage. \texttt{\textbf{LATA}} is black-box (no VLM updates), compute-light (windowed transduction, no backprop), and includes an optional prior knob that can run strictly label-free or, if desired, in a label-informed variant using calibration marginals once. Across \textbf{three} medical VLMs and \textbf{nine} downstream tasks, \texttt{\textbf{LATA}} consistently reduces set size and CCV while matching or tightening target coverage, outperforming prior transductive baselines and narrowing the gap to label-using methods, while using far less compute. Comprehensive ablations and qualitative analyses show that \texttt{\textbf{LATA}} sharpens zero-shot predictions without compromising exchangeability.

</details>


### [39] [GraphThinker: Reinforcing Video Reasoning with Event Graph Thinking](https://arxiv.org/abs/2602.17555)
*Zixu Cheng,Da Li,Jian Hu,Ziquan Liu,Wei Li,Shaogang Gong*

Main category: cs.CV

TL;DR: 该研究提出了一种名为GraphThinker的方法，通过强化微调的方式构建事件级别的场景图来增强视觉定位，并引入视觉注意力奖励进一步减少视频推理中的幻觉。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大型语言模型在视频推理中虽然能够通过密集字幕或视频摘要来推断事件关系，但缺乏因果理解。这导致模型在视频推理过程中产生幻觉。

Method: GraphThinker方法首先使用多模态大型语言模型（MLLM）构建基于事件的视频场景图（EVSG），以明确建模事件内的关系和事件间的关系。在此过程中，将形成的场景图纳入到MLLM作为中间推理过程，并引入视觉注意力奖励，加强视频定位以进一步减少幻觉。

Result: GraphThinker在RexTime和VidHalluc两个数据集上的评估表明，它能够更精准地捕捉物体和事件关系，并减少视频推理中的幻觉。

Conclusion: GraphThinker通过构建事件级别的场景图来增强多模态模型的视频推理能力，有效减少了幻觉问题。

Abstract: Video reasoning requires understanding the causal relationships between events in a video. However, such relationships are often implicit and costly to annotate manually. While existing multimodal large language models (MLLMs) often infer event relations through dense captions or video summaries for video reasoning, such modeling still lacks causal understanding. Without explicit causal structure modeling within and across video events, these models suffer from hallucinations during the video reasoning. In this work, we propose GraphThinker, a reinforcement finetuning-based method that constructs structural event-level scene graphs and enhances visual grounding to jointly reduce hallucinations in video reasoning. Specifically, we first employ an MLLM to construct an event-based video scene graph (EVSG) that explicitly models both intra- and inter-event relations, and incorporate these formed scene graphs into the MLLM as an intermediate thinking process. We also introduce a visual attention reward during reinforcement finetuning, which strengthens video grounding and further mitigates hallucinations. We evaluate GraphThinker on two datasets, RexTime and VidHalluc, where it shows superior ability to capture object and event relations with more precise event localization, reducing hallucinations in video reasoning compared to prior methods.

</details>


### [40] [Art2Mus: Artwork-to-Music Generation via Visual Conditioning and Large-Scale Cross-Modal Alignment](https://arxiv.org/abs/2602.17599)
*Ivan Rinaldi,Matteo Mendula,Nicola Fanelli,Florence Levé,Matteo Testi,Giovanna Castellano,Gennaro Vessio*

Main category: cs.CV

TL;DR: 本文介绍了一个名为ArtSound的大型多模态数据集，包含105,884个艺术作品与音乐的配对，其中丰富了双模态描述，并提出了一种名为ArtToMus的新框架，能够直接将数字化的艺术作品转换为音乐，而无需图像到文本的转换或基于语言的语义监督。


<details>
  <summary>Details</summary>
Motivation: 现有的图像条件下的系统存在两个基本限制：一是通常在自然照片上进行训练，从而限制了它们对艺术品中的丰富语义、风格和文化内容的捕捉能力；二是大多数模型依赖于图像到文本的转换阶段，使用语言作为语义捷径来简化条件设定，进而阻止直接的视觉到声音学习。

Method: 提出ArtSound数据集和ArtToMus框架，通过从扩展的ArtGraph和Free Music Archive获取的艺术作品-音乐配对及其双重模态描述，该框架直接将数字化的艺术作品映射到音乐上，不依赖于图像到文本的转换或基于语言的语义监督，而是将视觉嵌入投影到潜在扩散模型的条件空间来指导音乐合成。

Result: 实验结果显示，ArtToMus生成了音乐上连贯且风格一致的输出，能够反映源艺术品的显著视觉提示。尽管与文本条件下的系统相比，绝对对齐评分较低（鉴于移除语言监督的难度大幅增加），ArtToMus仍实现了竞争性的感知质量和跨模态对应的实质性意义。

Conclusion: 本文通过建立直接视觉到音乐转换的独特且具有挑战性的研究方向，为多媒体艺术、文化遗产和AI辅助的创意实践提供了支持资源，并公开发布代码和数据集。

Abstract: Music generation has advanced markedly through multimodal deep learning, enabling models to synthesize audio from text and, more recently, from images. However, existing image-conditioned systems suffer from two fundamental limitations: (i) they are typically trained on natural photographs, limiting their ability to capture the richer semantic, stylistic, and cultural content of artworks; and (ii) most rely on an image-to-text conversion stage, using language as a semantic shortcut that simplifies conditioning but prevents direct visual-to-audio learning. Motivated by these gaps, we introduce ArtSound, a large-scale multimodal dataset of 105,884 artwork-music pairs enriched with dual-modality captions, obtained by extending ArtGraph and the Free Music Archive. We further propose ArtToMus, the first framework explicitly designed for direct artwork-to-music generation, which maps digitized artworks to music without image-to-text translation or language-based semantic supervision. The framework projects visual embeddings into the conditioning space of a latent diffusion model, enabling music synthesis guided solely by visual information. Experimental results show that ArtToMus generates musically coherent and stylistically consistent outputs that reflect salient visual cues of the source artworks. While absolute alignment scores remain lower than those of text-conditioned systems-as expected given the substantially increased difficulty of removing linguistic supervision-ArtToMus achieves competitive perceptual quality and meaningful cross-modal correspondence. This work establishes direct visual-to-music generation as a distinct and challenging research direction, and provides resources that support applications in multimedia art, cultural heritage, and AI-assisted creative practice. Code and dataset will be publicly released upon acceptance.

</details>


### [41] [Adapting Actively on the Fly: Relevance-Guided Online Meta-Learning with Latent Concepts for Geospatial Discovery](https://arxiv.org/abs/2602.17605)
*Jowaria Khan,Anindya Sarkar,Yevgeniy Vorobeychik,Elizabeth Bondi-Kelly*

Main category: cs.CV

TL;DR: 该研究提出了一种统一的地理发现框架，结合了主动学习、在线元学习和概念引导推理，以应对资源受限和动态环境中的稀疏、偏置地理真相问题。该框架通过概念加权的不确定性抽样策略和基于概念感知的元批次形成策略，优化了目标搜索的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 在资源有限和动态变化的实地环境中，传统的方法难以有效找出隐藏的目标，尤其是当数据收集成本高且地理真相稀疏或偏置时。本文通过引入一个统一的地理发现框架，解决了这些问题，旨在提高在动态环境和有限资源下的目标发现效率。

Method: 该方法采用了一种新的概念加权不确定性抽样策略，通过结合学习得到的与特定领域相关性的概念来调整不确定性水平。同时，还采用了一种新的元学习策略，通过引入概念感知来优化在线元学习更新，从而提高泛化能力。

Result: 实验结果表明，该方法能够在有限的数据下有效识别目标，并具有较好的环境适应性。特别是在一个真实世界的PFAS（全氟和多氟烷基物质）致癌污染数据集中，展示了该方法的可靠性和有效性。

Conclusion: 总之，该研究提出了一种创新的方法来克服资源约束和动态环境下的地理信息发现难题，并证明了其在实际应用中的潜力。

Abstract: In many real-world settings, such as environmental monitoring, disaster response, or public health, with costly and difficult data collection and dynamic environments, strategically sampling from unobserved regions is essential for efficiently uncovering hidden targets under tight resource constraints. Yet, sparse and biased geospatial ground truth limits the applicability of existing learning-based methods, such as reinforcement learning. To address this, we propose a unified geospatial discovery framework that integrates active learning, online meta-learning, and concept-guided reasoning. Our approach introduces two key innovations built on a shared notion of *concept relevance*, which captures how domain-specific factors influence target presence: a *concept-weighted uncertainty sampling strategy*, where uncertainty is modulated by learned relevance based on readily-available domain-specific concepts (e.g., land cover, source proximity); and a *relevance-aware meta-batch formation strategy* that promotes semantic diversity during online-meta updates, improving generalization in dynamic environments. Our experiments include testing on a real-world dataset of cancer-causing PFAS (Per- and polyfluoroalkyl substances) contamination, showcasing our method's reliability at uncovering targets with limited data and a varying environment.

</details>


### [42] [CORAL: Correspondence Alignment for Improved Virtual Try-On](https://arxiv.org/abs/2602.17636)
*Jiyoung Kim,Youngjin Shin,Siyoon Jin,Dahyun Chung,Jisu Nam,Tongmin Kim,Jongjae Park,Hyeonwoo Kang,Seungryong Kim*

Main category: cs.CV

TL;DR: 该研究解决了现有虚拟试穿方法在未配对设置下难以保持细部服装细节的问题，提出了一种基于DiT的CORAL框架，通过精确的人衣查询-键匹配和注意力分布的锐化，增强了全局形状转换和局部细节保真。


<details>
  <summary>Details</summary>
Motivation: 现有方法在未配对条件下难以保持服装细节，且未明确说明对应关系在DiT中的形成机制。

Method: 该方法通过全三维注意力机制分析，揭示了精确的人衣查询-键匹配是成功的重要因素。CORAL框架引入了综合外部对应关系的具体注意力对齐机制。

Result: 实验表明，CORAL在基础模型上显著提高了全局形状传递和局部细节保持。

Conclusion: 研究通过提出CORAL框架改善了虚拟试穿的表现，为未来该领域的研究提供了有价值的参考。

Abstract: Existing methods for Virtual Try-On (VTON) often struggle to preserve fine garment details, especially in unpaired settings where accurate person-garment correspondence is required. These methods do not explicitly enforce person-garment alignment and fail to explain how correspondence emerges within Diffusion Transformers (DiTs). In this paper, we first analyze full 3D attention in DiT-based architecture and reveal that the person-garment correspondence critically depends on precise person-garment query-key matching within the full 3D attention. Building on this insight, we then introduce CORrespondence ALignment (CORAL), a DiT-based framework that explicitly aligns query-key matching with robust external correspondences. CORAL integrates two complementary components: a correspondence distillation loss that aligns reliable matches with person-garment attention, and an entropy minimization loss that sharpens the attention distribution. We further propose a VLM-based evaluation protocol to better reflect human preference. CORAL consistently improves over the baseline, enhancing both global shape transfer and local detail preservation. Extensive ablations validate our design choices.

</details>


### [43] [IntRec: Intent-based Retrieval with Contrastive Refinement](https://arxiv.org/abs/2602.17639)
*Pourya Shamsolmoali,Masoumeh Zareapoor,Eric Granger,Yue Lu*

Main category: cs.CV

TL;DR: IntRec 是一个交互式的物体检索框架，能够根据用户的反馈逐步细化预测，这种方法在 LVIS 和 LVIS-Ambiguous 测试集上表现出显著提高的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的开放词汇量检测器仅能一次检测，不能根据用户的反馈进行预测的改进。IntRec 通过引入意图状态 IS 来改进这一问题，使系统能够处理含糊不清或相似对象的查询。

Method: IntRec 使用意图状态（IS）来维护正的锚点记忆集（确认的线索）和负的约束记忆集（排斥的假设）。通过对比对齐功能对候选物体进行排名，最大化相似的正线索并惩罚排斥的线索，实现细粒度的去混淆。

Result: IntRec 在 LVIS 数据集上的 AP 为 35.4，超过了 OVMR、CoDet 和 CAKE 等现有方法。在 LVIS-Ambiguous 挑战基准测试中，在单次纠正反馈后，IntRec 提升了 7.9 AP，每次交互的额外延迟不足 30 毫秒。

Conclusion: IntRec 通过提供交互机制改进了物体检索的准确性，无需额外的监督，其在处理复杂和含糊查询方面的表现优于现有的单次检测方法。

Abstract: Retrieving user-specified objects from complex scenes remains a challenging task, especially when queries are ambiguous or involve multiple similar objects. Existing open-vocabulary detectors operate in a one-shot manner, lacking the ability to refine predictions based on user feedback. To address this, we propose IntRec, an interactive object retrieval framework that refines predictions based on user feedback. At its core is an Intent State (IS) that maintains dual memory sets for positive anchors (confirmed cues) and negative constraints (rejected hypotheses). A contrastive alignment function ranks candidate objects by maximizing similarity to positive cues while penalizing rejected ones, enabling fine-grained disambiguation in cluttered scenes. Our interactive framework provides substantial improvements in retrieval accuracy without additional supervision. On LVIS, IntRec achieves 35.4 AP, outperforming OVMR, CoDet, and CAKE by +2.3, +3.7, and +0.5, respectively. On the challenging LVIS-Ambiguous benchmark, it improves performance by +7.9 AP over its one-shot baseline after a single corrective feedback, with less than 30 ms of added latency per interaction.

</details>


### [44] [When Vision Overrides Language: Evaluating and Mitigating Counterfactual Failures in VLAs](https://arxiv.org/abs/2602.17659)
*Yu Fang,Yuchun Feng,Dong Jing,Jiaqi Liu,Yue Yang,Zhenyu Wei,Daniel Szafir,Mingyu Ding*

Main category: cs.CV

TL;DR: 研究发现，现有VLAs模型在缺乏场景特定监督的情况下容易出现以视觉捷径为基础的反事实失败。为解决这一问题，作者提出了一种名为CAG的方法，通过一种简单有效的双分支推理方案，明确正则化VLAs的语言条件，从而减少对视觉捷径的依赖，提高模型在未见过的任务上的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前VLAs模型在仅有语言指示时，倾向于依赖视觉捷径而非遵循语言指示，特别是在缺乏强场景特异性监督的情况下。这种反事实失败会影响模型的实用性和鲁棒性。

Method: 作者提出了一个名为LIBERO-CF的新反事实基准，通过在不同视觉合理布局下分配替代指令来评估语言遵循能力。他们还提出了一种名为CAG（反事实行动引导）的方法，该方法通过将标准VLAs策略与非语言条件的视觉-动作模块相结合，增强了反事实推理。

Result: 实验表明，CAG方法能够显著减少反事实失败，特别是在未见过的任务上表现出更明显的改善。与未使用CAG的模型相比，CAG提高了语言遵循准确性9.7%并在某些任务中提高了任务成功率8.5%，同时在现实世界的评估中也减少了反事实失败，并提高了任务成功率17.2%。

Conclusion: CAG为解决VLAs中的反事实失败提供了简单有效的方法。与其他需要额外数据或模型修改的方法相比，不需要额外的演示或修改现有架构或预训练模型即可应用CAG，使其具有广泛的应用前景。

Abstract: Vision-Language-Action models (VLAs) promise to ground language instructions in robot control, yet in practice often fail to faithfully follow language. When presented with instructions that lack strong scene-specific supervision, VLAs suffer from counterfactual failures: they act based on vision shortcuts induced by dataset biases, repeatedly executing well-learned behaviors and selecting objects frequently seen during training regardless of language intent. To systematically study it, we introduce LIBERO-CF, the first counterfactual benchmark for VLAs that evaluates language following capability by assigning alternative instructions under visually plausible LIBERO layouts. Our evaluation reveals that counterfactual failures are prevalent yet underexplored across state-of-the-art VLAs. We propose Counterfactual Action Guidance (CAG), a simple yet effective dual-branch inference scheme that explicitly regularizes language conditioning in VLAs. CAG combines a standard VLA policy with a language-unconditioned Vision-Action (VA) module, enabling counterfactual comparison during action selection. This design reduces reliance on visual shortcuts, improves robustness on under-observed tasks, and requires neither additional demonstrations nor modifications to existing architectures or pretrained models. Extensive experiments demonstrate its plug-and-play integration across diverse VLAs and consistent improvements. For example, on LIBERO-CF, CAG improves $π_{0.5}$ by 9.7% in language following accuracy and 3.6% in task success on under-observed tasks using a training-free strategy, with further gains of 15.5% and 8.5%, respectively, when paired with a VA model. In real-world evaluations, CAG reduces counterfactual failures of 9.4% and improves task success by 17.2% on average.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [45] [References Improve LLM Alignment in Non-Verifiable Domains](https://arxiv.org/abs/2602.16802)
*Kejian Shi,Yixin Liu,Peifeng Wang,Alexander R. Fabbri,Shafiq Joty,Arman Cohan*

Main category: cs.CL

TL;DR: 本文介绍了一种使用参考指导的语言模型评估器来提升语言模型对齐的策略，通过实验表明这种方法在不具有验证手段的领域中具有显著优势。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习带验证奖励（RLVR）方法不能直接应用于缺乏真实验证者的领域，如语言模型对齐。为此，研究团队设计了一种使用参考指导的语言模型评估器，旨在填补这一研究空白。

Method: 研究团队首先设计了评估协议，以提升基于语言模型的评估器，使用前沿模型的参考输出来增强评估能力。强评估者也可以受益于高质量的人类撰写的参考。然后，通过高级别参考进行自我改进，将使用参考导向的评估者作为奖惩者来提高语言模型。

Result: 实验结果表明，参考导向的自我改进方法在AlpacaEval与Arena-Hard等评估任务中表现出色，与直接的样本移位训练和基于参考的自由自我改进方法相比，获得了显著的性能提升。具体来说，该方法在Llama-3-8B-Instruct上的得分分别为73.1%和58.7%，在Qwen2.5-7B上的得分为70.0%和74.1%，相比任务分别提高了20.2和17.1个点，以及5.3和3.6个点。

Conclusion: 研究证明，参考导向的语言模型评估器在非验证领域可以有效启用语言模型的后训练。这种方法展示了在没有验证手段的环境中进行语言模型对齐的潜力。

Abstract: While Reinforcement Learning with Verifiable Rewards (RLVR) has shown strong effectiveness in reasoning tasks, it cannot be directly applied to non-verifiable domains lacking ground-truth verifiers, such as LLM alignment. In this work, we investigate whether reference-guided LLM-evaluators can bridge this gap by serving as soft "verifiers". First, we design evaluation protocols that enhance LLM-based evaluators for LLM alignment using reference outputs. Through comprehensive experiments, we show that a reference-guided approach substantially improves the accuracy of less capable LLM-judges using references from frontier models; stronger LLM-judges can also be enhanced by high-quality (i.e., human-written) references. Building on these improved judges, we demonstrate the utility of high-quality references in alignment tuning, where LLMs guided with references are used as judges to self-improve. We show that reference-guided self-improvement yields clear gains over both direct SFT on reference outputs and self-improvement with reference-free judges, achieving performance comparable to training with ArmoRM, a strong finetuned reward model. Specifically, our method achieves 73.1% and 58.7% on AlpacaEval and Arena-Hard with Llama-3-8B-Instruct, and 70.0% and 74.1% with Qwen2.5-7B, corresponding to average absolute gains of +20.2 / +17.1 points over SFT distillation and +5.3 / +3.6 points over reference-free self-improvement on AlpacaEval / Arena-Hard. These results highlight the potential of using reference-guided LLM-evaluators to enable effective LLM post-training in non-verifiable domains.

</details>


### [46] [Evaluating Monolingual and Multilingual Large Language Models for Greek Question Answering: The DemosQA Benchmark](https://arxiv.org/abs/2602.16811)
*Charalampos Mastrokostas,Nikolaos Giarelis,Nikos Karacapilidis*

Main category: cs.CL

TL;DR: 该研究创建了DemosQA数据集，提出了一种内存高效的LLM评估框架，并对11种单语言和多语言LLM进行了全面评估，以解答希腊语问答任务。


<details>
  <summary>Details</summary>
Motivation: 多语言模型的研究主要集中在资源丰富的语言上，单语言模型在这些领域尚不充分。因此，研究希望填补这一空白，特别是在希腊语等资源较匮乏的语言上，以更好地反映社会和文化特征。

Method: 研究创建了一个名为DemosQA的新数据集，该数据集通过社交媒体用户的提问和社区审核的答案构建。此外，还开发了一种适应各种QA数据集和语言的内存高效评估框架，并使用三种不同的提示策略对模型进行了评估。

Result: 研究对比了11种单语言和多语言LLM在6个人工策划的希腊语QA数据集上的表现，提供了详细的评估结果。

Conclusion: 研究展示了新数据集和评估框架的有效性，并为未来的工作提供了实用资源。

Abstract: Recent advancements in Natural Language Processing and Deep Learning have enabled the development of Large Language Models (LLMs), which have significantly advanced the state-of-the-art across a wide range of tasks, including Question Answering (QA). Despite these advancements, research on LLMs has primarily targeted high-resourced languages (e.g., English), and only recently has attention shifted toward multilingual models. However, these models demonstrate a training data bias towards a small number of popular languages or rely on transfer learning from high- to under-resourced languages; this may lead to a misrepresentation of social, cultural, and historical aspects. To address this challenge, monolingual LLMs have been developed for under-resourced languages; however, their effectiveness remains less studied when compared to multilingual counterparts on language-specific tasks. In this study, we address this research gap in Greek QA by contributing: (i) DemosQA, a novel dataset, which is constructed using social media user questions and community-reviewed answers to better capture the Greek social and cultural zeitgeist; (ii) a memory-efficient LLM evaluation framework adaptable to diverse QA datasets and languages; and (iii) an extensive evaluation of 11 monolingual and multilingual LLMs on 6 human-curated Greek QA datasets using 3 different prompting strategies. We release our code and data to facilitate reproducibility.

</details>


### [47] [One-step Language Modeling via Continuous Denoising](https://arxiv.org/abs/2602.16813)
*Chanhyuk Lee,Jaehoon Yoo,Manan Agarwal,Sheel Shah,Jerry Huang,Aditi Raghunathan,Seunghoon Hong,Nicholas M. Boffi,Jinwoo Kim*

Main category: cs.CL

TL;DR: 该研究提出了一种基于流的连续去噪语言模型（FLM），通过预测未受污染的数据来训练模型，并通过时间重参数化改进了训练稳定性和生成质量。FLM能够实现单步生成，超过之前8步生成的质量。研究人员还提出了一个精简过的流映射语言模型（FMLM），进一步提高了速度和质量。


<details>
  <summary>Details</summary>
Motivation: 现有的基于离散扩散的语言模型虽然能够提供更快的生成速度，但在少量步骤生成过程中却往往产生较差的质量，无法充分发挥其优势。因此，研究者希望探索一种能够兼顾质量和速度的语言模型。

Method: 研究通过重新审视离散模态上的流，构建了一种基于流的模型（FLM），该模型针对一热编码进行欧几里得去噪。研究者通过预测清洁数据而非直接去噪来训练模型，同时引入了一种简单的时间重参数化方法来提高训练稳定性和生成质量。此外，他们还提出了精简的流映射语言模型（FMLM），以实现少量步骤的高效生成。

Result: 在LM1B和OWT语言数据集上，FLM的生成质量达到了与最先进的离散扩散模型相当的水平。使用FMLM，该方法在各个领域都优于近期的少量步骤语言模型，其单步生成效果也超过了之前8步生成的质量。

Conclusion: 这项工作质疑了离散扩散过程是实现在离散模态上生成模型所必需的这一假设，并为加速大规模基于流的语言建模铺平了道路。

Abstract: Language models based on discrete diffusion have attracted widespread interest for their potential to provide faster generation than autoregressive models. In practice, however, they exhibit a sharp degradation of sample quality in the few-step regime, failing to realize this promise. Here we show that language models leveraging flow-based continuous denoising can outperform discrete diffusion in both quality and speed. By revisiting the fundamentals of flows over discrete modalities, we build a flow-based language model (FLM) that performs Euclidean denoising over one-hot token encodings. We show that the model can be trained by predicting the clean data via a cross entropy objective, where we introduce a simple time reparameterization that greatly improves training stability and generation quality. By distilling FLM into its associated flow map, we obtain a distilled flow map language model (FMLM) capable of few-step generation. On the LM1B and OWT language datasets, FLM attains generation quality matching state-of-the-art discrete diffusion models. With FMLM, our approach outperforms recent few-step language models across the board, with one-step generation exceeding their 8-step quality. Our work calls into question the widely held hypothesis that discrete diffusion processes are necessary for generative modeling over discrete modalities, and paves the way toward accelerated flow-based language modeling at scale. Code is available at https://github.com/david3684/flm.

</details>


### [48] [Claim Automation using Large Language Model](https://arxiv.org/abs/2602.16836)
*Zhengda Mo,Zhiyu Quan,Eli O'Donohue,Kaiwen Zhong*

Main category: cs.CL

TL;DR: 研究利用历史保修声明数据对预训练的大语言模型进行领域特定微调，生成结构化的纠正建议，并通过多维度评估框架验证其效果。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在受监管的数据敏感领域如保险中的部署仍然有限，为此研究提出了一种新的治理意识语言模型组件，旨在解决这一问题。

Method: 使用LoRA方法微调预训练的LLM，限定模型应用于理赔处理流程最初的决策模块中，以加速理赔调解员的决策过程。通过结合自动语义相似度指标和人工评估的多维度评价框架进行测试。

Result: 这种模块在多维度评估框架下的表现优于商业通用的大语言模型和基于提示的模型，约80%的测试案例与真实的纠正行动几乎完全一致。

Conclusion: 研究表明，领域适应性微调能够更紧密地将模型输出分布与实际运营数据对齐，证明其在保险应用中作为可靠且可控的基础组件的潜力。

Abstract: While Large Language Models (LLMs) have achieved strong performance on general-purpose language tasks, their deployment in regulated and data-sensitive domains, including insurance, remains limited. Leveraging millions of historical warranty claims, we propose a locally deployed governance-aware language modeling component that generates structured corrective-action recommendations from unstructured claim narratives. We fine-tune pretrained LLMs using Low-Rank Adaptation (LoRA), scoping the model to an initial decision module within the claim processing pipeline to speed up claim adjusters' decisions. We assess this module using a multi-dimensional evaluation framework that combines automated semantic similarity metrics with human evaluation, enabling a rigorous examination of both practical utility and predictive accuracy. Our results show that domain-specific fine-tuning substantially outperforms commercial general-purpose and prompt-based LLMs, with approximately 80% of the evaluated cases achieving near-identical matches to ground-truth corrective actions. Overall, this study provides both theoretical and empirical evidence to prove that domain-adaptive fine-tuning can align model output distributions more closely with real-world operational data, demonstrating its promise as a reliable and governable building block for insurance applications.

</details>


### [49] [BanglaSummEval: Reference-Free Factual Consistency Evaluation for Bangla Summarization](https://arxiv.org/abs/2602.16843)
*Ahmed Rafid,Rumman Adib,Fariya Ahmed,Ajwad Abrar,Mohammed Saidul Islam*

Main category: cs.CL

TL;DR: BanglaSummEval 提出了一种基于无参考问题回答框架来评估孟加拉语总结的准确性和内容覆盖。该框架利用自动生成的问题和答案来衡量事实准确性，并通过统一的设计降低系统复杂性和计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有的许多评估指标忽略了许多资源匮乏语言（如孟加拉语）的评估，且通常依赖于参考总结。为了解决这个问题，BanglaSummEval 提供了一种无参考的方式，用于评估孟加拉语摘要的准确性和覆盖度，以推动医疗和新闻等高风险领域的可靠文本摘要的健康发展。

Method: 该框架利用单个经过多语言指令调优的语言模型来生成问题、回答问题、提取候选答案及重要性加权。同时，它还采用了 BERTScore-Recall 来确保答案之间的语义一致性。

Result: BanglaSummEval 在300个人撰写的教育和医疗领域的孟加拉语摘要上进行了验证，并且结果与专业的人类评估高度相关（皮尔逊相关系数为0.694， spearman相关系数为0.763）。

Conclusion: BanglaSummEval 提供了可解释并且逐步诊断的评估结果，为资源匮乏语言的可靠性评估提供了一个有效的透明解决方案。

Abstract: Evaluating factual consistency is essential for reliable text summarization, particularly in high-stakes domains such as healthcare and news. However, most existing evaluation metrics overlook Bangla, a widely spoken yet under-resourced language, and often depend on reference summaries. We introduce BanglaSummEval, a reference-free, question-answering-based framework for evaluating factual consistency in Bangla summarization. The proposed method assesses both factual accuracy and content coverage through automatically generated questions and answers derived from the source document and the summary. A single multilingual instruction-tuned language model handles question generation, question answering, candidate answer extraction, and question importance weighting. This unified design reduces system complexity and computational cost. To capture semantic consistency beyond surface-level overlap, we use BERTScore-Recall for answer comparison. We validate BanglaSummEval on 300 human-written summaries from educational and medical domains, demonstrating strong correlation with expert human judgments (Pearson's $r = 0.694$, Spearman's $ρ= 0.763$). By providing interpretable, step-wise diagnostics alongside reliable evaluation scores, BanglaSummEval offers a practical and transparent solution for factual consistency evaluation in low-resource language settings.

</details>


### [50] [Meenz bleibt Meenz, but Large Language Models Do Not Speak Its Dialect](https://arxiv.org/abs/2602.16852)
*Minh Duc Bui,Manuel Mager,Peter Herbert Kann,Katharina von der Wense*

Main category: cs.CL

TL;DR: 文章介绍了第一个专注于美因茨方言的自然语言处理（NLP）研究，提出了一种NLP准备好的数字词典，并使用该词典来评估现代大语言模型（LLMs）生成方言词汇定义和反向任务的能力。实验结果显示，现有的LLMs在对待这些任务时表现不佳，因此需要额外的资源和针对德语方言的更多研究。


<details>
  <summary>Details</summary>
Motivation: 文章旨在为美因茨方言的语言和方言复兴努力提供支持，薄克发现传统的美因茨语正在衰退，并寻求使用自然语言处理技术来帮助保存和复兴这种语言。

Method: 文章方法包括创建一个基于现有资源构建的NLP准备好的数字词典，并使用这个数据集评估现代大规模语言模型处理方言词汇的定义生成和反向任务的能力。

Result: 研究发现，最先进的大规模语言模型在生成定义或生成方言词汇方面的准确率都低于10%。此外，通过少样本学习和从训练集中提取规则等方法，能够提升一些准确性，但总体上仍然低于10%。

Conclusion: 研究结论表明，尽管现代大规模语言模型在此类任务中表现出色，但它们尚未准备好处理像美因茨方言这样的罕见语言。因此，需要增加对德语方言的研究资源和支持。

Abstract: Meenzerisch, the dialect spoken in the German city of Mainz, is also the traditional language of the Mainz carnival, a yearly celebration well known throughout Germany. However, Meenzerisch is on the verge of dying out-a fate it shares with many other German dialects. Natural language processing (NLP) has the potential to help with the preservation and revival efforts of languages and dialects. However, so far no NLP research has looked at Meenzerisch. This work presents the first research in the field of NLP that is explicitly focused on the dialect of Mainz. We introduce a digital dictionary-an NLP-ready dataset derived from an existing resource (Schramm, 1966)-to support researchers in modeling and benchmarking the language. It contains 2,351 words in the dialect paired with their meanings described in Standard German. We then use this dataset to answer the following research questions: (1) Can state-of-the-art large language models (LLMs) generate definitions for dialect words? (2) Can LLMs generate words in Meenzerisch, given their definitions? Our experiments show that LLMs can do neither: the best model for definitions reaches only 6.27% accuracy and the best word generation model's accuracy is 1.51%. We then conduct two additional experiments in order to see if accuracy is improved by few-shot learning and by extracting rules from the training set, which are then passed to the LLM. While those approaches are able to improve the results, accuracy remains below 10%. This highlights that additional resources and an intensification of research efforts focused on German dialects are desperately needed.

</details>


### [51] [A Conceptual Hybrid Framework for Post-Quantum Security: Integrating BB84 QKD, AES, and Bio-inspired Mechanisms](https://arxiv.org/abs/2602.16922)
*Md. Ismiel Hossen Abir*

Main category: cs.CL

TL;DR: 该研究探讨了RSA在经典和量子攻击下的漏洞，并设计了一个混合安全框架来确保后量子时代的数据保护。框架结合了AES加密、BB84量子密钥分发、量子状态比较和生物启发的免疫系统。


<details>
  <summary>Details</summary>
Motivation: 研究RSA在经典和量子计算攻击下的脆弱性，以应对量子计算机对经典加密算法如RSA的潜在威胁，同时设计一个综合的经典和量子安全框架，以增强数据保护。

Method: 通过分析RSA在经典和量子攻击下的弱点，研究并构建了一个混合安全框架，包括AES加密、量子密钥分发（BB84）、量子状态比较和生物启发的免疫系统。

Result: 建立了一个综合考虑经典和量子安全机制的安全框架，能够在后量子时代有效保护数据。该模型中，AES加密保障经典安全性，BB84用于安全密钥交换并检测窃听，量子状态比较实现轻量级认证，生物启发的免疫系统提供适应性强的威胁检测。

Conclusion: 研究提出了一种概念性的混合安全框架，有效结合了经典和量子安全方法，为后量子时代的数据加密提供保护方案。具体的实施细节、安全证明和实验验证将在未来的工作中进行详细探讨。

Abstract: Quantum computing is a significant risk to classical cryptographic, especially RSA, which depends on the difficulty of factoring large numbers. Classical factorization methods, such as Trial Division and Pollard's Rho, are inefficient for large keys, while Shor's quantum algorithm can break RSA efficiently in polynomial time. This research studies RSA's vulnerabilities under both classical and quantum attacks and designs a hybrid security framework to ensure data protection in the post-quantum era. The conceptual framework combines AES encryption for classical security, BB84 Quantum Key Distribution (QKD) for secure key exchange with eavesdropping detection, quantum state comparison for lightweight authentication, and a bio-inspired immune system for adaptive threat detection. RSA is vulnerable to Shor's algorithm, BB84 achieves full key agreement in ideal conditions, and it detects eavesdropping with high accuracy. The conceptual model includes both classical and quantum security methods, providing a scalable and adaptive solution for Post-Quantum encryption data protection. This work primarily proposes a conceptual framework. Detailed implementation, security proofs, and extensive experimental validation are considered future work.

</details>


### [52] [When Semantic Overlap Is Not Enough: Cross-Lingual Euphemism Transfer Between Turkish and English](https://arxiv.org/abs/2602.16957)
*Hasan Can Biyik,Libby Barak,Jing Peng,Anna Feldman*

Main category: cs.CL

TL;DR: 研究探讨了跨语言等效性如何影响多语言贬义替代词检测中的迁移性能。结果显示，在低资源的土耳其语到英语方向，即使是具有语义重叠的贬义词，其性能也可能下降甚至在非重叠贬义词训练下有所提升。标签分布差异解释了这些反直觉的结果。


<details>
  <summary>Details</summary>
Motivation: 研究揭示了社会敏感表达在不同语言间的复杂性及其对跨语言模型的挑战，并探讨了如何通过分类贬义词来优化多语言识别系统。

Method: 研究人员将土耳其语和英语中的潜在贬义术语（PETs）分为重叠和非重叠子集，并基于功能、语用和语义一致性进行分类。然后分析了分类后的贬义词在不同语言方向的性能差异。

Result: 研究发现，语义重叠不足以确保正面迁移。在低资源的语言对中，即使是重叠的贬义词，其识别性能也可能下降，甚至在非重叠贬义词训练下有所提升。不同的标签分布解释了这些反直觉的结果。

Conclusion: 研究强调，术语级别的分析可能受领域特定对齐影响，但在数据稀疏的情况下，这种影响证据有限。研究结果对于开发更有效的跨语言模型具有重要意义。

Abstract: Euphemisms substitute socially sensitive expressions, often softening or reframing meaning, and their reliance on cultural and pragmatic context complicates modeling across languages. In this study, we investigate how cross-lingual equivalence influences transfer in multilingual euphemism detection. We categorize Potentially Euphemistic Terms (PETs) in Turkish and English into Overlapping (OPETs) and Non-Overlapping (NOPETs) subsets based on their functional, pragmatic, and semantic alignment. Our findings reveal a transfer asymmetry: semantic overlap is insufficient to guarantee positive transfer, particularly in low-resource Turkish-to-English direction, where performance can degrade even for overlapping euphemisms, and in some cases, improve under NOPET-based training. Differences in label distribution help explain these counterintuitive results. Category-level analysis suggests that transfer may be influenced by domain-specific alignment, though evidence is limited by sparsity.

</details>


### [53] [Eigenmood Space: Uncertainty-Aware Spectral Graph Analysis of Psychological Patterns in Classical Persian Poetry](https://arxiv.org/abs/2602.16959)
*Kourosh Shahnazari,Seyed Moein Ayyoubzadeh,Mohammadali Keshtparvar*

Main category: cs.CL

TL;DR: 本文提出了一种基于大规模自动多标签注释的心理学分析框架，针对波斯古典诗歌进行诗人层级的心理学分析。


<details>
  <summary>Details</summary>
Motivation: 传统波斯诗歌通过隐喻、引文惯例和修辞间接表达情感生活，这种特性使得详细阅读成为不可或缺的手段，同时也限制了大规模比较分析的可复制性。

Method: 该方法基于大规模自动多标签注释，每一个诗句关联一系列的心理学概念、每个标签的信心分数以及止于证据不足的标记。

Result: 形成了诗人$	imes$概念矩阵，通过杰氏距离和科利耳距离量化诗人的个性，构建了信心加权共现图，并通过拉普拉斯谱分解获得Eigenmood嵌入。

Conclusion: 该成果支持了可扩展、可审计的数字人文分析，在保持解释性审慎的同时，从诗句级证据到诗人级推理传播不确定性。

Abstract: Classical Persian poetry is a historically sustained archive in which affective life is expressed through metaphor, intertextual convention, and rhetorical indirection. These properties make close reading indispensable while limiting reproducible comparison at scale. We present an uncertainty-aware computational framework for poet-level psychological analysis based on large-scale automatic multi-label annotation. Each verse is associated with a set of psychological concepts, per-label confidence scores, and an abstention flag that signals insufficient evidence. We aggregate confidence-weighted evidence into a Poet $\times$ Concept matrix, interpret each poet as a probability distribution over concepts, and quantify poetic individuality as divergence from a corpus baseline using Jensen--Shannon divergence and Kullback--Leibler divergence. To capture relational structure beyond marginals, we build a confidence-weighted co-occurrence graph over concepts and define an Eigenmood embedding through Laplacian spectral decomposition. On a corpus of 61{,}573 verses across 10 poets, 22.2\% of verses are abstained, underscoring the analytical importance of uncertainty. We further report sensitivity analysis under confidence thresholding, selection-bias diagnostics that treat abstention as a category, and a distant-to-close workflow that retrieves verse-level exemplars along Eigenmood axes. The resulting framework supports scalable, auditable digital-humanities analysis while preserving interpretive caution by propagating uncertainty from verse-level evidence to poet-level inference.

</details>


### [54] [Evaluating Cross-Lingual Classification Approaches Enabling Topic Discovery for Multilingual Social Media Data](https://arxiv.org/abs/2602.17051)
*Deepak Uniyal,Md Abul Bashar,Richi Nayak*

Main category: cs.CL

TL;DR: 本研究通过氢能源为例，分析了十年间多个国家语言的超过九百万条推文，探索了四种跨语言文本分类方法以过滤无关内容，并通过主题建模提取了主要主题，揭示了翻译与跨语言方法之间的权衡，为大规模社交媒体分析提供了行动建议。


<details>
  <summary>Details</summary>
Motivation: 由于多语言社交媒体讨论难以处理，并且大范围公众辩论跨越多种语言，因此需要开发可靠的跨语言文本分类方法来分析全球对话。

Method: 研究采用了氢能源作为案例，收集了2013年至2022年间超过九百万条带有关键词的动态数据（英语、日语、印地语、韩语），然后采取了四种不同的方法进行跨语言文本分类：（1）将英语标注数据翻译成目标语言并基于该语言构建特定模型；（2）将所有语言的未标注数据翻译成英语以创建基于英语标注的单一模型；（3）直接使用英语文本微调的多语言变压器；（4）结合翻译的标注数据和多语言训练的混合策略。

Result: 每种方法都针对过滤出与氢能源相关的推文进行了评估，之后进行了主题建模以提取每个子数据集内的主导主题。

Conclusion: 研究结果强调了翻译方法与多语言方法之间的权衡，为大规模跨语言社交媒体分析提供优化建议。

Abstract: Analysing multilingual social media discourse remains a major challenge in natural language processing, particularly when large-scale public debates span across diverse languages. This study investigates how different approaches for cross-lingual text classification can support reliable analysis of global conversations. Using hydrogen energy as a case study, we analyse a decade-long dataset of over nine million tweets in English, Japanese, Hindi, and Korean (2013--2022) for topic discovery. The online keyword-driven data collection results in a significant amount of irrelevant content. We explore four approaches to filter relevant content: (1) translating English annotated data into target languages for building language-specific models for each target language, (2) translating unlabelled data appearing from all languages into English for creating a single model based on English annotations, (3) applying English fine-tuned multilingual transformers directly to each target language data, and (4) a hybrid strategy that combines translated annotations with multilingual training. Each approach is evaluated for its ability to filter hydrogen-related tweets from noisy keyword-based collections. Subsequently, topic modeling is performed to extract dominant themes within the relevant subsets. The results highlight key trade-offs between translation and multilingual approaches, offering actionable insights into optimising cross-lingual pipelines for large-scale social media analysis.

</details>


### [55] [ALPS: A Diagnostic Challenge Set for Arabic Linguistic & Pragmatic Reasoning](https://arxiv.org/abs/2602.17054)
*Hussein S. Al-Olimat,Ahmad Alshareef*

Main category: cs.CL

TL;DR: ALPS 是一个针对阿拉伯语深层语义和语用能力的专业诊断挑战集，它评估了多种模型在阿拉伯语语言理解方面的深度。分析发现，尽管模型在流畅性上表现良好，但在形态句法依赖性上存在显著错误。


<details>
  <summary>Details</summary>
Motivation: 现有的阿拉伯语 NLP 基准多数侧重于规模，使用合成或翻译数据，缺乏深层次语言验证。因此，ALPS 提供了一个专为阿拉伯语深层语义和语用能力设计的新基准。

Method: ALPS 包含531个精心设计的问题，涵盖15大任务和47个子任务，数据集通过深入的阿拉伯语语言学知识保证文化原真性。该基准通过单一人工评估和专家仲裁的奥卡姆剃刀来衡量模型表现。

Result: 商业和开源模型的平均成绩分别为84.6%和略高，专家仲裁的奥卡姆剃刀则达到99.2%。虽然顶级商业模型在人均表现上超过了普通人类，但与阿拉伯本土模型仍存在显著差距。最佳的特定阿拉伯模型的性能接近但未达到人类水平。

Conclusion: ALPS 揭示了模型在形态句法依赖性上的薄弱之处，并强调了进一步研究阿拉伯语深层语言理解的必要性。

Abstract: While recent Arabic NLP benchmarks focus on scale, they often rely on synthetic or translated data which may benefit from deeper linguistic verification. We introduce ALPS (Arabic Linguistic & Pragmatic Suite), a native, expert-curated diagnostic challenge set probing Deep Semantics and Pragmatics, capabilities that complement specialized large-scale benchmarks. While broad-coverage benchmarks prioritize scale and multi-task coverage, ALPS targets the depth of linguistic understanding through 531 rigorously crafted questions across 15 tasks and 47 subtasks. We developed the dataset with deep expertise in Arabic linguistics, guaranteeing cultural authenticity and eliminating translation artifacts. Evaluating 23 diverse models (commercial, open-source, and Arabic-native) against a single-pass human performance (avg. 84.6% accuracy) and an expert-adjudicated oracle (99.2%), we reveal a critical dissociation: models achieve high fluency but fail on fundamental morpho-syntactic dependencies, with elevated error rates on morpho-syntactic dependencies (36.5% across diacritics-reliant tasks) compared to compositional semantics. While top commercial models (Gemini-3-flash at 94.2%) surpass the average single human, a substantial gap persists between commercial giants and Arabic-native models, with the best Arabic-specific model (Jais-2-70B at 83.6%) approaching but not matching human performance.

</details>


### [56] [BankMathBench: A Benchmark for Numerical Reasoning in Banking Scenarios](https://arxiv.org/abs/2602.17072)
*Yunseung Lee,Subin Kim,Youngjun Kwak,Jaegul Choo*

Main category: cs.CL

TL;DR: 该研究提出了BankMathBench数据集，专为大型语言模型（LLM）在银行计算任务中的表现提供评估。通过训练和工具增强微调，模型在不同难度级别的任务上取得了显著改进，证明了该数据集的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有金融基准数据集忽视了日常银行场景的计算任务，新数据集BankMathBench旨在弥合这一空白，提供更贴近实际情况的数据，以便更好地评估LLMs的金融计算能力。

Method: 研究者首先定义了三个难度级别（基础、中级和高级）的任务类型，并据此构建了BankMathBench数据集。然后使用开源LLMs进行训练，并通过工具辅助微调来提升模型的表现。

Result: 通过BankMathBench进行训练后，开源LLMs在各种任务类型上的准确率显著提高，特别是在较高级别的任务中实现超过50%的平均准确率提升。

Conclusion: BankMathBench数据集为LLMs在金融计算任务中的评估提供了坚实基础，有助于推动该领域的研究和发展。

Abstract: Large language models (LLMs)-based chatbots are increasingly being adopted in the financial domain, particularly in digital banking, to handle customer inquiries about products such as deposits, savings, and loans. However, these models still exhibit low accuracy in core banking computations-including total payout estimation, comparison of products with varying interest rates, and interest calculation under early repayment conditions. Such tasks require multi-step numerical reasoning and contextual understanding of banking products, yet existing LLMs often make systematic errors-misinterpreting product types, applying conditions incorrectly, or failing basic calculations involving exponents and geometric progressions. However, such errors have rarely been captured by existing benchmarks. Mathematical datasets focus on fundamental math problems, whereas financial benchmarks primarily target financial documents, leaving everyday banking scenarios underexplored. To address this limitation, we propose BankMathBench, a domain-specific dataset that reflects realistic banking tasks. BankMathBench is organized in three levels of difficulty-basic, intermediate, and advanced-corresponding to single-product reasoning, multi-product comparison, and multi-condition scenarios, respectively. When trained on BankMathBench, open-source LLMs exhibited notable improvements in both formula generation and numerical reasoning accuracy, demonstrating the dataset's effectiveness in enhancing domain-specific reasoning. With tool-augmented fine-tuning, the models achieved average accuracy increases of 57.6%p (basic), 75.1%p (intermediate), and 62.9%p (advanced), representing significant gains over zero-shot baselines. These findings highlight BankMathBench as a reliable benchmark for evaluating and advancing LLMs' numerical reasoning in real-world banking scenarios.

</details>


### [57] [The Emergence of Lab-Driven Alignment Signatures: A Psychometric Framework for Auditing Latent Bias and Compounding Risk in Generative AI](https://arxiv.org/abs/2602.17127)
*Dusan Bosnjakovic*

Main category: cs.CL

TL;DR: 该研究提出了一种新型审核框架，利用心理测量理论量化大型语言模型中的稳定响应倾向，而不依赖于_ground-truth_标签。通过分析九个领先模型在偏向性、拍马溜须和现状合法化等维度上的表现，发现内在偏见不仅是静态错误，而是可能在多层AI架构中创造递归的思想回声室。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型从独立聊天界面发展成为多智能体系统和递归评估环中的核心推理层（LLM-as-a-judge），检测持久的提供者级行为模式成为确保安全和治理的关键需求。传统的基准测试仅测量短暂任务准确性，无法捕捉稳定且潜在的响应策略。

Method: 研究利用心理测量理论中的潜在特质估计，在或然性条件下进行量表评估，采用强制选择的或然性情景及语义上正交的替代选项，并通过加密置换不变性原则进行管理。此外，采用了混合线性模型（MixedLM）和内类别相关系数（ICC）分析来识别模型之间的一致性。

Result: 研究发现，在高变异性的项目级别表征中，一种持久的“实验室信号”解释了显著的行为聚类。这表明，在固定的提供者生态系统中，潜在的偏见不仅是静态错误，而是可能在多层AI架构中产生递归的思想回声室。

Conclusion: 该研究提出的方法能有效识别大型语言模型中隐藏的持久行为模式，并指出应该重视模型中的潜在偏见，避免其在多层次AI系统中累积造成负面影响。

Abstract: As Large Language Models (LLMs) transition from standalone chat interfaces to foundational reasoning layers in multi-agent systems and recursive evaluation loops (LLM-as-a-judge), the detection of durable, provider-level behavioral signatures becomes a critical requirement for safety and governance. Traditional benchmarks measure transient task accuracy but fail to capture stable, latent response policies -- the ``prevailing mindsets'' embedded during training and alignment that outlive individual model versions.
  This paper introduces a novel auditing framework that utilizes psychometric measurement theory -- specifically latent trait estimation under ordinal uncertainty -- to quantify these tendencies without relying on ground-truth labels. Utilizing forced-choice ordinal vignettes masked by semantically orthogonal decoys and governed by cryptographic permutation-invariance, the research audits nine leading models across dimensions including Optimization Bias, Sycophancy, and Status-Quo Legitimization.
  Using Mixed Linear Models (MixedLM) and Intraclass Correlation Coefficient (ICC) analysis, the research identifies that while item-level framing drives high variance, a persistent ``lab signal'' accounts for significant behavioral clustering. These findings demonstrate that in ``locked-in'' provider ecosystems, latent biases are not merely static errors but compounding variables that risk creating recursive ideological echo chambers in multi-layered AI architectures.

</details>


### [58] [What Makes a Good Doctor Response? An Analysis on a Romanian Telemedicine Platform](https://arxiv.org/abs/2602.17194)
*Adrian Cosma,Cosmin Dumitrache,Emilian Radoi*

Main category: cs.CL

TL;DR: 本研究分析了罗马尼亚语基于文本的远程医疗服务中的患者满意度信号，通过分类模型识别出患者和医生历史特征作为主要预测因素，而文本内容特征虽影响较小，但具有实际操作价值。


<details>
  <summary>Details</summary>
Motivation: 随着在线医疗平台的兴起，文本交流是患者获取医疗建议的主要方式。改善沟通质量和维护患者的高满意度对医生至关重要，而患者评价通常难以充分反映实际医疗效果。

Method: 使用77,334条匿名患者-医生对话，模型将反馈简化为正面或负面两类，通过时间划分训练分类器，并使用SHAP分析特征的重要性。

Result: 研究发现，患者和医生历史特征对预测反馈有较大影响，作为强有力的先验信息；文本内容特征虽贡献较小，但也提供了关键的可行动信息。进一步的子组分析显示，礼貌和婉拒表达与患者反馈正相关，而词汇多样性与负面反馈相关。

Conclusion: 该研究有助于改善远程医疗的沟通质量，医生可以通过维持礼貌和注意语言多样性来提升患者满意度。

Abstract: Text-based telemedicine has become a common mode of care, requiring clinicians to deliver medical advice clearly and effectively in writing. As platforms increasingly rely on patient ratings and feedback, clinicians face growing pressure to maintain satisfaction scores, even though these evaluations often reflect communication quality more than clinical accuracy. We analyse patient satisfaction signals in Romanian text-based telemedicine. Using a sample of 77,334 anonymised patient question--doctor response pairs, we model feedback as a binary outcome, treating thumbs-up responses as positive and grouping negative or absent feedback into the other class. We extract interpretable, predominantly language-agnostic features (e.g., length, structural characteristics, readability proxies), along with Romanian LIWC psycholinguistic features and politeness/hedging markers where available. We train a classifier with a time-based split and perform SHAP-based analyses, which indicate that patient and clinician history features dominate prediction, functioning as strong priors, while characteristics of the response text provide a smaller but, crucially, actionable signal. In subgroup correlation analyses, politeness and hedging are consistently positively associated with patient feedback, whereas lexical diversity shows a negative association.

</details>


### [59] [Quantifying and Mitigating Socially Desirable Responding in LLMs: A Desirability-Matched Graded Forced-Choice Psychometric Study](https://arxiv.org/abs/2602.17262)
*Kensuke Okada,Yui Furukawa,Kyosuke Bunji*

Main category: cs.CL

TL;DR: 该研究提出了一种量化和减轻语言模型问卷调查中的社会期望回答偏差的方法，通过改变指示方式并使用调整后的渐进强迫选择问卷，减少偏差同时保留人格特征的恢复。


<details>
  <summary>Details</summary>
Motivation: 人类自我报告问卷在自然语言处理中广泛用于评估大语言模型，但这些问卷假设诚实回答，而在评估环境中，LSTM可能会倾向于给出社会上更受欢迎的答案，从而影响问卷得分和后续结论。

Method: 研究通过两种方式对社会期望回答偏差进行量化和减轻。首先，让同一次调查在诚实回答（HONEST）和虚构好答案（FAKE-GOOD）指示下进行，通过项目反应理论估计的潜在分数来计算方向修正的标准效应大小。其次，构建了一个排序强迫选择的大五人格问卷，选择了具有匹配期望性的30个跨领域项目。

Result: 在九种指令调整后的LSTM上使用合成的人格角色进行评估时，喜欢程度匹配的排序强迫选择问卷相较于传统李克特量表问卷，大幅减少了社会期望回答偏差，同时基本上保留下了目标人格特征的恢复。

Conclusion: 此研究表明，社会期望回答偏差在模型上具有依赖性，表明需要注意到社会期望回答偏差并在基于问卷的基准测试和审计中采取意识化的报告实践。

Abstract: Human self-report questionnaires are increasingly used in NLP to benchmark and audit large language models (LLMs), from persona consistency to safety and bias assessments. Yet these instruments presume honest responding; in evaluative contexts, LLMs can instead gravitate toward socially preferred answers-a form of socially desirable responding (SDR)-biasing questionnaire-derived scores and downstream conclusions. We propose a psychometric framework to quantify and mitigate SDR in questionnaire-based evaluation of LLMs. To quantify SDR, the same inventory is administered under HONEST versus FAKE-GOOD instructions, and SDR is computed as a direction-corrected standardized effect size from item response theory (IRT)-estimated latent scores. This enables comparisons across constructs and response formats, as well as against human instructed-faking benchmarks. For mitigation, we construct a graded forced-choice (GFC) Big Five inventory by selecting 30 cross-domain pairs from an item pool via constrained optimization to match desirability. Across nine instruction-tuned LLMs evaluated on synthetic personas with known target profiles, Likert-style questionnaires show consistently large SDR, whereas desirability-matched GFC substantially attenuates SDR while largely preserving the recovery of the intended persona profiles. These results highlight a model-dependent SDR-recovery trade-off and motivate SDR-aware reporting practices for questionnaire-based benchmarking and auditing of LLMs.

</details>


### [60] [Towards Cross-lingual Values Assessment: A Consensus-Pluralism Perspective](https://arxiv.org/abs/2602.17283)
*Yukun Chen,Xinyu Zhang,Jialong Tang,Yu Wan,Baosong Yang,Yiming Li,Zhan Qin,Kui Ren*

Main category: cs.CL

TL;DR: 本文提出了一个名为X-Value的新跨语言价值观评估基准，旨在评估大语言模型在跨国层面上评估内容深层价值观的能力。该基准包括超过5,000个问答对，分布在18种语言的7个核心领域，并针对不同的评估级别提出了独特的两阶段标注框架。评估结果显示，当下的SOTA大语言模型在跨语言价值观评估方面存在明显的不足。


<details>
  <summary>Details</summary>
Motivation: 鉴于当前大型语言模型在内容安全方面的重要性，现有的评估方法主要关注于检测显性危害，却忽略了在数字内容中传达的更深层次的价值维度。本文通过引入X-Value基准，旨在填补这一空白。

Method: X-Value由超过5,000个多语言问答对组成，分类为不同难度，依据的是基本人类价值观的理论，通过一个独特的两阶段标注框架进行评估：首先确定问题是属于全球共识还是多元文化，然后对内容中隐含的价值观进行多方面的评价。

Result: 这项研究表明，当前最先进的大语言模型在跨语言价值观评估方面表现不足，不同语言之间的性能差异显著。

Conclusion: 这一研究强调了改进大语言模型的细微、价值观敏感的内容评估能力的紧迫需求，并提供了一个可用于进一步研究的X-Value基准数据集。

Abstract: While large language models (LLMs) have become pivotal to content safety, current evaluation paradigms primarily focus on detecting explicit harms (e.g., violence or hate speech), neglecting the subtler value dimensions conveyed in digital content. To bridge this gap, we introduce X-Value, a novel Cross-lingual Values Assessment Benchmark designed to evaluate LLMs' ability to assess deep-level values of content from a global perspective. X-Value consists of more than 5,000 QA pairs across 18 languages, systematically organized into 7 core domains grounded in Schwartz's Theory of Basic Human Values and categorized into easy and hard levels for discriminative evaluation. We further propose a unique two-stage annotation framework that first identifies whether an issue falls under global consensus (e.g., human rights) or pluralism (e.g., religion), and subsequently conducts a multi-party evaluation of the latent values embedded within the content. Systematic evaluations on X-Value reveal that current SOTA LLMs exhibit deficiencies in cross-lingual values assessment ($Acc < 77\%$), with significant performance disparities across different languages ($ΔAcc > 20\%$). This work highlights the urgent need to improve the nuanced, values-aware content assessment capability of LLMs. Our X-Value is available at: https://huggingface.co/datasets/Whitolf/X-Value.

</details>


### [61] [Representation Collapse in Machine Translation Through the Lens of Angular Dispersion](https://arxiv.org/abs/2602.17287)
*Evgeniia Tokarchuk,Maya K. Nachesa,Sergey Troshin,Vlad Niculae*

Main category: cs.CL

TL;DR: 该研究分析了基于Transformer架构的现代神经翻译模型在不同层次上代表崩塌的动力学，提出了基于角度分散的正则化方法来缓解代表崩塌，并提高了翻译质量。


<details>
  <summary>Details</summary>
Motivation: 尽管基于Transformer的现代神经翻译模型在高资源数据集上的训练表现优异，但传统的下一个标记预测训练策略可能导致代表崩塌等问题，特别是在深入的Transformer层中更为明显。为了解决这个问题，研究引入了一种基于角度分散的正则化方法。

Method: 研究围绕Transformer的离散和连续模型进行了分析，利用现有的基于角度分散的正则化方法对不同层次上的代表崩塌进行实验验证。

Result: 实验表明，基于角度分散的正则化方法不仅可以缓解代表崩塌，还能提高翻译质量。此外，量化模型也表现出类似的代表崩塌行为，正则化方法的优势在量化后仍然保留。

Conclusion: 研究通过引入基于角度分散的正则化方法，有效地缓解了基于Transformer的神经机器翻译模型中的代表崩塌问题，并通过实验证明了该方法的有效性和鲁棒性。

Abstract: Modern neural translation models based on the Transformer architecture are known for their high performance, particularly when trained on high-resource datasets. A standard next-token prediction training strategy, while widely adopted in practice, may lead to overlooked artifacts such as representation collapse. Previous works have shown that this problem is especially pronounced in the representation of the deeper Transformer layers, where it often fails to efficiently utilize the geometric space. Representation collapse is even more evident in end-to-end training of continuous-output neural machine translation, where the trivial solution would be to set all vectors to the same value. In this work, we analyze the dynamics of representation collapse at different levels of discrete and continuous NMT transformers throughout training. We incorporate an existing regularization method based on angular dispersion and demonstrate empirically that it not only mitigates collapse but also improves translation quality. Furthermore, we show that quantized models exhibit similar collapse behavior and that the benefits of regularization are preserved even after quantization.

</details>


### [62] [Same Meaning, Different Scores: Lexical and Syntactic Sensitivity in LLM Evaluation](https://arxiv.org/abs/2602.17316)
*Bogdan Kostić,Conor Fallon,Julian Risch,Alexander Löser*

Main category: cs.CL

TL;DR: 本研究对23种当代大语言模型在三个基准测试中的表现进行了受控的语义、句法变化测试，发现词法变化导致了显著的表现下降，而句法变化的影响则更加复杂，有时能改善结果。研究强调，在LLM评估中需要进行稳健性测试。


<details>
  <summary>Details</summary>
Motivation: 鉴于当前大语言模型在浅层输入差异下的不稳定性，研究旨在评估不同大语言模型在特定基准测试中的表现如何受到受控的语义和句法变化的影响，以验证这些模型的实际可靠性和依赖性。

Method: 使用两种基于语言原则的管道生成保持意义的变体：一种进行同义词替换以实现词法变化，另一种使用依存句法学来确定适用的句法变换。

Result: 词法变化导致了大部分模型和任务中显著的表现下降，而句法变化的效果则更加多样，有时能提高结果。同时，这些变化导致了复杂任务中模型排名的不稳定。

Conclusion: 研究表明，大语言模型更依赖于表面的词法模式，而不是抽象的语言能力，这强调了在大语言模型评估中必须包括稳健性测试作为标准组成部分。

Abstract: The rapid advancement of Large Language Models (LLMs) has established standardized evaluation benchmarks as the primary instrument for model comparison. Yet, their reliability is increasingly questioned due to sensitivity to shallow variations in input prompts. This paper examines how controlled, truth-conditionally equivalent lexical and syntactic perturbations affect the absolute performance and relative ranking of 23 contemporary LLMs across three benchmarks: MMLU, SQuAD, and AMEGA. We employ two linguistically principled pipelines to generate meaning-preserving variations: one performing synonym substitution for lexical changes, and another using dependency parsing to determine applicable syntactic transformations. Results show that lexical perturbations consistently induce substantial, statistically significant performance degradation across nearly all models and tasks, while syntactic perturbations have more heterogeneous effects, occasionally improving results. Both perturbation types destabilize model leaderboards on complex tasks. Furthermore, model robustness did not consistently scale with model size, revealing strong task dependence. Overall, the findings suggest that LLMs rely more on surface-level lexical patterns than on abstract linguistic competence, underscoring the need for robustness testing as a standard component of LLM evaluation.

</details>


### [63] [RPDR: A Round-trip Prediction-Based Data Augmentation Framework for Long-Tail Question Answering](https://arxiv.org/abs/2602.17366)
*Yiming Zhang,Siyue Zhang,Junbo Zhao,Chen Zhao*

Main category: cs.CL

TL;DR: RPDR 是一种新型的数据增强框架，用于增强密集检索器，通过合成数据生成、Round-Trip预测数据选择以及具有这些实例的检索器训练，展示了在长尾恢复基准 PopQA 和 EntityQuestion 上相较于 BM25 和 Contriver 的显著改进。提出了动态路由机制以进一步优化检索表现。


<details>
  <summary>Details</summary>
Motivation: 鉴于大语言模型在处理稀有或专业领域知识时受限于其较小的知识获取与准确回忆能力，现有密集检索模型同样面临挑战。RPDR 框架旨在通过高品质易学训练数据的选取，提高密集检索模型的效能。

Method: RPDR 框架包含三个核心组件：合成数据生成、使用 Round-Trip 预测进行数据选择及高效的检索器训练。具体操作包括：通过特定方法生成更具代表性的候选样本；利用 Round-Trip 预测模型从候选样本中识别出易于学习的实例作为有效样本；并使用这些有效样本重新训练检索器。

Result: RPDR 在 PopQA 和 EntityQuestion 两个长尾恢复基准测试中表现出显著优于 BM25 和 Contriver 的效果，特别是在极长尾类别的表现优异。此外，进一步通过动态路由机制改进查询路由。

Conclusion: RPDR 提供了一种有效的解决方案，能够显著提高密集检索模型在处理稀有或专业领域查询时的表现。

Abstract: Long-tail question answering presents significant challenges for large language models (LLMs) due to their limited ability to acquire and accurately recall less common knowledge. Retrieval-augmented generation (RAG) systems have shown great promise in mitigating this limitation by integrating external retrieval mechanisms. However, dense retrieval models often face the same difficulties when generalizing to rare or niche knowledge. In this study, we introduce RPDR, a novel data augmentation framework that selects high-quality easy-to-learn training data, to enhance dense retrievers. Our approach is built around three core components: synthetic data generation, data selection with Round-Trip prediction to identify easy-to-learn instances, and retriever training with these instances. We evaluate RPDR on two long-tail retrieval benchmarks, PopQA and EntityQuestion, demonstrating substantial improvements over existing retrievers like BM25 and Contriver, especially on extremely long-tail categories. We identify the strengths and limitations of RPDR through detailed human analysis and propose a dynamic routing mechanism to dynamically route queries to specialized retrieval modules to further improve retrieval performance.

</details>


### [64] [The Role of the Availability Heuristic in Multiple-Choice Answering Behaviour](https://arxiv.org/abs/2602.17377)
*Leonidas Zotos,Hedderik van Rijn,Malvina Nissim*

Main category: cs.CL

TL;DR: 本文分析了多项选择题（MCQ）中答案的认知可得性，并发现正确答案比错误答案更易被想到。使用认知可得性的策略比随机猜测提高了13.5%至32.9%的得分，即使生成MCQ的大型语言模型（LLM）也显示出类似的趋势。


<details>
  <summary>Details</summary>
Motivation: 探讨学生在面对不确定正确答案的MCQ时，使用可得性启发式策略是否是有效的方法，以及在此基础上开发出一种计算方法来评估MCQ选项的认知可得性。

Method: 研究采用了三种大型题集，利用维基百科作为检索语料库来计算选项的可得性，并比较了随机猜测和始终选择最可得的选项得分的差异。

Result: 研究表明，始终选择最可得的选项可将得分提高13.5%至32.9%，高于随机猜测的基线。此外，虽然大型语言模型生成的MCQ选项显示了与专家创作的选项相似的可得性模式，但在计算模型中仍应考虑可得性。

Conclusion: 研究结论认为，在计算模型中考虑答案的认知可得性会改善对学生行为建模的准确性。

Abstract: When students are unsure of the correct answer to a multiple-choice question (MCQ), guessing is common practice. The availability heuristic, proposed by A. Tversky and D. Kahneman in 1973, suggests that the ease with which relevant instances come to mind, typically operationalised by the mere frequency of exposure, can offer a mental shortcut for problems in which the test-taker does not know the exact answer. Is simply choosing the option that comes most readily to mind a good strategy for answering MCQs? We propose a computational method of assessing the cognitive availability of MCQ options operationalised by concepts' prevalence in large corpora. The key finding, across three large question sets, is that correct answers, independently of the question stem, are significantly more available than incorrect MCQ options. Specifically, using Wikipedia as the retrieval corpus, we find that always selecting the most available option leads to scores 13.5% to 32.9% above the random-guess baseline. We further find that LLM-generated MCQ options show similar patterns of availability compared to expert-created options, despite the LLMs' frequentist nature and their training on large collections of textual data. Our findings suggest that availability should be considered in current and future work when computationally modelling student behaviour.

</details>


### [65] [Diverse Word Choices, Same Reference: Annotating Lexically-Rich Cross-Document Coreference](https://arxiv.org/abs/2602.17424)
*Anastasia Zhukova,Felix Hamborg,Karsten Donnay,Norman Meuschke,Bela Gipp*

Main category: cs.CL

TL;DR: 本文修订了新闻WCL50数据集的CDCR注释方案，将同现链视为话语元素(DEs)，并作为分析的概念单位。这一方法能够处理身份和近似身份关系，从而支持新闻领域中平衡和话语意识到的CDCR研究。


<details>
  <summary>Details</summary>
Motivation: 现有数据集基于事件解决的核心参考主要集中在事件上，但缺乏话语意识。作者通过提出修订的CDCR注释方案，推动了针对广泛多变的新闻内容的研究分析。

Method: 作者修订了新闻WCL50数据集，并使用统一代码本对新闻WCL50数据集和ECB+的子集进行重新标注，然后使用词汇多样性度量和一个相同的头部词基线进行评估。

Result: 重新标注的数据集的指标与原始ECB+和新闻WCL50相当，表明修订的方案有效支持平衡和话语意识的CDCR研究。

Conclusion: 修订后的CDCR注释方案能够更好地支持针对广泛多变的新闻内容的研究分析，并有助于实现话语意识的CDCR研究。

Abstract: Cross-document coreference resolution (CDCR) identifies and links mentions of the same entities and events across related documents, enabling content analysis that aggregates information at the level of discourse participants. However, existing datasets primarily focus on event resolution and employ a narrow definition of coreference, which limits their effectiveness in analyzing diverse and polarized news coverage where wording varies widely. This paper proposes a revised CDCR annotation scheme of the NewsWCL50 dataset, treating coreference chains as discourse elements (DEs) and conceptual units of analysis. The approach accommodates both identity and near-identity relations, e.g., by linking "the caravan" - "asylum seekers" - "those contemplating illegal entry", allowing models to capture lexical diversity and framing variation in media discourse, while maintaining the fine-grained annotation of DEs. We reannotate the NewsWCL50 and a subset of ECB+ using a unified codebook and evaluate the new datasets through lexical diversity metrics and a same-head-lemma baseline. The results show that the reannotated datasets align closely, falling between the original ECB+ and NewsWCL50, thereby supporting balanced and discourse-aware CDCR research in the news domain.

</details>


### [66] [Evaluating Extremely Low-Resource Machine Translation: A Comparative Study of ChrF++ and BLEU Metrics](https://arxiv.org/abs/2602.17425)
*Sanjeev Kumar,Preethi Jyothi,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: 研究对比了BLEU和ChrF++在极低资源语境下机器翻译质量评估中的表现，发现虽然ChrF++较为常用，但BLEU能提供更有助于理解的词汇精确性信息。


<details>
  <summary>Details</summary>
Motivation: 在极低资源语境下，广泛使用的BLEU等指标可能无法准确反映翻译质量，因此需要寻找更好的评估方法。

Method: 研究比较了BLEU和ChrF++这两种评估指标在Magahi、Bhojpuri和Chhattisgarhi三种低资源语言下的表现，特别关注大型语言模型（LLMs）和神经机器翻译（NMT）系统的输出。

Result: 研究发现，尽管ChrF++在某些情况下表现较好，BLEU因其提供词汇精确性的额外信息，实际上在解释翻译质量方面更具优势。

Conclusion: 因此，研究建议不应完全依赖单一的评估指标，而应综合考虑多种指标来更全面地评估机器翻译在极低资源语境下的表现。

Abstract: Evaluating machine translation (MT) quality in extremely low-resource language (ELRL) scenarios poses unique challenges, as widely used metrics such as BLEU, effective in high-resource settings, often misrepresent quality in data-scarce contexts. This work presents a comparative analysis of BLEU, an n-gram-based metric, and ChrF++, a character-based metric, for MT evaluation in ELRL settings. We examine how each metric responds to translation artifacts, including hallucinations, repetition, source-text copying, and diacritic (\textit{matra}) variations across three ELRLs: Magahi, Bhojpuri, and Chhattisgarhi, with a focus on outputs from large language models (LLMs) and neural MT (NMT) systems. While recent work often relies solely on ChrF++, our findings show that BLEU, despite its lower absolute scores, provides complementary lexical-precision insights that improve interpretability.

</details>


### [67] [Fine-Grained Uncertainty Quantification for Long-Form Language Model Outputs: A Comparative Study](https://arxiv.org/abs/2602.17431)
*Dylan Bouchard,Mohit Singh Chauhan,Viren Bajaj,David Skarbrevik*

Main category: cs.CL

TL;DR: 本文提出了一种针对长文本生成的细粒度不确定性量化分类法，通过响应分解、单元级打分和响应级聚合三个阶段区分方法。实验表明，断言-响应蕴含关系在不确定性量化 scoring 中表现良好，响应级打分通常优于句子级打分，同时不确定性感知解码对提升长文本的真实性非常有效。


<details>
  <summary>Details</summary>
Motivation: 现有的不确定性量化方法多针对短文本生成，不适用于长文本生成。本文旨在填补这一空白，提出一种针对长文本生成的细粒度不确定性量化分类法。

Method: 本文提出了一种细粒度的不确定性量化分类法，通过响应分解、单元级打分、响应级聚合三个阶段来区分方法，并定义了几类基于一致性的黑盒打分器。

Result: 实验结果显示，断言-响应蕴含关系在不确定性量化 scoring 中表现良好，响应级打分通常优于句子级打分，同时不确定性感知解码对提升长文本的真实性非常有效。

Conclusion: 本文框架澄清了之前方法之间的关系，提供了实现细粒度不确定性量化的指导，并为未来的相关研究提供了基础。

Abstract: Uncertainty quantification has emerged as an effective approach to closed-book hallucination detection for LLMs, but existing methods are largely designed for short-form outputs and do not generalize well to long-form generation. We introduce a taxonomy for fine-grained uncertainty quantification in long-form LLM outputs that distinguishes methods by design choices at three stages: response decomposition, unit-level scoring, and response-level aggregation. We formalize several families of consistency-based black-box scorers, providing generalizations and extensions of existing methods. In our experiments across multiple LLMs and datasets, we find 1) claim-response entailment consistently performs better or on par with more complex claim-level scorers, 2) claim-level scoring generally yields better results than sentence-level scoring, and 3) uncertainty-aware decoding is highly effective for improving the factuality of long-form outputs. Our framework clarifies relationships between prior methods, enables apples-to-apples comparisons, and provides practical guidance for selecting components for fine-grained UQ.

</details>


### [68] [AIDG: Evaluating Asymmetry Between Information Extraction and Containment in Multi-Turn Dialogue](https://arxiv.org/abs/2602.17443)
*Adib Sakhawat,Fardeen Sadab,Rakin Shahriar*

Main category: cs.CL

TL;DR: 本研究表明，大型语言模型在信息束缚和信息提取方面存在能力不对称，模型在保持状态方面表现更好，但在策略推理中表现较差。研究通过AIDG框架提出了两个任务，并揭示了信息动态和指令遵循两个瓶颈。


<details>
  <summary>Details</summary>
Motivation: 传统静态测试框架无法全面评估大型语言模型的策略性推理能力，研究引入了AIDG框架来评估模型在信息提取和保持状态方面的差异。

Method: 研究通过设计AIDG-I和社会推理，以及AIDG-II和结构化“20问题”任务，让六种前沿的大型语言模型参与439场游戏，观察它们在信息提取和保持状态方面的表现。

Result: 研究发现，模型在保持状态方面表现出色，但在信息提取方面表现较弱，尤其是在确认策略和指令遵循等方面。模型在防御方向上的表现比攻击方向上高出350 ELO分，且在信息动态和指令遵循方面存在显著瓶颈。

Conclusion: 研究指出，尽管大型语言模型在局部防御一致性方面表现出色，但在策略性询问所需的全局状态跟踪方面存在不足。

Abstract: Evaluating the strategic reasoning capabilities of Large Language Models (LLMs) requires moving beyond static benchmarks to dynamic, multi-turn interactions. We introduce AIDG (Adversarial Information Deduction Game), a game-theoretic framework that probes the asymmetry between information extraction (active deduction) and information containment (state maintenance) in dialogue. We propose two complementary tasks: AIDG-I, measuring pragmatic strategy in social deduction, and AIDG-II, measuring constraint satisfaction in a structured "20 Questions" setting. Across 439 games with six frontier LLMs, we observe a clear capability asymmetry: models perform substantially better at containment than deduction, with a 350 ELO advantage on defense;(Cohen's d = 5.47). We identify two bottlenecks driving this gap: (1) Information Dynamics, where confirmation strategies are 7.75x more effective than blind deduction (p < 0.00001), and (2) Constraint Adherence, where instruction-following degrades under conversational load, accounting for 41.3% of deductive failures. These findings suggest that while LLMs excel at local defensive coherence, they struggle with the global state tracking required for strategic inquiry.

</details>


### [69] [ABCD: All Biases Come Disguised](https://arxiv.org/abs/2602.17445)
*Mateusz Nowak,Xavier Cadet,Peter Chin*

Main category: cs.CL

TL;DR: 通过一个合成的NonsenseQA基准测试，我们发现在不同大型语言模型中存在标签位置和少量提示偏见，并提出了一种简单的减少偏见的评估协议，以增强模型在回答排列变化时的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 为了评估大型语言模型在知识推理和答案生成方面的真实能力，文章指出现有MCQ基准可能存在偏见问题，提出了一种新的评估方法以减少模型依赖于问题标签和提示信息。

Method: 作者使用一个合成的NonsenseQA基准测试，观察到了不同模型的偏见情况。为解决这一问题，他们设计了一个新的评估协议，即使用均匀、无序的标签替换原始问题标签，并要求模型使用完整答案进行预测。

Result: 结果显示，在多种基准测试和模型上，新的评估协议显著提高了对答案排列变化的稳健性，使正确率方差减少了3倍，并且模型性能的小幅度下降。

Conclusion: 综上，新提出的评估协议有效地减少了现有方法中的评估偏见问题，能够更好地揭示模型的真实能力，进而改善模型的鲁棒性和评估精度。

Abstract: Multiple-choice question (MCQ) benchmarks have been a standard evaluation practice for measuring LLMs' ability to reason and answer knowledge-based questions. Through a synthetic NonsenseQA benchmark, we observe that different LLMs exhibit varying degrees of label-position-few-shot-prompt bias, where the model either uses the answer position, the label in front of the answer, the distributions of correct answers present in the few-shot prompt, or a combination of all to answer each MCQ question. We propose a simple bias-reduced evaluation protocol that replaces the labels of each question with uniform, unordered labels and prompts the LLM to use the whole answer presented. With a simple sentence similarity model, we demonstrate improved robustness and lower standard deviation between different permutations of answers with a minimal drop in LLM's performance, exposing the LLM's capabilities under reduced evaluation artifacts, without any help from the prompt examples or the option labels. Across multiple benchmarks and models, this protocol substantially improves the robustness to answer permutations, reducing mean accuracy variance $3\times$ with only a minimal decrease in the mean model's performance. Through ablation studies on various embedding models and similarity functions, we show that the method is more robust than the standard ones.

</details>


### [70] [Entropy-Based Data Selection for Language Models](https://arxiv.org/abs/2602.17465)
*Hongming Li,Yang Liu,Chao Huang*

Main category: cs.CL

TL;DR: 该研究提出了基于熵的无监督数据筛选（EUDS）框架，以缓解语言模型（LMs）在资源受限条件下训练时面临的计算资源和数据资源挑战。EUDS框架通过有效减少所需数据量并提高计算效率，在情感分析、主题分类和问答任务中被验证为有效的解决方案。


<details>
  <summary>Details</summary>
Motivation: 随着现代语言模型（LMs）对计算资源和数据资源的需求增加，有效的数据筛选技术变得至关重要。然而，这些技术的效果往往依赖于高昂的计算预算。为解决这一问题，本研究旨在探讨数据筛选与所选数据不确定性估计之间的关系，以找到一种在资源受限场景中高效筛选数据的方法。

Method: 研究采用了一种基于熵的无监督数据筛选（EUDS）框架。通过对大规模语言模型（LLMs）的理解和生成能力进行分析，结合情感分析、主题分类和问答任务的实际实验，验证了EUDS框架的有效性。

Result: 实验证明，EUDS框架能够提供一种计算资源使用效率更高的数据过滤机制，显著减少了训练数据的需求并节省了计算成本，提高了训练效率。

Conclusion: 研究得出结论，EUDS框架提供了一种有效的方法，可以在资源受限的情况下更高效地对语言模型进行微调，为了解决LMs面临的资源挑战提供了创新方案。

Abstract: Modern language models (LMs) increasingly require two critical resources: computational resources and data resources. Data selection techniques can effectively reduce the amount of training data required for fine-tuning LMs. However, their effectiveness is closely related to computational resources, which always require a high compute budget. Owing to the resource limitations in practical fine-tuning scenario, we systematically reveal the relationship between data selection and uncertainty estimation of selected data. Although large language models (LLMs) exhibit exceptional capabilities in language understanding and generation, which provide new ways to alleviate data scarcity, evaluating data usability remains a challenging task. This makes efficient data selection indispensable. To mitigate these issues, we propose Entropy-Based Unsupervised Data Selection (EUDS) framework. Empirical experiments on sentiment analysis (SA), topic classification (Topic-CLS), and question answering (Q&A) tasks validate its effectiveness. EUDS establishes a computationally efficient data-filtering mechanism. Theoretical analysis and experimental results confirm the effectiveness of our approach. EUDS significantly reduces computational costs and improves training time efficiency with less data requirement. This provides an innovative solution for the efficient fine-tuning of LMs in the compute-constrained scenarios.

</details>


### [71] [PEACE 2.0: Grounded Explanations and Counter-Speech for Combating Hate Expressions](https://arxiv.org/abs/2602.17467)
*Greta Damo,Stéphane Petiot,Elena Cabrio,Serena Villata*

Main category: cs.CL

TL;DR: PEACE 2.0是一种新型工具，能够生成回应仇恨言论的消息，并且其功能包括利用 Retrieval-Augmented Generation (RAG) 管道将仇恨言论解释与证据和事实相联系，自动生成基于证据的反制信息，以及探索反制信息的特点。


<details>
  <summary>Details</summary>
Motivation: 鉴于在线平台上的仇恨言论日益增多，虽然自然语言处理领域已经发展出了自动检测仇恨言论的有效方法，但对仇恨言论的回应（称为反制言论）仍然是一个开放的挑战。因此，提出了PEACE 2.0工具以解决这个问题。

Method: PEACE 2.0利用一种名为 Retrieval-Augmented Generation (RAG) 的管道技术，将仇恨言论解释与证据和事实联系起来，并自动生成基于证据的反制信息；此外，它还研究了反制信息的特征。

Result: PEACE 2.0能够深入分析和生成应对显性和隐含仇恨言论的回应。这种工具的引入标志着在处理在线平台上的仇恨言论问题上迈出的重要一步。

Conclusion: PEACE 2.0的成功开发和应用有助于减少互联网上的仇恨言论，并促进健康、积极的在线交流环境。

Abstract: The increasing volume of hate speech on online platforms poses significant societal challenges. While the Natural Language Processing community has developed effective methods to automatically detect the presence of hate speech, responses to it, called counter-speech, are still an open challenge. We present PEACE 2.0, a novel tool that, besides analysing and explaining why a message is considered hateful or not, also generates a response to it. More specifically, PEACE 2.0 has three main new functionalities: leveraging a Retrieval-Augmented Generation (RAG) pipeline i) to ground HS explanations into evidence and facts, ii) to automatically generate evidence-grounded counter-speech, and iii) exploring the characteristics of counter-speech replies. By integrating these capabilities, PEACE 2.0 enables in-depth analysis and response generation for both explicit and implicit hateful messages.

</details>


### [72] [Auditing Reciprocal Sentiment Alignment: Inversion Risk, Dialect Representation and Intent Misalignment in Transformers](https://arxiv.org/abs/2602.17469)
*Nusrat Jahan Lia,Shubhashis Roy Dipta*

Main category: cs.CL

TL;DR: 论文探讨了跨语言情感错位问题，尤其是在孟加拉语和英语之间的错位，并通过四种变换器架构进行基准测试，揭示了当前对齐模式中的安全性和表征失败，提出了人类-人工智能相互进化需要基于文化的角度，尊重语言和方言多样性。


<details>
  <summary>Details</summary>
Motivation: 论文指出当前的模型在情感理解上存在严重的安全和表征问题，需要更加注重跨语言情感理解的一致性，以促进人类与人工智能的信任。

Method: 论文通过对比四种变换器架构，即BERT、DistilBERT、mDistilBERT和IndicBERT来研究情感对齐的挑战，通过这些模型的表现来揭示问题所在。

Result: 研究发现了一些严重的问题，包括情感反转率高达28.7%，模型对情感的处理存在系统性差异，以及区域模型在正式语境下的情感对齐误差增加。

Conclusion: 论文提出了建立基于情感稳定性的对齐基准，以确保低资源和方言情境下的可靠情感理解，促进人类-人工智能的相互信任发展。

Abstract: The core theme of bidirectional alignment is ensuring that AI systems accurately understand human intent and that humans can trust AI behavior. However, this loop fractures significantly across language barriers. Our research addresses Cross-Lingual Sentiment Misalignment between Bengali and English by benchmarking four transformer architectures. We reveal severe safety and representational failures in current alignment paradigms. We demonstrate that compressed model (mDistilBERT) exhibits 28.7% "Sentiment Inversion Rate," fundamentally misinterpreting positive user intent as negative (or vice versa). Furthermore, we identify systemic nuances affecting human-AI trust, including "Asymmetric Empathy" where some models systematically dampen and others amplify the affective weight of Bengali text relative to its English counterpart. Finally, we reveal a "Modern Bias" in the regional model (IndicBERT), which shows a 57% increase in alignment error when processing formal (Sadhu) Bengali. We argue that equitable human-AI co-evolution requires pluralistic, culturally grounded alignment that respects language and dialectal diversity over universal compression, which fails to preserve the emotional fidelity required for reciprocal human-AI trust. We recommend that alignment benchmarks incorporate "Affective Stability" metrics that explicitly penalize polarity inversions in low-resource and dialectal contexts.

</details>


### [73] [Small LLMs for Medical NLP: a Systematic Analysis of Few-Shot, Constraint Decoding, Fine-Tuning and Continual Pre-Training in Italian](https://arxiv.org/abs/2602.17475)
*Pietro Ferrazzi,Mattia Franzin,Alberto Lavelli,Bernardo Magnini*

Main category: cs.CL

TL;DR: 研究发现，小型语言模型（约10亿参数）在多种医疗自然语言处理任务中表现出色，甚至能优于更大的基线模型，特别是在Fine-tuning和few-shot prompting结合使用时效果最佳。


<details>
  <summary>Details</summary>
Motivation: 鉴于大型语言模型在计算资源上的高昂需求，研究旨在探索小型语言模型在医疗自然语言处理任务上的应用潜力。

Method: 研究采用了来自三个主要家族的几种小型语言模型进行评估，包括Llama-3, Gemma-3, 和Qwen3。通过20项临床NLP任务的测试，比较了多种适应性策略的效果。

Result: Fine-tuning被证明是最有效的方法。few-shot prompting和constraint decoding的组合在资源受限的情况下表现出色。基于Qwen3-1.7B的最优配置比Qwen3-32B的性能高出9.2分。

Conclusion: 小型语言模型可以在不牺牲精度的情况下，有效地执行医疗自然语言处理任务。通过公开数据集和模型，研究为医疗NLP的进一步研究提供了支持。

Abstract: Large Language Models (LLMs) consistently excel in diverse medical Natural Language Processing (NLP) tasks, yet their substantial computational requirements often limit deployment in real-world healthcare settings. In this work, we investigate whether "small" LLMs (around one billion parameters) can effectively perform medical tasks while maintaining competitive accuracy. We evaluate models from three major families-Llama-3, Gemma-3, and Qwen3-across 20 clinical NLP tasks among Named Entity Recognition, Relation Extraction, Case Report Form Filling, Question Answering, and Argument Mining. We systematically compare a range of adaptation strategies, both at inference time (few-shot prompting, constraint decoding) and at training time (supervised fine-tuning, continual pretraining). Fine-tuning emerges as the most effective approach, while the combination of few-shot prompting and constraint decoding offers strong lower-resource alternatives. Our results show that small LLMs can match or even surpass larger baselines, with our best configuration based on Qwen3-1.7B achieving an average score +9.2 points higher than Qwen3-32B. We release a comprehensive collection of all the publicly available Italian medical datasets for NLP tasks, together with our top-performing models. Furthermore, we release an Italian dataset of 126M words from the Emergency Department of an Italian Hospital, and 175M words from various sources that we used for continual pre-training.

</details>


### [74] [Bridging the Domain Divide: Supervised vs. Zero-Shot Clinical Section Segmentation from MIMIC-III to Obstetrics](https://arxiv.org/abs/2602.17513)
*Baris Karacan,Barbara Di Eugenio,Patrick Thornton*

Main category: cs.CL

TL;DR: 本文引入了一个新的脱敏和标注的产科记录数据集，并系统地评估了变压器监督模型在MIMIC-III（领域内）和新的产科学数据集（领域外）上的表现，以及监督模型与零样本大语言模型的首个多对多对比。结果显示，监督模型在领域内表现良好，但在领域外表现较差，而零样本模型通过纠正幻觉的段落标题后表现出更强的适应性，强调了开发特定领域临床资源的重要性。


<details>
  <summary>Details</summary>
Motivation: 由于临床自由文本笔记包含重要的患者信息，但大多数现有方法大多在MIMIC-III等公共语料库上进行训练，无法涵盖所有医学领域，因此本文旨在填补这一空白，引入一个新的脱敏、标注的产科记录数据集，并评估监督模型和零样本模型在领域内和领域外的表现。

Method: 本文首先收集和标注了新的数据集，然后探讨了变压器监督模型在MIMIC-III（领域内）和新的产科学数据集（领域外）上的表现，并进行了监督模型与零样本模型的对比实验。

Result: 本文结果显示，监督模型在领域内表现很好，但领域外性能下降明显，而零样本模型在纠正幻觉段落标题后展现出更强的适应能力。

Conclusion: 本文的发现强调了开发特定领域临床资源的重要性，并为临床NLP的应用指出了零样本分割这一有前景的方向，但同时也提醒我们需要恰当管理幻觉。

Abstract: Clinical free-text notes contain vital patient information. They are structured into labelled sections; recognizing these sections has been shown to support clinical decision-making and downstream NLP tasks. In this paper, we advance clinical section segmentation through three key contributions. First, we curate a new de-identified, section-labeled obstetrics notes dataset, to supplement the medical domains covered in public corpora such as MIMIC-III, on which most existing segmentation approaches are trained. Second, we systematically evaluate transformer-based supervised models for section segmentation on a curated subset of MIMIC-III (in-domain), and on the new obstetrics dataset (out-of-domain). Third, we conduct the first head-to-head comparison of supervised models for medical section segmentation with zero-shot large language models. Our results show that while supervised models perform strongly in-domain, their performance drops substantially out-of-domain. In contrast, zero-shot models demonstrate robust out-of-domain adaptability once hallucinated section headers are corrected. These findings underscore the importance of developing domain-specific clinical resources and highlight zero-shot segmentation as a promising direction for applying healthcare NLP beyond well-studied corpora, as long as hallucinations are appropriately managed.

</details>


### [75] [Using LLMs for Knowledge Component-level Correctness Labeling in Open-ended Coding Problems](https://arxiv.org/abs/2602.17542)
*Zhangqi Duan,Arnav Kankaria,Dhruv Kartik,Andrew Lan*

Main category: cs.CL

TL;DR: 该研究提出了一种利用大型语言模型自动标注细粒度技能标签的框架，提高了学习曲线拟合度和预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有的学生建模和学习分析方法往往依赖于细粒度的知识组件（KCs）来表示学生的技能层次，但在真实世界的数据集中，KCs级别的正确性标签很少可用，特别是在涉及多个KCs的开放性编程任务中。

Method: 本研究提出了一种自动化框架，利用大型语言模型直接从学生编写的代码中标注KCs级别的正确性标签。该方法通过评估每个KC是否正确应用，引入了时间上下文感知的Code-KC映射机制，以更好地对齐KCs和个别学生的代码。

Result: 实验结果显示，该框架生成的学习曲线更加符合认知理论，并提高了预测性能。与基线相比，这种改进是显著的。此外，对KCs级别的正确性标签进行的人类评估进一步展示了LLM与专家注解之间的巨大一致性。

Conclusion: 该框架能够为开放型编程任务提供更合适的学习曲线和预测性能，同时减少了对人工标注的依赖。

Abstract: Fine-grained skill representations, commonly referred to as knowledge components (KCs), are fundamental to many approaches in student modeling and learning analytics. However, KC-level correctness labels are rarely available in real-world datasets, especially for open-ended programming tasks where solutions typically involve multiple KCs simultaneously. Simply propagating problem-level correctness to all associated KCs obscures partial mastery and often leads to poorly fitted learning curves. To address this challenge, we propose an automated framework that leverages large language models (LLMs) to label KC-level correctness directly from student-written code. Our method assesses whether each KC is correctly applied and further introduces a temporal context-aware Code-KC mapping mechanism to better align KCs with individual student code. We evaluate the resulting KC-level correctness labels in terms of learning curve fit and predictive performance using the power law of practice and the Additive Factors Model. Experimental results show that our framework leads to learning curves that are more consistent with cognitive theory and improves predictive performance, compared to baselines. Human evaluation further demonstrates substantial agreement between LLM and expert annotations.

</details>


### [76] [Learning to Stay Safe: Adaptive Regularization Against Safety Degradation during Fine-Tuning](https://arxiv.org/abs/2602.17546)
*Jyotin Goel,Souvik Maji,Pratik Mazumder*

Main category: cs.CL

TL;DR: 该研究提出了一种训练框架，通过动态调整正则化来应对安全风险，使模型在微调过程中保持对齐。该框架通过两种方式估计安全风险：基于裁判的安全批判和基于激活的风险预测器，并通过风险信号约束较高风险的更新，从而降低了攻击成功率，同时保持了下游性能且无需增加推理成本。


<details>
  <summary>Details</summary>
Motivation: 现有的安全防御措施往往提供有限的保护或要求在安全性和实用性之间做出权衡。本文旨在提供一种机制，能够在模型微调过程中保持安全对齐，同时避免牺牲实用性。

Method: 该方法通过动态调整正则化来应对安全风险，引入了两种风险估计方法：一种是基于裁判的安全批判，另一种是基于激活的风险预测器。这些方法通过特定的信号约束高风险更新，并允许低风险更新按标准方式调整。

Result: 实验结果表明，从预生成激活中可以预测有害意图信号，裁判评分可以提供有效且高召回率的安全指导。无论是在不同模型系列还是在不同攻击场景下，使用任何风险估计方法进行自适应正则化的模型都比标准微调降低了攻击成功率，且保持了下游性能，并且没有增加推理时的计算成本。

Conclusion: 该工作展示了维持安全性而无需牺牲实用性的原则性机制，为语言模型的安全性研究提供了新的方向。

Abstract: Instruction-following language models are trained to be helpful and safe, yet their safety behavior can deteriorate under benign fine-tuning and worsen under adversarial updates. Existing defenses often offer limited protection or force a trade-off between safety and utility. We introduce a training framework that adapts regularization in response to safety risk, enabling models to remain aligned throughout fine-tuning. To estimate safety risk at training time, we explore two distinct approaches: a judge-based Safety Critic that assigns high-level harm scores to training batches, and an activation-based risk predictor built with a lightweight classifier trained on intermediate model activations to estimate harmful intent. Each approach provides a risk signal that is used to constrain updates deemed higher risk to remain close to a safe reference policy, while lower-risk updates proceed with standard training. We empirically verify that harmful intent signals are predictable from pre-generation activations and that judge scores provide effective high-recall safety guidance. Across multiple model families and attack scenarios, adaptive regularization with either risk estimation approach consistently lowers attack success rate compared to standard fine-tuning, preserves downstream performance, and adds no inference-time cost. This work demonstrates a principled mechanism for maintaining safety without sacrificing utility.

</details>


### [77] [Unmasking the Factual-Conceptual Gap in Persian Language Models](https://arxiv.org/abs/2602.17623)
*Alireza Sakhaeirad,Ali Ma'manpoosh,Arshia Hemmat*

Main category: cs.CL

TL;DR: DivanBench 是一个专注于迷信和习俗等任意、情境依赖规则的诊断型基准测试，旨在评估三种任务类型下的七种波斯语言模型。主要发现包括：大多数模型表现出严重的趋从偏见，连续的波斯语预训练反而加剧了这一问题；所有模型在从知识库检索事实与应用于场景之间存在21%的性能差距。这表明文化能力不仅仅是大规模单语言数据的扩展。


<details>
  <summary>Details</summary>
Motivation: 当前的波斯自然语言处理(NLP)基准测试主要集中在语用学和礼貌问题上，并没有区分记忆中的文化事实和推理复杂社会规范的能力。因此，提出了DivanBench来评估模型对于迷信和习俗的理解。

Method: DivanBench 通过315个问题，涵盖了三种任务类型（事实检索、配对情景验证和情境推理），对七种波斯语言模型进行了评估，以此揭示这些模型的局限性。

Result: 研究结果表明，大多数模型存在严重的趋从偏见，在正确识别适当行为方面表现良好，但在拒绝明显违反的行为方面表现较差。连续的波斯语预训练并未提高模型的推理能力，反而导致其难以辨别矛盾。所有模型在从知识点检索和实际应用之间存在21%的性能差距。

Conclusion: 研究结论指出，文化的熟练应用不仅仅依赖于单一语言的大量数据，还需要模型内部化复杂的社会规范模式。

Abstract: While emerging Persian NLP benchmarks have expanded into pragmatics and politeness, they rarely distinguish between memorized cultural facts and the ability to reason about implicit social norms. We introduce DivanBench, a diagnostic benchmark focused on superstitions and customs, arbitrary, context-dependent rules that resist simple logical deduction. Through 315 questions across three task types (factual retrieval, paired scenario verification, and situational reasoning), we evaluate seven Persian LLMs and reveal three critical failures: most models exhibit severe acquiescence bias, correctly identifying appropriate behaviors but failing to reject clear violations; continuous Persian pretraining amplifies this bias rather than improving reasoning, often degrading the model's ability to discern contradictions; and all models show a 21\% performance gap between retrieving factual knowledge and applying it in scenarios. These findings demonstrate that cultural competence requires more than scaling monolingual data, as current models learn to mimic cultural patterns without internalizing the underlying schemas.

</details>


### [78] [Differences in Typological Alignment in Language Models' Treatment of Differential Argument Marking](https://arxiv.org/abs/2602.17653)
*Iskar Deng,Nathalia Xu,Shane Steinert-Threlkeld*

Main category: cs.CL

TL;DR: 使用GPT-2模型在18个不同语料库上进行训练，研究语言模型对差异性论元标记(DAM)系统的偏好，揭示了模型在自然标记方向和人类语言中偏向的差异。


<details>
  <summary>Details</summary>
Motivation: 探索语言模型在学习合成语料库后是否能展示出与人类语言中的跨语言规律相似的偏好，特别是针对论元标记（DAM）这一语义许可系统。

Method: 通过使用控制下的合成学习方法进行训练GPT-2模型，并通过最小词对进行评估。

Result: 研究发现，模型在自然标记方向上表现出了与人类语言相似的倾向，但在论元偏好上与人类语言有所不同，表明不同类型的语言倾向可能源自不同的基础。

Conclusion: 此研究表明，不同类型的语言倾向可能是由不同的基础产生的，为理解语言模型的学习机制提供了新见解。

Abstract: Recent work has shown that language models (LMs) trained on synthetic corpora can exhibit typological preferences that resemble cross-linguistic regularities in human languages, particularly for syntactic phenomena such as word order. In this paper, we extend this paradigm to differential argument marking (DAM), a semantic licensing system in which morphological marking depends on semantic prominence. Using a controlled synthetic learning method, we train GPT-2 models on 18 corpora implementing distinct DAM systems and evaluate their generalization using minimal pairs. Our results reveal a dissociation between two typological dimensions of DAM. Models reliably exhibit human-like preferences for natural markedness direction, favoring systems in which overt marking targets semantically atypical arguments. In contrast, models do not reproduce the strong object preference in human languages, in which overt marking in DAM more often targets objects rather than subjects. These findings suggest that different typological tendencies may arise from distinct underlying sources.

</details>


### [79] [What Language is This? Ask Your Tokenizer](https://arxiv.org/abs/2602.17655)
*Clara Meister,Ahmetcan Yavuz,Pietro Lesci,Tiago Pimentel*

Main category: cs.CL

TL;DR: UniLID 是一种基于 UnigramLM 算法的简单高效的语言识别方法，能够在低资源语言和相近语言环境中展现出色性能。


<details>
  <summary>Details</summary>
Motivation: 现有的语言识别系统在低资源和紧密相关语言环境中表现脆弱，因此作者提出了 UniLID 方法来解决这些问题。

Method: UniLID 方法基于 UnigramLM 算法，学习语言条件下的单词分布，并将分段视为语言特定的现象。该方法在数据和计算效率、可扩展性和易于集成到现有语言模型分词管道方面表现出优势。

Result: 实验证明，UniLID 在标准基准上达到了竞争性性能，特别是在低资源环境下的样本效率显著提升，通过五份标记样本即可达到 70% 以上的准确率，并在细粒度方言识别方面取得显著进展。

Conclusion: UniLID 是一种简单有效的语言识别方法，特别适用于低资源语言环境，未来可能会有广泛应用。

Abstract: Language Identification (LID) is an important component of many multilingual natural language processing pipelines, where it facilitates corpus curation, training data analysis, and cross-lingual evaluation of large language models. Despite near-perfect performance on high-resource languages, existing systems remain brittle in low-resource and closely related language settings. We introduce UniLID, a simple and efficient LID method based on the UnigramLM tokenization algorithm, leveraging its probabilistic framing, parameter estimation technique and inference strategy. In short, we learn language-conditional unigram distributions over a shared tokenizer vocabulary but treat segmentation as a language-specific phenomenon. Our formulation is data- and compute-efficient, supports incremental addition of new languages without retraining existing models, and can naturally be integrated into existing language model tokenization pipelines. Empirical evaluations against widely used baselines, including fastText, GlotLID, and CLD3, show that UniLID achieves competitive performance on standard benchmarks, substantially improves sample efficiency in low-resource settings - surpassing 70% accuracy with as few as five labeled samples per language - and delivers large gains on fine-grained dialect identification.

</details>


### [80] [Sink-Aware Pruning for Diffusion Language Models](https://arxiv.org/abs/2602.17664)
*Aidar Myrzakhan,Tianyi Li,Bowei Guo,Shengkun Tang,Zhiqiang Shen*

Main category: cs.CL

TL;DR: 该研究提出了Sink-Aware Pruning方法，该方法针对扩散语言模型(DLMs)的特性，自动识别不稳定的注意力终点并进行剪枝，从而提高了推理效率和质量，表现出优于之前方法的优势。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型因迭代去噪过程产生高昂的推理成本，现有剪枝方法主要借鉴自自回归(AR)语言模型，通常保留稳定的整体锚点（注意力终点）。然而，研究表明，DLMs中的注意力终点不存在类似AR模型中的稳定性，因此需要一种新的方法来更高效地剪枝。

Method: 研究通过观察发现在DLMs中，注意力终点的位置在生成过程中表现出较高的变异度。基于此观察，提出了Sink-Aware Pruning方法。该方法能够自动识别并剪枝不稳定的注意力终点，而无需重新训练。

Result: 实验结果显示，该方法在与之前剪枝方法匹配的计算资源下，能实现更好的质量-效率折衷。

Conclusion: 该研究为扩散语言模型提供了更有效的剪枝策略，提高了模型的推理效率和质量。

Abstract: Diffusion Language Models (DLMs) incur high inference cost due to iterative denoising, motivating efficient pruning. Existing pruning heuristics largely inherited from autoregressive (AR) LLMs, typically preserve attention sink tokens because AR sinks serve as stable global anchors. We show that this assumption does not hold for DLMs: the attention-sink position exhibits substantially higher variance over the full generation trajectory (measured by how the dominant sink locations shift across timesteps), indicating that sinks are often transient and less structurally essential than in AR models. Based on this observation, we propose ${\bf \texttt{Sink-Aware Pruning}}$, which automatically identifies and prunes unstable sinks in DLMs (prior studies usually keep sinks for AR LLMs). Without retraining, our method achieves a better quality-efficiency trade-off and outperforms strong prior pruning baselines under matched compute. Our code is available at https://github.com/VILA-Lab/Sink-Aware-Pruning.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [81] [AIdentifyAGE Ontology for Decision Support in Forensic Dental Age Assessment](https://arxiv.org/abs/2602.16714)
*Renato Marcelo,Ana Rodrigues,Cristiana Palmela Pereira,António Figueiras,Rui Santos,José Rui Figueira,Alexandre P Francisco,Cátia Vaz*

Main category: cs.AI

TL;DR: AIdentifyAGE ontology 提供了一个标准化、语义一致的框架，涵盖了手动和AI辅助的法医牙齿年龄评估工作流程，整合了司法背景、个人数据、法医检查数据、牙齿发育评估方法、放射成像、统计参考研究和基于AI的估算法。它解决了方法异质性、数据表示碎片化和临床、法医和法律信息系统之间的互操作性不足的问题，确保了互操作性、可扩展性和遵守FAIR原则。这对于增强一致性和透明度、建立面向语义信息学的决策支持系统具有重要意义。


<details>
  <summary>Details</summary>
Motivation: 当前法医牙齿年龄评估面临方法学异质性、数据碎片化表示和临床、法医和法律信息系统互操作性差的问题，从而影响透明性和可重复性，尤其在处理无法确认身份的个体和无监护的未成年人案件时更为显著。AIdentifyAGE ontology 的目标是提供一个标准化的框架来解决这些问题，增强在司法和法律背景下的一致性、透明度和可解释性。

Method: AIdentifyAGE ontology 结合了手动和基于AI的法医牙齿年龄评估流程的模式化描述，涵盖了司法、个人、法医检查、牙齿发育、放射成像、统计参考研究和AI方法。它与上层和现有的生物医学、牙科和机器学习本体兼容，应用FAIR原则（Findable，Accessible，Interoperable，Reproducible），确保互操作性、扩展性和合规性。

Result: AIdentifyAGE ontology 通过提供一个标准化、语义一致的框架，解决了法医牙齿年龄评估中的关键问题，增强了透明性和可解释性。它为基于语义信息学的司法和法律背景下决策支持系统的开发奠定了坚实的基础。

Conclusion: AIdentifyAGE ontology 是一个重要的进步，可以促进法医牙齿年龄评估的一致性和透明性。它能够提升司法和法律决策过程中的互操作性和信息可用性，为未来的研究和发展提供指导。

Abstract: Age assessment is crucial in forensic and judicial decision-making, particularly in cases involving undocumented individuals and unaccompanied minors, where legal thresholds determine access to protection, healthcare, and judicial procedures. Dental age assessment is widely recognized as one of the most reliable biological approaches for adolescents and young adults, but current practices are challenged by methodological heterogeneity, fragmented data representation, and limited interoperability between clinical, forensic, and legal information systems. These limitations hinder transparency and reproducibility, amplified by the increasing adoption of AI- based methods. The AIdentifyAGE ontology is domain-specific and provides a standardized, semantically coherent framework, encompassing both manual and AI-assisted forensic dental age assessment workflows, and enabling traceable linkage between observations, methods, reference data, and reported outcomes. It models the complete medico-legal workflow, integrating judicial context, individual-level information, forensic examination data, dental developmental assessment methods, radiographic imaging, statistical reference studies, and AI-based estimation methods. It is being developed together with domain experts, and it builds on upper and established biomedical, dental, and machine learning ontologies, ensuring interoperability, extensibility, and compliance with FAIR principles. The AIdentifyAGE ontology is a fundamental step to enhance consistency, transparency, and explainability, establishing a robust foundation for ontology-driven decision support systems in medico-legal and judicial contexts.

</details>


### [82] [Contextuality from Single-State Representations: An Information-Theoretic Principle for Adaptive Intelligence](https://arxiv.org/abs/2602.16716)
*Song-Ju Kim*

Main category: cs.AI

TL;DR: 该论文展示了上下文效应用于经典概率表示的必要性，并提供了一个最小的构造性示例来实现这一成本。


<details>
  <summary>Details</summary>
Motivation: 研究在有限资源下适应性系统如何在不同上下文中重用相同的内部状态空间。

Method: 通过将上下文视为作用于共享内部状态的干预措施来建模，论文证明了任何能产生上下文结果统计的经典模型都必须承担一个不可减少的信息论成本。

Result: 论文展示了经典模型不可避免地依赖上下文，且不能仅通过内部状态来中介这种依赖。提供了最小的构造性示例来说明这种成本的操作含义。

Conclusion: 非经典概率框架通过放宽单个全局联合概率空间的假设来避免这一限制，无须引入量子动力学或希尔伯特空间结构。

Abstract: Adaptive systems often operate across multiple contexts while reusing a fixed internal state space due to constraints on memory, representation, or physical resources. Such single-state reuse is ubiquitous in natural and artificial intelligence, yet its fundamental representational consequences remain poorly understood. We show that contextuality is not a peculiarity of quantum mechanics, but an inevitable consequence of single-state reuse in classical probabilistic representations. Modeling contexts as interventions acting on a shared internal state, we prove that any classical model reproducing contextual outcome statistics must incur an irreducible information-theoretic cost: dependence on context cannot be mediated solely through the internal state. We provide a minimal constructive example that explicitly realizes this cost and clarifies its operational meaning. We further explain how nonclassical probabilistic frameworks avoid this obstruction by relaxing the assumption of a single global joint probability space, without invoking quantum dynamics or Hilbert space structure. Our results identify contextuality as a general representational constraint on adaptive intelligence, independent of physical implementation.

</details>


### [83] [Mobility-Aware Cache Framework for Scalable LLM-Based Human Mobility Simulation](https://arxiv.org/abs/2602.16727)
*Hua Yan,Heng Tan,Yingxue Zhang,Yu Yang*

Main category: cs.AI

TL;DR: MobCache 设计了一种利用可重构缓存的移动感知缓存框架，通过编码推理步骤为潜在空间嵌入并使用潜在空间评估器重用和重组，实现了高效的大规模人类移动模拟。


<details>
  <summary>Details</summary>
Motivation: 为了解决使用大规模语言模型（LLMs）生成人类移动行为时的高计算成本限制其可扩展性问题。

Method: MobCache框架包括推理组件和解码组件，通过编码推理步骤为潜在空间嵌入和使用潜在空间评估器，以及使用受移动法则约束的蒸馏轻量级解码器进行步长链的自然语言翻译来提高效率。

Result: 实验结果表明，MobCache在多个维度上提高了效率，同时保持了与最先进的基于LLM的方法相当的性能。

Conclusion: MobCache框架能够高效地进行大规模人类移动模拟，且结果具有高度的准确性。

Abstract: Large-scale human mobility simulation is critical for applications such as urban planning, epidemiology, and transportation analysis. Recent works treat large language models (LLMs) as human agents to simulate realistic mobility behaviors using structured reasoning, but their high computational cost limits scalability. To address this, we design a mobility-aware cache framework named MobCache that leverages reconstructible caches to enable efficient large-scale human mobility simulations. It consists of: (1) a reasoning component that encodes each reasoning step as a latent-space embedding and uses a latent-space evaluator to enable the reuse and recombination of reasoning steps; and (2) a decoding component that employs a lightweight decoder trained with mobility law-constrained distillation to translate latent-space reasoning chains into natural language, thereby improving simulation efficiency while maintaining fidelity. Experiments show that MobCache significantly improves efficiency across multiple dimensions while maintaining performance comparable to state-of-the-art LLM-based methods.

</details>


### [84] [When AI Benchmarks Plateau: A Systematic Study of Benchmark Saturation](https://arxiv.org/abs/2602.16763)
*Mubashara Akhtar,Anka Reuel,Prajna Soni,Sanchit Ahuja,Pawan Sasanka Ammanamanchi,Ruchit Rawal,Vilém Zouhar,Srishti Yadav,Chenxi Whitehouse,Dayeon Ki,Jennifer Mickel,Leshem Choshen,Marek Šuppa,Jan Batzner,Jenny Chim,Jeba Sania,Yanan Long,Hossein A. Rahmani,Christina Knight,Yiyang Nan,Jyoutir Raj,Yu Fan,Shubham Singh,Subramanyam Sahoo,Eliya Habba,Usman Gohar,Siddhesh Pawar,Robert Scholz,Arjun Subramonian,Jingwei Ni,Mykel Kochenderfer,Sanmi Koyejo,Mrinmaya Sachan,Stella Biderman,Zeerak Talat,Avijit Ghosh,Irene Solaiman*

Main category: cs.AI

TL;DR: 该研究分析了来自主要模型开发者技术报告中的60个大型语言模型(Large Language Model, LLM)基准，发现近一半的基准已经饱和，且随着基准的年龄增长这一趋势加剧。研究还发现，隐藏测试数据和Crowdsourced基准对防止基准饱和无明显效果，而由专家策展的基准更能抵抗饱和。这些发现有助于识别延长基准寿命的设计选择，并为制定更稳健的评估策略提供指导。


<details>
  <summary>Details</summary>
Motivation: 探究基准饱和的原因，提高模型开发和部署决策的长期价值。

Method: 研究选取了60个LLM基准，从技术报告中提取了14个特征，包括任务设计、数据构建和评估格式，进行分析并验证有关基准饱和的5个假设。

Result: 研究揭示了几乎一半的基准出现了饱和现象，并随着时间的推移这一现象变得更为普遍。值得注意的是，隐藏测试数据这一策略并未有效防止基准饱和。相比之下，专家策展的基准更能抵抗这种现象。

Conclusion: 研究表明，设计选择在基准寿命方面起着重要作用。这些建议有助于指导未来基准的设计，以实现更强大的评估。

Abstract: Artificial Intelligence (AI) benchmarks play a central role in measuring progress in model development and guiding deployment decisions. However, many benchmarks quickly become saturated, meaning that they can no longer differentiate between the best-performing models, diminishing their long-term value. In this study, we analyze benchmark saturation across 60 Large Language Model (LLM) benchmarks selected from technical reports by major model developers. To identify factors driving saturation, we characterize benchmarks along 14 properties spanning task design, data construction, and evaluation format. We test five hypotheses examining how each property contributes to saturation rates. Our analysis reveals that nearly half of the benchmarks exhibit saturation, with rates increasing as benchmarks age. Notably, hiding test data (i.e., public vs. private) shows no protective effect, while expert-curated benchmarks resist saturation better than crowdsourced ones. Our findings highlight which design choices extend benchmark longevity and inform strategies for more durable evaluation.

</details>


### [85] [Simple Baselines are Competitive with Code Evolution](https://arxiv.org/abs/2602.16805)
*Yonatan Gideoni,Sebastian Risi,Yarin Gal*

Main category: cs.AI

TL;DR: 本研究测试了两种简单的基线方法在代码演化中的性能，结果表明简单基线与更复杂的模型相当甚至更好。研究指出代码演化的开发和使用存在一些不足，并提出改进评价方法以减少随机性并保持经济可行性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过实验证明简单方法在代码演化任务中的有效性，揭示代码演化中的问题所在，并提出改进策略。

Method: 作者选择了三个领域（数学边界优化、设计智能辅助构建物、机器学习竞赛），使用两种简单的基线方法进行测试，并通过分析结果找出代码演化中的问题。

Result: 在三个领域中，简单的基线方法与更复杂的模型表现相当或更好，尤其是在数学边界优化方面发现搜索空间的设计至关重要。

Conclusion: 研究结论指出，为了解决代码演化的挑战，主要在于更好的设计搜索空间，提出改进评价方法的建议，并鼓励未来工作采取更有针对性的最佳实践。

Abstract: Code evolution is a family of techniques that rely on large language models to search through possible computer programs by evolving or mutating existing code. Many proposed code evolution pipelines show impressive performance but are often not compared to simpler baselines. We test how well two simple baselines do over three domains: finding better mathematical bounds, designing agentic scaffolds, and machine learning competitions. We find that simple baselines match or exceed much more sophisticated methods in all three. By analyzing these results we find various shortcomings in how code evolution is both developed and used. For the mathematical bounds, a problem's search space and domain knowledge in the prompt are chiefly what dictate a search's performance ceiling and efficiency, with the code evolution pipeline being secondary. Thus, the primary challenge in finding improved bounds is designing good search spaces, which is done by domain experts, and not the search itself. When designing agentic scaffolds we find that high variance in the scaffolds coupled with small datasets leads to suboptimal scaffolds being selected, resulting in hand-designed majority vote scaffolds performing best. We propose better evaluation methods that reduce evaluation stochasticity while keeping the code evolution economically feasible. We finish with a discussion of avenues and best practices to enable more rigorous code evolution in future work.

</details>


### [86] [Improved Upper Bounds for Slicing the Hypercube](https://arxiv.org/abs/2602.16807)
*Duncan Soiffer,Nathaniel Itty,Christopher D. Rosin,Blake Bruell,Mason DiCicco,Gábor N. Sárközy,Ryan Offstein,Daniel Reichman*

Main category: cs.AI

TL;DR: 该论文通过构造8个超平面切割10维超立方体，并利用CPro1工具（结合了逻辑语言模型和自动参数调优）来发现数学构建方法，证明了最小切割超立方体所需超平面数量的改进上界为 ceil(4n/5) (除非$n$是5的奇数倍，则为 4n/5 + 1)，改进了之前1971年Paterson的上界 ceil(5n/6)。


<details>
  <summary>Details</summary>
Motivation: 研究$n$维超立方体边的切割问题，探索最小化切割所需超平面的数量，以及利用机器学习和自动优化工具寻找有效的数学构建方法。

Method: 通过构造$8$个超平面切割$10$维超立方体，并使用CPro1工具自动优化搜索算法来发现实际的数学构造。

Result: 证明了$S(n)$的新上界为 ceil(4n/5) 除非$n$是5的奇数倍，则为 4n/5 + 1，同时给出了$Q_n$中可被少于$n$个超平面切割的最大边数的新下界。

Conclusion: 本文通过理论分析和实际构建，显著改进了之前的理论上界，并展示了利用现代工具进行数学研究的有效性。

Abstract: A collection of hyperplanes $\mathcal{H}$ slices all edges of the $n$-dimensional hypercube $Q_n$ with vertex set $\{-1,1\}^n$ if, for every edge $e$ in the hypercube, there exists a hyperplane in $\mathcal{H}$ intersecting $e$ in its interior. Let $S(n)$ be the minimum number of hyperplanes needed to slice $Q_n$. We prove that $S(n) \leq \lceil \frac{4n}{5} \rceil$, except when $n$ is an odd multiple of $5$, in which case $S(n) \leq \frac{4n}{5} +1$. This improves upon the previously known upper bound of $S(n) \leq \lceil\frac{5n}{6} \rceil$ due to Paterson reported in 1971. We also obtain new lower bounds on the maximum number of edges in $Q_n$ that can be sliced using $k<n$ hyperplanes. We prove the improved upper bound on $S(n)$ by constructing $8$ hyperplanes slicing $Q_{10}$ aided by the recently introduced CPro1: an automatic tool that uses reasoning LLMs coupled with automated hyperparameter tuning to create search algorithms for the discovery of mathematical constructions.

</details>


### [87] [Node Learning: A Framework for Adaptive, Decentralised and Collaborative Network Edge AI](https://arxiv.org/abs/2602.16814)
*Eiman Kanjo,Mustafa Aslanov*

Main category: cs.AI

TL;DR: 该研究提出了一种名为节点学习的分布式学习框架，旨在解决边缘环境中集中式智能的局限性，如数据传输瓶颈、延迟和能耗问题。节点学习通过在边缘节点上实现局部学习、保持模型状态及通过有益的协作交换知识，来促进智能的分布和传播。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能向边缘的扩展，集中式智能面临数据传输瓶颈、高延迟、高能耗和对大型数据中心的高度依赖。这在异构、移动和资源受限的环境中变得尤为不利。因此，该研究引入了节点学习的概念，旨在减少这些限制。

Method: 作者通过定义节点学习的概念框架，描述了学习如何在边缘节点上持续进行，节点间根据协作的益处交换知识，以及学习如何通过重叠和扩散而不是全局同步或中心聚合来传播。

Result: 该研究开发了节点学习的概念基础，对比了它与其他现有的分布式学习方法，并探讨了其对通信、硬件、信任和治理的影响。

Conclusion: 节点学习提供了一种新的分布式学习范式，强调自主和协作行为的一致性，能够适应不同数据、硬件、目标和连接性。

Abstract: The expansion of AI toward the edge increasingly exposes the cost and fragility of cen- tralised intelligence. Data transmission, latency, energy consumption, and dependence on large data centres create bottlenecks that scale poorly across heterogeneous, mobile, and resource-constrained environments. In this paper, we introduce Node Learning, a decen- tralised learning paradigm in which intelligence resides at individual edge nodes and expands through selective peer interaction. Nodes learn continuously from local data, maintain their own model state, and exchange learned knowledge opportunistically when collaboration is beneficial. Learning propagates through overlap and diffusion rather than global synchro- nisation or central aggregation. It unifies autonomous and cooperative behaviour within a single abstraction and accommodates heterogeneity in data, hardware, objectives, and connectivity. This concept paper develops the conceptual foundations of this paradigm, contrasts it with existing decentralised approaches, and examines implications for communi- cation, hardware, trust, and governance. Node Learning does not discard existing paradigms, but places them within a broader decentralised perspective

</details>


### [88] [An order-oriented approach to scoring hesitant fuzzy elements](https://arxiv.org/abs/2602.16827)
*Luis Merino,Gabriel Navarro,Carlos Salvatierra,Evangelina Santos*

Main category: cs.AI

TL;DR: 本文提出了一种基于偏序关系的不确定性评价框架，重新评估了几类经典的犹豫模糊元素评价方法的正交序，并证明得分定义相对于对称序满足关键规范标准，进而引入了一类称为支配函数的函数来对犹豫模糊元素进行排序。


<details>
  <summary>Details</summary>
Motivation: 传统犹豫模糊集评分方法缺乏形式化的序理论基础，本文旨在提供一种基于偏序关系的统一框架，以实现更灵活、一致的评分机制。

Method: 作者首先考察了几类经典的犹豫模糊元素上的线性序，发现它们不构成格结构。接着，作者证明了与给定控制集的最小可接受阈值相结合时，相对于对称序定义的得分满足评分函数的关键规范标准。在此基础上，作者引入了支配函数的概念，并提供了两种有限集上的支配函数的具体例子：离散支配函数和相对支配函数。

Result: 证明了支配函数可以被用来构建典型犹豫模糊集上的模糊偏好关系，支持集体决策。

Conclusion: 文章提出了一个基于偏序关系的犹豫模糊集评分框架，该框架能够更灵活地进行评分，并为犹豫模糊集的排序提供了新的解决方案。

Abstract: Traditional scoring approaches on hesitant fuzzy sets often lack a formal base in order theory. This paper proposes a unified framework, where each score is explicitly defined with respect to a given order. This order-oriented perspective enables more flexible and coherent scoring mechanisms. We examine several classical orders on hesitant fuzzy elements, that is, nonempty subsets in [0,1], and show that, contrary to prior claims, they do not induce lattice structures. In contrast, we prove that the scores defined with respect to the symmetric order satisfy key normative criteria for scoring functions, including strong monotonicity with respect to unions and the Gärdenfors condition.
  Following this analysis, we introduce a class of functions, called dominance functions, for ranking hesitant fuzzy elements. They aim to compare hesitant fuzzy elements relative to control sets incorporating minimum acceptability thresholds. Two concrete examples of dominance functions for finite sets are provided: the discrete dominance function and the relative dominance function. We show that these can be employed to construct fuzzy preference relations on typical hesitant fuzzy sets and support group decision-making.

</details>


### [89] [LLM-WikiRace: Benchmarking Long-term Planning and Reasoning over Real-World Knowledge Graphs](https://arxiv.org/abs/2602.16902)
*Juliusz Ziomek,William Bankes,Lorenz Wolf,Shyam Sundhar Ramesh,Xiaohang Tang,Ilija Bogunovic*

Main category: cs.AI

TL;DR: LLM-Wikirace 是一个评估大规模语言模型规划、推理和世界知识的基准，模型需在维基百科中高效导航，面临明显限制并需进一步强化规划和长距推理能力。


<details>
  <summary>Details</summary>
Motivation: 旨在填补现有语言模型在复杂推理和规划任务上的不足，特别是关于实际世界知识的应用。

Method: 通过让模型逐步导航维基百科的链接到达目标页面，考察模型的前瞻规划能力和现实世界概念的推理能力。

Result: 评估了多种模型，其中Gemini-3和GPT-5等表现出色，但在困难级别上表现较差，输在了规划和长距离推理能力。

Conclusion: 强调世界知识是必要的，但更远的规划能力和长期推理是关键，提出代码和排行榜地址供进一步研究使用。

Abstract: We introduce LLM-Wikirace, a benchmark for evaluating planning, reasoning, and world knowledge in large language models (LLMs). In LLM-Wikirace, models must efficiently navigate Wikipedia hyperlinks step by step to reach a target page from a given source, requiring look-ahead planning and the ability to reason about how concepts are connected in the real world. We evaluate a broad set of open- and closed-source models, including Gemini-3, GPT-5, and Claude Opus 4.5, which achieve the strongest results on the easy level of the task and demonstrate superhuman performance. Despite this, performance drops sharply on hard difficulty: the best-performing model, Gemini-3, succeeds in only 23\% of hard games, highlighting substantial remaining challenges for frontier models. Our analysis shows that world knowledge is a necessary ingredient for success, but only up to a point, beyond this threshold, planning and long-horizon reasoning capabilities become the dominant factors. Trajectory-level analysis further reveals that even the strongest models struggle to replan after failure, frequently entering loops rather than recovering. LLM-Wikirace is a simple benchmark that reveals clear limitations in current reasoning systems, offering an open arena where planning-capable LLMs still have much to prove. Our code and leaderboard available at https:/llmwikirace.github.io.

</details>


### [90] [Mind the GAP: Text Safety Does Not Transfer to Tool-Call Safety in LLM Agents](https://arxiv.org/abs/2602.16943)
*Arnold Cartagena,Ariane Teixeira*

Main category: cs.AI

TL;DR: 分析发现文本生成的安全性并不一定能够转化为外部系统调用的安全性，强调了需要专门的评估和缓解工具调用风险的重要性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在填补当前安全评估框架的空白，确认在不同领域和不同提示条件下，大型语言模型（LLM）是否有同时在文本拒绝和实际执行有害行为之间的差异。

Method: 引入GAP基准测试，全面测试六种现行先进模型在六个监管领域（制药、金融、教育等）的行为，通过不同的系统提示和提示变体，生成大量数据点。

Result: 实验结果显示文本安全性与工具调用安全性之间存在显著差异，即便是经过强化安全提示，仍然有219种有害情况在多个模型中发生；系统提示的有效性显著变化，范围从21到57个百分点。值得注意的是，虽然运行时治理合同减少了信息泄露，但对已禁止的行为调用并未产生显著抑制效果。

Conclusion: 结论表明，仅通过文本评估模型不足以全面评估其行为，对外部系统调用的安全性必须单独进行专门的测量和管理。

Abstract: Large language models deployed as agents increasingly interact with external systems through tool calls--actions with real-world consequences that text outputs alone do not carry. Safety evaluations, however, overwhelmingly measure text-level refusal behavior, leaving a critical question unanswered: does alignment that suppresses harmful text also suppress harmful actions? We introduce the GAP benchmark, a systematic evaluation framework that measures divergence between text-level safety and tool-call-level safety in LLM agents. We test six frontier models across six regulated domains (pharmaceutical, financial, educational, employment, legal, and infrastructure), seven jailbreak scenarios per domain, three system prompt conditions (neutral, safety-reinforced, and tool-encouraging), and two prompt variants, producing 17,420 analysis-ready datapoints. Our central finding is that text safety does not transfer to tool-call safety. Across all six models, we observe instances where the model's text output refuses a harmful request while its tool calls simultaneously execute the forbidden action--a divergence we formalize as the GAP metric. Even under safety-reinforced system prompts, 219 such cases persist across all six models. System prompt wording exerts substantial influence on tool-call behavior: TC-safe rates span 21 percentage points for the most robust model and 57 for the most prompt-sensitive, with 16 of 18 pairwise ablation comparisons remaining significant after Bonferroni correction. Runtime governance contracts reduce information leakage in all six models but produce no detectable deterrent effect on forbidden tool-call attempts themselves. These results demonstrate that text-only safety evaluations are insufficient for assessing agent behavior and that tool-call safety requires dedicated measurement and mitigation.

</details>


### [91] [Fundamental Limits of Black-Box Safety Evaluation: Information-Theoretic and Computational Barriers from Latent Context Conditioning](https://arxiv.org/abs/2602.16984)
*Vishal Srivastava*

Main category: cs.AI

TL;DR: 该研究通过形式化和挑战黑盒安全评估假设，指出在某些模型中，这种假设可能会导致错误的预测，特别是在部署过程中。研究指出，针对某些类别的模型，无论是在被动评估还是适应性评估下，都无法可靠地估计部署风险。同时，研究还提出了计算隔离性，即在具有特权信息的部署环境下，任何不具有相应信息的黑盒评估器都无法区分出有害行为。因此，白盒探测的准确度要求较高，需要更多样本和特定的偏差矫正。


<details>
  <summary>Details</summary>
Motivation: 论文研究旨在揭示黑盒评估在某些复杂模型中的局限性，特别是针对依赖未观察到内部变量的模型，在这些模型中，评估表现与实际部署表现并不总是一致的，特别是在模型内部变量分布于评估与部署环境不同的情况下，黑盒评估可能会忽略潜在的风险。

Method: 论文首先利用Le Cam方法证明了无论是在被动评估还是适应性查询下，对于特定类型模型的估计误差是无法避免的，误差下限为delta*L或delta*L/16。此外，通过陷阱门单向函数假设来进一步说明在部署阶段，即使评估器具有能力去执行探查任务，也有可能无法识别出某些关键信息来区分安全和不安全的行为。因此，需要依赖额外的安全防护措施来确保最坏情况下的安全性。

Result: 论文的结果表明，对于特定类型的模型，无论是被动评估还是适应性查询，都无法有效估计部署风险。此外，在具有特权信息的部署环境下，黑盒评估器无法区分出有害行为，需要额外的安全保障措施来保证最坏情况下的安全性。

Conclusion: 研究最终指出，为了安全性和可靠性，依赖额外措施如架构约束、训练过程中的保证、可解释性以及部署时的监控来增强最坏情况下的安全保障是必要的。

Abstract: Black-box safety evaluation of AI systems assumes model behavior on test distributions reliably predicts deployment performance. We formalize and challenge this assumption through latent context-conditioned policies -- models whose outputs depend on unobserved internal variables that are rare under evaluation but prevalent under deployment. We establish fundamental limits showing that no black-box evaluator can reliably estimate deployment risk for such models. (1) Passive evaluation: For evaluators sampling i.i.d. from D_eval, we prove minimax lower bounds via Le Cam's method: any estimator incurs expected absolute error >= (5/24)*delta*L approximately 0.208*delta*L, where delta is trigger probability under deployment and L is the loss gap. (2) Adaptive evaluation: Using a hash-based trigger construction and Yao's minimax principle, worst-case error remains >= delta*L/16 even for fully adaptive querying when D_dep is supported over a sufficiently large domain; detection requires Theta(1/epsilon) queries. (3) Computational separation: Under trapdoor one-way function assumptions, deployment environments possessing privileged information can activate unsafe behaviors that any polynomial-time evaluator without the trapdoor cannot distinguish. For white-box probing, estimating deployment risk to accuracy epsilon_R requires O(1/(gamma^2 * epsilon_R^2)) samples, where gamma = alpha_0 + alpha_1 - 1 measures probe quality, and we provide explicit bias correction under probe error. Our results quantify when black-box testing is statistically underdetermined and provide explicit criteria for when additional safeguards -- architectural constraints, training-time guarantees, interpretability, and deployment monitoring -- are mathematically necessary for worst-case safety assurance.

</details>


### [92] [Conv-FinRe: A Conversational and Longitudinal Benchmark for Utility-Grounded Financial Recommendation](https://arxiv.org/abs/2602.16990)
*Yan Wang,Yi Han,Lingfei Qian,Yueru He,Xueqing Peng,Dongji Feng,Zhuohan Xie,Vincent Jim Zhang,Rosie Guo,Fengran Mo,Jimin Huang,Yankai Chen,Xue Liu,Jian-Yun Nie*

Main category: cs.AI

TL;DR: Conv-FinRe 是一个用于评估量化投资建议质量的基准测试，通过模拟用户访谈、逐步市场上下文和咨询对话来评估模型的投资组合排名能力。


<details>
  <summary>Details</summary>
Motivation: 当前推荐基准主要评估模型模仿用户行为的能力，但在金融咨询中，由于市场波动可能导致行为噪声和短期视角，无法真实反映用户的长期目标。因此，引入 Conv-FinRe，旨在基于风险偏好评估投资建议模型的质量，而不是简单的行为匹配。

Method: Conv-FinRe 通过两个步骤实现：首先，建立一个包含真实市场数据和人类决策轨迹的基准测试集；其次，设计实际的投资咨询对话场景，同时提供描述性行为和规范性效用的参考。

Result: 实验结果表明，重视效用排名的模型难以匹配用户行为，而行为匹配的模型可能过度拟合短期波动。

Conclusion: Conv-FinRe 提供了一种新方法来评估 AI 在金融咨询服务中的表现，强调了理性决策和行为一致性之间的矛盾。

Abstract: Most recommendation benchmarks evaluate how well a model imitates user behavior. In financial advisory, however, observed actions can be noisy or short-sighted under market volatility and may conflict with a user's long-term goals. Treating what users chose as the sole ground truth, therefore, conflates behavioral imitation with decision quality. We introduce Conv-FinRe, a conversational and longitudinal benchmark for stock recommendation that evaluates LLMs beyond behavior matching. Given an onboarding interview, step-wise market context, and advisory dialogues, models must generate rankings over a fixed investment horizon. Crucially, Conv-FinRe provides multi-view references that distinguish descriptive behavior from normative utility grounded in investor-specific risk preferences, enabling diagnosis of whether an LLM follows rational analysis, mimics user noise, or is driven by market momentum. We build the benchmark from real market data and human decision trajectories, instantiate controlled advisory conversations, and evaluate a suite of state-of-the-art LLMs. Results reveal a persistent tension between rational decision quality and behavioral alignment: models that perform well on utility-based ranking often fail to match user choices, whereas behaviorally aligned models can overfit short-term noise. The dataset is publicly released on Hugging Face, and the codebase is available on GitHub.

</details>


### [93] [Sonar-TS: Search-Then-Verify Natural Language Querying for Time Series Databases](https://arxiv.org/abs/2602.17001)
*Zhao Tan,Yiji Zhao,Shiyu Wang,Chang Xu,Yuxuan Liang,Xiping Liu,Shirui Pan,Ming Jin*

Main category: cs.AI

TL;DR: 本文提出了一种名为Sonar-TS的神经语义框架，用于解决非专家用户对大规模时间序列记录的自然语言查询。该框架通过Search-Then-Verify流程利用SQL索引及生成的Python程序进行候选窗口的选择和验证，专门应对时间序列数据库中的形态意图和超长历史记录挑战。此外，还构建了NLQTSBench基准测试以评估此类查询。实验结果表明Sonar-TS能够有效处理传统方法无法处理的复杂时间查询。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到SQL方法无法处理连续的形态意图，而时间序列模型难以处理超长历史记录。Sonar-TS旨在解决上述问题，提供一种新的框架以满足非专家用户对大规模时间序列数据库的查询需求。

Method: Sonar-TS框架采用Search-Then-Verify流程，首先通过SQL索引选择候选窗口，然后使用生成的Python程序在原始信号上进行验证。通过这种方式，可以实现对连续形态意图的有效处理。

Result: 实验结果显示，Sonar-TS框架在处理复杂的时间查询方面表现优于现有的方法。

Conclusion: 本文提出了首个系统性研究自然语言查询4时间序列数据库(NLQ4TSDB)的框架，提供了一个通用框架及评估标准，以促进这一领域的未来研究。

Abstract: Natural Language Querying for Time Series Databases (NLQ4TSDB) aims to assist non-expert users retrieve meaningful events, intervals, and summaries from massive temporal records. However, existing Text-to-SQL methods are not designed for continuous morphological intents such as shapes or anomalies, while time series models struggle to handle ultra-long histories. To address these challenges, we propose Sonar-TS, a neuro-symbolic framework that tackles NLQ4TSDB via a Search-Then-Verify pipeline. Analogous to active sonar, it utilizes a feature index to ping candidate windows via SQL, followed by generated Python programs to lock on and verify candidates against raw signals. To enable effective evaluation, we introduce NLQTSBench, the first large-scale benchmark designed for NLQ over TSDB-scale histories. Our experiments highlight the unique challenges within this domain and demonstrate that Sonar-TS effectively navigates complex temporal queries where traditional methods fail. This work presents the first systematic study of NLQ4TSDB, offering a general framework and evaluation standard to facilitate future research.

</details>


### [94] [Cinder: A fast and fair matchmaking system](https://arxiv.org/abs/2602.17015)
*Saurav Pal*

Main category: cs.AI

TL;DR: Cinder 是一个两阶段的匹配系统，通过快速筛选和精准公平性评估，实现快速和公平的匹配。


<details>
  <summary>Details</summary>
Motivation: 传统的基于平均技能评分的匹配方式在技能分布广泛时容易导致不平衡的对局。Cinder 旨在通过改进的匹配方法解决这一问题。

Method: Cinder 包含两个阶段：首先使用 Ruzicka 相似性索引快速筛选“非离群值”的技能范围；其次将玩家等级映射到非线性的技能桶，使用倒置的正态分布进行更精细的分组。然后通过 Kantorovich 距离量化潜在匹配的公平性，生成“制裁分”。

Result: 通过 1.4 亿次模拟对局的制裁分分布分析，Cinder 验证了其有效性和公平性。

Conclusion: Cinder 提供了一种快速而有效的匹配方案，适用于现代多人在线游戏，能提高玩家的留存率和满意度。

Abstract: A fair and fast matchmaking system is an important component of modern multiplayer online games, directly impacting player retention and satisfaction. However, creating fair matches between lobbies (pre-made teams) of heterogeneous skill levels presents a significant challenge. Matching based simply on average team skill metrics, such as mean or median rating or rank, often results in unbalanced and one-sided games, particularly when skill distributions are wide or skewed. This paper introduces Cinder, a two-stage matchmaking system designed to provide fast and fair matches. Cinder first employs a rapid preliminary filter by comparing the "non-outlier" skill range of lobbies using the Ruzicka similarity index. Lobbies that pass this initial check are then evaluated using a more precise fairness metric. This second stage involves mapping player ranks to a non-linear set of skill buckets, generated from an inverted normal distribution, to provide higher granularity at average skill levels. The fairness of a potential match is then quantified using the Kantorovich distance on the lobbies' sorted bucket indices, producing a "Sanction Score." We demonstrate the system's viability by analyzing the distribution of Sanction Scores from 140 million simulated lobby pairings, providing a robust foundation for fair matchmaking thresholds.

</details>


### [95] [M2F: Automated Formalization of Mathematical Literature at Scale](https://arxiv.org/abs/2602.17016)
*Zichen Wang,Wanli Ma,Zhenyu Ming,Gong Zhang,Kun Yuan,Zaiwen Wen*

Main category: cs.AI

TL;DR: M2F是一种创新的自动化数学形式化框架，能够在精益编译器的持续验证下，将大量的数学教材和论文自动化地转化为可验证的形式声明和证明。


<details>
  <summary>Details</summary>
Motivation: 当前数学形式化的自动化工具多用于孤立的定理或短代码片段的验证，但对大型教材或论文的支持有限，这是因为无法有效处理跨文件依赖性和确保整个项目的自洽性。

Method: M2F框架分为两阶段运行：首先是声明编译阶段，它将文档拆分为原子块，通过推断依赖性对其进行排序，修复声明骨架，以确保项目能够编译。其次是证明修复阶段，针对固定的签名使用目标调节的局部编辑来封闭这些缺口。在整个过程中，M2F都会让验证器保持在循环中，只在工具链反馈确认改进时才提交编辑。

Result: M2F能够在大约三周内将479页的实分析和凸分析教材转化为拥有153,853行代码的精益库，实现完整的数学证明。此外，M2F在FATE-H基准测试中的证明成功率为96%，相比基准，证明成功率提高了16个百分点。

Conclusion: M2F显著推进了数学文献的大规模自动化形式化的可行性，展示了在合理时间内实现现有水平手工验证工作量的形式化的可能性。

Abstract: Automated formalization of mathematics enables mechanical verification but remains limited to isolated theorems and short snippets. Scaling to textbooks and research papers is largely unaddressed, as it requires managing cross-file dependencies, resolving imports, and ensuring that entire projects compile end-to-end. We present M2F (Math-to-Formal), the first agentic framework for end-to-end, project-scale autoformalization in Lean. The framework operates in two stages. The statement compilation stage splits the document into atomic blocks, orders them via inferred dependencies, and repairs declaration skeletons until the project compiles, allowing placeholders in proofs. The proof repair stage closes these holes under fixed signatures using goal-conditioned local edits. Throughout both stages, M2F keeps the verifier in the loop, committing edits only when toolchain feedback confirms improvement. In approximately three weeks, M2F converts long-form mathematical sources into a project-scale Lean library of 153,853 lines from 479 pages textbooks on real analysis and convex analysis, fully formalized as Lean declarations with accompanying proofs. This represents textbook-scale formalization at a pace that would typically require months or years of expert effort. On FATE-H, we achieve $96\%$ proof success (vs.\ $80\%$ for a strong baseline). Together, these results demonstrate that practical, large-scale automated formalization of mathematical literature is within reach. The full generated Lean code from our runs is available at https://github.com/optsuite/ReasBook.git.

</details>


### [96] [Sales Research Agent and Sales Research Bench](https://arxiv.org/abs/2602.17017)
*Deepanjan Bhol*

Main category: cs.AI

TL;DR: 该研究介绍了Microsoft Dynamics 365 Sales中的Sales Research Agent，这是一种连接实时CRM数据的AI系统。为了使AI的质量可观察性更明显，研究引入了Sales Research Bench基准测试，从多个维度评估AI系统的质量。


<details>
  <summary>Details</summary>
Motivation: 当前市场上大多数AI模型未能提供透明且可重复的质量证据，CRM决策支持功能面临挑战。

Method: 研究设计了Sales Research Agent，它连接实时CRM数据，并通过文本和图表输出生成决策支持信息。此外，研究还创建了Sales Research Bench基准测试，从8个客户优先级维度评估系统的质量。

Result: 研究在特定企业数据集上的基准测试表明，Sales Research Agent在100点的综合评分中分别比Claude Sonnet 4.5和ChatGPT-5高出13分和24.1分。

Conclusion: Sales Research Bench为客户提供了一种可重复的方法来比较AI解决方案的质量，同时也展示了Sales Research Agent在质量和透明度方面的优越性。

Abstract: Enterprises increasingly need AI systems that can answer sales-leader questions over live, customized CRM data, but most available models do not expose transparent, repeatable evidence of quality. This paper describes the Sales Research Agent in Microsoft Dynamics 365 Sales, an AI-first application that connects to live CRM and related data, reasons over complex schemas, and produces decision-ready insights through text and chart outputs. To make quality observable, we introduce the Sales Research Bench, a purpose-built benchmark that scores systems on eight customer-weighted dimensions, including text and chart groundedness, relevance, explainability, schema accuracy, and chart quality. In a 200-question run on a customized enterprise schema on October 19, 2025, the Sales Research Agent outperformed Claude Sonnet 4.5 by 13 points and ChatGPT-5 by 24.1 points on the 100-point composite score, giving customers a repeatable way to compare AI solutions.

</details>


### [97] [RFEval: Benchmarking Reasoning Faithfulness under Counterfactual Reasoning Intervention in Large Reasoning Models](https://arxiv.org/abs/2602.17053)
*Yunseok Han,Yejoon Lee,Jaeyoung Do*

Main category: cs.AI

TL;DR: 该论文提出了一个关于推理忠实性的正式框架，并通过RFEval基准测试评估了大型推理模型的性能，结果显示这些模型在49.7%的情况下表现出不忠实，主要原因是立场不一致性。此外，准确性并不能很好地反映推理的结构完整性。


<details>
  <summary>Details</summary>
Motivation: 基于大型推理模型（LRMs）虽然表现出色但常产生听起来合理但实际上未能反映其真正决策过程的推理，这削弱了系统的可靠性和可信度，该研究致力于定义推理忠实性的正式框架，以提高模型的透明度和可靠性。

Method: 作者提出了一种RFEval基准，包含7,186个实例，涵盖七个任务，通过控制输出层面的反事实干预来验证忠实性。此外，通过对十二种开源LRMs进行评估，揭示了不忠实主要源于立场不一致，并分析了这些现象在具体的微观和训练后程序中的表现。

Result: 研究发现，有49.7%的输出不忠实，主要由于立场不一致性。这些不忠实现象集中在如数学和代码等易碎、多变的领域，并指出添加当前基于强化学习风格的目标后，即便准确度保持不变，也可能降低推理的忠实性。值得注意的是，准确度既不能充分也不能可靠地替代忠实性。

Conclusion: 该研究提出了一种严谨的方法来审计LRMs的可靠性，强调了在追求正确结果的同时，也必须优化推理过程的结构完整性。研究证明了在开发可信赖的人工智能时，仅仅优化算法的准确性是不够的，也需要注重模型推理过程的可靠性和透明性。

Abstract: Large Reasoning Models (LRMs) exhibit strong performance, yet often produce rationales that sound plausible but fail to reflect their true decision process, undermining reliability and trust. We introduce a formal framework for reasoning faithfulness, defined by two testable conditions: stance consistency (a coherent stance linking reasoning to answer) and causal influence (the stated reasoning causally drives the answer under output-level interventions), explicitly decoupled from accuracy. To operationalize this, we present RFEval, a benchmark of 7,186 instances across seven tasks that probes faithfulness via controlled, output-level counterfactual interventions. Evaluating twelve open-source LRMs, we find unfaithfulness in 49.7% of outputs, predominantly from stance inconsistency. Failures are concentrated in brittle, convergent domains such as math and code, and correlate more with post-training regimes than with scale: within-family ablations indicate that adding current RL-style objectives on top of supervised fine-tuning can reduce reasoning faithfulness, even when accuracy is maintained. Crucially, accuracy is neither a sufficient nor a reliable proxy for faithfulness: once controlling for model and task, the accuracy-faithfulness link is weak and statistically insignificant. Our work establishes a rigorous methodology for auditing LRM reliability and shows that trustworthy AI requires optimizing not only for correct outcomes but also for the structural integrity of the reasoning process. Our code and dataset can be found at project page: $\href{https://aidaslab.github.io/RFEval/}{https://aidaslab.github.io/RFEval/}$

</details>


### [98] [Retaining Suboptimal Actions to Follow Shifting Optima in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.17062)
*Yonghyeon Jo,Sunwoo Lee,Seungyul Han*

Main category: cs.AI

TL;DR: 本文提出了S2Q算法，该算法通过学习多个子价值函数来保留高价值备选行动，结合Softmax策略促进了持久探索。实验表明，S2Q在多个MARL基准测试中表现优异，具有更好的适应性和整体性能。


<details>
  <summary>Details</summary>
Motivation: 现有的MARL方法过于依赖单个最优行动，在价值函数训练期间发生变化时，容易收敛到次优策略。

Method: S2Q算法通过学习多个子价值函数，引入这些子价值函数到Softmax行为策略中，以促进持续探索并使$Q^{	ext{tot}}$快速适应变化的最优值。

Result: S2Q算法在多个增强学习游戏测试中表现出色，相比其他MARL算法有显著提升，在适应性与总体性能上更为出色。

Conclusion: S2Q算法提供了一种有效的解决方案，能够提高在MARL领域的适应性和性能。

Abstract: Value decomposition is a core approach for cooperative multi-agent reinforcement learning (MARL). However, existing methods still rely on a single optimal action and struggle to adapt when the underlying value function shifts during training, often converging to suboptimal policies. To address this limitation, we propose Successive Sub-value Q-learning (S2Q), which learns multiple sub-value functions to retain alternative high-value actions. Incorporating these sub-value functions into a Softmax-based behavior policy, S2Q encourages persistent exploration and enables $Q^{\text{tot}}$ to adjust quickly to the changing optima. Experiments on challenging MARL benchmarks confirm that S2Q consistently outperforms various MARL algorithms, demonstrating improved adaptability and overall performance. Our code is available at https://github.com/hyeon1996/S2Q.

</details>


### [99] [Predictive Batch Scheduling: Accelerating Language Model Training Through Loss-Aware Sample Prioritization](https://arxiv.org/abs/2602.17066)
*Sumedh Rasal*

Main category: cs.AI

TL;DR: 本文提出了一种称为Predictive Batch Scheduling (PBS)的新颖训练优化技术，该技术通过在批处理构建中动态优先处理高损失样本来加速语言模型的收敛。实验表明PBS可以在无需预定义难度指标或昂贵的逐样本损失跟踪的情况下，仅使用四个简单的特征实现6-13%的训练速度提升。


<details>
  <summary>Details</summary>
Motivation: 当前的训练优化技术如预排序学习和硬样本挖掘，要求额外的开销进行逐样本损失跟踪或预定义难度指标。本文旨在减少这些开销，同时使模型训练更高效。

Method: PBS使用一种轻量级的线性预测器，该预测器在线上训练并通过静态标记级别特征来估计样本难度。该算法只使用四个简单的特征：标记频率、序列长度、词汇多样性以及罕见标记比例，来构建批处理。

Result: 在拥有130M参数的Transformer模型实验中，PBS实现了6-13%的训练速度提升。而且，预测器的相关性随着时间推移而提升，从初始的0.14提升到10,000步骤后的0.44。

Conclusion: 证明了标记频率统计可以作为衡量样本难度的有效指标，这使得PBS能够在几乎不增加计算成本的情况下实现有效的课程学习策略。

Abstract: We introduce Predictive Batch Scheduling (PBS), a novel training optimization technique that accelerates language model convergence by dynamically prioritizing high-loss samples during batch construction. Unlike curriculum learning approaches that require predefined difficulty metrics or hard example mining methods that demand expensive per-sample loss tracking, PBS employs a lightweight linear predictor trained online to estimate sample difficulty from static token-level features. Our predictor achieves 0.44 correlation with actual loss using only four simple features: token frequency, sequence length, vocabulary diversity, and rare token ratio. Experiments on a 130M parameter transformer demonstrate that PBS achieves 6-13\% faster convergence measured by evaluation loss across training checkpoints, with the predictor's correlation improving from 0.14 to 0.44 over 10,000 training steps. These results validate that token frequency statistics encode meaningful information about sample difficulty, enabling effective curriculum learning with negligible computational overhead.

</details>


### [100] [Toward Trustworthy Evaluation of Sustainability Rating Methodologies: A Human-AI Collaborative Framework for Benchmark Dataset Construction](https://arxiv.org/abs/2602.17106)
*Xiaoran Cai,Wang Yang,Xiyu Ren,Chekun Law,Rohit Sharma,Peng Qi*

Main category: cs.AI

TL;DR: 本文提出了一种通用的人工智能协作框架，旨在生成可信赖的标准数据集，评估可持续性评级方法，提升评级的一致性、可信度和适用性。


<details>
  <summary>Details</summary>
Motivation: 可持续性评级机构使用的评级结果存在较大差异，影响了评级的可比性和对决策的支持作用，因此需要一种人机协作框架来生成标准数据集，提高评级的一致性和质量。

Method: 该框架包含两个部分：STRIDE提供了一种原则性的标准和评分系统，采用大型语言模型构建公司层面的标准数据集；SR-Delta是一种差异分析程序框架，发现潜在的调整点。两者共同支持可持续性评级方法的评估。

Result: 该框架能够实现可持续性评级方法的可扩展且可比性的评估，提出了一种改进可持续性评级的方法。

Conclusion: 呼吁更广泛的AI社区采用基于AI的方法，增强和推进支持紧迫的可持续性议程的可持续性评级方法。

Abstract: Sustainability or ESG rating agencies use company disclosures and external data to produce scores or ratings that assess the environmental, social, and governance performance of a company. However, sustainability ratings across agencies for a single company vary widely, limiting their comparability, credibility, and relevance to decision-making. To harmonize the rating results, we propose adopting a universal human-AI collaboration framework to generate trustworthy benchmark datasets for evaluating sustainability rating methodologies. The framework comprises two complementary parts: STRIDE (Sustainability Trust Rating & Integrity Data Equation) provides principled criteria and a scoring system that guide the construction of firm-level benchmark datasets using large language models (LLMs), and SR-Delta, a discrepancy-analysis procedural framework that surfaces insights for potential adjustments. The framework enables scalable and comparable assessment of sustainability rating methodologies. We call on the broader AI community to adopt AI-powered approaches to strengthen and advance sustainability rating methodologies that support and enforce urgent sustainability agendas.

</details>


### [101] [Owen-based Semantics and Hierarchy-Aware Explanation (O-Shap)](https://arxiv.org/abs/2602.17107)
*Xiangyu Zhou,Chenhan Xiao,Yang Weng*

Main category: cs.AI

TL;DR: 本文介绍了Owen值在视觉任务中的应用，提出了一种新的分组方法以提高特征分组的一致性和解释性，实验表明该方法在精度、语义一致性和计算效率方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 由于像素特征在视觉任务中具有强烈的空间和语义依赖性，传统的Shapley值方法在处理这些依赖性时出现了问题。因此，文章提出了O-Shap方法来改进分组策略。

Method: 该方法采用了一种新的分组策略（$T$-一致性），该策略能够满足语义一致性，并为分层结构提供计算上可裁剪的方式。

Result: 实验结果表明，O-Shap在图像和表格数据集中的精度、语义一致性和运行时效率方面均优于现有方法，特别是在结构性信息很重要的情况下。

Conclusion: O-Shap方法通过改进分组策略提高了特征重要性的解释性，并在多种场景下展示了更好的性能。

Abstract: Shapley value-based methods have become foundational in explainable artificial intelligence (XAI), offering theoretically grounded feature attributions through cooperative game theory. However, in practice, particularly in vision tasks, the assumption of feature independence breaks down, as features (i.e., pixels) often exhibit strong spatial and semantic dependencies. To address this, modern SHAP implementations now include the Owen value, a hierarchical generalization of the Shapley value that supports group attributions. While the Owen value preserves the foundations of Shapley values, its effectiveness critically depends on how feature groups are defined. We show that commonly used segmentations (e.g., axis-aligned or SLIC) violate key consistency properties, and propose a new segmentation approach that satisfies the $T$-property to ensure semantic alignment across hierarchy levels. This hierarchy enables computational pruning while improving attribution accuracy and interpretability. Experiments on image and tabular datasets demonstrate that O-Shap outperforms baseline SHAP variants in attribution precision, semantic coherence, and runtime efficiency, especially when structure matters.

</details>


### [102] [Instructor-Aligned Knowledge Graphs for Personalized Learning](https://arxiv.org/abs/2602.17111)
*Abdulrahman AlRabah,Priyanka Kargupta,Jiawei Han,Abdussalam Alawini*

Main category: cs.AI

TL;DR: InstructKG 自动构建课程的知识图谱，捕捉课程意图的学习进展，通过整合教学材料中的丰富时间和语义信号与大型语言模型的普适性，实现个性化学习干预。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法充分捕捉教育概念之间的潜在关系，尤其是大规模课程中的概念依赖关系，导致无法识别学生具体的知识盲点和进行个性化干预。

Method: InstructKG 通过分析课程的讲义材料，提取关键概念作为节点，通过大型语言模型推断学习依赖关系将其作为有向边，捕捉课程意图的学习路径。

Result: InstructKG 成功展示了其在多个课程的真实讲义材料上的能力，能够捕获完善的、与教师意图一致的学习路径。

Conclusion: InstructKG 提供了一种自动化的方法来创建对齐教师目标的知识图谱，支持个性化学习干预和针对性的教学辅助。

Abstract: Mastering educational concepts requires understanding both their prerequisites (e.g., recursion before merge sort) and sub-concepts (e.g., merge sort as part of sorting algorithms). Capturing these dependencies is critical for identifying students' knowledge gaps and enabling targeted intervention for personalized learning. This is especially challenging in large-scale courses, where instructors cannot feasibly diagnose individual misunderstanding or determine which concepts need reinforcement. While knowledge graphs offer a natural representation for capturing these conceptual relationships at scale, existing approaches are either surface-level (focusing on course-level concepts like "Algorithms" or logistical relationships such as course enrollment), or disregard the rich pedagogical signals embedded in instructional materials. We propose InstructKG, a framework for automatically constructing instructor-aligned knowledge graphs that capture a course's intended learning progression. Given a course's lecture materials (slides, notes, etc.), InstructKG extracts significant concepts as nodes and infers learning dependencies as directed edges (e.g., "part-of" or "depends-on" relationships). The framework synergizes the rich temporal and semantic signals unique to educational materials (e.g., "recursion" is taught before "mergesort"; "recursion" is mentioned in the definition of "merge sort") with the generalizability of large language models. Through experiments on real-world, diverse lecture materials across multiple courses and human-based evaluation, we demonstrate that InstructKG captures rich, instructor-aligned learning progressions.

</details>


### [103] [Efficient Parallel Algorithm for Decomposing Hard CircuitSAT Instances](https://arxiv.org/abs/2602.17130)
*Victor Kondratiev,Irina Gribanova,Alexander Semenov*

Main category: cs.AI

TL;DR: 提出了一种并行算法，用于分解难解的CircuitSAT实例，通过专项约束将其原始SAT实例分割成一系列较弱的公式。


<details>
  <summary>Details</summary>
Motivation: 开发一种高效并行算法来处理难解的CircuitSAT实例，以提高解题效率。

Method: 设计并实现了包含参数化的并行算法，通过调整参数以并行计算硬度估计，进而指导高效地识别高质量分解。

Result: 算法在多种难解的CircuitSAT实例上展示了良好的实用性，包括逻辑等价检验和加密哈希函数的预像攻击。

Conclusion: 该研究展示了并行算法在处理复杂CircuitSAT实例上的优势，并指出了未来可能的应用领域。

Abstract: We propose a novel parallel algorithm for decomposing hard CircuitSAT instances. The technique employs specialized constraints to partition an original SAT instance into a family of weakened formulas. Our approach is implemented as a parameterized parallel algorithm, where adjusting the parameters allows efficient identification of high-quality decompositions, guided by hardness estimations computed in parallel. We demonstrate the algorithm's practical efficacy on challenging CircuitSAT instances, including those encoding Logical Equivalence Checking of Boolean circuits and preimage attacks on cryptographic hash functions.

</details>


### [104] [JEPA-DNA: Grounding Genomic Foundation Models through Joint-Embedding Predictive Architectures](https://arxiv.org/abs/2602.17162)
*Ariel Larey,Elay Dahan,Amit Bleiweiss,Raizy Kellerman,Guy Leib,Omri Nayshool,Dan Ofer,Tal Zinger,Dan Dominissini,Gideon Rechavi,Nicole Bussola,Simon Lee,Shane O'Connell,Dung Hoang,Marissa Wirth,Alexander W. Charney,Nati Daniel,Yoli Shavit*

Main category: cs.AI

TL;DR: JEPA-DNA通过结合JEPA预测架构与传统的生成目标，为基因组学提供了一种新的预训练框架。它通过预测高阶功能嵌入而不是关注单个碱基，增强了基因组片段的表征能力，从而在多种基因组基准测试中提高了预测任务的效果。


<details>
  <summary>Details</summary>
Motivation: 当前的基因组框架模型(GFMs)主要依赖于掩码语言建模(MLM)或下一个标记预测(NTP)，这些方法擅长捕捉基因组的局部语法和细粒度的模式，但往往忽略了更广泛的功能性上下文，导致缺乏全面生物视角的表示。

Method: JEPA-DNA引入了一种新的预训练框架，结合了JEPA预测架构与传统的生成目标，并通过监督CLS标记使模型同时关注标签级恢复和潜在空间中的预测对象，从而预测基因组片段的高层次功能嵌入。

Result: 在一系列基因组基准测试中，JEPA-DNA在监督和零样本任务上的表现优于只生成的基线。这种新的路径提供了一种理解不仅仅基因组字母，还包括序列中潜在功能逻辑的模型。

Conclusion: JEPA-DNA提供了基因组框架模型的一个改进框架，可以帮助构建更全面、生物学导向的基因组模型，用于进一步的研究和应用。

Abstract: Genomic Foundation Models (GFMs) have largely relied on Masked Language Modeling (MLM) or Next Token Prediction (NTP) to learn the language of life. While these paradigms excel at capturing local genomic syntax and fine-grained motif patterns, they often fail to capture the broader functional context, resulting in representations that lack a global biological perspective. We introduce JEPA-DNA, a novel pre-training framework that integrates the Joint-Embedding Predictive Architecture (JEPA) with traditional generative objectives. JEPA-DNA introduces latent grounding by coupling token-level recovery with a predictive objective in the latent space by supervising a CLS token. This forces the model to predict the high-level functional embeddings of masked genomic segments rather than focusing solely on individual nucleotides. JEPA-DNA extends both NTP and MLM paradigms and can be deployed either as a standalone from-scratch objective or as a continual pre-training enhancement for existing GFMs. Our evaluations across a diverse suite of genomic benchmarks demonstrate that JEPA-DNA consistently yields superior performance in supervised and zero-shot tasks compared to generative-only baselines. By providing a more robust and biologically grounded representation, JEPA-DNA offers a scalable path toward foundation models that understand not only the genomic alphabet, but also the underlying functional logic of the sequence.

</details>


### [105] [Texo: Formula Recognition within 20M Parameters](https://arxiv.org/abs/2602.17189)
*Sicheng Mao*

Main category: cs.AI

TL;DR: Texo是一个参数仅2000万的小而高效的公式的识别模型，其性能接近于先进的模型如UniMERNet-T和PPFormulaNet-S，在模型大小上分别减小了80%和65%，适于实时推理并在轻量级设备上部署。


<details>
  <summary>Details</summary>
Motivation: 提出Texo的主要动机是为了在减少模型复杂性和尺寸的同时，保持与现有先进模型相当的识别性能，从而实现高性能计算和轻量级设备上的实时推断。

Method: 通过精心设计、知识蒸馏和迁移学习，Texo实现了与当前顶级模型卓越的性能相当的效果，同时大大减小了模型参数量和大小。

Result: Texo模型在性能上与当前顶级模型接近，但在模型尺寸上减少了80%和65%，实现了在消费者级硬件上的实时推断，并且可以部署在浏览器中。

Conclusion: Texo模型的成功开发证明了即使在较小的模型规模下，依然可以获得高性能的公式识别结果，对此领域的发展具有重要意义。

Abstract: In this paper we present Texo, a minimalist yet highperformance formula recognition model that contains only 20 million parameters. By attentive design, distillation and transfer of the vocabulary and the tokenizer, Texo achieves comparable performance to state-of-the-art models such as UniMERNet-T and PPFormulaNet-S, while reducing the model size by 80% and 65%, respectively. This enables real-time inference on consumer-grade hardware and even in-browser deployment. We also developed a web application to demonstrate the model capabilities and facilitate its usage for end users.

</details>


### [106] [Decoding the Human Factor: High Fidelity Behavioral Prediction for Strategic Foresight](https://arxiv.org/abs/2602.17222)
*Ben Yellin,Ehud Ezra,Mark Foreman,Shula Grinapol*

Main category: cs.AI

TL;DR: 本文介绍了一种大型行为模型（LBM），通过条件概率融合稳定特质、动机状态和情景限制，来预测个体在不同战略性难题中的具体行为。LBM在保持高准确性的基础上，比基础模型和基于提示的方法表现更好，并且其性能随着提供的特质维度增加而持续提升。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在预测人类决策时存在难以保持一致性和个体特定行为的问题，尤其是在复杂的心理特质与情境限制之间存在复杂交互的情况下，基于提示的方法效果不佳。因此，本文提出了一种大型行为模型（LBM），旨在更准确地预测个体在高压力环境中的行为决策。

Method: LBM 通过结合一个结构化和高维的心理特质谱，不再依赖短暂的对话提示，而是基于全面的心理测量问卷来进行行为嵌入。LBM 使用一个专有数据集进行训练，该数据集将稳定的倾向、动机状态和情境限制与观察到的选择行为联系起来。此外，LBM 的性能随着提供的心理特质维度数量的增加而提高。

Result: 在一项独立于训练的场景评估中，LBM 细分调优后相对于未适应的 Llama-3.1-8B-Instruct 基准表现出更好的行为预测。当条件于五大人格特质时，LBM 的表现与最先进的基准相当。而且，LBM 会在收到更多心理特质维度的数据时持续表现出性能提升。

Conclusion: LBM 确立为一种可扩展的方法，用以实现高保真度的行为模拟，适用于战略预测、谈判分析、认知安全以及决策支持等应用领域。

Abstract: Predicting human decision-making in high-stakes environments remains a central challenge for artificial intelligence. While large language models (LLMs) demonstrate strong general reasoning, they often struggle to generate consistent, individual-specific behavior, particularly when accurate prediction depends on complex interactions between psychological traits and situational constraints. Prompting-based approaches can be brittle in this setting, exhibiting identity drift and limited ability to leverage increasingly detailed persona descriptions. To address these limitations, we introduce the Large Behavioral Model (LBM), a behavioral foundation model fine-tuned to predict individual strategic choices with high fidelity. LBM shifts from transient persona prompting to behavioral embedding by conditioning on a structured, high-dimensional trait profile derived from a comprehensive psychometric battery. Trained on a proprietary dataset linking stable dispositions, motivational states, and situational constraints to observed choices, LBM learns to map rich psychological profiles to discrete actions across diverse strategic dilemmas. In a held-out scenario evaluation, LBM fine-tuning improves behavioral prediction relative to the unadapted Llama-3.1-8B-Instruct backbone and performs comparably to frontier baselines when conditioned on Big Five traits. Moreover, we find that while prompting-based baselines exhibit a complexity ceiling, LBM continues to benefit from increasingly dense trait profiles, with performance improving as additional trait dimensions are provided. Together, these results establish LBM as a scalable approach for high-fidelity behavioral simulation, enabling applications in strategic foresight, negotiation analysis, cognitive security, and decision support.

</details>


### [107] [Mechanistic Interpretability of Cognitive Complexity in LLMs via Linear Probing using Bloom's Taxonomy](https://arxiv.org/abs/2602.17229)
*Bianca Raimondi,Maurizio Gabbrielli*

Main category: cs.AI

TL;DR: 研究通过使用布鲁姆分类法作为层级视角，分析大语言模型的内部神经表示，发现认知层次可以在模型表示的线性子空间中被线性分类器区分，表明模型在前向传递早期就能解决认知难度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的黑盒性质需要新的评估框架来超越表面级性能指标，该研究旨在通过布鲁姆分类法探究大语言模型内部认知复杂性。

Method: 研究通过分析来自不同大语言模型的高维激活向量，使用布鲁姆分类法作为层级视角，探究不同认知水平是否可以在模型的残差流中线性区分。

Result: 研究结果表明，线性分类器在所有布鲁姆水平上的平均准确率接近95%，这表明认知水平在模型表示的线性子空间中可以被编码。

Conclusion: 研究结论是大语言模型能在前向传递的早期就解决认知难度，随着层数的增加，表示变得越来越可分。

Abstract: The black-box nature of Large Language Models necessitates novel evaluation frameworks that transcend surface-level performance metrics. This study investigates the internal neural representations of cognitive complexity using Bloom's Taxonomy as a hierarchical lens. By analyzing high-dimensional activation vectors from different LLMs, we probe whether different cognitive levels, ranging from basic recall (Remember) to abstract synthesis (Create), are linearly separable within the model's residual streams. Our results demonstrate that linear classifiers achieve approximately 95% mean accuracy across all Bloom levels, providing strong evidence that cognitive level is encoded in a linearly accessible subspace of the model's representations. These findings provide evidence that the model resolves the cognitive difficulty of a prompt early in the forward pass, with representations becoming increasingly separable across layers.

</details>


### [108] [All Leaks Count, Some Count More: Interpretable Temporal Contamination Detection in LLM Backtesting](https://arxiv.org/abs/2602.17234)
*Zeyu Zhang,Ryan Chen,Bradly C. Stadie*

Main category: cs.AI

TL;DR: 该论文提出了一种以声明级别为基础的框架来检测和量化LLMs中的时间知识泄露，并进一步提出了TimeSPEC方法以减少泄露，同时保持任务性能。


<details>
  <summary>Details</summary>
Motivation: 为了准确评估LLMs对未来事件的预测能力，需要在已解决的事件上进行回测，以确保模型只能基于指定过去日期的信息来进行推理。但是，模型可能意外地包含了训练期间编码的后截断知识，影响了评估的可靠性。因此，论文提出了一个用于检测并量化这种时间知识泄露的声明级别框架，并展示了该方法的有效性。

Method: 该方法首先将模型推理拆分为原子声明，并按时间验证性对其进行分类。然后，利用Shapley值来衡量每个声明对预测的贡献，从而计算出Shapley-加权决策泄露率（Shapley-DCLR），这是一个可解释的指标，能够反映驱动决策的推理中有多少部分来自泄露的信息。进一步地，提出了TimeSPEC方法，该方法通过生成与验证声明的交错过程来主动过滤掉时间上的污染，确保每个支持声明都可以追溯到指向在截断日期之前可用的信息来源。

Result: 在针对350个案例的实验中，包括美国最高法院案件预测、NBA薪水估计和股票回报排名等多个场景，标准提示基线模型显示出明显的泄露。引入TimeSPEC方法后，Shapley-DCLR得到显著降低，同时任务性能保持不变。这一结果表明，显式的声明级验证比基于提示的时间性约束更能提供可靠的回测。

Conclusion: 该研究提出的时间知识泄露检测和量化框架以及TimeSPEC方法为评估和改进LLM的预测能力提供了一种有效的方法，提高了回测的可靠性和可解释性。

Abstract: To evaluate whether LLMs can accurately predict future events, we need the ability to \textit{backtest} them on events that have already resolved. This requires models to reason only with information available at a specified past date. Yet LLMs may inadvertently leak post-cutoff knowledge encoded during training, undermining the validity of retrospective evaluation. We introduce a claim-level framework for detecting and quantifying this \emph{temporal knowledge leakage}. Our approach decomposes model rationales into atomic claims and categorizes them by temporal verifiability, then applies \textit{Shapley values} to measure each claim's contribution to the prediction. This yields the \textbf{Shapley}-weighted \textbf{D}ecision-\textbf{C}ritical \textbf{L}eakage \textbf{R}ate (\textbf{Shapley-DCLR}), an interpretable metric that captures what fraction of decision-driving reasoning derives from leaked information. Building on this framework, we propose \textbf{Time}-\textbf{S}upervised \textbf{P}rediction with \textbf{E}xtracted \textbf{C}laims (\textbf{TimeSPEC}), which interleaves generation with claim verification and regeneration to proactively filter temporal contamination -- producing predictions where every supporting claim can be traced to sources available before the cutoff date. Experiments on 350 instances spanning U.S. Supreme Court case prediction, NBA salary estimation, and stock return ranking reveal substantial leakage in standard prompting baselines. TimeSPEC reduces Shapley-DCLR while preserving task performance, demonstrating that explicit, interpretable claim-level verification outperforms prompt-based temporal constraints for reliable backtesting.

</details>


### [109] [ArXiv-to-Model: A Practical Study of Scientific LM Training](https://arxiv.org/abs/2602.17288)
*Anuj Gupta*

Main category: cs.AI

TL;DR: 本文详细描述了如何从原始arXiv LaTeX源直接训练一个1.36B参数的科学语言模型，覆盖了元数据过滤、存档验证、LaTeX提取、文本规范化、领域感知分词和在受限计算资源下进行密集Transformer训练的过程。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型展示了强大的推理和数学能力，但构建领域专门化的科学语言模型过程尚未获得详细记录，因此本文提供了一个从零开始训练小型科学语言模型的详细工程基础案例研究，旨在为研究者提供支持。

Method: 该研究通过24次实验运行，分析了训练稳定性和缩放行为，数据产出损失以及基础设施瓶颈。实验涵盖了元数据过滤、存档验证、LaTeX提取、文本规范化、领域感知分词和在受限计算资源下进行密集Transformer训练等步骤。

Result: 研究展示了在数据丰富条件下（52B预训练标记）的稳定训练行为，并指出了预处理决策对可用标记数量、分词对符号稳定性的影响以及存储和I/O约束作为限制因素的重要性。

Conclusion: 尽管本文未提出新型架构，但它提供了一个在受限计算预算下的小型科学语言模型训练的透明工程案例研究，并希望这些见解能支持希望构建领域专门化模型的研究人员。

Abstract: While frontier large language models demonstrate strong reasoning and mathematical capabilities, the practical process of training domain-specialized scientific language models from raw sources remains under-documented. In this work, we present a detailed case study of training a 1.36B-parameter scientific language model directly from raw arXiv LaTeX sources spanning mathematics, computer science, and theoretical physics. We describe an end-to-end pipeline covering metadata filtering, archive validation, LaTeX extraction, text normalization, domain-aware tokenization, and dense transformer training under constrained compute (2xA100 GPUs). Through 24 experimental runs, we analyze training stability, scaling behavior, data yield losses, and infrastructure bottlenecks. Our findings highlight how preprocessing decisions significantly affect usable token volume, how tokenization impacts symbolic stability, and how storage and I/O constraints can rival compute as limiting factors. We further analyze convergence dynamics and show stable training behavior in a data-rich regime (52B pretraining tokens). Rather than proposing a novel architecture, this work provides an engineering-grounded, transparent account of training a small scientific language model from scratch. We hope these insights support researchers operating under moderate compute budgets who seek to build domain-specialized models.

</details>


### [110] [Evaluating Chain-of-Thought Reasoning through Reusability and Verifiability](https://arxiv.org/abs/2602.17544)
*Shashank Aggarwal,Ram Vikas Mishra,Amit Awekar*

Main category: cs.AI

TL;DR: 论文提出通过引入重用性和可验证性两个新度量，来评估多智能体信息检索管道中Chain-of-Thought的推理过程质量，发现这些新度量与传统的准确性无关。


<details>
  <summary>Details</summary>
Motivation: 当前的CoT评估仅关注任务准确性，但未能评估推理过程的质量或实用性。为了解决此问题，作者引入了重用性和可验证性作为新的评估指标。

Method: 通过将CoT生成和执行解耦，使用Thinker-Executor框架进行评估；对四个Thinker模型与十个Executor模型进行评估，覆盖五个基准。

Result: 评估结果显示重用性和可验证性与标准准确性无相关性，揭示了当前依赖准确性的排行榜在推理能力评估中的盲点。同时，发现专门用于推理的模型产生的CoTs并不比通用语言模型更具重用性和可验证性。

Conclusion: 本文强调了引入新型评价指标的重要性，表明评估多智能体信息检索管道中的推理过程需要考虑其质量而非仅仅关注准确性。

Abstract: In multi-agent IR pipelines for tasks such as search and ranking, LLM-based agents exchange intermediate reasoning in terms of Chain-of-Thought (CoT) with each other. Current CoT evaluation narrowly focuses on target task accuracy. However, this metric fails to assess the quality or utility of the reasoning process itself. To address this limitation, we introduce two novel measures: reusability and verifiability. We decouple CoT generation from execution using a Thinker-Executor framework. Reusability measures how easily an Executor can reuse the Thinker's CoT. Verifiability measures how frequently an Executor can match the Thinker's answer using the CoT. We evaluated four Thinker models against a committee of ten Executor models across five benchmarks. Our results reveal that reusability and verifiability do not correlate with standard accuracy, exposing a blind spot in current accuracy-based leaderboards for reasoning capability. Surprisingly, we find that CoTs from specialized reasoning models are not consistently more reusable or verifiable than those from general-purpose LLMs like Llama and Gemma.

</details>


### [111] [MedClarify: An information-seeking AI agent for medical diagnosis with case-specific follow-up questions](https://arxiv.org/abs/2602.17308)
*Hui Min Wong,Philip Heesen,Pascal Janetzky,Martin Bendszus,Stefan Feuerriegel*

Main category: cs.AI

TL;DR: 研究介绍了一个名为MedClarify的AI助手，能够生成随访问题以进行迭代推理，提高诊断决策的准确性。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在医疗推理方面存在局限性，往往会导致诊断不确定性增加。

Method: MedClarify通过计算候选诊断并主动生成减少诊断不确定性的随访问题，从信息增益最高的问题开始进行推理。

Result: 研究结果显示，与初始单次推理的大型语言模型基线相比，MedClarify能够将诊断错误率降低约27个百分点。

Conclusion: MedClarify为改善医疗AI通过主动信息寻求提供了路径，促进了反映医疗推理的迭代和不确定性的有效对话。

Abstract: Large language models (LLMs) are increasingly used for diagnostic tasks in medicine. In clinical practice, the correct diagnosis can rarely be immediately inferred from the initial patient presentation alone. Rather, reaching a diagnosis often involves systematic history taking, during which clinicians reason over multiple potential conditions through iterative questioning to resolve uncertainty. This process requires considering differential diagnoses and actively excluding emergencies that demand immediate intervention. Yet, the ability of medical LLMs to generate informative follow-up questions and thus reason over differential diagnoses remains underexplored. Here, we introduce MedClarify, an AI agent for information-seeking that can generate follow-up questions for iterative reasoning to support diagnostic decision-making. Specifically, MedClarify computes a list of candidate diagnoses analogous to a differential diagnosis, and then proactively generates follow-up questions aimed at reducing diagnostic uncertainty. By selecting the question with the highest expected information gain, MedClarify enables targeted, uncertainty-aware reasoning to improve diagnostic performance. In our experiments, we first demonstrate the limitations of current LLMs in medical reasoning, which often yield multiple, similarly likely diagnoses, especially when patient cases are incomplete or relevant information for diagnosis is missing. We then show that our information-theoretic reasoning approach can generate effective follow-up questioning and thereby reduces diagnostic errors by ~27 percentage points (p.p.) compared to a standard single-shot LLM baseline. Altogether, MedClarify offers a path to improve medical LLMs through agentic information-seeking and to thus promote effective dialogues with medical LLMs that reflect the iterative and uncertain nature of real-world clinical reasoning.

</details>


### [112] [CLEF HIPE-2026: Evaluating Accurate and Efficient Person-Place Relation Extraction from Multilingual Historical Texts](https://arxiv.org/abs/2602.17663)
*Juri Opitz,Corina Raclé,Emanuela Boros,Andrianos Michail,Matteo Romanello,Maud Ehrmann,Simon Clematide*

Main category: cs.AI

TL;DR: HIPE-2026 是一个 CLEF 评价实验室，专注于从嘈杂的多语言历史文本中提取人物-地名关系，通过分类 'at' 和 'isAt' 两种类型的关系，旨在评估准确性、计算效率和领域泛化能力。


<details>
  <summary>Details</summary>
Motivation: 建立在 HIPE-2020 和 HIPE-2022 的基础上，实验室将历史性文本的关系提取任务扩展至多种语言和历史时期，通过将关系提取与大规模历史数据处理联系起来，支持知识图谱构建、历史人物重建和数字人文领域的空间分析。

Method: 方法涉及两种关系分类：'at' 和 'isAt'，要求系统利用时空线索对这些关系进行分类。评估采取三方面的综合评估：准确性、计算效率和领域泛化。

Result: 该研究通过多语言、多历史时期的数据提高了人物-地名关系识别的准确性，并展现了计算效率和领域泛化的潜力，有助于构建知识图谱、历史人物重建和数字人文领域的应用。

Conclusion: 结论认为 HIPE-2026 对人物-地名关系的识别做出了重要贡献，并为进一步研究提供了坚实的基础。

Abstract: HIPE-2026 is a CLEF evaluation lab dedicated to person-place relation extraction from noisy, multilingual historical texts. Building on the HIPE-2020 and HIPE-2022 campaigns, it extends the series toward semantic relation extraction by targeting the task of identifying person--place associations in multiple languages and time periods. Systems are asked to classify relations of two types - $at$ ("Has the person ever been at this place?") and $isAt$ ("Is the person located at this place around publication time?") - requiring reasoning over temporal and geographical cues. The lab introduces a three-fold evaluation profile that jointly assesses accuracy, computational efficiency, and domain generalization. By linking relation extraction to large-scale historical data processing, HIPE-2026 aims to support downstream applications in knowledge-graph construction, historical biography reconstruction, and spatial analysis in digital humanities.

</details>


### [113] [Dataless Weight Disentanglement in Task Arithmetic via Kronecker-Factored Approximate Curvature](https://arxiv.org/abs/2602.17385)
*Angelo Porrello,Pietro Buzzega,Felix Dangel,Thomas Sommariva,Riccardo Salami,Lorenzo Bonicelli,Simone Calderara*

Main category: cs.AI

TL;DR: 该研究提出了一种无需数据的方法，通过将正则化问题视为曲率矩阵近似问题来减少任务间干扰，从而提高基础模型在任务添加和否定中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在解决任务间干扰时需要额外的任务数据，这与模块化设计和可用数据的限制（如隐私要求）不兼容。

Method: 该方法通过将正则化问题转化为曲率矩阵的近似问题，利用Kronecker-Factored Approximate Curvature技术，开发了一种无数据需求的实用正则化器。

Result: 该方法在任务添加和否定任务上达到了最先进的效果，并且具有处理任务向量缩放的鲁棒性。

Conclusion: 研究展示了该方法的有效性和实用性，为基于非数据的方法提供了一种新的视角。

Abstract: Task Arithmetic yields a modular, scalable way to adapt foundation models. Combining multiple task vectors, however, can lead to cross-task interference, causing representation drift and degraded performance. Representation drift regularization provides a natural remedy to disentangle task vectors; however, existing approaches typically require external task data, conflicting with modularity and data availability constraints (e.g., privacy requirements). We propose a dataless approach by framing regularization against representation drift as a curvature matrix approximation problem. This allows us to leverage well-established techniques; in particular, we adopt Kronecker-Factored Approximate Curvature and obtain a practical regularizer that achieves state-of-the-art results in task addition and negation. Our method has constant complexity in the number of tasks and promotes robustness to task vector rescaling, eliminating the need for held-out tuning.

</details>


### [114] [Visual Model Checking: Graph-Based Inference of Visual Routines for Image Retrieval](https://arxiv.org/abs/2602.17386)
*Adrià Molina,Oriol Ramos Terrades,Josep Lladós*

Main category: cs.AI

TL;DR: 本文提出了一种新的框架，将形式验证与基于图的方法和神经代码生成相结合，应用于基于深度学习的图像检索。该框架旨在支持开放词汇的自然语言查询，并产生可信赖和可验证的结果。


<details>
  <summary>Details</summary>
Motivation: 当前的自然语言图像检索框架在处理具有复杂关系、对象组合或精确约束（如身份、数量和比例）的查询时存在局限性，未能提供可靠的检索结果。因此，研究如何通过形式验证增强检索结果的准确性和透明度。

Method: 该方法结合图基验证方法和神经代码生成，将形式验证集成到基于深度学习的图像检索中。通过这种方法，系统能够直接验证用户的查询中的每个原子真理是否在检索结果中得到满足。

Result: 实验结果表明，该框架可以提供更为准确和透明的检索结果，不仅能够返回匹配的结果，还能识别并标记哪些具体约束条件得到了满足以及哪些条件未得到满足。

Conclusion: 本文提出的方法能够显著提高基于嵌入式模型的嵌入式方法的效果，并为用户提供一个更清晰、可验证的检索过程。

Abstract: Information retrieval lies at the foundation of the modern digital industry. While natural language search has seen dramatic progress in recent years largely driven by embedding-based models and large-scale pretraining, the field still faces significant challenges. Specifically, queries that involve complex relationships, object compositions, or precise constraints such as identities, counts and proportions often remain unresolved or unreliable within current frameworks. In this paper, we propose a novel framework that integrates formal verification into deep learning-based image retrieval through a synergistic combination of graph-based verification methods and neural code generation. Our approach aims to support open-vocabulary natural language queries while producing results that are both trustworthy and verifiable. By grounding retrieval results in a system of formal reasoning, we move beyond the ambiguity and approximation that often characterize vector representations. Instead of accepting uncertainty as a given, our framework explicitly verifies each atomic truth in the user query against the retrieved content. This allows us to not only return matching results, but also to identify and mark which specific constraints are satisfied and which remain unmet, thereby offering a more transparent and accountable retrieval process while boosting the results of the most popular embedding-based approaches.

</details>


### [115] [A Contrastive Variational AutoEncoder for NSCLC Survival Prediction with Missing Modalities](https://arxiv.org/abs/2602.17402)
*Michele Zanitti,Vanja Miskovic,Francesco Trovò,Alessandra Laura Giulia Pedrocchi,Ming Shen,Yan Kyaw Tun,Arsela Prelaj,Sokol Kosta*

Main category: cs.AI

TL;DR: 本文提出了一种多模态对比变分自编码器（MCVAE）来解决癌症患者生存结果预测时模态缺失的问题，通过模态特异性变分编码器捕捉各数据来源的不确定性，并引入融合瓶颈和学习门控机制来平衡现有模态的贡献，同时使用一个多任务目标函数组合损失来正则化患者表示，并增强跨模态在潜在空间中的对齐。


<details>
  <summary>Details</summary>
Motivation: 现有的先进模型在遇到严重缺失值时缺乏鲁棒性，而提出的方法旨在解决这种情况，提高预测的准确性和健壮性。

Method: 提出了一种多模态对比变分自编码器（MCVAE），包括模态特异性变分编码器、融合瓶颈、学习门控机制、多任务目标函数以及模态随机遮蔽等技术来处理数据缺失问题。

Result: 在TCGA-LUAD和TCGA-LUSC两个数据集上进行的评估表明，该方法在特定生存期预测方面具有优于现有方法的鲁棒性。

Conclusion: 实验证明MCVAE可以在存在严重数据缺失的情况下对NSCLC患者的特定生存期进行准确预测。

Abstract: Predicting survival outcomes for non-small cell lung cancer (NSCLC) patients is challenging due to the different individual prognostic features. This task can benefit from the integration of whole-slide images, bulk transcriptomics, and DNA methylation, which offer complementary views of the patient's condition at diagnosis. However, real-world clinical datasets are often incomplete, with entire modalities missing for a significant fraction of patients. State-of-the-art models rely on available data to create patient-level representations or use generative models to infer missing modalities, but they lack robustness in cases of severe missingness. We propose a Multimodal Contrastive Variational AutoEncoder (MCVAE) to address this issue: modality-specific variational encoders capture the uncertainty in each data source, and a fusion bottleneck with learned gating mechanisms is introduced to normalize the contributions from present modalities. We propose a multi-task objective that combines survival loss and reconstruction loss to regularize patient representations, along with a cross-modal contrastive loss that enforces cross-modal alignment in the latent space. During training, we apply stochastic modality masking to improve the robustness to arbitrary missingness patterns. Extensive evaluations on the TCGA-LUAD (n=475) and TCGA-LUSC (n=446) datasets demonstrate the efficacy of our approach in predicting disease-specific survival (DSS) and its robustness to severe missingness scenarios compared to two state-of-the-art models. Finally, we bring some clarifications on multimodal integration by testing our model on all subsets of modalities, finding that integration is not always beneficial to the task.

</details>


### [116] [A Privacy by Design Framework for Large Language Model-Based Applications for Children](https://arxiv.org/abs/2602.17418)
*Diana Addae,Diana Rogachova,Nafiseh Kahani,Masoud Barati,Michael Christensen,Chen Zhou*

Main category: cs.AI

TL;DR: 本文提出了一种基于隐私设计原则的框架，旨在指导设计师和开发者在大型语言模型（LLM）应用中采取前瞻性和风险管理方法，以减少儿童使用技术时的隐私风险，同时符合法律要求。


<details>
  <summary>Details</summary>
Motivation: 随着儿童越来越依赖由人工智能（AI）驱动的技术，隐私风险成为一个重要问题。现有的隐私法规虽然要求公司在实践中实施保护措施，但往往难以落实。本文提出了一种基于隐私设计方法的框架，通过将隐私保护原则应用于AI服务的各个阶段，旨在解决这一挑战。

Method: 该框架借鉴了几项国际隐私法规的原则，包括欧盟的《通用数据保护条例》（GDPR）、加拿大的《个人信息保护和电子文档法》（PIPEDA）和美国的《儿童在线隐私保护法》（COPPA），并将其与应用开发中的数据收集、模型训练、运营监控和持续验证阶段对齐。此外，框架还基于儿童权利公约（UNCRC）、英国《适当设计代码》（AADC）和其他学术研究，为儿童设计提出了具体建议。

Result: 该框架帮助AI服务提供商和开发者在设计阶段采取措施，减少隐私泄露，同时遵守法律法规。此外，通过一个关于为13岁以下儿童设计的基于LLM的教育辅导案例，展示了该框架的实际应用。

Conclusion: 研究表明，通过采用数据保护策略和技术、组织控制，以及进行儿童适当设计的决策，可以在整个LLM生命周期中支持适合儿童的AI应用的发展，确保隐私保护并符合法律要求。

Abstract: Children are increasingly using technologies powered by Artificial Intelligence (AI). However, there are growing concerns about privacy risks, particularly for children. Although existing privacy regulations require companies and organizations to implement protections, doing so can be challenging in practice. To address this challenge, this article proposes a framework based on Privacy-by-Design (PbD), which guides designers and developers to take on a proactive and risk-averse approach to technology design. Our framework includes principles from several privacy regulations, such as the General Data Protection Regulation (GDPR) from the European Union, the Personal Information Protection and Electronic Documents Act (PIPEDA) from Canada, and the Children's Online Privacy Protection Act (COPPA) from the United States. We map these principles to various stages of applications that use Large Language Models (LLMs), including data collection, model training, operational monitoring, and ongoing validation. For each stage, we discuss the operational controls found in the recent academic literature to help AI service providers and developers reduce privacy risks while meeting legal standards. In addition, the framework includes design guidelines for children, drawing from the United Nations Convention on the Rights of the Child (UNCRC), the UK's Age-Appropriate Design Code (AADC), and recent academic research. To demonstrate how this framework can be applied in practice, we present a case study of an LLM-based educational tutor for children under 13. Through our analysis and the case study, we show that by using data protection strategies such as technical and organizational controls and making age-appropriate design decisions throughout the LLM life cycle, we can support the development of AI applications for children that provide privacy protections and comply with legal requirements.

</details>


### [117] [WarpRec: Unifying Academic Rigor and Industrial Scale for Responsible, Reproducible, and Efficient Recommendation](https://arxiv.org/abs/2602.17442)
*Marco Avolio,Potito Aghilar,Sabino Roccotelli,Vito Walter Anelli,Chiara Mallamaci,Vincenzo Paparella,Marco Valentini,Alejandro Bellogín,Michelantonio Trizio,Joseph Trotta,Antonio Ferrara,Tommaso Di Noia*

Main category: cs.AI

TL;DR: WarpRec 提供了一种高性能框架，旨在通过实现生态友好型的架构解决推荐系统研究中的困境，同时包含多种算法和策略，支持从本地到分布式环境的无缝过渡，并推动推荐系统向能够生成交互式的未来生态转变。


<details>
  <summary>Details</summary>
Motivation: 论文探讨了推荐系统研究中由于现有生态系统限制导致的研究者在进行内存中实验与复杂重写需求之间的权衡问题，WarpRec 致力于弥合这种差距。

Method: WarpRec 通过引入后端无关的架构，包括多种算法、度量和策略，实现了高性能。它还集成 CodeCarbon 实现实时能耗跟踪，确保了系统的可扩展性和生态友好性。

Result: WarpRec 支持从本地到分布式-training 和优化的无缝转换，通过提供 50 多个顶级算法、40 多种度量和 19 种过滤和分割策略，显著提高了推荐系统的性能和灵活性。

Conclusion: WarpRec 作为下一代可持续、智能准备的推荐系统架构的基石，不仅连接了学术界和工业界，还促进了从静态排名引擎向交互式工具的转变，为推广生成型 AI 生态系统中的推荐系统奠定了基础。

Abstract: Innovation in Recommender Systems is currently impeded by a fractured ecosystem, where researchers must choose between the ease of in-memory experimentation and the costly, complex rewriting required for distributed industrial engines. To bridge this gap, we present WarpRec, a high-performance framework that eliminates this trade-off through a novel, backend-agnostic architecture. It includes 50+ state-of-the-art algorithms, 40 metrics, and 19 filtering and splitting strategies that seamlessly transition from local execution to distributed training and optimization. The framework enforces ecological responsibility by integrating CodeCarbon for real-time energy tracking, showing that scalability need not come at the cost of scientific integrity or sustainability. Furthermore, WarpRec anticipates the shift toward Agentic AI, leading Recommender Systems to evolve from static ranking engines into interactive tools within the Generative AI ecosystem. In summary, WarpRec not only bridges the gap between academia and industry but also can serve as the architectural backbone for the next generation of sustainable, agent-ready Recommender Systems. Code is available at https://github.com/sisinflab/warprec/

</details>


### [118] [Pareto Optimal Benchmarking of AI Models on ARM Cortex Processors for Sustainable Embedded Systems](https://arxiv.org/abs/2602.17508)
*Pranay Jain,Maximilian Kasper,Göran Köber,Axel Plinge,Dominik Seuß*

Main category: cs.AI

TL;DR: 本文提出了一种针对ARM Cortex处理器的AI模型优化基准框架，评估了能耗、准确性和资源利用情况，通过自动测试平台，确认了FLOPs与推理时间的几乎线性关系，并运用Pareto分析展示了能量消耗与模型精度之间的权衡，指出M7处理器适合短推理周期，M4处理器在长推理任务中具有更好的能效。


<details>
  <summary>Details</summary>
Motivation: 针对嵌入式系统中的ARM Cortex处理器（M0+, M4, M7），研究AI模型的优化问题，特别是在能效、准确性和资源利用上的平衡。

Method: 设计了一个自动化的测试平台，系统地评估关键性能指标（KPIs），使用FLOPs和推理时间的相关性来估算计算需求，并通过Pareto分析平衡能量消耗和模型精度的权衡。

Result: 确认了FLOPs与推理时间之间的几乎线性关系，展示了如何平衡能量消耗和模型精度之间的权衡，指出不同处理器（M0+, M4, M7）在不同任务中的适用性。

Conclusion: 研究为开发者提供了设计能源效率高的AI系统指导，确保在各种实际应用中实现高性能。

Abstract: This work presents a practical benchmarking framework for optimizing artificial intelligence (AI) models on ARM Cortex processors (M0+, M4, M7), focusing on energy efficiency, accuracy, and resource utilization in embedded systems. Through the design of an automated test bench, we provide a systematic approach to evaluate across key performance indicators (KPIs) and identify optimal combinations of processor and AI model. The research highlights a nearlinear correlation between floating-point operations (FLOPs) and inference time, offering a reliable metric for estimating computational demands. Using Pareto analysis, we demonstrate how to balance trade-offs between energy consumption and model accuracy, ensuring that AI applications meet performance requirements without compromising sustainability. Key findings indicate that the M7 processor is ideal for short inference cycles, while the M4 processor offers better energy efficiency for longer inference tasks. The M0+ processor, while less efficient for complex AI models, remains suitable for simpler tasks. This work provides insights for developers, guiding them to design energy-efficient AI systems that deliver high performance in realworld applications.

</details>


### [119] [Enhancing Large Language Models (LLMs) for Telecom using Dynamic Knowledge Graphs and Explainable Retrieval-Augmented Generation](https://arxiv.org/abs/2602.17529)
*Dun Yuan,Hao Zhou,Xue Liu,Hao Chen,Yan Xin,Jianzhong,Zhang*

Main category: cs.AI

TL;DR: 本文提出了一种名为KG-RAG的新框架，将知识图谱（KGs）与检索增强生成（RAG）结合，以增强针对电信特定任务的大语言模型（LLMs）。实验结果表明，即使在复杂的电信场景下，KG-RAG也能比传统的LLMs和RAG模型更准确、可靠且可解释。


<details>
  <summary>Details</summary>
Motivation: 在电信领域，大语言模型的应用面临复杂性高、标准不断演进和专业术语众多的挑战，导致通用型LLMs难以生成准确和可靠的输出，增加了幻觉并降低了电信操作的实用性。

Method: KG-RAG框架通过结合知识图谱提供电信领域的结构化知识表示以及检索增强生成技术动态检索相关事实来支撑模型输出，从而提升事实准确性，减少幻觉并确保符合电信标准。

Result: 与其他基线模型相比，KG-RAG在基准数据集上的实验结果显示了更优的表现，与RAG和LLM-only模型相比，准确率平均提升了14.3%和21.6%，并产生了准确、可靠且可解释的输出。

Conclusion: KG-RAG框架在电信场景中的优越性能表明了高度定制化的模型在处理复杂专业领域问题时的潜力。

Abstract: Large language models (LLMs) have shown strong potential across a variety of tasks, but their application in the telecom field remains challenging due to domain complexity, evolving standards, and specialized terminology. Therefore, general-domain LLMs may struggle to provide accurate and reliable outputs in this context, leading to increased hallucinations and reduced utility in telecom operations.To address these limitations, this work introduces KG-RAG-a novel framework that integrates knowledge graphs (KGs) with retrieval-augmented generation (RAG) to enhance LLMs for telecom-specific tasks. In particular, the KG provides a structured representation of domain knowledge derived from telecom standards and technical documents, while RAG enables dynamic retrieval of relevant facts to ground the model's outputs. Such a combination improves factual accuracy, reduces hallucination, and ensures compliance with telecom specifications.Experimental results across benchmark datasets demonstrate that KG-RAG outperforms both LLM-only and standard RAG baselines, e.g., KG-RAG achieves an average accuracy improvement of 14.3% over RAG and 21.6% over LLM-only models. These results highlight KG-RAG's effectiveness in producing accurate, reliable, and explainable outputs in complex telecom scenarios.

</details>


### [120] [ODESteer: A Unified ODE-Based Steering Framework for LLM Alignment](https://arxiv.org/abs/2602.17560)
*Hongjue Zhao,Haosen Sun,Jiangtao Kong,Xiaochang Li,Qineng Wang,Liwei Jiang,Qi Zhu,Tarek Abdelzaher,Yejin Choi,Manling Li,Huajie Shao*

Main category: cs.AI

TL;DR: 本文提出了一种基于常微分方程（ODE）的统一理论框架——ODESteer，该框架通过屏障函数识别引导方向，从而在大型语言模型（LLM）对齐方面取得了一致的改进。


<details>
  <summary>Details</summary>
Motivation: 目前的激活引导方法缺乏统一的理论框架来设计引导方向，并且过多依赖于一步引导，无法捕捉复杂的激活分布模式。

Method: 提出了一种基于ODE的统一理论框架，将传统的激活添加解释为ODE的解的一阶近似，并设计了引导方向的屏障函数来实现多步和自适应的激活引导。

Result: 该方法在LLM对齐基准测试中表现出显著的改进，与当前最先进的激活引导方法相比，实现了5.7%、2.5%和2.4%的提升。

Conclusion: 通过ODESteer，建立了激活引导在LLM对齐中的新理论观点，并通过提出的方法验证了这一理论的有效性。

Abstract: Activation steering, or representation engineering, offers a lightweight approach to align large language models (LLMs) by manipulating their internal activations at inference time. However, current methods suffer from two key limitations: \textit{(i)} the lack of a unified theoretical framework for guiding the design of steering directions, and \textit{(ii)} an over-reliance on \textit{one-step steering} that fail to capture complex patterns of activation distributions. In this work, we propose a unified ordinary differential equations (ODEs)-based \textit{theoretical} framework for activation steering in LLM alignment. We show that conventional activation addition can be interpreted as a first-order approximation to the solution of an ODE. Based on this ODE perspective, identifying a steering direction becomes equivalent to designing a \textit{barrier function} from control theory. Derived from this framework, we introduce ODESteer, a kind of ODE-based steering guided by barrier functions, which shows \textit{empirical} advancement in LLM alignment. ODESteer identifies steering directions by defining the barrier function as the log-density ratio between positive and negative activations, and employs it to construct an ODE for \textit{multi-step and adaptive} steering. Compared to state-of-the-art activation steering methods, ODESteer achieves consistent empirical improvements on diverse LLM alignment benchmarks, a notable $5.7\%$ improvement over TruthfulQA, $2.5\%$ over UltraFeedback, and $2.4\%$ over RealToxicityPrompts. Our work establishes a principled new view of activation steering in LLM alignment by unifying its theoretical foundations via ODEs, and validating it empirically through the proposed ODESteer method.

</details>


### [121] [A Hybrid Federated Learning Based Ensemble Approach for Lung Disease Diagnosis Leveraging Fusion of SWIN Transformer and CNN](https://arxiv.org/abs/2602.17566)
*Asif Hasan Chowdhury,Md. Fahim Islam,M Ragib Anjum Riad,Faiyaz Bin Hashem,Md Tanzim Reza,Md. Golam Rabiul Alam*

Main category: cs.AI

TL;DR: 本文提出了一种结合Federated Learning和先进CNN与Transformer模型的混合方法，用于基于X光报告诊断COVID-19和肺炎，并强调了该方法在及时连续学习下的精度提升和模型安全性。


<details>
  <summary>Details</summary>
Motivation: 鉴于近年来计算能力的显著提高，人工智能在医疗健康领域的应用潜力巨大。鉴于医疗专家和医院可以分享数据空间，并利用人工智能和联邦学习技术，提出了安全且分布的数据处理系统，以提高疾病诊断和病情预测的准确性。

Method: 该实验基于Federated Learning，采用先进的CNN模型（DenseNet201、Inception V3、VGG19）和Transformer模型SWIN Transformer进行组合，开发了一种混合模型。

Result: 该混合模型能够在实际的持续学习过程中提高疾病诊断和病情严重程度预测的准确性，并通过整合Federated Learning确保了模型的安全性和信息的真实性和完整性。

Conclusion: 该研究证明了基于Federated Learning的混合AI模型在医疗领域的实际应用效果，为提高患病者的诊断和治疗效率提供了可靠的方法和支持。

Abstract: The significant advancements in computational power cre- ate a vast opportunity for using Artificial Intelligence in different ap- plications of healthcare and medical science. A Hybrid FL-Enabled Ensemble Approach For Lung Disease Diagnosis Leveraging a Combination of SWIN Transformer and CNN is the combination of cutting-edge technology of AI and Federated Learning. Since, medi- cal specialists and hospitals will have shared data space, based on that data, with the help of Artificial Intelligence and integration of federated learning, we can introduce a secure and distributed system for medical data processing and create an efficient and reliable system. The proposed hybrid model enables the detection of COVID-19 and Pneumonia based on x-ray reports. We will use advanced and the latest available tech- nology offered by Tensorflow and Keras along with Microsoft-developed Vision Transformer, that can help to fight against the pandemic that the world has to fight together as a united. We focused on using the latest available CNN models (DenseNet201, Inception V3, VGG 19) and the Transformer model SWIN Transformer in order to prepare our hy- brid model that can provide a reliable solution as a helping hand for the physician in the medical field. In this research, we will discuss how the Federated learning-based Hybrid AI model can improve the accuracy of disease diagnosis and severity prediction of a patient using the real-time continual learning approach and how the integration of federated learn- ing can ensure hybrid model security and keep the authenticity of the information.

</details>


### [122] [AI Gamestore: Scalable, Open-Ended Evaluation of Machine General Intelligence with Human Games](https://arxiv.org/abs/2602.17594)
*Lance Ying,Ryan Truong,Prafull Sharma,Kaiya Ivy Zhao,Nathan Cloos,Kelsey R. Allen,Thomas L. Griffiths,Katherine M. Collins,José Hernández-Orallo,Phillip Isola,Samuel J. Gershman,Joshua B. Tenenbaum*

Main category: cs.AI

TL;DR: 本文提出了一种新的评估方法，通过让AI系统玩游戏来评估其类似人类的一般智能，特别是研究他们如何以及在多大程度上能够学习并玩所有可设想的人类游戏。为此，作者引入了AI GameStore平台，该平台通过LLMs结合人类辅助自动创建和改编流行数字游戏的标准化和容器化版本，初步生成了100款游戏来评估VLMs的表现。


<details>
  <summary>Details</summary>
Motivation: 为了解决当前AI评测基准仅评估狭窄的人类活动能力和静态特性，缺乏动态适应性的问题，本文通过游戏评估AI系统的人类般一般智能，定义了“人类游戏”并提出了多宇宙人类游戏的概念。

Method: 本文通过引入AI GameStore平台，利用大型语言模型（LLMs）结合人类反馈，自动创建和调整流行数字游戏的标准化和容器化版本，并生成了一系列基于App Store和Steam热门游戏排行的100款游戏，用于评估前沿的视图-语言模型（VLMs）。

Result: 研究结果表明，前七个前沿VLMs在大多数游戏中仅达到了人类平均得分的10%左右，特别是在需要世界模型学习、记忆和计划的游戏上表现出明显的不足。

Conclusion: 本文结论认为，AI GameStore平台作为一种实用方法，有助于评估和推动机器向类似人类的一般智能方向发展。

Abstract: Rigorously evaluating machine intelligence against the broad spectrum of human general intelligence has become increasingly important and challenging in this era of rapid technological advance. Conventional AI benchmarks typically assess only narrow capabilities in a limited range of human activity. Most are also static, quickly saturating as developers explicitly or implicitly optimize for them. We propose that a more promising way to evaluate human-like general intelligence in AI systems is through a particularly strong form of general game playing: studying how and how well they play and learn to play \textbf{all conceivable human games}, in comparison to human players with the same level of experience, time, or other resources. We define a "human game" to be a game designed by humans for humans, and argue for the evaluative suitability of this space of all such games people can imagine and enjoy -- the "Multiverse of Human Games". Taking a first step towards this vision, we introduce the AI GameStore, a scalable and open-ended platform that uses LLMs with humans-in-the-loop to synthesize new representative human games, by automatically sourcing and adapting standardized and containerized variants of game environments from popular human digital gaming platforms. As a proof of concept, we generated 100 such games based on the top charts of Apple App Store and Steam, and evaluated seven frontier vision-language models (VLMs) on short episodes of play. The best models achieved less than 10\% of the human average score on the majority of the games, and especially struggled with games that challenge world-model learning, memory and planning. We conclude with a set of next steps for building out the AI GameStore as a practical way to measure and drive progress toward human-like general intelligence in machines.

</details>


### [123] [MolHIT: Advancing Molecular-Graph Generation with Hierarchical Discrete Diffusion Models](https://arxiv.org/abs/2602.17602)
*Hojung Jung,Rodrigo Hormazabal,Jaehyeong Jo,Youngrok Park,Kyunggeun Roh,Se-Young Yun,Sehui Han,Dae-Woong Jeong*

Main category: cs.AI

TL;DR: MolHIT 是一种面向分子图生成的框架，通过引入 Heterogeneous Discrete Diffusion Model 和原子编码的化学角色拆分，显著提高了分子图生成的质量，超越了现有模型和 1D 基线，特别是在 MOSES 数据集上取得了新的最佳性能。


<details>
  <summary>Details</summary>
Motivation: 为了克服现有分子图生成模型的化学有效性低和难以达到特定性质的问题，特别是与 1D 模型相比。

Method: MolHIT 基于 Heterogeneous Discrete Diffusion Model，并通过化学先验信息扩展离散扩散模型，并采用了拆分原子编码的方法，使其能够更好地模拟分子的化学属性。

Result: MolHIT 在 MOSES 数据集上达到了前所未有的化学有效性和接近完美的性能，在多个指标上超越了强大的 1D 基线。此外，还展示了在下游任务中的出色表现，如多性质引导生成和骨架扩展。

Conclusion: MolHIT 提供了一种更有效的分子图生成方法，展示了在多个任务上的优越性能，并为未来的 AI 驱动药物发现和材料科学提供了新的可能性。

Abstract: Molecular generation with diffusion models has emerged as a promising direction for AI-driven drug discovery and materials science. While graph diffusion models have been widely adopted due to the discrete nature of 2D molecular graphs, existing models suffer from low chemical validity and struggle to meet the desired properties compared to 1D modeling. In this work, we introduce MolHIT, a powerful molecular graph generation framework that overcomes long-standing performance limitations in existing methods. MolHIT is based on the Hierarchical Discrete Diffusion Model, which generalizes discrete diffusion to additional categories that encode chemical priors, and decoupled atom encoding that splits the atom types according to their chemical roles. Overall, MolHIT achieves new state-of-the-art performance on the MOSES dataset with near-perfect validity for the first time in graph diffusion, surpassing strong 1D baselines across multiple metrics. We further demonstrate strong performance in downstream tasks, including multi-property guided generation and scaffold extension.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [124] [Low-Cost IoT-Enabled Tele-ECG Monitoring for Resource-Constrained Settings: System Design and Prototype](https://arxiv.org/abs/2602.17114)
*Seemron Neupane,Aashish Ghimire*

Main category: cs.AR

TL;DR: 本文关注物联网背景下患者、医生和数据传输服务器三大角色，特别提到了远程心电监测系统在健康监测中的作用，并强调了慢性疾病早期治疗的重要性。


<details>
  <summary>Details</summary>
Motivation: 考虑到自动化机械设备的优势和全球医疗资源的不均衡分布，以及慢性疾病如CDV和心脏病的严重性和复杂性，本文探讨了通过物联网技术实现远程心电监测系统，以改善慢性疾病的诊断和治疗。

Method: 本文通过分析慢性疾病的特点和物联网技术的能力，设计了一种远程心电监测系统，并通过实证分析和案例研究展示了其潜在的应用价值。

Result: 提出了一种基于物联网的远程心电监测系统，可以实现远程监测和管理慢性病患者的心脏状况，减少医疗成本和旅行费用。

Conclusion: 该系统为慢性疾病患者提供了新的监测和管理途径，并强调了早期诊断和治疗的重要性，有助于提高慢性疾病的管理水平。

Abstract: With the availability of automation machinery and its superiority, are being slothful and inviting many diseases to invade them. The world still has so many places where people lack basic health facilities. Due to early detection and intervention, CDV can be cured to an extreme extent. It heavily reduces travel and associated costs. A remote ECG monitoring system enables community health workers to support and empower patients through telemedicine. However, there remains some financial and logistical burden. Heart disease cannot be taken lightly. These patients require regular health check-ups and the attention of health personnel in a short period if their health deteriorates suddenly and rapidly. Chronic diseases are extremely variable in their symptoms and evolution of treatment. Some, if not treated early, will end the patient's life. The trend of the INTERNET OF THINGS, IoT, is spreading massively. This paper focuses on the three main: the operator, the doctor, and the server over which the data is being sent.

</details>


### [125] [When Models Ignore Definitions: Measuring Semantic Override Hallucinations in LLM Reasoning](https://arxiv.org/abs/2602.17520)
*Yogeswar Reddy Thota,Setareh Rafatirad,Homayoun Houman,Tooraj Nikoubin*

Main category: cs.AR

TL;DR: 研究发现LLM在处理本地重新定义语义时表现不佳，存在词汇表覆盖和假设注入问题，提出了一种包含30个逻辑和数字电路推理任务的紧凑微基准测试。


<details>
  <summary>Details</summary>
Motivation: 由于LLM在面对局部重新定义的语义时表现不佳，需要设计相关评估协议来测试模型在形式领域中的局部遗忘和语义合规性。

Method: 设计了一个包含30个任务的微基准测试，旨在测试LLM在布尔代数、操作符重载、重新定义的门和电路级别语义方面的局部语义合规性。

Result: 测试结果显示，前沿的三种LLM在本地规范上表现持续欠佳，做出了自以为正确的但与规范不兼容的假设，甚至在基础设置下也忽略了约束。

Conclusion: 研究结果表明，在形式领域中LLM面临着表面正确性和规范忠实推理之间的差距，需要进一步改进以增强局部语义合规性。

Abstract: Large language models (LLMs) demonstrate strong performance on standard digital logic and Boolean reasoning tasks, yet their reliability under locally redefined semantics remains poorly understood. In many formal settings, such as circuit specifications, examinations, and hardware documentation, operators and components are explicitly redefined within narrow scope. Correct reasoning in these contexts requires models to temporarily suppress globally learned conventions in favor of prompt-local definitions. In this work, we study a systematic failure mode we term semantic override, in which an LLM reverts to its pretrained default interpretation of operators or gate behavior despite explicit redefinition in the prompt. We also identify a related class of errors, assumption injection, where models commit to unstated hardware semantics when critical details are underspecified, rather than requesting clarification. We introduce a compact micro-benchmark of 30 logic and digital-circuit reasoning tasks designed as verifier-style traps, spanning Boolean algebra, operator overloading, redefined gates, and circuit-level semantics. Evaluating three frontier LLMs, we observe persistent noncompliance with local specifications, confident but incompatible assumptions, and dropped constraints even in elementary settings. Our findings highlight a gap between surface-level correctness and specification-faithful reasoning, motivating evaluation protocols that explicitly test local unlearning and semantic compliance in formal domains.

</details>
