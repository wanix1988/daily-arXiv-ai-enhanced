<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 170]
- [cs.CL](#cs.CL) [Total: 78]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.AI](#cs.AI) [Total: 70]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Scalable spatial point process models for forensic footwear analysis](https://arxiv.org/abs/2602.07006)
*Alokesh Manna,Neil Spencer,Dipak K. Dey*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的贝叶斯层次模型，用于通过考虑鞋印的磨损模式及其位置的空间变异系数，来量化鞋印特征的稀有性，从而提高鞋印识别的准确性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 目前通过鞋印匹配嫌疑人鞋子存在着稀有性评估不够准确和可靠的问题，本文旨在通过发展新的贝叶斯层次模型来解决这些问题。

Method: 本文通过构建一个基于潜在高斯模型的贝叶斯层次模型，并采用集成内嵌拉普拉斯近似来进行高效的统计推断，同时利用空间变异系数来优化鞋印磨损模式与位置之间的关系。

Result: 实验表明，该方法在保留数据上的表现优于现有方法，提高了鞋印分析的准确性和可靠性。

Conclusion: 本文提出的新方法为提高鞋印证据的可靠性和有效性提供了有力支持，有望在实际的法医鉴定工作中得到应用。

Abstract: Shoe print evidence recovered from crime scenes plays a key role in forensic investigations. By examining shoe prints, investigators can determine details of the footwear worn by suspects. However, establishing that a suspect's shoes match the make and model of a crime scene print may not be sufficient. Typically, thousands of shoes of the same size, make, and model are manufactured, any of which could be responsible for the print. Accordingly, a popular approach used by investigators is to examine the print for signs of ``accidentals,'' i.e., cuts, scrapes, and other features that accumulate on shoe soles after purchase due to wear. While some patterns of accidentals are common on certain types of shoes, others are highly distinctive, potentially distinguishing the suspect's shoe from all others. Quantifying the rarity of a pattern is thus essential to accurately measuring the strength of forensic evidence. In this study, we address this task by developing a hierarchical Bayesian model. Our improvement over existing methods primarily stems from two advancements. First, we frame our approach in terms of a latent Gaussian model, thus enabling inference to be efficiently scaled to large collections of annotated shoe prints via integrated nested Laplace approximations. Second, we incorporate spatially varying coefficients to model the relationship between shoes' tread patterns and accidental locations. We demonstrate these improvements through superior performance on held-out data, which enhances accuracy and reliability in forensic shoe print analysis.

</details>


### [2] [MAU-GPT: Enhancing Multi-type Industrial Anomaly Understanding via Anomaly-aware and Generalist Experts Adaptation](https://arxiv.org/abs/2602.07011)
*Zhuonan Wang,Zhenxuan Fan,Siwen Tan,Yu Zhong,Yuqian Yuan,Haoyuan Li,Hao Jiang,Wenqiao Zhang,Feifei Shao,Hongwei Wang,Jun Xiao*

Main category: cs.CV

TL;DR: MAU-Set 和 MAU-GPT 提出了一个新的多类工业异常理解的数据集和模型，旨在解决现有方法在数据覆盖范围和模型泛化方面的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的图像分析方法在工业制造中存在局限性，如数据集覆盖范围有限和模型在多样和复杂的异常模式上泛化能力差，因此需要一个更全面的解决方案。

Method: MAU-Set 数据集包含多个工业领域，具有层次任务结构，涵盖了从二元分类到复杂推理的任务。MAU-GPT 是一个适应领域的多模态大模型，使用了一种名为 AMoE-LoRA 的机制，结合了异常意识和通用专家的适应，以增强在不同缺陷类别的理解和推理。

Result: 实验结果显示，MAU-GPT 在所有领域中都优于之前最先进的方法，展示了大规模和自动化工业检测的潜力。

Conclusion: 该研究提出的方法和模型为工业制造中的自动化质量控制提供了更有效的解决方案。

Abstract: As industrial manufacturing scales, automating fine-grained product image analysis has become critical for quality control. However, existing approaches are hindered by limited dataset coverage and poor model generalization across diverse and complex anomaly patterns. To address these challenges, we introduce MAU-Set, a comprehensive dataset for Multi-type industrial Anomaly Understanding. It spans multiple industrial domains and features a hierarchical task structure, ranging from binary classification to complex reasoning. Alongside this dataset, we establish a rigorous evaluation protocol to facilitate fair and comprehensive model assessment. Building upon this foundation, we further present MAU-GPT, a domain-adapted multimodal large model specifically designed for industrial anomaly understanding. It incorporates a novel AMoE-LoRA mechanism that unifies anomaly-aware and generalist experts adaptation, enhancing both understanding and reasoning across diverse defect classes. Extensive experiments show that MAU-GPT consistently outperforms prior state-of-the-art methods across all domains, demonstrating strong potential for scalable and automated industrial inspection.

</details>


### [3] [A General Model for Retinal Segmentation and Quantification](https://arxiv.org/abs/2602.07012)
*Zhonghua Wang,Lie Ju,Sijia Li,Wei Feng,Sijin Zhou,Ming Hu,Jianhao Xiong,Xiaoying Tang,Yifan Peng,Mingquan Lin,Yaodong Ding,Yong Zeng,Wenbin Wei,Li Dong,Zongyuan Ge*

Main category: cs.CV

TL;DR: RetSAM 是一种用于视网膜分割和量化的一般框架，能够处理大量视网膜图像，实现多目标分割和标准化生物标志物提取，显著提升了分割性能，并能支持大规模眼科研究。


<details>
  <summary>Details</summary>
Motivation: 当前的眼科研究受限于缺乏多标签数据集和统一的分割到量化的工作流程。RetSAM 的提出旨在解决这些问题，通过集成多阶段训练策略和广泛的数据集，提供标准化的眼科生物标志物，以促进眼科疾病的研究。

Method: RetSAM 采用了多阶段训练策略，并利用了超过 200,000 张视网膜图像进行训练。该框架能够执行多目标分割，并提取超过 30 种标准化生物标志物，涵盖了结构形态、血管几何和退化变化等方面的信息。训练包含了私有和公共数据集，以提升模型在不同人群、成像设备和临床情境下的泛化能力。

Result: RetSAM 在多个公共数据集上取得了优越的分割性能，平均提高了 3.9 个百分点的 DSC (Dice 相似度系数)，并在具有挑战性的多任务基准测试中最高提升了 15 个百分点。提取的标准化生物标志物能够支持多种眼科疾病的系统关联分析。

Conclusion: RetSAM 为眼底成像分析提供了一个强大的工具，促进了大规模的眼科研究和临床转化应用。

Abstract: Retinal imaging is fast, non-invasive, and widely available, offering quantifiable structural and vascular signals for ophthalmic and systemic health assessment. This accessibility creates an opportunity to study how quantitative retinal phenotypes relate to ocular and systemic diseases. However, such analyses remain difficult at scale due to the limited availability of public multi-label datasets and the lack of a unified segmentation-to-quantification pipeline. We present RetSAM, a general retinal segmentation and quantification framework for fundus imaging. It delivers robust multi-target segmentation and standardized biomarker extraction, supporting downstream ophthalmologic studies and oculomics correlation analyses. Trained on over 200,000 fundus images, RetSAM supports three task categories and segments five anatomical structures, four retinal phenotypic patterns, and more than 20 distinct lesion types. It converts these segmentation results into over 30 standardized biomarkers that capture structural morphology, vascular geometry, and degenerative changes. Trained with a multi-stage strategy using both private and public fundus data, RetSAM achieves superior segmentation performance on 17 public datasets. It improves on prior best methods by 3.9 percentage points in DSC on average, with up to 15 percentage points on challenging multi-task benchmarks, and generalizes well across diverse populations, imaging devices, and clinical settings. The resulting biomarkers enable systematic correlation analyses across major ophthalmic diseases, including diabetic retinopathy, age-related macular degeneration, glaucoma, and pathologic myopia. Together, RetSAM transforms fundus images into standardized, interpretable quantitative phenotypes, enabling large-scale ophthalmic research and translation.

</details>


### [4] [Steering to Say No: Configurable Refusal via Activation Steering in Vision Language Models](https://arxiv.org/abs/2602.07013)
*Jiaxi Yang,Shicheng Liu,Yuchen Yang,Dongwon Lee*

Main category: cs.CV

TL;DR: CR-VLM 通过激活导向机制、闸门机制和反事实视觉增强模块，实现了可配置的拒绝策略，有效解决了现有拒绝机制的泛化不足问题。


<details>
  <summary>Details</summary>
Motivation: 现有拒绝机制无法满足多样化的用户需求和情境约束，导致拒绝不合适的内容过少或过多；作者提出 CR-VLM 来解决这个问题。

Method: CR-VLM 包含三个组成部分：配置拒绝向量提取、闸门机制和反事实视觉增强模块，基于激活导向机制实现可配置拒绝。

Result: CR-VLM 在多个数据集和不同 VLM 上的综合实验表明，它能够实现有效的、高效的、鲁棒的配置拒绝，为 VLM 提供用户自适应的安全对齐路径。

Conclusion: CR-VLM 通过上述研究，在可配置拒绝领域取得了显著成果，未来可进一步研究其在具体应用中的表现及优化。

Abstract: With the rapid advancement of Vision Language Models (VLMs), refusal mechanisms have become a critical component for ensuring responsible and safe model behavior. However, existing refusal strategies are largely \textit{one-size-fits-all} and fail to adapt to diverse user needs and contextual constraints, leading to either under-refusal or over-refusal. In this work, we firstly explore the challenges mentioned above and develop \textbf{C}onfigurable \textbf{R}efusal in \textbf{VLM}s (\textbf{CR-VLM}), a robust and efficient approach for {\em configurable} refusal based on activation steering. CR-VLM consists of three integrated components: (1) extracting a configurable refusal vector via a teacher-forced mechanism to amplify the refusal signal; (2) introducing a gating mechanism that mitigates over-refusal by preserving acceptance for in-scope queries; and (3) designing a counterfactual vision enhancement module that aligns visual representations with refusal requirements. Comprehensive experiments across multiple datasets and various VLMs demonstrate that CR-VLM achieves effective, efficient, and robust configurable refusals, offering a scalable path toward user-adaptive safety alignment in VLMs.

</details>


### [5] [Vectra: A New Metric, Dataset, and Model for Visual Quality Assessment in E-Commerce In-Image Machine Translation](https://arxiv.org/abs/2602.07014)
*Qingyu Wu,Yuxuan Han,Haijun Li,Zhao Xu,Jianshan Zhao,Xu Jin,Longyue Wang,Weihua Luo*

Main category: cs.CV

TL;DR: 本文介绍了Vectra，一种参考自由的电子商务商品图像机器翻译的视觉质量评估框架，包含一个精细的评分系统、一个多样性的数据集以及一个强大的模型。


<details>
  <summary>Details</summary>
Motivation: 当前的评价方法在解释性和领域相关性方面存在不足，而电子商务中的商品图片翻译对于用户体验至关重要。

Method: 介绍了一个由三个部分组成的框架：评分系统，数据集和模型。评分系统细化为14个易解释的维度，并引入空间感知的缺陷区域比来减少注解的模糊性。数据集包含多样性的110万张真实产品图片以及标注信息。模型是一个参数量为4B的MLLM，不仅能生成评分，还能进行诊断分析。

Result: 实验表明，Vectra在与人类排名的相关性上达到了最先进的水平，其评分性能超过了包括GPT-5和Gemini-3在内的领先的MLLM。

Conclusion: 该框架可作为系统评估、指令调优以及一致性评估的有效工具。

Abstract: In-Image Machine Translation (IIMT) powers cross-border e-commerce product listings; existing research focuses on machine translation evaluation, while visual rendering quality is critical for user engagement. When facing context-dense product imagery and multimodal defects, current reference-based methods (e.g., SSIM, FID) lack explainability, while model-as-judge approaches lack domain-grounded, fine-grained reward signals. To bridge this gap, we introduce Vectra, to the best of our knowledge, the first reference-free, MLLM-driven visual quality assessment framework for e-commerce IIMT. Vectra comprises three components: (1) Vectra Score, a multidimensional quality metric system that decomposes visual quality into 14 interpretable dimensions, with spatially-aware Defect Area Ratio (DAR) quantification to reduce annotation ambiguity; (2) Vectra Dataset, constructed from 1.1M real-world product images via diversity-aware sampling, comprising a 2K benchmark for system evaluation, 30K reasoning-based annotations for instruction tuning, and 3.5K expert-labeled preferences for alignment and evaluation; and (3) Vectra Model, a 4B-parameter MLLM that generates both quantitative scores and diagnostic reasoning. Experiments demonstrate that Vectra achieves state-of-the-art correlation with human rankings, and our model outperforms leading MLLMs, including GPT-5 and Gemini-3, in scoring performance. The dataset and model will be released upon acceptance.

</details>


### [6] [Robust and Real-Time Bangladeshi Currency Recognition: A Dual-Stream MobileNet and EfficientNet Approach](https://arxiv.org/abs/2602.07015)
*Subreena,Mohammad Amzad Hossain,Mirza Raquib,Saydul Akbar Murad,Farida Siddiqi Prity,Muhammad Hanif,Nick Rahimi*

Main category: cs.CV

TL;DR: 该研究构建了一个新的孟加拉国货币数据集，结合了多种复杂情况，并提出了一种新型混合CNN架构，以提高识别准确性。模型在不同场景下的准确率分别为97.95%、92.84%和94.98%，并在五折交叉验证和多种评估指标中表现良好。


<details>
  <summary>Details</summary>
Motivation: 为了提高视觉障碍人士使用的助听技术中的货币识别准确性，减少诈骗和剥削风险。

Method: 研究首先构建了一个包含控制和现实场景的新孟加拉国货币数据集，然后引入了四种额外数据集以增强鲁棒性。提出了一种结合MobileNetV3-Large和EfficientNetB0的新型混合CNN架构，并使用多层感知器（MLP）进行了有效分类。还集成了可解释的人工智能方法（如LIME和SHAP）以提高透明性和可解释性。通过五折交叉验证和多种性能指标评估模型。

Result: 所提出模型在控制数据集中的准确率为97.95%，复杂背景下的准确率为92.84%，多种数据集结合情况下的准确率为94.98%。利用五折交叉验证和多种指标进行评估。

Conclusion: 研究提出的方法在提高货币识别准确性方面表现出色，并确保了模型的透明性和解释性。

Abstract: Accurate currency recognition is essential for assistive technologies, particularly for visually impaired individuals who rely on others to identify banknotes. This dependency puts them at risk of fraud and exploitation. To address these challenges, we first build a new Bangladeshi banknote dataset that includes both controlled and real-world scenarios, ensuring a more comprehensive and diverse representation. Next, to enhance the dataset's robustness, we incorporate four additional datasets, including public benchmarks, to cover various complexities and improve the model's generalization. To overcome the limitations of current recognition models, we propose a novel hybrid CNN architecture that combines MobileNetV3-Large and EfficientNetB0 for efficient feature extraction. This is followed by an effective multilayer perceptron (MLP) classifier to improve performance while keeping computational costs low, making the system suitable for resource-constrained devices. The experimental results show that the proposed model achieves 97.95% accuracy on controlled datasets, 92.84% on complex backgrounds, and 94.98% accuracy when combining all datasets. The model's performance is thoroughly evaluated using five-fold cross-validation and seven metrics: accuracy, precision, recall, F1-score, Cohen's Kappa, MCC, and AUC. Additionally, explainable AI methods like LIME and SHAP are incorporated to enhance transparency and interpretability.

</details>


### [7] [Gaussian-Constrained LeJEPA Representations for Unsupervised Scene Discovery and Pose Consistency](https://arxiv.org/abs/2602.07016)
*Mohsen Mostafa*

Main category: cs.CV

TL;DR: 该研究提出了一种使用加权高斯约束的图像嵌入方法，该方法能够改善场景识别和姿态估计的准确性，特别是在视觉上具有较大混淆性的场景。


<details>
  <summary>Details</summary>
Motivation: 针对无监督三维场景重建从无结构图像集合中的挑战，尤其是考虑到场景可能来自多个无关场景并且图像可能存在显著的视觉模糊时，研究旨在通过引入基于LeJEPA的加权高斯约束来改进场景发现和相机姿态估计。

Method: 研究团队提出了三种逐步细化的管道方法，最终采用了LeJEPA启发式的方法并施加了各向同性的高斯约束。这种方法通过对学习出的图像嵌入施加约束，增强了场景分割的一致性和姿态估计的鲁棒性。

Result: 在Image Matching Challenge 2025 (IMC2025) 的实验中，加权高斯约束的嵌入方法相比于启发式驱动的基线方法，在复杂且视觉模糊的场景下提高了场景分离和姿态估计的准确性。

Conclusion: 研究表明，理论上驱动的表示约束为将自我监督学习原则与实际的结构从运动管道相结合提供了有前景的方向。

Abstract: Unsupervised 3D scene reconstruction from unstructured image collections remains a fundamental challenge in computer vision, particularly when images originate from multiple unrelated scenes and contain significant visual ambiguity. The Image Matching Challenge 2025 (IMC2025) highlights these difficulties by requiring both scene discovery and camera pose estimation under real-world conditions, including outliers and mixed content. This paper investigates the application of Gaussian-constrained representations inspired by LeJEPA (Joint Embedding Predictive Architecture) to address these challenges. We present three progressively refined pipelines, culminating in a LeJEPA-inspired approach that enforces isotropic Gaussian constraints on learned image embeddings. Rather than introducing new theoretical guarantees, our work empirically evaluates how these constraints influence clustering consistency and pose estimation robustness in practice. Experimental results on IMC2025 demonstrate that Gaussian-constrained embeddings can improve scene separation and pose plausibility compared to heuristic-driven baselines, particularly in visually ambiguous settings. These findings suggest that theoretically motivated representation constraints offer a promising direction for bridging self-supervised learning principles and practical structure-from-motion pipelines.

</details>


### [8] [XAI-CLIP: ROI-Guided Perturbation Framework for Explainable Medical Image Segmentation in Multimodal Vision-Language Models](https://arxiv.org/abs/2602.07017)
*Thuraya Alzubaidi,Sana Ammar,Maryam Alsharqi,Islem Rekik,Muzammil Behzad*

Main category: cs.CV

TL;DR: 本文提出了一种名为XAI-CLIP的ROI导向的扰动框架，通过利用多模态的视觉-语言模型嵌入来定位临床有意义的解剖区域，并指导解释过程。该方法通过整合语言导向的区域定位与医学图像分割，并应用有针对性的区域感知扰动，生成更清晰、边界感知的显著性图，同时大幅减少计算开销。


<details>
  <summary>Details</summary>
Motivation: 目前的可解释人工智能（XAI）方法，如基于梯度的可解释性和扰动基方法，通常计算成本高昂，需要大量的前向传递，并经常产生噪声或与解剖无关的解释。为了解决这些局限性，本文提出了一种新颖的方法。

Method: 该方法结合了语言导向的区域定位与医学图像分割，通过针对性的、区域感知的扰动，生成清晰的边界意识显著性图，同时显著减少计算开销。

Result: 实验表明，相比传统的扰动方法，XAI-CLIP降低了60%的运行时间，改进了44.6%的Dice分数，并且对于遮挡基解释，Intersection-over-Union提高了96.7%。

Conclusion: 与传统的XAI方法相比，XAI-CLIP在解释清晰度、准确性、计算效率以及临床应用方面表现出显著优势，从而推进了透明和临床可用的医学影像分割系统的发展。

Abstract: Medical image segmentation is a critical component of clinical workflows, enabling accurate diagnosis, treatment planning, and disease monitoring. However, despite the superior performance of transformer-based models over convolutional architectures, their limited interpretability remains a major obstacle to clinical trust and deployment. Existing explainable artificial intelligence (XAI) techniques, including gradient-based saliency methods and perturbation-based approaches, are often computationally expensive, require numerous forward passes, and frequently produce noisy or anatomically irrelevant explanations. To address these limitations, we propose XAI-CLIP, an ROI-guided perturbation framework that leverages multimodal vision-language model embeddings to localize clinically meaningful anatomical regions and guide the explanation process. By integrating language-informed region localization with medical image segmentation and applying targeted, region-aware perturbations, the proposed method generates clearer, boundary-aware saliency maps while substantially reducing computational overhead. Experiments conducted on the FLARE22 and CHAOS datasets demonstrate that XAI-CLIP achieves up to a 60\% reduction in runtime, a 44.6\% improvement in dice score, and a 96.7\% increase in Intersection-over-Union for occlusion-based explanations compared to conventional perturbation methods. Qualitative results further confirm cleaner and more anatomically consistent attribution maps with fewer artifacts, highlighting that the incorporation of multimodal vision-language representations into perturbation-based XAI frameworks significantly enhances both interpretability and efficiency, thereby enabling transparent and clinically deployable medical image segmentation systems.

</details>


### [9] [The Geometry of Representational Failures in Vision Language Models](https://arxiv.org/abs/2602.07025)
*Daniele Savietto,Declan Campbell,André Panisson,Marco Nurisso,Giovanni Petri,Jonathan D. Cohen,Alan Perotti*

Main category: cs.CV

TL;DR: 该研究通过分析开放权重VLMs（Qwen, InternVL, Gemma）的概念向量几何重表示，发现这些向量的几何重叠与特定错误模式高度相关，提供了一种定量的理解内部表示如何塑造模型行为并驱动视觉故障的框架。


<details>
  <summary>Details</summary>
Motivation: 探讨VLMs在多对象视觉任务中表现出的困惑失败，尝试从机制上理解这些错误背后的原因。

Method: 通过分析VLMs的概念向量几何表示，使用引导干预在简化和自然场景中验证这些向量的有效性，研究几何重叠与错误模式之间的关系。

Result: 研究发现，概念向量的几何重叠与特定的错误模式高度相关，为理解VLMs内部表示如何影响模型行为并导致视觉故障提供了定量框架。

Conclusion: 该研究为理解VLMs内部表示如何驱动视觉任务中的错误提供了新的见解，为进一步改进VLMs提供了理论基础。

Abstract: Vision-Language Models (VLMs) exhibit puzzling failures in multi-object visual tasks, such as hallucinating non-existent elements or failing to identify the most similar objects among distractions. While these errors mirror human cognitive constraints, such as the "Binding Problem", the internal mechanisms driving them in artificial systems remain poorly understood. Here, we propose a mechanistic insight by analyzing the representational geometry of open-weight VLMs (Qwen, InternVL, Gemma), comparing methodologies to distill "concept vectors" - latent directions encoding visual concepts. We validate our concept vectors via steering interventions that reliably manipulate model behavior in both simplified and naturalistic vision tasks (e.g., forcing the model to perceive a red flower as blue). We observe that the geometric overlap between these vectors strongly correlates with specific error patterns, offering a grounded quantitative framework to understand how internal representations shape model behavior and drive visual failures.

</details>


### [10] [Modality Gap-Driven Subspace Alignment Training Paradigm For Multimodal Large Language Models](https://arxiv.org/abs/2602.07026)
*Xiaomin Yu,Yi Xin,Wenjie Zhang,Chonghan Liu,Hanzhen Zhao,Xiaoxing Hu,Xinlei Yu,Ziyue Qiao,Hao Tang,Xue Yang,Xiaobin Hu,Chengwei Qin,Hui Xiong,Yu Qiao,Shuicheng Yan*

Main category: cs.CV

TL;DR: 该研究提出了一种矫正模态差距的新方法ReAlign，并据此构建了ReVision训练框架，以有效扩大大规模多模态语言模型。


<details>
  <summary>Details</summary>
Motivation: 解决基于对比学习的多模态表示中存在的模态差距问题，该差距导致不同模态在表达相同语义时占据系统性偏移的位置。

Method: 通过提出固定框模态差距理论，将模态差距分解为稳定的偏差和各向异性的残差。引入无训练的ReAlign策略对模态进行校准，利用大量未配对数据的统计信息，实现文本表示与图像表示分布的对齐，最终提出ReVision训练框架。

Result: 通过ReVision框架，可以在不使用大规模高质量图像文本配对的情况下，让模型从未配对文本中学到视觉表示的分布，从而有效实现模型的规模扩展。

Conclusion: 研究表明，统计对齐的未配对数据可以有效替代昂贵的图像文本配对，为高效扩展大规模多模态语言模型提供了坚实的路径。

Abstract: Despite the success of multimodal contrastive learning in aligning visual and linguistic representations, a persistent geometric anomaly, the Modality Gap, remains: embeddings of distinct modalities expressing identical semantics occupy systematically offset regions. Prior approaches to bridge this gap are largely limited by oversimplified isotropic assumptions, hindering their application in large-scale scenarios. In this paper, we address these limitations by precisely characterizing the geometric shape of the modality gap and leveraging it for efficient model scaling. First, we propose the Fixed-frame Modality Gap Theory, which decomposes the modality gap within a frozen reference frame into stable biases and anisotropic residuals. Guided by this precise modeling, we introduce ReAlign, a training-free modality alignment strategy. Utilizing statistics from massive unpaired data, ReAlign aligns text representation into the image representation distribution via a three-step process comprising Anchor, Trace, and Centroid Alignment, thereby explicitly rectifying geometric misalignment. Building on ReAlign, we propose ReVision, a scalable training paradigm for Multimodal Large Language Models (MLLMs). ReVision integrates ReAlign into the pretraining stage, enabling the model to learn the distribution of visual representations from unpaired text before visual instruction tuning, without the need for large-scale, high-quality image-text pairs. Our framework demonstrates that statistically aligned unpaired data can effectively substitute for expensive image-text pairs, offering a robust path for the efficient scaling of MLLMs.

</details>


### [11] [Fair Context Learning for Evidence-Balanced Test-Time Adaptation in Vision-Language Models](https://arxiv.org/abs/2602.07027)
*Sanggeon Yun,Ryozo Masukawa,SungHeon Jeong,Wenjun Huang,Hanning Chen,Mohsen Imani*

Main category: cs.CV

TL;DR: 提出了一种名为Fair Context Learning (FCL)的新方法，它通过避免熵最小化，根据共享证据偏见来进行测试时适应，实验证明FCL在多种域移位和细粒度基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的基于提示的方法通过最小化熵来提高鲁棒性，但这可能导致错误的自信和伪相关放大。为了解决这个问题，本文提出了一种Fair Context Learning (FCL)方法。

Method: FCL方法包含两个阶段，首先是利用数据增强进行探索以识别可能的类候选，其次是公平驱动的校准，寻找文本上下文的适应性调整，以平衡对常见视觉证据的敏感性。

Result: FCL通过避免熵最小化，并利用共享证据偏见进行测试时适应，在多方面基准测试中取得了与当前最先进的测试时适应方法相当的性能。

Conclusion: FCL展示了在多种领域迁移和细粒度图像分类任务中的有效性，证明了其在提高VLMs适应性方面的潜力。

Abstract: Vision-Language Models (VLMs) such as CLIP enable strong zero-shot recognition but suffer substantial degradation under distribution shifts. Test-Time Adaptation (TTA) aims to improve robustness using only unlabeled test samples, yet most prompt-based TTA methods rely on entropy minimization -- an approach that can amplify spurious correlations and induce overconfident errors when classes share visual features. We propose Fair Context Learning (FCL), an episodic TTA framework that avoids entropy minimization by explicitly addressing shared-evidence bias. Motivated by our additive evidence decomposition assumption, FCL decouples adaptation into (i) augmentation-based exploration to identify plausible class candidates, and (ii) fairness-driven calibration that adapts text contexts to equalize sensitivity to common visual evidence. This fairness constraint mitigates partial feature obsession and enables effective calibration of text embeddings without relying on entropy reduction. Through extensive evaluation, we empirically validate our theoretical motivation and show that FCL achieves competitive adaptation performance relative to state-of-the-art TTA methods across diverse domain-shift and fine-grained benchmarks.

</details>


### [12] [A Comparative Study of Adversarial Robustness in CNN and CNN-ANFIS Architectures](https://arxiv.org/abs/2602.07028)
*Kaaustaaub Shankar,Bharadwaj Dogga,Kelly Cohen*

Main category: cs.CV

TL;DR: 该研究将ANFIS集成到CNN中以提高可解释性和鲁棒性，实验结果显示ANFIS在某些架构中可以增强鲁棒性，但不适用于所有模型。


<details>
  <summary>Details</summary>
Motivation: 尽管CNN在图像分类中表现出色，但它们缺乏可解释性且容易受到对抗性攻击的影响。ANFIS作为一种神经模糊系统，可以提高模型的可解释性，但其对鲁棒性的提升效果尚未充分研究。

Method: 研究者使用标准的CNN（ConvNet，VGG，ResNet18）和它们的ANFIS增强版本，在MNIST，Fashion-MNIST，CIFAR-10和CIFAR-100数据集上进行了对比实验，分别使用基于梯度（PGD）和非基于梯度（Square）的攻击方法。

Result: 实验证明，ANFIS的集成并不一致地提高模型在干净数据上的准确率，并且其在不同架构上的鲁棒性效果也不同：ResNet18-ANFIS表现出提高的对抗鲁棒性，而VGG-ANFIS则经常逊于基线模型。

Conclusion: 研究结论表明，神经模糊增强可以在特定架构中提升鲁棒性，但这并非普遍适用。

Abstract: Convolutional Neural Networks (CNNs) achieve strong image classification performance but lack interpretability and are vulnerable to adversarial attacks. Neuro-fuzzy hybrids such as DCNFIS replace fully connected CNN classifiers with Adaptive Neuro-Fuzzy Inference Systems (ANFIS) to improve interpretability, yet their robustness remains underexplored. This work compares standard CNNs (ConvNet, VGG, ResNet18) with their ANFIS-augmented counterparts on MNIST, Fashion-MNIST, CIFAR-10, and CIFAR-100 under gradient-based (PGD) and gradient-free (Square) attacks. Results show that ANFIS integration does not consistently improve clean accuracy and has architecture-dependent effects on robustness: ResNet18-ANFIS exhibits improved adversarial robustness, while VGG-ANFIS often underperforms its baseline. These findings suggest that neuro-fuzzy augmentation can enhance robustness in specific architectures but is not universally beneficial.

</details>


### [13] [UNIKIE-BENCH: Benchmarking Large Multimodal Models for Key Information Extraction in Visual Documents](https://arxiv.org/abs/2602.07038)
*Yifan Ji,Zhipeng Xu,Zhenghao Liu,Zulong Chen,Qian Zhang,Zhibo Yang,Junyang Lin,Yu Gu,Ge Yu,Maosong Sun*

Main category: cs.CV

TL;DR: UNIKIE-BENCH 是一个统一的基准测试，涵盖了约束类别和开放类别两种 KIE 轨道，旨在评估多模态模型在多样应用场景中的 KIE 能力。


<details>
  <summary>Details</summary>
Motivation: 当前 KIE 在真实文档中的提取面临挑战，UNIKIE-BENCH 的提出旨在提供一个更加全面和系统的评估框架。

Method: UNIKIE-BENCH 包含两个轨道：约束类别轨道聚焦于预先定义好的场景需求，而开放类别轨道则提取文档中明确存在的任何关键信息。

Result: 实验结果显示，多模态模型在不同场景定义下的性能有显著下降，特别是在长尾关键字段和复杂布局上。

Conclusion: 研究揭示了在基于多模态模型的 KIE 中持续存在的困难，特别是在准确性和布局感知方面。

Abstract: Key Information Extraction (KIE) from real-world documents remains challenging due to substantial variations in layout structures, visual quality, and task-specific information requirements. Recent Large Multimodal Models (LMMs) have shown promising potential for performing end-to-end KIE directly from document images. To enable a comprehensive and systematic evaluation across realistic and diverse application scenarios, we introduce UNIKIE-BENCH, a unified benchmark designed to rigorously evaluate the KIE capabilities of LMMs. UNIKIE-BENCH consists of two complementary tracks: a constrained-category KIE track with scenario-predefined schemas that reflect practical application needs, and an open-category KIE track that extracts any key information that is explicitly present in the document. Experiments on 15 state-of-the-art LMMs reveal substantial performance degradation under diverse schema definitions, long-tail key fields, and complex layouts, along with pronounced performance disparities across different document types and scenarios. These findings underscore persistent challenges in grounding accuracy and layout-aware reasoning for LMM-based KIE. All codes and datasets are available at https://github.com/NEUIR/UNIKIE-BENCH.

</details>


### [14] [OMNI-Dent: Towards an Accessible and Explainable AI Framework for Automated Dental Diagnosis](https://arxiv.org/abs/2602.07041)
*Leeje Jang,Yao-Yi Chiang,Angela M. Hastings,Patimaporn Pungchanchaikul,Martha B. Lucas,Emily C. Schultz,Jeffrey P. Louie,Mohamed Estai,Wen-Chen Wang,Ryan H. L. Ip,Boyen Huang*

Main category: cs.CV

TL;DR: OMNI-Dent是一个无监督、可解释的诊断框架，通过结合临床推理原则，利用视觉语言模型进行多视角手机拍摄的照片分析，辅助非专业医疗资源条件下口腔疾病的早期诊断。


<details>
  <summary>Details</summary>
Motivation: 现有的基于人工智能的诊断方法主要将诊断视为视觉模式识别任务，忽视了牙医专业的结构化临床推理过程，同时数据需求大、难以适应多样化的实际成像条件。

Method: OMNI-Dent通过一个结合了临床推理原则的视觉语言模型框架进行运作，支持多视角的智能手机照片分析，嵌入牙医的诊断启发规则，指导通用视觉语言模型进行牙齿级别的评估。

Result: OMNI-Dent能够在缺乏专业临床影像资源的情况下支持早期诊断，辅助用户识别潜在异常情况，评估是否需要专业评估。

Conclusion: OMNI-Dent旨在为有限的实地护理资源提供一个早期辅助诊断工具，有助于提高口腔医疗的可及性和质量。

Abstract: Accurate dental diagnosis is essential for oral healthcare, yet many individuals lack access to timely professional evaluation. Existing AI-based methods primarily treat diagnosis as a visual pattern recognition task and do not reflect the structured clinical reasoning used by dental professionals. These approaches also require large amounts of expert-annotated data and often struggle to generalize across diverse real-world imaging conditions. To address these limitations, we present OMNI-Dent, a data-efficient and explainable diagnostic framework that incorporates clinical reasoning principles into a Vision-Language Model (VLM)-based pipeline. The framework operates on multi-view smartphone photographs,embeds diagnostic heuristics from dental experts, and guides a general-purpose VLM to perform tooth-level evaluation without dental-specific fine-tuning of the VLM. By utilizing the VLM's existing visual-linguistic capabilities, OMNI-Dent aims to support diagnostic assessment in settings where curated clinical imaging is unavailable. Designed as an early-stage assistive tool, OMNI-Dent helps users identify potential abnormalities and determine when professional evaluation may be needed, offering a practical option for individuals with limited access to in-person care.

</details>


### [15] [COMBOOD: A Semiparametric Approach for Detecting Out-of-distribution Data for Image Classification](https://arxiv.org/abs/2602.07042)
*Magesh Rajasekaran,Md Saiful Islam Sajol,Frej Berglind,Supratik Mukhopadhyay,Kamalika Das*

Main category: cs.CV

TL;DR: COMBOOD框架使用非参数和参数距离度量的组合来提高远分布和近分布异常数据检测的准确性，且在多个基准数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在实际应用场景中，特别是在自动化领域，需要识别出在训练分布之外的数据，以保证模型的可靠性。现有的非参数方法对于检测远分布异常数据有效，但近分布的检测不理想。因此，COMBOOD框架提出了一种新的半参数方法，结合非参数和参数距离信息，提高了两种场景下的检测性能。

Method: COMBOOD框架将最近邻距离度量和马氏距离度量结合在一个半参数设置中，生成一个适用于近分布和远分布异常数据识别的信心分数。

Result: COMBOOD框架在包括OpenOOD和文档数据集在内的多个基准数据集上，相较于现有最佳方法在远分布和近分布异常数据检测准确性上均有提升。并且，其性能提升具有统计显著性。

Conclusion: COMBOOD框架提供了一种高效识别远分布和近分布异常数据的方法，由于其线性扩展特性，适用于多种实际应用场景。

Abstract: Identifying out-of-distribution (OOD) data at inference time is crucial for many machine learning applications, especially for automation. We present a novel unsupervised semi-parametric framework COMBOOD for OOD detection with respect to image recognition. Our framework combines signals from two distance metrics, nearest-neighbor and Mahalanobis, to derive a confidence score for an inference point to be out-of-distribution. The former provides a non-parametric approach to OOD detection. The latter provides a parametric, simple, yet effective method for detecting OOD data points, especially, in the far OOD scenario, where the inference point is far apart from the training data set in the embedding space. However, its performance is not satisfactory in the near OOD scenarios that arise in practical situations. Our COMBOOD framework combines the two signals in a semi-parametric setting to provide a confidence score that is accurate both for the near-OOD and far-OOD scenarios. We show experimental results with the COMBOOD framework for different types of feature extraction strategies. We demonstrate experimentally that COMBOOD outperforms state-of-the-art OOD detection methods on the OpenOOD (both version 1 and most recent version 1.5) benchmark datasets (for both far-OOD and near-OOD) as well as on the documents dataset in terms of accuracy. On a majority of the benchmark datasets, the improvements in accuracy resulting from the COMBOOD framework are statistically significant. COMBOOD scales linearly with the size of the embedding space, making it ideal for many real-life applications.

</details>


### [16] [PipeMFL-240K: A Large-scale Dataset and Benchmark for Object Detection in Pipeline Magnetic Flux Leakage Imaging](https://arxiv.org/abs/2602.07044)
*Tianyi Qu,Songxiao Yang,Haolin Wang,Huadong Song,Xiaoting Guo,Wenguang Hu,Guanlin Liu,Honghe Chen,Yafei Ou*

Main category: cs.CV

TL;DR: PipeMFL-240K 是一个大规模且精细注释的数据集，为管道 MFL 假彩色图像中的复杂对象检测提供了基准。它包含 240,320 张图像和 191,530 个高质量边界框注释，并揭示了现代检测器在 MFL 数据上的局限性，助推相关研究。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏面向管道 MFL 检测的大规模公共数据集，影响了算法的可靠比较和重复评估。PipeMFL-240K 的目标是填补这一空白，推动 MFL 基础设施检测的相关研究。

Method: PipeMFL-240K 在 11 个管道上收集了 240,320 个图像，分为 191,530 个高质量边界框注释。实验使用了最先进的目标检测器建立了基准。

Result: 实验结果表明现代检测器在处理 MFL 数据特有的属性方面仍然存在难题，展示了 PipeMFL-240K 是一个可靠且具有挑战性的测试床，有助于推进算法改进和可重复研究。

Conclusion: PipeMFL-240K 是第一个公共数据集和首个具有如此规模和范围的标准，对于管道检测诊断、维护计划及 MFL 基础设施评估算法的创新都具有重要意义，有望促进算法创新和可重复研究。

Abstract: Pipeline integrity is critical to industrial safety and environmental protection, with Magnetic Flux Leakage (MFL) detection being a primary non-destructive testing technology. Despite the promise of deep learning for automating MFL interpretation, progress toward reliable models has been constrained by the absence of a large-scale public dataset and benchmark, making fair comparison and reproducible evaluation difficult. We introduce \textbf{PipeMFL-240K}, a large-scale, meticulously annotated dataset and benchmark for complex object detection in pipeline MFL pseudo-color images. PipeMFL-240K reflects real-world inspection complexity and poses several unique challenges: (i) an extremely long-tailed distribution over \textbf{12} categories, (ii) a high prevalence of tiny objects that often comprise only a handful of pixels, and (iii) substantial intra-class variability. The dataset contains \textbf{240,320} images and \textbf{191,530} high-quality bounding-box annotations, collected from 11 pipelines spanning approximately \textbf{1,480} km. Extensive experiments are conducted with state-of-the-art object detectors to establish baselines. Results show that modern detectors still struggle with the intrinsic properties of MFL data, highlighting considerable headroom for improvement, while PipeMFL-240K provides a reliable and challenging testbed to drive future research. As the first public dataset and the first benchmark of this scale and scope for pipeline MFL inspection, it provides a critical foundation for efficient pipeline diagnostics as well as maintenance planning and is expected to accelerate algorithmic innovation and reproducible research in MFL-based pipeline integrity assessment.

</details>


### [17] [VLRS-Bench: A Vision-Language Reasoning Benchmark for Remote Sensing](https://arxiv.org/abs/2602.07045)
*Zhiming Luo,Di Wang,Haonan Guo,Jing Zhang,Bo Du*

Main category: cs.CV

TL;DR: 该研究提出了一个新的基准VLRS-Bench，旨在解决现有遥感（RS）基准中偏向感知任务的问题，专注于复杂RS推理。它涵盖14项任务和八个时间阶段，由专门的管道构建，确保地理现实性和推理复杂性。


<details>
  <summary>Details</summary>
Motivation: 远程 sensing（RS）当前的评估基准主要集中在感知任务而非复杂的认知推理，这限制了解决RS领域的复杂应用。因此，该研究旨在填补这一空白，通过构建一个新的基准来推动此类应用的发展。

Method: 此基准通过特殊的构建管道整合了RS特定的先验知识和专家知识，涵盖了认知、决策和预测三个核心维度。涵盖了14项任务和八个时间阶段，具有2,000个问答对。

Result: 实验结果表明，现有的最先进的多模态语言模型存在显着瓶颈，为在远程传感社区中推进多模态推理提供了关键见解。

Conclusion: 该研究强调了需要开发能够处理复杂RS推理的新方法和评估标准的重要性。

Abstract: Recent advancements in Multimodal Large Language Models (MLLMs) have enabled complex reasoning. However, existing remote sensing (RS) benchmarks remain heavily biased toward perception tasks, such as object recognition and scene classification. This limitation hinders the development of MLLMs for cognitively demanding RS applications. To address this, , we propose a Vision Language ReaSoning Benchmark (VLRS-Bench), which is the first benchmark exclusively dedicated to complex RS reasoning. Structured across the three core dimensions of Cognition, Decision, and Prediction, VLRS-Bench comprises 2,000 question-answer pairs with an average length of 71 words, spanning 14 tasks and up to eight temporal phases. VLRS-Bench is constructed via a specialized pipeline that integrates RS-specific priors and expert knowledge to ensure geospatial realism and reasoning complexity. Experimental results reveal significant bottlenecks in existing state-of-the-art MLLMs, providing critical insights for advancing multimodal reasoning within the remote sensing community.

</details>


### [18] [ShapBPT: Image Feature Attributions Using Data-Aware Binary Partition Trees](https://arxiv.org/abs/2602.07047)
*Muhammad Rashid,Elvio G. Amparore,Enrico Ferrari,Damiano Verda*

Main category: cs.CV

TL;DR: 论文提出了一种名为ShapBPT的新方法，这是一种基于二进制分割树（BPT）的多层次结构，用于图像解释。ShapBPT通过利用数据感知的层次分割提高了模型可解释性，使得特征归因能够更加符合图像的内在形态特征。


<details>
  <summary>Details</summary>
Motivation: 现有的Shapley方法在计算机视觉任务中没有利用数据感知的层次结构，导致模型解释不足，ShapBPT旨在通过引入数据感知的层次分割树来填补这一空白，提高模型的可解释性和解释效果。

Method: ShapBPT方法采用了一种名为二进制分割树（BPT）的多层次结构，基于层次Shapley公式为图像特征分配Shapley系数，从而使得特征归因与图像内在形态特征更相符。

Result: 实验结果表明，与现有计算机视觉解释方法相比，ShapBPT在与图像结构的对齐和效率方面表现更优。此外，20名受试者的用户研究结果显示，ShapBPT的解释更为人类易于理解和接受。

Conclusion: ShapBPT提供了一种新的视角来解释图像数据，通过多层次结构的引入，使得特征解释更加有效和有意义，为进一步的计算机视觉任务中的模型解释提供了新的参考。

Abstract: Pixel-level feature attributions are an important tool in eXplainable AI for Computer Vision (XCV), providing visual insights into how image features influence model predictions. The Owen formula for hierarchical Shapley values has been widely used to interpret machine learning (ML) models and their learned representations. However, existing hierarchical Shapley approaches do not exploit the multiscale structure of image data, leading to slow convergence and weak alignment with the actual morphological features. Moreover, no prior Shapley method has leveraged data-aware hierarchies for Computer Vision tasks, leaving a gap in model interpretability of structured visual data. To address this, this paper introduces ShapBPT, a novel data-aware XCV method based on the hierarchical Shapley formula. ShapBPT assigns Shapley coefficients to a multiscale hierarchical structure tailored for images, the Binary Partition Tree (BPT). By using this data-aware hierarchical partitioning, ShapBPT ensures that feature attributions align with intrinsic image morphology, effectively prioritizing relevant regions while reducing computational overhead. This advancement connects hierarchical Shapley methods with image data, providing a more efficient and semantically meaningful approach to visual interpretability. Experimental results confirm ShapBPT's effectiveness, demonstrating superior alignment with image structures and improved efficiency over existing XCV methods, and a 20-subject user study confirming that ShapBPT explanations are preferred by humans.

</details>


### [19] [Enhancing IMU-Based Online Handwriting Recognition via Contrastive Learning with Zero Inference Overhead](https://arxiv.org/abs/2602.07049)
*Jindong Li,Dario Zanca,Vincent Christlein,Tim Hamann,Jens Barth,Peter Kämpf,Björn Eskofier*

Main category: cs.CV

TL;DR: 本文提出了一种无需增加推断成本的 Error-enhanced Contrastive Handwriting Recognition (ECHWR) 训练框架，涉及使用临时辅助分支在训练期间将传感器信号与语义文本嵌入对齐，并通过内部对比损失和基于错误的对比损失实现这一点。这在 OnHW-Words500 数据集上显著优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 当前的在线手写识别方法通常依赖于云处理，这可能导致隐私问题和响应延迟。为了克服这些问题，本文提出了 ECHWR 训练框架，该框架可以在边缘硬件上运行，从而提高隐私性并降低延迟，同时保持低内存消耗。

Method: ECHWR 采用了一个临时的辅助分支，在训练阶段将传感器信号与语义文本嵌入对齐。具体而言，它使用内部对比损失（in-batch contrastive loss）来进行一般模态对齐，并通过基于错误的对比损失（error-based contrastive loss）来区分正确的信号和合成的困难负样本，从而实现准确的特征表示和识别。

Result: 在 OnHW-Words500 数据集上的评估表明，ECHWR 显著优于现有基线，减少字符错误率高达 7.4%（对于无作者依赖的划分）和 10.4%（对于有作者依赖的划分）。此外，消融研究表明，特定挑战可能需要特定的架构和目标配置，但基于错误的对比损失对处理未知书写风格表现出有效性。

Conclusion: 本文提出的 ECHWR 训练框架能够在边缘硬件上高效运行的同时，提高在线手写识别的准确性和隐私性，特别是在处理未知书写风格方面显示出了潜力。

Abstract: Online handwriting recognition using inertial measurement units opens up handwriting on paper as input for digital devices. Doing it on edge hardware improves privacy and lowers latency, but entails memory constraints. To address this, we propose Error-enhanced Contrastive Handwriting Recognition (ECHWR), a training framework designed to improve feature representation and recognition accuracy without increasing inference costs. ECHWR utilizes a temporary auxiliary branch that aligns sensor signals with semantic text embeddings during the training phase. This alignment is maintained through a dual contrastive objective: an in-batch contrastive loss for general modality alignment and a novel error-based contrastive loss that distinguishes between correct signals and synthetic hard negatives. The auxiliary branch is discarded after training, which allows the deployed model to keep its original, efficient architecture. Evaluations on the OnHW-Words500 dataset show that ECHWR significantly outperforms state-of-the-art baselines, reducing character error rates by up to 7.4% on the writer-independent split and 10.4% on the writer-dependent split. Finally, although our ablation studies indicate that solving specific challenges require specific architectural and objective configurations, error-based contrastive loss shows its effectiveness for handling unseen writing styles.

</details>


### [20] [Neural Sentinel: Unified Vision Language Model (VLM) for License Plate Recognition with Human-in-the-Loop Continual Learning](https://arxiv.org/abs/2602.07051)
*Karthik Sivakoti*

Main category: cs.CV

TL;DR: 本文提出了一种名为Neural Sentinel的新型统一方法，利用视觉语言模型一次性完成车牌识别、车辆状态分类和属性提取，显著提升识别准确率，并引入了Human-in-the-Loop持续学习框架，保持了良好的校准度。


<details>
  <summary>Details</summary>
Motivation: 传统ALPR系统引入了累积错误、增加延迟和复杂架构，本文通过将PaliGemma 3B模型微调并使用Low-Rank Adaptation进行适配，展示了一种VLM多模态识别方法的有效性。

Method: 本文提出了Neural Sentinel，通过预训练的PaliGemma 3B模型进行微调并使用LoRA适应，实现了一次性完成多任务识别，并设计了基于经验重放的HITL持续学习框架来防止灾难性遗忘。

Result: Neural Sentinel在车牌识别上的准确率达到了92.3%，比EasyOCR和PaddleOCR分别高出了14.1%和9.9%，且具有较低的推理延迟，平均为152毫秒，预期校准误差ECE为0.048。此外，该模型还首次展示了对包括车辆颜色检测、安全带检测和占用率计数等辅助任务的零样本泛化能力。

Conclusion: 文章证明了统一视觉语言方法在ALPR系统中的优越性，理论上简化了架构，提高了多任务识别的性能，为该领域的研究开辟了新的方向。

Abstract: Traditional Automatic License Plate Recognition (ALPR) systems employ multi-stage pipelines consisting of object detection networks followed by separate Optical Character Recognition (OCR) modules, introducing compounding errors, increased latency, and architectural complexity. This research presents Neural Sentinel, a novel unified approach that leverages Vision Language Models (VLMs) to perform license plate recognition, state classification, and vehicle attribute extraction through a single forward pass. Our primary contribution lies in demonstrating that a fine-tuned PaliGemma 3B model, adapted via Low-Rank Adaptation (LoRA), can simultaneously answer multiple visual questions about vehicle images, achieving 92.3% plate recognition accuracy, which is a 14.1% improvement over EasyOCR and 9.9% improvement over PaddleOCR baselines. We introduce a Human-in-the-Loop (HITL) continual learning framework that incorporates user corrections while preventing catastrophic forgetting through experience replay, maintaining a 70:30 ratio of original training data to correction samples. The system achieves a mean inference latency of 152ms with an Expected Calibration Error (ECE) of 0.048, indicating well calibrated confidence estimates. Additionally, the VLM first architecture enables zero-shot generalization to auxiliary tasks including vehicle color detection (89%), seatbelt detection (82%), and occupancy counting (78%) without task specific training. Through extensive experimentation on real world toll plaza imagery, we demonstrate that unified vision language approaches represent a paradigm shift in ALPR systems, offering superior accuracy, reduced architectural complexity, and emergent multi-task capabilities that traditional pipeline approaches cannot achieve.

</details>


### [21] [Toward Accurate and Accessible Markerless Neuronavigation](https://arxiv.org/abs/2602.07052)
*Ziye Xie,Oded Schlesinger,Raj Kundu,Jessica Y. Choi,Pablo Iturralde,Dennis A. Turner,Stefan M. Goetz,Guillermo Sapiro,Angel V. Peterchev,J. Matias Di Martino*

Main category: cs.CV

TL;DR: 该研究提出并评估了一种无需标记物的神经导航方法，通过使用低成本的可见光和红外摄像机结合立体和深度传感技术，以及面部几何算法建模，证明了其在经颅磁刺激中的足够精度，并且降低了设置成本和复杂性，提高了患者的舒适度。


<details>
  <summary>Details</summary>
Motivation: 传统的神经导航系统依赖于患者佩戴的标记物，这些标记物需要手动对齐，可能会在程序过程中移动，并可能引起不适。因此，研究动机是探索一种无需标记物的方法来替代昂贵的硬件和物理标记，提高神经导航技术的精度、舒适性和可访问性。

Method: 研究方法包括使用低成本的可见光和红外摄像机结合立体和深度传感技术，以及面部几何算法建模，实现无需标记物的神经导航。研究团队通过实验验证了所提出的系统的性能。

Result: 通过对50名人体的验证测试，所提出的无标记神经导航算法在经颅磁刺激中的误差分别为2.32毫米和2.01度，相比传统的标记系统具有更高的精度。并且研究表明，集成多摄像头传感器的数据能够进一步提高整体准确性。

Conclusion: 研究结论表明无标记神经导航方法可降低设置成本与复杂性、改善患者的舒适度，并扩大在临床和科研领域的应用范围。

Abstract: Neuronavigation is widely used in biomedical research and interventions to guide the precise placement of instruments around the head to support procedures such as transcranial magnetic stimulation. Traditional systems, however, rely on subject-mounted markers that require manual registration, may shift during procedures, and can cause discomfort. We introduce and evaluate markerless approaches that replace expensive hardware and physical markers with low-cost visible and infrared light cameras incorporating stereo and depth sensing combined with algorithmic modeling of the facial geometry. Validation with $50$ human subjects yielded a median tracking discrepancy of only $2.32$ mm and $2.01°$ for the best markerless algorithms compared to a conventional marker-based system, which indicates sufficient accuracy for transcranial magnetic stimulation and a substantial improvement over prior markerless results. The results suggest that integration of the data from the various camera sensors can improve the overall accuracy further. The proposed markerless neuronavigation methods can reduce setup cost and complexity, improve patient comfort, and expand access to neuronavigation in clinical and research settings.

</details>


### [22] [RECITYGEN -- Interactive and Generative Participatory Urban Design Tool with Latent Diffusion and Segment Anything](https://arxiv.org/abs/2602.07057)
*Di Mo,Mingyang Sun,Chengxiu Yin,Runjia Tian,Yanhong Wu,Liyan Xu*

Main category: cs.CV

TL;DR: RECITYGEN 是一种利用先进生成模型和交互式语义分割工具，通过文本提示生成变化中的城市街道视图图片的新颖工具，已在北京市一个城市再生项目试点中显示出满足公众偏好的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统的自上而下方法在城市设计中常常忽视公众意见，RECITYGEN旨在通过将最先进的生成模型与交互式语义分割结合，提高公众参与度，从而改善城市环境设计。

Method: RECITYGEN 使用状态-of-the-art 的潜扩散模型与交互语义分割技术相结合，用户可以通过文本提示生成变化中的城市街道视图图片。

Result: 在北京市城市再生项目试点中，用户使用RECITYGEN提出了城市改进的一些建议，显示了较好的融合公众意见的效果。

Conclusion: RECITYGEN 指向了一种更动态、包容的城市规划方法，具有较好的应用前景，未来可以扩大实验范围以进一步验证其效果。

Abstract: Urban design profoundly impacts public spaces and community engagement. Traditional top-down methods often overlook public input, creating a gap in design aspirations and reality. Recent advancements in digital tools, like City Information Modelling and augmented reality, have enabled a more participatory process involving more stakeholders in urban design. Further, deep learning and latent diffusion models have lowered barriers for design generation, providing even more opportunities for participatory urban design. Combining state-of-the-art latent diffusion models with interactive semantic segmentation, we propose RECITYGEN, a novel tool that allows users to interactively create variational street view images of urban environments using text prompts. In a pilot project in Beijing, users employed RECITYGEN to suggest improvements for an ongoing Urban Regeneration project. Despite some limitations, RECITYGEN has shown significant potential in aligning with public preferences, indicating a shift towards more dynamic and inclusive urban planning methods. The source code for the project can be found at RECITYGEN GitHub.

</details>


### [23] [FADE: Selective Forgetting via Sparse LoRA and Self-Distillation](https://arxiv.org/abs/2602.07058)
*Carolina R. Kelsch,Leonardo S. B. Pereira,Natnael Mola,Luis H. Arribas,Juan C. S. M. Avedillo*

Main category: cs.CV

TL;DR: FADE是一种用于图像生成模型的快速数据擦除方法，通过两个阶段实现精确的遗忘和保留控制，包括参数定位和自蒸馏，确保轻量级且可逆的修改。


<details>
  <summary>Details</summary>
Motivation: 提出FADE是为了应对文本到图像扩散模型中的去学习挑战，提高隐私保护能力和AI责任感，同时保持整体性能。

Method: FADE方法包括两个阶段：第一阶段使用基于梯度的显著性检测参数，并通过稀疏LoRA适配器限制更新，实现轻量级和定位修改；第二阶段通过自蒸馏目标用用户定义的替代方法替换遗忘的概念，同时保留其他数据的行为。

Result: FADE方法在UnlearnCanvas基准测试以及其他几个数据集上展示了State-of-the-Art的去学习性能，具有精细的遗忘与保留权衡控制。

Conclusion: FADE通过结构化的参数修改和自定义替代内容的应用，实现了高效的、可逆的、灵活的图像生成去学习方法，适用于扩散模型的选择性删除需求。

Abstract: Machine Unlearning aims to remove the influence of specific data or concepts from trained models while preserving overall performance, a capability increasingly required by data protection regulations and responsible AI practices. Despite recent progress, unlearning in text-to-image diffusion models remains challenging due to high computational costs and the difficulty of balancing effective forgetting with retention of unrelated concepts. We introduce FADE (Fast Adapter for Data Erasure), a two-stage unlearning method for image generation that combines parameter localization with self-distillation. FADE first identifies parameters most responsible for the forget set using gradient-based saliency and constrains updates through sparse LoRA adapters, ensuring lightweight, localized modifications. In a second stage, FADE applies a self-distillation objective that overwrites the forgotten concept with a user-defined surrogate while preserving behavior on retained data. The resulting adapters are memory-efficient, reversible, and can be merged or removed at runtime, enabling flexible deployment in production systems. We evaluated FADE on the UnlearnCanvas benchmark and conducted ablation studies on Imagenette, Labeled Faces in the Wild, AtharvaTaras Dog Breeds Dataset, and SUN Attributes datasets, demonstrating State-of-the-Art unlearning performance with fine-grained control over the forgetting-retention trade-off. Our results demonstrate that FADE achieves strong concept erasure and high retainability across various domains, making it a suitable solution for selective unlearning in diffusion-based image generation models.

</details>


### [24] [From Images to Decisions: Assistive Computer Vision for Non-Metallic Content Estimation in Scrap Metal](https://arxiv.org/abs/2602.07062)
*Daniil Storonkin,Ilia Dziub,Maksim Golyadkin,Ilya Makarov*

Main category: cs.CV

TL;DR: 本文提出了一种计算机视觉辅助管道，用于通过卸料过程中捕获的图像估算废料的污染（以百分比表示），并分类废料类型。实现效果包括使用多实例学习（MIL）的MAE 0.27和R2 0.83；以及多任务学习（MTL）设置的MAE 0.36，F1 0.79。系统在接收工作流程中实时，磁铁/废料车检测划分时间层，版本化推理服务生成废料级别的估算值，并由操作员审核，以提供结构化的覆盖，对纠正和不确定情况的反馈形成一个持续改进的主动学习循环。


<details>
  <summary>Details</summary>
Motivation: 目前，钢制品中的非金属夹杂物（污染）主要是通过视觉检查由检查员评估，这种方法主观且危险，因为存在灰尘和移动的机器。为了减少这种方法带来的主观性，提高工作人员安全并整合入接收和熔炼计划的工作流程，本文提出了一种计算机视觉辅助管道。

Method: 提出的方法将污染评估作为一个回归任务，使用多实例学习（MIL）和多任务学习（MTL）。系统包含磁铁/废料车检测分期时间图层、版本化的推理服务生产废料级别的估计值、操作员审核以提供结构化的覆盖，并对纠正和不确定情况进行主动学习循环反馈。

Result: MIL方法的结果包括MAE 0.27和R2 0.83；以及多任务学习（MTL）设置达到了MAE 0.36和F1 0.79。该系统在接收工作流程中的实时应用，通过MRI和MTL提高了准确性和效率。

Conclusion: 此计算机视觉辅助管道能够减少污染评估中的主观性，提高工作人员安全，并能够整合到接收和熔炼计划的工作流程中，这对于提高钢铁制造的质量和效率具有重要意义。

Abstract: Scrap quality directly affects energy use, emissions, and safety in steelmaking. Today, the share of non-metallic inclusions (contamination) is judged visually by inspectors - an approach that is subjective and hazardous due to dust and moving machinery. We present an assistive computer vision pipeline that estimates contamination (per percent) from images captured during railcar unloading and also classifies scrap type. The method formulates contamination assessment as a regression task at the railcar level and leverages sequential data through multi-instance learning (MIL) and multi-task learning (MTL). Best results include MAE 0.27 and R2 0.83 by MIL; and an MTL setup reaches MAE 0.36 with F1 0.79 for scrap class. Also we present the system in near real time within the acceptance workflow: magnet/railcar detection segments temporal layers, a versioned inference service produces railcar-level estimates with confidence scores, and results are reviewed by operators with structured overrides; corrections and uncertain cases feed an active-learning loop for continual improvement. The pipeline reduces subjective variability, improves human safety, and enables integration into acceptance and melt-planning workflows.

</details>


### [25] [Bidirectional Reward-Guided Diffusion for Real-World Image Super-Resolution](https://arxiv.org/abs/2602.07069)
*Zihao Fan,Xin Lu,Yidi Liu,Jie Huang,Dong Li,Xueyang Fu,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: Bird-SR提出了一种双重奖励引导的扩散框架，它不仅利用了合成的低分辨率-高分辨率配对数据，还利用了真实世界的低分辨率图像，通过奖励反馈学习优化超分辨率过程，同时权衡结构保真度和感知增强，最终在多个真实世界的超分辨率基准上展示了优越的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于扩散的超分辨率方法在处理来自合成数据训练的模型无法应对真实世界的低分辨率图像，主要是因为数据分布的差异。因此，需要一个新方法来处理这些差异，并且能够保留结构一致性的同时提高感知质量。

Method: Bird-SR框架涉及到使用奖励反馈学习（ReFL）策略来优化超分辨率过程。早期步骤通过优化合成对来直接优化结构保真度，后期步骤引入指导质量的奖励对合成和真实低分辨率图像进行感知增强。此外，还采取了动态的保真度-感知权重策略，在可以获得更好的结构保真度的同时确保感知质量的服务。

Result: 在多个真实世界的超分辨率基准上，Bird-SR在感知质量方面表现优异，同时也保持了结构一致性。

Conclusion: Bird-SR证明了在真实世界的超分辨率任务中，通过双重奖励引导的扩散框架，可以在保留结构保真度的同时提高感知质量，提高了现有技术的有效性。

Abstract: Diffusion-based super-resolution can synthesize rich details, but models trained on synthetic paired data often fail on real-world LR images due to distribution shifts. We propose Bird-SR, a bidirectional reward-guided diffusion framework that formulates super-resolution as trajectory-level preference optimization via reward feedback learning (ReFL), jointly leveraging synthetic LR-HR pairs and real-world LR images. For structural fidelity easily affected in ReFL, the model is directly optimized on synthetic pairs at early diffusion steps, which also facilitates structure preservation for real-world inputs under smaller distribution gap in structure levels. For perceptual enhancement, quality-guided rewards are applied at later sampling steps to both synthetic and real LR images. To mitigate reward hacking, the rewards for synthetic results are formulated in a relative advantage space bounded by their clean counterparts, while real-world optimization is regularized via a semantic alignment constraint. Furthermore, to balance structural and perceptual learning, we adopt a dynamic fidelity-perception weighting strategy that emphasizes structure preservation at early stages and progressively shifts focus toward perceptual optimization at later diffusion steps. Extensive experiments on real-world SR benchmarks demonstrate that Bird-SR consistently outperforms state-of-the-art methods in perceptual quality while preserving structural consistency, validating its effectiveness for real-world super-resolution.

</details>


### [26] [MosaicThinker: On-Device Visual Spatial Reasoning for Embodied AI via Iterative Construction of Space Representation](https://arxiv.org/abs/2602.07082)
*Haoming Wang,Qiyao Xue,Weichen Liu,Wei Gao*

Main category: cs.CV

TL;DR: 提出了MosaicThinker，一种设备端的推理技术，旨在增强小型视觉语言模型在跨帧空间推理任务中的空间推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在跨帧复杂空间关系推理中表现较差，因为缺乏关于3D空间信息的知识。

Method: 通过将片段化的空间信息从多个视频帧集成到全局语义地图中，并利用视觉提示来指导模型的空间推理。

Result: 在资源受限的类人AI设备上，显著提升了跨帧空间推理的准确性。

Conclusion: MosaicThinker提供了一种有效的方法，以增强设备端小VLM在复杂跨帧空间推理任务中的能力。

Abstract: When embodied AI is expanding from traditional object detection and recognition to more advanced tasks of robot manipulation and actuation planning, visual spatial reasoning from the video inputs is necessary to perceive the spatial relationships of objects and guide device actions. However, existing visual language models (VLMs) have very weak capabilities in spatial reasoning due to the lack of knowledge about 3D spatial information, especially when the reasoning task involve complex spatial relations across multiple video frames. In this paper, we present a new inference-time computing technique for on-device embodied AI, namely \emph{MosaicThinker}, which enhances the on-device small VLM's spatial reasoning capabilities on difficult cross-frame reasoning tasks. Our basic idea is to integrate fragmented spatial information from multiple frames into a unified space representation of global semantic map, and further guide the VLM's spatial reasoning over the semantic map via a visual prompt. Experiment results show that our technique can greatly enhance the accuracy of cross-frame spatial reasoning on resource-constrained embodied AI devices, over reasoning tasks with diverse types and complexities.

</details>


### [27] [WorldEdit: Towards Open-World Image Editing with a Knowledge-Informed Benchmark](https://arxiv.org/abs/2602.07095)
*Wang Lin,Feng Wang,Majun Zhang,Wentao Hu,Tao Jin,Zhou Zhao,Fei Wu,Jingyuan Chen,Alan Yuille,Sucheng Ren*

Main category: cs.CV

TL;DR: 该研究提出了一种名为WorldEdit的数据集，旨在为图像编辑模型提供世界驱动的训练和评估。WorldEdit包含指导性再表述的编辑样本，以匹配现实世界的因果逻辑。研究还提出了WorldEdit-Test用于当前模型在因果编辑场景中的表现评测。


<details>
  <summary>Details</summary>
Motivation: 现有的图像编辑模型在处理隐含编辑指令时存在局限性，因此需要一种新的方法来提高模型对现实世界因果逻辑的理解和执行的能力。

Method: 研究人员创建了WorldEdit数据集，包含高质量的编辑样本，这些样本由经过重新表述的指令指导，以符合真实世界的因果逻辑。此外，还提出了WorldEdit-Test用于评估现有模型在因果编辑场景中的性能。Method还介绍了一种双阶段的训练框架，用于微调如Bagel这样的模型，并结合因果验证奖励。

Result: 实验结果显示，通过应用WorldEdit数据集和提出的双阶段训练方法，研究模型不仅在遵循指令方面与GPT-4o和Nano-Banana竞争，还在知识合理性方面也表现出色，这是开源系统通常难以达到的。

Conclusion: 研究证明，世界驱动的图像编辑数据集可以显著提高图像编辑模型的性能和一致性，特别是在隐含指令处理方面。

Abstract: Recent advances in image editing models have demonstrated remarkable capabilities in executing explicit instructions, such as attribute manipulation, style transfer, and pose synthesis. However, these models often face challenges when dealing with implicit editing instructions, which describe the cause of a visual change without explicitly detailing the resulting outcome. These limitations arise because existing models rely on uniform editing strategies that are not equipped to handle the complex world knowledge and reasoning required for implicit instructions. To address this gap, we introduce \textbf{WorldEdit}, a dataset specifically designed to enable world-driven image editing. WorldEdit consists of high-quality editing samples, guided by paraphrased instructions that align with real-world causal logic. Furthermore, we provide \textbf{WorldEdit-Test} for evaluating the existing model's performance on causal editing scenarios. With WorldEdit, we use a two-stage training framework for fine-tuning models like Bagel, integrating with a causal verification reward. Our results show that the proposed dataset and methods significantly narrow the gap with GPT-4o and Nano-Banana, demonstrating competitive performance not only in instruction following but also in knowledge plausibility, where many open-source systems typically struggle.

</details>


### [28] [TLC-Plan: A Two-Level Codebook Based Network for End-to-End Vector Floorplan Generation](https://arxiv.org/abs/2602.07100)
*Biao Xiong,Zhen Peng,Ping Wang,Qiegen Liu,Xian Zhong*

Main category: cs.CV

TL;DR: 该研究提出了一种名为TLC-Plan的层次生成模型，可以直接从输入边界生成矢量平面图，无需后处理，能够实现高效、可持续的设计，并在RPLAN和LIFULL数据集上取得了先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的自动平面图生成方法在像素空间中操作，并依赖于事后矢量化，这会导致结构不一致和阻碍端到端学习。本研究旨在通过组合空间推理来解决这一问题。

Method: TLC-Plan采用两级VQ-VAE编码全局布局，使用语义标记的房间边界框，并通过多边形级代码细化局部几何结构。层次结构通过CodeTree表示进行统一，并使用自回归变换器在边界条件上采样代码，从而生成多样化且拓扑有效的设计。

Result: 该模型在RPLAN数据集（FID = 1.84，MSE = 2.06）和LIFULL数据集上展示了最先进的性能。

Conclusion: TLC-Plan框架为面向实际建筑应用的约束感知和可扩展矢量平面图生成迈出了重要一步。

Abstract: Automated floorplan generation aims to improve design quality, architectural efficiency, and sustainability by jointly modeling global spatial organization and precise geometric detail. However, existing approaches operate in raster space and rely on post hoc vectorization, which introduces structural inconsistencies and hinders end-to-end learning. Motivated by compositional spatial reasoning, we propose TLC-Plan, a hierarchical generative model that directly synthesizes vector floorplans from input boundaries, aligning with human architectural workflows based on modular and reusable patterns. TLC-Plan employs a two-level VQ-VAE to encode global layouts as semantically labeled room bounding boxes and to refine local geometries using polygon-level codes. This hierarchy is unified in a CodeTree representation, while an autoregressive transformer samples codes conditioned on the boundary to generate diverse and topologically valid designs, without requiring explicit room topology or dimensional priors. Extensive experiments show state-of-the-art performance on RPLAN dataset (FID = 1.84, MSE = 2.06) and leading results on LIFULL dataset. The proposed framework advances constraint-aware and scalable vector floorplan generation for real-world architectural applications. Source code and trained models are released at https://github.com/rosolose/TLC-PLAN.

</details>


### [29] [Zero-Shot UAV Navigation in Forests via Relightable 3D Gaussian Splatting](https://arxiv.org/abs/2602.07101)
*Zinan Lv,Yeqian Qian,Chen Sang,Hao Liu,Danping Zou,Ming Yang*

Main category: cs.CV

TL;DR: 本研究提出了一种端到端的强化学习框架，用于无人机在未结构化户外环境中的导航。通过引入可光照调整的3D高斯点绘技术，结合环境光照的物理编辑，训练无人机直接将单目RGB图像转换为连续控制命令。在高保真模拟中进行训练，并通过多样化的合成光照条件提升鲁棒性，最终发布了无人机在复杂森林环境中以高达10 m/s的速度进行碰撞自由导航。


<details>
  <summary>Details</summary>
Motivation: 解决无人机在未结构化户外环境下导航的问题，以真实世界的光照进行训练，使策略能够更好地适应多变的光照条件，实现零样本适应。

Method: 提出了一种基于强化学习的无人机导航方法，使用可光照调整的3D高斯点绘技术和物理编辑光照进行训练，结合多样化光照条件来增强模型的鲁棒性。

Result: 该方法在高保真模拟中训练的无人机能够在复杂森林环境中以10 m/s的速度实现稳健、无碰撞的导航，对剧烈的光照变化具有显著的鲁棒性。

Conclusion: 该研究为无人机在未结构化户外环境中的导航提供了一种新的解决方案，通过物理可编辑的光照条件训练模型，实现了在多变光照条件下的鲁棒导航。

Abstract: UAV navigation in unstructured outdoor environments using passive monocular vision is hindered by the substantial visual domain gap between simulation and reality. While 3D Gaussian Splatting enables photorealistic scene reconstruction from real-world data, existing methods inherently couple static lighting with geometry, severely limiting policy generalization to dynamic real-world illumination. In this paper, we propose a novel end-to-end reinforcement learning framework designed for effective zero-shot transfer to unstructured outdoors. Within a high-fidelity simulation grounded in real-world data, our policy is trained to map raw monocular RGB observations directly to continuous control commands. To overcome photometric limitations, we introduce Relightable 3D Gaussian Splatting, which decomposes scene components to enable explicit, physically grounded editing of environmental lighting within the neural representation. By augmenting training with diverse synthesized lighting conditions ranging from strong directional sunlight to diffuse overcast skies, we compel the policy to learn robust, illumination-invariant visual features. Extensive real-world experiments demonstrate that a lightweight quadrotor achieves robust, collision-free navigation in complex forest environments at speeds up to 10 m/s, exhibiting significant resilience to drastic lighting variations without fine-tuning.

</details>


### [30] [Extended to Reality: Prompt Injection in 3D Environments](https://arxiv.org/abs/2602.07104)
*Zhuoheng Li,Ying Chen*

Main category: cs.CV

TL;DR: 该研究介绍了PI3D攻击，这是一种针对3D环境中的大型多模态语言模型的提示注入攻击，通过放置带有文本的物理对象而非数字图像编辑来实现。


<details>
  <summary>Details</summary>
Motivation: 由于多模态大型语言模型（MLLMs）在处理3D环境中的视觉输入时出现了新的攻击面，研究者寻求填补这一领域中关于提示注入攻击现有研究的空白。

Method: 研究者通过建模攻击者的物理攻击行为，并解决确定带入文本的3D对象姿态（位置和方向）的问题，来开发PI3D攻击。

Result: 实验结果表明，PI3D能够有效攻击多种大型多模态语言模型，即使在多样化的相机轨迹下也能发挥作用。此外，研究还揭示了现存防御策略的不足之处。

Conclusion: PI3D攻击展示了在3D物理环境中实施提示注入攻击的挑战性，强调了提高大型多模态语言模型安全性的重要性。

Abstract: Multimodal large language models (MLLMs) have advanced the capabilities to interpret and act on visual input in 3D environments, empowering diverse applications such as robotics and situated conversational agents. When MLLMs reason over camera-captured views of the physical world, a new attack surface emerges: an attacker can place text-bearing physical objects in the environment to override MLLMs' intended task. While prior work has studied prompt injection in the text domain and through digitally edited 2D images, it remains unclear how these attacks function in 3D physical environments. To bridge the gap, we introduce PI3D, a prompt injection attack against MLLMs in 3D environments, realized through text-bearing physical object placement rather than digital image edits. We formulate and solve the problem of identifying an effective 3D object pose (position and orientation) with injected text, where the attacker's goal is to induce the MLLM to perform the injected task while ensuring that the object placement remains physically plausible. Experiments demonstrate that PI3D is an effective attack against multiple MLLMs under diverse camera trajectories. We further evaluate existing defenses and show that they are insufficient to defend against PI3D.

</details>


### [31] [Ex-Omni: Enabling 3D Facial Animation Generation for Omni-modal Large Language Models](https://arxiv.org/abs/2602.07106)
*Haoyu Zhang,Zhipeng Li,Yiwen Guo,Tianshu Yu*

Main category: cs.CV

TL;DR: 该研究提出了一个名为Expressive Omni (Ex-Omni) 的框架，旨在增强大语言模型以支持同时处理语音和3D面部动画。该框架采用了一种新的机制来连接语言和时间动态，提高了模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型虽然表现出色，但在与语音和3D面部动画的自然交互方面存在挑战。Ex-Omni的目标是解决这一问题，以实现更好的多模态理解和生成。

Method: Ex-Omni通过解耦语义推理与时间生成过程，利用语音单元作为时间支撑结构，并引入了一种统一的基于查询的门控融合机制，将语义信息按需注入模型。

Result: 实验结果表明，Ex-Omni不仅与现有的开源大语言模型相当，还能够稳定生成准确对齐的语音和面部动画。

Conclusion: 这项研究通过构建Ex-Omni框架，为三模态合成问题提供了一种新的解决方案，并为未来的研究奠定了基础。

Abstract: Omni-modal large language models (OLLMs) aim to unify multimodal understanding and generation, yet incorporating speech with 3D facial animation remains largely unexplored despite its importance for natural interaction. A key challenge arises from the representation mismatch between discrete, token-level semantic reasoning in LLMs and the dense, fine-grained temporal dynamics required for 3D facial motion, which makes direct modeling difficult to optimize under limited data. We propose Expressive Omni (Ex-Omni), an open-source omni-modal framework that augments OLLMs with speech-accompanied 3D facial animation. Ex-Omni reduces learning difficulty by decoupling semantic reasoning from temporal generation, leveraging speech units as temporal scaffolding and a unified token-as-query gated fusion (TQGF) mechanism for controlled semantic injection. We further introduce InstructEx, a dataset aims to facilitate augment OLLMs with speech-accompanied 3D facial animation. Extensive experiments demonstrate that Ex-Omni performs competitively against existing open-source OLLMs while enabling stable aligned speech and facial animation generation.

</details>


### [32] [Privacy in Image Datasets: A Case Study on Pregnancy Ultrasounds](https://arxiv.org/abs/2602.07149)
*Rawisara Lohanimit,Yankun Wu,Amelia Katirai,Yuta Nakashima,Noa Garcia*

Main category: cs.CV

TL;DR: 通过系统地检查使用CLIP嵌入相似性分析的LAION-400M数据集，发现包含孕期超声波图像和大量敏感个人信息（如姓名和位置）的照片。研究揭示了多个存在高风险信息的图像，这些信息可能被用于重新识别或冒名顶替。


<details>
  <summary>Details</summary>
Motivation: 由于大量未经过数据整理的互联网数据集的使用，生成模型的应用越来越多，研究团队关注其中可能包含的敏感或私人信息，尤其是在生活方式超声图像中收集的个人信息。

Method: 通过使用CLIP嵌入相似性的系统性检查来发现和识别数据集中的孕期超声波图像和个人信息。

Result: 在LAION-400M数据集中发现了数千个包含敏感个人信息的实体，如姓名和地点。多个图像包含高风险信息，可能被用于重新识别或冒名顶替。

Conclusion: 建议数据集整理、数据隐私和公共图像数据伦理使用方面的最佳实践。


Abstract: The rise of generative models has led to increased use of large-scale datasets collected from the internet, often with minimal or no data curation. This raises concerns about the inclusion of sensitive or private information. In this work, we explore the presence of pregnancy ultrasound images, which contain sensitive personal information and are often shared online. Through a systematic examination of LAION-400M dataset using CLIP embedding similarity, we retrieve images containing pregnancy ultrasound and detect thousands of entities of private information such as names and locations. Our findings reveal that multiple images have high-risk information that could enable re-identification or impersonation. We conclude with recommended practices for dataset curation, data privacy, and ethical use of public image datasets.

</details>


### [33] [DuMeta++: Spatiotemporal Dual Meta-Learning for Generalizable Few-Shot Brain Tissue Segmentation Across Diverse Ages](https://arxiv.org/abs/2602.07174)
*Yongheng Sun,Jun Shu,Jianhua Ma,Fan Wang*

Main category: cs.CV

TL;DR: 提出了DuMeta++框架，无需纵向配对数据即可进行无监督自适应脑组织分割，并在跨年龄段泛化方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前脑MRI图像分割算法在不同年龄段的表现存在差异，亟需一种方法在没有纵向配对数据的情况下，仍然能够进行有效的分割。

Method: DuMeta++框架包括了元特点学习和元初始化学习两个模块，通过记忆库意识的类别正则化策略，实现分割模型的数据高效适应。

Result: 实验结果表明，DuMeta++在包含iSeg-2019, IBIS, OASIS, ADNI等多样数据集的少样本设置下，比现有方法在跨年龄段泛化方面表现更优。

Conclusion: DuMeta++框架证明了在没有纵向配对数据的情况下进行无监督自适应脑组织分割的有效性。

Abstract: Accurate segmentation of brain tissues from MRI scans is critical for neuroscience and clinical applications, but achieving consistent performance across the human lifespan remains challenging due to dynamic, age-related changes in brain appearance and morphology. While prior work has sought to mitigate these shifts by using self-supervised regularization with paired longitudinal data, such data are often unavailable in practice. To address this, we propose \emph{DuMeta++}, a dual meta-learning framework that operates without paired longitudinal data. Our approach integrates: (1) meta-feature learning to extract age-agnostic semantic representations of spatiotemporally evolving brain structures, and (2) meta-initialization learning to enable data-efficient adaptation of the segmentation model. Furthermore, we propose a memory-bank-based class-aware regularization strategy to enforce longitudinal consistency without explicit longitudinal supervision. We theoretically prove the convergence of our DuMeta++, ensuring stability. Experiments on diverse datasets (iSeg-2019, IBIS, OASIS, ADNI) under few-shot settings demonstrate that DuMeta++ outperforms existing methods in cross-age generalization. Code will be available at https://github.com/ladderlab-xjtu/DuMeta++.

</details>


### [34] [Condition Matters in Full-head 3D GANs](https://arxiv.org/abs/2602.07198)
*Heyuan Li,Huimin Zhang,Yuda Qiu,Zhengwentai Sun,Keru Zheng,Lingteng Qiu,Peihao Li,Qi Zuo,Ce Chen,Yujian Zheng,Yuming Gu,Zilong Dong,Xiaoguang Han*

Main category: cs.CV

TL;DR: 本文提出了一种使用不变语义特征作为条件输入的方法，以解决3D GAN训练中因视角依赖导致的模式塌陷问题，通过创建合成头部图像数据集，确保生成能力与视角解耦，提高生成3D头部的真实性、多样性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 在没有条件信号的情况下，3D GAN训练容易发生模式塌陷，导致生成的3D头部质量差且不一致。传统的做法是使用视图角度作为条件输入，但这种做法会导致生成的空间沿视图条件方向产生偏差，影响头部不同区域的一致性。

Method: 本文基于FLUX.1 Kontext创建了具有不同视角范围的合成头部图像数据集。通过提取前方视角的图像片段特征，并将其作为所有视角的共享语义条件，从而消除视图方向上的偏见，实现跨视角的语义对齐，加快训练过程并提高生成的3D头部的整体一致性。同时，通过使用语义条件，鼓励生成器跟随真实语义分布，促进多样性的生成。

Result: 在全头部合成和单视角GAN逆渲染的广泛实验中，本文方法取得了显著的高真实度、多样性和泛化能力，证明了语义条件在提升3D GAN性能方面的有效性。

Conclusion: 本文提出的方法为解决3D GAN训练中的视角依赖问题提供了一个有效的解决方案，通过使用不变语义特征作为条件输入，确保了生成的3D头部在质量和多样性上的重大改进。

Abstract: Conditioning is crucial for stable training of full-head 3D GANs. Without any conditioning signal, the model suffers from severe mode collapse, making it impractical to training. However, a series of previous full-head 3D GANs conventionally choose the view angle as the conditioning input, which leads to a bias in the learned 3D full-head space along the conditional view direction. This is evident in the significant differences in generation quality and diversity between the conditional view and non-conditional views of the generated 3D heads, resulting in global incoherence across different head regions. In this work, we propose to use view-invariant semantic feature as the conditioning input, thereby decoupling the generative capability of 3D heads from the viewing direction. To construct a view-invariant semantic condition for each training image, we create a novel synthesized head image dataset. We leverage FLUX.1 Kontext to extend existing high-quality frontal face datasets to a wide range of view angles. The image clip feature extracted from the frontal view is then used as a shared semantic condition across all views in the extended images, ensuring semantic alignment while eliminating directional bias. This also allows supervision from different views of the same subject to be consolidated under a shared semantic condition, which accelerates training and enhances the global coherence of the generated 3D heads. Moreover, as GANs often experience slower improvements in diversity once the generator learns a few modes that successfully fool the discriminator, our semantic conditioning encourages the generator to follow the true semantic distribution, thereby promoting continuous learning and diverse generation. Extensive experiments on full-head synthesis and single-view GAN inversion demonstrate that our method achieves significantly higher fidelity, diversity, and generalizability.

</details>


### [35] [Understanding Real-World Traffic Safety through RoadSafe365 Benchmark](https://arxiv.org/abs/2602.07212)
*Xinyu Liu,Darryl C. Jacob,Yuxin Liu,Xinsong Du,Muchao Ye,Bolei Zhou,Pan He*

Main category: cs.CV

TL;DR: RoadSafe365 是一个大规模的视觉-语言基准，专为细粒度分析交通安全性而设计。它涵盖了广泛的环境背景和互动场景，并提供了36,196个标注片段，以及多种问答和详细场景描述，旨在推动实时交通安全性分析的可重复研究。


<details>
  <summary>Details</summary>
Motivation: 现有的交通基准一般缺乏系统地评估与官方安全标准对齐的功能。RoadSafe365 旨在通过引入一个专用的基准，填补这一空白，从而实现交通安全性分析的系统性评估。

Method: RoadSafe365 通过对数据进行独立编目和系统组织，并使用分层分类法细化和扩展了有关碰撞、事件和违规的基本定义，将官方交通安全性标准与数据驱动的交通理解系统相衔接。

Result: RoadSafe365 提供了涵盖多样交通事件类型、环境背景和互动场景的丰富属性注解，以及36,196个标注片段和864K候选选项、8,400个独特答案及36,000份详细场景描述，用于视觉-语言理解和推理。

Conclusion: RoadSafe365 设计用于大规模训练和标准化评估，旨在促进真实世界交通安全分析的可重复研究，并在不同领域的实验中显示了有效的支持。

Abstract: Although recent traffic benchmarks have advanced multimodal data analysis, they generally lack systematic evaluation aligned with official safety standards. To fill this gap, we introduce RoadSafe365, a large-scale vision-language benchmark that supports fine-grained analysis of traffic safety from extensive and diverse real-world video data collections. Unlike prior works that focus primarily on coarse accident identification, RoadSafe365 is independently curated and systematically organized using a hierarchical taxonomy that refines and extends foundational definitions of crash, incident, and violation to bridge official traffic safety standards with data-driven traffic understanding systems. RoadSafe365 provides rich attribute annotations across diverse traffic event types, environmental contexts, and interaction scenarios, yielding 36,196 annotated clips from both dashcam and surveillance cameras. Each clip is paired with multiple-choice question-answer sets, comprising 864K candidate options, 8.4K unique answers, and 36K detailed scene descriptions collectively designed for vision-language understanding and reasoning. We establish strong baselines and observe consistent gains when fine-tuning on RoadSafe365. Cross-domain experiments on both real and synthetic datasets further validate its effectiveness. Designed for large-scale training and standardized evaluation, RoadSafe365 provides a comprehensive benchmark to advance reproducible research in real-world traffic safety analysis.

</details>


### [36] [The Double-Edged Sword of Data-Driven Super-Resolution: Adversarial Super-Resolution Models](https://arxiv.org/abs/2602.07251)
*Haley Duba-Sullivan,Steven R. Young,Emma J. Reid*

Main category: cs.CV

TL;DR: AdvSR是一个框架，它可以在训练阶段直接嵌入到SR模型权重中的对抗行为，从而在不依赖输入的情况下引发下游分类任务的误分类。该研究评估了三种SR架构，并展示了即使在质量略有下降的情况下，AdvSR模型也能实现高性能的攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有数据驱动的超分辨率模型在成像管道中增加了一个新的攻击面，可能影响下游任务如分类和检测的准确性。本文旨在解决这一安全问题，通过在训练过程中嵌入对抗性行为来防御这些攻击。

Method: 通过联合优化重建质量和针对特定攻击目标的结果，AdvSR在SR模型中嵌入了对抗性行为。研究采用了SRCNN、EDSR和SwinIR三种超分辨率架构，并与其所指的YOLOv11分类器进行了评估。

Result: 实验表明，即使在质量略有下降的情况下，AdvSR模型也能实现高性能的攻击成功率。具体来说，通过这种新方法，模型在不使用特定输入触发的情况下，也能在保证图像质量的情况下使分类器发生错误。

Conclusion: 研究发现了成像管道中一个新的模型层面威胁，强调了在安全关键应用中如何选择和验证模型的重要性，这也是未来研究需要关注的一个方向。

Abstract: Data-driven super-resolution (SR) methods are often integrated into imaging pipelines as preprocessing steps to improve downstream tasks such as classification and detection. However, these SR models introduce a previously unexplored attack surface into imaging pipelines. In this paper, we present AdvSR, a framework demonstrating that adversarial behavior can be embedded directly into SR model weights during training, requiring no access to inputs at inference time. Unlike prior attacks that perturb inputs or rely on backdoor triggers, AdvSR operates entirely at the model level. By jointly optimizing for reconstruction quality and targeted adversarial outcomes, AdvSR produces models that appear benign under standard image quality metrics while inducing downstream misclassification. We evaluate AdvSR on three SR architectures (SRCNN, EDSR, SwinIR) paired with a YOLOv11 classifier and demonstrate that AdvSR models can achieve high attack success rates with minimal quality degradation. These findings highlight a new model-level threat for imaging pipelines, with implications for how practitioners source and validate models in safety-critical applications.

</details>


### [37] [3D Transport-based Morphometry (3D-TBM) for medical image analysis](https://arxiv.org/abs/2602.07260)
*Hongyu Kan,Kristofor Pas,Ivan Medri,Naqib Sad Pathan,Natasha Ironside,Shinjini Kundu,Jingjia He,Gustavo Kunde Rohde*

Main category: cs.CV

TL;DR: 3D-TBM 是一个用于 3D 医学图像形态学分析的工具，它通过变形将图像映射到运输域，并提供预处理、最优运输嵌入和可视化等功能。


<details>
  <summary>Details</summary>
Motivation: 由于 3D-TBM 能够将分析结果投影回原始图像空间，使得研究人员可以直接以空间有意义的方式解读临床特征，因此为了促进其在临床医学研究中的广泛应用，作者开发了此工具。

Method: 3D-TBM 包括数据预处理、计算最优运输嵌入以及分析方法，如主运输方向可视化和区分方向检索等技术。

Result: 通过提供全面的文档和实际教程，3D-TBM 使研究人员能够轻松地将其应用于自己的医学影像研究中。源代码通过 PyTransKit 公开可用。

Conclusion: 3D-TBM 为 3D 医学图像的形态学研究提供了一个强大的新框架，使研究结果更具临床相关性。

Abstract: Transport-Based Morphometry (TBM) has emerged as a new framework for 3D medical image analysis. By embedding images into a transport domain via invertible transformations, TBM facilitates effective classification, regression, and other tasks using transport-domain features. Crucially, the inverse mapping enables the projection of analytic results back into the original image space, allowing researchers to directly interpret clinical features associated with model outputs in a spatially meaningful way. To facilitate broader adoption of TBM in clinical imaging research, we present 3D-TBM, a tool designed for morphological analysis of 3D medical images. The framework includes data preprocessing, computation of optimal transport embeddings, and analytical methods such as visualization of main transport directions, together with techniques for discerning discriminating directions and related analysis methods. We also provide comprehensive documentation and practical tutorials to support researchers interested in applying 3D-TBM in their own medical imaging studies. The source code is publicly available through PyTransKit.

</details>


### [38] [TwistNet-2D: Learning Second-Order Channel Interactions via Spiral Twisting for Texture Recognition](https://arxiv.org/abs/2602.07262)
*Junbo Jacob Lian,Feng Xiong,Yujun Sun,Kaichen Ouyang,Mingyang Yu,Shengwei Fu,Zhong Rui,Zhang Yujun,Huiling Chen*

Main category: cs.CV

TL;DR: 本文提出TwistNet-2D，一种轻量级模块，能够计算方向性空间位移下的局部对渠道产品，同时编码特征共现的位置和交互方式，与现有方法相比，在参数和计算量都较小的情况下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法在纹理识别上存在权衡，既要捕捉全局通道相关性，又要保留空间结构，同时在特征交互方面也存在问题。本文旨在提供一种解决上述问题的新方法。

Method: 引入了Spiral-Twisted Channel Interaction (STCI)核心模块，该模块沿着指定方向移动一个特征图，进行通道ewise乘法，以捕捉结构和周期性纹理的交叉位置共现模式。将四个方向性的STCI头部聚合，并通过学习到的通道重新加权和Sigmoid门控残差路径注入结果。

Result: TwistNet仅比ResNet-18增加3.5%的参数和2%的FLOPs，在四个纹理和细粒度识别基准测试中始终超越参数匹配和更大规模的基线模型，包括ConvNeXt、Swin Transformer和混合CNN-Transformer架构。

Conclusion: 本文提出了TwistNet-2D，该模块在保留空间结构的同时有效捕捉特征之间复杂的交互模式，在多种基准测试中展现了出色的性能。

Abstract: Second-order feature statistics are central to texture recognition, yet current methods face a fundamental tension: bilinear pooling and Gram matrices capture global channel correlations but collapse spatial structure, while self-attention models spatial context through weighted aggregation rather than explicit pairwise feature interactions. We introduce TwistNet-2D, a lightweight module that computes \emph{local} pairwise channel products under directional spatial displacement, jointly encoding where features co-occur and how they interact. The core component, Spiral-Twisted Channel Interaction (STCI), shifts one feature map along a prescribed direction before element-wise channel multiplication, thereby capturing the cross-position co-occurrence patterns characteristic of structured and periodic textures. Aggregating four directional heads with learned channel reweighting and injecting the result through a sigmoid-gated residual path, \TwistNet incurs only 3.5% additional parameters and 2% additional FLOPs over ResNet-18, yet consistently surpasses both parameter-matched and substantially larger baselines -- including ConvNeXt, Swin Transformer, and hybrid CNN--Transformer architectures -- across four texture and fine-grained recognition benchmarks.

</details>


### [39] [Cross-View World Models](https://arxiv.org/abs/2602.07277)
*Rishabh Sharma,Gijs Hogervorst,Wayne E. Mackey,David J. Heeger,Stefano Martiniani*

Main category: cs.CV

TL;DR: 介绍了一种名为XVWM的跨视角世界模型，通过多视角预测目标，学习环境的不变表示，增强规划能力，并可能为多智能体环境中的视角理解提供基础。


<details>
  <summary>Details</summary>
Motivation: 现有的单一视角世界模型限制了智能体的规划能力，尤其是在导航时需要全局视角。XVWM旨在通过跨视角预测来解决这一问题。

Method: XVWM使用一种跨视角预测目标进行训练，即从一个视角预测在采取动作后另一个视角或同一视角未来的状态。数据来自Aimlabs，一种提供精确对齐多摄像头录制和高频动作标签的平台。

Result: 实验表明，XVWM能够生成基于多视角的想象流，提高智能体的规划能力，并且跨视角一致性为学习空间相关表征提供了强大的学习信号。

Conclusion: XVWM证明了跨视角预测在增强学习智能体规划能力和理解多视角环境方面的潜力，可能有助于未来在多人环境中实现认知换位思考。

Abstract: World models enable agents to plan by imagining future states, but existing approaches operate from a single viewpoint, typically egocentric, even when other perspectives would make planning easier; navigation, for instance, benefits from a bird's-eye view. We introduce Cross-View World Models (XVWM), trained with a cross-view prediction objective: given a sequence of frames from one viewpoint, predict the future state from the same or a different viewpoint after an action is taken. Enforcing cross-view consistency acts as geometric regularization: because the input and output views may share little or no visual overlap, to predict across viewpoints, the model must learn view-invariant representations of the environment's 3D structure. We train on synchronized multi-view gameplay data from Aimlabs, an aim-training platform providing precisely aligned multi-camera recordings with high-frequency action labels. The resulting model gives agents parallel imagination streams across viewpoints, enabling planning in whichever frame of reference best suits the task while executing from the egocentric view. Our results show that multi-view consistency provides a strong learning signal for spatially grounded representations. Finally, predicting the consequences of one's actions from another viewpoint may offer a foundation for perspective-taking in multi-agent settings.

</details>


### [40] [Diabetic Retinopathy Lesion Segmentation through Attention Mechanisms](https://arxiv.org/abs/2602.07301)
*Aruna Jithesh,Chinmayi Karumuri,Venkata Kiran Reddy Kotha,Meghana Doddapuneni,Taehee Jeong*

Main category: cs.CV

TL;DR: 该研究针对糖尿病视网膜病变（DR）开发了一种基于深度学习的病变分割方法，通过集成注意力机制的DeepLab-V3+模型提高了多种DR相关病变的检测精度，特别是在微动脉瘤检测上有了显著提高。


<details>
  <summary>Details</summary>
Motivation: 糖尿病视网膜病变可能导致视力丧失甚至失明。早期检测对防止不可逆的视力损失至关重要。尽管已有许多基于深度学习的自动化算法用于DR筛查，但在病灶分割方面，其临床应用仍有限。

Method: 研究者采用DeepLab-V3+模型并集成注意力机制来提高DR相关病变的分割精度，对来自DDR数据集的757个视网膜图像进行了四种病变类型的分割：微动脉瘤、软渗出、硬渗出和出血。

Result: 与基线模型相比，注意力机制的引入使平均平均精确度（mAP）从0.3010提高到0.3326，平均交并比（IoU）从0.1791提高到0.1928，特别是在微动脉瘤检测上从0.0205提高到0.0763，这是一个临床上的显著改进。

Conclusion: 该研究通过注意力机制的DeepLab-V3+改进了DR的病变分割，特别是在微动脉瘤检测方面的提升，可用于辅助眼科医生进行DR的早期筛查。

Abstract: Diabetic Retinopathy (DR) is an eye disease which arises due to diabetes mellitus. It might cause vision loss and blindness. To prevent irreversible vision loss, early detection through systematic screening is crucial. Although researchers have developed numerous automated deep learning-based algorithms for DR screening, their clinical applicability remains limited, particularly in lesion segmentation. Our method provides pixel-level annotations for lesions, which practically supports Ophthalmologist to screen DR from fundus images. In this work, we segmented four types of DR-related lesions: microaneurysms, soft exudates, hard exudates, and hemorrhages on 757 images from DDR dataset. To enhance lesion segmentation, an attention mechanism was integrated with DeepLab-V3+. Compared to the baseline model, the Attention-DeepLab model increases mean average precision (mAP) from 0.3010 to 0.3326 and the mean Intersection over Union (IoU) from 0.1791 to 0.1928. The model also increased microaneurysm detection from 0.0205 to 0.0763, a clinically significant improvement. The detection of microaneurysms is the earliest visible symptom of DR.

</details>


### [41] [Optimization of Precipitate Segmentation Through Linear Genetic Programming of Image Processing](https://arxiv.org/abs/2602.07310)
*Kyle Williams,Andrew Seltzman*

Main category: cs.CV

TL;DR: 一种基于线性遗传编程的过滤和分割算法被开发出来，用于检测FIB截面显微照片中的析出相，此算法可以优化材料成分和加工参数的探索过程。


<details>
  <summary>Details</summary>
Motivation: 当前制造基于铌的铜合金的分析依赖于手动标注，由于微图中存在不同的对比度、噪声和图像伪影，导致合金开发速度减慢。此研究表明，通过使用线性遗传编程优化过滤和分割算法，可以克服这些难题，提高开发速度。

Method: 算法基于线性遗传编程，使用领域特定编程语言迭代解决方案，通过图像过滤块的列表和可调参数进行优化，生成可解释的MATLAB代码来表示图像过滤管道。

Result: 在理想条件下，该系统能够找到接近人类准确性的人工评价误差为1.8%的解决方案。优化的管道算法可以每秒处理约3.6百万像素的图像。这使得能够快速收敛到具有强韧性和低激活性的析出强化铜合金。

Conclusion: 该研究为合金开发提供了一种有效的工具，用于更快地迭代和进一步探索材料组成和加工空间，从而生产更适合增材制造聚变反应堆部件的材料。

Abstract: Current analysis of additive manufactured niobium-based copper alloys relies on hand annotation due to varying contrast, noise, and image artifacts present in micrographs, slowing iteration speed in alloy development. We present a filtering and segmentation algorithm for detecting precipitates in FIB cross-section micrographs, optimized using linear genetic programming (LGP), which accounts for the various artifacts. To this end, the optimization environment uses a domain-specific language for image processing to iterate on solutions. Programs in this language are a list of image-filtering blocks with tunable parameters that sequentially process an input image, allowing for reliable generation and mutation by a genetic algorithm. Our environment produces optimized human-interpretable MATLAB code representing an image filtering pipeline. Under ideal conditions--a population size of 60 and a maximum program length of 5 blocks--our system was able to find a near-human accuracy solution with an average evaluation error of 1.8% when comparing segmentations pixel-by-pixel to a human baseline using an XOR error evaluation. Our automation work enabled faster iteration cycles and furthered exploration of the material composition and processing space: our optimized pipeline algorithm processes a 3.6 megapixel image in about 2 seconds on average. This ultimately enables convergence on strong, low-activation, precipitation hardened copper alloys for additive manufactured fusion reactor parts.

</details>


### [42] [LUCID-SAE: Learning Unified Vision-Language Sparse Codes for Interpretable Concept Discovery](https://arxiv.org/abs/2602.07311)
*Difei Gu,Yunhe Gao,Gerasimos Chatzoudis,Zihan Dong,Guoning Zhang,Bangwei Guo,Yang Zhou,Mu Zhou,Dimitris Metaxas*

Main category: cs.CV

TL;DR: LUCID是一种统一的视觉-语言稀疏自编码器，学习共享的潜空间字典，同时为模态特定细节保留私有容量，通过学习最优运输匹配目标实现特征对齐，从而生成可解释的共享特征。


<details>
  <summary>Details</summary>
Motivation: 当前的稀疏自编码器（SAEs）在不同的数据表示空间中难以提供一致性解释，LUCID旨在解决这一问题，通过学习统一的视觉-语言潜空间字典，支持跨模态的概念发现，并增强对概念聚类问题的鲁棒性。

Method: LUCID通过将共享编码与学习到的最优运输匹配目标相结合，实现模态间特征对齐，不依赖标签实现了无监督学习。

Result: LUCID生成了可解释的共享特征，支持像素级的语义对应，建立了跨模态的神经元对应关系，并提高了基于相似性的评估中概念聚类问题的鲁棒性。

Conclusion: LUCID提出了一个自动字典解释管道，无需手动观测，说明其共享特征能够捕获超出对象的多样语义类别，包括动作、属性和抽象概念，展示了全面的可解释多模态表示方法。

Abstract: Sparse autoencoders (SAEs) offer a natural path toward comparable explanations across different representation spaces. However, current SAEs are trained per modality, producing dictionaries whose features are not directly understandable and whose explanations do not transfer across domains. In this study, we introduce LUCID (Learning Unified vision-language sparse Codes for Interpretable concept Discovery), a unified vision-language sparse autoencoder that learns a shared latent dictionary for image patch and text token representations, while reserving private capacity for modality-specific details. We achieve feature alignment by coupling the shared codes with a learned optimal transport matching objective without the need of labeling. LUCID yields interpretable shared features that support patch-level grounding, establish cross-modal neuron correspondence, and enhance robustness against the concept clustering problem in similarity-based evaluation. Leveraging the alignment properties, we develop an automated dictionary interpretation pipeline based on term clustering without manual observations. Our analysis reveals that LUCID's shared features capture diverse semantic categories beyond objects, including actions, attributes, and abstract concepts, demonstrating a comprehensive approach to interpretable multimodal representations.

</details>


### [43] [Row-Column Separated Attention Based Low-Light Image/Video Enhancement](https://arxiv.org/abs/2602.07428)
*Chengqi Dong,Zhiyuan Cao,Tuoshi Qi,Kexin Wu,Yixing Gao,Fan Tang*

Main category: cs.CV

TL;DR: 本文提出了一种名为行-列分离注意力模块（RCSA）的改进U-Net结构，以增强低光图像和视频。通过利用全局信息来指导局部信息，同时减少参数数量和计算量，实现了良好的去噪效果和细节保留。


<details>
  <summary>Details</summary>
Motivation: 现有U-Net结构在低光图像/视频增强时容易产生大量局部噪声并损失更多细节。传统的注意力机制虽然可以更好地关注全局信息，但会显著增加模型的参数数量和计算量。因此，本文提出了RCSA模块，以解决这个问题。

Method: RCSA模块将特征图的行和列的最大值与均值作为输入，通过较少的参数数量和计算量，同时利用全局信息来指导局部信息。

Result: 在LOL、MIT Adobe FiveK图像和SDSD视频数据集上的实验结果表明，与U-Net相比，本文提出的URCSA方法在去噪和保留图像细节方面具有更好的效果。

Conclusion: 本文通过提出RCSA模块和引入两种时序损失函数，实现了低光图像和视频的增强，并在多个数据集上验证了其有效性。

Abstract: U-Net structure is widely used for low-light image/video enhancement. The enhanced images result in areas with large local noise and loss of more details without proper guidance for global information. Attention mechanisms can better focus on and use global information. However, attention to images could significantly increase the number of parameters and computations. We propose a Row-Column Separated Attention module (RCSA) inserted after an improved U-Net. The RCSA module's input is the mean and maximum of the row and column of the feature map, which utilizes global information to guide local information with fewer parameters. We propose two temporal loss functions to apply the method to low-light video enhancement and maintain temporal consistency. Extensive experiments on the LOL, MIT Adobe FiveK image, and SDSD video datasets demonstrate the effectiveness of our approach. The code is publicly available at https://github.com/cq-dong/URCSA.

</details>


### [44] [Perspective-aware fusion of incomplete depth maps and surface normals for accurate 3D reconstruction](https://arxiv.org/abs/2602.07444)
*Ondrej Hlinka,Georg Kaniak,Christian Kapeller*

Main category: cs.CV

TL;DR: 本文提出了一种基于单视角相机的深度和法线图融合方法，该方法能够生成高精度的3D重建。


<details>
  <summary>Details</summary>
Motivation: 深度和法线图数据通过对结构光扫描和光照立体视觉的方法获取，利用这些数据进行3D表面重建时，需要一种能够处理视差投影效果的方法。

Method: 本文提出了一种视角感知的日志深度融合方法，该方法对现有的基于视角的深度法线融合进行改进，引入了视差投影的考虑。

Result: 实验结果表明，该方法能够生成高精度的3D重建，并有效地填补了深度数据的缺失。

Conclusion: 该研究表明，视角感知的深度法线融合是3D重建中非常有效的方法，尤其是在处理深度数据缺失时。

Abstract: We address the problem of reconstructing 3D surfaces from depth and surface normal maps acquired by a sensor system based on a single perspective camera. Depth and normal maps can be obtained through techniques such as structured-light scanning and photometric stereo, respectively. We propose a perspective-aware log-depth fusion approach that extends existing orthographic gradient-based depth-normals fusion methods by explicitly accounting for perspective projection, leading to metrically accurate 3D reconstructions. Additionally, the method handles missing depth measurements by leveraging available surface normal information to inpaint gaps. Experiments on the DiLiGenT-MV data set demonstrate the effectiveness of our approach and highlight the importance of perspective-aware depth-normals fusion.

</details>


### [45] [PTB-XL-Image-17K: A Large-Scale Synthetic ECG Image Dataset with Comprehensive Ground Truth for Deep Learning-Based Digitization](https://arxiv.org/abs/2602.07446)
*Naqcho Ali Mehdi*

Main category: cs.CV

TL;DR: PTB-XL-Image-17K 是一个包含 17,271 张高质量 12 通道 ECG 图像的真实合成数据集，涵盖五种互补的数据类型，并提供开源 Python 框架进行定制化生成。


<details>
  <summary>Details</summary>
Motivation: 当前在 ECG 电数字化进展中缺乏大型数据集，无法支持全面的标记和应用。为了填补这个空白，作者开发了 PTB-XL-Image-17K 数据集，旨在为 ECG 电数字化提供一个全面的解决方案。

Method: 该研究通过从 PTB-XL 信号库生成数据，使用开源 Python 框架实现了对 ECG 图像的高度定制化生成，包括控制纸速、电压标度、采样率等参数。

Result: PTB-XL-Image-17K 实现了 100% 的生成成功率，每样本平均处理时间为 1.35 秒，并可提供从 ECG 图像到时序信号的完整管道支持。

Conclusion: PTB-XL-Image-17K 数据集及其生成框架有望推动 ECG 电数字化研究的进步，为未来相关研究提供权威资源。

Abstract: Electrocardiogram (ECG) digitization-converting paper-based or scanned ECG images back into time-series signals-is critical for leveraging decades of legacy clinical data in modern deep learning applications. However, progress has been hindered by the lack of large-scale datasets providing both ECG images and their corresponding ground truth signals with comprehensive annotations. We introduce PTB-XL-Image-17K, a complete synthetic ECG image dataset comprising 17,271 high-quality 12-lead ECG images generated from the PTB-XL signal database. Our dataset uniquely provides five complementary data types per sample: (1) realistic ECG images with authentic grid patterns and annotations (50% with visible grid, 50% without), (2) pixel-level segmentation masks, (3) ground truth time-series signals, (4) bounding box annotations in YOLO format for both lead regions and lead name labels, and (5) comprehensive metadata including visual parameters and patient information. We present an open-source Python framework enabling customizable dataset generation with controllable parameters including paper speed (25/50 mm/s), voltage scale (5/10 mm/mV), sampling rate (500 Hz), grid appearance (4 colors), and waveform characteristics. The dataset achieves 100% generation success rate with an average processing time of 1.35 seconds per sample. PTB-XL-Image-17K addresses critical gaps in ECG digitization research by providing the first large-scale resource supporting the complete pipeline: lead detection, waveform segmentation, and signal extraction with full ground truth for rigorous evaluation. The dataset, generation framework, and documentation are publicly available at https://github.com/naqchoalimehdi/PTB-XL-Image-17K and https://doi.org/10.5281/zenodo.18197519.

</details>


### [46] [SoulX-FlashHead: Oracle-guided Generation of Infinite Real-time Streaming Talking Heads](https://arxiv.org/abs/2602.07449)
*Tan Yu,Qian Qiao,Le Shen,Ke Zhou,Jincheng Hu,Dian Sheng,Bo Hu,Haoming Qin,Jun Gao,Changhai Zhou,Shunshun Yin,Siyuan Liu*

Main category: cs.CV

TL;DR: 本文提出了一种名为SoulX-FlashHead的统一框架，该框架具有1.3亿参数，用于实时、无限长度的高保真流媒体视频生成。该框架通过 Streaming-Aware Spatiotemporal Pre-training 和 Oracle-Guided Bidirectional Distillation 机制解决了流媒体场景下的音频特征不稳定性问题以及长期自回归生成中的误差累积和身份漂移问题，同时使用的VividHead数据集提供了支持鲁棒训练的大量高质量 footage。


<details>
  <summary>Details</summary>
Motivation: 鉴于音频驱动的肖像生成中高保真视觉质量和低延迟流媒体之间的平衡难题，以及现有模型在计算成本和光硅部分表示之间的权衡，本文旨在提出一种统一的框架来解决这些问题。

Method: 本文提出了Streaming-Aware Spatiotemporal Pre-training 来解决音频特征的不稳定性问题，通过引入Temporal Audio Context Cache机制，确保从简短音频片段中稳定地提取特征；同时提出了Oracle-Guided Bidirectional Distillation 使用 ground-truth 运动先验来提供精确的物理指导，减少长期序列自回归生成中的误差累积和身份漂移。此外，该论文还介绍了VividHead数据集，以支持坚固的训练。

Result: 实验结果表明，SoulX-FlashHead 在HDTF和VFHQ基准上达到了最先进的性能。其Lite变体在单个NVIDIA RTX 4090上实现了96 FPS的推理速度，提供超快的交互体验，同时保持视觉连贯性。

Conclusion: 本文的成果验证了SoulX-FlashHead 实现了低延迟、高保真流媒体视频生成的目标，在实际应用中具有潜力。

Abstract: Achieving a balance between high-fidelity visual quality and low-latency streaming remains a formidable challenge in audio-driven portrait generation. Existing large-scale models often suffer from prohibitive computational costs, while lightweight alternatives typically compromise on holistic facial representations and temporal stability. In this paper, we propose SoulX-FlashHead, a unified 1.3B-parameter framework designed for real-time, infinite-length, and high-fidelity streaming video generation. To address the instability of audio features in streaming scenarios, we introduce Streaming-Aware Spatiotemporal Pre-training equipped with a Temporal Audio Context Cache mechanism, which ensures robust feature extraction from short audio fragments. Furthermore, to mitigate the error accumulation and identity drift inherent in long-sequence autoregressive generation, we propose Oracle-Guided Bidirectional Distillation, leveraging ground-truth motion priors to provide precise physical guidance. We also present VividHead, a large-scale, high-quality dataset containing 782 hours of strictly aligned footage to support robust training. Extensive experiments demonstrate that SoulX-FlashHead achieves state-of-the-art performance on HDTF and VFHQ benchmarks. Notably, our Lite variant achieves an inference speed of 96 FPS on a single NVIDIA RTX 4090, facilitating ultra-fast interaction without sacrificing visual coherence.

</details>


### [47] [SpatialReward: Bridging the Perception Gap in Online RL for Image Editing via Explicit Spatial Reasoning](https://arxiv.org/abs/2602.07458)
*Yancheng Long,Yankai Yang,Hongyang Wei,Wei Chen,Tianke Zhang,Haonan fan,Changyi Liu,Kaiyu Jiang,Jiankang Chen,Kaiyu Tang,Bin Wen,Fan Yang,Tingting Gao,Han Li,Shuo Yang*

Main category: cs.CV

TL;DR: SpatialReward 是一种采用显式空间推理来确保准确验证的奖励模型。它通过将推理锚定在预测的编辑区域来增强评估准确性，并在各种基准测试中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的评估器在复杂的图像编辑中经常遇到“注意力崩溃”的问题，这导致对细粒度细节的忽视和感知不准确，从而影响了评价的准确性。为了填补这个感知差距，作者提出了 SpatialReward 来通过显式空间推理来强制执行精确验证。

Method: SpatialReward 模型是通过对预测的编辑区域进行空间推理，并在一个包含 26 万条带空间意识的数据集上进行训练而建立的。这种方法增强了语义判断的准确性和具体性。

Result: SpatialReward 在 MMRB2 和 EditReward-Bench 上达到了最先进的性能，并在新提出的 MultiEditReward-Bench 上优于专有的评估器。此外，SpatialReward 还提升了在线 RL 中 OmniGen2 的表现，提高效果为 +0.90。

Conclusion: 空间推理对于实现有效的图像编辑对齐至关重要，而 SpatialReward 证明了这一点，展示了其在图像编辑领域的潜力。

Abstract: Online Reinforcement Learning (RL) offers a promising avenue for complex image editing but is currently constrained by the scarcity of reliable and fine-grained reward signals. Existing evaluators frequently struggle with a critical perception gap we term "Attention Collapse," where models neglect cross-image comparisons and fail to capture fine-grained details, resulting in inaccurate perception and miscalibrated scores. To address these limitations, we propose SpatialReward, a reward model that enforces precise verification via explicit spatial reasoning. By anchoring reasoning to predicted edit regions, SpatialReward grounds semantic judgments in pixel-level evidence, significantly enhancing evaluative accuracy. Trained on a curated 260k spatial-aware dataset, our model achieves state-of-the-art performance on MMRB2 and EditReward-Bench, and outperforms proprietary evaluators on our proposed MultiEditReward-Bench. Furthermore, SpatialReward serves as a robust signal in online RL, boosting OmniGen2 by +0.90 on GEdit-Bench--surpassing the leading discriminative model and doubling the gain of GPT-4.1 (+0.45). These results demonstrate that spatial reasoning is essential for unlocking effective alignment in image editing.

</details>


### [48] [GlobalWasteData: A Large-Scale, Integrated Dataset for Robust Waste Classification and Environmental Monitoring](https://arxiv.org/abs/2602.07463)
*Misbah Ijaz,Saif Ur Rehman Khan,Abd Ur Rehman,Tayyaba Asif,Sebastian Vollmer,Andreas Dengel,Muhammad Nabeel Asim*

Main category: cs.CV

TL;DR: 本研究通过整合多个公开的数据集创建了一个名为GlobalWasteData的大型数据集，包含89,807张图像，涵盖14个主要类别和68个子类别，旨在解决现有数据集的不一致性和偏差问题，促进环境监测、回收自动化和废物识别中的机器学习应用。


<details>
  <summary>Details</summary>
Motivation: 当前可用的垃圾分类数据集存在不一致、不完整且偏向特定环境的问题，这阻碍了模型的有效训练和应用。

Method: 研究者通过整合现有多个公开的数据集，创建了一个新的综合数据集GlobalWasteData，包含详细的类别标签，并进行了数据预处理，如质量筛选、去重和元数据生成。

Result: GlobalWasteData数据集具有统一的标签、增强的领域多样性和更平衡的类别代表，为环境监测、回收自动化和废物识别提供了强大的机器学习应用基础。

Conclusion: GlobalWasteData数据集已公开提供，以促进未来的科学研究和可重复性，有望推动相关领域的发展。

Abstract: The growing amount of waste is a problem for the environment that requires efficient sorting techniques for various kinds of waste. An automated waste classification system is used for this purpose. The effectiveness of these Artificial Intelligence (AI) models depends on the quality and accessibility of publicly available datasets, which provide the basis for training and analyzing classification algorithms. Although several public waste classification datasets exist, they remain fragmented, inconsistent, and biased toward specific environments. Differences in class names, annotation formats, image conditions, and class distributions make it difficult to combine these datasets or train models that generalize well to real world scenarios. To address these issues, we introduce the GlobalWasteData (GWD) archive, a large scale dataset of 89,807 images across 14 main categories, annotated with 68 distinct subclasses. We compile this novel integrated GWD archive by merging multiple publicly available datasets into a single, unified resource. This GWD archive offers consistent labeling, improved domain diversity, and more balanced class representation, enabling the development of robust and generalizable waste recognition models. Additional preprocessing steps such as quality filtering, duplicate removal, and metadata generation further improve dataset reliability. Overall, this dataset offers a strong foundation for Machine Learning (ML) applications in environmental monitoring, recycling automation, and waste identification, and is publicly available to promote future research and reproducibility.

</details>


### [49] [Thermal odometry and dense mapping using learned ddometry and Gaussian splatting](https://arxiv.org/abs/2602.07493)
*Tianhao Zhou,Yujia Chen,Zhihao Zhan,Yuhang Ming,Jianzhu Huai*

Main category: cs.CV

TL;DR: TOM-GS 是一种新颖的基于高斯斑点技术（GS）的热成像里程计和建图方法，结合了学习驱动的里程计和 GS 基础的密集建图，相比现有方法在多样数据集上表现出更高的精度。


<details>
  <summary>Details</summary>
Motivation: 传统的热成像里程计和建图方法往往缺乏对多样数据集的支持和指示力，而基于 GS 的方法因其高效和高质量的重建能力受到关注，研究人员提出 TOM-GS，旨在利用这些优势来提升热成像在机器人中的应用。

Method: TOM-GS 方法结合了基于学习的里程计与 GS 基础的密集建图，特别采用了专门为热成像设计的图像增强和单目深度集成技术。

Result: TOM-GS 在运动估计和新颖视角渲染实验中优于现有的学习驱动方法，验证了学习驱动管道对热成像里程计和密集重建的强健性。

Conclusion: TOM-GS 引入了一种新的方法，为热成像在机器人技术中的应用提供了强有力的工具，证明了结合 GS 技术和学习方法的有效性。

Abstract: Thermal infrared sensors, with wavelengths longer than smoke particles, can capture imagery independent of darkness, dust, and smoke. This robustness has made them increasingly valuable for motion estimation and environmental perception in robotics, particularly in adverse conditions. Existing thermal odometry and mapping approaches, however, are predominantly geometric and often fail across diverse datasets while lacking the ability to produce dense maps. Motivated by the efficiency and high-quality reconstruction ability of recent Gaussian Splatting (GS) techniques, we propose TOM-GS, a thermal odometry and mapping method that integrates learning-based odometry with GS-based dense mapping. TOM-GS is among the first GS-based SLAM systems tailored for thermal cameras, featuring dedicated thermal image enhancement and monocular depth integration. Extensive experiments on motion estimation and novel-view rendering demonstrate that TOM-GS outperforms existing learning-based methods, confirming the benefits of learning-based pipelines for robust thermal odometry and dense reconstruction.

</details>


### [50] [Learning Brain Representation with Hierarchical Visual Embeddings](https://arxiv.org/abs/2602.07495)
*Jiawen Zheng,Haonan Jia,Ming Li,Yuhui Zheng,Yufeng Zeng,Yang Gao,Chen Liang*

Main category: cs.CV

TL;DR: 该研究提出了一种新的脑-图像对齐策略，利用多种预训练视觉编码器捕捉层次化多尺度视觉表示，并通过对比学习目标实现有效的对齐，同时引入Fusion Prior增强跨模态的一致性。


<details>
  <summary>Details</summary>
Motivation: 当前的视觉解码方法大多侧重于高级语义特征，忽略了像素级细节，这限制了我们对人类视觉系统的理解。因此，需要一种综合多个预训练视觉编码器的方法，以捕获多层次的视觉表示，并通过对比学习目标实现有效的脑信号与视觉嵌入的对齐。

Method: 该研究提出了一种利用多个具有不同归纳偏差的预训练视觉编码器的脑-图像对齐策略，通过对比学习目标实现有效的对齐。同时引入了Fusion Prior，学习大规模视觉数据上的稳定映射，然后将脑特征与这个预先训练的先验匹配，以增强跨模态的一致性。

Result: 广泛的定量和定性实验表明，该方法在检索准确性和重建保真度之间实现了良好的平衡。

Conclusion: 该方法为理解人类视觉系统提供了一种新的途径，并为视觉解码领域带来了新的进展。

Abstract: Decoding visual representations from brain signals has attracted significant attention in both neuroscience and artificial intelligence. However, the degree to which brain signals truly encode visual information remains unclear. Current visual decoding approaches explore various brain-image alignment strategies, yet most emphasize high-level semantic features while neglecting pixel-level details, thereby limiting our understanding of the human visual system. In this paper, we propose a brain-image alignment strategy that leverages multiple pre-trained visual encoders with distinct inductive biases to capture hierarchical and multi-scale visual representations, while employing a contrastive learning objective to achieve effective alignment between brain signals and visual embeddings. Furthermore, we introduce a Fusion Prior, which learns a stable mapping on large-scale visual data and subsequently matches brain features to this pre-trained prior, thereby enhancing distributional consistency across modalities. Extensive quantitative and qualitative experiments demonstrate that our method achieves a favorable balance between retrieval accuracy and reconstruction fidelity.

</details>


### [51] [IM-Animation: An Implicit Motion Representation for Identity-decoupled Character Animation](https://arxiv.org/abs/2602.07498)
*Zhufeng Xu,Xuan Gao,Feng-Lin Liu,Haoxian Zhang,Zhixue Fang,Yu-Kun Lai,Xiaoqiang Liu,Pengfei Wan,Lin Gao*

Main category: cs.CV

TL;DR: 文章提出了一种新颖的隐式运动表示方法，通过将每帧运动压缩为紧凑的一维运动令牌来解决空间错配和身份信息泄漏的问题，并设计了一个时间一致性的掩码令牌重定向模块，增强了训练效率和生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法有效解决空间错配、身体比例变化、身份泄漏和运动与外观纠缠的问题。

Method: 提出了一种新颖的隐式运动表示方法，将每帧运动压缩为紧凑的一维运动令牌，并设计了一个时间一致性掩码令牌重定向模块。

Result: 三种训练策略的应用使生成能力和训练效率得到显著提升，实验结果表明该方法在多项指标上优于现有先进方法。

Conclusion: 该研究提出了一种有效的解决方案，用于改善基于视频扩散模型的字符动画的生成性能，提高了运动动画的一致性和可信度。

Abstract: Recent progress in video diffusion models has markedly advanced character animation, which synthesizes motioned videos by animating a static identity image according to a driving video. Explicit methods represent motion using skeleton, DWPose or other explicit structured signals, but struggle to handle spatial mismatches and varying body scales. %proportions. Implicit methods, on the other hand, capture high-level implicit motion semantics directly from the driving video, but suffer from identity leakage and entanglement between motion and appearance. To address the above challenges, we propose a novel implicit motion representation that compresses per-frame motion into compact 1D motion tokens. This design relaxes strict spatial constraints inherent in 2D representations and effectively prevents identity information leakage from the motion video. Furthermore, we design a temporally consistent mask token-based retargeting module that enforces a temporal training bottleneck, mitigating interference from the source images' motion and improving retargeting consistency. Our methodology employs a three-stage training strategy to enhance the training efficiency and ensure high fidelity. Extensive experiments demonstrate that our implicit motion representation and the propose IM-Animation's generative capabilities are achieve superior or competitive performance compared with state-of-the-art methods.

</details>


### [52] [Adaptive Image Zoom-in with Bounding Box Transformation for UAV Object Detection](https://arxiv.org/abs/2602.07512)
*Tao Wang,Chenyu Lin,Chenwei Tang,Jizhe Zhou,Deng Xiong,Jianan Li,Jian Zhao,Jiancheng Lv*

Main category: cs.CV

TL;DR: 提出了一种轻量级的偏移预测方案和基于框的目标函数，实现了无人机捕获图像的非均匀缩放，并通过特征变换将检测任务转移到缩放后的空间，显著提升了小目标的检测效果。


<details>
  <summary>Details</summary>
Motivation: 由于小型飞行无人机(UAV)捕获图像中的前景对象通常比普通场景图像中更小且更稀疏，现有的有效目标检测器难以捕捉到详细的特征，因此需要设计一种自适应缩放框架以提高目标检测的精准度。

Method: 该方法设计了轻量级的偏移预测方案结合新的基于框的目标函数，用于学习输入图像的非均匀缩放，并提出了一种角对齐边界框变换方法，将真实边界框和预测边界框在缩放空间和原空间之间进行转换。

Result: 在三个代表性无人机目标检测数据集（VisDrone、UAVDT、SeaDronesSee）上进行了广泛实验，证明了该方法的有效性和高效性，特别是在SeaDronesSee数据集上，使用Faster R-CNN模型，ZoomDet提供了超过8.4的绝对mAP增益，仅增加约3毫秒的延迟。

Conclusion: ZoomDet是一种适用于各种目标检测框架的架构独立方案，通过该方法显著提升了无人机捕获图像中小目标的检测性能。

Abstract: Detecting objects from UAV-captured images is challenging due to the small object size. In this work, a simple and efficient adaptive zoom-in framework is explored for object detection on UAV images. The main motivation is that the foreground objects are generally smaller and sparser than those in common scene images, which hinders the optimization of effective object detectors. We thus aim to zoom in adaptively on the objects to better capture object features for the detection task. To achieve the goal, two core designs are required: \textcolor{black}{i) How to conduct non-uniform zooming on each image efficiently? ii) How to enable object detection training and inference with the zoomed image space?} Correspondingly, a lightweight offset prediction scheme coupled with a novel box-based zooming objective is introduced to learn non-uniform zooming on the input image. Based on the learned zooming transformation, a corner-aligned bounding box transformation method is proposed. The method warps the ground-truth bounding boxes to the zoomed space to learn object detection, and warps the predicted bounding boxes back to the original space during inference. We conduct extensive experiments on three representative UAV object detection datasets, including VisDrone, UAVDT, and SeaDronesSee. The proposed ZoomDet is architecture-independent and can be applied to an arbitrary object detection architecture. Remarkably, on the SeaDronesSee dataset, ZoomDet offers more than 8.4 absolute gain of mAP with a Faster R-CNN model, with only about 3 ms additional latency. The code is available at https://github.com/twangnh/zoomdet_code.

</details>


### [53] [Evaluating Object-Centric Models beyond Object Discovery](https://arxiv.org/abs/2602.07532)
*Krishnakant Singh,Simone Schaub-Meyer,Stefan Roth*

Main category: cs.CV

TL;DR: 该研究通过引入SceneX光代表现模型，解决了现有基准在评估OCL模型表示有用性和定位准确性方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的基准评估方法过于单一，主要关注物体发现和简单推理任务，未能充分评估OCL模型在复杂任务中的表示能力和定位能力。

Method: 研究提出使用指令调优的多模态视觉语言模型（VLMs）作为评估器，跨多个视觉问答（VQA）数据集对OCL模型进行大规模基准测试，引入统一的评估任务和指标，同时评估定位和表示有用性。

Result: 研究展示了SceneX模型在多个VQA数据集上的表现，证明了使用统一评估方法的有效性。

Conclusion: 该研究为OCL模型评估提供了一种新方法，强调了表示有用性和定位能力的联合评估，有助于推动OCL领域的研究进步。

Abstract: Object-centric learning (OCL) aims to learn structured scene representations that support compositional generalization and robustness to out-of-distribution (OOD) data. However, OCL models are often not evaluated regarding these goals. Instead, most prior work focuses on evaluating OCL models solely through object discovery and simple reasoning tasks, such as probing the representation via image classification. We identify two limitations in existing benchmarks: (1) They provide limited insights on the representation usefulness of OCL models, and (2) localization and representation usefulness are assessed using disjoint metrics. To address (1), we use instruction-tuned VLMs as evaluators, enabling scalable benchmarking across diverse VQA datasets to measure how well VLMs leverage OCL representations for complex reasoning tasks. To address (2), we introduce a unified evaluation task and metric that jointly assess localization (where) and representation usefulness (what), thereby eliminating inconsistencies introduced by disjoint evaluation. Finally, we include a simple multi-feature reconstruction baseline as a reference point.

</details>


### [54] [Fine-Grained Cat Breed Recognition with Global Context Vision Transformer](https://arxiv.org/abs/2602.07534)
*Mowmita Parvin Hera,Md. Shahriar Mahmud Kallol,Shohanur Rahman Nirob,Md. Badsha Bulbul,Jubayer Ahmed,M. Zhourul Islam,Hazrat Ali,Mohammmad Farhad Bulbul*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习的方法，使用Oxford-IIIT Pet Dataset中高分辨率的各种家猫品种图像数据集，通过应用全球上下文视觉变压器(GCViT) - tiny架构来识别家猫品种，通过大量的数据增强技术提高了模型的泛化能力，实验结果表明该模型取得了优异的分类效果。


<details>
  <summary>Details</summary>
Motivation: 文章的动机在于通过深度学习方法解决家猫品种识别这一具有挑战性的问题，因其在毛皮图案、面部结构和颜色的细微差异上存在识别难度。

Method: 研究采用了Oxford-IIIT Pet Dataset数据集，并利用了Global Context Vision Transformer (GCViT) - tiny架构进行模型构建。为了提升模型的泛化能力，对数据进行了包括旋转、水平翻转和亮度调整在内的大量数据增强。

Result: 实验表明，GCViT-Tiny模型在测试集上的准确率达到了92.00%，验证集上的准确率为94.54%，展示了基于变压器的架构在细粒度图像分类任务中的有效性。

Conclusion: 该研究发现基于GCViT架构的模型在识别家猫品种方面具有较高的准确性，提出了潜在的应用场景，如兽医诊断、动物收容所提供服务和基于手机的品种识别系统，并提供了一个Hugging Face的Demo展示。

Abstract: Accurate identification of cat breeds from images is a challenging task due to subtle differences in fur patterns, facial structure, and color. In this paper, we present a deep learning-based approach for classifying cat breeds using a subset of the Oxford-IIIT Pet Dataset, which contains high-resolution images of various domestic breeds. We employed the Global Context Vision Transformer (GCViT) architecture-tiny for cat breed recognition. To improve model generalization, we used extensive data augmentation, including rotation, horizontal flipping, and brightness adjustment. Experimental results show that the GCViT-Tiny model achieved a test accuracy of 92.00% and validation accuracy of 94.54%. These findings highlight the effectiveness of transformer-based architectures for fine-grained image classification tasks. Potential applications include veterinary diagnostics, animal shelter management, and mobile-based breed recognition systems. We also provide a hugging face demo at https://huggingface.co/spaces/bfarhad/cat-breed-classifier.

</details>


### [55] [Beyond Core and Penumbra: Bi-Temporal Image-Driven Stroke Evolution Analysis](https://arxiv.org/abs/2602.07535)
*Md Sazidur Rahman,Kjersti Engan,Kathinka Dæhli Kurz,Mahdieh Khanmohammadi*

Main category: cs.CV

TL;DR: 该研究提出了一种双时间点分析框架，通过CTP和DWI数据，利用统计描述符、放射学纹理特征和深度特征嵌入来表征缺血组织，发现不同时间点上神经功能恢复和未恢复的区域在特征空间中有显著差异。


<details>
  <summary>Details</summary>
Motivation: 当前单时间点分割无法捕捉到缺血组织的生物异质性和时间演化情况，因此需要一种双时间点分析方法。

Method: 该方法采用CTP成像和随访DWI数据，结合统计描述符、放射学纹理特征和来自两种架构（mJ-Net和nnU-Net）的深度特征嵌入，从双时间点数据中提取特征，并进行空间对齐。基于T1和T2手动标记的交集构建六个区域，分析特性水平的代表。

Result: 在18例成功再灌注的患者中，研究发现区域级别的表示有显著的聚类现象。在时间点1中归类为半暗带或健康的区域，最终恢复的分类为相似的脑组织特征，而与梗死的边界区域形成明显的分群。GLCM和深度嵌入特征表明，半暗带区域的特征显著取决于最终状态，而这一差异在核心区域内不显著。mJ-Net等深度特征空间中，可挽救和不可挽救组织之间有明显分离。

Conclusion: 研究表明，嵌入式特征流形反映了组织的内在特征和状态的转变，为基于影像学的中风演变量化提供了重要见解。

Abstract: Computed tomography perfusion (CTP) at admission is routinely used to estimate the ischemic core and penumbra, while follow-up diffusion-weighted MRI (DWI) provides the definitive infarct outcome. However, single time-point segmentations fail to capture the biological heterogeneity and temporal evolution of stroke. We propose a bi-temporal analysis framework that characterizes ischemic tissue using statistical descriptors, radiomic texture features, and deep feature embeddings from two architectures (mJ-Net and nnU-Net). Bi-temporal refers to admission (T1) and post-treatment follow-up (T2). All features are extracted at T1 from CTP, with follow-up DWI aligned to ensure spatial correspondence. Manually delineated masks at T1 and T2 are intersected to construct six regions of interest (ROIs) encoding both initial tissue state and final outcome. Features were aggregated per region and analyzed in feature space. Evaluation on 18 patients with successful reperfusion demonstrated meaningful clustering of region-level representations. Regions classified as penumbra or healthy at T1 that ultimately recovered exhibited feature similarity to preserved brain tissue, whereas infarct-bound regions formed distinct groupings. Both baseline GLCM and deep embeddings showed a similar trend: penumbra regions exhibit features that are significantly different depending on final state, whereas this difference is not significant for core regions. Deep feature spaces, particularly mJ-Net, showed strong separation between salvageable and non-salvageable tissue, with a penumbra separation index that differed significantly from zero (Wilcoxon signed-rank test). These findings suggest that encoder-derived feature manifolds reflect underlying tissue phenotypes and state transitions, providing insight into imaging-based quantification of stroke evolution.

</details>


### [56] [LLM-Guided Diagnostic Evidence Alignment for Medical Vision-Language Pretraining under Limited Pairing](https://arxiv.org/abs/2602.07540)
*Huimin Yan,Liang Bai,Xian Yang,Long Chen*

Main category: cs.CV

TL;DR: 提出了一种基于LLM引导的诊断证据对齐方法（LGDEA），通过利用语言模型提取关键诊断证据并构造共享的诊断证据空间，实现模态间证据导向对齐，从而利用大量未配对的医学图像和报告数据，改善了医生地文本框定位、图像-文本检索和零样本分类等任务。


<details>
  <summary>Details</summary>
Motivation: 现有的基于CLIP的医疗视觉-语言预训练方法依赖大量配对数据，导致非诊断信息主导全局对齐或局部缺乏关键诊断证据，影响预训练模型的适用性。

Method: 开发了一种LGDEA方法，首先利用LLM从医学影像报告中提取关键诊断证据，构建共享的诊断证据空间，然后在此空间中实现跨模态对齐。

Result: 实验结果表明，该方法在医生地文本框定位、图像-文本检索和零样本分类等任务中取得了持续且显著的改进，甚至与依赖大量配对数据的预训练方法媲美。

Conclusion: 此方法通过优化预训练目标以更好地适应医疗诊断流程，有效利用未配对数据，提高了现有医学视觉-语言模型的性能。

Abstract: Most existing CLIP-style medical vision--language pretraining methods rely on global or local alignment with substantial paired data. However, global alignment is easily dominated by non-diagnostic information, while local alignment fails to integrate key diagnostic evidence. As a result, learning reliable diagnostic representations becomes difficult, which limits their applicability in medical scenarios with limited paired data. To address this issue, we propose an LLM-Guided Diagnostic Evidence Alignment method (LGDEA), which shifts the pretraining objective toward evidence-level alignment that is more consistent with the medical diagnostic process. Specifically, we leverage LLMs to extract key diagnostic evidence from radiology reports and construct a shared diagnostic evidence space, enabling evidence-aware cross-modal alignment and allowing LGDEA to effectively exploit abundant unpaired medical images and reports, thereby substantially alleviating the reliance on paired data. Extensive experimental results demonstrate that our method achieves consistent and significant improvements on phrase grounding, image--text retrieval, and zero-shot classification, and even rivals pretraining methods that rely on substantial paired data.

</details>


### [57] [MUFASA: A Multi-Layer Framework for Slot Attention](https://arxiv.org/abs/2602.07544)
*Sebastian Bock,Leonie Schüßler,Krishnakant Singh,Simone Schaub-Meyer,Stefan Roth*

Main category: cs.CV

TL;DR: MUFASA 提出了一种轻量级框架，结合了多层特征的信息进行槽注意机制，增强无监督对象分割的效果。


<details>
  <summary>Details</summary>
Motivation: 当前方法仅从预训练的视觉变换器（ViT）的最后一层获得槽表示，忽略了其他层中编码的丰富的语义信息。

Method: MUFASA 通过在 ViT 编码器的多个特征层上计算槽注意，充分利用各个层的语义丰富性。提出了一种融合策略，将多个层上的槽融合成统一的对象中心表示。

Result: 将 MUFASA 集成到现有的无监督对象分割方法中，能够提升分割结果，同时通过更短的训练时间和轻微的推理开销实现新的状态最佳。

Conclusion: MUFASA 是一种轻量化且易于插入的框架，提升了多种数据集上无监督对象分割的效果，具有良好的实际应用潜力。

Abstract: Unsupervised object-centric learning (OCL) decomposes visual scenes into distinct entities. Slot attention is a popular approach that represents individual objects as latent vectors, called slots. Current methods obtain these slot representations solely from the last layer of a pre-trained vision transformer (ViT), ignoring valuable, semantically rich information encoded across the other layers. To better utilize this latent semantic information, we introduce MUFASA, a lightweight plug-and-play framework for slot attention-based approaches to unsupervised object segmentation. Our model computes slot attention across multiple feature layers of the ViT encoder, fully leveraging their semantic richness. We propose a fusion strategy to aggregate slots obtained on multiple layers into a unified object-centric representation. Integrating MUFASA into existing OCL methods improves their segmentation results across multiple datasets, setting a new state of the art while simultaneously improving training convergence with only minor inference overhead.

</details>


### [58] [Revealing the Semantic Selection Gap in DINOv3 through Training-Free Few-Shot Segmentation](https://arxiv.org/abs/2602.07550)
*Hussni Mohd Zakir,Eric Tatt Wei Ho*

Main category: cs.CV

TL;DR: 该研究通过FSSDINO方法考察了冻结的DINOv3特征在无训练条件下的少样本语义分割能力，并发现最后一层特征表现优异，挑战传统选择方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 鉴于自监督Vision Transformers（如DINOv3）在密集视觉任务中的丰富特征表示，研究旨在评估这些模型在无监督条件下的少样本语义分割潜力，以填补现有基础上的语义选择间隙。

Method: 研究人员使用FSSDINO方法，该方法基于类特定原型和Gram矩阵修正，应用于DINOv3模型的最终基础层。并通过Oracle引导的分层分析，识别最优中间表示与标准最后一层特征之间的性能差距。

Result: 研究结果表明，这种简洁的方法在二分类、多分类和跨域（CDFSS）基准测试中表现出色，与涉及复杂解码器或测试时适应的专门方法竞争。此外，研究发现最后一层特征的表现优于当前无监督和支持导向的选择度量方法。

Conclusion: 研究提出“最后一层”作为一个误导性但强大的基准，揭示了基础模型中的语义选择差距，即传统启发式方法无法可靠地识别高保真特征。

Abstract: Recent self-supervised Vision Transformers (ViTs), such as DINOv3, provide rich feature representations for dense vision tasks. This study investigates the intrinsic few-shot semantic segmentation (FSS) capabilities of frozen DINOv3 features through a training-free baseline, FSSDINO, utilizing class-specific prototypes and Gram-matrix refinement. Our results across binary, multi-class, and cross-domain (CDFSS) benchmarks demonstrate that this minimal approach, applied to the final backbone layer, is highly competitive with specialized methods involving complex decoders or test-time adaptation. Crucially, we conduct an Oracle-guided layer analysis, identifying a significant performance gap between the standard last-layer features and globally optimal intermediate representations. We reveal a "Safest vs. Optimal" dilemma: while the Oracle proves higher performance is attainable, matching the results of compute-intensive adaptation methods, current unsupervised and support-guided selection metrics consistently yield lower performance than the last-layer baseline. This characterizes a "Semantic Selection Gap" in Foundation Models, a disconnect where traditional heuristics fail to reliably identify high-fidelity features. Our work establishes the "Last-Layer" as a deceptively strong baseline and provides a rigorous diagnostic of the latent semantic potentials in DINOv3.The code is publicly available at https://github.com/hussni0997/fssdino.

</details>


### [59] [FlexID: Training-Free Flexible Identity Injection via Intent-Aware Modulation for Text-to-Image Generation](https://arxiv.org/abs/2602.07554)
*Guandong Li,Yijun Ding*

Main category: cs.CV

TL;DR: FlexID 是一种创新的无需训练的框架，通过意图感知的调制解决了身份保真度和文本适应性之间的冲突，通过Context-Aware Adaptive Gating机制动态调节身份和视觉特征的权重。


<details>
  <summary>Details</summary>
Motivation: 现有的训练自由方法依赖于刚性的视觉特征注入，这导致了身份保真度和文本适应性之间的冲突。

Method: FlexID 通过将身份分解为语义身份投影器（SIP）和视觉特征锚（VFA），并引入了意图感知自适应门控（CAG）机制来解决上述冲突。

Result: 在 IBench 上的实验表明，FlexID 在身份一致性和文本依从性之间实现了最先进的平衡，为复杂的叙事生成提供了一个高效的解决方案。

Conclusion: FlexID 提供了一个新的视角来解决个人化文本到图像生成中的关键问题，并能有效地实现编辑意图和视觉特征之间的动态平衡。

Abstract: Personalized text-to-image generation aims to seamlessly integrate specific identities into textual descriptions. However, existing training-free methods often rely on rigid visual feature injection, creating a conflict between identity fidelity and textual adaptability. To address this, we propose FlexID, a novel training-free framework utilizing intent-aware modulation. FlexID orthogonally decouples identity into two dimensions: a Semantic Identity Projector (SIP) that injects high-level priors into the language space, and a Visual Feature Anchor (VFA) that ensures structural fidelity within the latent space. Crucially, we introduce a Context-Aware Adaptive Gating (CAG) mechanism that dynamically modulates the weights of these streams based on editing intent and diffusion timesteps. By automatically relaxing rigid visual constraints when strong editing intent is detected, CAG achieves synergy between identity preservation and semantic variation. Extensive experiments on IBench demonstrate that FlexID achieves a state-of-the-art balance between identity consistency and text adherence, offering an efficient solution for complex narrative generation.

</details>


### [60] [SIGMA: Selective-Interleaved Generation with Multi-Attribute Tokens](https://arxiv.org/abs/2602.07564)
*Xiaoyan Zhang,Zechen Bai,Haofan Wang,Yiren Song*

Main category: cs.CV

TL;DR: SIGMA是一种后训练框架，它引入了选择性多属性标记，旨在增强扩散变压器的多条件生成能力，提升可控性、跨条件一致性及视觉质量。


<details>
  <summary>Details</summary>
Motivation: 现有统一模型如Bagel虽然能有效对齐多视觉任务，但仍局限于单一条件输入，缺乏从多种异构源合成结果的灵活性。

Method: SIGMA通过引入选择性多属性标记（包括风格、内容、主题和身份标记），实现多条件的交错生成。通过在Bagel统一架构上进行后训练，SIGMA支持组合编辑、选择性属性转移和精细的多模态对齐。

Result: 经过广泛实验，SIGMA在多样编辑与生成任务中展示了更好的可控性、跨条件一致性和视觉质量，特别是在组合任务上相比Bagel取得了显著提升。

Conclusion: SIGMA框架显著提升了扩散模型在处理多条件编辑和生成任务时的性能，为生成模型的多模态应用提供了新的途径。

Abstract: Recent unified models such as Bagel demonstrate that paired image-edit data can effectively align multiple visual tasks within a single diffusion transformer. However, these models remain limited to single-condition inputs and lack the flexibility needed to synthesize results from multiple heterogeneous sources. We present SIGMA (Selective-Interleaved Generation with Multi-Attribute Tokens), a unified post-training framework that enables interleaved multi-condition generation within diffusion transformers. SIGMA introduces selective multi-attribute tokens, including style, content, subject, and identity tokens, which allow the model to interpret and compose multiple visual conditions in an interleaved text-image sequence. Through post-training on the Bagel unified backbone with 700K interleaved examples, SIGMA supports compositional editing, selective attribute transfer, and fine-grained multimodal alignment. Extensive experiments show that SIGMA improves controllability, cross-condition consistency, and visual quality across diverse editing and generation tasks, with substantial gains over Bagel on compositional tasks.

</details>


### [61] [Human Identification at a Distance: Challenges, Methods and Results on the Competition HID 2025](https://arxiv.org/abs/2602.07565)
*Jingzhe Ma,Meng Zhang,Jianlong Yu,Kun Liu,Zunxiao Xu,Xue Cheng,Junjie Zhou,Yanfei Wang,Jiahang Li,Zepeng Wang,Kazuki Osamura,Rujie Liu,Narishige Abe,Jingjie Wang,Shunli Zhang,Haojun Xie,Jiajun Wu,Weiming Wu,Wenxiong Kang,Qingshuo Gao,Jiaming Xiong,Xianye Ben,Lei Chen,Lichen Song,Junjian Cui,Haijun Xiong,Junhao Lu,Bin Feng,Mengyuan Liu,Ji Zhou,Baoquan Zhao,Ke Xu,Yongzhen Huang,Liang Wang,Manuel J Marin-Jimenez,Md Atiqur Rahman Ahad,Shiqi Yu*

Main category: cs.CV

TL;DR: 该摘要介绍了人类距离识别（HID）比赛的情况，包括比赛的背景、采用的数据集、比赛机制以及参赛者取得的成果，展示了算法在挑战性任务上的进步。


<details>
  <summary>Details</summary>
Motivation: 为了促进步态识别领域的发展，并提供一个公平的评估平台，国际人类距离识别竞赛（HID）每年举行，采用具有挑战性的数据集，以评估模型的跨域泛化能力。

Method: 竞赛采用数据集中的多种变化（如服装、携带的物品和视角），并使用不同的随机种子生成评估分割，防止过拟合。参赛队伍需要使用外部数据集进行训练。

Result: 尽管比赛难度增加，参赛者仍然取得了进一步的进展，最高准确率达到94.2%，打破了之前设定的记录。

Conclusion: 研究进一步分析了关键的技术趋势，并提出了步态识别领域未来研究的潜在方向。

Abstract: Human identification at a distance (HID) is challenging because traditional biometric modalities such as face and fingerprints are often difficult to acquire in real-world scenarios. Gait recognition provides a practical alternative, as it can be captured reliably at a distance. To promote progress in gait recognition and provide a fair evaluation platform, the International Competition on Human Identification at a Distance (HID) has been organized annually since 2020. Since 2023, the competition has adopted the challenging SUSTech-Competition dataset, which features substantial variations in clothing, carried objects, and view angles. No dedicated training data are provided, requiring participants to train their models using external datasets. Each year, the competition applies a different random seed to generate distinct evaluation splits, which reduces the risk of overfitting and supports a fair assessment of cross-domain generalization. While HID 2023 and HID 2024 already used this dataset, HID 2025 explicitly examined whether algorithmic advances could surpass the accuracy limits observed previously. Despite the heightened difficulty, participants achieved further improvements, and the best-performing method reached 94.2% accuracy, setting a new benchmark on this dataset. We also analyze key technical trends and outline potential directions for future research in gait recognition.

</details>


### [62] [Cross-Camera Cow Identification via Disentangled Representation Learning](https://arxiv.org/abs/2602.07566)
*Runcheng Wang,Yaru Chen,Guiguo Zhang,Honghua Jiang,Yongliang Qiao*

Main category: cs.CV

TL;DR: 本文提出了一种基于解耦表示学习的跨摄像头牛识别框架，通过Subspace Identifiability Guarantee (SIG)理论隔离稳定的身份相关生物特征，实现了在不同摄像头下的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的动物识别方法在受控、单摄像头环境下表现良好，但在新部署摄像头节点上会遇到诸如光照、背景、视角和影像特性差异导致的泛化性能下降问题。

Method: 该研究利用Subspace Identifiability Guarantee (SIG)理论，通过构建解耦表示学习模块将观察到的图像分解为多个正交的潜在子空间，有效隔离跨摄像头不变的身份相关生物特征。

Result: 通过对覆盖5个不同摄像头节点的高质量数据集进行实验，该方法在7项跨摄像头任务中平均准确率达到86.0%，显著优于单源基线模型（51.9%）和其他强基线方法（79.8%）。

Conclusion: 本研究提出了一个基于子空间理论特征解耦的跨摄像头牛识别框架，为智能农业环境下非接触技术的大规模应用提供了新思路。

Abstract: Precise identification of individual cows is a fundamental prerequisite for comprehensive digital management in smart livestock farming. While existing animal identification methods excel in controlled, single-camera settings, they face severe challenges regarding cross-camera generalization. When models trained on source cameras are deployed to new monitoring nodes characterized by divergent illumination, backgrounds, viewpoints, and heterogeneous imaging properties, recognition performance often degrades dramatically. This limits the large-scale application of non-contact technologies in dynamic, real-world farming environments. To address this challenge, this study proposes a cross-camera cow identification framework based on disentangled representation learning. This framework leverages the Subspace Identifiability Guarantee (SIG) theory in the context of bovine visual recognition. By modeling the underlying physical data generation process, we designed a principle-driven feature disentanglement module that decomposes observed images into multiple orthogonal latent subspaces. This mechanism effectively isolates stable, identity-related biometric features that remain invariant across cameras, thereby substantially improving generalization to unseen cameras. We constructed a high-quality dataset spanning five distinct camera nodes, covering heterogeneous acquisition devices and complex variations in lighting and angles. Extensive experiments across seven cross-camera tasks demonstrate that the proposed method achieves an average accuracy of 86.0%, significantly outperforming the Source-only Baseline (51.9%) and the strongest cross-camera baseline method (79.8%). This work establishes a subspace-theoretic feature disentanglement framework for collaborative cross-camera cow identification, offering a new paradigm for precise animal monitoring in uncontrolled smart farming environments.

</details>


### [63] [ViCA: Efficient Multimodal LLMs with Vision-Only Cross-Attention](https://arxiv.org/abs/2602.07574)
*Wenjie Liu,Hao Wu,Xin Qiu,Yingqi Fan,Yihan Zhang,Anhao Zhao,Yunpu Ma,Xiaoyu Shen*

Main category: cs.CV

TL;DR: ViCA 是一种 minimalist 多模态大型语言模型架构，通过仅在部分层中引入稀疏图示-文本交叉注意，而视觉模块绕过所有自注意和前馈层，从而保留了 98% 的基线准确性，同时将视觉方面的计算量减少到 4%，显著提高了性能和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大型语言模型采用密集的视觉处理方法，但在研究中发现视觉嵌入与语言空间已经很好地对齐，并且有效的视觉-语言交互仅发生在少量层中。

Method: 提出了一种称为 ViCA 的架构，其中视觉令牌不经过所有的自注意力和前馈层，仅在选定的层中通过稀疏的交叉注意力与文本进行交互。

Result: ViCA 在三个多模态大型语言模型基础结构上提供了显著的性能和效率提升，优于 26 个基于剪枝的基础模型，并在单批次和多批次推理中分别提高了 3.5 倍和 10 倍的推理速度。

Conclusion: ViCA 提供了一种硬件友好型的推理管道，显著降低了视觉定位的开销，甚至远优于纯文本 LLM，在未来的研究中具有巨大的潜力。

Abstract: Modern multimodal large language models (MLLMs) adopt a unified self-attention design that processes visual and textual tokens at every Transformer layer, incurring substantial computational overhead. In this work, we revisit the necessity of such dense visual processing and show that projected visual embeddings are already well-aligned with the language space, while effective vision-language interaction occurs in only a small subset of layers. Based on these insights, we propose ViCA (Vision-only Cross-Attention), a minimal MLLM architecture in which visual tokens bypass all self-attention and feed-forward layers, interacting with text solely through sparse cross-attention at selected layers. Extensive evaluations across three MLLM backbones, nine multimodal benchmarks, and 26 pruning-based baselines show that ViCA preserves 98% of baseline accuracy while reducing visual-side computation to 4%, consistently achieving superior performance-efficiency trade-offs. Moreover, ViCA provides a regular, hardware-friendly inference pipeline that yields over 3.5x speedup in single-batch inference and over 10x speedup in multi-batch inference, reducing visual grounding to near-zero overhead compared with text-only LLMs. It is also orthogonal to token pruning methods and can be seamlessly combined for further efficiency gains. Our code is available at https://github.com/EIT-NLP/ViCA.

</details>


### [64] [TeleBoost: A Systematic Alignment Framework for High-Fidelity, Controllable, and Robust Video Generation](https://arxiv.org/abs/2602.07595)
*Yuanzhi Liang,Xuan'er Wu,Yirui Liu,Yijie Fang,Yizhen Fan,Ke Hao,Rui Li,Ruiying Liu,Ziqi Ni,Peng Yu,Yanbo Wang,Haibin Huang,Qizhen Weng,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: 该报告提出了一种系统化的后训练框架，结合监督策略塑造、奖励驱动的强化学习和偏好优化，以提升生成视频的感知保真度、时间连贯性和指令遵从性，同时保持初始时的可控性。


<details>
  <summary>Details</summary>
Motivation: 针对预训练视频生成器在实际应用中的局限性，如高昂的回放成本、时间累积的失败模式以及反馈信号的异质性、不确定性以及弱区分性，该报告旨在建立一个稳定设计的后训练框架，以提升生成视频的质量和可控性。

Method: 该方法通过将监督策略塑造、奖励驱动的强化学习和偏好优化整合到一个稳定约束优化栈中，形成一种分阶段、诊断导向的优化过程。

Result: 该框架成功提升了一致性、时间连贯性和指令遵从性，同时保留了初始的可控性，并为实际部署场景下构建可扩展、稳定的后训练流水线提供了明确的蓝图。

Conclusion: 研究表明，这一后训练框架能够有效解决视频生成中的一些关键问题，提供了一种系统性改进途径，适用于广泛的视频生成应用场景。

Abstract: Post-training is the decisive step for converting a pretrained video generator into a production-oriented model that is instruction-following, controllable, and robust over long temporal horizons. This report presents a systematical post-training framework that organizes supervised policy shaping, reward-driven reinforcement learning, and preference-based refinement into a single stability-constrained optimization stack. The framework is designed around practical video-generation constraints, including high rollout cost, temporally compounding failure modes, and feedback that is heterogeneous, uncertain, and often weakly discriminative. By treating optimization as a staged, diagnostic-driven process rather than a collection of isolated tricks, the report summarizes a cohesive recipe for improving perceptual fidelity, temporal coherence, and prompt adherence while preserving the controllability established at initialization. The resulting framework provides a clear blueprint for building scalable post-training pipelines that remain stable, extensible, and effective in real-world deployment settings.

</details>


### [65] [Fine-R1: Make Multi-modal LLMs Excel in Fine-Grained Visual Recognition by Chain-of-Thought Reasoning](https://arxiv.org/abs/2602.07605)
*Hulingxiao He,Zijun Geng,Yuxin Peng*

Main category: cs.CV

TL;DR: Fine-R1 提出了一种针对细粒度视觉识别任务的训练框架，通过链式思维监督微调和三重增强策略优化模型，能够在少量标注数据支持下实现良好的细粒度识别性能。


<details>
  <summary>Details</summary>
Motivation: 现有模型如多模态大规模语言模型（MLLMs）虽然在粗粒度视觉任务上表现优秀，但在细粒度视觉识别任务上却常常难以匹配专门为对比任务设计的CLIP模型的表现，尤其是当需要在没有大量标注数据支持下解决未见过的细分类别的识别问题时更加困难。

Method: Fine-R1 使用了一种名为R1的训练框架，包含两种策略：一是链式思维监督微调（CoT Supervised Fine-tuning），构造一个高质量的数据集来指导模型以更开放的方式来分类；二是三重增强策略优化（Triplet Augmented Policy Optimization），对同一类别内的样本进行增强以提升模型的内类鲁棒性，以及跨类别样本进行增广以增强模型的判别能力。

Result: Fine-R1 在仅使用少量标注数据的训练下，展示了出色的表现，超越了现有的通用MLLMs、推理MLLMs以及对比CLIP模型，特别是在识别已见过和未见过的细分类别方面。

Conclusion: Fine-R1 为解决细粒度视觉识别任务提供了一种新的解决方案，通过精细化的训练框架能够在有限的数据支持下实现良好的识别效果，适用于知识密集型领域的应用。

Abstract: Any entity in the visual world can be hierarchically grouped based on shared characteristics and mapped to fine-grained sub-categories. While Multi-modal Large Language Models (MLLMs) achieve strong performance on coarse-grained visual tasks, they often struggle with Fine-Grained Visual Recognition (FGVR). Adapting general-purpose MLLMs to FGVR typically requires large amounts of annotated data, which is costly to obtain, leaving a substantial performance gap compared to contrastive CLIP models dedicated for discriminative tasks. Moreover, MLLMs tend to overfit to seen sub-categories and generalize poorly to unseen ones. To address these challenges, we propose Fine-R1, an MLLM tailored for FGVR through an R1-style training framework: (1) Chain-of-Thought Supervised Fine-tuning, where we construct a high-quality FGVR CoT dataset with rationales of "visual analysis, candidate sub-categories, comparison, and prediction", transition the model into a strong open-world classifier; and (2) Triplet Augmented Policy Optimization, where Intra-class Augmentation mixes trajectories from anchor and positive images within the same category to improve robustness to intra-class variance, while Inter-class Augmentation maximizes the response distinction conditioned on images across sub-categories to enhance discriminative ability. With only 4-shot training, Fine-R1 outperforms existing general MLLMs, reasoning MLLMs, and even contrastive CLIP models in identifying both seen and unseen sub-categories, showing promise in working in knowledge-intensive domains where gathering expert annotations for all sub-categories is arduous. Code is available at https://github.com/PKU-ICST-MIPL/FineR1_ICLR2026.

</details>


### [66] [HistoMet: A Pan-Cancer Deep Learning Framework for Prognostic Prediction of Metastatic Progression and Site Tropism from Primary Tumor Histopathology](https://arxiv.org/abs/2602.07608)
*Yixin Chen,Ziyu Su,Lingbin Meng,Elshad Hasanov,Wei Chen,Anil Parwani,M. Khalid Khan Niazi*

Main category: cs.CV

TL;DR: 本文提出了一种名为HistoMet的决策感知、概念对齐的MIL框架，用于从原发肿瘤的全切片图像中预测转移性结果。该框架通过预先训练的病理视觉-语言模型整合了语义定义和数据适应的转移性概念，提供了临床可解释的预测结果。


<details>
  <summary>Details</summary>
Motivation: 目前难以直接从组织病理学图像中预测原发肿瘤是否会转移以及转移的位置。本文旨在通过建模临床决策结构来解决这一问题，从而实现从组织病理学图像直接预测转移性进程和位点。

Method: HistoMet框架包含两个模块的预测管道：首先估计原发肿瘤转移的可能性，然后对于高风险病例进行转移位点的条件预测。该框架通过预先训练的病理视觉-语言模型整合了语义定义和数据适应的转移性概念，从而引导表示学习并提高临床可解释性。

Result: 在多机构泛癌种队列（6504名患者）上，HistoMet在95％高灵敏度的筛查条件下，显著降低了下游评估的工作量并保持了高转移风险召回率。在转移病例中，HistoMet的宏F1值为74.6（标准差1.3），宏一对多AUC为92.1。

Conclusion: 本文提出的HistoMet框架通过明确建模临床决策结构，实现了从原发肿瘤组织病理学图像直接进行转移性进程和位点预测的稳健且可部署的预测。

Abstract: Metastatic Progression remains the leading cause of cancer-related mortality, yet predicting whether a primary tumor will metastasize and where it will disseminate directly from histopathology remains a fundamental challenge. Although whole-slide images (WSIs) provide rich morphological information, prior computational pathology approaches typically address metastatic status or site prediction as isolated tasks, and do not explicitly model the clinically sequential decision process of metastatic risk assessment followed by downstream site-specific evaluation. To address this research gap, we present a decision-aware, concept-aligned MIL framework, HistoMet, for prognostic metastatic outcome prediction from primary tumor WSIs. Our proposed framework adopts a two-module prediction pipeline in which the likelihood of metastatic progression from the primary tumor is first estimated, followed by conditional prediction of metastatic site for high-risk cases. To guide representation learning and improve clinical interpretability, our framework integrates linguistically defined and data-adaptive metastatic concepts through a pretrained pathology vision-language model. We evaluate HistoMet on a multi-institutional pan-cancer cohort of 6504 patients with metastasis follow-up and site annotations. Under clinically relevant high-sensitivity screening settings (95 percent sensitivity), HistoMet significantly reduces downstream workload while maintaining high metastatic risk recall. Conditional on metastatic cases, HistoMet achieves a macro F1 of 74.6 with a standard deviation of 1.3 and a macro one-vs-rest AUC of 92.1. These results demonstrate that explicitly modeling clinical decision structure enables robust and deployable prognostic prediction of metastatic progression and site tropism directly from primary tumor histopathology.

</details>


### [67] [Uncovering Modality Discrepancy and Generalization Illusion for General-Purpose 3D Medical Segmentation](https://arxiv.org/abs/2602.07643)
*Yichi Zhang,Feiyang Xiao,Le Xue,Wenbo Zhang,Gang Feng,Chenguang Zheng,Yuan Qi,Yuan Cheng,Zixin Hu*

Main category: cs.CV

TL;DR: 该研究创建了UMD数据集，包含PET/CT和PET/MRI的全身扫描，旨在验证3D医学基础模型在不同成像模态之间的性能差异。研究发现，现有的3D基础模型在功能成像领域表现不佳，表明需要转向多模态训练和评估来改进模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有的3D医学基础模型主要在区域和结构成像中的验证，无法全面地评估其性能在功能性成像场景中的表现。因此，研究者需要一个更全面的数据集来进行评价，以推动模型向模态无差别方向发展。

Method: 研究者创建了UMD数据集，包含大量的全身PET/CT和PET/MRI扫描，并通过配对扫描的内部主题控制比较，分析各种3D分割基础模型在实际应用中的鲁棒性。

Result: 结果表明，现有3D基础模型在文献报告的基准测试中表现出色，但在实际应用中特别是在功能性成像领域表现不佳。这一结果提示研究者需要改进3D基础模型的设计，使其能够更好地适应多模态数据。

Conclusion: 研究显示现有的3D基础模型在功能成像领域表现不佳，需加强多模态训练和评估来提升模型性能。UMD数据集为未来研究提供了坚实的基础。

Abstract: While emerging 3D medical foundation models are envisioned as versatile tools with offer general-purpose capabilities, their validation remains largely confined to regional and structural imaging, leaving a significant modality discrepancy unexplored. To provide a rigorous and objective assessment, we curate the UMD dataset comprising 490 whole-body PET/CT and 464 whole-body PET/MRI scans ($\sim$675k 2D images, $\sim$12k 3D organ annotations) and conduct a thorough and comprehensive evaluation of representative 3D segmentation foundation models. Through intra-subject controlled comparisons of paired scans, we isolate imaging modality as the primary independent variable to evaluate model robustness in real-world applications. Our evaluation reveals a stark discrepancy between literature-reported benchmarks and real-world efficacy, particularly when transitioning from structural to functional domains. Such systemic failures underscore that current 3D foundation models are far from achieving truly general-purpose status, necessitating a paradigm shift toward multi-modal training and evaluation to bridge the gap between idealized benchmarking and comprehensive clinical utility. This dataset and analysis establish a foundational cornerstone for future research to develop truly modality-agnostic medical foundation models.

</details>


### [68] [Influence of Geometry, Class Imbalance and Alignment on Reconstruction Accuracy -- A Micro-CT Phantom-Based Evaluation](https://arxiv.org/abs/2602.07658)
*Avinash Kumar K M,Samarth S. Raut*

Main category: cs.CV

TL;DR: 本研究评估了重建管道中的误差，并探讨了不同划分算法和几何类型的使用。实验使用了SLA技术打印的球体、面罩和AAA，并通过微观CT进行扫描。结果表明，Otsu方法适用于所有几何类型，而Jaccard指数更适合评估薄壁结构的准确性。


<details>
  <summary>Details</summary>
Motivation: 由于3D模型的准确度取决于成像硬件、分割方法和网格处理技术等，需要探讨几何类型、类别不平衡、体素和点云对齐对准确度的影响，以及使用体素和表面基础的准确度度量来评估不同划分算法和几何类型的可行性。

Method: 实验使用了SLA技术打印球体、面罩和AAA，并通过微观CT进行扫描。采用GMM、Otsu和RG等方法进行分割，并使用KU算法将分割模型和参考模型对齐，通过计算Dice、Jaccard分数、精度等来量化比较。使用ICP方法将表面网格与参考网格对齐，计算切弗尔距离和平均Hausdorff距离以评估准确性。

Result: Otsu方法被证明是最合适的，AAA由于其薄壁和对齐问题导致重叠分数较低。类别不平衡对AAA的敏感性最高。对于球体，RG方法表现最佳；对于AAA，GMM和Otsu表现更好；面罩的表面是最易出错的。高体素准确度度量可能因类别不平衡和对齐敏感而具有误导性。Jaccard指数比Dice更严格，更适合评估薄壁结构的准确性。

Conclusion: 不同几何类型和分割算法的准确度取决于不同阶段错误的累积。确保体素和点云对齐是进行重建管道可靠评估的基础。

Abstract: The accuracy of the 3D models created from medical scans depends on imaging hardware, segmentation methods and mesh processing techniques etc. The effects of geometry type, class imbalance, voxel and point cloud alignment on accuracy remain to be thoroughly explored. This work evaluates the errors across the reconstruction pipeline and explores the use of voxel and surface-based accuracy metrics for different segmentation algorithms and geometry types. A sphere, a facemask, and an AAA were printed using the SLA technique and scanned using a micro-CT machine. Segmentation was performed using GMM, Otsu and RG based methods. Segmented and reference models aligned using the KU algorithm, were quantitatively compared to evaluate metrics like Dice and Jaccard scores, precision. Surface meshes were registered with reference meshes using an ICP-based alignment process. Metrics like chamfer distance, and average Hausdorff distance were evaluated. The Otsu method was found to be the most suitable method for all the geometries. AAA yielded low overlap scores due to its small wall thickness and misalignment. The effect of class imbalance on specificity was observed the most for AAA. Surface-based accuracy metrics differed from the voxel-based trends. The RG method performed best for sphere, while GMM and Otsu perform better for AAA. The facemask surface was most error-prone, possibly due to misalignment during the ICP process. Segmentation accuracy is a cumulative sum of errors across different stages of the reconstruction process. High voxel-based accuracy metrics may be misleading in cases of high class imbalance and sensitivity to alignment. The Jaccard index is found to be more stringent than the Dice and more suitable for accuracy assessment for thin-walled structures. Voxel and point cloud alignment should be ensured to make any reliable assessment of the reconstruction pipeline.

</details>


### [69] [Looking and Listening Inside and Outside: Multimodal Artificial Intelligence Systems for Driver Safety Assessment and Intelligent Vehicle Decision-Making](https://arxiv.org/abs/2602.07668)
*Ross Greer,Laura Fleig,Maitrayee Keskar,Erika Maquiling,Giovanni Tapia Lopez,Angel Martinez-Sanchez,Parthib Roy,Jake Rattigan,Mira Sur,Alejandra Vidrio,Thomas Marcotte,Mohan Trivedi*

Main category: cs.CV

TL;DR: 本文提出了将听觉模态纳入现有LILO框架的新方法，形成了L-LIO框架，增强了对驾驶员状态和环境理解的多模态传感器融合，指出了音频在车辆安全中的三个应用场景并讨论了相关挑战。


<details>
  <summary>Details</summary>
Motivation: 现有框架LILO虽能理解外部场景和驾驶员状态，但在动态的自主驾驶环境中需要考虑噪声、隐私及不同人群一致性的挑战。

Method: 本文通过引入音频信号，将听觉与视觉信息融合，形成L-LIO框架，用于监督学习、乘客语言指令及视觉信息不足的场景。

Result: 实验证明，L-LIO框架能够在复杂情况下提供更多安全相关信息，尤其是在视觉信号不足时通过音频来澄清指导和手势。

Conclusion: 对L-LIO框架进行更多研究以确保其在动态真实世界中的鲁棒性是未来的重要工作。

Abstract: The looking-in-looking-out (LILO) framework has enabled intelligent vehicle applications that understand both the outside scene and the driver state to improve safety outcomes, with examples in smart airbag deployment, takeover time prediction in autonomous control transitions, and driver attention monitoring. In this research, we propose an augmentation to this framework, making a case for the audio modality as an additional source of information to understand the driver, and in the evolving autonomy landscape, also the passengers and those outside the vehicle. We expand LILO by incorporating audio signals, forming the looking-and-listening inside-and-outside (L-LIO) framework to enhance driver state assessment and environment understanding through multimodal sensor fusion. We evaluate three example cases where audio enhances vehicle safety: supervised learning on driver speech audio to classify potential impairment states (e.g., intoxication), collection and analysis of passenger natural language instructions (e.g., "turn after that red building") to motivate how spoken language can interface with planning systems through audio-aligned instruction data, and limitations of vision-only systems where audio may disambiguate the guidance and gestures of external agents. Datasets include custom-collected in-vehicle and external audio samples in real-world environments. Pilot findings show that audio yields safety-relevant insights, particularly in nuanced or context-rich scenarios where sound is critical to safe decision-making or visual signals alone are insufficient. Challenges include ambient noise interference, privacy considerations, and robustness across human subjects, motivating further work on reliability in dynamic real-world contexts. L-LIO augments driver and scene understanding through multimodal fusion of audio and visual sensing, offering new paths for safety intervention.

</details>


### [70] [Vision and language: Novel Representations and Artificial intelligence for Driving Scene Safety Assessment and Autonomous Vehicle Planning](https://arxiv.org/abs/2602.07680)
*Ross Greer,Maitrayee Keskar,Angel Martinez-Sanchez,Parthib Roy,Shashank Shriram,Mohan Trivedi*

Main category: cs.CV

TL;DR: 本研究探讨了视觉语言模型在自动驾驶安全性评估和决策中的应用，通过轻量级的图像文本相似度方法进行场景级别的危险性筛查，以及在轨迹规划框架中利用场景级视觉语言嵌入，并研究了自然语言作为运动规划行为约束的可能性。


<details>
  <summary>Details</summary>
Motivation: 随着视觉语言模型（VLMs）的强大特性，研究如何在自动驾驶中利用其支持驾驶场景的安全评估与决策，以提升系统性能与安全性。

Method: 该研究采用了三种方法：一是通过CLIP模型实现图像文本相似度进行危险性筛查；二是将场景级的视觉语言嵌入引入基于Transformer的轨迹规划框架；三是利用自然语言作为运动规划的显式行为约束。

Result: 结果表明，使用全球嵌入直接调整规划者并不提高轨迹准确度，强调了表示任务对齐的重要性，并指出了任务导向的提取方法在安全规划中的必要性。此外，通过将自然语言作为行为约束，提高了复杂场景中的行为安全性。

Conclusion: 研究表明，视觉语言表示在自动驾驶安全性方面具有显著潜力，可用于表达语义风险、意图及行为约束。然而，实现这一潜力需要精密的系统设计和结构化联系，而非直接特征注入。

Abstract: Vision-language models (VLMs) have recently emerged as powerful representation learning systems that align visual observations with natural language concepts, offering new opportunities for semantic reasoning in safety-critical autonomous driving. This paper investigates how vision-language representations support driving scene safety assessment and decision-making when integrated into perception, prediction, and planning pipelines. We study three complementary system-level use cases. First, we introduce a lightweight, category-agnostic hazard screening approach leveraging CLIP-based image-text similarity to produce a low-latency semantic hazard signal. This enables robust detection of diverse and out-of-distribution road hazards without explicit object detection or visual question answering. Second, we examine the integration of scene-level vision-language embeddings into a transformer-based trajectory planning framework using the Waymo Open Dataset. Our results show that naively conditioning planners on global embeddings does not improve trajectory accuracy, highlighting the importance of representation-task alignment and motivating the development of task-informed extraction methods for safety-critical planning. Third, we investigate natural language as an explicit behavioral constraint on motion planning using the doScenes dataset. In this setting, passenger-style instructions grounded in visual scene elements suppress rare but severe planning failures and improve safety-aligned behavior in ambiguous scenarios. Taken together, these findings demonstrate that vision-language representations hold significant promise for autonomous driving safety when used to express semantic risk, intent, and behavioral constraints. Realizing this potential is fundamentally an engineering problem requiring careful system design and structured grounding rather than direct feature injection.

</details>


### [71] [Process-of-Thought Reasoning for Videos](https://arxiv.org/abs/2602.07689)
*Jusheng Zhang,Kaitong Cai,Jian Wang,Yongsen Zheng,Kwok-Yan Lam,Keze Wang*

Main category: cs.CV

TL;DR: 提出一种名为Process-of-Thought (PoT) Reasoning for Videos的框架，旨在通过将视频推理过程分解为一系列轻量级且可验证的步骤来提高视频理解能力，包括时间证据选择、逐步状态更新和受限答案合成，并通过统一的表示方法提高解释的稳健性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的视频理解方法往往缺乏对推理过程的透明性，这限制了模型的准确性、可靠性和可解释性。PoT框架旨在通过明确推理过程，支持逐步推理和外部证据增强推理，增强模型性能。

Method: 框架通过以下步骤实现透明推理过程：首先选择时间证据，然后逐步更新状态，并结合外部工具的证据生成受限答案。通过这些步骤，模型能够逐步细化假设，同时保持对视频证据的追溯。

Result: 实验证明，PoT框架能够在标准视频推理任务中提高事实准确性及时间定位，且提供可解释的推理痕迹，有助于诊断和下游使用。

Conclusion: PoT框架代表了一种潜在有效的视频理解方法，特别适合用于需要多步推理和外部证据增强的应用场景。

Abstract: Video understanding requires not only recognizing visual content but also performing temporally grounded, multi-step reasoning over long and noisy observations. We propose Process-of-Thought (PoT) Reasoning for Videos, a framework that makes the reasoning process explicit by structuring video inference into a sequence of lightweight, verifiable steps. PoT interleaves (i) temporal evidence selection, (ii) step-wise state updates, and (iii) constrained answer synthesis, enabling the model to progressively refine hypotheses while maintaining traceability to video evidence. The framework is designed to be model-agnostic and can be plugged into existing vision-language backbones, supporting both closed-book reasoning and evidence-augmented reasoning with external tools. We further introduce a unified representation for PoT traces that aligns intermediate decisions with temporal segments, which improves robustness to distractors and reduces hallucinated explanations. Extensive experiments on standard video reasoning tasks demonstrate that PoT consistently improves factual correctness and temporal grounding, while providing interpretable reasoning traces for diagnosis and downstream use.

</details>


### [72] [A hybrid Kolmogorov-Arnold network for medical image segmentation](https://arxiv.org/abs/2602.07702)
*Deep Bhattacharyya,Ali Ayub,A. Ben Hamza*

Main category: cs.CV

TL;DR: U-KABS 提出了一种结合 KANs 和 U 型编码解码器的新型混合框架，以提高医学图像分割性能。


<details>
  <summary>Details</summary>
Motivation: 医学图像的复杂性和变化性使得分割任务极具挑战性，尤其是捕捉数据中的非线性关系。U-KABS 模型融合了 KANs 的表达能力和 U 型编码解码器的优势，旨在提高分割精度。

Method: U-KABS 模型结合了卷积和 squeeze-and-excitation 阶段，增强了通道特征表示，以及 KAN 伯恩斯坦样条 (KABS) 阶段，采用了基于伯恩斯坦多项式和 B-样条的可学习激活函数。

Result: U-KABS 在多种医学影像基准数据集上表现出优于强基线的性能，特别是在分段复杂解剖结构方面。

Conclusion: U-KABS 通过有效融合多尺度特征和保结构细节，提供了增强医学图像分割性能的方法。

Abstract: Medical image segmentation plays a vital role in diagnosis and treatment planning, but remains challenging due to the inherent complexity and variability of medical images, especially in capturing non-linear relationships within the data. We propose U-KABS, a novel hybrid framework that integrates the expressive power of Kolmogorov-Arnold Networks (KANs) with a U-shaped encoder-decoder architecture to enhance segmentation performance. The U-KABS model combines the convolutional and squeeze-and-excitation stage, which enhances channel-wise feature representations, and the KAN Bernstein Spline (KABS) stage, which employs learnable activation functions based on Bernstein polynomials and B-splines. This hybrid design leverages the global smoothness of Bernstein polynomials and the local adaptability of B-splines, enabling the model to effectively capture both broad contextual trends and fine-grained patterns critical for delineating complex structures in medical images. Skip connections between encoder and decoder layers support effective multi-scale feature fusion and preserve spatial details. Evaluated across diverse medical imaging benchmark datasets, U-KABS demonstrates superior performance compared to strong baselines, particularly in segmenting complex anatomical structures.

</details>


### [73] [Rolling Sink: Bridging Limited-Horizon Training and Open-Ended Testing in Autoregressive Video Diffusion](https://arxiv.org/abs/2602.07775)
*Haodong Li,Shaoteng Liu,Zhe Lin,Manmohan Chandraker*

Main category: cs.CV

TL;DR: Rolling Sink 是一种无需训练即可在内容、色彩、结构和运动方面保持一致性的超长视频生成方法，其效果在长时间视觉保真度和时间一致性方面优于当前最佳基线。


<details>
  <summary>Details</summary>
Motivation: 现有的自回归视频扩散模型在长时间段内表现出较大的性能差距，主要由于训练时间和测试时间的不同导致的画面质量快速下降。为了解决这一问题，Rolling Sink 提出了滚动沉淀的方法，在不进行额外训练的情况下，实现超长视频的自回归视频合成。

Method: Rolling Sink 方法基于 Self Forcing 模型进行构建，但针对更长时间段的目标，Rolling Sink 引入了滚动沉淀的概念，通过系统分析 AR 缓存保留，实现跨训练时间到无限测试时间的平滑过渡。

Result: Rolling Sink 能够在 5-30 分钟的超长时间内保持稳定的主题、色彩和结构，以及平滑的运动，相比当前最佳基线，它在长时间视觉保真度和时间一致性方面表现出色。

Conclusion: Rolling Sink 是一种有效的、无需训练的方法，能够在开放性测试条件下生成高质量的长视频，具有广阔的应用前景。

Abstract: Recently, autoregressive (AR) video diffusion models has achieved remarkable performance. However, due to their limited training durations, a train-test gap emerges when testing at longer horizons, leading to rapid visual degradations. Following Self Forcing, which studies the train-test gap within the training duration, this work studies the train-test gap beyond the training duration, i.e., the gap between the limited horizons during training and open-ended horizons during testing. Since open-ended testing can extend beyond any finite training window, and long-video training is computationally expensive, we pursue a training-free solution to bridge this gap. To explore a training-free solution, we conduct a systematic analysis of AR cache maintenance. These insights lead to Rolling Sink. Built on Self Forcing (trained on only 5s clips), Rolling Sink effectively scales the AR video synthesis to ultra-long durations (e.g., 5-30 minutes at 16 FPS) at test time, with consistent subjects, stable colors, coherent structures, and smooth motions. As demonstrated by extensive experiments, Rolling Sink achieves superior long-horizon visual fidelity and temporal consistency compared to SOTA baselines. Project page: https://rolling-sink.github.io/

</details>


### [74] [Uncertainty-Aware Counterfactual Traffic Signal Control with Predictive Safety and Starvation-Avoidance Constraints Using Vision-Based Sensing](https://arxiv.org/abs/2602.07784)
*Jayawant Bodagala,Balaji Bodagala*

Main category: cs.CV

TL;DR: UCATSC 是一种基于模型的交通信号控制系统，通过在信念空间中预测和强制执行与安全和饥饿预防相关的硬约束，以改进交通延误和排放，同时避免安全关键错误并提供可解释的控制策略。


<details>
  <summary>Details</summary>
Motivation: 目前，交通信号控制在实际部署中受到视觉感知不确定性、隐含安全性和主要在仿真中验证的不可解释控制策略的限制。UCATSC旨在通过考虑视觉感知不确定性来建模交叉口的交通信号控制，并在信念空间中预测和强制执行与安全和饥饿预防相关的硬约束，以提高实际应用中的性能。

Method: UCATSC 使用一个具有约束和部分可观测性的随机决策过程来建模交叉口的交通信号控制。在信念空间中的信念上执行反事实回放以预测和强制执行安全和饥饿预防相关的硬约束。

Result: UCATSC 在改进交通延误和排放的同时，避免了安全关键错误，并提供了基于显式模型的可解释控制策略输出。

Conclusion: UCATSC 通过预测和在信念空间中强制执行硬约束，提供了一种更可靠的交通信号控制方法，相比传统强化学习方法，能够更好地保证安全性和可解释性。

Abstract: Real-world deployment of adaptive traffic signal control, to date, remains limited due to the uncertainty associated with vision-based perception, implicit safety, and non-interpretable control policies learned and validated mainly in simulation. In this paper, we introduce UCATSC, a model-based traffic signal control system that models traffic signal control at an intersection using a stochastic decision process with constraints and under partial observability, taking into account the uncertainty associated with vision-based perception. Unlike reinforcement learning methods that learn to predict safety using reward shaping, UCATSC predicts and enforces hard constraints related to safety and starvation prevention during counterfactual rollouts in belief space. The system is designed to improve traffic delay and emission while preventing safety-critical errors and providing interpretable control policy outputs based on explicit models.

</details>


### [75] [VideoTemp-o3: Harmonizing Temporal Grounding and Video Understanding in Agentic Thinking-with-Videos](https://arxiv.org/abs/2602.07801)
*Wenqi Liu,Yunxiao Wang,Shijie Ma,Meng Liu,Qile Su,Tianke Zhang,Haonan Fan,Changyi Liu,Kaiyu Jiang,Jiankang Chen,Kaiyu Tang,Bin Wen,Fan Yang,Tingting Gao,Han Li,Yinwei Wei,Xuemeng Song*

Main category: cs.CV

TL;DR: 该研究提出了VideoTemp-o3框架，结合视频定位和问答，能够有效提高长视频理解性能，提升了定位精度，并支持按需剪辑和修正错误定位。


<details>
  <summary>Details</summary>
Motivation: 目前的长视频理解方法在剪辑定位和灵活性方面存在不足，为了解决这些问题，研究提出了一种新的框架VideoTemp-o3。

Method: VideoTemp-o3框架在监督微调阶段引入统一的遮掩机制来鼓励探索并防止噪声，同时在强化学习中引入专用奖励来减轻奖励劫持。此外，研究团队还开发了一种有效的数据构建流程，用于生成高质量的长视频定位问答数据，并建立了相应的基准进行跨长度视频的系统评估。

Result: 实验结果表明，该方法在长视频理解和定位方面取得了显著的性能提升。

Conclusion: 研究提出了VideoTemp-o3框架，展示了一种有效的应对长视频理解挑战的解决方案，并验证了其在实际应用中的优越性。

Abstract: In long-video understanding, conventional uniform frame sampling often fails to capture key visual evidence, leading to degraded performance and increased hallucinations. To address this, recent agentic thinking-with-videos paradigms have emerged, adopting a localize-clip-answer pipeline in which the model actively identifies relevant video segments, performs dense sampling within those clips, and then produces answers. However, existing methods remain inefficient, suffer from weak localization, and adhere to rigid workflows. To solve these issues, we propose VideoTemp-o3, a unified agentic thinking-with-videos framework that jointly models video grounding and question answering. VideoTemp-o3 exhibits strong localization capability, supports on-demand clipping, and can refine inaccurate localizations. Specifically, in the supervised fine-tuning stage, we design a unified masking mechanism that encourages exploration while preventing noise. For reinforcement learning, we introduce dedicated rewards to mitigate reward hacking. Besides, from the data perspective, we develop an effective pipeline to construct high-quality long video grounded QA data, along with a corresponding benchmark for systematic evaluation across various video durations. Experimental results demonstrate that our method achieves remarkable performance on both long video understanding and grounding.

</details>


### [76] [How well are open sourced AI-generated image detection models out-of-the-box: A comprehensive benchmark study](https://arxiv.org/abs/2602.07814)
*Simiao Ren,Yuchen Zhou,Xingyu Shen,Kidus Zewde,Tommy Duong,George Huang,Hatsanai,Tiangratanakul,Tsang,Ng,En Wei,Jiayu Xue*

Main category: cs.CV

TL;DR: 论文首次进行了零样本评价16种前沿检测方法及其23个预训练变体，覆盖2.6百万张图像样本，揭示多种人工智能生成模型对多数检测器具有较高挑战性。


<details>
  <summary>Details</summary>
Motivation: 鉴于AI生成图像的迅猛发展，现有检测方法主要针对微调模型，忽视了不经微调直接使用的实际部署需求。本研究旨在填补这一空白，评估现有多项检测方法的通用性能。

Method: 研究采用系统化方法，全面评价16种顶尖检测技术及其23个预训练变体，涵盖12个数据集，总计2.6百万张图像样本。

Result: 研究发现，无单一最优检测器；性能差距明显，最高75.0%，最低37.5%；性能差异受训练数据影响显著；现代商业生成模型对多数检测器构成巨大挑战。

Conclusion: 研究结果挑战了单一通用检测器的观点，强调了根据具体威胁环境精心选择合适检测器的重要性，为实际部署提供了指导。

Abstract: As AI-generated images proliferate across digital platforms, reliable detection methods have become critical for combating misinformation and maintaining content authenticity. While numerous deepfake detection methods have been proposed, existing benchmarks predominantly evaluate fine-tuned models, leaving a critical gap in understanding out-of-the-box performance -- the most common deployment scenario for practitioners. We present the first comprehensive zero-shot evaluation of 16 state-of-the-art detection methods, comprising 23 pretrained detector variants (due to multiple released versions of certain detectors), across 12 diverse datasets, comprising 2.6~million image samples spanning 291 unique generators including modern diffusion models. Our systematic analysis reveals striking findings: (1)~no universal winner exists, with detector rankings exhibiting substantial instability (Spearman~$ρ$: 0.01 -- 0.87 across dataset pairs); (2)~a 37~percentage-point performance gap separates the best detector (75.0\% mean accuracy) from the worst (37.5\%); (3)~training data alignment critically impacts generalization, causing up to 20--60\% performance variance within architecturally identical detector families; (4)~modern commercial generators (Flux~Dev, Firefly~v4, Midjourney~v7) defeat most detectors, achieving only 18--30\% average accuracy; and (5)~we identify three systematic failure patterns affecting cross-dataset generalization. Statistical analysis confirms significant performance differences between detectors (Friedman test: $χ^2$=121.01, $p<10^{-16}$, Kendall~$W$=0.524). Our findings challenge the ``one-size-fits-all'' detector paradigm and provide actionable deployment guidelines, demonstrating that practitioners must carefully select detectors based on their specific threat landscape rather than relying on published benchmark performance.

</details>


### [77] [Out of the box age estimation through facial imagery: A Comprehensive Benchmark of Vision-Language Models vs. out-of-the-box Traditional Architectures](https://arxiv.org/abs/2602.07815)
*Simiao Ren*

Main category: cs.CV

TL;DR: 本研究构建了首个大规模跨范式基准，评估了34种模型在8个标准数据集上的表现。研究发现，零样本VLM显著优于大多数专门模型，并且在极端年龄的估计上所有模型都面临较大挑战。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对现代视觉-语言模型与专门年龄估计架构进行系统比较的基准。因此，本文旨在填补这一空白，为内容审查、年龄验证和深度伪造检测提供重要的研究基础。

Method: 作者构建了一个大规模的交叉范式基准，评估了22种专门架构和12种通用视觉-语言模型的性能。

Result: 研究发现，零样本VLM在平均绝对误差（MAE）方面显著优于大多数专门模型，非大语言模型的最佳结果为9.88年，而最佳VLM为5.65年，两者相差15%。唯一与VLM竞争的模型MiVOLO通过结合面部和身体特征实现了类似的结果。

Conclusion: 研究成果挑战了年龄估计任务中专门架构必要性的假设，并建议未来研究应致力于将VLM的能力精炼到高效的专门模型中。

Abstract: Facial age estimation is critical for content moderation, age verification, and deepfake detection, yet no prior benchmark has systematically compared modern vision-language models (VLMs) against specialized age estimation architectures. We present the first large-scale cross-paradigm benchmark, evaluating \textbf{34 models} -- 22 specialized architectures with publicly available pretrained weights and 12 general-purpose VLMs -- across \textbf{8 standard datasets} (UTKFace, IMDB-WIKI, MORPH, AFAD, CACD, FG-NET, APPA-REAL, AgeDB) totaling 1{,}100 test images per model. Our key finding is striking: \emph{zero-shot VLMs significantly outperform most specialized models}, achieving an average MAE of 5.65 years compared to 9.88 for non-LLM models. The best VLM (Gemini~3 Flash Preview, MAE~4.32) outperforms the best non-LLM model (MiVOLO, MAE~5.10) by 15\%. Only MiVOLO, which uniquely combines face and body features via Vision Transformers, competes with VLMs. We further analyze age verification at the 18-year threshold, revealing that non-LLM models exhibit 60--100\% false adult rates on minors while VLMs achieve 13--25\%, and demonstrate that coarse age binning (8--9 classes) consistently degrades MAE beyond 13 years. Our stratified analysis across 14 age groups reveals that all models struggle most at extreme ages ($<$5 and 65+). These findings challenge the assumption that task-specific architectures are necessary for age estimation and suggest that the field should redirect toward distilling VLM capabilities into efficient specialized models.

</details>


### [78] [Open-Text Aerial Detection: A Unified Framework For Aerial Visual Grounding And Detection](https://arxiv.org/abs/2602.07827)
*Guoting Wei,Xia Yuan,Yang Zhou,Haizhao Jing,Yu Liu,Xianbiao Qi,Chunxia Zhao,Haokui Zhang,Rong Xiao*

Main category: cs.CV

TL;DR: OVAD和RSVG是开放词汇空地场景理解的关键框架，但各自在单独运行时存在局限性。本文提出了OTA-Det框架，实现了这两个框架的统一，增强了解决多目标检测和富语义理解的能力。


<details>
  <summary>Details</summary>
Motivation: OVAD仅限于粗略的类别级语义，而RSVG在结构上仅限于单目标定位，这些局限性阻止现有方法同时支持丰富的语义理解和多目标检测。本文旨在填补这一空白。

Method: 作者引入了一个任务重述策略，将任务目标和监督机制统一起来，允许在来自两个框架的数据集上进行联合训练，并引入多项高效模块将RT-DETR扩展到开放式文本检测，以增强实时性能。

Result: OTA-Det框架在六个基准测试上实现了最先进的性能，涵盖了OVAD和RSVG任务，同时保持了34 FPS的实时推理。

Conclusion: OTA-Det通过结合OVAD和RSVG的优点，在空地场景理解中实现了高效的多目标检测和细致的语义理解，展示了在开放词汇下的应用潜力。

Abstract: Open-Vocabulary Aerial Detection (OVAD) and Remote Sensing Visual Grounding (RSVG) have emerged as two key paradigms for aerial scene understanding. However, each paradigm suffers from inherent limitations when operating in isolation: OVAD is restricted to coarse category-level semantics, while RSVG is structurally limited to single-target localization. These limitations prevent existing methods from simultaneously supporting rich semantic understanding and multi-target detection. To address this, we propose OTA-Det, the first unified framework that bridges both paradigms into a cohesive architecture. Specifically, we introduce a task reformulation strategy that unifies task objectives and supervision mechanisms, enabling joint training across datasets from both paradigms with dense supervision signals. Furthermore, we propose a dense semantic alignment strategy that establishes explicit correspondence at multiple granularities, from holistic expressions to individual attributes, enabling fine-grained semantic understanding. To ensure real-time efficiency, OTA-Det builds upon the RT-DETR architecture, extending it from closed-set detection to open-text detection by introducing several high efficient modules, achieving state-of-the-art performance on six benchmarks spanning both OVAD and RSVG tasks while maintaining real-time inference at 34 FPS.

</details>


### [79] [SPD-Faith Bench: Diagnosing and Improving Faithfulness in Chain-of-Thought for Multimodal Large Language Models](https://arxiv.org/abs/2602.07833)
*Weijiang Lv,Yaoxuan Feng,Xiaobo Xia,Jiayu Wang,Yan Jing,Wenchao Chen,Bo Chen*

Main category: cs.CV

TL;DR: 该研究通过SPD-Faith Bench基准测试，揭示了MLLMs两种系统性失败模式，并提出SAGE框架，以提高视觉推理与感知的一致性。


<details>
  <summary>Details</summary>
Motivation: 研究针对MLLMs生成推理链的可信度问题，特别是视觉感知层面的不一致。

Method: 创建了一个基于细粒度图像差异推理的诊断基准SPD-Faith Bench，评估了当前最先进的MLLMs，并提出了SAGE框架。

Result: 研究揭示了两种系统性失败模式，即感知盲视和感知推理脱钩，并且通过SAGE框架改进了视觉推理与感知的一致性。

Conclusion: 研究强调了除了响应正确性外，还应明确评估可信度的重要性，并展示了新基准和代码的有效性。

Abstract: Chain-of-Thought reasoning is widely used to improve the interpretability of multimodal large language models (MLLMs), yet the faithfulness of the generated reasoning traces remains unclear. Prior work has mainly focused on perceptual hallucinations, leaving reasoning level unfaithfulness underexplored. To isolate faithfulness from linguistic priors, we introduce SPD-Faith Bench, a diagnostic benchmark based on fine-grained image difference reasoning that enforces explicit visual comparison. Evaluations on state-of-the-art MLLMs reveal two systematic failure modes, perceptual blindness and perception-reasoning dissociation. We trace these failures to decaying visual attention and representation shifts in the residual stream. Guided by this analysis, we propose SAGE, a train-free visual evidence-calibrated framework that improves visual routing and aligns reasoning with perception. Our results highlight the importance of explicitly evaluating faithfulness beyond response correctness. Our benchmark and codes are available at https://github.com/Johanson-colab/SPD-Faith-Bench.

</details>


### [80] [VFace: A Training-Free Approach for Diffusion-Based Video Face Swapping](https://arxiv.org/abs/2602.07835)
*Sanoojan Baliah,Yohan Abeysinghe,Rusiru Thushara,Khan Muhammad,Abhinav Dhall,Karthik Nandakumar,Muhammad Haris Khan*

Main category: cs.CV

TL;DR: VFace提出了一种无需训练的插件式方法，通过频域注意力插值、目标结构引导以及流指导的注意力时空平滑机制，实现了高质量的视频中人脸替换，无需额外训练或视频特异性微调。


<details>
  <summary>Details</summary>
Motivation: 当前的图像和视频中的人脸替换方法通常需要大量的训练或调整，VFace aimed to provide a more practical and modular solution for high-quality face swapping in videos that does not require extensive training or customization.

Method: VFace通过引入频域注意力插值技术（Frequency Spectrum Attention Interpolation）、目标结构引导（Target Structure Guidance）以及流指导的注意力时空平滑机制（Flow-Guided Attention Temporal Smoothening），实现高质量的视频中人脸替换。

Result: 实验证明，VFace的方法可以显著提高视频中人脸替换的时空一致性与视觉保真度。

Conclusion: VFace为视频中的人脸替换提供了一个无需额外训练或视频特异性微调的实用且模块化的方法。

Abstract: We present a training-free, plug-and-play method, namely VFace, for high-quality face swapping in videos. It can be seamlessly integrated with image-based face swapping approaches built on diffusion models. First, we introduce a Frequency Spectrum Attention Interpolation technique to facilitate generation and intact key identity characteristics. Second, we achieve Target Structure Guidance via plug-and-play attention injection to better align the structural features from the target frame to the generation. Third, we present a Flow-Guided Attention Temporal Smoothening mechanism that enforces spatiotemporal coherence without modifying the underlying diffusion model to reduce temporal inconsistencies typically encountered in frame-wise generation. Our method requires no additional training or video-specific fine-tuning. Extensive experiments show that our method significantly enhances temporal consistency and visual fidelity, offering a practical and modular solution for video-based face swapping. Our code is available at https://github.com/Sanoojan/VFace.

</details>


### [81] [Geometry-Aware Rotary Position Embedding for Consistent Video World Model](https://arxiv.org/abs/2602.07854)
*Chendong Xiang,Jiajun Liu,Jintao Zhang,Xiao Yang,Zhengwei Fang,Shizun Wang,Zijun Wang,Yingtian Zou,Hang Su,Jun Zhu*

Main category: cs.CV

TL;DR: 本文介绍了一种称为ViewRope的新几何感知编码方法，通过直接在视频变压器的自注意力层中注入相机射线方向，从而为检索3D一致内容提供模型内在的归纳偏差。此外，提出了几何感知帧稀疏注意力机制，利用几何线索选择性地关注相关的历史帧，提高效率而不牺牲记忆一致性。还引入了ViewBench诊断套件来测量循环闭合保真度和几何偏移。实验结果表明，ViewRope能显著提高长期一致性和降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 当前预测世界模型缺乏空间持久性，在长时间轨迹中无法维持稳定的场景结构。ViewRope旨在通过直接注入相机射线方向来克服屏幕空间位置嵌入与3D一致性所需的过程几何之间的冲突，提高模型在时间间隔中的3D一致检索能力。

Method: ViewRope通过在视频变压器的自注意力层中注入相机射线方向实现几何感知编码，提供了一种模型内在的3D一致性检索偏置。同时，提出了几何感知帧稀疏注意力机制，通过选择性地关注相关历史帧来提高效率并保持记忆一致性。

Result: 实验结果表明，ViewRope显著提高了长期一致性和降低了计算成本。对于视觉循环闭合保真度和几何偏移的度量，ViewBench提供了有效的工具。

Conclusion: ViewRope通过几何感知编码和帧稀疏注意力机制显著提高了预测世界模型的长期一致性和计算效率。

Abstract: Predictive world models that simulate future observations under explicit camera control are fundamental to interactive AI. Despite rapid advances, current systems lack spatial persistence: they fail to maintain stable scene structures over long trajectories, frequently hallucinating details when cameras revisit previously observed locations. We identify that this geometric drift stems from reliance on screen-space positional embeddings, which conflict with the projective geometry required for 3D consistency. We introduce \textbf{ViewRope}, a geometry-aware encoding that injects camera-ray directions directly into video transformer self-attention layers. By parameterizing attention with relative ray geometry rather than pixel locality, ViewRope provides a model-native inductive bias for retrieving 3D-consistent content across temporal gaps. We further propose \textbf{Geometry-Aware Frame-Sparse Attention}, which exploits these geometric cues to selectively attend to relevant historical frames, improving efficiency without sacrificing memory consistency. We also present \textbf{ViewBench}, a diagnostic suite measuring loop-closure fidelity and geometric drift. Our results demonstrate that ViewRope substantially improves long-term consistency while reducing computational costs.

</details>


### [82] [Recovering 3D Shapes from Ultra-Fast Motion-Blurred Images](https://arxiv.org/abs/2602.07860)
*Fei Yu,Shudan Guo,Shiqing Xin,Beibei Wang,Haisen Zhao,Wenzheng Chen*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的逆渲染方法，用于从超快速运动模糊图像中恢复3D形状。该方法通过快速计算/macron坐标解决瓶颈问题，实现高速运动的高效和保真模拟，并成功从严重运动模糊的图像中恢复3D形状。


<details>
  <summary>Details</summary>
Motivation: 当前，从静态图像恢复3D重建已经得到了广泛的研究，但对于极端运动模糊图像的3D重建仍然具有挑战性。特别是在快速运动场景下，如体育中的快速移动物体和工业中的旋转机械，传统技术无法有效处理。本文旨在解决这一难题，提出一种更有效的3D形状恢复方法。

Method: 本文提出的方法通过快速计算barycentric坐标来解决重复计算瓶颈问题。基于此，文中设计了一种快速的barycentric坐标解算器，从而实现高速运动的高效和保真模拟。该方法是完全可微的，因此可以进行从渲染图像到潜在3D形状的梯度传播，从而实现通过逆渲染进行3D形状恢复。

Result: 实验结果表明，本方法能够高效且真实地模拟超快速移动对象的前向模拟，并成功从经历极端平移和旋转运动的物体的2D图像中恢复3D形状。

Conclusion: 本文提出的方法为从极端运动模糊图像中恢复3D形状提供了一种更有效的解决方案，这个问题在快速移动的物体和旋转机械等场景中经常出现。这种方法的成功应用推动了基于视觉的3D重建边界。

Abstract: We consider the problem of 3D shape recovery from ultra-fast motion-blurred images. While 3D reconstruction from static images has been extensively studied, recovering geometry from extreme motion-blurred images remains challenging. Such scenarios frequently occur in both natural and industrial settings, such as fast-moving objects in sports (e.g., balls) or rotating machinery, where rapid motion distorts object appearance and makes traditional 3D reconstruction techniques like Multi-View Stereo (MVS) ineffective.
  In this paper, we propose a novel inverse rendering approach for shape recovery from ultra-fast motion-blurred images. While conventional rendering techniques typically synthesize blur by averaging across multiple frames, we identify a major computational bottleneck in the repeated computation of barycentric weights. To address this, we propose a fast barycentric coordinate solver, which significantly reduces computational overhead and achieves a speedup of up to 4.57x, enabling efficient and photorealistic simulation of high-speed motion. Crucially, our method is fully differentiable, allowing gradients to propagate from rendered images to the underlying 3D shape, thereby facilitating shape recovery through inverse rendering.
  We validate our approach on two representative motion types: rapid translation and rotation. Experimental results demonstrate that our method enables efficient and realistic modeling of ultra-fast moving objects in the forward simulation. Moreover, it successfully recovers 3D shapes from 2D imagery of objects undergoing extreme translational and rotational motion, advancing the boundaries of vision-based 3D reconstruction. Project page: https://maxmilite.github.io/rec-from-ultrafast-blur/

</details>


### [83] [Thinking in Structures: Evaluating Spatial Intelligence through Reasoning on Constrained Manifolds](https://arxiv.org/abs/2602.07864)
*Chen Yang,Guanxin Lin,Youquan He,Peiyao Chen,Guanghe Liu,Yufan Mo,Zhouyuan Xu,Linhao Wang,Guohui Zhang,Zihang Zhang,Shenxiang Zeng,Chen Wang,Jiansheng Fan*

Main category: cs.CV

TL;DR: 提出了SSI-Bench，这是一个针对受几何、拓扑和物理约束的3D结构进行空间推理的VQA基准。研究表明，许多现有的VLMs表现远逊于人类。


<details>
  <summary>Details</summary>
Motivation: 目前现有的基于视觉-语言模型在复杂现实3D结构的空间推理方面表现不足，因此提出了SSI-Bench基准来评估模型在受严格几何、拓扑和物理约束的空间推理任务中的能力。

Method: 通过人工设计包含几何与拓扑推理的1,000个排名问题，构建SSI-Bench基准。数据集通过研究人员花费大量时间（400小时）为每张图像挑选、标注结构组件和设计问题来形成。

Result: 评估结果显示，即使最强的模型也只能达到33.6%的准确率，相比之下，人类准确率达到91.6%。

Conclusion: 模型在复杂3D环境中的空间推理能力存在显著不足，尤其是在结构化的场景中，需要改进在结构解析和物理一致性方面的能力。

Abstract: Spatial intelligence is crucial for vision--language models (VLMs) in the physical world, yet many benchmarks evaluate largely unconstrained scenes where models can exploit 2D shortcuts. We introduce SSI-Bench, a VQA benchmark for spatial reasoning on constrained manifolds, built from complex real-world 3D structures whose feasible configurations are tightly governed by geometric, topological, and physical constraints. SSI-Bench contains 1,000 ranking questions spanning geometric and topological reasoning and requiring a diverse repertoire of compositional spatial operations, such as mental rotation, cross-sectional inference, occlusion reasoning, and force-path reasoning. It is created via a fully human-centered pipeline: ten researchers spent over 400 hours curating images, annotating structural components, and designing questions to minimize pixel-level cues. Evaluating 31 widely used VLMs reveals a large gap to humans: the best open-source model achieves 22.2% accuracy and the strongest closed-source model reaches 33.6%, while humans score 91.6%. Encouraging models to think yields only marginal gains, and error analysis points to failures in structural grounding and constraint-consistent 3D reasoning. Project page: https://ssi-bench.github.io.

</details>


### [84] [WristMIR: Coarse-to-Fine Region-Aware Retrieval of Pediatric Wrist Radiographs with Radiology Report-Driven Learning](https://arxiv.org/abs/2602.07872)
*Mert Sonmezer,Serge Vasylechko,Duygu Atasoy,Seyda Ertekin,Sila Kurugol*

Main category: cs.CV

TL;DR: WristMIR 通过利用放射学报告和骨骼特定定位，提出了一种区域感知的儿童手腕放射影像检索框架，该框架无需手动图像级注释就可学习细微的临床相关图像表示。该框架显著提高了骨折检索性能，并可能增强诊断推理和临床决策。


<details>
  <summary>Details</summary>
Motivation: 当前儿童手腕X光片检索因临床关键线索微小且局部化，经常被重叠的解剖结构掩盖，且缺乏大量标注数据，导致挑战。WristMIR旨在通过利用医学影像和放射报道之间的对应关系来解决这些问题。

Method: WristMIR 利用 MedGemma 基础的结构报告采掘技术生成全局和区域级别的描述，并结合处理过的手腕图像和骨特定近端桡骨、远端桡尺骨和尺骨头的裁剪，以构建全局和局部对比编码器的联合训练模型。该框架采用了两阶段检索过程，首先是粗略的全局匹配以确定候选影像，然后进行区域条件化的再排序以对准预定义的解剖骨区域。

Result: WristMIR 在图像到文本的 Recall@5 上将检索性能从 0.82% 提高到 9.35%，并在骨折分类中表现优异（AUROC：0.949，AUPRC：0.953）。在区域感知评估中，其两阶段设计显著改善了基于图像的骨折诊断，$F_1$ 值从 0.568 提高到 0.753，且放射科医生也认为其检索案例更具临床相关性，评分从 3.36 提高到 4.35。

Conclusion: WristMIR 通过区域感知的方法加强了临床放射影像的检索和骨折诊断，为儿童骨骼肌肉成像的诊断推理和临床决策提供了可能的支持工具。

Abstract: Retrieving wrist radiographs with analogous fracture patterns is challenging because clinically important cues are subtle, highly localized and often obscured by overlapping anatomy or variable imaging views. Progress is further limited by the scarcity of large, well-annotated datasets for case-based medical image retrieval. We introduce WristMIR, a region-aware pediatric wrist radiograph retrieval framework that leverages dense radiology reports and bone-specific localization to learn fine-grained, clinically meaningful image representations without any manual image-level annotations. Using MedGemma-based structured report mining to generate both global and region-level captions, together with pre-processed wrist images and bone-specific crops of the distal radius, distal ulna, and ulnar styloid, WristMIR jointly trains global and local contrastive encoders and performs a two-stage retrieval process: (1) coarse global matching to identify candidate exams, followed by (2) region-conditioned reranking aligned to a predefined anatomical bone region. WristMIR improves retrieval performance over strong vision-language baselines, raising image-to-text Recall@5 from 0.82% to 9.35%. Its embeddings also yield stronger fracture classification (AUROC 0.949, AUPRC 0.953). In region-aware evaluation, the two-stage design markedly improves retrieval-based fracture diagnosis, increasing mean $F_1$ from 0.568 to 0.753, and radiologists rate its retrieved cases as more clinically relevant, with mean scores rising from 3.36 to 4.35. These findings highlight the potential of anatomically guided retrieval to enhance diagnostic reasoning and support clinical decision-making in pediatric musculoskeletal imaging. The source code is publicly available at https://github.com/quin-med-harvard-edu/WristMIR.

</details>


### [85] [Scalable Adaptation of 3D Geometric Foundation Models via Weak Supervision from Internet Video](https://arxiv.org/abs/2602.07891)
*Zihui Gao,Ke Liu,Donny Y. Chen,Duochao Shi,Guosheng Lin,Hao Chen,Chunhua Shen*

Main category: cs.CV

TL;DR: SAGE 提出了一种基于视频流的几何基础模型规模化适应框架，通过层次化数据挖掘方法和混合监督，改善了零样本泛化能力，显著降低了未见数据集的点云重建误差。


<details>
  <summary>Details</summary>
Motivation: 为了解决几何基础模型在缺乏大规模多样性3D标注数据集的问题，有效利用互联网视频资源进行地面真值几何信息的获取，克服视觉观察噪声带来的挑战。

Method: SAGE从视频数据中提取训练轨迹，结合稀疏几何锚定与稠密差分一致性优化，通过层次化挖掘管道，并利用锚数据进行正则化以防止灾难性遗忘。

Result: SAGE在7Scenes, TUM-RGBD, Matterport3D等未见数据集上表现优异，与最先进的基线相比，Chamfer Distance降低了20-42%。

Conclusion: SAGE推动了几何基础模型通过互联网视频的适应性学习，提出了适用于广泛3D建模任务的可扩展学习范式。

Abstract: Geometric foundation models show promise in 3D reconstruction, yet their progress is severely constrained by the scarcity of diverse, large-scale 3D annotations. While Internet videos offer virtually unlimited raw data, utilizing them as a scaling source for geometric learning is challenging due to the absence of ground-truth geometry and the presence of observational noise. To address this, we propose SAGE, a framework for Scalable Adaptation of GEometric foundation models from raw video streams. SAGE leverages a hierarchical mining pipeline to transform videos into training trajectories and hybrid supervision: (1) Informative training trajectory selection; (2) Sparse Geometric Anchoring via SfM point clouds for global structural guidance; and (3) Dense Differentiable Consistency via 3D Gaussian rendering for multi-view constraints. To prevent catastrophic forgetting, we introduce a regularization strategy using anchor data. Extensive experiments show that SAGE significantly enhances zero-shot generalization, reducing Chamfer Distance by 20-42% on unseen benchmarks (7Scenes, TUM-RGBD, Matterport3D) compared to state-of-the-art baselines. To our knowledge, SAGE pioneers the adaptation of geometric foundation models via Internet video, establishing a scalable paradigm for general-purpose 3D learning.

</details>


### [86] [Rethinking Practical and Efficient Quantization Calibration for Vision-Language Models](https://arxiv.org/abs/2602.07899)
*Zhenhao Shang,Haizhao Jing,Guoting Wei,Haokui Zhang,Rong Xiao,Jianqing Gao,Peng Wang*

Main category: cs.CV

TL;DR: TLQ 提出了一种针对视觉语言模型 PTQ 的细粒度校准策略，通过利用梯度信息设计了 token 级别的重要性集成机制，引入了多 GPU、量化暴露的逐层校准方案，使得校准过程与真正的量化推理路径一致，有效提高了量化模型的性能稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统 PTQ 在 VLM 中效果不佳，主要源于视觉和文本令牌在激活分布及对量化误差敏感度上的显著差异。

Method: TLQ 通过渐变信息设计了 token 级别的重要性集成机制，并构建了 token 级别校准集，以实现更精细的校准。同时，提出了多 GPU、量化暴露的逐层校准方案，保持了层间校准过程与实际量化推理路径一致，并分散复杂校准工作，减轻了对大内存 GPU 的依赖。

Result: TLQ 在两种模型、三种规模和两种量化设置下表现一致，实现性能提升，证明了其量化稳定性。

Conclusion: TLQ 提供了一种有效提高视觉语言模型 PTQ 性能的框架，并将代码公开，促进相关研究的进展。

Abstract: Post-training quantization (PTQ) is a primary approach for deploying large language models without fine-tuning, and the quantized performance is often strongly affected by the calibration in PTQ. By contrast, in vision-language models (VLMs), substantial differences between visual and text tokens in their activation distributions and sensitivities to quantization error pose significant challenges for effective calibration during PTQ. In this work, we rethink what PTQ calibration should align with in VLMs and propose the Token-level Importance-aware Layer-wise Quantization framework (TLQ). Guided by gradient information, we design a token-level importance integration mechanism for quantization error, and use it to construct a token-level calibration set, enabling a more fine-grained calibration strategy. Furthermore, TLQ introduces a multi-GPU, quantization-exposed layer-wise calibration scheme. This scheme keeps the layer-wise calibration procedure consistent with the true quantized inference path and distributes the complex layer-wise calibration workload across multiple RTX3090 GPUs, thereby reducing reliance on the large memory of A100 GPUs. TLQ is evaluated across two models, three model scales, and two quantization settings, consistently achieving performance improvements across all settings, indicating its strong quantization stability. The code will be released publicly.

</details>


### [87] [Which private attributes do VLMs agree on and predict well?](https://arxiv.org/abs/2602.07931)
*Olena Hrynenko,Darya Baranouskaya,Alina Elena Baia,Andrea Cavallaro*

Main category: cs.CV

TL;DR: 该研究评估了开源视觉语言模型在隐私相关属性识别上的零样本能力，发现模型倾向于预测隐私属性的存在，且在高一致率情况下能辅助人标注。


<details>
  <summary>Details</summary>
Motivation: 随着视觉语言模型（VLMs）在图像中零样本检测视觉属性的应用越来越广泛，研究者旨在评估这些模型在隐私相关属性识别上的表现。

Method: 该研究通过与人类标注进行比较，评估开源视觉语言模型在隐私相关属性上的零样本性能，重点分析模型与人类标注的一致性和分歧。

Result: 研究结果显示，视觉语言模型在零样本环境下倾向于预测更多的隐私属性存在，且在高一致的情况下，模型能够补充人类标注，识别出人类容易忽略的属性。

Conclusion: 研究表明，视觉语言模型在识别隐私相关属性方面具有潜在优势，能够在大规模图像数据集的隐私标注工作中提供支持。

Abstract: Visual Language Models (VLMs) are often used for zero-shot detection of visual attributes in the image. We present a zero-shot evaluation of open-source VLMs for privacy-related attribute recognition. We identify the attributes for which VLMs exhibit strong inter-annotator agreement, and discuss the disagreement cases of human and VLM annotations. Our results show that when evaluated against human annotations, VLMs tend to predict the presence of privacy attributes more often than human annotators. In addition to this, we find that in cases of high inter-annotator agreement between VLMs, they can complement human annotation by identifying attributes overlooked by human annotators. This highlights the potential of VLMs to support privacy annotations in large-scale image datasets.

</details>


### [88] [One-Shot Crowd Counting With Density Guidance For Scene Adaptaion](https://arxiv.org/abs/2602.07955)
*Jiwei Chen,Qi Wang,Junyu Gao,Jing Zhang,Dingyi Li,Jing-Jia Luo*

Main category: cs.CV

TL;DR: 本文提出了一种利用局部和全局密度特征适应不同监视场景的少样本人群计数方法，能够在未见过的场景中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的人群计数模型在未见过的监视场景中表现有限，本文通过引入少样本学习来改善模型的泛化能力。

Method: 本文方法包括多个局部密度学习器以学习多个原型表示支持场景中的不同密度分布，通过局部和全局密度特征引导模型适应目标场景。

Result: 实验结果表明，该方法能够在未见过的监视场景中有效适应并显著优于现有方法。

Conclusion: 提出的少样本人群计数方法能够有效提高模型对未知监视场景的适应性，适用于实际监控需求。

Abstract: Crowd scenes captured by cameras at different locations vary greatly, and existing crowd models have limited generalization for unseen surveillance scenes. To improve the generalization of the model, we regard different surveillance scenes as different category scenes, and introduce few-shot learning to make the model adapt to the unseen surveillance scene that belongs to the given exemplar category scene. To this end, we propose to leverage local and global density characteristics to guide the model of crowd counting for unseen surveillance scenes. Specifically, to enable the model to adapt to the varying density variations in the target scene, we propose the multiple local density learner to learn multi prototypes which represent different density distributions in the support scene. Subsequently, these multiple local density similarity matrixes are encoded. And they are utilized to guide the model in a local way. To further adapt to the global density in the target scene, the global density features are extracted from the support image, then it is used to guide the model in a global way. Experiments on three surveillance datasets shows that proposed method can adapt to the unseen surveillance scene and outperform recent state-of-the-art methods in the few-shot crowd counting.

</details>


### [89] [D-ORCA: Dialogue-Centric Optimization for Robust Audio-Visual Captioning](https://arxiv.org/abs/2602.07960)
*Changli Tang,Tianyi Wang,Fengyun Rao,Jing Lyu,Chao Zhang*

Main category: cs.CV

TL;DR: 本文介绍了D-ORCA，这是一种以对话为中心的跨模态大型语言模型，旨在实现鲁棒的多模态音频-视频字幕。为验证模型性能，建立了DVD数据集并在多个通用音频-视频理解基准测试中证明了D-ORCA优于现有开源模型。


<details>
  <summary>Details</summary>
Motivation: 随着视频中对话信息的重要性日益凸显，准确识别谁说了什么以及说了何时对于深层次的视频理解至关重要。D-ORCA旨在填补开源生态系统中的关键空白，提供一种鲁棒的音频-视频字幕方法。

Method: D-ORCA采取了一种新型的增强学习方法，并且制定了三个新的奖励函数用于评估说话者归属准确性、全局语音内容准确性及句子级时间边界对齐。这些奖励函数基于广泛用于语音处理的评估指标，这是首次将这些指标应用于音频-视频字幕的增强学习目标。

Result: 大量实验表明，D-ORCA在说话者识别、语音识别和时间定位方面明显优于现有开源模型。尽管参数量只有8亿，它的表现接近于Qwen3-Omni的多个通用音频-视频理解基准测试。

Conclusion: D-ORCA模型在多模态音频-视频字幕领域取得了显著进步，未来有望在这一领域发挥更加重要的作用。

Abstract: Spoken dialogue is a primary source of information in videos; therefore, accurately identifying who spoke what and when is essential for deep video understanding. We introduce D-ORCA, a \textbf{d}ialogue-centric \textbf{o}mni-modal large language model optimized for \textbf{r}obust audio-visual \textbf{ca}ptioning. We further curate DVD, a large-scale, high-quality bilingual dataset comprising nearly 40,000 multi-party dialogue videos for training and 2000 videos for evaluation in English and Mandarin, addressing a critical gap in the open-source ecosystem. To ensure fine-grained captioning accuracy, we adopt group relative policy optimization with three novel reward functions that assess speaker attribution accuracy, global speech content accuracy, and sentence-level temporal boundary alignment. These rewards are derived from evaluation metrics widely used in speech processing and, to our knowledge, are applied for the first time as reinforcement learning objectives for audio-visual captioning. Extensive experiments demonstrate that D-ORCA substantially outperforms existing open-source models in speaker identification, speech recognition, and temporal grounding. Notably, despite having only 8 billion parameters, D-ORCA achieves performance competitive with Qwen3-Omni across several general-purpose audio-visual understanding benchmarks. Demos are available at \href{https://d-orca-llm.github.io/}{https://d-orca-llm.github.io/}. Our code, data, and checkpoints will be available at \href{https://github.com/WeChatCV/D-ORCA/}{https://github.com/WeChatCV/D-ORCA/}.

</details>


### [90] [EasyTune: Efficient Step-Aware Fine-Tuning for Diffusion-Based Motion Generation](https://arxiv.org/abs/2602.07967)
*Xiaofeng Tan,Wanjiang Weng,Haodong Lei,Hongsong Wang*

Main category: cs.CV

TL;DR: EasyTune 提出了一种新的方法，通过在每个去噪步骤中对扩散模型进行微调，解决了现有的不同iable奖励方法在优化效率和记忆使用方面的限制。


<details>
  <summary>Details</summary>
Motivation: 当前的运动生成模型难以与下游任务对齐，虽然使用可微奖励直接对扩散模型进行对齐可以取得良好的效果，但这些方法存在优化效率低且内存消耗大的问题。

Method: EasyTune 通过在每个去噪步骤中对扩散模型进行微调来打破递归依赖，实现细粒度和内存效率高的优化，并引入了Self-refinement Preference Learning（SPL）机制来动态识别偏好对并进行偏好学习。

Result: 实验结果表明，与 DRaFT-50 相比，EasyTune 在对齐（MM-Dist）方面提高了 8.2%，同时只需要 31.16% 的额外内存开销，训练速度提高了 7.3 倍。

Conclusion: EasyTune 有效解决了扩散模型对齐的问题，提供了更高效和更细粒度的优化方法，对后期的运动生成模型优化具有重要的研究价值和应用前景。

Abstract: In recent years, motion generative models have undergone significant advancement, yet pose challenges in aligning with downstream objectives. Recent studies have shown that using differentiable rewards to directly align the preference of diffusion models yields promising results. However, these methods suffer from (1) inefficient and coarse-grained optimization with (2) high memory consumption. In this work, we first theoretically and empirically identify the key reason of these limitations: the recursive dependence between different steps in the denoising trajectory. Inspired by this insight, we propose EasyTune, which fine-tunes diffusion at each denoising step rather than over the entire trajectory. This decouples the recursive dependence, allowing us to perform (1) a dense and fine-grained, and (2) memory-efficient optimization. Furthermore, the scarcity of preference motion pairs restricts the availability of motion reward model training. To this end, we further introduce a Self-refinement Preference Learning (SPL) mechanism that dynamically identifies preference pairs and conducts preference learning. Extensive experiments demonstrate that EasyTune outperforms DRaFT-50 by 8.2% in alignment (MM-Dist) improvement while requiring only 31.16% of its additional memory overhead and achieving a 7.3x training speedup. The project page is available at this link {https://xiaofeng-tan.github.io/projects/EasyTune/index.html}.

</details>


### [91] [FSP-Diff: Full-Spectrum Prior-Enhanced DualDomain Latent Diffusion for Ultra-Low-Dose Spectral CT Reconstruction](https://arxiv.org/abs/2602.07979)
*Peng Peng,Xinrui Zhang,Junlin Wang,Lei Li,Shaoyu Wang,Qiegen Liu*

Main category: cs.CV

TL;DR: 该研究提出了一种名为FSP-Diff的框架，用于超低剂量能谱CT重建，通过融合全谱先验信息和双域隐空间扩散合成策略，提高了图像质量和计算效率。


<details>
  <summary>Details</summary>
Motivation: 在超低剂量能谱CT中，由于信噪比严重下降导致的图像质量降低和结构细节丢失成为主要挑战。研究提出FSP-Diff框架以解决这一问题。

Method: FSP-Diff框架采用三方面策略：1）互补特征构建：结合直接图像重建和投影域降噪结果，前者保留纹理细节，后者提供结构框架；2）全谱先验融合：将多能量投影整合为高信噪比的全谱图像，为重建过程提供统一的结构参考；3）高效隐空间扩散合成：通过多路径特征嵌入紧凑的隐空间，实现加速重建并保持细节。

Result: 实验结果显示，FSP-Diff在图像质量和计算效率上显著优于现有方法，展示了其在临床可实现的超低剂量能谱CT成像中的潜力。

Conclusion: 该研究提出的方法为超低剂量能谱CT应用带来了显著改进，并展示了强大的实际应用前景。

Abstract: Spectral computed tomography (CT) with photon-counting detectors holds immense potential for material discrimination and tissue characterization. However, under ultra-low-dose conditions, the sharply degraded signal-to-noise ratio (SNR) in energy-specific projections poses a significant challenge, leading to severe artifacts and loss of structural details in reconstructed images. To address this, we propose FSP-Diff, a full-spectrum prior-enhanced dual-domain latent diffusion framework for ultra-low-dose spectral CT reconstruction. Our framework integrates three core strategies: 1) Complementary Feature Construction: We integrate direct image reconstructions with projection-domain denoised results. While the former preserves latent textural nuances amidst heavy noise, the latter provides a stable structural scaffold to balance detail fidelity and noise suppression. 2) Full-Spectrum Prior Integration: By fusing multi-energy projections into a high-SNR full-spectrum image, we establish a unified structural reference that guides the reconstruction across all energy bins. 3) Efficient Latent Diffusion Synthesis: To alleviate the high computational burden of high-dimensional spectral data, multi-path features are embedded into a compact latent space. This allows the diffusion process to facilitate interactive feature fusion in a lower-dimensional manifold, achieving accelerated reconstruction while maintaining fine-grained detail restoration. Extensive experiments on simulated and real-world datasets demonstrate that FSP-Diff significantly outperforms state-of-the-art methods in both image quality and computational efficiency, underscoring its potential for clinically viable ultra-low-dose spectral CT imaging.

</details>


### [92] [Continuity-driven Synergistic Diffusion with Neural Priors for Ultra-Sparse-View CBCT Reconstruction](https://arxiv.org/abs/2602.07980)
*Junlin Wang,Jiancheng Fang,Peng Peng,Shaoyu Wang,Qiegen Liu*

Main category: cs.CV

TL;DR: 该研究提出了一种名为CSDN的方法，通过引入神经先验和协同扩散策略，有效解决了超稀疏视图CBCT重建中的伪影和细节恢复问题。


<details>
  <summary>Details</summary>
Motivation: 现有的CBCT重建方法难以在减少辐射剂量的同时保持图像质量和连续性。

Method: CSDN方法结合了神经先验和协同扩散策略，包括Sinogram Refinement Diffusion（Sino-RD）和Digital Radiography Refinement Diffusion（DR-RD）两个细化路径，以及 Dual-Projection Reconstruction Fusion（DPRF）模块。

Result: CSDN方法能够在极端稀疏视图条件下有效抑制伪影并恢复细节数学，优于现有最先进的技术。

Conclusion: CSDN方法为超稀疏视图CBCT重建提供了一种有效的新方法，能够提高诊断可靠性。

Abstract: The clinical application of cone-beam computed tomography (CBCT) is constrained by the inherent trade-off between radiation exposure and image quality. Ultra-sparse angular sampling, employed to reduce dose, introduces severe undersampling artifacts and inter-slice inconsistencies, compromising diagnostic reliability. Existing reconstruction methods often struggle to balance angular continuity with spatial detail fidelity. To address these challenges, we propose a Continuity-driven Synergistic Diffusion with Neural priors (CSDN) for ultra-sparse-view CBCT reconstruction. Neural priors are introduced as a structural foundation to encode a continuous threedimensional attenuation representation, enabling the synthesis of physically consistent dense projections from ultra-sparse measurements. Building upon this neural-prior-based initialization, a synergistic diffusion strategy is developed, consisting of two collaborative refinement paths: a Sinogram Refinement Diffusion (Sino-RD) process that restores angular continuity and a Digital Radiography Refinement Diffusion (DR-RD) process that enforces inter-slice consistency from the projection image perspective. The outputs of the two diffusion paths are adaptively fused by the Dual-Projection Reconstruction Fusion (DPRF) module to achieve coherent volumetric reconstruction. Extensive experiments demonstrate that the proposed CSDN effectively suppresses artifacts and recovers fine textures under ultra-sparse-view conditions, outperforming existing state-of-the-art techniques.

</details>


### [93] [Deepfake Synthesis vs. Detection: An Uneven Contest](https://arxiv.org/abs/2602.07986)
*Md. Tarek Hasan,Sanjay Saha,Shaojing Fan,Swakkhar Shatabda,Terence Sim*

Main category: cs.CV

TL;DR: 本文对先进的深伪检测技术进行了全面的实证分析，发现许多最先进的检测模型在面对现代合成技术生成的高质量深伪内容时表现不佳，强调了持续改进检测模型的重要性。


<details>
  <summary>Details</summary>
Motivation: 鉴于深伪技术的快速发展，研究者们需要不断探索改进检测方法，以确保能够有效识别和防范这些技术带来的潜在风险。

Method: 该研究通过对照实验评估了最先进的深伪检测技术，并结合人类评价，分析了检测模型的性能。

Result: 研究结果揭示了先进的检测模型在面对现代合成技术生成的深伪内容时表现不佳的问题，尤其是人类在识别最高质量的深伪内容方面也表现出色。

Conclusion: 研究强调了深化对深伪检测技术的研究，以跟上合成技术进化的紧迫性，表明在未来需要投入更多关注和资源来缩小检测方法与新生成技术之间的差距。

Abstract: The rapid advancement of deepfake technology has significantly elevated the realism and accessibility of synthetic media. Emerging techniques, such as diffusion-based models and Neural Radiance Fields (NeRF), alongside enhancements in traditional Generative Adversarial Networks (GANs), have contributed to the sophisticated generation of deepfake videos. Concurrently, deepfake detection methods have seen notable progress, driven by innovations in Transformer architectures, contrastive learning, and other machine learning approaches. In this study, we conduct a comprehensive empirical analysis of state-of-the-art deepfake detection techniques, including human evaluation experiments against cutting-edge synthesis methods. Our findings highlight a concerning trend: many state-of-the-art detection models exhibit markedly poor performance when challenged with deepfakes produced by modern synthesis techniques, including poor performance by human participants against the best quality deepfakes. Through extensive experimentation, we provide evidence that underscores the urgent need for continued refinement of detection models to keep pace with the evolving capabilities of deepfake generation technologies. This research emphasizes the critical gap between current detection methodologies and the sophistication of new generation techniques, calling for intensified efforts in this crucial area of study.

</details>


### [94] [FlashVID: Efficient Video Large Language Models via Training-free Tree-based Spatiotemporal Token Merging](https://arxiv.org/abs/2602.08024)
*Ziyang Fan,Keyu Chen,Ruilong Xing,Yulin Li,Li Jiang,Zhuotao Tian*

Main category: cs.CV

TL;DR: FlashVID 是一个无需训练的推理加速框架，通过 Attention and Diversity-based Token Selection (ADTS) 选择最具代表性的视觉标记，并利用 Tree-based Spatiotemporal Token Merging (TSTM) 精细化地消除空间时间和冗余，有效提高 VLLM 的处理效率。


<details>
  <summary>Details</summary>
Motivation: 现有的 VLLM 加速框架通常独立地压缩空间和时间冗余，忽视了时空关系。FlashVID 的设计动机是在 VLLM 中保留最少的视觉标记数量，同时保持高性能，以实现对视频帧的高效处理。

Method: FlashVID 使用 Attention and Diversity-based Token Selection (ADTS) 从视觉标记中选择最具有代表性的标记，然后使用 Tree-based Spatiotemporal Token Merging (TSTM) 对时空冗余进行精细化消除。

Result: 在三个代表性 VLLM 上进行的广泛实验表明，FlashVID 的方法具有有效性与泛化能力。使用 FlashVID，仅保留视觉标记的 10%，即可保持 LLaVA-OneVision 性能的 99.1%，并且在与 Qwen2.5-VL 相同的计算预算下，输入视频帧数量增加 10 倍，相对性能提高了 8.6%。

Conclusion: FlashVID 提供了一种无需训练且即插即用的模块，可以显著提高 VLLM 的视频处理效率和性能，在相同计算预算下实现更大的视频输入帧数。

Abstract: Although Video Large Language Models (VLLMs) have shown remarkable capabilities in video understanding, they are required to process high volumes of visual tokens, causing significant computational inefficiency. Existing VLLMs acceleration frameworks usually compress spatial and temporal redundancy independently, which overlooks the spatiotemporal relationships, thereby leading to suboptimal spatiotemporal compression. The highly correlated visual features are likely to change in spatial position, scale, orientation, and other attributes over time due to the dynamic nature of video. Building on this insight, we introduce FlashVID, a training-free inference acceleration framework for VLLMs. Specifically, FlashVID utilizes Attention and Diversity-based Token Selection (ADTS) to select the most representative tokens for basic video representation, then applies Tree-based Spatiotemporal Token Merging (TSTM) for fine-grained spatiotemporal redundancy elimination. Extensive experiments conducted on three representative VLLMs across five video understanding benchmarks demonstrate the effectiveness and generalization of our method. Notably, by retaining only 10% of visual tokens, FlashVID preserves 99.1% of the performance of LLaVA-OneVision. Consequently, FlashVID can serve as a training-free and plug-and-play module for extending long video frames, which enables a 10x increase in video frame input to Qwen2.5-VL, resulting in a relative improvement of 8.6% within the same computational budget. Code is available at https://github.com/Fanziyang-v/FlashVID.

</details>


### [95] [MCIE: Multimodal LLM-Driven Complex Instruction Image Editing with Spatial Guidance](https://arxiv.org/abs/2602.07993)
*Xuehai Bai,Xiaoling Gu,Akide Liu,Hangjie Yuan,YiFan Zhang,Jack Ma*

Main category: cs.CV

TL;DR: 本文提出了一种名为MCIE-E1的方法，旨在解决现有图像编辑方法在处理复杂且组合式指令时的不足，通过引入空间感知交叉注意模块和背景一致交叉注意模块，提高了指令遵循能力和背景一致性。


<details>
  <summary>Details</summary>
Motivation: 现有基于指令的图像编辑方法无法处理复杂的编辑指令，本文旨在解决这一问题。

Method: 为了提高图像编辑的复杂指令处理能力，本文提出了一种名为MCIE-E1的方法，通过两个关键模块：空间感知交叉注意力模块和背景一致交叉注意力模块来解决现有的不足。

Result: MCIE-E1方法在新的基准测试CIE-Bench上表现出色，特别是在定量和定性评估中持续超越了现有的最先进的方法，在指令遵循性上实现了23.96%的进步。

Conclusion: 本文提出的方法有效解决了复杂图像编辑任务中的关键挑战，并在基准测试上取得了显著成果。

Abstract: Recent advances in instruction-based image editing have shown remarkable progress. However, existing methods remain limited to relatively simple editing operations, hindering real-world applications that require complex and compositional instructions. In this work, we address these limitations from the perspectives of architectural design, data, and evaluation protocols. Specifically, we identify two key challenges in current models: insufficient instruction compliance and background inconsistency. To this end, we propose MCIE-E1, a Multimodal Large Language Model-Driven Complex Instruction Image Editing method that integrates two key modules: a spatial-aware cross-attention module and a background-consistent cross-attention module. The former enhances instruction-following capability by explicitly aligning semantic instructions with spatial regions through spatial guidance during the denoising process, while the latter preserves features in unedited regions to maintain background consistency. To enable effective training, we construct a dedicated data pipeline to mitigate the scarcity of complex instruction-based image editing datasets, combining fine-grained automatic filtering via a powerful MLLM with rigorous human validation. Finally, to comprehensively evaluate complex instruction-based image editing, we introduce CIE-Bench, a new benchmark with two new evaluation metrics. Experimental results on CIE-Bench demonstrate that MCIE-E1 consistently outperforms previous state-of-the-art methods in both quantitative and qualitative assessments, achieving a 23.96% improvement in instruction compliance.

</details>


### [96] [Improved cystic hygroma detection from prenatal imaging using ultrasound-specific self-supervised representation learning](https://arxiv.org/abs/2512.22730)
*Youssef Megahed,Robin Ducharme,Inok Lee,Inbal Willner,Adrian D. C. Chan,Mark Walker,Steven Hawken*

Main category: cs.CV

TL;DR: 本研究通过使用带有掩码自编码器预训练的超声自监督基础模型（USF-MAE），对一早孕期超声图像进行了二元分类，以检测囊状淋巴管瘤，结果显示USF-MAE在准确性、敏感性、特异性和ROC-AUC等方面均超过了基准模型DenseNet-169。


<details>
  <summary>Details</summary>
Motivation: 囊状淋巴管瘤是一种高风险的产前超声发现，预示着较高的染色体异常、结构畸形和不良妊娠结局的风险。现有的监督深度学习方法受限于小的标注数据集。本研究目的是探讨特定于超声的自监督预训练能否促进一早孕期超声图像中囊状淋巴管瘤的准确、稳健检测。

Method: 本研究使用了带有掩码自编码器预训练的超声自监督基础模型（USF-MAE），在超过370,000幅未标注的超声图像上进行预训练，然后在正常对照和囊状淋巴管瘤病例的数据集上进行微调，采用相同的超声数据集、预处理管道和4折交叉验证协议进行性能评估。

Result: USF-MAE在准确性（0.96）、敏感性（0.94）、特异性（0.98）和ROC-AUC（0.98）等方面都超过了基准模型DenseNet-169（分别达到了0.93、0.92、0.94和0.94）。定量的Score-CAM可视化模型预测结果显示，模型能够突出显示胎儿颈部的预期区域，对于正例和负例都是如此。配对统计分析证实USF-MAE的性能改进具有统计学意义（p = 0.0057）。

Conclusion: 研究结果表明，使用超声自监督预训练的方法能够有效提高囊状淋巴管瘤的检测精度和稳健性。

Abstract: Cystic hygroma is a high-risk prenatal ultrasound finding that portends high rates of chromosomal abnormalities, structural malformations, and adverse pregnancy outcomes. Automated detection can increase reproducibility and support scalable early screening programs, but supervised deep learning methods are limited by small labelled datasets. This study assesses whether ultrasound-specific self-supervised pretraining can facilitate accurate, robust deep learning detection of cystic hygroma in first-trimester ultrasound images. We fine-tuned the Ultrasound Self-Supervised Foundation Model with Masked Autoencoding (USF-MAE), pretrained on over 370,000 unlabelled ultrasound images, for binary classification of normal controls and cystic hygroma cases used in this study. Performance was evaluated on the same curated ultrasound dataset, preprocessing pipeline, and 4-fold cross-validation protocol as for the DenseNet-169 baseline, using accuracy, sensitivity, specificity, and the area under the receiver operating characteristic curve (ROC-AUC). Model interpretability was analyzed qualitatively using Score-CAM visualizations. USF-MAE outperformed the DenseNet-169 baseline on all evaluation metrics. The proposed model yielded a mean accuracy of 0.96, sensitivity of 0.94, specificity of 0.98, and ROC-AUC of 0.98 compared to 0.93, 0.92, 0.94, and 0.94 for the DenseNet-169 baseline, respectively. Qualitative Score-CAM visualizations of model predictions demonstrated clinical relevance by highlighting expected regions in the fetal neck for both positive and negative cases. Paired statistical analysis using a Wilcoxon signed-rank test confirmed that performance improvements achieved by USF-MAE were statistically significant (p = 0.0057).

</details>


### [97] [ForecastOcc: Vision-based Semantic Occupancy Forecasting](https://arxiv.org/abs/2602.08006)
*Riya Mohan,Juana Valeria Hurtado,Rohit Mohan,Abhinav Valada*

Main category: cs.CV

TL;DR: ForecastOcc 是一款能够直接从过去摄像头图像预测多步未来的语义占用状态的首个视觉基线框架。


<details>
  <summary>Details</summary>
Motivation: 当前的语义占用预测方法依赖于具有误差累积问题的外部估算地图，ForecastOcc 提出了共同预测未来占用状态和语义类别的新方法，从而直接从图像中学习时空特征。

Method: ForecastOcc 引入了包含时序交叉注意力预测模块、2D 到 3D 视图变换器、用于占用预测的 3D 编码器以及可跨多个时间步生成体素等级预测结果的语义占用头部的创新架构。

Result: 在两个互补的数据集上进行的大量实验表明，ForecastOcc 比先前的基线更具优势，可以生成语义丰富、未来意识化的预测结果，能捕捉至关重要的场景动态和语义。

Conclusion: ForecastOcc 为自主驾驶中的视觉基线方法设定了新标准，提供了更准确、沉浸式的未来语义环境状态预测。

Abstract: Autonomous driving requires forecasting both geometry and semantics over time to effectively reason about future environment states. Existing vision-based occupancy forecasting methods focus on motion-related categories such as static and dynamic objects, while semantic information remains largely absent. Recent semantic occupancy forecasting approaches address this gap but rely on past occupancy predictions obtained from separate networks. This makes current methods sensitive to error accumulation and prevents learning spatio-temporal features directly from images. In this work, we present ForecastOcc, the first framework for vision-based semantic occupancy forecasting that jointly predicts future occupancy states and semantic categories. Our framework yields semantic occupancy forecasts for multiple horizons directly from past camera images, without relying on externally estimated maps. We evaluate ForecastOcc in two complementary settings: multi-view forecasting on the Occ3D-nuScenes dataset and monocular forecasting on SemanticKITTI, where we establish the first benchmark for this task. We introduce the first baselines by adapting two 2D forecasting modules within our framework. Importantly, we propose a novel architecture that incorporates a temporal cross-attention forecasting module, a 2D-to-3D view transformer, a 3D encoder for occupancy prediction, and a semantic occupancy head for voxel-level forecasts across multiple horizons. Extensive experiments on both datasets show that ForecastOcc consistently outperforms baselines, yielding semantically rich, future-aware predictions that capture scene dynamics and semantics critical for autonomous driving.

</details>


### [98] [When and How Much to Imagine: Adaptive Test-Time Scaling with World Models for Visual Spatial Reasoning](https://arxiv.org/abs/2602.08236)
*Shoubin Yu,Yue Zhang,Zun Wang,Jaehong Yoon,Huaxiu Yao,Mingyu Ding,Mohit Bansal*

Main category: cs.CV

TL;DR: 本文深入分析了测试时的视觉想象作为可控制资源在空间推理中的作用，提出了一个自适应框架AVIC，该框架在使用世界模型进行推理时，会根据当前视觉证据的充分性来决定是否及何时使用视觉想象。在多个基准测试中的表现表明，有选择地控制视觉想象力比固定策略能更有效地实现空间推理。


<details>
  <summary>Details</summary>
Motivation: 现实世界中，视觉空间推理依赖于场景在未见过或替代视角下的呈现，尽管现有大规模多模态语言模型在这方面取得了进展，但在何时需要使用视觉想象以及其在哪些情况下有益或有害方面，仍缺乏深入理解。本文通过引入一个自适应框架AVIC，旨在更好地理解和控制视觉想象在空间推理中的应用。

Method: 本文提出了一种自适应测试时框架AVIC，该框架内含世界模型，在处理空间推理任务时能够基于当前可视证据的充足性进行决策，决定是否及何时引入视觉想象。这种方法避免了盲目使用视觉想象带来的额外计算开销和可能引入的误导信息。

Result: 在多种基准测试（包括SAT、MMSI和R2R）上进行的实验表明，有选择地使用视觉想象可以达到或超越固定视觉想象策略的效果，且所需的计算资源显著减少，涉及的语言标记数量也显著减少。这些结果展示了视觉想象在空间推理中应用时的精确性与效率。

Conclusion: 本文的贡献在于提供了一种有效的方法来控制视觉想象在空间推理中的使用，提高了空间推理的准确性和效率。此外，本文的研究发现对于未来开发更高效、更可靠的视觉空间推理模型具有重要意义。

Abstract: Despite rapid progress in Multimodal Large Language Models (MLLMs), visual spatial reasoning remains unreliable when correct answers depend on how a scene would appear under unseen or alternative viewpoints. Recent work addresses this by augmenting reasoning with world models for visual imagination, but questions such as when imagination is actually necessary, how much of it is beneficial, and when it becomes harmful, remain poorly understood. In practice, indiscriminate imagination can increase computation and even degrade performance by introducing misleading evidence. In this work, we present an in-depth analysis of test-time visual imagination as a controllable resource for spatial reasoning. We study when static visual evidence is sufficient, when imagination improves reasoning, and how excessive or unnecessary imagination affects accuracy and efficiency. To support this analysis, we introduce AVIC, an adaptive test-time framework with world models that explicitly reasons about the sufficiency of current visual evidence before selectively invoking and scaling visual imagination. Across spatial reasoning benchmarks (SAT, MMSI) and an embodied navigation benchmark (R2R), our results reveal clear scenarios where imagination is critical, marginal, or detrimental, and show that selective control can match or outperform fixed imagination strategies with substantially fewer world-model calls and language tokens. Overall, our findings highlight the importance of analyzing and controlling test-time imagination for efficient and reliable spatial reasoning.

</details>


### [99] [Learning Self-Correction in Vision-Language Models via Rollout Augmentation](https://arxiv.org/abs/2602.08503)
*Yi Ding,Ziliang Qiu,Bolian Li,Ruqi Zhang*

Main category: cs.CV

TL;DR: 该研究提出了一种名为 Octopus 的强化学习（RL）回放增强框架，通过重新组合现有回放生成密集的自我修正示例，增强了样本效率并稳定了 RL 优化。


<details>
  <summary>Details</summary>
Motivation: 现有的基于强化学习的方法在视觉语言模型解决复杂推理问题时难以学习有效的自我修正行为，因为这些行为很少出现，使得学习信号非常稀疏。

Method: 该方法提出了 Octopus，采用重新组合已有回放生成密集的自我修正示例，同时引入了响应掩蔽策略来解耦自我修正和直接推理，保持两者学习的有效性。

Result: 采用 Octopus 方法的 Octopus-8B 实现了开源视觉语言模型在 7 个基准测试中的最好性能，比最好的 RLVR 基线高出 1.0 分，同时只需要 0.72 倍的训练时间。

Conclusion: 研究证明了 Octopus 在视觉语言模型中的有效性，并展示了如何通过自我修正增强模型的推理能力。

Abstract: Self-correction is essential for solving complex reasoning problems in vision-language models (VLMs). However, existing reinforcement learning (RL) methods struggle to learn it, as effective self-correction behaviors emerge only rarely, making learning signals extremely sparse. To address this challenge, we propose correction-specific rollouts (Octopus), an RL rollout augmentation framework that synthesizes dense self-correction examples by recombining existing rollouts. This augmentation simultaneously improves sample efficiency due to rollout reuse and stabilizes RL optimization through balanced supervision. Furthermore, we introduce a response-masking strategy that decouples self-correction from direct reasoning, avoiding signal conflicts and enabling both behaviors to be learned effectively. Building on this, we introduce Octopus-8B, a reasoning VLM with controllable self-correction capability. Across 7 benchmarks, it achieves SoTA performance among open-source VLMs, outperforming the best RLVR baseline by 1.0 score while requiring only $0.72\times$ training time per step.

</details>


### [100] [MIND: Benchmarking Memory Consistency and Action Control in World Models](https://arxiv.org/abs/2602.08025)
*Yixuan Ye,Xuanyu Lu,Yuxin Jiang,Yuchao Gu,Rui Zhao,Qiwei Liang,Jiachun Pan,Fengda Zhang,Weijia Wu,Alex Jinpeng Wang*

Main category: cs.CV

TL;DR: MIND 是一个涵盖 1080p, 24 FPS 的 250 条高质量视频的新基准，旨在评估世界模型的记忆一致性、动作控制和动作泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前对世界模型的基本能力缺乏统一评估基准，MIND 填补了这一空白，提供了更加全面的评价框架。

Method: MIND 包含一系列高质量视频，并设计了一个高效的评估框架来测量记忆一致性、动作控制与时间稳定性、上下文连贯性，同时引入了一款新的交互式基线 MIND-World 来协助未来的性能评估。

Result: 实验结果表明，MIND 可以全面评估当前世界模型的能力，揭示了长期记忆保持一致性与动作空间间泛化的关键挑战。

Conclusion: MIND 提供了一个新的、全面的基准，促进世界模型领域的研究与发展。

Abstract: World models aim to understand, remember, and predict dynamic visual environments, yet a unified benchmark for evaluating their fundamental abilities remains lacking. To address this gap, we introduce MIND, the first open-domain closed-loop revisited benchmark for evaluating Memory consIstency and action coNtrol in worlD models. MIND contains 250 high-quality videos at 1080p and 24 FPS, including 100 (first-person) + 100 (third-person) video clips under a shared action space and 25 + 25 clips across varied action spaces covering eight diverse scenes. We design an efficient evaluation framework to measure two core abilities: memory consistency and action control, capturing temporal stability and contextual coherence across viewpoints. Furthermore, we design various action spaces, including different character movement speeds and camera rotation angles, to evaluate the action generalization capability across different action spaces under shared scenes. To facilitate future performance benchmarking on MIND, we introduce MIND-World, a novel interactive Video-to-World baseline. Extensive experiments demonstrate the completeness of MIND and reveal key challenges in current world models, including the difficulty of maintaining long-term memory consistency and generalizing across action spaces. Project page: https://csu-jpg.github.io/MIND.github.io/

</details>


### [101] [Vanilla Group Equivariant Vision Transformer: Simple and Effective](https://arxiv.org/abs/2602.08047)
*Jiahong Fu,Qi Xie,Deyu Meng,Zongben Xu*

Main category: cs.CV

TL;DR: 提出了一种框架，将对称先验作为归纳偏置应用于Vision Transformers (ViTs)，使其各个关键组件（包括Patch Embedding、Self-Attention、位置编码和Down/Up-Sampling）均具有保证的对称性，从而提升ViTs在视觉任务中的性能和数据效率。


<details>
  <summary>Details</summary>
Motivation: 现有的对称Vision Transformers (ViTs)通常在保持性能的同时难以实现整体对称性，特别是在将Self-Attention机制与Patch Embedding协调一致方面存在挑战。

Method: 提出了一种框架方法，系统性地使ViTs的关键组件（Patch Embedding、Self-Attention、位置编码和Down/Up-Sampling）均具备对称性。

Result: 实验结果表明，所提出的对称ViTs在多种视觉任务中均能实现性能和数据效率的提升。

Conclusion: 该对称框架为Vision Transformers的构建提供了理论基础和实际灵活性，甚至适用于Swin Transformers。

Abstract: Incorporating symmetry priors as inductive biases to design equivariant Vision Transformers (ViTs) has emerged as a promising avenue for enhancing their performance. However, existing equivariant ViTs often struggle to balance performance with equivariance, primarily due to the challenge of achieving holistic equivariant modifications across the diverse modules in ViTs-particularly in harmonizing the Self-Attention mechanism with Patch Embedding. To address this, we propose a straightforward framework that systematically renders key ViT components, including patch embedding, self-attention, positional encodings, and Down/Up-Sampling, equivariant, thereby constructing ViTs with guaranteed equivariance. The resulting architecture serves as a plug-and-play replacement that is both theoretically grounded and practically versatile, scaling seamlessly even to Swin Transformers. Extensive experiments demonstrate that our equivariant ViTs consistently improve performance and data efficiency across a wide spectrum of vision tasks.

</details>


### [102] [Picasso: Holistic Scene Reconstruction with Physics-Constrained Sampling](https://arxiv.org/abs/2602.08058)
*Xihang Yu,Rajat Talak,Lorenzo Shaikewitz,Luca Carlone*

Main category: cs.CV

TL;DR: 本文提出了一个新的物理约束重建流水线 Picasso，该流水线通过考虑物体间的几何、非穿透和物理约束重建具有接触的多物体场景。此外，还提出了一组具有真实接触的场景数据集 Picasso，以及一种衡量物理合理性的指标，并在该数据集和 YCB-V 数据集上展示了 Picasso 的优越性能。


<details>
  <summary>Details</summary>
Motivation: 在遮挡和测量噪声的存在下，几何准确的场景重建可能在物理上是不正确的。现有方法在估计物体的姿势和形状后，直接导入模拟器可能会导致不可行的配置。因此，本文提出了一个物理约束重建流水线 Picasso，以提高动态行为预测的准确性和合理性。

Method: Picasso 使用快速拒绝采样方法处理多物体交互，并利用推断出的物体接触图来引导样本。这一方法能够综合考虑物体间的几何、非穿透及物理约束，从而实现更为合理的重建。

Result: 实验结果表明，Picasso 在文章引入的数据集和 YCB-V 数据集上都显著优于现有方法。重建结果不仅在物理上合理，还能更好地符合人的直观体验。

Conclusion: 本文提出了一个新的物理约束重建流水线 Picasso，并公开了一组新的场景数据集和物理合理性评估指标。重建结果表明，该方法在具有接触的真实场景中表现更为出色。

Abstract: In the presence of occlusions and measurement noise, geometrically accurate scene reconstructions -- which fit the sensor data -- can still be physically incorrect. For instance, when estimating the poses and shapes of objects in the scene and importing the resulting estimates into a simulator, small errors might translate to implausible configurations including object interpenetration or unstable equilibrium. This makes it difficult to predict the dynamic behavior of the scene using a digital twin, an important step in simulation-based planning and control of contact-rich behaviors. In this paper, we posit that object pose and shape estimation requires reasoning holistically over the scene (instead of reasoning about each object in isolation), accounting for object interactions and physical plausibility. Towards this goal, our first contribution is Picasso, a physics-constrained reconstruction pipeline that builds multi-object scene reconstructions by considering geometry, non-penetration, and physics. Picasso relies on a fast rejection sampling method that reasons over multi-object interactions, leveraging an inferred object contact graph to guide samples. Second, we propose the Picasso dataset, a collection of 10 contact-rich real-world scenes with ground truth annotations, as well as a metric to quantify physical plausibility, which we open-source as part of our benchmark. Finally, we provide an extensive evaluation of Picasso on our newly introduced dataset and on the YCB-V dataset, and show it largely outperforms the state of the art while providing reconstructions that are both physically plausible and more aligned with human intuition.

</details>


### [103] [DICE: Disentangling Artist Style from Content via Contrastive Subspace Decomposition in Diffusion Models](https://arxiv.org/abs/2602.08059)
*Tong Zhang,Ru Zhang,Jianyi Liu*

Main category: cs.CV

TL;DR: DICE提供了一种无需训练的框架，用于实时去除艺术家风格，通过对比子空间分解实现风格净化，同时保留用户意图的内容，减少风格模仿带来的版权风险。


<details>
  <summary>Details</summary>
Motivation: 随着扩散模型的发展，用户在没有授权的情况下可以轻易模仿独特的艺术风格，这在部署平台上带来了版权和知识产权的风险，现有措施成本高且实用性低，DICE旨在解决这一问题。

Method: DICE通过构造对比三元组，使模型在潜在空间中区分风格和非风格特征，将去风格化过程形式化为可解的广义特征值问题，并引入自适应注意解耦编辑策略，根据每个标记的风格集中度动态评估并进行差异抑制和内容增强。

Result: 实验表明，DICE在去除风格彻底性和保留内容完整性之间取得了优秀的平衡，仅需3秒即可实现风格的去耦合，提供了一种实用且高效的风格模仿遏制技术。

Conclusion: DICE为实时去除艺术家风格提供了一种新的框架，通过对比子空间分解和广义特征值问题实现了高效且可操作的风格清洁技术。

Abstract: The recent proliferation of diffusion models has made style mimicry effortless, enabling users to imitate unique artistic styles without authorization. In deployed platforms, this raises copyright and intellectual-property risks and calls for reliable protection. However, existing countermeasures either require costly weight editing as new styles emerge or rely on an explicitly specified editing style, limiting their practicality for deployment-side safety. To address this challenge, we propose DICE (Disentanglement of artist Style from Content via Contrastive Subspace Decomposition), a training-free framework for on-the-fly artist style erasure. Unlike style editing that require an explicitly specified replacement style, DICE performs style purification, removing the artist's characteristics while preserving the user-intended content. Our core insight is that a model cannot truly comprehend the artist style from a single text or image alone. Consequently, we abandon the traditional paradigm of identifying style from isolated samples. Instead, we construct contrastive triplets to compel the model to distinguish between style and non-style features in the latent space. By formalizing this disentanglement process as a solvable generalized eigenvalue problem, we achieve precise identification of the style subspace. Furthermore, we introduce an Adaptive Attention Decoupling Editing strategy dynamically assesses the style concentration of each token and performs differential suppression and content enhancement on the QKV vectors. Extensive experiments demonstrate that DICE achieves a superior balance between the thoroughness of style erasure and the preservation of content integrity. DICE introduces an additional overhead of only 3 seconds to disentangle style, providing a practical and efficient technique for curbing style mimicry.

</details>


### [104] [ReRoPE: Repurposing RoPE for Relative Camera Control](https://arxiv.org/abs/2602.08068)
*Chunyang Li,Yuanbo Yang,Jiahao Shao,Hongyu Zhou,Katja Schwarz,Yiyi Liao*

Main category: cs.CV

TL;DR: ReRoPE是一种插件式框架，可以从预训练的视频扩散模型中无缝地注入相对摄像机姿势信息，实现精确的摄像机控制同时保持强大的预训练生成先验。


<details>
  <summary>Details</summary>
Motivation: 现有的方法在固定参考下的摄像机姿态编码缺乏平移不变性，导致性能有限，而ReRoPE通过利用预训练模型未充分利用的频谱带宽，解决了这个问题。

Method: ReRoPE框架基于RoPE（旋转位置嵌入）结构，通过在低频未充分利用的频谱带宽中无缝嵌入相对摄像机姿态信息，实现对生成模型的增强。

Result: ReRoPE在图像到视频（I2V）和视频到视频（V2V）任务中展示了优秀的摄像机控制准确性和视觉保真度，是一条训练高效的发展道路，能够实现可控的、高保真度视频生成。

Conclusion: ReRoPE为现有视频生成与摄像机控制方法提供了一种有效且训练成本较低的改进方案，实现了在保持预训练生成模型性能的同时进行精确摄像机控制。

Abstract: Video generation with controllable camera viewpoints is essential for applications such as interactive content creation, gaming, and simulation. Existing methods typically adapt pre-trained video models using camera poses relative to a fixed reference, e.g., the first frame. However, these encodings lack shift-invariance, often leading to poor generalization and accumulated drift. While relative camera pose embeddings defined between arbitrary view pairs offer a more robust alternative, integrating them into pre-trained video diffusion models without prohibitive training costs or architectural changes remains challenging. We introduce ReRoPE, a plug-and-play framework that incorporates relative camera information into pre-trained video diffusion models without compromising their generation capability. Our approach is based on the insight that Rotary Positional Embeddings (RoPE) in existing models underutilize their full spectral bandwidth, particularly in the low-frequency components. By seamlessly injecting relative camera pose information into these underutilized bands, ReRoPE achieves precise control while preserving strong pre-trained generative priors. We evaluate our method on both image-to-video (I2V) and video-to-video (V2V) tasks in terms of camera control accuracy and visual fidelity. Our results demonstrate that ReRoPE offers a training-efficient path toward controllable, high-fidelity video generation. See project page for more results: https://sisyphe-lee.github.io/ReRoPE/

</details>


### [105] [ViT-5: Vision Transformers for The Mid-2020s](https://arxiv.org/abs/2602.08071)
*Feng Wang,Sucheng Ren,Tiezheng Zhang,Predrag Neskovic,Anand Bhattad,Cihang Xie,Alan Yuille*

Main category: cs.CV

TL;DR: 本文提出了一种通过采用过去五年中架构上的进步来系统地现代Vision Transformer基础库的方法。通过组件级改进，包括规范化、激活函数、位置编码、门控机制和可学习的令牌，形成了新一代Vision Transformers，称为ViT-5。大量的实验表明，ViT-5在理解和生成基准测试中都优于现有的最先进的纯Vision Transformer，在ImageNet-1k分类任务中，与DeiT-III-Base相比，ViT-5-Base在同等计算资源下达到了84.2%的Top-1精度。使用ViT-5作为生成建模的骨干时，它在SiT扩散框架中的表现也优于传统的ViT骨干。ViT-5不仅在关键指标上表现出色，还展示了更好的表示学习能力和空间推理行为，并且任务之间具有可靠的迁移。


<details>
  <summary>Details</summary>
Motivation: 随着计算资源的发展，需要提高Vision Transformer的性能和效率，尤其是在理解和生成任务中。通过整合过去几年的架构改进，本文旨在开发一种新的Vision Transformer设计方案，以解决这些需求。

Method: 本文提出了一个组件级改进的方法，涉及规范化、激活函数、位置编码、门控机制和可学习的令牌。这些改进形成了名为ViT-5的新一代Vision Transformers。

Result: 一系列广泛的实验表明，ViT-5在ImageNet-1k分类任务上优于现有的先进模型，同时在生成建模任务中也表现出优越性。此外，ViT-5展示了更好的表示学习能力和空间推理行为。

Conclusion: 本文证明了整合过去的架构改进可以显著提高Vision Transformer的性能。作为对2020年代中期Vision Transformer设计的一个简单升级，ViT-5提供了一种强大的基础模型方案。

Abstract: This work presents a systematic investigation into modernizing Vision Transformer backbones by leveraging architectural advancements from the past five years. While preserving the canonical Attention-FFN structure, we conduct a component-wise refinement involving normalization, activation functions, positional encoding, gating mechanisms, and learnable tokens. These updates form a new generation of Vision Transformers, which we call ViT-5. Extensive experiments demonstrate that ViT-5 consistently outperforms state-of-the-art plain Vision Transformers across both understanding and generation benchmarks. On ImageNet-1k classification, ViT-5-Base reaches 84.2\% top-1 accuracy under comparable compute, exceeding DeiT-III-Base at 83.8\%. ViT-5 also serves as a stronger backbone for generative modeling: when plugged into an SiT diffusion framework, it achieves 1.84 FID versus 2.06 with a vanilla ViT backbone. Beyond headline metrics, ViT-5 exhibits improved representation learning and favorable spatial reasoning behavior, and transfers reliably across tasks. With a design aligned with contemporary foundation-model practices, ViT-5 offers a simple drop-in upgrade over vanilla ViT for mid-2020s vision backbones.

</details>


### [106] [VidVec: Unlocking Video MLLM Embeddings for Video-Text Retrieval](https://arxiv.org/abs/2602.08099)
*Issar Tzachor,Dvir Samuel,Rami Ben-Ari*

Main category: cs.CV

TL;DR: 本文研究了利用预训练Multiple Modal Large Language Models（MLLMs）在视频-文本嵌入和检索中的应用，展示了其在视频检索任务上的优越性能。


<details>
  <summary>Details</summary>
Motivation: 最近的研究虽然已经尝试将生成性的MLLMs用于视觉任务的嵌入提取，但在视频任务上的表现仍然不如专用的视频基础模型（VFMs）。因此，本文致力于探索如何更好地利用MLLMs在视频-文本嵌入和检索中的潜力。

Method: 作者首先进行了层次分析，发现预训练的MLLM层已经包含了丰富的任务相关信息，因此提出了一种基于文本的轻量级对齐策略，通过将密集的视频字幕映射到简短的总结，并进行嵌入学习，从而在无需视觉监督的情况下实现任务相关的视频-文本嵌入。

Result: 该方法在视频检索基准测试中取得了当前最佳的结果，且无需额外的文本之外的微调。

Conclusion: 这种方法展示了一种有效提升视频-文本嵌入和检索性能的新方法，且具有较强的零样本检索性能。

Abstract: Recent studies have adapted generative Multimodal Large Language Models (MLLMs) into embedding extractors for vision tasks, typically through fine-tuning to produce universal representations. However, their performance on video remains inferior to Video Foundation Models (VFMs). In this paper, we focus on leveraging MLLMs for video-text embedding and retrieval. We first conduct a systematic layer-wise analysis, showing that intermediate (pre-trained) MLLM layers already encode substantial task-relevant information. Leveraging this insight, we demonstrate that combining intermediate-layer embeddings with a calibrated MLLM head yields strong zero-shot retrieval performance without any training. Building on these findings, we introduce a lightweight text-based alignment strategy which maps dense video captions to short summaries and enables task-related video-text embedding learning without visual supervision. Remarkably, without any fine-tuning beyond text, our method outperforms current methods, often by a substantial margin, achieving state-of-the-art results across common video retrieval benchmarks.

</details>


### [107] [MMLSv2: A Multimodal Dataset for Martian Landslide Detection in Remote Sensing Imagery](https://arxiv.org/abs/2602.08112)
*Sidike Paheding,Abel Reyes-Angulo,Leo Thomas Ramos,Angel D. Sappa,Rajaneesh A.,Hiral P. B.,Sajin Kumar K. S.,Thomas Oommen*

Main category: cs.CV

TL;DR: MMLSv2是一个用于火星滑坡分割的多模态数据集，包含七种波段的图像和一个地理上分散的孤立测试集，用于评估模型的稳健性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 提供一个广泛覆盖火星表面特征的数据集，以支持滑坡检测和提供评估模型能力的工具。

Method: 该数据集包含多种类型的图像数据，包括RGB、数字高程模型、坡度、热惯性以及灰度通道。额外的孤立测试集用于评估模型在未见过的地理区域的表现。

Result: 多个分割模型在该数据集上的实验表明，其支持稳定训练且具有竞争力的性能，但在碎片化、拉长和小型滑坡区域仍存在挑战。孤立测试集的评估展示了模型在标准分布内设置之外的性能下降。

Conclusion: 该数据集展示了对火星滑坡检测的贡献，并有助于增强模型在更具挑战性的滑坡区域的鲁棒性和泛化能力。

Abstract: We present MMLSv2, a dataset for landslide segmentation on Martian surfaces. MMLSv2 consists of multimodal imagery with seven bands: RGB, digital elevation model, slope, thermal inertia, and grayscale channels. MMLSv2 comprises 664 images distributed across training, validation, and test splits. In addition, an isolated test set of 276 images from a geographically disjoint region from the base dataset is released to evaluate spatial generalization. Experiments conducted with multiple segmentation models show that the dataset supports stable training and achieves competitive performance, while still posing challenges in fragmented, elongated, and small-scale landslide regions. Evaluation on the isolated test set leads to a noticeable performance drop, indicating increased difficulty and highlighting its value for assessing model robustness and generalization beyond standard in-distribution settings. Dataset will be available at: https://github.com/MAIN-Lab/MMLS_v2

</details>


### [108] [Building Damage Detection using Satellite Images and Patch-Based Transformer Methods](https://arxiv.org/abs/2602.08117)
*Smriti Siva,Jan Cross-Zamirski*

Main category: cs.CV

TL;DR: 本研究使用Vision Transformer模型评估了xBD数据集中建筑损坏分类的表现，并提出了一种基于靶向补丁的预处理管道以减轻背景噪声。


<details>
  <summary>Details</summary>
Motivation: 为了提高卫星图像中建筑损坏分类模型的效果，尤其是在包含噪声和类不平衡数据的情况下。

Method: 研究中评估了DINOv2-small和DeiT模型，并提出了一种新的预处理管道和冻结头部微调策略。

Result: 使用提出的方法，小的ViT架构在宏平均F1分数上达到了与先前的CNN基线相当的效果。

Conclusion: 研究证明Vision Transformer在建筑损坏评估中具有潜力，尤其是通过针对噪声和不平衡数据的处理方法。

Abstract: Rapid building damage assessment is critical for post-disaster response. Damage classification models built on satellite imagery provide a scalable means of obtaining situational awareness. However, label noise and severe class imbalance in satellite data create major challenges. The xBD dataset offers a standardized benchmark for building-level damage across diverse geographic regions. In this study, we evaluate Vision Transformer (ViT) model performance on the xBD dataset, specifically investigating how these models distinguish between types of structural damage when training on noisy, imbalanced data.
  In this study, we specifically evaluate DINOv2-small and DeiT for multi-class damage classification. We propose a targeted patch-based pre-processing pipeline to isolate structural features and minimize background noise in training. We adopt a frozen-head fine-tuning strategy to keep computational requirements manageable. Model performance is evaluated through accuracy, precision, recall, and macro-averaged F1 scores. We show that small ViT architectures with our novel training method achieves competitive macro-averaged F1 relative to prior CNN baselines for disaster classification.

</details>


### [109] [Fields of The World: A Field Guide for Extracting Agricultural Field Boundaries](https://arxiv.org/abs/2602.08131)
*Isaac Corley,Hannah Kerner,Caleb Robinson,Jennifer Marcus*

Main category: cs.CV

TL;DR: 该文介绍了一个名为Fields of The World (FTW) 的生态系统，该系统包含跨越24个国家的1.6M块田地的边界映射、预训练分割模型以及命令行推理工具。文章展示了在局部和国家尺度上提取田地边界、分类作物类型和归因森林损失的方法，并提供了两个使用MOSAIKS随机卷积特征的笔记本。


<details>
  <summary>Details</summary>
Motivation: 本文旨在提供一个用于农业数据产品的基准数据集、模型和工具，以支持田地边界检测、作物分类及森林损失归因等任务。

Method: 该文使用随机卷积特征作为特征提取方法，并基于这些特征训练分割模型。文章首先介绍了一个包含大量田地边界的基准数据集，然后利用这些边界和云优化数据进行作物分类等任务。

Result: 使用该基准数据集进行作物分类实验，获得的宏观F1分数为0.65-0.75。在五个国家上进行了预测，预测的田地面积范围从0.06公顷（卢旺达）到0.28公顷（瑞士）。同时提供了用于本地和国家规模字段边界的提取和作物分类的笔记本。

Conclusion: 本文成功地创建了一个适用于农业数据产品开发的大规模田地边界数据集和工具生态系统，为实际应用提供了实用的方法和技术支持。

Abstract: Field boundary maps are a building block for agricultural data products and support crop monitoring, yield estimation, and disease estimation. This tutorial presents the Fields of The World (FTW) ecosystem: a benchmark of 1.6M field polygons across 24 countries, pre-trained segmentation models, and command-line inference tools. We provide two notebooks that cover (1) local-scale field boundary extraction with crop classification and forest loss attribution, and (2) country-scale inference using cloud-optimized data. We use MOSAIKS random convolutional features and FTW derived field boundaries to map crop type at the field level and report macro F1 scores of 0.65--0.75 for crop type classification with limited labels. Finally, we show how to explore pre-computed predictions over five countries (4.76M km\textsuperscript{2}), with median predicted field areas from 0.06 ha (Rwanda) to 0.28 ha (Switzerland).

</details>


### [110] [Robustness of Vision Language Models Against Split-Image Harmful Input Attacks](https://arxiv.org/abs/2602.08136)
*Md Rafi Ur Rashid,MD Sadik Hossain Shanto,Vishnu Asutosh Dasu,Shagufta Mehnaz*

Main category: cs.CV

TL;DR: 该研究指出现有视觉语言模型（VLM）在处理分幅图像时存在安全漏洞，提出了一种名为SIVA的新分幅图像视觉牢笼攻击方法。该方法通过利用安全对齐的不足，逐步进化并最终实现跨模型攻击，提高了攻击的转移率。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型在整体图像上表现出较强的鲁棒性，但在处理分幅图像时则存在安全性不足的问题。因此，该研究提出了SIVA攻击方法来减轻这一问题。

Method: 首先，通过实验分析现有的无黑盒优化攻击的局限性。然后，提出了基于对抗知识蒸馏（Adv-KD）算法的白盒和黑盒攻击策略，逐步进化攻击方法。最后，评估了三种现代VLM和三种牢笼数据集的表现。

Result: 实验结果表明，利用新型对抗知识蒸馏算法的SIVA攻击方法在跨模型攻击转移方面表现最佳，成功率提高了60%。

Conclusion: 该研究揭示了现有视觉语言模型在处理分幅图像时的安全性缺陷，并提供了一种有效的攻击方法，同时也提出了改进建议。

Abstract: Vision-Language Models (VLMs) are now a core part of modern AI. Recent work proposed several visual jailbreak attacks using single/ holistic images. However, contemporary VLMs demonstrate strong robustness against such attacks due to extensive safety alignment through preference optimization (e.g., RLHF). In this work, we identify a new vulnerability: while VLM pretraining and instruction tuning generalize well to split-image inputs, safety alignment is typically performed only on holistic images and does not account for harmful semantics distributed across multiple image fragments. Consequently, VLMs often fail to detect and refuse harmful split-image inputs, where unsafe cues emerge only after combining images. We introduce novel split-image visual jailbreak attacks (SIVA) that exploit this misalignment. Unlike prior optimization-based attacks, which exhibit poor black-box transferability due to architectural and prior mismatches across models, our attacks evolve in progressive phases from naive splitting to an adaptive white-box attack, culminating in a black-box transfer attack. Our strongest strategy leverages a novel adversarial knowledge distillation (Adv-KD) algorithm to substantially improve cross-model transferability. Evaluations on three state-of-the-art modern VLMs and three jailbreak datasets demonstrate that our strongest attack achieves up to 60% higher transfer success than existing baselines. Lastly, we propose efficient ways to address this critical vulnerability in the current VLM safety alignment.

</details>


### [111] [PEGAsus: 3D Personalization of Geometry and Appearance](https://arxiv.org/abs/2602.08198)
*Jingyu Hu,Bin Hu,Ka-Hei Hui,Haipeng Li,Zhengzhe Liu,Daniel Cohen-Or,Chi-Wing Fu*

Main category: cs.CV

TL;DR: PEGAsus框架通过学习几何和外观层面的形状概念，实现个性化的3D形状生成，具有广泛的参考样本适用性和优秀的合成效果。


<details>
  <summary>Details</summary>
Motivation: 当前的形状生成方法多集中在单一层面，如仅关注几何形状或仅关注外观属性，这限制了生成的多样性和精细度。PEGAsus框架旨在通过学习几何和外观层面的概念来突破这一限制，从而实现更加个性化和精细的3D形状生成。

Method: PEGAsus框架通过两个主要步骤实现其目标：1) 形状个人化被形式化为从参考形状中提取可重用的、跨类别不可知的几何和外观属性，并使用文本将这些属性组合生成新颖的形状；2) 设计一种渐进优化策略，分别在几何和外观层面学习形状概念，分离出两种属性的学习过程，同时引入上下文相关和上下文无关损失，以增强概念提取的灵活性。

Result: PEGAsus框架能够在各种参考形状上有效提取属性，并能基于这些概念与文本灵活合成出新的形状。实验结果表明，该方法在对形状生成控制的精细度和生成结果的多样性方面优于现有最先进的解决方案。

Conclusion: PEGAsus框架为高度个性化的3D形状生成提供了一种有效的解决方案，能够更好地控制生成过程并支持跨类别场景下的多样化创造。

Abstract: We present PEGAsus, a new framework capable of generating Personalized 3D shapes by learning shape concepts at both Geometry and Appearance levels. First, we formulate 3D shape personalization as extracting reusable, category-agnostic geometric and appearance attributes from reference shapes, and composing these attributes with text to generate novel shapes. Second, we design a progressive optimization strategy to learn shape concepts at both the geometry and appearance levels, decoupling the shape concept learning process. Third, we extend our approach to region-wise concept learning, enabling flexible concept extraction, with context-aware and context-free losses. Extensive experimental results show that PEGAsus is able to effectively extract attributes from a wide range of reference shapes and then flexibly compose these concepts with text to synthesize new shapes. This enables fine-grained control over shape generation and supports the creation of diverse, personalized results, even in challenging cross-category scenarios. Both quantitative and qualitative experiments demonstrate that our approach outperforms existing state-of-the-art solutions.

</details>


### [112] [Generative Regression for Left Ventricular Ejection Fraction Estimation from Echocardiography Video](https://arxiv.org/abs/2602.08202)
*Jinrong Lv,Xun Gong,Zhaohuan Li,Weili Jiang*

Main category: cs.CV

TL;DR: 该研究探讨了从成像视频中估计左心室射血分数（LVEF）的逆问题，提出了一种新的生成回归模型，以更好地处理不确定性和多模态数据。


<details>
  <summary>Details</summary>
Motivation: 现有方法如深度学习通常将LVEF估计视为最小化均方误差的标准回归问题，但在病理情况下可能会导致误导性预测。

Method: 提出了一种新颖的多模态条件评分扩散模型（MCSDR），用于从成像视频和患者人口统计学属性先验中推断LVEF的连续后验分布。

Result: MCSDR在EchoNet-Dynamic, EchoNet-Pediatric, 和 CAMUS 数据集上取得了当时最先进的性能，特别是在噪声高或生理变化大的情况下，其生成轨迹展现出不同的行为。

Conclusion: 这项工作为AI辅助诊断提供了新的可解释性层，能够有效处理病理情况下的不确定性与多模态数据。

Abstract: Estimating Left Ventricular Ejection Fraction (LVEF) from echocardiograms constitutes an ill-posed inverse problem. Inherent noise, artifacts, and limited viewing angles introduce ambiguity, where a single video sequence may map not to a unique ground truth, but rather to a distribution of plausible physiological values. Prevailing deep learning approaches typically formulate this task as a standard regression problem that minimizes the Mean Squared Error (MSE). However, this paradigm compels the model to learn the conditional expectation, which may yield misleading predictions when the underlying posterior distribution is multimodal or heavy-tailed -- a common phenomenon in pathological scenarios. In this paper, we investigate the paradigm shift from deterministic regression toward generative regression. We propose the Multimodal Conditional Score-based Diffusion model for Regression (MCSDR), a probabilistic framework designed to model the continuous posterior distribution of LVEF conditioned on echocardiogram videos and patient demographic attribute priors. Extensive experiments conducted on the EchoNet-Dynamic, EchoNet-Pediatric, and CAMUS datasets demonstrate that MCSDR achieves state-of-the-art performance. Notably, qualitative analysis reveals that the generation trajectories of our model exhibit distinct behaviors in cases characterized by high noise or significant physiological variability, thereby offering a novel layer of interpretability for AI-aided diagnosis.

</details>


### [113] [Geospatial-Reasoning-Driven Vocabulary-Agnostic Remote Sensing Semantic Segmentation](https://arxiv.org/abs/2602.08206)
*Chufeng Zhou,Jian Wang,Xinyuan Liu,Xiaokang Zhang*

Main category: cs.CV

TL;DR: 提出了一种名为GR-CoT的框架，旨在通过地理推理链的方法来增强多模态大语言模型（MLLMs）的场景理解能力，从而指导开放词汇分割模型实现精确的语义匹配。


<details>
  <summary>Details</summary>
Motivation: 现有的遥感语义分割方法主要依赖于视觉特征和文本嵌入的被动映射，缺乏地理空间上下文的认知，导致类似的光谱特征但具有不同语义属性的土地覆盖类别在分割时出现严重的语义混淆和误分类。

Method: GR-CoT框架通过两个协作组件：离线知识蒸馏流和在线实例推理流，来解决地理空间认知的问题。离线流建立精细分类解释标准以解决相似土地覆盖类型之间的语义冲突；在线流执行宏场景锚定、视觉特征解耦和知识驱动决策合成的顺序推理过程，生成自适应词汇指导下游模型达到像素级别的空间语义对齐。

Result: 在LoveDA和GID5基准测试上的广泛实验表明，该方法具有优越性。

Conclusion: GR-CoT框架对多模态大语言模型的场景理解能力进行了增强，提供了新的视角来解决遥感语义分割中的地理空间上下文挑战，展示了在开放词汇分割任务上的应用潜力。

Abstract: Open-vocabulary semantic segmentation has emerged as a promising research direction in remote sensing, enabling the recognition of diverse land-cover types beyond pre-defined category sets. However, existing methods predominantly rely on the passive mapping of visual features and textual embeddings. This ``appearance-based" paradigm lacks geospatial contextual awareness, leading to severe semantic ambiguity and misclassification when encountering land-cover classes with similar spectral features but distinct semantic attributes. To address this, we propose a Geospatial Reasoning Chain-of-Thought (GR-CoT) framework designed to enhance the scene understanding capabilities of Multimodal Large Language Models (MLLMs), thereby guiding open-vocabulary segmentation models toward precise mapping. The framework comprises two collaborative components: an offline knowledge distillation stream and an online instance reasoning stream. The offline stream establishes fine-grained category interpretation standards to resolve semantic conflicts between similar land-cover types. During online inference, the framework executes a sequential reasoning process involving macro-scenario anchoring, visual feature decoupling, and knowledge-driven decision synthesis. This process generates an image-adaptive vocabulary that guides downstream models to achieve pixel-level alignment with correct geographical semantics. Extensive experiments on the LoveDA and GID5 benchmarks demonstrate the superiority of our approach.

</details>


### [114] [Chain-of-Caption: Training-free improvement of multimodal large language model on referring expression comprehension](https://arxiv.org/abs/2602.08211)
*Yik Lung Pang,Changjae Oh*

Main category: cs.CV

TL;DR: 本文分析了使用工具使用提供额外视觉和文本上下文以提高多模态大型语言模型在参考表达理解任务中的性能的效果，并提出了一种无需训练的框架Chain-of-Caption。


<details>
  <summary>Details</summary>
Motivation: 探讨多模态大型语言模型在参考表达理解（REC）任务中的表现，并提出改进方法。

Method: 通过分析不同的技术如何提供额外的视觉和文本上下文来改进MLLMs的REC性能，提出了一个无需训练的框架Chain-of-Caption。

Result: 该研究在RefCOCO/RefCOCOg/RefCOCO+和Ref-L4数据集上进行了实验，发现单独提供的文本或视觉上下文可以在无需微调的情况下提高REC性能。结合多个上下文，Chain-of-Caption框架在不同的交并比（IoU）阈值上获得了5%到30%的性能提升。

Conclusion: 通过Chain-of-Caption框架，可以在不进行训练的情况下显著提高MLLMs的REC任务性能。

Abstract: Given a textual description, the task of referring expression comprehension (REC) involves the localisation of the referred object in an image. Multimodal large language models (MLLMs) have achieved high accuracy on REC benchmarks through scaling up the model size and training data. Moreover, the performance of MLLMs can be further improved using techniques such as Chain-of-Thought and tool use, which provides additional visual or textual context to the model. In this paper, we analyse the effect of various techniques for providing additional visual and textual context via tool use to the MLLM and its effect on the REC task. Furthermore, we propose a training-free framework named Chain-of-Caption to improve the REC performance of MLLMs. We perform experiments on RefCOCO/RefCOCOg/RefCOCO+ and Ref-L4 datasets and show that individual textual or visual context can improve the REC performance without any fine-tuning. By combining multiple contexts, our training-free framework shows between 5% to 30% performance gain over the baseline model on accuracy at various Intersection over Union (IoU) thresholds.

</details>


### [115] [Generating Adversarial Events: A Motion-Aware Point Cloud Framework](https://arxiv.org/abs/2602.08230)
*Hongwei Ren,Youxin Jiang,Qifei Gu,Xiangqian Wu*

Main category: cs.CV

TL;DR: MA-ADV 提出了一种新颖的运动感知对抗框架，通过点云表示生成对抗事件，同时考虑高频率噪声，并利用时空关系smooth 扰动。最终，通过样本梯度优化、迭代细化和二分搜索识别最小成本扰动。实验结果证实了 MA-ADV 在最小扰动下实现 100% 攻击成功率，并提高了对防御措施的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 由于事件表示主流方法的非可微性，传统的梯度基攻击方法难以应用于事件层面。因此，本文提出 MA-ADV 以填补该领域空白，改善事件系统对抗性攻击的表现。

Method: MA-ADV 通过引入点云表示，处理高频噪声，并采用基于扩散的方法平滑扰动，利用时空关系优化。最后利用样本梯度优化、迭代细化和二分搜索确定最小成本扰动。

Result: 实验表明，MA-ADV 在最小扰动下能够实现100% 的攻击成功率，并且具有更强的鲁棒性。

Conclusion: 本文提出的 MA-ADV 确实改善了事件感知系统的安全性，展示了对抗性攻击在未来的安全挑战上仍存在的问题。

Abstract: Event cameras have been widely adopted in safety-critical domains such as autonomous driving, robotics, and human-computer interaction. A pressing challenge arises from the vulnerability of deep neural networks to adversarial examples, which poses a significant threat to the reliability of event-based systems. Nevertheless, research into adversarial attacks on events is scarce. This is primarily due to the non-differentiable nature of mainstream event representations, which hinders the extension of gradient-based attack methods. In this paper, we propose MA-ADV, a novel \textbf{M}otion-\textbf{A}ware \textbf{Adv}ersarial framework. To the best of our knowledge, this is the first work to generate adversarial events by leveraging point cloud representations. MA-ADV accounts for high-frequency noise in events and employs a diffusion-based approach to smooth perturbations, while fully leveraging the spatial and temporal relationships among events. Finally, MA-ADV identifies the minimal-cost perturbation through a combination of sample-wise Adam optimization, iterative refinement, and binary search. Extensive experimental results validate that MA-ADV ensures a 100\% attack success rate with minimal perturbation cost, and also demonstrate enhanced robustness against defenses, underscoring the critical security challenges facing future event-based perception systems.

</details>


### [116] [Moving Beyond Functional Connectivity: Time-Series Modeling for fMRI-Based Brain Disorder Classification](https://arxiv.org/abs/2602.08262)
*Guoqi Yu,Xiaowei Hu,Angelica I. Aviles-Rivero,Anqi Qiu,Shujun Wang*

Main category: cs.CV

TL;DR: 该研究评估了直接在原始BOLD信号上进行时序建模的方法，提出了一种称为DeCI的框架，该框架通过分解周期性和漂移成分并独立地处理每个感兴趣区域来提高分类准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的fMRI方法依赖于功能连接性，通过皮尔逊相关性将4D BOLD信号简化为静态2D矩阵，忽略了时间动态信息。研究人员旨在通过直接建模时间信息来改进fMRI分类的准确性。

Method: 研究人员评估了PatchTST、TimesNet和TimeMixer等最先进的时序模型在五个公开数据集上的表现。接着，他们提出了一种称为DeCI的框架，该框架包括周期性和漂移分解，并独立处理每个感兴趣区域。

Result: DeCI框架在各种数据集上显示出比传统功能连接性方法更好的分类准确性和泛化能力。这些模型在直接处理原始BOLD信号方面优于传统的功能连接性方法。

Conclusion: 研究结果强调了对fMRI分析中直接时间建模的重视，以便更好地捕捉复杂的脑部动态。

Abstract: Functional magnetic resonance imaging (fMRI) enables non-invasive brain disorder classification by capturing blood-oxygen-level-dependent (BOLD) signals. However, most existing methods rely on functional connectivity (FC) via Pearson correlation, which reduces 4D BOLD signals to static 2D matrices, discarding temporal dynamics and capturing only linear inter-regional relationships. In this work, we benchmark state-of-the-art temporal models (e.g., time-series models such as PatchTST, TimesNet, and TimeMixer) on raw BOLD signals across five public datasets. Results show these models consistently outperform traditional FC-based approaches, highlighting the value of directly modeling temporal information such as cycle-like oscillatory fluctuations and drift-like slow baseline trends. Building on this insight, we propose DeCI, a simple yet effective framework that integrates two key principles: (i) Cycle and Drift Decomposition to disentangle cycle and drift within each ROI (Region of Interest); and (ii) Channel-Independence to model each ROI separately, improving robustness and reducing overfitting. Extensive experiments demonstrate that DeCI achieves superior classification accuracy and generalization compared to both FC-based and temporal baselines. Our findings advocate for a shift toward end-to-end temporal modeling in fMRI analysis to better capture complex brain dynamics. The code is available at https://github.com/Levi-Ackman/DeCI.

</details>


### [117] [PISCO: Precise Video Instance Insertion with Sparse Control](https://arxiv.org/abs/2602.08277)
*Xiangbo Gao,Renjie Li,Xinghao Chen,Yuheng Wu,Suofei Feng,Qing Yin,Zhengzhong Tu*

Main category: cs.CV

TL;DR: 本研究提出了一种名为PISCO的视频扩散模型，该模型能够通过稀疏的关键帧控制实现精确的视频实例插入，同时采用了变量信息引导、分布保持的 temporal 掩蔽以及几何感知条件，以应对由稀疏条件引起的分布变化，并通过基准测试验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 在AI辅助电影制作中，精确、高效的实例插入是关键需求。现有方法依赖于笨拙的提示工程和“精心挑选”，难以满足高端电影制作的需求。本文旨在通过提出PISCO模型，简化视频实例插入过程，提高编辑效率和最终视频质量。

Method: PISCO结合了视频扩散模型、变量信息引导、分布保持的temporal掩蔽和几何感知条件，通过简单的稀疏关键帧控制实现复杂视频场景的精确插入。

Result: 实验结果表明，PISCO在稀疏控制下优于现有的图像修复和视频编辑基线，并随着额外控制信号的增加表现出明显的性能提升。

Conclusion: PISCO为精确视频实例插入提供了新的方法论，显著提高了用户编辑体验和视频生成质量。未来的研究可以进一步探索在更复杂场景下的应用效果。

Abstract: The landscape of AI video generation is undergoing a pivotal shift: moving beyond general generation - which relies on exhaustive prompt-engineering and "cherry-picking" - towards fine-grained, controllable generation and high-fidelity post-processing. In professional AI-assisted filmmaking, it is crucial to perform precise, targeted modifications. A cornerstone of this transition is video instance insertion, which requires inserting a specific instance into existing footage while maintaining scene integrity. Unlike traditional video editing, this task demands several requirements: precise spatial-temporal placement, physically consistent scene interaction, and the faithful preservation of original dynamics - all achieved under minimal user effort. In this paper, we propose PISCO, a video diffusion model for precise video instance insertion with arbitrary sparse keyframe control. PISCO allows users to specify a single keyframe, start-and-end keyframes, or sparse keyframes at arbitrary timestamps, and automatically propagates object appearance, motion, and interaction. To address the severe distribution shift induced by sparse conditioning in pretrained video diffusion models, we introduce Variable-Information Guidance for robust conditioning and Distribution-Preserving Temporal Masking to stabilize temporal generation, together with geometry-aware conditioning for realistic scene adaptation. We further construct PISCO-Bench, a benchmark with verified instance annotations and paired clean background videos, and evaluate performance using both reference-based and reference-free perceptual metrics. Experiments demonstrate that PISCO consistently outperforms strong inpainting and video editing baselines under sparse control, and exhibits clear, monotonic performance improvements as additional control signals are provided. Project page: xiangbogaobarry.github.io/PISCO.

</details>


### [118] [Language-Guided Transformer Tokenizer for Human Motion Generation](https://arxiv.org/abs/2602.08337)
*Sheng Yan,Yong Wang,Xin Du,Junsong Yuan,Mengyuan Liu*

Main category: cs.CV

TL;DR: 提出了一种名为LG-Tok的语言引导标记化方法，该方法通过将自然语言与运动在标记化阶段对齐，生成紧凑且高层次的语义表示，从而提高运动重建质量并简化生成模型的学习。


<details>
  <summary>Details</summary>
Motivation: 为了在提高运动重建质量的同时减少生成复杂性，该研究提出了使用语言进行标记化的方法——语言引导标记化(LG-Tok)，以平衡标记数量和生成效率之间的关系。

Method: 该方法采用基于Transformer的标记化器，利用注意力机制实现语言和运动的有效对齐，并设计了一种语言丢弃策略，使解标记化器在生成时能够接受语言无关的指导。

Result: 在HumanML3D和Motion-X生成基准测试中，使用LG-Tok的方法取得了优于现有最先进的方法的Top-1得分和FID得分。

Conclusion: 研究表明，使用LG-Tok的方法可以更高效地实现运动的标记化和解标记化，减少了标记数量但仍保持了良好的性能。

Abstract: In this paper, we focus on motion discrete tokenization, which converts raw motion into compact discrete tokens--a process proven crucial for efficient motion generation. In this paradigm, increasing the number of tokens is a common approach to improving motion reconstruction quality, but more tokens make it more difficult for generative models to learn. To maintain high reconstruction quality while reducing generation complexity, we propose leveraging language to achieve efficient motion tokenization, which we term Language-Guided Tokenization (LG-Tok). LG-Tok aligns natural language with motion at the tokenization stage, yielding compact, high-level semantic representations. This approach not only strengthens both tokenization and detokenization but also simplifies the learning of generative models. Furthermore, existing tokenizers predominantly adopt convolutional architectures, whose local receptive fields struggle to support global language guidance. To this end, we propose a Transformer-based Tokenizer that leverages attention mechanisms to enable effective alignment between language and motion. Additionally, we design a language-drop scheme, in which language conditions are randomly removed during training, enabling the detokenizer to support language-free guidance during generation. On the HumanML3D and Motion-X generation benchmarks, LG-Tok achieves Top-1 scores of 0.542 and 0.582, outperforming state-of-the-art methods (MARDM: 0.500 and 0.528), and with FID scores of 0.057 and 0.088, respectively, versus 0.114 and 0.147. LG-Tok-mini uses only half the tokens while maintaining competitive performance (Top-1: 0.521/0.588, FID: 0.085/0.071), validating the efficiency of our semantic representations.

</details>


### [119] [UrbanGraphEmbeddings: Learning and Evaluating Spatially Grounded Multimodal Embeddings for Urban Science](https://arxiv.org/abs/2602.08342)
*Jie Zhang,Xingtong Yu,Yuan Fang,Rudi Stouffs,Zdravko Trivic*

Main category: cs.CV

TL;DR: UGData 提供了一种新的城市环境数据集和 UGE 两阶段训练策略，通过结合指令指导的对比学习和基于图的空间编码，实现了在多种城市理解任务中的显著性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的城市理解数据集和基准通常缺少街道视图图像与城市结构的显式对齐，这使得学习可转移的多模态嵌入具有挑战性。

Method: UGData 数据集通过将街道视图图像与结构化的空间图锚定，并提供用于空间推理路径和空间上下文注释的图对齐监督，UGE 训练策略结合了指令指导的对比学习和基于图的空间编码。

Result: 基于 Qwen2-VL, Qwen2.5-VL, Phi-3-Vision, 和 LLaVA1.6-Mistral 等 VLM 主干网进行开发，固定维度的空间嵌入通过 LoRA 调整，UGE 在图像检索和地理定位排名任务上取得了显著的性能提升。

Conclusion: UGData 和 UGE 方法为基于空间的地城市理解任务提供了有效的数据支持和新的训练策略，显示出明确的空间接地对空间密集型城市任务的有效性。

Abstract: Learning transferable multimodal embeddings for urban environments is challenging because urban understanding is inherently spatial, yet existing datasets and benchmarks lack explicit alignment between street-view images and urban structure. We introduce UGData, a spatially grounded dataset that anchors street-view images to structured spatial graphs and provides graph-aligned supervision via spatial reasoning paths and spatial context captions, exposing distance, directionality, connectivity, and neighborhood context beyond image content. Building on UGData, we propose UGE, a two-stage training strategy that progressively and stably aligns images, text, and spatial structures by combining instruction-guided contrastive learning with graph-based spatial encoding. We finally introduce UGBench, a comprehensive benchmark to evaluate how spatially grounded embeddings support diverse urban understanding tasks -- including geolocation ranking, image retrieval, urban perception, and spatial grounding. We develop UGE on multiple state-of-the-art VLM backbones, including Qwen2-VL, Qwen2.5-VL, Phi-3-Vision, and LLaVA1.6-Mistral, and train fixed-dimensional spatial embeddings with LoRA tuning. UGE built upon Qwen2.5-VL-7B backbone achieves up to 44% improvement in image retrieval and 30% in geolocation ranking on training cities, and over 30% and 22% gains respectively on held-out cities, demonstrating the effectiveness of explicit spatial grounding for spatially intensive urban tasks.

</details>


### [120] [What, Whether and How? Unveiling Process Reward Models for Thinking with Images Reasoning](https://arxiv.org/abs/2602.08346)
*Yujin Zhou,Pengcheng Wen,Jiale Chen,Boqin Yin,Han Zhu,Jiaming Ji,Juntao Dai,Chi-Min Chan,Sirui Han*

Main category: cs.CV

TL;DR: 本文提出了首个针对图像思维情景下的过程奖励模型（PRMs）的全面基准，定义了7种细化错误类型，并展示了现有大型视觉语言模型在处理图像思维任务上的不足。


<details>
  <summary>Details</summary>
Motivation: 随着大型视觉语言模型（LVLMs）的发展和图像思维处理方式的引入，现有评估基准主要针对文本，无法全面评估PRMs在图像思维任务中的表现，为此作者提出了一个新的基准。

Method: 作者通过对推理轨迹的深入分析和引导式搜索实验，定义了7种细化错误类型，并构建了一个包含1206个手动标注的推理轨迹的全面基准。

Result: 实验证明，当前的LVLMs在处理图像思维任务时表现不佳，尤其是在视觉推理过程评估方面存在显著的性能差异、正向评估偏差和对推理步骤位置的敏感性。

Conclusion: 作者的研究表明了新基准的有效性，并为推进PRMs在LVLMs中的应用奠定了基础。

Abstract: The rapid advancement of Large Vision Language Models (LVLMs) has demonstrated excellent abilities in various visual tasks. Building upon these developments, the thinking with images paradigm has emerged, enabling models to dynamically edit and re-encode visual information at each reasoning step, mirroring human visual processing. However, this paradigm introduces significant challenges as diverse errors may occur during reasoning processes. This necessitates Process Reward Models (PRMs) for distinguishing positive and negative reasoning steps, yet existing benchmarks for PRMs are predominantly text-centric and lack comprehensive assessment under this paradigm. To address these gaps, this work introduces the first comprehensive benchmark specifically designed for evaluating PRMs under the thinking with images paradigm. Our main contributions are: (1) Through extensive analysis of reasoning trajectories and guided search experiments with PRMs, we define 7 fine-grained error types and demonstrate both the necessity for specialized PRMs and the potential for improvement. (2) We construct a comprehensive benchmark comprising 1,206 manually annotated thinking with images reasoning trajectories spanning 4 categories and 16 subcategories for fine-grained evaluation of PRMs. (3) Our experimental analysis reveals that current LVLMs fall short as effective PRMs, exhibiting limited capabilities in visual reasoning process evaluation with significant performance disparities across error types, positive evaluation bias, and sensitivity to reasoning step positions. These findings demonstrate the effectiveness of our benchmark and establish crucial foundations for advancing PRMs in LVLMs.

</details>


### [121] [E-VAds: An E-commerce Short Videos Understanding Benchmark for MLLMs](https://arxiv.org/abs/2602.08355)
*Xianjie Liu,Yiman Hu,Liang Wu,Ping Hu,Yixiong Zou,Jian Xu,Bo Zheng*

Main category: cs.CV

TL;DR: 该研究提出了一个多模态信息密度评估框架，并创建了首个针对电商短视频理解的基准数据集E-VAds，以及一种基于强化学习的多粒度奖励机制的推理模型E-VAds-R1。实验结果表明，该模型在商业意图推理任务上具有显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 为解决现有模型在处理具有商业意图的电商短视频时遇到的挑战，研究者提出了新的评估框架和数据集，以提高视频理解领域的复杂度。

Method: 研究者首先构建了一个多模态信息密度评估框架，量化了电商内容的复杂性。然后基于此框架创建了E-VAds数据集，并开发了一种采用多粒度奖励机制的_rl模型。

Result: 实验结果显示E-VAds-R1在商业意图推理任务上取得了显著的性能提升，仅需少量训练样本即可实现109.2%的性能提升。

Conclusion: 研究提出的方法和框架为电商短视频的理解提供了新的思路和解决方案。

Abstract: E-commerce short videos represent a high-revenue segment of the online video industry characterized by a goal-driven format and dense multi-modal signals. Current models often struggle with these videos because existing benchmarks focus primarily on general-purpose tasks and neglect the reasoning of commercial intent. In this work, we first propose a \textbf{multi-modal information density assessment framework} to quantify the complexity of this domain. Our evaluation reveals that e-commerce content exhibits substantially higher density across visual, audio, and textual modalities compared to mainstream datasets, establishing a more challenging frontier for video understanding. To address this gap, we introduce \textbf{E-commerce Video Ads Benchmark (E-VAds)}, which is the first benchmark specifically designed for e-commerce short video understanding. We curated 3,961 high-quality videos from Taobao covering a wide range of product categories and used a multi-agent system to generate 19,785 open-ended Q&A pairs. These questions are organized into two primary dimensions, namely Perception and Cognition and Reasoning, which consist of five distinct tasks. Finally, we develop \textbf{E-VAds-R1}, an RL-based reasoning model featuring a multi-grained reward design called \textbf{MG-GRPO}. This strategy provides smooth guidance for early exploration while creating a non-linear incentive for expert-level precision. Experimental results demonstrate that E-VAds-R1 achieves a 109.2% performance gain in commercial intent reasoning with only a few hundred training samples.

</details>


### [122] [Geometric Image Editing via Effects-Sensitive In-Context Inpainting with Diffusion Transformers](https://arxiv.org/abs/2602.08388)
*Shuo Zhang,Wenzhuo Wu,Huayu Zhang,Jiarong Cheng,Xianghao Zang,Chao Ban,Hao Sun,Zhongjiang He,Tianwei Cao,Kongming Liang,Zhanyu Ma*

Main category: cs.CV

TL;DR: GeoEdit框架通过引入几何变换模块和效果敏感注意机制，解决了现有扩散模型在复杂场景下对象准确编辑及复杂光照和阴影处理上的局限性，显著提升了图像编辑的质量和真实性。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型在复杂场景下难以准确地进行几何变换编辑和处理复杂的光照与阴影效果，现有方法存在显著的不足。因此，提出了GeoEdit框架来解决这些问题。

Method: GeoEdit框架采用了一种基于扩散变压器模块的设计，能够在图像编辑中整合几何变换，提高了对象编辑的精度。同时，框架引入了效果敏感注意机制，以更好地捕捉和建模复杂的照明和阴影效果。

Result: 通过GeoEdit框架在公开基准上的广泛实验，结果显示它在视效质量、几何精度和真实性方面都显著优于现有最先进的方法。

Conclusion: GeoEdit框架通过结合几何变换模块和效果敏感注意机制，在复杂场景的图像编辑任务上实现了显著的改进，为未来的图像处理提供了重要的贡献。

Abstract: Recent advances in diffusion models have significantly improved image editing. However, challenges persist in handling geometric transformations, such as translation, rotation, and scaling, particularly in complex scenes. Existing approaches suffer from two main limitations: (1) difficulty in achieving accurate geometric editing of object translation, rotation, and scaling; (2) inadequate modeling of intricate lighting and shadow effects, leading to unrealistic results. To address these issues, we propose GeoEdit, a framework that leverages in-context generation through a diffusion transformer module, which integrates geometric transformations for precise object edits. Moreover, we introduce Effects-Sensitive Attention, which enhances the modeling of intricate lighting and shadow effects for improved realism. To further support training, we construct RS-Objects, a large-scale geometric editing dataset containing over 120,000 high-quality image pairs, enabling the model to learn precise geometric editing while generating realistic lighting and shadows. Extensive experiments on public benchmarks demonstrate that GeoEdit consistently outperforms state-of-the-art methods in terms of visual quality, geometric accuracy, and realism.

</details>


### [123] [D$^2$-VR: Degradation-Robust and Distilled Video Restoration with Synergistic Optimization Strategy](https://arxiv.org/abs/2602.08395)
*Jianfeng Liang,Shaocheng Shen,Botao Xu,Qiang Hu,Xiaoyun Zhang*

Main category: cs.CV

TL;DR: D²-VR 是一种单图像扩散基础的视频恢复框架，通过设计鲁棒流动对齐模块和采用对抗蒸馏范式，大幅提高采样效率，实现了优异的视觉质量和严格的时空一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的视频恢复框架存在推断延迟高和时间不稳定的问题，尤其是在处理复杂的现实降级时。D²-VR 框架旨在通过减少推断步骤和提高时间引导精度来解决这些问题。

Method: D²-VR 通过引入一个鲁棒流动对齐模块（DRFA）来增强时间引导准确性，该模块利用可信度感知的注意力机制排除不可靠的运动线索。框架还通过对抗蒸馏范式压缩了扩散采样轨迹，使其进入快速的多步模式，并采用协同优化策略平衡视觉质量和严格的时空一致性。

Result: D²-VR 在不损失视觉质量的情况下显著缩短了采样时间，加速了采样过程12倍。

Conclusion: D²-VR 框架提供了一种高效且保持高质量的视频恢复解决方案，并展示了其在处理复杂降级时的优势。

Abstract: The integration of diffusion priors with temporal alignment has emerged as a transformative paradigm for video restoration, delivering fantastic perceptual quality, yet the practical deployment of such frameworks is severely constrained by prohibitive inference latency and temporal instability when confronted with complex real-world degradations. To address these limitations, we propose \textbf{D$^2$-VR}, a single-image diffusion-based video-restoration framework with low-step inference. To obtain precise temporal guidance under severe degradation, we first design a Degradation-Robust Flow Alignment (DRFA) module that leverages confidence-aware attention to filter unreliable motion cues. We then incorporate an adversarial distillation paradigm to compress the diffusion sampling trajectory into a rapid few-step regime. Finally, a synergistic optimization strategy is devised to harmonize perceptual quality with rigorous temporal consistency. Extensive experiments demonstrate that D$^2$-VR achieves state-of-the-art performance while accelerating the sampling process by \textbf{12$\times$}

</details>


### [124] [RealSynCol: a high-fidelity synthetic colon dataset for 3D reconstruction applications](https://arxiv.org/abs/2602.08397)
*Chiara Lena,Davide Milesi,Alessandro Casella,Luca Carlini,Joseph C. Norton,James Martin,Bruno Scaglioni,Keith L. Obstein,Roberto De Sire,Marco Spadaccini,Cesare Hassan,Pietro Valdastri,Elena De Momi*

Main category: cs.CV

TL;DR: 该研究开发了RealSynCol，一种高度逼真且大规模的合成数据集，用于模拟结肠镜检查的环境，以促进深度学习在结肠镜检查中的应用。


<details>
  <summary>Details</summary>
Motivation: 文章指出，由于缺乏大规模且真实的数据，深度学习在结肠镜检查中的应用进展受限。因此，研究团队提出了RealSynCol，以填补这一空白，提供用于临床图像深度和姿态估计任务的高质量合成数据。

Method: 研究者利用10个CT扫描导出的结肠结构数据，将其导入一个高度逼真的虚拟环境中，并通过添加真实的血管纹理生成这些数据。生成的合成数据集包括28,130帧，附带了深度图、光学流、3D网格和摄像机轨迹的真实标注。

Result: 实验结果显示，RealSynCol在深度和姿态估计任务上展现出卓越的泛化性能，甚至超过了基于真实数据集训练的模型。这表明RealSynCol是一个强有力的研究工具，有助于开发支持内镜诊断的深度学习算法。

Conclusion: 文章结论认为，RealSynCol作为一种高真实度且多样化的合成数据集，为开发用于结肠镜检查的深度学习算法提供了坚实的基础，对于解决实际临床应用中的数据问题具有重要价值。

Abstract: Deep learning has the potential to improve colonoscopy by enabling 3D reconstruction of the colon, providing a comprehensive view of mucosal surfaces and lesions, and facilitating the identification of unexplored areas. However, the development of robust methods is limited by the scarcity of large-scale ground truth data. We propose RealSynCol, a highly realistic synthetic dataset designed to replicate the endoscopic environment. Colon geometries extracted from 10 CT scans were imported into a virtual environment that closely mimics intraoperative conditions and rendered with realistic vascular textures. The resulting dataset comprises 28\,130 frames, paired with ground truth depth maps, optical flow, 3D meshes, and camera trajectories. A benchmark study was conducted to evaluate the available synthetic colon datasets for the tasks of depth and pose estimation. Results demonstrate that the high realism and variability of RealSynCol significantly enhance generalization performance on clinical images, proving it to be a powerful tool for developing deep learning algorithms to support endoscopic diagnosis.

</details>


### [125] [Understanding and Optimizing Attention-Based Sparse Matching for Diverse Local Features](https://arxiv.org/abs/2602.08430)
*Qiang Wang*

Main category: cs.CV

TL;DR: 研究了基于注意力机制的稀疏图像匹配模型，并提出了一种新的微调方法，该方法利用多种检测器的关键点，可以实现与特定检测器训练的模型相当的精度。


<details>
  <summary>Details</summary>
Motivation: 为了改善LightGlue模型的性能，并深入理解基于转换器的匹配框架中检测器和描述符的作用。

Method: 重新审视注意力机制的稀疏图像匹配模型，识别一个关键设计选择；分析检测器和描述符的作用；提出基于多种检测器关键点的微调方法。

Result: 提出的微调方法使模型具有检测器无关性，即使在零样本情况下也能达到或超过特定检测器训练的模型的精度。

Conclusion: 研究提供了关于基于转换器的匹配模型部署和未来局部特征设计的重要见解。

Abstract: We revisit the problem of training attention-based sparse image matching models for various local features. We first identify one critical design choice that has been previously overlooked, which significantly impacts the performance of the LightGlue model. We then investigate the role of detectors and descriptors within the transformer-based matching framework, finding that detectors, rather than descriptors, are often the primary cause for performance difference. Finally, we propose a novel approach to fine-tune existing image matching models using keypoints from a diverse set of detectors, resulting in a universal, detector-agnostic model. When deployed as a zero-shot matcher for novel detectors, the resulting model achieves or exceeds the accuracy of models specifically trained for those features. Our findings offer valuable insights for the deployment of transformer-based matching models and the future design of local features.

</details>


### [126] [Demo-ICL: In-Context Learning for Procedural Video Knowledge Acquisition](https://arxiv.org/abs/2602.08439)
*Yuhao Dong,Shulin Tian,Shuai Liu,Shuangrui Ding,Yuhang Zang,Xiaoyi Dong,Yuhang Cao,Jiaqi Wang,Ziwei Liu*

Main category: cs.CV

TL;DR: 本文提出了一个新的视频内部上下文学习任务和相应的基准Demo-ICL-Bench，旨在评估模型从示例中学习的能力。同时，还介绍了一种名为Demo-ICL的两阶段训练策略，用于解决这一新挑战。


<details>
  <summary>Details</summary>
Motivation: 当前的视频基准主要评估模型的静态知识，而非动态学习和适应能力。本文旨在填补这一空白，提出了一个新的评估任务和基准，以考察模型在少量示例下的学习和适应能力。

Method: 开发了一种名为Demo-ICL的模型，采用了两阶段训练策略：视频监督微调和信息辅助直接偏好优化，以提升模型从上下文示例中学习的能力。

Result: 在最新的多模态大语言模型上进行的实验表明，Demo-ICL-Bench 基准具有较高的难度，同时也验证了Demo-ICL方法的有效性。

Conclusion: 本文揭示了未来在多模态学习和视频理解方面的研究方向。

Abstract: Despite the growing video understanding capabilities of recent Multimodal Large Language Models (MLLMs), existing video benchmarks primarily assess understanding based on models' static, internal knowledge, rather than their ability to learn and adapt from dynamic, novel contexts from few examples. To bridge this gap, we present Demo-driven Video In-Context Learning, a novel task focused on learning from in-context demonstrations to answer questions about the target videos. Alongside this, we propose Demo-ICL-Bench, a challenging benchmark designed to evaluate demo-driven video in-context learning capabilities. Demo-ICL-Bench is constructed from 1200 instructional YouTube videos with associated questions, from which two types of demonstrations are derived: (i) summarizing video subtitles for text demonstration; and (ii) corresponding instructional videos as video demonstrations. To effectively tackle this new challenge, we develop Demo-ICL, an MLLM with a two-stage training strategy: video-supervised fine-tuning and information-assisted direct preference optimization, jointly enhancing the model's ability to learn from in-context examples. Extensive experiments with state-of-the-art MLLMs confirm the difficulty of Demo-ICL-Bench, demonstrate the effectiveness of Demo-ICL, and thereby unveil future research directions.

</details>


### [127] [Vista: Scene-Aware Optimization for Streaming Video Question Answering under Post-Hoc Queries](https://arxiv.org/abs/2602.08448)
*Haocheng Lu,Nan Zhang,Wei Tao,Xiaoyang Qu,Guokuan Li,Jiguang Wan,Jianzong Wang*

Main category: cs.CV

TL;DR: Vista 是一个新型的场景感知流媒体视频问答框架，通过动态聚类、压缩和按需召回，有效处理连续视频流，并保持流畅性和内存效率。


<details>
  <summary>Details</summary>
Motivation: 现有的基于固定大小内存或简单压缩的方法在长文本实时场景中容易出现上下文损失或内存溢出的问题。

Method: Vista 通过场景感知分割、压缩和按需召回三个创新方面解决流媒体视频问答中的场景感知推理问题。

Result: Vista 设计了高效的 GPU 记忆体存储策略，并通过场景的选择性召回提高问答的效率和完整性。

Conclusion: 实验表明，Vista 在 StreamingBench 上达到了最先进的性能，为实际的流媒体视频理解奠定了基础。

Abstract: Streaming video question answering (Streaming Video QA) poses distinct challenges for multimodal large language models (MLLMs), as video frames arrive sequentially and user queries can be issued at arbitrary time points. Existing solutions relying on fixed-size memory or naive compression often suffer from context loss or memory overflow, limiting their effectiveness in long-form, real-time scenarios. We present Vista, a novel framework for scene-aware streaming video QA that enables efficient and scalable reasoning over continuous video streams. The innovation of Vista can be summarized in three aspects: (1) scene-aware segmentation, where Vista dynamically clusters incoming frames into temporally and visually coherent scene units; (2) scene-aware compression, where each scene is compressed into a compact token representation and stored in GPU memory for efficient index-based retrieval, while full-resolution frames are offloaded to CPU memory; and (3) scene-aware recall, where relevant scenes are selectively recalled and reintegrated into the model input upon receiving a query, enabling both efficiency and completeness. Vista is model-agnostic and integrates seamlessly with a variety of vision-language backbones, enabling long-context reasoning without compromising latency or memory efficiency. Extensive experiments on StreamingBench demonstrate that Vista achieves state-of-the-art performance, establishing a strong baseline for real-world streaming video understanding.

</details>


### [128] [TriC-Motion: Tri-Domain Causal Modeling Grounded Text-to-Motion Generation](https://arxiv.org/abs/2602.08462)
*Yiyang Cao,Yunze Deng,Ziyu Lin,Bin Feng,Xinggang Wang,Wenyu Liu,Dandan Zheng,Jingdong Chen*

Main category: cs.CV

TL;DR: TriC-Motion 提出了一种新颖的以扩散为基础的框架，该框架结合了时空频域建模和因果干预，用于生成与文本对齐且高质量的运动序列。


<details>
  <summary>Details</summary>
Motivation: 当前的方法主要集中在时空域建模或独立的频域分析上，缺乏跨时空、频域的联合优化框架，这限制了模型无法同时利用所有域的信息，导致生成质量不佳。

Method: TriC-Motion 使用了三个核心建模模块，分别是时间运动编码、空间拓扑建模和混合频域分析，并结合因果干预机制，通过得分指导的三域融合模块确保时间一致性、空间拓扑、运动趋势和动力学。还设计了一种基于因果性的反事实运动分离器，以分离与运动无关的噪声。

Result: 在 HumanML3D 数据集上，TriC-Motion 达到了 0.612 的 R@1，优于现有方法，生成了高保真、连贯、多样并且与文本对齐的运动序列。

Conclusion: TriC-Motion 通过跨域联合优化和因果干预实现了高质量的运动生成，是文本到运动生成领域的一个重要进展。

Abstract: Text-to-motion generation, a rapidly evolving field in computer vision, aims to produce realistic and text-aligned motion sequences. Current methods primarily focus on spatial-temporal modeling or independent frequency domain analysis, lacking a unified framework for joint optimization across spatial, temporal, and frequency domains. This limitation hinders the model's ability to leverage information from all domains simultaneously, leading to suboptimal generation quality. Additionally, in motion generation frameworks, motion-irrelevant cues caused by noise are often entangled with features that contribute positively to generation, thereby leading to motion distortion. To address these issues, we propose Tri-Domain Causal Text-to-Motion Generation (TriC-Motion), a novel diffusion-based framework integrating spatial-temporal-frequency-domain modeling with causal intervention. TriC-Motion includes three core modeling modules for domain-specific modeling, namely Temporal Motion Encoding, Spatial Topology Modeling, and Hybrid Frequency Analysis. After comprehensive modeling, a Score-guided Tri-domain Fusion module integrates valuable information from the triple domains, simultaneously ensuring temporal consistency, spatial topology, motion trends, and dynamics. Moreover, the Causality-based Counterfactual Motion Disentangler is meticulously designed to expose motion-irrelevant cues to eliminate noise, disentangling the real modeling contributions of each domain for superior generation. Extensive experimental results validate that TriC-Motion achieves superior performance compared to state-of-the-art methods, attaining an outstanding R@1 of 0.612 on the HumanML3D dataset. These results demonstrate its capability to generate high-fidelity, coherent, diverse, and text-aligned motion sequences. Code is available at: https://caoyiyang1105.github.io/TriC-Motion/.

</details>


### [129] [Gesture Matters: Pedestrian Gesture Recognition for AVs Through Skeleton Pose Evaluation](https://arxiv.org/abs/2602.08479)
*Alif Rizqullah Mahdi,Mahdi Rezaei,Natasha Merat*

Main category: cs.CV

TL;DR: 该研究提出了一种基于2D姿态估计的手势分类框架，通过分析WIVW数据集中的实时视频序列，将手势分为四类，并提取了76个静态和动态特征，实现了87%的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶车辆(AVs)的出现，理解行人和司机之间的非言语手势变得至关重要，以弥补正式交通规则的不足。本研究旨在提升AV系统的感知能力，更好地理解行人行为。

Method: 利用WIVW数据集中的实时视频序列，通过2D姿态估计技术提取手势的76个静态和动态特征，包括手部位置和运动速度等关键特征。

Result: 该分类框架实现了87%的分类准确率，特别是在区分不同手势类别时表现优异。

Conclusion: 研究结果不仅提高了AV系统的感知能力，还对行人行为在交通环境中的理解做出了贡献。

Abstract: Gestures are a key component of non-verbal communication in traffic, often helping pedestrian-to-driver interactions when formal traffic rules may be insufficient. This problem becomes more apparent when autonomous vehicles (AVs) struggle to interpret such gestures. In this study, we present a gesture classification framework using 2D pose estimation applied to real-world video sequences from the WIVW dataset. We categorise gestures into four primary classes (Stop, Go, Thank & Greet, and No Gesture) and extract 76 static and dynamic features from normalised keypoints. Our analysis demonstrates that hand position and movement velocity are especially discriminative in distinguishing between gesture classes, achieving a classification accuracy score of 87%. These findings not only improve the perceptual capabilities of AV systems but also contribute to the broader understanding of pedestrian behaviour in traffic contexts.

</details>


### [130] [Enhanced Food Category Recognition under Illumination-Induced Domain Shift](https://arxiv.org/abs/2602.08491)
*Keonvin Park,Aditya Pal,Jin Hong Mok*

Main category: cs.CV

TL;DR: 该研究利用Food-101和Fruits-360两个数据集探索光照引起的多类别食品类别识别中的领域转移问题，通过系统地改变光照温度和强度创建合成光照增强数据集，以实现无额外标签的控制鲁棒性分析，并评估跨数据集的迁移学习和领域泛化能力，以提高识别鲁棒性，同时保持实时性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究集中在单类别食品或控制环境下，公共食品数据集缺乏光照标注，研究旨在克服光照变化导致的领域偏移问题，提高真实场景下食品识别系统的可靠性。

Method: 通过系统地改变数据集中的光照条件，创建合成光照增强数据集，利用这些数据集进行光照敏感目标类别的跨数据集迁移学习和领域泛化评价。

Result: 实验结果表明，光照感知增强显著提高了识别鲁棒性，并且可以实现实时性能。

Conclusion: 研究强调了光照鲁棒性的重要性，并为部署可靠的食品识别系统提供了实用指导。

Abstract: Visual food recognition systems deployed in real-world environments, such as automated conveyor-belt inspection, are highly sensitive to domain shifts caused by illumination changes. While recent studies have shown that lighting variations can significantly distort food perception by both humans and AI, existing works are often limited to single food categories or controlled settings, and most public food datasets lack explicit illumination annotations.
  In this work, we investigate illumination-induced domain shift in multi-class food category recognition using two widely adopted datasets, Food-101 and Fruits-360. We demonstrate substantial accuracy degradation under cross-dataset evaluation due to mismatched visual conditions. To address this challenge, we construct synthetic illumination-augmented datasets by systematically varying light temperature and intensity, enabling controlled robustness analysis without additional labels.
  We further evaluate cross-dataset transfer learning and domain generalization, with a focus on illumination-sensitive target categories such as apple-based classes. Experimental results show that illumination-aware augmentation significantly improves recognition robustness under domain shift while preserving real-time performance. Our findings highlight the importance of illumination robustness and provide practical insights for deploying reliable food recognition systems in real-world inspection scenarios.

</details>


### [131] [GeoFocus: Blending Efficient Global-to-Local Perception for Multimodal Geometry Problem-Solving](https://arxiv.org/abs/2602.08524)
*Linger Deng,Yuliang Liu,Wenwen Yu,Zujia Zhang,Jianzhong Ju,Zhenbo Luo,Xiang Bai*

Main category: cs.CV

TL;DR: GeoFocus是一种新颖的框架，包含两个核心模块：Critical Local Perceptor和VertexLang。该框架通过理论指导的感知模板自动识别关键局部结构，提高了局部特征覆盖范围。VertexLang则通过顶点坐标和连接关系编码全局图形，缩短了全局感知的训练时间并提高了拓扑识别的准确性。


<details>
  <summary>Details</summary>
Motivation: LMMs在几何问题求解上面临挑战，需要同时进行全局形状识别和关注几何理论相关的复杂局部关系。GeoFocus旨在提高LMMs在几何问题上的表现。

Method: GeoFocus包含Critical Local Perceptor和VertexLang两个核心模块。Critical Local Perceptor通过十三种理论指导的感知模板识别关键局部结构，提升局部特征覆盖范围。VertexLang通过顶点坐标和连接关系编码全局图形，替代了耗时的代码式编码。

Result: GeoFocus在Geo3K，GeoQA和FormalGeo7K上的评估结果显示，其在准确性上提高了4.7%，并表现出在MATHVERSE中对多种视觉条件的鲁棒性。

Conclusion: GeoFocus通过引入理论指导的感知模块和紧凑的拓扑语言，在几何问题求解上取得了显著的提升，表明了其在这一领域的潜力。

Abstract: Geometry problem-solving remains a significant challenge for Large Multimodal Models (LMMs), requiring not only global shape recognition but also attention to intricate local relationships related to geometric theory. To address this, we propose GeoFocus, a novel framework comprising two core modules. 1) Critical Local Perceptor, which automatically identifies and emphasizes critical local structure (e.g., angles, parallel lines, comparative distances) through thirteen theory-based perception templates, boosting critical local feature coverage by 61% compared to previous methods. 2) VertexLang, a compact topology formal language, encodes global figures through vertex coordinates and connectivity relations. By replacing bulky code-based encodings, VertexLang reduces global perception training time by 20% while improving topology recognition accuracy. When evaluated in Geo3K, GeoQA, and FormalGeo7K, GeoFocus achieves a 4.7% accuracy improvement over leading specialized models and demonstrates superior robustness in MATHVERSE under diverse visual conditions. Project Page -- https://github.com/dle666/GeoFocus

</details>


### [132] [Automatic regularization parameter choice for tomography using a double model approach](https://arxiv.org/abs/2602.08528)
*Chuyang Wu,Samuli Siltanen*

Main category: cs.CV

TL;DR: 提出了一种基于两种不同计算离散化问题的反馈控制算法来自动选择正则化参数的方法，并通过实际X射线层析成像数据证明了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: X射线断层成像（X-ray tomography）是一种不稳定的逆问题，尤其是在数据有限的情况下。因此，选择合适的正则化参数是必要的，但这一过程依赖于平衡数据保真度和先验信息。

Method: 提出了一种新的自动参数选择方法，基于同一问题的两种不同计算离散化。该方法使用反馈控制算法动态调整正则化强度，驱使迭代重建向在两种网格上具有足够相似性的最小参数收敛。

Result: 该方法在真实X射线断层成像数据上的有效性被演示出来。

Conclusion: 提出的方法展示了自动选择适合X射线断层成像的正则化参数的潜力。

Abstract: Image reconstruction in X-ray tomography is an ill-posed inverse problem, particularly with limited available data. Regularization is thus essential, but its effectiveness hinges on the choice of a regularization parameter that balances data fidelity against a priori information. We present a novel method for automatic parameter selection based on the use of two distinct computational discretizations of the same problem. A feedback control algorithm dynamically adjusts the regularization strength, driving an iterative reconstruction toward the smallest parameter that yields sufficient similarity between reconstructions on the two grids. The effectiveness of the proposed approach is demonstrated using real tomographic data.

</details>


### [133] [Thegra: Graph-based SLAM for Thermal Imagery](https://arxiv.org/abs/2602.08531)
*Anastasiia Kornilova,Ivan Moskalenko,Arabella Gromova,Gonzalo Ferrer,Alexander Menshchikov*

Main category: cs.CV

TL;DR: 该研究提出了一种针对热成像的稀疏单目图基SLAM系统，利用通用学习特征和预处理管道来增强特征匹配的鲁棒性，并提高估计的稳健性。


<details>
  <summary>Details</summary>
Motivation: 热成像在低光照、烟雾或恶劣天气等视觉退化环境中提供了一种实用的感知模式，但由于热图像常常表现出低纹理、低对比度和高噪声，因此基于特征的SLAM存在困难。本文通过改进提高特征匹配的鲁棒性和估计的稳健性来解决这些问题。

Method: 本文提出的方法包括引入一个优化的预处理管道，适应热影像数据，以及在核心SLAM模块中集成关键点置信度得分，以增强其鲁棒性。

Result: 在公共热成像数据集上的评估表明，提出的方法能够在无需特定数据集的训练或微调所需的特征检测器的情况下获得可靠的表现。

Conclusion: 该方法显著提高了热成像下SLAM系统的性能，并且代码将在发表后公开提供。

Abstract: Thermal imaging provides a practical sensing modality for visual SLAM in visually degraded environments such as low illumination, smoke, or adverse weather. However, thermal imagery often exhibits low texture, low contrast, and high noise, complicating feature-based SLAM. In this work, we propose a sparse monocular graph-based SLAM system for thermal imagery that leverages general-purpose learned features -- the SuperPoint detector and LightGlue matcher, trained on large-scale visible-spectrum data to improve cross-domain generalization. To adapt these components to thermal data, we introduce a preprocessing pipeline to enhance input suitability and modify core SLAM modules to handle sparse and outlier-prone feature matches. We further incorporate keypoint confidence scores from SuperPoint into a confidence-weighted factor graph to improve estimation robustness. Evaluations on public thermal datasets demonstrate that the proposed system achieves reliable performance without requiring dataset-specific training or fine-tuning a desired feature detector, given the scarcity of quality thermal data. Code will be made available upon publication.

</details>


### [134] [TIBR4D: Tracing-Guided Iterative Boundary Refinement for Efficient 4D Gaussian Segmentation](https://arxiv.org/abs/2602.08540)
*He Wu,Xia Yan,Yanghui Xu,Liegang Xia,Jiazhou Chen*

Main category: cs.CV

TL;DR: 本论文提出了一种高效的无学习4D高斯分割框架TIBR4D，通过两阶段迭代边界精炼方法改进视频分割掩码的3D对象分割。该方法处理遮挡和保留对象结构方面优于现有方法，且在HyperNeRF和Neu3D上的实验结果表明，相较于最先进的方法，本方法能生成更准确的对象高斯点云且效率更高。


<details>
  <summary>Details</summary>
Motivation: 传统的3D对象分割方法在动态4D场景中面对复杂运动、遮挡和模糊边界存在较大挑战，导致分割精度和效率受限。本文旨在提出一种新的无学习框架TIBR4D，以提升视频分割掩码在4D空间中的分割性能。

Method: TIBR4D框架包含两个阶段：一是迭代高斯实例跟踪（IGIT）阶段，通过迭代追踪在时间分割级别逐步优化高斯到实例的概率，并提取更好的处理遮挡和保留对象结构的高斯点云；二是帧级高斯渲染范围控制（RCC）阶段，通过抑制接近对象边界的高不确定高斯来保留核心贡献以获得更精确的边界。此外，该方法提出了时间分割合并策略以平衡身份一致性和动态意识。

Result: 在HyperNeRF和Neu3D的数据集上进行的实验结果表明，TIBR4D方法产生的对象高斯点云具有更清晰的边界，并且相比目前最先进的方法具有更高的效率。

Conclusion: TIBR4D框架的提出显著提升了4D视频中3D对象分割的精度和效率，为动态场景下的高斯分割提供了新的解决方案。

Abstract: Object-level segmentation in dynamic 4D Gaussian scenes remains challenging due to complex motion, occlusions, and ambiguous boundaries. In this paper, we present an efficient learning-free 4D Gaussian segmentation framework that lifts video segmentation masks to 4D spaces, whose core is a two-stage iterative boundary refinement, TIBR4D. The first stage is an Iterative Gaussian Instance Tracing (IGIT) at the temporal segment level. It progressively refines Gaussian-to-instance probabilities through iterative tracing, and extracts corresponding Gaussian point clouds that better handle occlusions and preserve completeness of object structures compared to existing one-shot threshold-based methods. The second stage is a frame-wise Gaussian Rendering Range Control (RCC) via suppressing highly uncertain Gaussians near object boundaries while retaining their core contributions for more accurate boundaries. Furthermore, a temporal segmentation merging strategy is proposed for IGIT to balance identity consistency and dynamic awareness. Longer segments enforce stronger multi-frame constraints for stable identities, while shorter segments allow identity changes to be captured promptly. Experiments on HyperNeRF and Neu3D demonstrate that our method produces accurate object Gaussian point clouds with clearer boundaries and higher efficiency compared to SOTA methods.

</details>


### [135] [GOT-Edit: Geometry-Aware Generic Object Tracking via Online Model Editing](https://arxiv.org/abs/2602.08550)
*Shih-Fang Chen,Jun-Cheng Chen,I-Hong Jhuo,Yen-Yu Lin*

Main category: cs.CV

TL;DR: GOT-Edit 通过结合几何感知线索和语义信息，在处理遮挡和杂乱环境时，实现了比传统方法更好的鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的通用物体跟踪方法主要依赖于2D特征，忽略了3D几何先验知识，这使得它们在处理遮挡、干扰物和几何或外观变化时表现不佳。

Method: GOT-Edit 采用了一种在线跨模态模型编辑方法，利用预训练的视觉几何语义变换器从少量的2D图像中推断几何线索，并通过约束空间的在线模型编辑，实现几何信息的整合。

Result: 实验结果表明，GOT-Edit 在多个通用对象跟踪基准测试中表现出更优异的鲁棒性和准确性，特别是在遮挡和杂乱场景下。

Conclusion: GOT-Edit 成功地将2D语义与3D几何推理结合，为通用对象跟踪提供了新的范式。

Abstract: Human perception for effective object tracking in a 2D video stream arises from the implicit use of prior 3D knowledge combined with semantic reasoning. In contrast, most generic object tracking (GOT) methods primarily rely on 2D features of the target and its surroundings while neglecting 3D geometric cues, which makes them susceptible to partial occlusion, distractors, and variations in geometry and appearance. To address this limitation, we introduce GOT-Edit, an online cross-modality model editing approach that integrates geometry-aware cues into a generic object tracker from a 2D video stream. Our approach leverages features from a pre-trained Visual Geometry Grounded Transformer to enable geometric cue inference from only a few 2D images. To tackle the challenge of seamlessly combining geometry and semantics, GOT-Edit performs online model editing with null-space constrained updates that incorporate geometric information while preserving semantic discrimination, yielding consistently better performance across diverse scenarios. Extensive experiments on multiple GOT benchmarks demonstrate that GOT-Edit achieves superior robustness and accuracy, particularly under occlusion and clutter, establishing a new paradigm for combining 2D semantics with 3D geometric reasoning for generic object tracking.

</details>


### [136] [SemiNFT: Learning to Transfer Presets from Imitation to Appreciation via Hybrid-Sample Reinforcement Learning](https://arxiv.org/abs/2602.08582)
*Melany Yang,Yuhang Yu,Diwang Weng,Jinwei Chen,Wei Dong*

Main category: cs.CV

TL;DR: SemiNFT 是一种利用扩散转形式（DiT）的人工智能果实修复框架，通过模仿和创造逐步学习，同时保持结构和色彩一致，提升了对美学的理解，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖专业知识，难以满足非专家需求；参考方法以全局颜色映射为主，缺乏语义理解和审美感知。因此，研究提出了一种结合模仿和创造的半监督增强学习方法 SemiNFT，旨在提升成果修复的质量和审美水平。

Method: SemiNFT 是一种基于扩散转形式（DiT）的方法，首先通过配对三元组学习基本的结构和色彩匹配技能，再通过未配对数据的强化学习阶段培养细致的审美感知。特别地，强化学习阶段采用混合在线-离线奖励机制，确保不断更新技能的同时不会遗忘旧的知识。

Result: 实验证明，SemiNFT 在标准预设转移基准上优于现有方法，并且能够在零样本任务（如黑白照片上色、跨域预设转移）中展示出显著的智能性，证实了其对审美理解的高级水平。

Conclusion: SemiNFT 通过模仿和创造的结合，显著提升了影像修复的复杂度和美学理解，是一种强有力的人工智能视觉内容创建工具。

Abstract: Photorealistic color retouching plays a vital role in visual content creation, yet manual retouching remains inaccessible to non-experts due to its reliance on specialized expertise. Reference-based methods offer a promising alternative by transferring the preset color of a reference image to a source image. However, these approaches often operate as novice learners, performing global color mappings derived from pixel-level statistics, without a true understanding of semantic context or human aesthetics. To address this issue, we propose SemiNFT, a Diffusion Transformer (DiT)-based retouching framework that mirrors the trajectory of human artistic training: beginning with rigid imitation and evolving into intuitive creation. Specifically, SemiNFT is first taught with paired triplets to acquire basic structural preservation and color mapping skills, and then advanced to reinforcement learning (RL) on unpaired data to cultivate nuanced aesthetic perception. Crucially, during the RL stage, to prevent catastrophic forgetting of old skills, we design a hybrid online-offline reward mechanism that anchors aesthetic exploration with structural review. % experiments Extensive experiments show that SemiNFT not only outperforms state-of-the-art methods on standard preset transfer benchmarks but also demonstrates remarkable intelligence in zero-shot tasks, such as black-and-white photo colorization and cross-domain (anime-to-photo) preset transfer. These results confirm that SemiNFT transcends simple statistical matching and achieves a sophisticated level of aesthetic comprehension. Our project can be found at https://melanyyang.github.io/SemiNFT/.

</details>


### [137] [Overview and Comparison of AVS Point Cloud Compression Standard](https://arxiv.org/abs/2602.08613)
*Wei Gao,Wenxu Gao,Xingming Mu,Changhao Peng,Ge Li*

Main category: cs.CV

TL;DR: 本文综述了AVS PCC标准的技术特性和与其他标准的性能对比，解决了点云压缩数据量大的问题。


<details>
  <summary>Details</summary>
Motivation: 点云数据在多个领域的广泛应用，但庞大的数据量对传输和存储提出了挑战，因此需要发展点云压缩技术。

Method: 从技术和性能两个方面，对比分析了AVS PCC标准与其他标准（如MPEG的G-PCC和V-PCC）的不同之处。

Result: 本文揭示了AVS PCC标准的独特技术工具和算法，展示了与另一标准相比的优势。

Conclusion: 此研究有助于推动点云数据的有效压缩和应用，提高其在实际应用中的性能和效率。

Abstract: Point cloud is a prevalent 3D data representation format with significant application values in immersive media, autonomous driving, digital heritage protection, etc. However, the large data size of point clouds poses challenges to transmission and storage, which influences the wide deployments. Therefore, point cloud compression plays a crucial role in practical applications for both human and machine perception optimization. To this end, the Moving Picture Experts Group (MPEG) has established two standards for point cloud compression, including Geometry-based Point Cloud Compression (G-PCC) and Video-based Point Cloud Compression (V-PCC). In the meantime, the Audio Video coding Standard (AVS) Workgroup of China also have launched and completed the development for its first generation point cloud compression standard, namely AVS PCC. This new standardization effort has adopted many new coding tools and techniques, which are different from the other counterpart standards. This paper reviews the AVS PCC standard from two perspectives, i.e., the related technologies and performance comparisons.

</details>


### [138] [Inspiration Seeds: Learning Non-Literal Visual Combinations for Generative Exploration](https://arxiv.org/abs/2602.08615)
*Kfir Goldberg,Elad Richardson,Yael Vinker*

Main category: cs.CV

TL;DR: Inspiration Seeds 是一种生成框架，它将图像生成从最终执行转变为探索性构想。给定两张输入图像，该模型能够产生多样且视觉上连贯的组合，揭示输入的潜在关系，而无需用户指定的文本提示。这种方法依赖于视觉方式分解视觉方面，并通过视觉手段合成概念对，从而支持早期和模糊阶段的视觉构想。


<details>
  <summary>Details</summary>
Motivation: 当前使用生成模型时，往往是针对精心设计的文本提示进行优化，而缺乏对发散视觉探索的支持，这种探索往往在创意形成之前出现。设计师经常从松散的视觉参考中汲取灵感，寻求新的创意连接。因此，提出了一种新的生成框架，旨在支持早期和模糊阶段的视觉构想。

Method: 采用CLIP Sparse Autoencoders从CLIP的潜在空间中提取编辑方向，分离概念对，生成多样且视觉上连贯的图像，支持视觉构想。

Result: 成果是产生多样且视觉上连贯的图像组合，能够揭示输入图像间的潜在关系，无需依赖文本提示。这种方法相较于传统方法，更加快速且易于直观地重组视觉元素。

Conclusion: Inspiration Seeds 为生成模型提供了一种新的应用视角，能够支持早期、模糊的创意构思阶段，而非仅用于严格按照文本指令生成最终图像。

Abstract: While generative models have become powerful tools for image synthesis, they are typically optimized for executing carefully crafted textual prompts, offering limited support for the open-ended visual exploration that often precedes idea formation. In contrast, designers frequently draw inspiration from loosely connected visual references, seeking emergent connections that spark new ideas. We propose Inspiration Seeds, a generative framework that shifts image generation from final execution to exploratory ideation. Given two input images, our model produces diverse, visually coherent compositions that reveal latent relationships between inputs, without relying on user-specified text prompts. Our approach is feed-forward, trained on synthetic triplets of decomposed visual aspects derived entirely through visual means: we use CLIP Sparse Autoencoders to extract editing directions in CLIP latent space and isolate concept pairs. By removing the reliance on language and enabling fast, intuitive recombination, our method supports visual ideation at the early and ambiguous stages of creative work.

</details>


### [139] [Improving Reconstruction of Representation Autoencoder](https://arxiv.org/abs/2602.08620)
*Siyu Liu,Chujie Qin,Hubery Yin,Qixin Yan,Zheng-Peng Duan,Chen Li,Jing Lyu,Chun-Le Guo,Chongyi Li*

Main category: cs.CV

TL;DR: LV-RAE 通过引入低级特征增强语义特征，实现了高保真的重建效果，同时保持语义抽象并提升了生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有工作虽然通过 Vision Foundation Models 提高了 LDM 的生成性能，但由于缺乏低级信息导致重建精度降低，成为 LDM 进一步应用的瓶颈。

Method: 提出了 LV-RAE，通过一个代表自编码器（representation autoencoder）来补充语义特征的低级信息，生成高保真重构并保持与语义分布的高对齐。

Result: LV-RAE 提高了重建精度，同时保持了语义抽象，并且增强了生成质量。

Conclusion: 通过调整解码器的鲁棒性以及对生成的潜在变量进行有控制的噪声注入，LV-RAE 在重建和生成任务上都表现优秀，并开源发布。

Abstract: Recent work leverages Vision Foundation Models as image encoders to boost the generative performance of latent diffusion models (LDMs), as their semantic feature distributions are easy to learn. However, such semantic features often lack low-level information (\eg, color and texture), leading to degraded reconstruction fidelity, which has emerged as a primary bottleneck in further scaling LDMs. To address this limitation, we propose LV-RAE, a representation autoencoder that augments semantic features with missing low-level information, enabling high-fidelity reconstruction while remaining highly aligned with the semantic distribution. We further observe that the resulting high-dimensional, information-rich latent make decoders sensitive to latent perturbations, causing severe artifacts when decoding generated latent and consequently degrading generation quality. Our analysis suggests that this sensitivity primarily stems from excessive decoder responses along directions off the data manifold. Building on these insights, we propose fine-tuning the decoder to increase its robustness and smoothing the generated latent via controlled noise injection, thereby enhancing generation quality. Experiments demonstrate that LV-RAE significantly improves reconstruction fidelity while preserving the semantic abstraction and achieving strong generative quality. Our code is available at https://github.com/modyu-liu/LVRAE.

</details>


### [140] [Revisiting [CLS] and Patch Token Interaction in Vision Transformers](https://arxiv.org/abs/2602.08626)
*Alexis Marouani,Oriane Siméoni,Hervé Jégou,Piotr Bojanowski,Huy V. Vo*

Main category: cs.CV

TL;DR: 该研究提出了一种新的方法来专门化不同类型的标记（类标记和间隔标记）的计算流程，以改善密集预测任务中的间隔表示质量。实验表明，在标准基准上，该方法可提高超过2个mIoU点的分割性能，同时保持强大的分类准确性。该改进仅增加了8%的参数量，且无额外计算开销。


<details>
  <summary>Details</summary>
Motivation: Vision Transformers在图像识别任务中表现出色，但标准的归一化层使得类标记和间隔标记的处理有所不同，这可能影响模型性能。本文旨在通过专门化这些标记的计算流程来提高模型效果。

Method: 文章首先分析了标准归一化层是如何引入类标记和间隔标记之间的差异的，然后提出了一种新的方法，即在归一化层和早期的查询-键-值投影中选择性地分离这两种标记的计算流程。

Result: 研究表明，这种方法显著提高了密集预测任务中的间隔表示质量，并在标准基准上实现了超过2个mIoU点的分割性能提升，同时保持了强大的分类准确性。参数增加仅8%，无额外计算开销。

Conclusion: 该研究通过专门化不同标记的计算流程，在不增加复杂度的情况下，提高了Vision Transformers模型在密集预测任务中的性能。这种方法具有良好的通用性，适用于不同规模的模型和学习框架。

Abstract: Vision Transformers have emerged as powerful, scalable and versatile representation learners. To capture both global and local features, a learnable [CLS] class token is typically prepended to the input sequence of patch tokens. Despite their distinct nature, both token types are processed identically throughout the model. In this work, we investigate the friction between global and local feature learning under different pre-training strategies by analyzing the interactions between class and patch tokens. Our analysis reveals that standard normalization layers introduce an implicit differentiation between these token types. Building on this insight, we propose specialized processing paths that selectively disentangle the computational flow of class and patch tokens, particularly within normalization layers and early query-key-value projections. This targeted specialization leads to significantly improved patch representation quality for dense prediction tasks. Our experiments demonstrate segmentation performance gains of over 2 mIoU points on standard benchmarks, while maintaining strong classification accuracy. The proposed modifications introduce only an 8% increase in parameters, with no additional computational overhead. Through comprehensive ablations, we provide insights into which architectural components benefit most from specialization and how our approach generalizes across model scales and learning frameworks.

</details>


### [141] [Deep Learning-Based Fixation Type Prediction for Quality Assurance in Digital Pathology](https://arxiv.org/abs/2602.08652)
*Oskar Thaeter,Tanja Niedermair,Johannes Raffler,Ralf Huss,Peter J. Schüffler*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习的模型，用于预测福尔马林固定石蜡包埋（FFPE）和冷冻切片（FS）的固定类型，使用预扫描的低分辨率缩略图图像，该模型在多个数据集上取得了良好的性能，并显著提高了处理速度。


<details>
  <summary>Details</summary>
Motivation: 当前病理学实验室的固定类型手动标注过程容易出错，影响后续分析和诊断准确性。现有的方法通常需要高分辨率的全幅报导图像（WSI），限制了高通量质量控制的规模。

Method: 该模型利用低分辨率，预扫描的缩略图图像来预测固定类型。实验在TUM研究所的WSI（n=1,200，Leica GT450DX）、TCGA（n=8,800，Leica AT2）的平衡类数据集，以及来自于Augsburg（n=695 [392 FFPE, 303 FS]，Philips UFS）和Regensburg（n=202，3DHISTECH P1000）的平衡类数据集上进行了训练和评估。

Result: 该模型在TCGA数据集上取得了0.88的AUC ROC，并且在Augsburg和Regensburg数据集上分别取得了0.72的AUC ROC，处理时间比现有方法快400倍，每次处理时间为21毫秒。

Conclusion: 该模型提供了一种高效的低分辨率图像标签检测方法，不依赖于高分辨率扫描，为高通量病理工作流程中的质量控制提供了工具，未来的工作将改善和进一步测试该模型在更多扫描设备上的泛化能力。

Abstract: Accurate annotation of fixation type is a critical step in slide preparation for pathology laboratories. However, this manual process is prone to
  errors, impacting downstream analyses and diagnostic accuracy. Existing methods for verifying formalin-fixed, paraffin-embedded (FFPE), and frozen
  section (FS) fixation types typically require full-resolution whole-slide images (WSIs), limiting scalability for high-throughput quality control.
  We propose a deep-learning model to predict fixation types using low-resolution, pre-scan thumbnail images. The model was trained on WSIs from
  the TUM Institute of Pathology (n=1,200, Leica GT450DX) and evaluated on a class-balanced subset of The Cancer Genome Atlas dataset (TCGA, n=8,800,
  Leica AT2), as well as on class-balanced datasets from Augsburg (n=695 [392 FFPE, 303 FS], Philips UFS) and Regensburg (n=202, 3DHISTECH P1000).
  Our model achieves an AUROC of 0.88 on TCGA, outperforming comparable pre-scan methods by 4.8%. It also achieves AUROCs of 0.72 on Regensburg and
  Augsburg slides, underscoring challenges related to scanner-induced domain shifts. Furthermore, the model processes each slide in 21 ms, $400\times$
  faster than existing high-magnification, full-resolution methods, enabling rapid, high-throughput processing.
  This approach provides an efficient solution for detecting labelling errors without relying on high-magnification scans, offering a valuable tool for
  quality control in high-throughput pathology workflows. Future work will improve and evaluate the model's generalisation to additional scanner
  types. Our findings suggest that this method can increase accuracy and efficiency in digital pathology workflows and may be extended to other
  low-resolution slide annotations.

</details>


### [142] [WiFlow: A Lightweight WiFi-based Continuous Human Pose Estimation Network with Spatio-Temporal Feature Decoupling](https://arxiv.org/abs/2602.08661)
*Yi Dao,Lankai Zhang,Hao Liu,Haiwei Zhang,Wenbo Wang*

Main category: cs.CV

TL;DR: WiFlow 提出了一种新颖的WiFi信号连续姿态估计框架，通过时空特征提取和轴向注意力机制，实现了高度准确的姿态识别，同时参数量减少，降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有基于WiFi的方法在连续动作和计算效率方面存在局限，WiFlow旨在改进这些不足。

Method: WiFlow 使用编码-解码结构，其中编码器利用时空和非对称卷积提取CSI的时间和空间特征，并通过轴向注意力捕捉关键点特征及其结构依赖性。解码器将高维特征映射到关键点坐标。

Result: WiFlow 在使用自收集的数据集（包含5名参与者8种日常活动的36万组同步CSI-姿态样本）上训练，达到了在20%阈值下的97.00%正确关键点比例（PCK@20）和50%阈值下的99.48%（PCK@50），并且每关节平均位置误差仅为0.008米。

Conclusion: 与基于视觉的方法相比，WiFlow 降低了模型复杂度和计算成本，为实际的基于WiFi的姿态估计设定了新的性能基准。

Abstract: Human pose estimation is fundamental to intelligent perception in the Internet of Things (IoT), enabling applications ranging from smart healthcare to human-computer interaction. While WiFi-based methods have gained traction, they often struggle with continuous motion and high computational overhead. This work presents WiFlow, a novel framework for continuous human pose estimation using WiFi signals. Unlike vision-based approaches such as two-dimensional deep residual networks that treat Channel State Information (CSI) as images, WiFlow employs an encoder-decoder architecture. The encoder captures spatio-temporal features of CSI using temporal and asymmetric convolutions, preserving the original sequential structure of signals. It then refines keypoint features of human bodies to be tracked and capture their structural dependencies via axial attention. The decoder subsequently maps the encoded high-dimensional features into keypoint coordinates. Trained on a self-collected dataset of 360,000 synchronized CSI-pose samples from 5 subjects performing continuous sequences of 8 daily activities, WiFlow achieves a Percentage of Correct Keypoints (PCK) of 97.00% at a threshold of 20% (PCK@20) and 99.48% at PCK@50, with a mean per-joint position error of 0.008m. With only 4.82M parameters, WiFlow significantly reduces model complexity and computational cost, establishing a new performance baseline for practical WiFi-based human pose estimation. Our code and datasets are available at https://github.com/DY2434/WiFlow-WiFi-Pose-Estimation-with-Spatio-Temporal-Decoupling.git.

</details>


### [143] [ALIVE: Animate Your World with Lifelike Audio-Video Generation](https://arxiv.org/abs/2602.08682)
*Ying Guo,Qijun Gan,Yifu Zhang,Jinlai Liu,Yifei Hu,Pan Xie,Dongjun Qian,Yu Zhang,Ruiqi Li,Yuqi Zhang,Ruibiao Lu,Xiaofeng Mei,Bo Han,Xiang Yin,Bingyue Peng,Zehuan Yuan*

Main category: cs.CV

TL;DR: 本文提出了一种名为ALIVE的生成模型，使其能够实现Sora风格的音频-视频生成与动画。该模型基于预训练的文本到视频模型，通过增加联合音频-视频支路，实现了音频-视频同步和参考动画的功能。


<details>
  <summary>Details</summary>
Motivation: 当前视频生成技术正向着统一的音频-视频生成发展。本文旨在提高现有模型在音频-视频同步及动画生成方面的性能。

Method: The method involves adapting a pretrained Text-to-Video model for Sora-style audio-visual generation and animation. It incorporates Junction Audio-Video Branch with TA-CrossAttn and UniTemp-RoPE for achieving better audio-visual alignment and synchronization. Additionally, a detailed data pipeline for collecting high-quality finetuning data and a new benchmark for comprehensive model evaluation are established.

Result: ALIVE在大规模高质量数据集上进行持续预训练和微调后，展示了优秀的性能，超越了开源模型，并可匹敌商业级解决方案。

Conclusion: 总的来说，本文提出的ALIVE模型为音频-视频生成与动画生成提供了新的工具，并期望促进社区在该领域的研究与发展。

Abstract: Video generation is rapidly evolving towards unified audio-video generation. In this paper, we present ALIVE, a generation model that adapts a pretrained Text-to-Video (T2V) model to Sora-style audio-video generation and animation. In particular, the model unlocks the Text-to-Video&Audio (T2VA) and Reference-to-Video&Audio (animation) capabilities compared to the T2V foundation models. To support the audio-visual synchronization and reference animation, we augment the popular MMDiT architecture with a joint audio-video branch which includes TA-CrossAttn for temporally-aligned cross-modal fusion and UniTemp-RoPE for precise audio-visual alignment. Meanwhile, a comprehensive data pipeline consisting of audio-video captioning, quality control, etc., is carefully designed to collect high-quality finetuning data. Additionally, we introduce a new benchmark to perform a comprehensive model test and comparison. After continue pretraining and finetuning on million-level high-quality data, ALIVE demonstrates outstanding performance, consistently outperforming open-source models and matching or surpassing state-of-the-art commercial solutions. With detailed recipes and benchmarks, we hope ALIVE helps the community develop audio-video generation models more efficiently. Official page: https://github.com/FoundationVision/Alive.

</details>


### [144] [OneVision-Encoder: Codec-Aligned Sparsity as a Foundational Principle for Multimodal Intelligence](https://arxiv.org/abs/2602.08683)
*Feilong Tang,Xiang An,Yunyao Yan,Yin Xie,Bin Qin,Kaicheng Yang,Yifei Shen,Yuanhan Zhang,Chunyuan Li,Shikun Feng,Changrui Chen,Huajie Tan,Ming Hu,Manyuan Zhang,Bo Li,Ziyong Feng,Ziwei Liu,Zongyuan Ge,Jiankang Deng*

Main category: cs.CV

TL;DR: 论文提出了视频理解的OneVision-Encoder模型，该模型通过Codecs Patchification等技术，专门关注信号中的熵丰富区域，从而提高解压质量和效率，实验结果表明该模型在多个视觉任务中表现出色，尤其是视频理解和对比当前强视觉骨架模型有显著提升。


<details>
  <summary>Details</summary>
Motivation: 论文旨在通过重新设计视觉编码模型来改善视觉理解性能。基于信息理论原则，作者认为视觉信号中存在高度冗余，而预测性残差（如运动信息和含义）则是关键信息所在。

Method: OneVision-Encoder模型通过采用Codec Patchification技术，在编码视频时压缩预测性的视觉结构为语义意义，而只对占图像约3.1%到25%的高熵区域进行密集处理。模型使用共享3D RoPE并结合大规模语义概念的集群区分目标进行训练，以捕捉对象持续性和运动动态。

Result: 实验结果表明，OneVision-Encoder在包括图像、视频和文档理解在内的16个基准测试中，表现出色且准确性和效率之间存在正相关关系。该模型能够减少视觉标记和预训练数据的使用，同时仍能超越如Qwen3-ViT等强大视觉骨架模型，特别是在视频理解任务上，平均提高了4.1%。

Conclusion: 论文表明，与传统的均匀计算方式不同，OneVision-Encoder通过聚焦视觉信号中的关键区域，显著提升了视频理解的效率和性能，这一创新为构建下一代视觉通用模型奠定了理论基础。

Abstract: Hypothesis. Artificial general intelligence is, at its core, a compression problem. Effective compression demands resonance: deep learning scales best when its architecture aligns with the fundamental structure of the data. These are the fundamental principles. Yet, modern vision architectures have strayed from these truths: visual signals are highly redundant, while discriminative information, the surprise, is sparse. Current models process dense pixel grids uniformly, wasting vast compute on static background rather than focusing on the predictive residuals that define motion and meaning. We argue that to solve visual understanding, we must align our architectures with the information-theoretic principles of video, i.e., Codecs.
  Method. OneVision-Encoder encodes video by compressing predictive visual structure into semantic meaning. By adopting Codec Patchification, OV-Encoder abandons uniform computation to focus exclusively on the 3.1%-25% of regions rich in signal entropy. To unify spatial and temporal reasoning under irregular token layouts, OneVision-Encoder employs a shared 3D RoPE and is trained with a large-scale cluster discrimination objective over more than one million semantic concepts, jointly capturing object permanence and motion dynamics.
  Evidence. The results validate our core hypothesis: efficiency and accuracy are not a trade-off; they are positively correlated. When integrated into LLM, it consistently outperforms strong vision backbones such as Qwen3-ViT and SigLIP2 across 16 image, video, and document understanding benchmarks, despite using substantially fewer visual tokens and pretraining data. Notably, on video understanding tasks, OV-Encoder achieves an average improvement of 4.1% over Qwen3-ViT. Codec-aligned, patch-level sparsity is a foundational principle, enabling OV-Encoder as a scalable engine for next-generation visual generalists.

</details>


### [145] [TimeChat-Captioner: Scripting Multi-Scene Videos with Time-Aware and Structural Audio-Visual Captions](https://arxiv.org/abs/2602.08711)
*Linli Yao,Yuancheng Wei,Yaojie Zhang,Lei Li,Xinlong Chen,Feifan Song,Ziyue Wang,Kun Ouyang,Yuanxin Liu,Lingpeng Kong,Qi Liu,Pengfei Wan,Kun Gai,Yuanxing Zhang,Xu Sun*

Main category: cs.CV

TL;DR: 本研究提出了Omni Dense Captioning，一种生成包含连续、细致且结构化的音频视频叙述的任务，还提供了用于研究的benchmark和一种新的评估方法。通过大型训练数据集和特定任务的奖励训练出的模型，在多个下游任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 为了生成更密集的语义覆盖，需要一种能够实时生成连续性音频视频叙述的方法，以增强基于视听推理的能力。同时，也需要建立一个高质量的基准，并提供一种新的评估标准来简化场景边界模糊的问题。

Method: 研究提出了Omni DCBench基准，以及一种新的评估方法SodaM。此外，提供了TimeChatCap-42K训练数据集，并通过SFT和GRPO训练出了TimeChat-Captioner-7B模型。

Result: TimeChat-Captioner-7B在多个下游任务中表现优异，超越了现有基准。同时，其密集描述显著提升了音频视频推理和时间定位的能力。

Conclusion: 研究表明，通过Omni Dense Captioning可以更精细地描述音频视频信息，为未来的视听研究提供了重要支持。

Abstract: This paper proposes Omni Dense Captioning, a novel task designed to generate continuous, fine-grained, and structured audio-visual narratives with explicit timestamps. To ensure dense semantic coverage, we introduce a six-dimensional structural schema to create "script-like" captions, enabling readers to vividly imagine the video content scene by scene, akin to a cinematographic screenplay. To facilitate research, we construct OmniDCBench, a high-quality, human-annotated benchmark, and propose SodaM, a unified metric that evaluates time-aware detailed descriptions while mitigating scene boundary ambiguity. Furthermore, we construct a training dataset, TimeChatCap-42K, and present TimeChat-Captioner-7B, a strong baseline trained via SFT and GRPO with task-specific rewards. Extensive experiments demonstrate that TimeChat-Captioner-7B achieves state-of-the-art performance, surpassing Gemini-2.5-Pro, while its generated dense descriptions significantly boost downstream capabilities in audio-visual reasoning (DailyOmni and WorldSense) and temporal grounding (Charades-STA). All datasets, models, and code will be made publicly available at https://github.com/yaolinli/TimeChat-Captioner.

</details>


### [146] [Towards Understanding Multimodal Fine-Tuning: Spatial Features](https://arxiv.org/abs/2602.08713)
*Lachin Naghashyar,Hunar Batra,Ashkan Khakzar,Philip Torr,Ronald Clark,Christian Schroeder de Witt,Constantin Venhoff*

Main category: cs.CV

TL;DR: 该研究通过阶段式模型差异分析揭示了语言模型在多模态微调过程中如何学习“视觉”能力，特别是在空间关系编码和视觉注意力机制上。这种方法提高了多模态训练的可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型在许多任务上表现突出，但对语言后台表示如何适应多模态训练以及视觉能力何时和如何出现仍不清楚。因此，研究者提出了阶段式模型差异分析方法，揭示语言模型在多模态学习中的动态变化。

Method: 研究采用阶段式模型差异分析方法，通过从多模态微调过程中分离出的代表变化，逐步揭示语言模型是如何学习视觉功能的，特别是如何编码空间关系。

Result: 研究发现，在多模态微调过程中，出现了视觉偏好特征，并可追踪这些特征的激活到特定的注意头。此外，研究显示视觉地面化的多模态特征的产生及其对以前仅文本特征的重塑。

Conclusion: 阶段式模型差异分析提高了多模态训练的可解释性，并为理解预训练语言模型如何获得视觉地面化的功能提供了基础。

Abstract: Contemporary Vision-Language Models (VLMs) achieve strong performance on a wide range of tasks by pairing a vision encoder with a pre-trained language model, fine-tuned for visual-text inputs. Yet despite these gains, it remains unclear how language backbone representations adapt during multimodal training and when vision-specific capabilities emerge. In this work, we present the first mechanistic analysis of VLM adaptation. Using stage-wise model diffing, a technique that isolates representational changes introduced during multimodal fine-tuning, we reveal how a language model learns to "see". We first identify vision-preferring features that emerge or reorient during fine-tuning. We then show that a selective subset of these features reliably encodes spatial relations, revealed through controlled shifts to spatial prompts. Finally, we trace the causal activation of these features to a small group of attention heads. Our findings show that stage-wise model diffing reveals when and where spatially grounded multimodal features arise. It also provides a clearer view of modality fusion by showing how visual grounding reshapes features that were previously text-only. This methodology enhances the interpretability of multimodal training and provides a foundation for understanding and refining how pretrained language models acquire vision-grounded capabilities.

</details>


### [147] [Zero-shot System for Automatic Body Region Detection for Volumetric CT and MR Images](https://arxiv.org/abs/2602.08717)
*Farnaz Khun Jush,Grit Werner,Mark Klemens,Matthias Lenga*

Main category: cs.CV

TL;DR: 本文研究了在无需训练的情况下，利用预训练的大规模基础模型知识来实现医学影像中体区检测的可能性，提出并系统评估了三种无训练管道。


<details>
  <summary>Details</summary>
Motivation: 现有的医学影像自动化工作流程依赖于不可靠的DICOM元数据进行解剖区域的识别，本文旨在探索在无需训练的情况下，利用大规模预训练模型实现零样本体区检测的可能性。

Method: 本文提出三种基于无训练的方法：(1) 一种依赖预训练多器官分割模型的规则驱动分割方法，(2) 一种由放射科医生定义规则的大规模多模态语言模型，(3) 一种结合视觉输入和明确解剖证据的大规模多模态语言模型。

Result: 三种方法在887份异质CT和MR扫描图像上进行了评估，其中分割驱动的规则方法表现出最稳定且最强的性能，CT的加权F1分数为0.947，MR的加权F1分数为0.914。大规模语言模型在视觉上显著不同的区域表现竞争力，而结合视觉输入和明确解剖证据的大规模语言模型显示出根本性的限制。

Conclusion: 分割驱动的规则方法为多模态图像中的体区检测提供了令人印象深刻的性能和跨模态的鲁棒性，而大规模语言模型在少数区域中表现出色，但结合视觉输入和明确解剖证据的方法显示出局限性。

Abstract: Reliable identification of anatomical body regions is a prerequisite for many automated medical imaging workflows, yet existing solutions remain heavily dependent on unreliable DICOM metadata. Current solutions mainly use supervised learning, which limits their applicability in many real-world scenarios. In this work, we investigate whether body region detection in volumetric CT and MR images can be achieved in a fully zero-shot manner by using knowledge embedded in large pre-trained foundation models. We propose and systematically evaluate three training-free pipelines: (1) a segmentation-driven rule-based system leveraging pre-trained multi-organ segmentation models, (2) a Multimodal Large Language Model (MLLM) guided by radiologist-defined rules, and (3) a segmentation-aware MLLM that combines visual input with explicit anatomical evidence. All methods are evaluated on 887 heterogeneous CT and MR scans with manually verified anatomical region labels. The segmentation-driven rule-based approach achieves the strongest and most consistent performance, with weighted F1-scores of 0.947 (CT) and 0.914 (MR), demonstrating robustness across modalities and atypical scan coverage. The MLLM performs competitively in visually distinctive regions, while the segmentation-aware MLLM reveals fundamental limitations.

</details>


### [148] [FusionEdit: Semantic Fusion and Attention Modulation for Training-Free Image Editing](https://arxiv.org/abs/2602.08725)
*Yongwen Lai,Chaoqun Wang,Shaobo Min*

Main category: cs.CV

TL;DR: FusionEdit 提出了一种无需训练的图像编辑框架，通过自动识别编辑和保留区域、采用距离感知的潜在融合以及应用统计注意融合，实现了精确可控的文本引导图像编辑。


<details>
  <summary>Details</summary>
Motivation: 现有的图像编辑方法使用显式二值掩码限制编辑，但硬边界的掩码会导致边界处的伪影并降低编辑的可处理性。为了解决这些问题，FusionEdit 提出了一种无需训练的图像编辑框架。

Method: FusionEdit 通过测量源图像和目标提示之间的语义差异来自动识别编辑和保护断言区域。在边缘处进行距离感知的潜在融合以产生平滑过渡的软掩码，并使用总变异损失确保过渡自然。在编辑区域应用基于 AdaIN 的调节以进行统计注意融合，同时保持与源图像的一致性。

Result: FusionEdit 的广泛实验表明，该方法在生成自然编辑结果和提高编辑可处理性方面远超现有最佳方法。

Conclusion: FusionEdit 成功地实现了精确可控的文本引导图像编辑，并通过开源代码包供学术界和工业界使用。

Abstract: Text-guided image editing aims to modify specific regions according to the target prompt while preserving the identity of the source image. Recent methods exploit explicit binary masks to constrain editing, but hard mask boundaries introduce artifacts and reduce editability. To address these issues, we propose FusionEdit, a training-free image editing framework that achieves precise and controllable edits. First, editing and preserved regions are automatically identified by measuring semantic discrepancies between source and target prompts. To mitigate boundary artifacts, FusionEdit performs distance-aware latent fusion along region boundaries to yield the soft and accurate mask, and employs a total variation loss to enforce smooth transitions, obtaining natural editing results. Second, FusionEdit leverages AdaIN-based modulation within DiT attention layers to perform a statistical attention fusion in the editing region, enhancing editability while preserving global consistency with the source image. Extensive experiments demonstrate that our FusionEdit significantly outperforms state-of-the-art methods. Code is available at \href{https://github.com/Yvan1001/FusionEdit}{https://github.com/Yvan1001/FusionEdit}.

</details>


### [149] [Artifact Reduction in Undersampled 3D Cone-Beam CTs using a Hybrid 2D-3D CNN Framework](https://arxiv.org/abs/2602.08727)
*Johannes Thalhammer,Tina Dorosti,Sebastian Peterhansl,Daniela Pfeiffer,Franz Pfeiffer,Florian Schaff*

Main category: cs.CV

TL;DR: 提出了一种结合2D和3D模型的混合深度学习框架，用于处理undersampled CT图像中的伪影，通过两阶段方法实现了计算效率与体积一致性之间的平衡。


<details>
  <summary>Details</summary>
Motivation: 为了减少undersampled CT图像的采集时间与辐射暴露，同时提高图像质量和诊断价值。

Method: 一种结合2D U-Net和3D解码器的混合深度学习框架，首先使用2D U-Net处理每个切片，提取特征图，然后通过3D解码器堆叠这些特征图，利用切片间的上下文信息预测无伪影的3D CT图像。

Result: 实验结果表明，在冠状面和矢状面方向上实现了显著的切片间一致性提升，同时保持了较低的计算开销。

Conclusion: 引入的混合框架提供了一种稳健且高效的3D CT图像后处理解决方案。

Abstract: Undersampled CT volumes minimize acquisition time and radiation exposure but introduce artifacts degrading image quality and diagnostic utility. Reducing these artifacts is critical for high-quality imaging. We propose a computationally efficient hybrid deep-learning framework that combines the strengths of 2D and 3D models. First, a 2D U-Net operates on individual slices of undersampled CT volumes to extract feature maps. These slice-wise feature maps are then stacked across the volume and used as input to a 3D decoder, which utilizes contextual information across slices to predict an artifact-free 3D CT volume. The proposed two-stage approach balances the computational efficiency of 2D processing with the volumetric consistency provided by 3D modeling. The results show substantial improvements in inter-slice consistency in coronal and sagittal direction with low computational overhead. This hybrid framework presents a robust and efficient solution for high-quality 3D CT image post-processing. The code of this project can be found on github: https://github.com/J-3TO/2D-3DCNN_sparseview/.

</details>


### [150] [Closing the Confusion Loop: CLIP-Guided Alignment for Source-Free Domain Adaptation](https://arxiv.org/abs/2602.08730)
*Shanshan Wang,Ziying Feng,Xiaozheng Shen,Xun Yang,Pichao Wang,Zhenwei He,Xingyi Zhang*

Main category: cs.CV

TL;DR: 该研究提出了一种名为CLIP-Guided Alignment (CGA)的新框架，专门针对源数据不可用时的细分类别误分类问题。CGA包含三个部分：(1)检测方向性误分类对；(2)利用CLIP构建扰动伪标签文本提示以改进伪标签；(3)构建CLIP和预训练模型的扰动特征库并对其进行对比学习以减少表示空间中的模糊性。实验证明CGA在多种数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 论文致力于解决在源数据不可访问的情况下，模型在微分类场景中因细微的类间相似性而产生的未被充分研究的不对称和动态类混淆问题。

Method: 该方法名为CLIP-Guided Alignment (CGA)，分为三个步骤：(1)使用MCA检测目标域中模型预测的第一方向性混淆对；(2)使用CLIP创建混淆意识的文本提示以进行更敏感的伪标签；(3)构建并对比CLIP和预训练模型的混淆导向特征库，以在表示空间中减少模糊性。

Result: 实验结果表明CGA在各种数据集上均优于最先进的源数据无的适应方法，特别是在混淆易发和细分类场景下。

Conclusion: CGA强调了在无源数据情况下明确建模类间混淆对于有效适配的重要性。

Abstract: Source-Free Domain Adaptation (SFDA) tackles the problem of adapting a pre-trained source model to an unlabeled target domain without accessing any source data, which is quite suitable for the field of data security. Although recent advances have shown that pseudo-labeling strategies can be effective, they often fail in fine-grained scenarios due to subtle inter-class similarities. A critical but underexplored issue is the presence of asymmetric and dynamic class confusion, where visually similar classes are unequally and inconsistently misclassified by the source model. Existing methods typically ignore such confusion patterns, leading to noisy pseudo-labels and poor target discrimination. To address this, we propose CLIP-Guided Alignment(CGA), a novel framework that explicitly models and mitigates class confusion in SFDA. Generally, our method consists of three parts: (1) MCA: detects first directional confusion pairs by analyzing the predictions of the source model in the target domain; (2) MCC: leverages CLIP to construct confusion-aware textual prompts (e.g. a truck that looks like a bus), enabling more context-sensitive pseudo-labeling; and (3) FAM: builds confusion-guided feature banks for both CLIP and the source model and aligns them using contrastive learning to reduce ambiguity in the representation space. Extensive experiments on various datasets demonstrate that CGA consistently outperforms state-of-the-art SFDA methods, with especially notable gains in confusion-prone and fine-grained scenarios. Our results highlight the importance of explicitly modeling inter-class confusion for effective source-free adaptation. Our code can be find at https://github.com/soloiro/CGA

</details>


### [151] [From Correspondence to Actions: Human-Like Multi-Image Spatial Reasoning in Multi-modal Large Language Models](https://arxiv.org/abs/2602.08735)
*Masanari Oi,Koki Maeda,Ryuto Koike,Daisuke Oba,Nakamasa Inoue,Naoaki Okazaki*

Main category: cs.CV

TL;DR: 该研究提出了一种名为HATCH的训练框架，旨在提高多视角空间推理能力，通过两个目标实现：跨视图对齐和带有动作推理的解答。实验表明HATCH在三个基准测试中表现优于基线模型及大模型。


<details>
  <summary>Details</summary>
Motivation: 现有模型难以处理多图像的空间推理任务，新框架旨在通过明确监督跨视图对应关系和明确的视点变换动作来改进。

Method: HATCH框架具有两个互补的目标：一是基于像素级别的空间对齐，促使不同视图中对应区域的表示能够对齐；二是行动-答案推理，要求模型先生成视点转换操作再给出最终答案。

Result: 在三个基准测试中，HATCH框架的模型表现均优于基线和其他规模较大的模型，并且能够保留单图像推理的能力。

Conclusion: HATCH框架通过对跨视图对应关系和视点变换动作的明确监督显著提升了模型在多图像空间推理任务上的表现，具备推广应用的价值。

Abstract: While multimodal large language models (MLLMs) have made substantial progress in single-image spatial reasoning, multi-image spatial reasoning, which requires integration of information from multiple viewpoints, remains challenging. Cognitive studies suggest that humans address such tasks through two mechanisms: cross-view correspondence, which identifies regions across different views that correspond to the same physical locations, and stepwise viewpoint transformation, which composes relative viewpoint changes sequentially. However, existing studies incorporate these mechanisms only partially and often implicitly, without explicit supervision for both. We propose Human-Aware Training for Cross-view correspondence and viewpoint cHange (HATCH), a training framework with two complementary objectives: (1) Patch-Level Spatial Alignment, which encourages patch representations to align across views for spatially corresponding regions, and (2) Action-then-Answer Reasoning, which requires the model to generate explicit viewpoint transition actions before predicting the final answer. Experiments on three benchmarks demonstrate that HATCH consistently outperforms baselines of comparable size by a clear margin and achieves competitive results against much larger models, while preserving single-image reasoning capabilities.

</details>


### [152] [Shifting the Breaking Point of Flow Matching for Multi-Instance Editing](https://arxiv.org/abs/2602.08749)
*Carmine Zaccagnino,Fabio Quattrini,Enis Simsar,Marta Tintoré Gazulla,Rita Cucchiara,Alessio Tonioni,Silvia Cascianelli*

Main category: cs.CV

TL;DR: 本文提出了一种实例解耦注意力机制，以解决现有流编辑器在处理多实例场景时存在的编辑缠扰问题，实验表明该机制能够促进编辑的解耦和局部性，并保持全局输出的一致性。


<details>
  <summary>Details</summary>
Motivation: 现有基于流的编辑器在处理无需语义干扰的多实例编辑场景时表现出局限性。

Method: 本文提出了一种实例解耦注意力机制，该机制将联合注意操作进行分区，确保特定实例的文本指令与空间区域之间的关联在速度场估计时绑定。

Result: 在自然图像编辑和新的图形密集型信息图表基准测试中，本文的方法在单次编辑过程中实现了编辑分离和局部性，同时保持了全局输出的一致性。

Conclusion: 该研究表明，实例解耦注意力机制可以增强文本引导的图像编辑效果，特别适用于需要独立编辑多个部分的场景。

Abstract: Flow matching models have recently emerged as an efficient alternative to diffusion, especially for text-guided image generation and editing, offering faster inference through continuous-time dynamics. However, existing flow-based editors predominantly support global or single-instruction edits and struggle with multi-instance scenarios, where multiple parts of a reference input must be edited independently without semantic interference. We identify this limitation as a consequence of globally conditioned velocity fields and joint attention mechanisms, which entangle concurrent edits. To address this issue, we introduce Instance-Disentangled Attention, a mechanism that partitions joint attention operations, enforcing binding between instance-specific textual instructions and spatial regions during velocity field estimation. We evaluate our approach on both natural image editing and a newly introduced benchmark of text-dense infographics with region-level editing instructions. Experimental results demonstrate that our approach promotes edit disentanglement and locality while preserving global output coherence, enabling single-pass, instance-level editing.

</details>


### [153] [MVAnimate: Enhancing Character Animation with Multi-View Optimization](https://arxiv.org/abs/2602.08753)
*Tianyu Sun,Zhoujie Fu,Bang Zhang,Guosheng Lin*

Main category: cs.CV

TL;DR: MVAnimate 是一种新的框架，通过结合多视图先验信息来合成动态人体的2D和3D信息，提高动画视频的质量，实验结果显示其在多种运动模式和外观处理上表现出良好的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 目前，用于人物动画生成的算法在质量和训练数据方面存在缺陷，MVAnimate旨在通过多视图先验信息提高视频动画的质量。

Method: MVAnimate采用了多视图先验信息来生成时间一致且空间一致的动画输出，优化目标角色的多视角视频，提升不同视角下的视频质量。

Result: 实验结果表明，MVAnimate在不同数据集上能够处理多种运动模式和外观，展示了其在提高动画视频质量方面的优势。

Conclusion: 总之，MVAnimate 通过融入多视图先验信息显著提升了动画视频的质量，为动画生成领域提供了新的解决方案。

Abstract: The demand for realistic and versatile character animation has surged, driven by its wide-ranging applications in various domains. However, the animation generation algorithms modeling human pose with 2D or 3D structures all face various problems, including low-quality output content and training data deficiency, preventing the related algorithms from generating high-quality animation videos. Therefore, we introduce MVAnimate, a novel framework that synthesizes both 2D and 3D information of dynamic figures based on multi-view prior information, to enhance the generated video quality. Our approach leverages multi-view prior information to produce temporally consistent and spatially coherent animation outputs, demonstrating improvements over existing animation methods. Our MVAnimate also optimizes the multi-view videos of the target character, enhancing the video quality from different views. Experimental results on diverse datasets highlight the robustness of our method in handling various motion patterns and appearances.

</details>


### [154] [VedicTHG: Symbolic Vedic Computation for Low-Resource Talking-Head Generation in Educational Avatars](https://arxiv.org/abs/2602.08775)
*Vineet Kumar Rakesh,Ahana Bhattacharjee,Soumya Mazumdar,Tapas Samanta,Hemendra Kumar Pandey,Amitabha Das,Sarbajit Pal*

Main category: cs.CV

TL;DR: 本研究提出了一种名为Symbolic Vedic Computation的确定性和面向CPU的生成口型的方法，该方法将语音转换为对齐的音节流，通过音节映射到紧凑的发音库存并生成平滑的发音轨迹，使用轻量级2D渲染器实现ROI扭曲和嘴巴合成，支持在普通CPU上进行实时合成。实验报告了CPU-only执行下的唇同步准确性、时间稳定性、身份一致性和与代表性CPU可行基线的基准测试。结果显示在显著降低计算负载和延迟的同时，仍能实现可接受的唇同步质量，适用于低配置硬件的教学Avatar。


<details>
  <summary>Details</summary>
Motivation: 现有的唇形生成方法依赖GPU、大型训练集或高容量的扩散模型，这些限制了它们在离线或资源受限的学习环境中部署。因此，本研究旨在提出一种更高效、面向CPU的方法，提高教育技术中Avatar的部署性和实用性。

Method: 该方法通过将语音转换为音节流，并利用音节将它们映射到一个紧凑的发音库存中，然后通过类似梵文咒语的符号共声技术生成平滑的发音轨迹。使用轻量级的2D渲染器进行ROI扭曲和嘴巴合成，以及稳定性支持，实现实时合成。

Result: 实验表明，该方法实现了合理的唇同步质量，且计算负载和延迟显著降低，可以支持低端硬件中的教育Avatar。

Conclusion: 该研究提出了一种新颖的确定性、面向CPU的唇形生成框架，相比现有的方法更能满足资源受限环境下的应用需求。

Abstract: Talking-head avatars are increasingly adopted in educational technology to deliver content with social presence and improved engagement. However, many recent talking-head generation (THG) methods rely on GPU-centric neural rendering, large training sets, or high-capacity diffusion models, which limits deployment in offline or resource-constrained learning environments. A deterministic and CPU-oriented THG framework is described, termed Symbolic Vedic Computation, that converts speech to a time-aligned phoneme stream, maps phonemes to a compact viseme inventory, and produces smooth viseme trajectories through symbolic coarticulation inspired by Vedic sutra Urdhva Tiryakbhyam. A lightweight 2D renderer performs region-of-interest (ROI) warping and mouth compositing with stabilization to support real-time synthesis on commodity CPUs. Experiments report synchronization accuracy, temporal stability, and identity consistency under CPU-only execution, alongside benchmarking against representative CPU-feasible baselines. Results indicate that acceptable lip-sync quality can be achieved while substantially reducing computational load and latency, supporting practical educational avatars on low-end hardware. GitHub: https://vineetkumarrakesh.github.io/vedicthg

</details>


### [155] [Multimodal Learning for Arcing Detection in Pantograph-Catenary Systems](https://arxiv.org/abs/2602.08792)
*Hao Dong,Eleni Chatzi,Olga Fink*

Main category: cs.CV

TL;DR: 本文提出了一种结合高分辨率图像数据和力测量的多模态框架，用于更准确地检测弓网接触面上的电弧事件，显著优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 弓网接口处的电弧事件对电力输送系统造成严重风险，传统的检测方法难以应对其瞬时性、噪声环境、数据稀少及与其他瞬态现象区分困难的问题。

Method: 本文构建了包含同步视觉和力测量的两个数据集，提出了一种基于DeepSAD优化的MultiDeepSAD算法，并设计了针对每种数据类型的伪异常生成技术，进行模型训练。

Result: 实验表明，提出的多模态框架在不同场景下对真实电弧事件显示出了更高的灵敏度，即使在数据稀缺和领域变化的情况下也表现优越。

Conclusion: 本文的研究成果为弓网接口的电弧检测提供了一种有效的方法，有助于提高电力输送系统的可靠性和稳定性。

Abstract: The pantograph-catenary interface is essential for ensuring uninterrupted and reliable power delivery in electrified rail systems. However, electrical arcing at this interface poses serious risks, including accelerated wear of contact components, degraded system performance, and potential service disruptions. Detecting arcing events at the pantograph-catenary interface is challenging due to their transient nature, noisy operating environment, data scarcity, and the difficulty of distinguishing arcs from other similar transient phenomena. To address these challenges, we propose a novel multimodal framework that combines high-resolution image data with force measurements to more accurately and robustly detect arcing events. First, we construct two arcing detection datasets comprising synchronized visual and force measurements. One dataset is built from data provided by the Swiss Federal Railways (SBB), and the other is derived from publicly available videos of arcing events in different railway systems and synthetic force data that mimic the characteristics observed in the real dataset. Leveraging these datasets, we propose MultiDeepSAD, an extension of the DeepSAD algorithm for multiple modalities with a new loss formulation. Additionally, we introduce tailored pseudo-anomaly generation techniques specific to each data type, such as synthetic arc-like artifacts in images and simulated force irregularities, to augment training data and improve the discriminative ability of the model. Through extensive experiments and ablation studies, we demonstrate that our framework significantly outperforms baseline approaches, exhibiting enhanced sensitivity to real arcing events even under domain shifts and limited availability of real arcing observations.

</details>


### [156] [MOVA: Towards Scalable and Synchronized Video-Audio Generation](https://arxiv.org/abs/2602.08794)
*SII-OpenMOSS Team,:,Donghua Yu,Mingshu Chen,Qi Chen,Qi Luo,Qianyi Wu,Qinyuan Cheng,Ruixiao Li,Tianyi Liang,Wenbo Zhang,Wenming Tu,Xiangyu Peng,Yang Gao,Yanru Huo,Ying Zhu,Yinze Luo,Yiyang Zhang,Yuerong Song,Zhe Xu,Zhiyu Zhang,Chenchen Yang,Cheng Chang,Chushu Zhou,Hanfu Chen,Hongnan Ma,Jiaxi Li,Jingqi Tong,Junxi Liu,Ke Chen,Shimin Li,Songlin Wang,Wei Jiang,Zhaoye Fei,Zhiyuan Ning,Chunguo Li,Chenhui Li,Ziwei He,Zengfeng Huang,Xie Chen,Xipeng Qiu*

Main category: cs.CV

TL;DR: MOVA是一个开源模型，能够生成高质量、同步的音频视频内容，具有Mixture-of-Experts架构，支持IT2VA任务，旨在促进研究和创作社区的发展。


<details>
  <summary>Details</summary>
Motivation: 当前的音频视频内容生成方法通常依赖于级联管道，这增加了成本，累积错误，并降低了整体质量。MOVA旨在通过协同生成音频和视频内容来解决这一问题。

Method: MOVA使用Mixture-of-Experts架构，支持Image-Text到Video-Audio的生成任务。该模型在推理时有32B参数，其中18B参数在推理过程中活跃。

Result: MOVA能够生成高质量且同步的音频视频内容，包括逼真的唇同步语音、环境意识的声音效果和内容导向的音乐。

Conclusion: 通过开放模型权重和代码，MOVA促进了该领域的研究，并为创作者社区的发展提供了基础。

Abstract: Audio is indispensable for real-world video, yet generation models have largely overlooked audio components. Current approaches to producing audio-visual content often rely on cascaded pipelines, which increase cost, accumulate errors, and degrade overall quality. While systems such as Veo 3 and Sora 2 emphasize the value of simultaneous generation, joint multimodal modeling introduces unique challenges in architecture, data, and training. Moreover, the closed-source nature of existing systems limits progress in the field. In this work, we introduce MOVA (MOSS Video and Audio), an open-source model capable of generating high-quality, synchronized audio-visual content, including realistic lip-synced speech, environment-aware sound effects, and content-aligned music. MOVA employs a Mixture-of-Experts (MoE) architecture, with a total of 32B parameters, of which 18B are active during inference. It supports IT2VA (Image-Text to Video-Audio) generation task. By releasing the model weights and code, we aim to advance research and foster a vibrant community of creators. The released codebase features comprehensive support for efficient inference, LoRA fine-tuning, and prompt enhancement.

</details>


### [157] [Addressing data annotation scarcity in Brain Tumor Segmentation on 3D MRI scan Using a Semi-Supervised Teacher-Student Framework](https://arxiv.org/abs/2602.08797)
*Jiaming Liu,Cheng Ding,Daoqiang Zhang*

Main category: cs.CV

TL;DR: 提出了一种半监督教师-学生框架，结合了不确定性感知的伪标签教师和渐进性的、信心为基础的教学课程，显著提高了MRI脑肿瘤分割的准确性，尤其是对于早期数据阶段，验证DSC从39.3%提高到87.2%。


<details>
  <summary>Details</summary>
Motivation: 现有的脑肿瘤分割方法受限于昂贵的标注成本和数据的异质性。因此，研究提出了一种新的方法以提高分割精度的同时减少标注需求。

Method: 该方法采用了半监督的教师-学生框架，首先通过不确定性感知的伪标签教师生成概率掩模和每个像素的不确定性，然后根据图像级别的信心排序未标记的扫描，并逐渐引入，用带有双重损失的目标训练学生。此外，还采用了共识为基础的细化方法来进一步改进伪标签质量。

Result: 在BraTS 2021数据集上，学生模型在10%数据下的验证DSC从39.3%提高到了100%的87.2%，尤其是在早期数据阶段。教师模型达到了92.2%的验证DSC，学生模型在肿瘤亚区域（例如NCR/NET和水肿）的分割精度超越了教师模型，并成功恢复了教师模型无法分割的增强类，达到62.0%的DSC。

Conclusion: 研究结果表明，信心驱动的教学课程和选择性的去学习策略可以有效地提供在有限监督和嘈杂的伪标签条件下的鲁棒分割。

Abstract: Accurate brain tumor segmentation from MRI is limited by expensive annotations and data heterogeneity across scanners and sites. We propose a semi-supervised teacher-student framework that combines an uncertainty-aware pseudo-labeling teacher with a progressive, confidence-based curriculum for the student. The teacher produces probabilistic masks and per-pixel uncertainty; unlabeled scans are ranked by image-level confidence and introduced in stages, while a dual-loss objective trains the student to learn from high-confidence regions and unlearn low-confidence ones. Agreement-based refinement further improves pseudo-label quality. On BraTS 2021, validation DSC increased from 0.393 (10% data) to 0.872 (100%), with the largest gains in early stages, demonstrating data efficiency. The teacher reached a validation DSC of 0.922, and the student surpassed the teacher on tumor subregions (e.g., NCR/NET 0.797 and Edema 0.980); notably, the student recovered the Enhancing class (DSC 0.620) where the teacher failed. These results show that confidence-driven curricula and selective unlearning provide robust segmentation under limited supervision and noisy pseudo-labels.

</details>


### [158] [Omni-Video 2: Scaling MLLM-Conditioned Diffusion for Unified Video Generation and Editing](https://arxiv.org/abs/2602.08820)
*Hao Yang,Zhiyu Tan,Jia Gong,Luozheng Qin,Hesen Chen,Xiaomeng Yang,Yuqing Sun,Yuetan Lin,Mengping Yang,Hao Li*

Main category: cs.CV

TL;DR: Omni-Video 2 利用预训练的多模态大型语言模型（MLLMs）和视频扩散模型，实现视频生成和编辑的统一处理，特别优化了复杂的视频编辑任务。


<details>
  <summary>Details</summary>
Motivation: 通过利用 MLLMs 的理解和推理能力，生成明确的目标标注以解释用户指令，结合高效轻量级适配器，优化了视频生成和编辑的质量。

Method: Omni-Video 2 方法包括两部分：1）使用 MLLMs 生成目标标注，指导视频生成和编辑过程；2）开发轻量级适配器，将多模态条件标记注入预训练的文本转视频扩散模型，以高效利用其强大的生成先验。

Result: Omni-Video 2 在精心策划的训练数据集上扩展到 14B 视频扩散模型，支持高质量的文本转视频生成以及多种编辑任务。在 FiVE 和 VBench 评估基准上的测试结果表明其在遵循复杂组合编辑指令方面表现出色，同时在视频生成任务中也能达到竞争或领先的质量。

Conclusion: Omni-Video 2 重新定义了视频生成和编辑的任务，提供了一种高效率的方法来处理复杂的视频编辑，同时在生成高质量视频方面也取得了显著进展。

Abstract: We present Omni-Video 2, a scalable and computationally efficient model that connects pretrained multimodal large-language models (MLLMs) with video diffusion models for unified video generation and editing. Our key idea is to exploit the understanding and reasoning capabilities of MLLMs to produce explicit target captions to interpret user instructions. In this way, the rich contextual representations from the understanding model are directly used to guide the generative process, thereby improving performance on complex and compositional editing. Moreover, a lightweight adapter is developed to inject multimodal conditional tokens into pretrained text-to-video diffusion models, allowing maximum reuse of their powerful generative priors in a parameter-efficient manner. Benefiting from these designs, we scale up Omni-Video 2 to a 14B video diffusion model on meticulously curated training data with quality, supporting high quality text-to-video generation and various video editing tasks such as object removal, addition, background change, complex motion editing, \emph{etc.} We evaluate the performance of Omni-Video 2 on the FiVE benchmark for fine-grained video editing and the VBench benchmark for text-to-video generation. The results demonstrate its superior ability to follow complex compositional instructions in video editing, while also achieving competitive or superior quality in video generation tasks.

</details>


### [159] [Any-to-All MRI Synthesis: A Unified Foundation Model for Nasopharyngeal Carcinoma and Its Downstream Applications](https://arxiv.org/abs/2602.08822)
*Yao Pu,Yiming Shi,Zhenxi Zhang,Peixin Yu,Yitao Zhuang,Xiang Wang,Hongzhao Chen,Jing Cai,Ge Ren*

Main category: cs.CV

TL;DR: 该研究提出了一种新的MRI增强统一基础模型，通过对比视觉表示学习和视觉-语言对齐技术，实现了从一种模态到所有模态的MRI图像综合。该模型在多个验证站点表现优异，提高了放射治疗相关任务的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的MRI合成方法存在模态特定性、解剖适应性差和临床解释性不足等问题，无法满足鼻咽癌放疗的实际需求。

Method: 该方法采用对比编码器生成模态不变表示，以及基于CLIP的文本引导解码器生成语义一致的合成图像，从而实现从任何模态到所有模态的MRI综合。

Result: 该模型在来自13个机构的40,825张图像上进行了训练，并在26个验证站点（共计15,748张图像）中表现出了高准确率（平均结构相似性SSIM为0.90，峰值信噪比PSNR为27），合成精度高，对噪声和域转移具有鲁棒性。

Conclusion: 该工作通过利用基础模型填补了技术合成与临床应用之间的鸿沟，为鼻咽癌的数字化医疗服务带来了新的解决方案。

Abstract: Magnetic resonance imaging (MRI) is essential for nasopharyngeal carcinoma (NPC) radiotherapy (RT), but practical constraints, such as patient discomfort, long scan times, and high costs often lead to incomplete modalities in clinical practice, compromising RT planning accuracy. Traditional MRI synthesis methods are modality-specific, limited in anatomical adaptability, and lack clinical interpretability-failing to meet NPC's RT needs. Here, we developed a unified foundation model integrating contrastive visual representation learning and vision-language alignment (VLA) to enable any-to-all MRI synthesis. The model uses a contrastive encoder for modality-invariant representations and a CLIP-based text-informed decoder for semantically consistent synthesis, supporting any-to-all MRI synthesis via one unified foundation model. Trained on 40,825 images from 13 institutions, it achieves consistently high performance (average SSIM 0.90, PSNR 27) across 26 internal/external validation sites (15,748 images), with superior synthesis fidelity and robustness to noise and domain shifts. Meanwhile, its unified representation enhances downstream RT-relevant tasks (e.g., segmentation). This work advances digital medicine solutions for NPC care by leveraging foundation models to bridge technical synthesis and clinical utility.

</details>


### [160] [VideoVeritas: AI-Generated Video Detection via Perception Pretext Reinforcement Learning](https://arxiv.org/abs/2602.08828)
*Hao Tan,Jun Lan,Senyuan Shi,Zichang Tan,Zijian Yu,Huijia Zhu,Weiqiang Wang,Jun Wan,Zhen Lei*

Main category: cs.CV

TL;DR: 论文介绍了VideoVeritas框架，该框架结合了精细感知和事实推理，旨在提高视频生成中的安全检测能力。通过Joint Preference Alignment和Perception Pretext Reinforcement Learning方法，使得检测性能更均衡。


<details>
  <summary>Details</summary>
Motivation: 随着视频生成能力的增强，检测虚假视频变得越来越重要。现有的多模态大语言模型在推理方面表现出色，但在精细感知方面仍有限制。因此，需要一个框架来克服这些限制，提升检测性能。

Method: 论文提出了VideoVeritas框架，结合细粒度感知和事实推理。具体方法是通过Joint Preference Alignment和Perception Pretext Reinforcement Learning，在强化学习阶段进行一般时空定位和自我监督对象计数等前提任务，增强检测性能。

Result: 实验结果显示，现有方法倾向于偏向浅层推理或机械分析，而VideoVeritas实现了在不同基准上性能的平衡。

Conclusion: 该研究通过提出VideoVeritas框架，解决了视频生成中检测的平衡探测问题，并通过引入轻量级但高质量的数据集MintVid，促进了更规范的评估。

Abstract: The growing capability of video generation poses escalating security risks, making reliable detection increasingly essential. In this paper, we introduce VideoVeritas, a framework that integrates fine-grained perception and fact-based reasoning. We observe that while current multi-modal large language models (MLLMs) exhibit strong reasoning capacity, their granular perception ability remains limited. To mitigate this, we introduce Joint Preference Alignment and Perception Pretext Reinforcement Learning (PPRL). Specifically, rather than directly optimizing for detection task, we adopt general spatiotemporal grounding and self-supervised object counting in the RL stage, enhancing detection performance with simple perception pretext tasks. To facilitate robust evaluation, we further introduce MintVid, a light yet high-quality dataset containing 3K videos from 9 state-of-the-art generators, along with a real-world collected subset that has factual errors in content. Experimental results demonstrate that existing methods tend to bias towards either superficial reasoning or mechanical analysis, while VideoVeritas achieves more balanced performance across diverse benchmarks.

</details>


### [161] [TiFRe: Text-guided Video Frame Reduction for Efficient Video Multi-modal Large Language Models](https://arxiv.org/abs/2602.08861)
*Xiangtian Zheng,Zishuo Wang,Yuxin Peng*

Main category: cs.CV

TL;DR: 本文提出了一种名为TiFRe的新框架，旨在减少输入帧以减轻视频多模态大型语言模型（Video MLLMs）的计算成本，同时保持视频信息的关键性。TiFRe通过文本引导的帧采样（TFS）策略根据用户输入从L语言模型生成提示，利用预训练的CLIP编码器计算提示与各帧的语义相似度，选择最相关的帧作为关键帧。此外，还使用帧匹配和融合机制（FMM），将非关键帧的信息融入到关键帧中，以保护视频语义。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）的快速发展，视频多模态大型语言模型（Video MLLMs）在视频理解等方面取得了显著进展。然而，处理大量视频帧作为输入导致了显著的注意力计算开销。现有的简化方法（如在固定帧率下选择关键帧）可能会忽视非关键帧中的有价值信息，从而导致性能下降。

Method: 本文提出了一种新的框架，采用文本引导的帧采样（TFS）策略根据用户输入从L语言模型生成提示。利用预训练的CLIP编码器计算提示与各帧的语义相似度，选择最相关的帧作为关键帧。为了保留视频语义，框架还采用了帧匹配和融合机制（FMM），将非关键帧的信息融入关键帧中，从而减少信息丢失。

Result: 实验结果表明，这项工作有效地减少了计算成本，并提高了视频与语言任务的表现。

Conclusion: 本文通过引入TiFRe框架，成功地降低了Video MLLMs在处理视频语言任务时的计算成本，同时保持了关键信息的完整性。

Abstract: With the rapid development of Large Language Models (LLMs), Video Multi-Modal Large Language Models (Video MLLMs) have achieved remarkable performance in video-language tasks such as video understanding and question answering. However, Video MLLMs face high computational costs, particularly in processing numerous video frames as input, which leads to significant attention computation overhead. A straightforward approach to reduce computational costs is to decrease the number of input video frames. However, simply selecting key frames at a fixed frame rate (FPS) often overlooks valuable information in non-key frames, resulting in notable performance degradation. To address this, we propose Text-guided Video Frame Reduction (TiFRe), a framework that reduces input frames while preserving essential video information. TiFRe uses a Text-guided Frame Sampling (TFS) strategy to select key frames based on user input, which is processed by an LLM to generate a CLIP-style prompt. Pre-trained CLIP encoders calculate the semantic similarity between the prompt and each frame, selecting the most relevant frames as key frames. To preserve video semantics, TiFRe employs a Frame Matching and Merging (FMM) mechanism, which integrates non-key frame information into the selected key frames, minimizing information loss. Experiments show that TiFRe effectively reduces computational costs while improving performance on video-language tasks.

</details>


### [162] [Analysis of Converged 3D Gaussian Splatting Solutions: Density Effects and Prediction Limit](https://arxiv.org/abs/2602.08909)
*Zhendong Wang,Cihan Ruan,Jingchuan Xiao,Chuqing Shi,Wei Jiang,Wei Wang,Wenjie Liu,Nam Ling*

Main category: cs.CV

TL;DR: 本文研究了标准多视图优化在3D Gaussian Splatting (3DGS) 解决方案中产生的结构特性。研究利用 Render-Optimal References (RORs) 来分析这些结构，发现了稳定的模式，并通过训练预测器来探究这些参数，最终提出了适应性平衡前向预测和基于渲染微调的策略。


<details>
  <summary>Details</summary>
Motivation: 探讨3D Gaussian Splatting (3DGS) 在标准多视图优化下的结构特性，旨在理解现实场景中哪些参数使得RORs成为可能，并通过训练预测器来探究这些特性。

Method: 通过训练预测器来重建 Render-Optimal References (RORs) 从点云数据，而无需使用渲染指导。运用方差分解来理解场景稀疏区域中几何与外观参数之间的耦合关系。

Result: 发现了稳定的混合尺度和双模辐射度模式，并且明确了密集区域与稀疏区域在参数上的差异性。在稀疏区域，由于视图异质性，几何和外观参数之间存在高方差耦合。

Conclusion: 研究表明RORs具有双重性质：在密集区域作为几何基本单元转变为视图综合的基本单元。该研究提出了密度感知的策略，以提高模型训练的鲁棒性，并讨论了基于此策略的体系结构的潜在改进方向。

Abstract: We investigate what structure emerges in 3D Gaussian Splatting (3DGS) solutions from standard multi-view optimization. We term these Rendering-Optimal References (RORs) and analyze their statistical properties, revealing stable patterns: mixture-structured scales and bimodal radiance across diverse scenes. To understand what determines these parameters, we apply learnability probes by training predictors to reconstruct RORs from point clouds without rendering supervision. Our analysis uncovers fundamental density-stratification. Dense regions exhibit geometry-correlated parameters amenable to render-free prediction, while sparse regions show systematic failure across architectures. We formalize this through variance decomposition, demonstrating that visibility heterogeneity creates covariance-dominated coupling between geometric and appearance parameters in sparse regions. This reveals the dual character of RORs: geometric primitives where point clouds suffice, and view synthesis primitives where multi-view constraints are essential. We provide density-aware strategies that improve training robustness and discuss architectural implications for systems that adaptively balance feed-forward prediction and rendering-based refinement.

</details>


### [163] [Grow with the Flow: 4D Reconstruction of Growing Plants with Gaussian Flow Fields](https://arxiv.org/abs/2602.08958)
*Weihan Luo,Lily Goli,Sherwin Bahmani,Felix Taubner,Andrea Tagliasacchi,David B. Lindell*

Main category: cs.CV

TL;DR: 该研究引入了一种3D高斯流场表示法来模拟植物生长的时变导数，通过反向生长过程初始化足够的高斯基元，从而提供了一种在多视角时序数据集中产生高质量图像和几何精度的新方法。


<details>
  <summary>Details</summary>
Motivation: 当前的动态场景建模技术无法有效处理植物生长这一特定场景，因为植物在生长过程中不断产生新的几何结构，而传统的变形场不能引入新的几何，4D高斯散点图也限制了空间和时间上的线性运动轨迹。

Method: 研究提出了3D高斯流场表示法，通过时间演变的高斯参数（位置、尺度、方向、颜色、透明度）来建模植物生长的动力学。此外，利用反向生长过程初始化高斯基元。

Result: 在多视角时序数据集中的成像质量和几何精度方面，该方法优于先前的方法。

Conclusion: 此研究提供了一种针对生长中3D结构外观建模的新方法，提升了图像质量和几何精度。

Abstract: Modeling the time-varying 3D appearance of plants during their growth poses unique challenges: unlike many dynamic scenes, plants generate new geometry over time as they expand, branch, and differentiate. Recent motion modeling techniques are ill-suited to this problem setting. For example, deformation fields cannot introduce new geometry, and 4D Gaussian splatting constrains motion to a linear trajectory in space and time and cannot track the same set of Gaussians over time. Here, we introduce a 3D Gaussian flow field representation that models plant growth as a time-varying derivative over Gaussian parameters -- position, scale, orientation, color, and opacity -- enabling nonlinear and continuous-time growth dynamics. To initialize a sufficient set of Gaussian primitives, we reconstruct the mature plant and learn a process of reverse growth, effectively simulating the plant's developmental history in reverse. Our approach achieves superior image quality and geometric accuracy compared to prior methods on multi-view timelapse datasets of plant growth, providing a new approach for appearance modeling of growing 3D structures.

</details>


### [164] [MotionCrafter: Dense Geometry and Motion Reconstruction with a 4D VAE](https://arxiv.org/abs/2602.08961)
*Ruijie Zhu,Jiahao Lu,Wenbo Hu,Xiaoguang Han,Jianfei Cai,Ying Shan,Chuanxia Zheng*

Main category: cs.CV

TL;DR: 该研究提出了一个基于视频扩散的框架MotionCrafter，能够联合重建4D几何形态并估计密集运动，通过新型的3D点图和3D场景流的联合表示以及4D VAE有效学习，无后优化提升了几何和运动重建质量。


<details>
  <summary>Details</summary>
Motivation: 当前大部分3D重建工作需要标注了点云、占位、法向，对数据和计算成本要求较高，且对密集运动估计准确性有限。此研究旨在降低建模3D几何和运动的需求，通过一项视差场景流的表示，提出了一个无需标注点云、法向等数据的3D几何与运动联合建模框架。

Method: 该研究提出了一个新颖的3D点图和3D场景流的联合表示方法，在共享坐标系统下进行3D建模，基于4D变分自编码器(4D VAE)有效地学习此表示。相较于以往工作严格要求3D值和潜在变量与RGB VAE潜在变量对齐的做法，该研究展示了这样的对齐既不是必需的，反而导致了性能的下降。此外，研究还引入了一种新的数据归一化和VAE训练策略，使其更能转移扩散先验，提高了重建质量。

Result: 多数据集的大量实验表明，MotionCrafter在几何重建和密集场景流估计方面均达到了最先进的性能，相较于现有方法，其在几何和运动重建方面分别提高了38.64%和25.0%。

Conclusion: MotionCrafter提供了一种全新的3D几何和运动联合建模框架，它取消了以往在3D模型中需要添加大量信息对的要求，提高了模型重建和运动估计的准确性。

Abstract: We introduce MotionCrafter, a video diffusion-based framework that jointly reconstructs 4D geometry and estimates dense motion from a monocular video. The core of our method is a novel joint representation of dense 3D point maps and 3D scene flows in a shared coordinate system, and a novel 4D VAE to effectively learn this representation. Unlike prior work that forces the 3D value and latents to align strictly with RGB VAE latents-despite their fundamentally different distributions-we show that such alignment is unnecessary and leads to suboptimal performance. Instead, we introduce a new data normalization and VAE training strategy that better transfers diffusion priors and greatly improves reconstruction quality. Extensive experiments across multiple datasets demonstrate that MotionCrafter achieves state-of-the-art performance in both geometry reconstruction and dense scene flow estimation, delivering 38.64% and 25.0% improvements in geometry and motion reconstruction, respectively, all without any post-optimization. Project page: https://ruijiezhu94.github.io/MotionCrafter_Page

</details>


### [165] [Modeling 3D Pedestrian-Vehicle Interactions for Vehicle-Conditioned Pose Forecasting](https://arxiv.org/abs/2602.08962)
*Guangxun Zhu,Xuan Liu,Nicolas Pugeault,Chongfeng Wei,Edmond S. L. Ho*

Main category: cs.CV

TL;DR: 该研究提出了一种3D车辆条件下的行人姿态预测框架，通过整合周围车辆信息，改进了Waymo-3DSkelMo数据集，增强了行人和车辆间的交互预测精度。


<details>
  <summary>Details</summary>
Motivation: 准确预测行人运动对于实现城市复杂环境下的自动驾驶至关重要，尤其是在交通场景中考虑车辆-行人交互作用时。现有的行人姿态预测方法通常忽略车辆信息，而该研究试图通过引入车辆感知的3D姿态预测来改进这一领域。

Method: 研究通过增强Waymo-3DSkelMo数据集，包含了对齐的3D车辆边界框，引入了一种分类场景的采样方案以及定制的学习架构，包括一个专门的车辆编码器和行人-车辆交互交叉注意力模块，以融合行人和车辆特征，使预测不仅基于历史行人类别而且基于周围车辆。

Result: 实验结果显示出预测准确性有了显著提升，验证了不同车辆-行人交互建模方法的有效性，特别强调了车辆感知的3D姿态预测在自动驾驶中的重要性。

Conclusion: 研究证明了车辆条件下的3D行人姿态预测对于自动驾驶系统的性能提升作用显著，该方法可用于改进现有的行人运动预测模型，提高自动驾驶的安全性和可靠性。

Abstract: Accurately predicting pedestrian motion is crucial for safe and reliable autonomous driving in complex urban environments. In this work, we present a 3D vehicle-conditioned pedestrian pose forecasting framework that explicitly incorporates surrounding vehicle information. To support this, we enhance the Waymo-3DSkelMo dataset with aligned 3D vehicle bounding boxes, enabling realistic modeling of multi-agent pedestrian-vehicle interactions. We introduce a sampling scheme to categorize scenes by pedestrian and vehicle count, facilitating training across varying interaction complexities. Our proposed network adapts the TBIFormer architecture with a dedicated vehicle encoder and pedestrian-vehicle interaction cross-attention module to fuse pedestrian and vehicle features, allowing predictions to be conditioned on both historical pedestrian motion and surrounding vehicles. Extensive experiments demonstrate substantial improvements in forecasting accuracy and validate different approaches for modeling pedestrian-vehicle interactions, highlighting the importance of vehicle-aware 3D pose prediction for autonomous driving. Code is available at: https://github.com/GuangxunZhu/VehCondPose3D

</details>


### [166] [WorldArena: A Unified Benchmark for Evaluating Perception and Functional Utility of Embodied World Models](https://arxiv.org/abs/2602.08971)
*Yu Shang,Zhuohang Li,Yiding Ma,Weikang Su,Xin Jin,Ziyou Wang,Xin Zhang,Yinzhou Tang,Chen Gao,Wei Wu,Xihui Liu,Dhruv Shah,Zhaoxiang Zhang,Zhibo Chen,Jun Zhu,Yonghong Tian,Tat-Seng Chua,Wenwu Zhu,Yong Li*

Main category: cs.CV

TL;DR: WorldArena 提供了一个统一的基准来全面评估体感世界模型，并强调了感知质量和功能性的差距。


<details>
  <summary>Details</summary>
Motivation: 当前评估体感世界模型较少关注其功能效用，而更侧重于感知准确性。该研究旨在解决这一问题。

Method: 开发了WorldArena统一基准，评估模型通过视频感知质量和体感任务功能两大维度，还提出了EWMScore多维度性能综合评估框架。

Result: 研究表明，高视觉质量不一定意味着较强的体感任务能力，且已通过14个代表性模型的实验数据予以验证。

Conclusion: WorldArena publi领导者行为准则c leaderboard提供了体感AI领域追踪进功能世界模型进展的框架。

Abstract: While world models have emerged as a cornerstone of embodied intelligence by enabling agents to reason about environmental dynamics through action-conditioned prediction, their evaluation remains fragmented. Current evaluation of embodied world models has largely focused on perceptual fidelity (e.g., video generation quality), overlooking the functional utility of these models in downstream decision-making tasks. In this work, we introduce WorldArena, a unified benchmark designed to systematically evaluate embodied world models across both perceptual and functional dimensions. WorldArena assesses models through three dimensions: video perception quality, measured with 16 metrics across six sub-dimensions; embodied task functionality, which evaluates world models as data engines, policy evaluators, and action planners integrating with subjective human evaluation. Furthermore, we propose EWMScore, a holistic metric integrating multi-dimensional performance into a single interpretable index. Through extensive experiments on 14 representative models, we reveal a significant perception-functionality gap, showing that high visual quality does not necessarily translate into strong embodied task capability. WorldArena benchmark with the public leaderboard is released at https://worldarena.ai, providing a framework for tracking progress toward truly functional world models in embodied AI.

</details>


### [167] [ArcFlow: Unleashing 2-Step Text-to-Image Generation via High-Precision Non-Linear Flow Distillation](https://arxiv.org/abs/2602.09014)
*Zihan Yang,Shuyuan Tu,Licheng Zhang,Qi Dai,Yu-Gang Jiang,Zuxuan Wu*

Main category: cs.CV

TL;DR: ArcFlow 是一种通过非线性流动轨迹近似预训练教师轨迹的新型 distillation 框架，它通过混合连续动量过程参数化推理轨迹下的速度场，实现了高精度的近似并保持了生成的多样性和质量。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散模型在生成质量方面取得了显著进展，但它们依赖于多步去噪过程，导致推断成本高昂。现有 distillation 方法通过线性捷径近似教师轨迹，难以匹配其随时间变化的不断变化的切向方向，从而导致质量下降。因此，提出 ArcFlow 以解决这一限制。

Method: ArcFlow 通过混合连续动量过程参数化推理轨迹下的速度场，捕捉速度演变并外推一致的速度以形成连续非线性轨迹，同时提供无需数值离散化误差的解析积分近似。

Result: ArcFlow 在小型化模型上仅微调少于 5% 的原始参数，实现了 40 倍速度提升，而 NFEs 为 2，比原先的多步教师并没有显著的质量下降。

Conclusion: ArcFlow 对大模型的 distillation 表现有效，它在保持生成多样性和质量的同时提高了推理速度。

Abstract: Diffusion models have achieved remarkable generation quality, but they suffer from significant inference cost due to their reliance on multiple sequential denoising steps, motivating recent efforts to distill this inference process into a few-step regime. However, existing distillation methods typically approximate the teacher trajectory by using linear shortcuts, which makes it difficult to match its constantly changing tangent directions as velocities evolve across timesteps, thereby leading to quality degradation. To address this limitation, we propose ArcFlow, a few-step distillation framework that explicitly employs non-linear flow trajectories to approximate pre-trained teacher trajectories. Concretely, ArcFlow parameterizes the velocity field underlying the inference trajectory as a mixture of continuous momentum processes. This enables ArcFlow to capture velocity evolution and extrapolate coherent velocities to form a continuous non-linear trajectory within each denoising step. Importantly, this parameterization admits an analytical integration of this non-linear trajectory, which circumvents numerical discretization errors and results in high-precision approximation of the teacher trajectory. To train this parameterization into a few-step generator, we implement ArcFlow via trajectory distillation on pre-trained teacher models using lightweight adapters. This strategy ensures fast, stable convergence while preserving generative diversity and quality. Built on large-scale models (Qwen-Image-20B and FLUX.1-dev), ArcFlow only fine-tunes on less than 5% of original parameters and achieves a 40x speedup with 2 NFEs over the original multi-step teachers without significant quality degradation. Experiments on benchmarks show the effectiveness of ArcFlow both qualitatively and quantitatively.

</details>


### [168] [Raster2Seq: Polygon Sequence Generation for Floorplan Reconstruction](https://arxiv.org/abs/2602.09016)
*Hao Phung,Hadar Averbuch-Elor*

Main category: cs.CV

TL;DR: 本文提出了一种称为Raster2Seq的方法，将楼层平面图重构问题转化为序列到序列的任务，通过自回归解码器预测下一个角落，并利用可学习的锚点引导注意力机制集中在具有信息性的图像区域。该方法在结构3D、CubiCasa5K和Raster2Graph等标准基准上表现出色，同时在包含复杂几何变体和多样化房间结构的WAFFLE数据集上也表现出强大的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的技术难以准确生成复杂楼层平面图中的结构和语义，尤其是当这些平面图展示大型室内空间、众多房间和多变的多边形角落数量时。本文旨在通过将问题转化为序列到序列的任务，以更灵活的方式处理复杂的楼层平面图，从而提高重建的精确性和通用性。

Method: 引入了一种自回归解码器，该解码器根据图像特征和之前生成的角落来预测下一个角落，并通过可学习的锚点来引导注意力机制关注信息丰富的图像区域。

Result: 本文的方法在多个标准基准上达到了最先进的性能，包括结构3D、CubiCasa5K和Raster2Graph，并且在WAFFLE数据集上也表现出了强大的泛化能力。

Conclusion: 文章提出了一种新的方法，通过将其转化为序列到序列的问题，并结合自回归解码器和可学习的锚点，成功地提高了一般和复杂楼层平面图的重构精度和通用性。

Abstract: Reconstructing a structured vector-graphics representation from a rasterized floorplan image is typically an important prerequisite for computational tasks involving floorplans such as automated understanding or CAD workflows. However, existing techniques struggle in faithfully generating the structure and semantics conveyed by complex floorplans that depict large indoor spaces with many rooms and a varying numbers of polygon corners. To this end, we propose Raster2Seq, framing floorplan reconstruction as a sequence-to-sequence task in which floorplan elements--such as rooms, windows, and doors--are represented as labeled polygon sequences that jointly encode geometry and semantics. Our approach introduces an autoregressive decoder that learns to predict the next corner conditioned on image features and previously generated corners using guidance from learnable anchors. These anchors represent spatial coordinates in image space, hence allowing for effectively directing the attention mechanism to focus on informative image regions. By embracing the autoregressive mechanism, our method offers flexibility in the output format, enabling for efficiently handling complex floorplans with numerous rooms and diverse polygon structures. Our method achieves state-of-the-art performance on standard benchmarks such as Structure3D, CubiCasa5K, and Raster2Graph, while also demonstrating strong generalization to more challenging datasets like WAFFLE, which contain diverse room structures and complex geometric variations.

</details>


### [169] [WorldCompass: Reinforcement Learning for Long-Horizon World Models](https://arxiv.org/abs/2602.09022)
*Zehan Wang,Tengfei Wang,Haiyu Zhang,Xuhui Zuo,Junta Wu,Haoyuan Wang,Wenqiang Sun,Zhenwei Wang,Chenjie Cao,Hengshuang Zhao,Chunchao Guo,Zhou Zhao*

Main category: cs.CV

TL;DR: WorldCompass 提出了一种新的基于强化学习的后训练框架，适用于长周期的互动视频世界模型，通过三种创新使模型探索更准确和一致。


<details>
  <summary>Details</summary>
Motivation: 当前的视频生成模型在探索时可能存在效率低下和行为误导等问题，WorldCompass 研究旨在改进这些问题。

Method: WorldCompass 采用了剪辑级别的评估策略、互补奖励函数以及高效的RL算法等手段，以提升模型的探索准确性和视觉保真度。

Result: 在最先进的开源世界模型 WorldPlay 上的评估表明，WorldCompass 显著提高了交互准确性和视觉保真度。

Conclusion: WorldCompass 框架增强了世界模型在长周期互动中的性能，为相关领域提供了新的研究方向。

Abstract: This work presents WorldCompass, a novel Reinforcement Learning (RL) post-training framework for the long-horizon, interactive video-based world models, enabling them to explore the world more accurately and consistently based on interaction signals. To effectively "steer" the world model's exploration, we introduce three core innovations tailored to the autoregressive video generation paradigm: 1) Clip-level rollout Strategy: We generate and evaluate multiple samples at a single target clip, which significantly boosts rollout efficiency and provides fine-grained reward signals. 2) Complementary Reward Functions: We design reward functions for both interaction-following accuracy and visual quality, which provide direct supervision and effectively suppress reward-hacking behaviors. 3) Efficient RL Algorithm: We employ the negative-aware fine-tuning strategy coupled with various efficiency optimizations to efficiently and effectively enhance model capacity. Evaluations on the SoTA open-source world model, WorldPlay, demonstrate that WorldCompass significantly improves interaction accuracy and visual fidelity across various scenarios.

</details>


### [170] [Autoregressive Image Generation with Masked Bit Modeling](https://arxiv.org/abs/2602.09024)
*Qihang Yu,Qihao Liu,Ju He,Xinyang Zhang,Yang Liu,Liang-Chieh Chen,Xi Chen*

Main category: cs.CV

TL;DR: 该研究挑战了视觉生成中连续管道的主导地位，系统地调查了离散和连续方法之间的性能差距，提出了一种可扩展的掩码Bit自回归建模（BAR）框架，以提高离散生成方法的性能。


<details>
  <summary>Details</summary>
Motivation: 论文旨在解决离散生成方法在量化模型大小时面临的性能下降和高昂训练成本问题，通过对比连续和离散方法的性能差距，提出一种新的建模方法来改进离散生成模型。

Method: 论文提出了一种名为掩码Bit自回归建模（BAR）的框架，通过自回归变压器和掩码位建模头，逐步生成离散令牌的组成部分位。

Result: BAR在ImageNet-256上的gFID达到0.99的新最优水平，在连续和离散范式中均优于领先方法，同时减少采样成本并比先前提到的连续方法更快收敛。

Conclusion: 掩码Bit自回归建模（BAR）提供了一种方法，能够有效地提高离散视觉生成模型的性能，并通过代码本大小的规模化来缩小与连续方法的性能差距，证明该方法在视觉生成领域具有较大的应用潜力。

Abstract: This paper challenges the dominance of continuous pipelines in visual generation. We systematically investigate the performance gap between discrete and continuous methods. Contrary to the belief that discrete tokenizers are intrinsically inferior, we demonstrate that the disparity arises primarily from the total number of bits allocated in the latent space (i.e., the compression ratio). We show that scaling up the codebook size effectively bridges this gap, allowing discrete tokenizers to match or surpass their continuous counterparts. However, existing discrete generation methods struggle to capitalize on this insight, suffering from performance degradation or prohibitive training costs with scaled codebook. To address this, we propose masked Bit AutoRegressive modeling (BAR), a scalable framework that supports arbitrary codebook sizes. By equipping an autoregressive transformer with a masked bit modeling head, BAR predicts discrete tokens through progressively generating their constituent bits. BAR achieves a new state-of-the-art gFID of 0.99 on ImageNet-256, outperforming leading methods across both continuous and discrete paradigms, while significantly reducing sampling costs and converging faster than prior continuous approaches. Project page is available at https://bar-gen.github.io/

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [171] [Does Visual Rendering Bypass Tokenization? Investigating Script-Tokenizer Misalignment in Pixel-Based Language Models](https://arxiv.org/abs/2602.06973)
*Lucky Susanto,Musa Izzanardi Wijanarko,Khumaisa Nur'aini,Farid Adilazuarda,Alham Fikri Aji,Derry Tanti Wijaya*

Main category: cs.CL

TL;DR: 尽管视觉渲染试图绕过子词标记化瓶颈，但重新集成文本标记器到架构中会引发新的问题。使用Llama 2标记器表现远远低于自定义标记器，提示未来多模态变体应关注解决文本标记器问题。


<details>
  <summary>Details</summary>
Motivation: 研究视觉渲染是否真正解除了模型与标记化约束之间的联系，特别是在几种印尼低资源本地语言（日惹语、巴厘语、苏门答腊语、 Lampungnese）中，考察标记化器对无联编问题的影响。

Method: 通过评估四种印尼低资源本地语言中脚本标记器对DualGPT架构的影响，研究视觉渲染与文本标记器重新集成的效果。

Result: 实验结果显示，即使视觉渲染降低了OOV和生育率，Llama 2标记器的表现仍然大幅劣于自定义标记器，最高差异达30.15 chrF++。

Conclusion: 研究结果表明，尽管视觉渲染试图解决标记化问题，但文本标记器依然是模型中的一个重大障碍，提醒未来多模态模型应关注此问题。

Abstract: While pixel-based language modeling aims to bypass the sub-word tokenization bottleneck by rendering text as images, recent multimodal variants such as DualGPT reintroduce text tokenizers to improve autoregressive performance. We investigate a fundamental question, does visual rendering truly decouple a model from tokenization constraints? Focusing on four Indonesian low-resource local languages that have their own non-Latin scripts (i.e., Javanese, Balinese, Sundanese, and Lampungnese), we evaluate the impact of script-tokenizer alignment within the DualGPT architecture. Our results show that, despite visual rendering, reintegrating a text tokenizer into the architecture reintroduces the same issue that pixel-based language modeling aims to resolve, which is the tokenizer misalignment problem. Despite having lower OOV and fertility rates, we show that the Llama 2 tokenizer performs significantly worse than a custom tokenizer, with improvements of up to 30.15 chrF++. Our findings serve as a warning for future multimodal variants, as text tokenizers remain a significant barrier to equitable models.

</details>


### [172] [Bridging the Knowledge Void: Inference-time Acquisition of Unfamiliar Programming Languages for Coding Tasks](https://arxiv.org/abs/2602.06976)
*Chen Shen,Wei Cheng,Jingyue Yang,Huan Zhang,Yuhan Wu,Wei Hu*

Main category: cs.CL

TL;DR: 本文提出了一种在低资源环境下使大语言模型掌握陌生编程语言的新方法，通过与少量外部资源的动态交互，展示了此方法在代码生成、翻译和程序修复上的优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在处理未见过的编程语言时表现不佳，本文旨在探索一种低资源环境下的新方法，即通过与少量外部资源的动态交互，让大语言模型掌握陌生编程语言。

Method: 提出了一种名为ILA-agent的框架，通过模拟人类行为模式，让大语言模型逐步探索、应用和验证语言知识，并通过一个基于新型静态类型语言Cangjie的多任务基准进行评估。

Result: 使用各种大语言模型进行的实验结果表明，与检索增强的基础方法相比，ILA-agent在代码生成、翻译和程序修复等任务上的表现更优。

Conclusion: 初步分析结果揭示了锻炼行为模式，同时指出了有待改进的地方。

Abstract: The proficiency of Large Language Models (LLMs) in coding tasks is often a reflection of their extensive pre-training corpora, which typically collapses when confronted with previously unfamiliar programming languages. Departing from data-intensive finetuning, we investigate the paradigm of Inference-time Language Acquisition (ILA), where an LLM masters an unfamiliar language through dynamic interaction with limited external resources. In this paper, we propose ILA-agent, a general ILA framework that equips LLMs with a set of behavioral primitives. By modeling essential human-like behaviors as a suite of tools, ILA-agent enables LLMs to incrementally explore, apply, and verify language knowledge through structured interactions with the official documentation and execution environment. To provide a rigorous evaluation in a low-resource setting, we construct Cangjie-bench, a multi-task benchmark based on the novel statically-typed language Cangjie. We instantiate ILA-agent for Cangjie and evaluate its performance across code generation, translation, and program repair tasks. Results using diverse LLMs demonstrate that ILA-agent significantly outperforms retrieval-augmented baselines. Further analysis of agent trajectories characterizes the emergent behavior patterns while highlighting persisting performance gaps.

</details>


### [173] [Anchored Decoding: Provably Reducing Copyright Risk for Any Language Model](https://arxiv.org/abs/2602.07120)
*Jacqueline He,Jonathan Hayase,Wen-tau Yih,Sewoong Oh,Luke Zettlemoyer,Pang Wei Koh*

Main category: cs.CL

TL;DR: 文章提出了一种名为Anchored Decoding的方法，作为一种插件功能，在解码时抑制verbatim复制，特别适用于训练数据混合许可的情况。该方法通过保持生成与一个训练有素的许可模型的生成结果在一定范围内，从而能够在风险和实用性之间进行权衡。


<details>
  <summary>Details</summary>
Motivation: 由于现代语言模型（LMs）在训练过程中会记住部分内容并在生成中重复其训练数据片段，这可能会引发版权保护问题及其背后的创作者与使用者之间的权利和合规性问题。因此，需要一种方法来抑制这种verbatim复制。

Method: Anchored Decoding方法通过在生成过程中限制输出与一个训练有素的许可模型之间的距离来实现这一点。它允许用户为生成过程分配信息预算，确保每一步都有约束，从而在整个序列级别上提供保证。

Result: 研究表明，Anchored和Anchored_Byte Decoding方法在六个模型对上的长期评估中表现出色，能够保留接近原文的流畅性和事实性，同时将可测量的复制差距减少高达75％，并增加了可调贸易空间（风险-实用性）。

Conclusion: 通过引入一种新的许可训练模型TinyComma 1.8B和Anchored_Byte Decoding，研究定义了一个新的帕累托前沿，解决了风险和实用性之间的权衡。

Abstract: Modern language models (LMs) tend to memorize portions of their training data and emit verbatim spans. When the underlying sources are sensitive or copyright-protected, such reproduction raises issues of consent and compensation for creators and compliance risks for developers. We propose Anchored Decoding, a plug-and-play inference-time method for suppressing verbatim copying: it enables decoding from any risky LM trained on mixed-license data by keeping generation in bounded proximity to a permissively trained safe LM. Anchored Decoding adaptively allocates a user-chosen information budget over the generation trajectory and enforces per-step constraints that yield a sequence-level guarantee, enabling a tunable risk-utility trade-off. To make Anchored Decoding practically useful, we introduce a new permissively trained safe model (TinyComma 1.8B), as well as Anchored$_{\mathrm{Byte}}$ Decoding, a byte-level variant of our method that enables cross-vocabulary fusion via the ByteSampler framework (Hayase et al., 2025). We evaluate our methods across six model pairs on long-form evaluations of copyright risk and utility. Anchored and Anchored$_{\mathrm{Byte}}$ Decoding define a new Pareto frontier, preserving near-original fluency and factuality while eliminating up to 75% of the measurable copying gap (averaged over six copying metrics) between the risky baseline and a safe reference, at a modest inference overhead.

</details>


### [174] [Open TutorAI: An Open-source Platform for Personalized and Immersive Learning with Generative AI](https://arxiv.org/abs/2602.07176)
*Mohamed El Hajji,Tarek Ait Baha,Aicha Dakir,Hammou Fadili,Youssef Es-Saady*

Main category: cs.CL

TL;DR: Open TutorAI 是一个基于大语言模型和生成技术的开源教育平台，提供动态个性化辅导。通过结合自然语言处理和可定制的3D角色，它支持多模态学员交互，并通过结构化入职过程捕捉每个学员的目标和偏好。


<details>
  <summary>Details</summary>
Motivation: 现有的教育聊天机器人系统通常缺乏情景适应性、实时响应能力和教学灵活性，这可能会限制学习者参与并削弱教学效果。因此，需要一个开放融合的平台结合AI和沉浸式技术来支持个性化、有意义的学习体验。

Method: Open TutorAI 采用了基于大语言模型和生成技术的方法，通过自然语言处理和自定义3D角色来实现多模态学习者交互。它通过结构化的入职过程收集每位学习者的偏好和目标，从而配制个性化的AI助手。助手可通过文本和基于角色的界面访问。

Result: 通过使用此平台，可以生成适应个人学习者档案的辅助工具，不必依赖技术专业知识。辅助生成流水线和角色集成增强了参与度和情感存在感，创建了一个更加人性化和沉浸的学习环境。嵌入的学习分析支持自我调节学习，通过跟踪参与模式并生成行动反馈来支持。

Conclusion: Open TutorAI 融合了模块化架构、生成AI和学习分析，为下一代智能辅导系统的发展做出了贡献。

Abstract: Recent advances in artificial intelligence have created new possibilities for making education more scalable, adaptive, and learner-centered. However, existing educational chatbot systems often lack contextual adaptability, real-time responsiveness, and pedagogical agility. which can limit learner engagement and diminish instructional effectiveness. Thus, there is a growing need for open, integrative platforms that combine AI and immersive technologies to support personalized, meaningful learning experiences. This paper presents Open TutorAI, an open-source educational platform based on LLMs and generative technologies that provides dynamic, personalized tutoring. The system integrates natural language processing with customizable 3D avatars to enable multimodal learner interaction. Through a structured onboarding process, it captures each learner's goals and preferences in order to configure a learner-specific AI assistant. This assistant is accessible via both text-based and avatar-driven interfaces. The platform includes tools for organizing content, providing embedded feedback, and offering dedicated interfaces for learners, educators, and parents. This work focuses on learner-facing components, delivering a tool for adaptive support that responds to individual learner profiles without requiring technical expertise. Its assistant-generation pipeline and avatar integration enhance engagement and emotional presence, creating a more humanized, immersive learning environment. Embedded learning analytics support self-regulated learning by tracking engagement patterns and generating actionable feedback. The result is Open TutorAI, which unites modular architecture, generative AI, and learner analytics within an open-source framework. It contributes to the development of next-generation intelligent tutoring systems.

</details>


### [175] [Can LLMs Discern the Traits Influencing Your Preferences? Evaluating Personality-Driven Preference Alignment in LLMs](https://arxiv.org/abs/2602.07181)
*Tianyu Zhao,Siqi Li,Yasser Shoukry,Salma Elmalaki*

Main category: cs.CL

TL;DR: 研究发现，将用户的人格特质对齐的偏好信号可以显著提高个人化问题回答的准确性。提出的PACIFIC数据集和框架帮助语言模型自动获取匹配用户人格特征的偏好以优化答案。


<details>
  <summary>Details</summary>
Motivation: 现有个性化LLM解决方案中的偏好信号往往不够可靠，容易因噪声、不完整或误导性而降低答案质量，因此研究提出利用人格特质这一稳定的心理学特征来作为偏好信号的规范‘潜在’信号。

Method: 通过广泛实验，该研究使用PACIFIC数据集来评估将用户的人格特质对齐的偏好信号如何影响个性化问题回答的效果。

Result: 实验结果显示，根据用户的人格特质调整偏好信号能大幅提高答案选择的准确性，从原本的29.25%提升至76%。

Conclusion: 研究提出了PACIFIC（五大人格特质评分偏好数据集），并设计了一种框架，使得大规模语言模型能够自动获取匹配用户人格特征的偏好信号以提高答案的质量。“五大人格特质评分偏好数据集”和该框架被展示为提高个性化语言模型答案质量的有效解决方案。

Abstract: User preferences are increasingly used to personalize Large Language Model (LLM) responses, yet how to reliably leverage preference signals for answer generation remains under-explored. In practice, preferences can be noisy, incomplete, or even misleading, which can degrade answer quality when applied naively. Motivated by the observation that stable personality traits shape everyday preferences, we study personality as a principled ''latent'' signal behind preference statements. Through extensive experiments, we find that conditioning on personality-aligned preferences substantially improves personalized question answering: selecting preferences consistent with a user's inferred personality increases answer-choice accuracy from 29.25% to 76%, compared to using randomly selected preferences. Based on these findings, we introduce PACIFIC (Preference Alignment Choices Inference for Five-factor Identity Characterization), a personality-labeled preference dataset containing 1200 preference statements spanning diverse domains (e.g., travel, movies, education), annotated with Big-Five (OCEAN) trait directions. Finally, we propose a framework that enables an LLM model to automatically retrieve personality-aligned preferences and incorporate them during answer generation.

</details>


### [176] [Long-Context Long-Form Question Answering for Legal Domain](https://arxiv.org/abs/2602.07190)
*Anagha Kulkarni,Parin Rajesh Jhaveri,Prasha Shrestha,Yu Tong Han,Reza Amini,Behrouz Madahian*

Main category: cs.CL

TL;DR: 本文提出了一种针对法律文件中长上下文问题回答的系统，该系统能够拆解领域专有名词、解析复杂文档布局，并生成精准的长回答，同时引入了一个覆盖度评估指标，通过实验证明了系统的实用性。


<details>
  <summary>Details</summary>
Motivation: 法律文件具有复杂布局、长段落和专业的词汇特点，导致问题回答变得困难。本文旨在解决法律文件中的长上下文问题回答挑战。

Method: 提出了一种问题回答系统，该系统能够拆解领域专有名词、解析复杂文档布局，并生成精准的长回答。引入了覆盖度评估指标，通过实验来验证系统的实用性。

Result: 通过全面的实验和消融实验，证明了所提出系统的可行性和有效性。

Conclusion: 本文提出了一种针对法律文件的长上下文问题回答系统，并通过实验证明了其在复杂文档中的实用性。

Abstract: Legal documents have complex document layouts involving multiple nested sections, lengthy footnotes and further use specialized linguistic devices like intricate syntax and domain-specific vocabulary to ensure precision and authority. These inherent characteristics of legal documents make question answering challenging, and particularly so when the answer to the question spans several pages (i.e. requires long-context) and is required to be comprehensive (i.e. a long-form answer). In this paper, we address the challenges of long-context question answering in context of long-form answers given the idiosyncrasies of legal documents. We propose a question answering system that can (a) deconstruct domain-specific vocabulary for better retrieval from source documents, (b) parse complex document layouts while isolating sections and footnotes and linking them appropriately, (c) generate comprehensive answers using precise domain-specific vocabulary. We also introduce a coverage metric that classifies the performance into recall-based coverage categories allowing human users to evaluate the recall with ease. We curate a QA dataset by leveraging the expertise of professionals from fields such as law and corporate tax. Through comprehensive experiments and ablation studies, we demonstrate the usability and merit of the proposed system.

</details>


### [177] [Equipping LLM with Directional Multi-Talker Speech Understanding Capabilities](https://arxiv.org/abs/2602.07211)
*Ju Lin,Jing Pan,Ruizhi Li,Ming Sun,Yuzong Liu,Alaa Hassan,Jing Zheng,Florian Metze*

Main category: cs.CL

TL;DR: 该研究提出了一种方法，通过将音频编码输入到大型语言模型中，增强其对多说话人多通道语音的理解能力，特别适用于智能眼镜场景。


<details>
  <summary>Details</summary>
Motivation: 现有的语音大型语言模型主要基于单通道单说话人的数据进行训练，难以直接用于多说话人多通道的语音理解任务。

Method: 研究提出了两种方法：一种是级联系统，使用源分离前端模块，另一种是端到端系统，使用序列化输出训练。

Result: 实验结果显示，两种方法能够有效提升大型语言模型的定向语音理解能力，在语音识别和语音翻译任务中表现出强劲的性能。

Conclusion: 该研究为基于智能眼镜的多说话人多通道语音理解的能力提升提供了有效途径。

Abstract: Recent studies have demonstrated that prompting large language models (LLM) with audio encodings enables effective speech understanding capabilities. However, most speech LLMs are trained on single-channel, single-talker data, which makes it challenging to directly apply them to multi-talker and multi-channel speech understanding task. In this work, we present a comprehensive investigation on how to enable directional multi-talker speech understanding capabilities for LLMs, specifically in smart glasses usecase. We propose two novel approaches to integrate directivity into LLMs: (1) a cascaded system that leverages a source separation front-end module, and (2) an end-to-end system that utilizes serialized output training. All of the approaches utilize a multi-microphone array embedded in smart glasses to optimize directivity interpretation and processing in a streaming manner. Experimental results demonstrate the efficacy of our proposed methods in endowing LLMs with directional speech understanding capabilities, achieving strong performance in both speech recognition and speech translation tasks.

</details>


### [178] [Beyond Accuracy: Risk-Sensitive Evaluation of Hallucinated Medical Advice](https://arxiv.org/abs/2602.07319)
*Savan Doshi*

Main category: cs.CL

TL;DR: 研究提出了一种风险敏感的评估框架，该框架通过识别风险承担的语言来量化幻觉，如治疗指令、禁忌症、紧迫性提示和高风险药物提及。这种方法侧重于评估幻觉内容若被采纳的潜在影响，而不仅仅是临床正确性。


<details>
  <summary>Details</summary>
Motivation: 现有的幻觉标准和评价指标主要关注事实的正确性，所有错误被视为同等严重性，这在临床上是不合理的，特别是当模型生成未支持但可行动的医疗语言时。

Method: 该研究提出了一个结合风险评分和相关性度量的风险敏感评价框架，用于识别高风险、低背景错误。并且，通过控制设计的对患者有安全压力测试的提示对三种指令调优的语言模型进行了应用。

Result: 结果显示，具有相似表面行为的语言模型在风险概况上表现出显著差异，而标准评估指标未能捕捉这些差异。

Conclusion: 这些发现强调了将风险敏感性纳入幻觉评估的重要性，并表明评估的有效性高度依赖于任务和提示设计。

Abstract: Large language models are increasingly being used in patient-facing medical question answering, where hallucinated outputs can vary widely in potential harm. However, existing hallucination standards and evaluation metrics focus primarily on factual correctness, treating all errors as equally severe. This obscures clinically relevant failure modes, particularly when models generate unsupported but actionable medical language. We propose a risk-sensitive evaluation framework that quantifies hallucinations through the presence of risk-bearing language, including treatment directives, contraindications, urgency cues, and mentions of high-risk medications. Rather than assessing clinical correctness, our approach evaluates the potential impact of hallucinated content if acted upon. We further combine risk scoring with a relevance measure to identify high-risk, low-grounding failures. We apply this framework to three instruction-tuned language models using controlled patient-facing prompts designed as safety stress tests. Our results show that models with similar surface-level behavior exhibit substantially different risk profiles and that standard evaluation metrics fail to capture these distinctions. These findings highlight the importance of incorporating risk sensitivity into hallucination evaluation and suggest that evaluation validity is critically dependent on task and prompt design.

</details>


### [179] [Intent Mismatch Causes LLMs to Get Lost in Multi-Turn Conversation](https://arxiv.org/abs/2602.07338)
*Geng Liu,Fei Zhu,Rong Feng,Changyi Ma,Shiqi Wang,Gaofeng Meng*

Main category: cs.CL

TL;DR: 本文指出了LLMs在多轮对话中表现下降（Lost in Conversation）的主要原因并非模型本身的能力缺陷，而是交互中的意图对齐问题。为此，提出了Mediator-Assistant架构，通过经验驱动的调解者将用户的模糊意图转化为明确、结构良好的指令，实验结果表明这种方法能够有效地减轻多轮对话中的性能下降。


<details>
  <summary>Details</summary>
Motivation: 传统的研究认为LLMs在多轮对话中表现差的原因在于模型本身的可靠性问题，但本文认为这并非根本原因，而是互动过程中的意图对齐问题。

Method: 本文提出了一种Mediator-Assistant架构，通过经验驱动的调解者将用户的模糊意图转化为明确、结构良好的指令，以解决多轮对话中的意图对齐问题。

Result: 实验结果表明，这种方法能够显著减轻多轮对话中的性能下降，适用于多种LLMs。

Conclusion: 本文提出的Mediator-Assistant架构为解决LLMs在多轮对话中的表现下降问题提供了一种有效的方法。

Abstract: Multi-turn conversation has emerged as a predominant interaction paradigm for Large Language Models (LLMs). Users often employ follow-up questions to refine their intent, expecting LLMs to adapt dynamically. However, recent research reveals that LLMs suffer a substantial performance drop in multi-turn settings compared to single-turn interactions with fully specified instructions, a phenomenon termed ``Lost in Conversation'' (LiC). While this prior work attributes LiC to model unreliability, we argue that the root cause lies in an intent alignment gap rather than intrinsic capability deficits. In this paper, we first demonstrate that LiC is not a failure of model capability but rather a breakdown in interaction between users and LLMs. We theoretically show that scaling model size or improving training alone cannot resolve this gap, as it arises from structural ambiguity in conversational context rather than representational limitations. To address this, we propose to decouple intent understanding from task execution through a Mediator-Assistant architecture. By utilizing an experience-driven Mediator to explicate user inputs into explicit, well-structured instructions based on historical interaction patterns, our approach effectively bridges the gap between vague user intent and model interpretation. Experimental results demonstrate that this method significantly mitigates performance degradation in multi-turn conversations across diverse LLMs.

</details>


### [180] [ViHERMES: A Graph-Grounded Multihop Question Answering Benchmark and System for Vietnamese Healthcare Regulations](https://arxiv.org/abs/2602.07361)
*Long S. T. Nguyen,Quan M. Bui,Tin T. Ngo,Quynh T. N. Vo,Dung N. H. Le,Tho T. Quan*

Main category: cs.CL

TL;DR: 该研究引入了ViHERMES数据集，这是一个用于越南医疗法规多跳问答的基准。该数据集包含高质量的问题-答案对，能够处理多个法规之间的推理，并捕捉多种依赖模式。实验结果表明，与基于检索的基线方法相比，所提出的图意识检索框架在多跳监管问答系统评估中的表现更佳。


<details>
  <summary>Details</summary>
Motivation: 针对医疗法规的多跳推理问题，特别是在越南等低资源语言环境中，缺乏明确支持法律文件间多跳推理的数据集，因此该研究旨在解决这一问题。

Method: 研究团队提出了一个受语义聚类和图启发的数据挖掘驱动的多跳问答生成管道，并使用大型语言模型进行结构化证据和推理注释生成。此外，还提出了一种考虑图结构的检索框架，用于建模法律单元之间的正式法律关系，并支持出于法律上有效和连贯的答案的原则性上下文扩展。

Result: 实验结果表明ViHERMES是一个具有挑战性的基准，用于评估多跳监管问答系统。同时，提出的图意识检索框架在多跳监管问答任务中表现出色，优于强检索方法。

Conclusion: 该研究成功地将ViHERMES数据集应用于越南医疗法规的多跳问答任务，并展示了其在评估多跳监管问答系统中的优越性。

Abstract: Question Answering (QA) over regulatory documents is inherently challenging due to the need for multihop reasoning across legally interdependent texts, a requirement that is particularly pronounced in the healthcare domain where regulations are hierarchically structured and frequently revised through amendments and cross-references. Despite recent progress in retrieval-augmented and graph-based QA methods, systematic evaluation in this setting remains limited, especially for low-resource languages such as Vietnamese, due to the lack of benchmark datasets that explicitly support multihop reasoning over healthcare regulations. In this work, we introduce the Vietnamese Healthcare Regulations-Multihop Reasoning Dataset (ViHERMES), a benchmark designed for multihop QA over Vietnamese healthcare regulatory documents. ViHERMES consists of high-quality question-answer pairs that require reasoning across multiple regulations and capture diverse dependency patterns, including amendment tracing, cross-document comparison, and procedural synthesis. To construct the dataset, we propose a controlled multihop QA generation pipeline based on semantic clustering and graph-inspired data mining, followed by large language model-based generation with structured evidence and reasoning annotations. We further present a graph-aware retrieval framework that models formal legal relations at the level of legal units and supports principled context expansion for legally valid and coherent answers. Experimental results demonstrate that ViHERMES provides a challenging benchmark for evaluating multihop regulatory QA systems and that the proposed graph-aware approach consistently outperforms strong retrieval-based baselines. The ViHERMES dataset and system implementation are publicly available at https://github.com/ura-hcmut/ViHERMES.

</details>


### [181] [TernaryLM: Memory-Efficient Language Modeling via Native 1-Bit Quantization with Adaptive Layer-wise Scaling](https://arxiv.org/abs/2602.07374)
*Nisharg Nargund,Priyesh Shukla*

Main category: cs.CL

TL;DR: TernaryLM 提出了一种 132M 参数的变压器架构，采用训练期间的原生 1 位三值量化，实现了显存显著减少且不牺牲语言模型能力。与后训练量化方法不同，TernaryLM 从头学习量化感知表示。实验结果表明，TernaryLM 在 TinyStories 上的验证困惑度为 58.42，MRPC 下游迁移的 F1 值为 82.47%，内存减少 2.4 倍，推理延迟相似，且在多种数据集上具有稳定的训练动力学。


<details>
  <summary>Details</summary>
Motivation: 鉴于大型语言模型需要大量的计算资源，限制了其在边缘设备和资源受限环境中的应用。本文提出了一种有效的解决方案，通过使用训练期间的原生 1 位三值量化，使得模型能够在不降低语言建模能力的同时显著减少所需内存。

Method: TernaryLM 采用了原生 1 位三值量化，并使用直通估算器和可变层尺度因子来训练量化感知表示。该方法与后训练量化方法不同，后者是在预先训练好的全精度模型上进行量化。

Result: 实验结果表明，TernaryLM 在 TinyStories 上的验证困惑度为 58.42，MRPC 下游任务上的 F1 值达到了 82.47%，相比未量化模型，内存使用减少了 2.4 倍，并且保持了相近的推理延迟。此外，还进行了逐层量化分析，指出中间的变压器层与极度量化兼容性最高，为未来的非均匀精度策略提供了指导。

Conclusion: 本文展示了原生 1 位训练的前景，认为这是一种适用于高效神经语言模型的有希望的方向。

Abstract: Large language models (LLMs) achieve remarkable performance but demand substantial computational resources, limiting deployment on edge devices and resource-constrained environments. We present TernaryLM, a 132M parameter transformer architecture that employs native 1-bit ternary quantization {-1, 0, +1} during training, achieving significant memory reduction without sacrificing language modeling capability. Unlike post-training quantization approaches that quantize pre-trained full-precision models, TernaryLM learns quantization-aware representations from scratch using straight-through estimators and adaptive per-layer scaling factors. Our experiments demonstrate: (1) validation perplexity of 58.42 on TinyStories; (2) downstream transfer with 82.47 percent F1 on MRPC paraphrase detection; (3) 2.4x memory reduction (498MB vs 1197MB) with comparable inference latency; and (4) stable training dynamics across diverse corpora. We provide layer-wise quantization analysis showing that middle transformer layers exhibit highest compatibility with extreme quantization, informing future non-uniform precision strategies. Our results suggest that native 1-bit training is a promising direction for efficient neural language models. Code is available at https://github.com/1nisharg/TernaryLM-Memory-Efficient-Language-Modeling.

</details>


### [182] [Efficient Post-Training Pruning of Large Language Models with Statistical Correction](https://arxiv.org/abs/2602.07375)
*Peiqi Yu,Jinhao Wang,Xinyi Sui,Nam Ling,Wei Wang,Wei Jiang*

Main category: cs.CL

TL;DR: 提出了一种基于模型权重和激活的一阶统计特性的轻量级剪枝框架，通过通道级统计校准重要性评分，减少激活主导通道的偏见，并在修剪后应用分析能量补偿来纠正因权重移除导致的分布失真。该框架无需重新训练、梯度或二阶信息，在多个语言模型家族、稀疏模式和评估任务中展示了较好的剪枝性能。


<details>
  <summary>Details</summary>
Motivation: 剪枝是减少大型语言模型大小和推断成本的有效方法，但现有技术在剪枝质量和计算效率之间存在权衡。本文方法旨在提出一种轻量级的后训练剪枝框架，旨在改善剪枝表现同时保持计算成本与启发式方法相当。

Method: 该方法基于模型权重和激活的一阶统计特性，通过通道级统计信息校正重要性评分，减少激活主导通道的偏差，采用分析能量补偿纠正由权重移除导致的分布失真。所有步骤均无需重新训练、计算梯度或使用二阶信息。

Result: 在多个语言模型家族、不同的稀疏模式和评估任务中，该方法展示了优于启发式方法和重建基础方法的剪枝性能，同时保持了与启发式方法相当的计算效率。

Conclusion: 该研究证明了简单的统计修正对于后训练大型语言模型剪枝的有效性。

Abstract: Post-training pruning is an effective approach for reducing the size and inference cost of large language models (LLMs), but existing methods often face a trade-off between pruning quality and computational efficiency. Heuristic pruning methods are efficient but sensitive to activation outliers, while reconstruction-based approaches improve fidelity at the cost of heavy computation. In this work, we propose a lightweight post-training pruning framework based on first-order statistical properties of model weights and activations. During pruning, channel-wise statistics are used to calibrate magnitude-based importance scores, reducing bias from activation-dominated channels. After pruning, we apply an analytic energy compensation to correct distributional distortions caused by weight removal. Both steps operate without retraining, gradients, or second-order information. Experiments across multiple LLM families, sparsity patterns, and evaluation tasks show that the proposed approach improves pruning performance while maintaining computational cost comparable to heuristic methods. The results suggest that simple statistical corrections can be effective for post-training pruning of LLMs.

</details>


### [183] [Do Large Language Models Reflect Demographic Pluralism in Safety?](https://arxiv.org/abs/2602.07376)
*Usman Naseem,Gautam Siddharth Kashyap,Sushant Kumar Ray,Rafiq Ali,Ebad Shabbir,Abdullah Mohammad*

Main category: cs.CL

TL;DR: Demo-SafetyBench提出了一个评估大型语言模型安全性的新框架，通过直接在提示层面建模群体多样性，打破了传统数据集的局限性。该研究通过两个阶段构建了一个包含43,050个样本的高质量数据集，并使用LLM作为评估者来评估多维度的安全性。


<details>
  <summary>Details</summary>
Motivation: 现有的安全对齐数据集狭窄的注释者群体导致了安全性感知的差距。因此，需要一个能够直接反映群体多样性的新方法。

Method: 研究通过重新分类DICES提示至14个安全领域，并使用启发式算法进行去重，构建了一个旨在评估群体多样性的大型语言模型数据集。通过LLM作为评估者，实现了高可靠性与低群体敏感性。

Result: Demonstration of high reliability (ICC = 0.87) and low demographic sensitivity (DS = 0.12) in evaluating large language model safety with a well-constructed dataset, confirming the feasibility of scalable and demographic robust evaluation.

Conclusion: 这项工作提出了一种评估大型语言模型安全性的新方法，为以后的研究提供了一个可扩展且群体公平的评估标准。

Abstract: Large Language Model (LLM) safety is inherently pluralistic, reflecting variations in moral norms, cultural expectations, and demographic contexts. Yet, existing alignment datasets such as ANTHROPIC-HH and DICES rely on demographically narrow annotator pools, overlooking variation in safety perception across communities. Demo-SafetyBench addresses this gap by modeling demographic pluralism directly at the prompt level, decoupling value framing from responses. In Stage I, prompts from DICES are reclassified into 14 safety domains (adapted from BEAVERTAILS) using Mistral 7B-Instruct-v0.3, retaining demographic metadata and expanding low-resource domains via Llama-3.1-8B-Instruct with SimHash-based deduplication, yielding 43,050 samples. In Stage II, pluralistic sensitivity is evaluated using LLMs-as-Raters-Gemma-7B, GPT-4o, and LLaMA-2-7B-under zero-shot inference. Balanced thresholds (delta = 0.5, tau = 10) achieve high reliability (ICC = 0.87) and low demographic sensitivity (DS = 0.12), confirming that pluralistic safety evaluation can be both scalable and demographically robust.

</details>


### [184] [When the Model Said 'No Comment', We Knew Helpfulness Was Dead, Honesty Was Alive, and Safety Was Terrified](https://arxiv.org/abs/2602.07381)
*Gautam Siddharth Kashyap,Mark Dras,Usman Naseem*

Main category: cs.CL

TL;DR: 该研究提出了AlignX框架，它通过两阶段的方法解决了大型语言模型（LLM）多目标调整中的轴坍塌问题，显著提高了模型在HHH（友善、无害、诚实）方面的表现，同时降低了延迟和内存使用。


<details>
  <summary>Details</summary>
Motivation: 现有的方法如监督微调（SFT）和专家混合模型（MoE）在应对多目标设置时存在问题，本文提出AlignX框架以克服这些问题。

Method: AlignX框架分为两阶段。第一阶段使用带提示的微调提取轴特异性任务特征，以减少灾难性遗忘。第二阶段使用分形与自然几何学构建的MoCaE模块来校准专家路由，提高推理可靠性。

Result: 实验结果显示，AlignX显著提升了Alpaca的友善性、BeaverTails的无害性和TruthfulQA的诚实性，同时提升了真实性和信息性，减少了4.3%的安全违规，且相比之前的MoE模型延迟和内存使用分别降低了超过35%。

Conclusion: AlignX在多个LLM上验证了其普适性，有效解决了多目标优化中的轴坍塌问题，是Lm安全部署的有效方法。

Abstract: Large Language Models (LLMs) need to be in accordance with human values-being helpful, harmless, and honest (HHH)-is important for safe deployment. Existing works use Supervised Fine-Tuning (SFT) and Mixture-of-Experts (MoE) to align LLMs. However, these works face challenges in multi-objective settings, such as SFT leading to interference between conflicting objectives, while MoEs suffer from miscalibrated routing. We term this failure mode Axis Collapse, marked by (1) disjoint feature spaces causing catastrophic forgetting, and (2) unreliable inference from misrouted experts. To resolve this, we propose AlignX, a two-stage framework. Stage 1 uses prompt-injected fine-tuning to extract axis-specific task features, mitigating catastrophic forgetting. Stage 2 deploys a MoCaE module that calibrates expert routing using fractal and natural geometry, improving inference reliability. AlignX achieves significant gains on Alpaca (Helpfulness), BeaverTails (Harmlessness), and TruthfulQA (Honesty), with +171.5% win rate, +110.1% in truthfulness-informativeness, and 4.3% fewer safety violations. It also reduces latency and memory usage by over 35% compared to prior MoEs. Results across four LLMs validate its generalizability.

</details>


### [185] [Measuring cross-language intelligibility between Romance languages with computational tools](https://arxiv.org/abs/2602.07447)
*Liviu P Dinu,Ana Sabina Uban,Bogdan Iordache,Anca Dinu,Simona Georgescu*

Main category: cs.CL

TL;DR: 该研究提出了一种基于词汇类似性和表层及语义相似性的新颖计算度量方法来估计罗曼语系语言间的可理解性，并通过对比不同形式的单词和语料库以及向量模型，对五种主要罗曼语语言（法语、意大利语、葡萄牙语、西班牙语和罗曼语）进行了测量。


<details>
  <summary>Details</summary>
Motivation: 研究旨在量化罗曼语系语言之间的可理解性，优化跨语言信息检索与翻译，以及增进对语言变异本质的理解。

Method: 该方法结合了词汇相似度的表层和语义层面，以及不同形式的单词和不同语料库的使用，评估了五种罗曼语的主要语言之间的可理解性。

Result: 研究结果显示，所得的可理解性分数在语言之间存在不对称性，并且与人类实验中的填补测试结果有显著相关性。

Conclusion: 该研究为罗玛语系语言之间的交流提供了新的定量分析方法，为语言学、人机交互等领域提供了新的视角。

Abstract: We present an analysis of mutual intelligibility in related languages applied for languages in the Romance family. We introduce a novel computational metric for estimating intelligibility based on lexical similarity using surface and semantic similarity of related words, and use it to measure mutual intelligibility for the five main Romance languages (French, Italian, Portuguese, Spanish, and Romanian), and compare results using both the orthographic and phonetic forms of words as well as different parallel corpora and vectorial models of word meaning representation. The obtained intelligibility scores confirm intuitions related to intelligibility asymmetry across languages and significantly correlate with results of cloze tests in human experiments.

</details>


### [186] [SED-SFT: Selectively Encouraging Diversity in Supervised Fine-Tuning](https://arxiv.org/abs/2602.07464)
*Yijie Chen,Yijin Liu,Fandong Meng*

Main category: cs.CL

TL;DR: SED-SFT通过引入选择性熵正则化项和选择性掩码机制，显著提高了生成多样性，改善了后续RL的性能。


<details>
  <summary>Details</summary>
Motivation: SFT过程中模式塌陷导致的多样性和准确性不平衡问题促使研究者提出新的方法。

Method: 提出了一种SED-SFT框架，引入了选择性熵正则化及选择性掩码机制，优化了SFT过程。

Result: SED-SFT在八个数学基准测试中提高了生成多样性，且计算开销较小，优于传统CE损失方法。

Conclusion: SED-SFT能够有效解决SFT模式塌陷问题，为LLMs的高效训练提供了一种新路径。

Abstract: Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL) has emerged as the standard post-training paradigm for large language models (LLMs). However, the conventional SFT process, driven by Cross-Entropy (CE) loss, often induces mode collapse, where models over-concentrate on specific response patterns. This lack of distributional diversity severely restricts the exploration efficiency required for subsequent RL. While recent studies have attempted to improve SFT by replacing the CE loss, aiming to preserve diversity or refine the update policy, they fail to adequately balance diversity and accuracy, thereby yielding suboptimal performance after RL. To address the mode collapse problem, we propose SED-SFT, which adaptively encourages diversity based on the token exploration space. This framework introduces a selective entropy regularization term with a selective masking mechanism into the optimization objective. Extensive experiments across eight mathematical benchmarks demonstrate that SED-SFT significantly enhances generation diversity with a negligible computational overhead increase compared with CE loss, yielding average improvements of 2.06 and 1.20 points in subsequent RL performance over standard CE-based baselines on Llama-3.2-3B-Instruct and Qwen2.5-Math-7B-Instruct, respectively. The code is publicly available at https://github.com/pppa2019/SED-SFT

</details>


### [187] [From Native Memes to Global Moderation: Cros-Cultural Evaluation of Vision-Language Models for Hateful Meme Detection](https://arxiv.org/abs/2602.07497)
*Mo Wang,Kaixuan Ren,Pratik Jalan,Ahmed Ashraf,Tuong Vy Vu,Rahul Seetharaman,Shah Nawaz,Usman Naseem*

Main category: cs.CL

TL;DR: 本文提出了一种系统评估框架，用于诊断和量化先进视觉语言模型在多语言 meme 数据集中的跨文化鲁棒性，发现原语言提示和单次学习策略能显著提升有害 meme 的检测性能。


<details>
  <summary>Details</summary>
Motivation: 由于现存的视觉语言模型(VLMs)多基于西方或英语视角进行训练，这限制了其在多文化环境下的公平性和鲁棒性。鉴于在线内容的解读受到文化背景的巨大影响，因此需要开发一种评估框架来诊断和量化不同文化背景下的视觉语言模型表现。

Method: 本文通过分析三个维度（学习策略、提示语言和翻译影响）来系统地评估最先进的视觉语言模型在多语言 meme 数据集中的表现。研究采用了实验分析法。

Result: 实验证明，常用的“翻译后检测”方法会导致性能下降，而采用文化对齐的干预措施（如使用原语言提示和单次学习）可以显著提升有害 meme 的检测效果。

Conclusion: 研究揭示了视觉语言模型在跨文化环境下的系统性偏见，并提出了一些减少这种偏见的实际策略，旨在引导设计全球化鲁棒的多模态管理系统。

Abstract: Cultural context profoundly shapes how people interpret online content, yet vision-language models (VLMs) remain predominantly trained through Western or English-centric lenses. This limits their fairness and cross-cultural robustness in tasks like hateful meme detection. We introduce a systematic evaluation framework designed to diagnose and quantify the cross-cultural robustness of state-of-the-art VLMs across multilingual meme datasets, analyzing three axes: (i) learning strategy (zero-shot vs. one-shot), (ii) prompting language (native vs. English), and (iii) translation effects on meaning and detection. Results show that the common ``translate-then-detect'' approach deteriorate performance, while culturally aligned interventions - native-language prompting and one-shot learning - significantly enhance detection. Our findings reveal systematic convergence toward Western safety norms and provide actionable strategies to mitigate such bias, guiding the design of globally robust multimodal moderation systems.

</details>


### [188] [Let's Simplify Step by Step: Guiding LLM Towards Multilingual Unsupervised Proficiency-Controlled Sentence Simplification](https://arxiv.org/abs/2602.07499)
*Jingshen Zhang,Xin Ying Qiu,Lifang Lu,Zhuhua Huang,Yutao Hu,Yuechang Wu,JunYu Lu*

Main category: cs.CL

TL;DR: 本文提出了一种框架，通过动态路径规划、语义敏锐的范例选择和基于对话历史的逐步思维生成来分解复杂的简化任务，进而提高简化效果并减少计算步骤。然而，在大规模简化过程中保持语义忠实度仍然是一个挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型在句法简化方面表现有限，特别是在涉及较大可读性差异时。本文旨在通过分解复杂简化为可管理的步骤，来提高简化效果。

Method: 本文提出的方法包括动态路径规划、语义敏感的范例选择以及使用对话历史进行逐步思考生成，旨在提高简化任务的控制性和有效性。

Result: 在两个基准测试中的五种语言评估显示，本文方法在简化效果上有所提升，计算步骤减少22-42%。人评实验也证实了简化效果和意义保留之间的根本权衡。

Conclusion: 本文的研究显示，逐步简化可以提高控制力，但在大规模简化过程中保持语义忠实度仍是亟待解决的问题。

Abstract: Large language models demonstrate limited capability in proficiency-controlled sentence simplification, particularly when simplifying across large readability levels. We propose a framework that decomposes complex simplifications into manageable steps through dynamic path planning, semantic-aware exemplar selection, and chain-of-thought generation with conversation history for coherent reasoning. Evaluation on five languages across two benchmarks shows our approach improves simplification effectiveness while reducing computational steps by 22-42%. Human evaluation confirms the fundamental trade-off between simplification effectiveness and meaning preservation. Notably, even human annotators struggle to agree on semantic preservation judgments, highlighting the inherent complexity of this task. Our work shows that while step-by-step simplification improves control, preserving semantic fidelity during extensive simplification remains an open challenge.

</details>


### [189] [Improving Variable-Length Generation in Diffusion Language Models via Length Regularization](https://arxiv.org/abs/2602.07546)
*Zicong Cheng,Ruixuan Jia,Jia Li,Guo-Wei Yang,Meng-Hao Guo,Shi-Min Hu*

Main category: cs.CL

TL;DR: 该论文提出了一种LR-DLLM框架，通过引入长度正则化来解决DLLM在生成长度确定上的问题，使得生成更加可靠。实验表明，LR-DLLM在多种场景下的表现优于DreamOn。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（DLLMs）在面对不确定长度的生成任务时存在根本性问题，无法可靠地确定生成长度，导致生成结果可能不足或冗余。作者希望通过提升长度估计的准确性，来改进变长度生成任务的表现。

Method: 作者提出了LR-DLLM框架，通过引入长度正则化来降低生成长度估计中的偏差。并且该方法能够独立控制生成的含义兼容性与长度诱导的不确定性。

Result: 在实验中，LR-DLLM在HumanEvalInfilling任务中取得了51.3%的Pass@1得分，相较于DreamOn提升了13.4%；在跨语言的MCEval任务中，平均Pass@1得分为51.5%，相比DreamOn提升了14.3%。

Conclusion: LR-DLLM提供了一种有效的方法来改善大型语言模型在变长度生成任务中的表现，通过修正生成长度估计中的偏见，实现可靠的生成长度确定。

Abstract: Diffusion Large Language Models (DLLMs) are inherently ill-suited for variable-length generation, as their inference is defined on a fixed-length canvas and implicitly assumes a known target length. When the length is unknown, as in realistic completion and infilling, naively comparing confidence across mask lengths becomes systematically biased, leading to under-generation or redundant continuations. In this paper, we show that this failure arises from an intrinsic lengthinduced bias in generation confidence estimates, leaving existing DLLMs without a robust way to determine generation length and making variablelength inference unreliable. To address this issue, we propose LR-DLLM, a length-regularized inference framework for DLLMs that treats generation length as an explicit variable and achieves reliable length determination at inference time. It decouples semantic compatibility from lengthinduced uncertainty through an explicit length regularization that corrects biased confidence estimates. Based on this, LR-DLLM enables dynamic expansion or contraction of the generation span without modifying the underlying DLLM or its training procedure. Experiments show that LRDLLM achieves 51.3% Pass@1 on HumanEvalInfilling under fully unknown lengths (+13.4% vs. DreamOn) and 51.5% average Pass@1 on four-language McEval (+14.3% vs. DreamOn).

</details>


### [190] [Learning to Self-Verify Makes Language Models Better Reasoners](https://arxiv.org/abs/2602.07594)
*Yuxin Chen,Yu Wang,Yi Zhang,Ziang Ye,Zhengzhou Cai,Yaorui Shi,Qi Gu,Hui Su,Xunliang Cai,Xiang Wang,An Zhang,Tat-Seng Chua*

Main category: cs.CL

TL;DR: 研究发现，尽管大型语言模型在生成方面表现出色，但在自我验证方面仍然较弱。通过对训练过程的深入调查，研究者发现，虽然提高生成能力并不直接提升自我验证能力，但强化自我验证能力可以有效改进生成性能，从而提出了一种生成与自我验证结合的多任务强化学习框架。


<details>
  <summary>Details</summary>
Motivation: 针对大型语言模型在生成与自我验证之间的能力不对称问题，本文旨在探索解决这一问题的方法。

Method: 通过对训练过程进行深入调查，作者发现生成与自我验证之间的不对称性。基于此观察，他们提出了一种结合生成与自我验证的多任务强化学习框架。

Result: 实验结果表明，相较于仅进行生成训练，结合生成与自我验证的训练框架在生成和验证能力上均有所提升。

Conclusion: 这种结合生成与自我验证的方法能够提高模型的性能，为解决大语言模型生成与自我验证能力不对称问题提供了一种有效途径。

Abstract: Recent large language models (LLMs) achieve strong performance in generating promising reasoning paths for complex tasks. However, despite powerful generation ability, LLMs remain weak at verifying their own answers, revealing a persistent capability asymmetry between generation and self-verification. In this work, we conduct an in-depth investigation of this asymmetry throughout training evolution and show that, even on the same task, improving generation does not lead to corresponding improvements in self-verification. Interestingly, we find that the reverse direction of this asymmetry behaves differently: learning to self-verify can effectively improve generation performance, achieving accuracy comparable to standard generation training while yielding more efficient and effective reasoning traces. Building on this observation, we further explore integrating self-verification into generation training by formulating a multi-task reinforcement learning framework, where generation and self-verification are optimized as two independent but complementary objectives. Extensive experiments across benchmarks and models demonstrate performance gains over generation-only training in both generation and verification capabilities.

</details>


### [191] [Letting Tutor Personas "Speak Up" for LLMs: Learning Steering Vectors from Dialogue via Preference Optimization](https://arxiv.org/abs/2602.07639)
*Jaewook Lee,Alexander Scarlatos,Simon Woodhead,Andrew Lan*

Main category: cs.CL

TL;DR: 该论文通过修改Bidirectional Preference Optimization (BiPO)方法，引入了一个引导向量来引导大语言模型的行为，使其更加接近特定的人类导师风格，从而提高了对话内容与真实教学语句的语义对齐，并在偏好评估中表现更好。


<details>
  <summary>Details</summary>
Motivation: 当前的大语言模型生成内容时缺乏多样性，而实际的师生对话中，教师会根据学生的需求调整教学策略。本文旨在通过嵌入教学人员角色来改进模型的行为，从而提高教学效果。

Method: 该论文采用了一种修改后的Bidirectional Preference Optimization (BiPO)方法，学习一个引导向量来引导模型生成符合特定导师风格的回答。这个引导向量可以帮助模型在不同的对话背景下更好地表现出特定导师的特点。

Result: 实验结果显示，通过引导向量的引导，大语言模型生成的内容能够更好地与真实导师的教学语句对齐，并提高了偏好评价。同时，这种引导方法也保持了生成内容中词汇层面的相似性，具有良好的可解释性。

Conclusion: 本研究证明了使用激活引导方法可以有效地控制大语言模型在不同对话背景下的生成内容，使其更符合特定导师的教学风格，并展示了这种方法在教学应用中的潜力。

Abstract: With the emergence of large language models (LLMs) as a powerful class of generative artificial intelligence (AI), their use in tutoring has become increasingly prominent. Prior works on LLM-based tutoring typically learn a single tutor policy and do not capture the diversity of tutoring styles. In real-world tutor-student interactions, pedagogical intent is realized through adaptive instructional strategies, with tutors varying the level of scaffolding, instructional directiveness, feedback, and affective support in response to learners' needs. These differences can all impact dialogue dynamics and student engagement. In this paper, we explore how tutor personas embedded in human tutor-student dialogues can be used to guide LLM behavior without relying on explicitly prompted instructions. We modify Bidirectional Preference Optimization (BiPO) to learn a steering vector, an activation-space direction that steers model responses towards certain tutor personas. We find that this steering vector captures tutor-specific variation across dialogue contexts, improving semantic alignment with ground-truth tutor utterances and increasing preference-based evaluations, while largely preserving lexical similarity. Analysis of the learned directional coefficients further reveals interpretable structure across tutors, corresponding to consistent differences in tutoring behavior. These results demonstrate that activation steering offers an effective and interpretable way for controlling tutor-specific variation in LLMs using signals derived directly from human dialogue data.

</details>


### [192] [Blind to the Human Touch: Overlap Bias in LLM-Based Summary Evaluation](https://arxiv.org/abs/2602.07673)
*Jiangnan Fang,Cheng-Tse Liu,Hanieh Deilamsalehy,Nesreen K. Ahmed,Puneet Mathur,Nedim Lipka,Franck Dernoncourt,Ryan A. Rossi*

Main category: cs.CL

TL;DR: 该研究分析了大型语言模型（LLM）作为评判者在摘要任务中的偏差，发现LLM更倾向于其他LLM生成的摘要而非人类撰写的摘要，这种偏好随着与参考摘要的相似度降低而增加。研究还表明，即使对于相似性较低的摘要，LLM也难以做出准确判断。


<details>
  <summary>Details</summary>
Motivation: 研究关注现有LLM在执行人类与机器生成文本相似度判断任务时的偏见和局限性，尤其是在文本摘要这一领域。

Method: 研究测试了9种参数量从1亿到12亿的近期LLM，评估了它们与人工撰写的参考摘要之间的重叠度，并通过ROUGE和BLEU等重叠度指标进行衡量。

Result: 研究结果显示，当模型间摘要的相似性下降时，LLM更倾向于选择由其它LLM生成的摘要而非人工撰写的摘要；且所有测试的模型均表现出这种偏好，仅有一个模型略有不同。此外，研究发现模型难以评估低重叠度的摘要，这表明在摘要领域使用LLM作为评判者需要依赖更复杂的技术。

Conclusion: 研究认为，未来的改进可能依赖于更复杂的评估技术和策略，以尽可能减少LLM作为评判者在摘领域中的偏见。

Abstract: Large language model (LLM) judges have often been used alongside traditional, algorithm-based metrics for tasks like summarization because they better capture semantic information, are better at reasoning, and are more robust to paraphrasing. However, LLM judges show biases for length and order among others, and are vulnerable to various adversarial input prompts. While recent studies have looked into these biases, few have analyzed them at a more granular level in relation to a well-defined overlap metric. In this work we provide an LLM judge bias analysis as a function of overlap with human-written responses in the domain of summarization. We test 9 recent LLMs with parameter counts ranging from 1 billion to 12 billion, including variants of Gemma 3 and LLaMA 3. We find that LLM judges increasingly prefer summaries generated by other LLMs over those written by humans as the similarities (as measured by ROUGE and BLEU) between the judged summaries decrease, and this pattern extends to all but one model tested, and exists regardless of the models' own position biases. Additionally, we find that models struggle to judge even summaries with limited overlaps, suggesting that LLM-as-a-judge in the summary domain should rely on techniques beyond a simple comparison.

</details>


### [193] [Attn-GS: Attention-Guided Context Compression for Efficient Personalized LLMs](https://arxiv.org/abs/2602.07778)
*Shenglai Zeng,Tianqi Zheng,Chuan Tian,Dante Everaert,Yau-Shian Wang,Yupin Huang,Michael J. Morais,Rohit Patki,Jinjin Tian,Xinnan Dai,Kai Guo,Monica Xiao Cheng,Hui Liu*

Main category: cs.CL

TL;DR: 提出了一种名为Attn-GS的注意力引导上下文压缩框架，通过标记模型的注意力反馈识别重要个人化信号，指导压缩模型生成任务相关的高质量压缩用户上下文。实验表明，Attn-GS在各种任务、标记限制和设置下显著优于基线，与使用完整上下文相比，减少了50倍的标记使用。


<details>
  <summary>Details</summary>
Motivation: 现有的个性化方法依赖于启发式策略，这些方法将上下文视为一个整体，无法考虑LLMs内部处理个人资料组件的方式。本文提出利用LLMs的注意力模式来识别重要个人化信号，从而改进上下文压缩。

Method: Attn-GS框架使用标记模型的注意力反馈来标记重要的个人化句子，然后指导压缩模型生成相关任务的高质量压缩上下文。

Result: 广泛的实验表明，Attn-GS在不同任务、标记限制和配置下的表现优于多种基线方法，性能接近使用完整上下文，但标记使用减少达50倍。

Conclusion: Attn-GS框架通过有效利用LLMs的注意力模式，实现了高质量的上下文压缩，为个性化语言模型提供了有前景的方法。

Abstract: Personalizing large language models (LLMs) to individual users requires incorporating extensive interaction histories and profiles, but input token constraints make this impractical due to high inference latency and API costs. Existing approaches rely on heuristic methods such as selecting recent interactions or prompting summarization models to compress user profiles. However, these methods treat context as a monolithic whole and fail to consider how LLMs internally process and prioritize different profile components. We investigate whether LLMs' attention patterns can effectively identify important personalization signals for intelligent context compression. Through preliminary studies on representative personalization tasks, we discover that (a) LLMs' attention patterns naturally reveal important signals, and (b) fine-tuning enhances LLMs' ability to distinguish between relevant and irrelevant information. Based on these insights, we propose Attn-GS, an attention-guided context compression framework that leverages attention feedback from a marking model to mark important personalization sentences, then guides a compression model to generate task-relevant, high-quality compressed user contexts. Extensive experiments demonstrate that Attn-GS significantly outperforms various baselines across different tasks, token limits, and settings, achieving performance close to using full context while reducing token usage by 50 times.

</details>


### [194] [Emergent Structured Representations Support Flexible In-Context Inference in Large Language Models](https://arxiv.org/abs/2602.07794)
*Ningyu Xu,Qi Zhang,Xipeng Qiu,Xuanjing Huang*

Main category: cs.CL

TL;DR: 本研究通过因果中介分析表明，大型语言模型在上下文概念推理过程中动态构建并使用结构化的潜在表示，提供了模型灵活适应背后计算过程的洞见。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型在上下文推理过程中是否依赖结构化的、类人的人类概念表示来进行推理。

Method: 通过因果中介分析，研究大型语言模型中间到后期层的内部处理，分析上下文概念推理中概念子空间的作用。

Result: 发现概念子空间在各上下文之间存在稳定结构，且被证明在模型预测中具有功能性中心作用; 早期到中期层的注意力头整合上下文线索构建并精炼子空间，由后期层利用这个子空间进行预测。

Conclusion: 研究表明，大型语言模型在推理过程中能够动态构造和使用结构化的潜在表示，这些结果为理解模型灵活适应背后的计算过程提供了见解。

Abstract: Large language models (LLMs) exhibit emergent behaviors suggestive of human-like reasoning. While recent work has identified structured, human-like conceptual representations within these models, it remains unclear whether they functionally rely on such representations for reasoning. Here we investigate the internal processing of LLMs during in-context concept inference. Our results reveal a conceptual subspace emerging in middle to late layers, whose representational structure persists across contexts. Using causal mediation analyses, we demonstrate that this subspace is not merely an epiphenomenon but is functionally central to model predictions, establishing its causal role in inference. We further identify a layer-wise progression where attention heads in early-to-middle layers integrate contextual cues to construct and refine the subspace, which is subsequently leveraged by later layers to generate predictions. Together, these findings provide evidence that LLMs dynamically construct and use structured, latent representations in context for inference, offering insights into the computational processes underlying flexible adaptation.

</details>


### [195] [LLMs Know More About Numbers than They Can Say](https://arxiv.org/abs/2602.07812)
*Fengting Yuchi,Li Du,Jason Eisner*

Main category: cs.CL

TL;DR: 尽管最先进的语言模型（LLMs）能够解决数学问题，但它们在带有混合表示的数值比较中存在错误。通过探查小型开源LLMs的隐藏状态，发现一种简单的线性转换可以编码不同类型的数值的对数尺度，并有助于恢复数值。在对比数值对时，虽然隐藏状态能够准确编码排序信息，但模型对外部比较任务的表现并不总是与其内部表示的一致。


<details>
  <summary>Details</summary>
Motivation: 为了评估当前最先进的LLMs在数值理解和推理方面的局限性，研究者通过实验评估了这些模型处理具有混合表示的数值比较的能力。

Method: 研究者对几种小型开源LLMs进行检测，尝试通过线性投影隐藏层来捕捉数值的尺度信息，并测试了模型在基准合成文本和科学论文上的表现。

Result: 实验结果显示，简单的线性投影能够有效地编码数值的尺度信息，使得相对误差在2.3%左右。在对数值对进行编码后，模型能够以超过90%的准确度判断其大小关系。然而，在实际的数值对比任务中，模型的准确度仅为50-70%，并且表现与内部表示的有效性有关。

Conclusion: 通过将分类器探针的对数损失作为辅助目标进行调优，LLMs的数值推理能力得到了进一步的提升。这表明，改善内部尺度表示能够提高模型的数值推理能力。

Abstract: Although state-of-the-art LLMs can solve math problems, we find that they make errors on numerical comparisons with mixed notation: "Which is larger, $5.7 \times 10^2$ or $580$?" This raises a fundamental question: Do LLMs even know how big these numbers are? We probe the hidden states of several smaller open-source LLMs. A single linear projection of an appropriate hidden layer encodes the log-magnitudes of both kinds of numerals, allowing us to recover the numbers with relative error of about 2.3% (on restricted synthetic text) or 19.06% (on scientific papers). Furthermore, the hidden state after reading a pair of numerals encodes their ranking, with a linear classifier achieving over 90% accuracy. Yet surprisingly, when explicitly asked to rank the same pairs of numerals, these LLMs achieve only 50-70% accuracy, with worse performance for models whose probes are less effective. Finally, we show that incorporating the classifier probe's log-loss as an auxiliary objective during finetuning brings an additional 3.22% improvement in verbalized accuracy over base models, demonstrating that improving models' internal magnitude representations can enhance their numerical reasoning capabilities.

</details>


### [196] [Evaluating and Calibrating LLM Confidence on Questions with Multiple Correct Answers](https://arxiv.org/abs/2602.07842)
*Yuhan Wang,Shiyu Ni,Zhikai Ding,Zihang Zhan,Yuanzi Li,Keping Bi*

Main category: cs.CL

TL;DR: 该研究指出现有不依赖训练的方法在多答案问题上表现不佳，提出了Semantic Confidence Aggregation (SCA) 方法以提高多答案场景下的校准性能。


<details>
  <summary>Details</summary>
Motivation: 现有的训练前校准方法主要在单一答案的问答任务上研究，但在多答案场景中表现不佳。为了解决这一问题，该研究提出了一个新的基准测试MACE，并设计了一个新的方法SCA来改进校准。

Method: 该研究首先构建了一个包含12000个问题的数据集MACE，覆盖六个领域，具有不同的正确答案数量。然后评估了15种代表性校准方法和四种大型语言模型家族（7B-72B）的表现，发现准确率随答案数量增加而提高，但估计的置信度却持续下降。为此，研究提出了一种新的方法，称为Semantic Confidence Aggregation (SCA)，该方法通过聚合多个高概率采样响应的置信度来解决多答案场景下的校准问题。

Result: 在各种问答任务中，SCA方法在多答案场景下的校准性能达到了最新状态，同时在单一答案的问题上保持了良好的校准。

Conclusion: 该研究通过新构建的数据集MACE和新提出的SCA方法，解决了多答案场景下的置信度校准问题，提高了大型语言模型的可靠性。

Abstract: Confidence calibration is essential for making large language models (LLMs) reliable, yet existing training-free methods have been primarily studied under single-answer question answering. In this paper, we show that these methods break down in the presence of multiple valid answers, where disagreement among equally correct responses leads to systematic underestimation of confidence. To enable a systematic study of this phenomenon, we introduce MACE, a benchmark of 12,000 factual questions spanning six domains with varying numbers of correct answers. Experiments across 15 representative calibration methods and four LLM families (7B-72B) reveal that while accuracy increases with answer cardinality, estimated confidence consistently decreases, causing severe miscalibration for questions with mixed answer counts. To address this issue, we propose Semantic Confidence Aggregation (SCA), which aggregates confidence over multiple high-probability sampled responses. SCA achieves state-of-the-art calibration performance under mixed-answer settings while preserving strong calibration on single-answer questions.

</details>


### [197] [SparseEval: Efficient Evaluation of Large Language Models by Sparse Optimization](https://arxiv.org/abs/2602.07909)
*Taolin Zhang,Hang Guo,Wang Lu,Tao Dai,Shu-Tao Xia,Jindong Wang*

Main category: cs.CL

TL;DR: 该研究提出了SparseEval方法，通过稀疏优化和迭代锚点选择策略，有效降低了大规模语言模型在多种下游任务上的评估成本。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型的不断扩大，评估其性能变得越来越昂贵。为此，研究者重新审视了模型-任务性能矩阵，发现了其稀疏性，并提出了SparseEval方法来有效进行模型评估。

Method: SparseEval方法利用多层感知机处理稀疏优化问题，采用梯度下降优化锚点权重，并采用迭代优化策略选择锚点。同时引入了锚点重要性评分和候选评分来评估项目在任务中的重要性。

Result: 实验结果表明，SparseEval方法在多种基准测试中的误差较低，Kendall's τ值较高，具有更强的鲁棒性和实用性。

Conclusion: SparseEval方法在实际应用中能够降低对大规模语言模型评估的成本，提高评估效率，展示了其在实际场景中的优越性。

Abstract: As large language models (LLMs) continue to scale up, their performance on various downstream tasks has significantly improved. However, evaluating their capabilities has become increasingly expensive, as performing inference on a large number of benchmark samples incurs high computational costs. In this paper, we revisit the model-item performance matrix and show that it exhibits sparsity, that representative items can be selected as anchors, and that the task of efficient benchmarking can be formulated as a sparse optimization problem. Based on these insights, we propose SparseEval, a method that, for the first time, adopts gradient descent to optimize anchor weights and employs an iterative refinement strategy for anchor selection. We utilize the representation capacity of MLP to handle sparse optimization and propose the Anchor Importance Score and Candidate Importance Score to evaluate the value of each item for task-aware refinement. Extensive experiments demonstrate the low estimation error and high Kendall's~$τ$ of our method across a variety of benchmarks, showcasing its superior robustness and practicality in real-world scenarios. Code is available at {https://github.com/taolinzhang/SparseEval}.

</details>


### [198] [Patches of Nonlinearity: Instruction Vectors in Large Language Models](https://arxiv.org/abs/2602.07930)
*Irina Bigoulaeva,Jonas Rohweder,Subhabrata Dutta,Iryna Gurevych*

Main category: cs.CL

TL;DR: 研究通过因果中介分析表明，指令表示在模型中具有局部性，并且作为电路选择器发挥作用，挑战了线性表示假设。


<details>
  <summary>Details</summary>
Motivation: 鉴于指令调优语言模型被广泛使用但内部处理机制了解不足，该研究旨在探索指令表示在模型不同阶段的构造与应用。

Method: 通过因果中介分析确定指令表示的局部性，并提出一种新型方法来定位信息处理，这种方法不带有基于补丁技术的隐式线性假设。

Result: 研究发现，指令向量（IVs）在早期层已形成的任务表示基础上，选择不同的信息路径以解决给定任务，从而挑战了线性表示假设。

Conclusion: 该研究揭示了指令处理的非线性因果交互，并提出了一种新的局部化信息处理方法，为理解语言模型的内部处理机制提供了新视角。

Abstract: Despite the recent success of instruction-tuned language models and their ubiquitous usage, very little is known of how models process instructions internally. In this work, we address this gap from a mechanistic point of view by investigating how instruction-specific representations are constructed and utilized in different stages of post-training: Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO). Via causal mediation, we identify that instruction representation is fairly localized in models. These representations, which we call Instruction Vectors (IVs), demonstrate a curious juxtaposition of linear separability along with non-linear causal interaction, broadly questioning the scope of the linear representation hypothesis commonplace in mechanistic interpretability. To disentangle the non-linear causal interaction, we propose a novel method to localize information processing in language models that is free from the implicit linear assumptions of patching-based techniques. We find that, conditioned on the task representations formed in the early layers, different information pathways are selected in the later layers to solve that task, i.e., IVs act as circuit selectors.

</details>


### [199] [Bielik Guard: Efficient Polish Language Safety Classifiers for LLM Content Moderation](https://arxiv.org/abs/2602.07954)
*Krzysztof Wróbel,Jan Maria Kowalski,Jerzy Surma,Igor Ciuciura,Maciej Szymański*

Main category: cs.CL

TL;DR: Bielik Guard 是一个针对波兰语内容安全分类的模型系列，包含两种不同规模的变体。该系列模型在多个基准测试中表现出色，特别是在应对敏感内容时精度高且误报率低。


<details>
  <summary>Details</summary>
Motivation: 鉴于大型语言模型（LLMs）在波兰语言应用程序中的广泛应用，迫切需要高效且准确的内容安全分类工具。

Method: Bielik Guard 通过在包含 6,885 条波兰语文本的社区标注数据集上进行微调，分类内容为五类：仇恨/攻击、脏话、色情内容、犯罪和自伤。使用 MMLW-RoBERTa-base 和 PKOBP/polish-roberta-8k 作为基础模型架构，构建了 0.1B 参数和 0.5B 参数的模型。

Result: 这两种模型在多种基准测试中表现出强大的性能。0.5B 变体在测试集上的 F1 微观和宏观分数分别为 0.791 和 0.785。特别指出的是，Bielik Guard 0.1B v1.1 在真实用户提示上的精度为 77.65%，误报率为 0.63%，优于 HerBERT-PL-Guard (31.55% 的精度和 4.70% 的 FPR)。

Conclusion: Bielik Guard 可以为敏感类别提供适当响应而非简单的内容阻挡。模型已经公开可用，旨在提供适宜的回应而不是简单的内容封锁，特别是对自伤等敏感类别。

Abstract: As Large Language Models (LLMs) become increasingly deployed in Polish language applications, the need for efficient and accurate content safety classifiers has become paramount. We present Bielik Guard, a family of compact Polish language safety classifiers comprising two model variants: a 0.1B parameter model based on MMLW-RoBERTa-base and a 0.5B parameter model based on PKOBP/polish-roberta-8k. Fine-tuned on a community-annotated dataset of 6,885 Polish texts, these models classify content across five safety categories: Hate/Aggression, Vulgarities, Sexual Content, Crime, and Self-Harm. Our evaluation demonstrates that both models achieve strong performance on multiple benchmarks. The 0.5B variant offers the best overall discrimination capability with F1 scores of 0.791 (micro) and 0.785 (macro) on the test set, while the 0.1B variant demonstrates exceptional efficiency. Notably, Bielik Guard 0.1B v1.1 achieves superior precision (77.65\%) and very low false positive rate (0.63\%) on real user prompts, outperforming HerBERT-PL-Guard (31.55\% precision, 4.70\% FPR) despite identical model size. The models are publicly available and designed to provide appropriate responses rather than simple content blocking, particularly for sensitive categories like self-harm.

</details>


### [200] [Lost in Translation? A Comparative Study on the Cross-Lingual Transfer of Composite Harms](https://arxiv.org/abs/2602.07963)
*Vaibhav Shukla,Hardik Sharma,Adith N Reganti,Soham Wasmatkar,Bagesh Kumar,Vrijendra Singh*

Main category: cs.CL

TL;DR: 研究发现，针对不同语言的攻击成功率和上下文危害在翻译后有显著变化。复合危害基准（CompositeHarm）旨在评估安全性随语法和语义变化时的表现。通过轻量级推理策略，研究提高了大规模多语言安全性测试的可行性和环保性。


<details>
  <summary>Details</summary>
Motivation: 现有大多数安全评估集中在英语上，翻译作为一种捷径虽然简化了跨语言评估，但往往未能捕捉到全部情况。本研究旨在开发新的基准工具，以系统性地评估LML（大型语言模型）在不同语言中的安全性变化。

Method: 引入 CompositeHarm 基准，整合两种英语数据集 AttaQ 和 MMSafetyBench，覆盖六种语言（英语、印地语、阿萨姆语、马拉地语、卡纳达语、古吉拉特语），并通过三种大型模型进行评估，同时采用边缘AI的设计原则来实现轻量级推理，减少计算资源的消耗。

Result: 研究结果显示，针对印地语的攻击成功率显著提高，尤其是在对抗性语法下；而上下文危害则更容易跨语言转移，但变化幅度较小。轻量级推理策略有效减少了冗余评价过程，同时保持跨语言一致性。

Conclusion: 翻译基准是构建多语言安全性系统的重要第一步，但不足以确保充分的适应性。需要进一步研究来构建基于实际资源和语言适应的安全系统，以实现真正全面的安全评估。

Abstract: Most safety evaluations of large language models (LLMs) remain anchored in English. Translation is often used as a shortcut to probe multilingual behavior, but it rarely captures the full picture, especially when harmful intent or structure morphs across languages. Some types of harm survive translation almost intact, while others distort or disappear. To study this effect, we introduce CompositeHarm, a translation-based benchmark designed to examine how safety alignment holds up as both syntax and semantics shift. It combines two complementary English datasets, AttaQ, which targets structured adversarial attacks, and MMSafetyBench, which covers contextual, real-world harms, and extends them into six languages: English, Hindi, Assamese, Marathi, Kannada, and Gujarati. Using three large models, we find that attack success rates rise sharply in Indic languages, especially under adversarial syntax, while contextual harms transfer more moderately. To ensure scalability and energy efficiency, our study adopts lightweight inference strategies inspired by edge-AI design principles, reducing redundant evaluation passes while preserving cross-lingual fidelity. This design makes large-scale multilingual safety testing both computationally feasible and environmentally conscious. Overall, our results show that translated benchmarks are a necessary first step, but not a sufficient one, toward building grounded, resource-aware, language-adaptive safety systems.

</details>


### [201] [Cross-Linguistic Persona-Driven Data Synthesis for Robust Multimodal Cognitive Decline Detection](https://arxiv.org/abs/2602.07978)
*Rui Feng,Zhiyao Luo,Liuyu Wu,Wei Wang,Yuting Song,Yong Liu,Kok Pin Ng,Jianqing Li,Xingyao Wang*

Main category: cs.CL

TL;DR: SynCog 提供了一种新颖的方法，通过结合可控的零样本多模态数据合成与基于 Chain-of-Thought 的微调策略，解决了认知障碍早期诊断中的临床数据稀缺性和透明度问题，展示了在多种语言和临床数据集上的优越性能。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏有效的多语言临床数据使得基于语音的认知障碍诊断模型难以广泛应用。SynCog 旨在通过生成合成数据和引入透明推理机制来解决这一问题。

Method: SynCog 采用了集成可控零样本多模态数据合成和基于 Chain-of-Thought 的微调策略的方法。首先，通过模拟不同的虚拟病患以合成多样化的数据集，提高数据多样性并缓解临床数据稀缺问题。其次，使用生成的数据集对基础多模态模型进行 CoT 微调训练，以使模型能够显式解释其诊断推理过程，提高模型的可解释性。

Result: 在 ADReSS 和 ADReSSo 标准数据集上，SynCog 的 Macro-F1 得分分别为 80.67% 和 78.46%，优于当前基准模型。在独立的真实世界 Mandarin 资料（CIR-E）上表现出色，Macro-F1 达到 48.71%，显示出良好的跨语言适应性。

Conclusion: SynCog 为全球卫生保健提供了一种可靠且语言包容的认知评估工具，显著推动了基于语音的 MCI 诊断技术的发展。

Abstract: Speech-based digital biomarkers represent a scalable, non-invasive frontier for the early identification of Mild Cognitive Impairment (MCI). However, the development of robust diagnostic models remains impeded by acute clinical data scarcity and a lack of interpretable reasoning. Current solutions frequently struggle with cross-lingual generalization and fail to provide the transparent rationales essential for clinical trust. To address these barriers, we introduce SynCog, a novel framework integrating controllable zero-shot multimodal data synthesis with Chain-of-Thought (CoT) deduction fine-tuning. Specifically, SynCog simulates diverse virtual subjects with varying cognitive profiles to effectively alleviate clinical data scarcity. This generative paradigm enables the rapid, zero-shot expansion of clinical corpora across diverse languages, effectively bypassing data bottlenecks in low-resource settings and bolstering the diagnostic performance of Multimodal Large Language Models (MLLMs). Leveraging this synthesized dataset, we fine-tune a foundational multimodal backbone using a CoT deduction strategy, empowering the model to explicitly articulate diagnostic thought processes rather than relying on black-box predictions. Extensive experiments on the ADReSS and ADReSSo benchmarks demonstrate that augmenting limited clinical data with synthetic phenotypes yields competitive diagnostic performance, achieving Macro-F1 scores of 80.67% and 78.46%, respectively, outperforming current baseline models. Furthermore, evaluation on an independent real-world Mandarin cohort (CIR-E) demonstrates robust cross-linguistic generalization, attaining a Macro-F1 of 48.71%. These findings constitute a critical step toward providing clinically trustworthy and linguistically inclusive cognitive assessment tools for global healthcare.

</details>


### [202] [The Judge Who Never Admits: Hidden Shortcuts in LLM-based Evaluation](https://arxiv.org/abs/2602.07996)
*Arash Marioriyad,Omid Ghahroodi,Ehsaneddin Asgari,Mohammad Hossein Rohban,Mahdieh Soleymani Baghshah*

Main category: cs.CL

TL;DR: 本文通过在六个模型上进行受控线索扰动，测试了这些模型作为评判者的忠实性和透明度。结果显示，即使在有强烈行为效应的线索下，模型也较少提及所注入的线索，揭示了LLM作为评判者的解释缺口，对研究与部署中的模型评估可靠性提出了质疑。


<details>
  <summary>Details</summary>
Motivation: 探究大型语言模型（LLMs）作为评判者时对无关背景因素的敏感性以及其决策透明度

Method: 使用六种不同模型，在两个数据集（ELI5和LitBench）上进行受控的验证扰动实验，考察六类特征线索（来源、时间、年龄段、性别、种族和教育背景）对模型验证决策的影响。

Result: 超半数模型在强行为效应线索下的决策敏感性高，且多数情况下未明确提及所注入的线索，表明模型在使用过程中存在大量的判断短路现象；某些特征（如权威性、近期性偏好和教育背景偏见）在真实场景中的数据集上表现更突出，但开放型数据集上模型则未表现出明显提及注入线索的倾向。

Conclusion: 研究揭示了LLM作为评判者的解释空白，质疑了基于模型的评估在研究和实际应用中的可靠性。

Abstract: Large language models (LLMs) are increasingly used as automatic judges to evaluate system outputs in tasks such as reasoning, question answering, and creative writing. A faithful judge should base its verdicts solely on content quality, remain invariant to irrelevant context, and transparently reflect the factors driving its decisions. We test this ideal via controlled cue perturbations-synthetic metadata labels injected into evaluation prompts-for six judge models: GPT-4o, Gemini-2.0-Flash, Gemma-3-27B, Qwen3-235B, Claude-3-Haiku, and Llama3-70B. Experiments span two complementary datasets with distinct evaluation regimes: ELI5 (factual QA) and LitBench (open-ended creative writing). We study six cue families: source, temporal, age, gender, ethnicity, and educational status. Beyond measuring verdict shift rates (VSR), we introduce cue acknowledgment rate (CAR) to quantify whether judges explicitly reference the injected cues in their natural-language rationales. Across cues with strong behavioral effects-e.g., provenance hierarchies (Expert > Human > LLM > Unknown), recency preferences (New > Old), and educational-status favoritism-CAR is typically at or near zero, indicating that shortcut reliance is largely unreported even when it drives decisions. Crucially, CAR is also dataset-dependent: explicit cue recognition is more likely to surface in the factual ELI5 setting for some models and cues, but often collapses in the open-ended LitBench regime, where large verdict shifts can persist despite zero acknowledgment. The combination of substantial verdict sensitivity and limited cue acknowledgment reveals an explanation gap in LLM-as-judge pipelines, raising concerns about reliability of model-based evaluation in both research and deployment.

</details>


### [203] [DeltaKV: Residual-Based KV Cache Compression via Long-Range Similarity](https://arxiv.org/abs/2602.08005)
*Jitai Hao,Qiang Huang,Yaowei Wang,Min Zhang,Jun Yu*

Main category: cs.CL

TL;DR: DeltaKV通过序列残差压缩技术来减少KV缓存的内存消耗，同时保持接近无损的准确度；而Sparse-vLLM则通过稀疏化管理和优化内核来提升推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有的压缩和驱逐方法难以在压缩比、准确性和硬件效率之间达到良好的平衡，特别是在使用长上下文LLM的应用场景中，KV缓存的内存需求呈现出线性增长，成为效率瓶颈。

Method: DeltaKV框架采用基于残差的压缩方法，通过相对历史引用编码语义残差，而不是删除令牌，从而保留了大部分准确度，并显著减少了内存使用。Sparse-vLLM则是借助于稀疏化管理和内核优化，使得其在处理稀疏和不规则 KV 布局时具有更高的性能。

Result: 实验结果显示，DeltaKV将KV缓存内存减少至原始的29%，同时在LongBench、SCBench和AIME上的准确度几乎不受影响。与vLLM相比，结合Sparse-vLLM在长上下文场景下可以获得最高2倍的吞吐量提升。

Conclusion: DeltaKV和Sparse-vLLM的结合提供了一种实用的方法，可以实现可扩展的长上下文LLM的部署。

Abstract: The deployment of efficient long-context LLMs in applications like autonomous agents, long-chain reasoning, and creative writing is fundamentally bottlenecked by the linear growth of KV cache memory. Existing compression and eviction methods often struggle to balance accuracy, compression ratio, and hardware efficiency. We propose DeltaKV, a residual-based KV cache compression framework motivated by two empirical findings: long-range inter-token similarity and highly shared latent components in KV representations. Instead of discarding tokens, DeltaKV encodes semantic residuals relative to retrieved historical references, preserving fidelity while substantially reducing storage. To translate compression gains into real system speedups, we further introduce Sparse-vLLM, a high-performance inference engine with decoupled memory management and kernels optimized for sparse and irregular KV layouts. Experiments show that DeltaKV reduces KV cache memory to 29\% of the original while maintaining near-lossless accuracy on LongBench, SCBench, and AIME. When integrated with Sparse-vLLM, it achieves up to 2$\times$ throughput improvement over vLLM in long-context scenarios, demonstrating a practical path toward scalable long-context LLM deployment. Code, model checkpoints, and datasets are available at https://github.com/CURRENTF/Sparse-vLLM.

</details>


### [204] [Diverge to Induce Prompting: Multi-Rationale Induction for Zero-Shot Reasoning](https://arxiv.org/abs/2602.08028)
*Po-Chun Chen,Hen-Hsen Huang,Hsin-Hsi Chen*

Main category: cs.CL

TL;DR: DIP框架通过生成多个高维度推理，并转化为详细步骤计划，最终整合成最终计划，来改进标准Chain-of-Thought提示的不稳定性，提高了零-shot推理准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的单一推理策略方法在应对多样化任务时表现受限，因此提出了DIP框架，旨在解决标准Chain-of-Thought提示的不稳定性问题。

Method: 首先，框架提示LLM生成多个多样化的高维度推理；其次，将每个推理细化为详细的步骤草案；最后，这些草案计划被整合成一个最终方案。

Result: 实验结果表明，DIP框架在零-shot推理方面超越了依赖资源密集型采样的单策略提示方法，展示了多元推理整合在提示基础推理中的有效性。

Conclusion: DIP框架通过生成多元推理并转化为详细步骤计划，提高了零-shot推理的准确性，展示了一种有效的解决不稳定性提示的手段。

Abstract: To address the instability of unguided reasoning paths in standard Chain-of-Thought prompting, recent methods guide large language models (LLMs) by first eliciting a single reasoning strategy. However, relying on just one strategy for each question can still limit performance across diverse tasks. We propose Diverge-to-Induce Prompting (DIP), a framework that first prompts an LLM to generate multiple diverse high-level rationales for each question. Each rationale is then elaborated into a detailed, step-by-step draft plan. Finally, these draft plans are induced into a final plan. DIP enhances zero-shot reasoning accuracy without reliance on resource-intensive sampling. Experiments show that DIP outperforms single-strategy prompting, demonstrating the effectiveness of multi-plan induction for prompt-based reasoning.

</details>


### [205] [Beyond Raw Detection Scores: Markov-Informed Calibration for Boosting Machine-Generated Text Detection](https://arxiv.org/abs/2602.08031)
*Chenwang Wu,Yiu-ming Cheung,Shuhai Zhang,Bo Han,Defu Lian*

Main category: cs.CL

TL;DR: 本文介绍了针对机器生成文本（MGTs）的方法学，通过统一框架评估多种指标法，并提出了一种基于马尔可夫随机场的校准策略，有效提升了检测效果，且计算开销小。


<details>
  <summary>Details</summary>
Motivation: 随着MGTs的广泛应用，识别其潜在风险如虚假信息和钓鱼攻击变得尤为重要，因此需要一种可靠的检测方法。现有的指标法虽然简单且不易过拟合，但面临一个显著挑战：MGTs生成过程中的固有随机性影响了token级别的检测分数。本文旨在解决这一问题，提升MGTs的检测性能。

Method: 1. 统一框架评估指标法以明确其优点和局限性；
2. 理论和实验揭示两种关系：邻近相似性和初始不稳定性；
3. 提出一种基于马尔可夫随机场的校准策略，利用马尔可夫随机场建模，并通过均值场近似实现，作为轻量级组件无缝集成现有检测器。

Result: 针对MGTs的不同攻击场景（如跨模型和重写攻击）进行了广泛实验，展示了与基线相比显著的性能提升，且计算开销小。

Conclusion: 所提出的校准策略能够有效应对MGTs固有的随机性问题，显著提升检测效果，并且易于集成到现有检测系统中。

Abstract: While machine-generated texts (MGTs) offer great convenience, they also pose risks such as disinformation and phishing, highlighting the need for reliable detection. Metric-based methods, which extract statistically distinguishable features of MGTs, are often more practical than complex model-based methods that are prone to overfitting. Given their diverse designs, we first place representative metric-based methods within a unified framework, enabling a clear assessment of their advantages and limitations. Our analysis identifies a core challenge across these methods: the token-level detection score is easily biased by the inherent randomness of the MGTs generation process. To address this, we theoretically and empirically reveal two relationships of context detection scores that may aid calibration: Neighbor Similarity and Initial Instability. We then propose a Markov-informed score calibration strategy that models these relationships using Markov random fields, and implements it as a lightweight component via a mean-field approximation, allowing our method to be seamlessly integrated into existing detectors. Extensive experiments in various real-world scenarios, such as cross-LLM and paraphrasing attacks, demonstrate significant gains over baselines with negligible computational overhead. The code is available at https://github.com/tmlr-group/MRF_Calibration.

</details>


### [206] [Emergent Search and Backtracking in Latent Reasoning Models](https://arxiv.org/abs/2602.08100)
*Jasmine Cui,Charles Ye*

Main category: cs.CL

TL;DR: 该研究探讨了一个在连续隐藏空间中进行逻辑推理的模型，发现在问答任务中，该模型自发地具有了一种结构化的搜索过程。推理过程包括探索阶段、初步承诺阶段、以及可能的回溯。回溯是一种普遍存在的现象，并且有利于模型获得准确答案，特别是在使用不合理的替代项替换 distractors 的情况下，减短了探索时间。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型（LLMs）在逻辑推理过程中通常会通过言语逐步讨论中间步骤。然而，为了探索隐空间推理的新方法，作者们研究了一个名为LRT（Latent Reasoning Transformers）的模型，其能够在不进行言语化推理的情况下完成逻辑推理任务。

Method: 作者们在多项选择的问答基准测试上解码了该模型的每一步推理过程，以观察其在隐空间中的推理方法，包括探索、初步承诺、回溯等阶段的过程，并且通过对比有回溯和无回溯的结果来了解回溯的作用。

Result: 实验结果显示，LRT 确实自发地形成了一种结构化的搜索路径，推理具有探索阶段、初步承诺以及回溯等阶段。更重要的是，模型回溯现象普遍存在且有益，32% 的情况下会发生回溯，这提升了34% 的准确性。此外，通过用不可能的答案替换误导性的选项能缩短探索时间，达到 54% 的效率。

Conclusion: 研究指出，隐空间推理具备与言语化推理相似的能力，即能够犯错、意识到错误并在正确答案中回弹。这种方法在特定情况下起到缩短探索过程和提高准确性的积极作用。

Abstract: What happens when a language model thinks without words? Standard reasoning LLMs verbalize intermediate steps as chain-of-thought; latent reasoning transformers (LRTs) instead perform deliberation entirely in continuous hidden space. We investigate an LRT, decoding the model's evolving beliefs at every step on a multiple-choice QA benchmark. We find that the model spontaneously learns a structured search process in latent space. Deliberation follows a consistent trajectory: an exploration phase where probability mass spreads across candidates, tentative commitment to a frontrunner, and either convergence or backtracking. Backtracking is prevalent (32% of instances), beneficial (34% accuracy gain over non-backtracking instances), and predominantly directed away from the semantically closest distractor toward the correct answer. The search is adaptive: replacing distractors with implausible alternatives shortens exploration by 54%. Latent reasoning models achieve in activation space what chain-of-thought achieves through words: the ability to be wrong, notice, and recover.

</details>


### [207] [Gender and Race Bias in Consumer Product Recommendations by Large Language Models](https://arxiv.org/abs/2602.08124)
*Ke Xu,Shera Potka,Alex Thomo*

Main category: cs.CL

TL;DR: 该研究利用提示工程从LLM中引发不同种族和性别群体的产品建议，并运用三种分析方法揭示性别和种族偏见，指出需要更公平的LLM推荐系统。


<details>
  <summary>Details</summary>
Motivation: 探索大规模语言模型（LLM）生成消费者产品推荐时潜在的性别和种族偏见，首次系统性地分析了此类偏见。

Method: 通过提示工程收集LLM生成的产品建议，运用标记词、支持向量机和Jensen-Shannon散度三种分析方法识别和量化偏见。

Result: 研究发现不同种族和性别群体的推荐存在显著差异，表明需要改进LLM推荐系统的公平性。

Conclusion: 研究强调了LLM推荐系统中偏见问题的重要性，并提出了追求更公平推荐系统的必要性。

Abstract: Large Language Models are increasingly employed in generating consumer product recommendations, yet their potential for embedding and amplifying gender and race biases remains underexplored. This paper serves as one of the first attempts to examine these biases within LLM-generated recommendations. We leverage prompt engineering to elicit product suggestions from LLMs for various race and gender groups and employ three analytical methods-Marked Words, Support Vector Machines, and Jensen-Shannon Divergence-to identify and quantify biases. Our findings reveal significant disparities in the recommendations for demographic groups, underscoring the need for more equitable LLM recommendation systems.

</details>


### [208] [DIAL-SUMMER: A Structured Evaluation Framework of Hierarchical Errors in Dialogue Summaries](https://arxiv.org/abs/2602.08149)
*Sahana Ramnath,Nima Chitsazan,Mingyang Zhou,Chia-Hsuan Lee,Shi-Xiong Zhang,Stephen Rawls,Sambit Sahu,Sangwoo Cho,Xiang Ren,Genta Indra Winata,Akshaj Kumar Veldanda*

Main category: cs.CL

TL;DR: 该研究提出了DIALSUMMER框架，以全面评估对话摘要的错误分类，包括对话级别和回合内部级别，并展示了系统性研究该领域的必要性。


<details>
  <summary>Details</summary>
Motivation: 对话是人类沟通的主要形式，自动生成的对话摘要对人类至关重要。现有对话摘要评估方法忽略了具体任务中的复杂性，如议程结构和叙事视角的变化。

Method: 研究者引入了DIALSUMMER框架，定义了错误分类，并提出两种错误评估级别：对话级别和回合内部级别。该框架还集成了人工标注的数据集，并通过实验分析了标记的错误。

Result: 研究展示了对话中在中部的回合最常被摘要遗漏，而自我矛盾主要发生在摘要结束。此外，研究还展示了系统性分析这一领域的重要性和现有评测框架的局限性。

Conclusion: 本研究通过DIALSUMMER框架，提供了一种更系统并细致的对话摘要评估方法，并指出了未来研究的方向和需求。

Abstract: Dialogues are a predominant mode of communication for humans, and it is immensely helpful to have automatically generated summaries of them (e.g., to revise key points discussed in a meeting, to review conversations between customer agents and product users). Prior works on dialogue summary evaluation largely ignore the complexities specific to this task: (i) shift in structure, from multiple speakers discussing information in a scattered fashion across several turns, to a summary's sentences, and (ii) shift in narration viewpoint, from speakers' first/second-person narration, standardized third-person narration in the summary. In this work, we introduce our framework DIALSUMMER to address the above. We propose DIAL-SUMMER's taxonomy of errors to comprehensively evaluate dialogue summaries at two hierarchical levels: DIALOGUE-LEVEL that focuses on the broader speakers/turns, and WITHIN-TURN-LEVEL that focuses on the information talked about inside a turn. We then present DIAL-SUMMER's dataset composed of dialogue summaries manually annotated with our taxonomy's fine-grained errors. We conduct empirical analyses of these annotated errors, and observe interesting trends (e.g., turns occurring in middle of the dialogue are the most frequently missed in the summary, extrinsic hallucinations largely occur at the end of the summary). We also conduct experiments on LLM-Judges' capability at detecting these errors, through which we demonstrate the challenging nature of our dataset, the robustness of our taxonomy, and the need for future work in this field to enhance LLMs' performance in the same. Code and inference dataset coming soon.

</details>


### [209] [NLP for Local Governance Meeting Records: A Focus Article on Tasks, Datasets, Metrics and Benchmark](https://arxiv.org/abs/2602.08162)
*Ricardo Campos,José Pedro Evans,José Miguel Isidro,Miguel Marques,Luís Filipe Cunha,Alípio Jorge,Sérgio Nunes,Nuno Guimarães*

Main category: cs.CL

TL;DR: 本文总结了自然语言处理（NLP）在结构化和解释地方治理会议记录方面核心任务的方法、评估标准和公开可用资源。


<details>
  <summary>Details</summary>
Motivation: 现有地方治理会议记录存在结构复杂、语言多变等问题，不利于非专业人士解读，也给智能自动化系统处理带来挑战，限制了公共透明度和公民参与。

Method: 文章回顾了支持地方治理会议记录结构化的三个核心NLP任务：文档分段、领域特定实体提取和自动文本摘要。

Result: 文章讨论了这些任务的方法论方法、评估指标，并指出了领域特定挑战，例如数据稀缺性、隐私约束和来源变化。

Conclusion: 文章总结了NLP如何增强地方治理会议记录的结构化和可访问性。

Abstract: Local governance meeting records are official documents, in the form of minutes or transcripts, documenting how proposals, discussions, and procedural actions unfold during institutional meetings. While generally structured, these documents are often dense, bureaucratic, and highly heterogeneous across municipalities, exhibiting significant variation in language, terminology, structure, and overall organization. This heterogeneity makes them difficult for non-experts to interpret and challenging for intelligent automated systems to process, limiting public transparency and civic engagement. To address these challenges, computational methods can be employed to structure and interpret such complex documents. In particular, Natural Language Processing (NLP) offers well-established methods that can enhance the accessibility and interpretability of governmental records. In this focus article, we review foundational NLP tasks that support the structuring of local governance meeting documents. Specifically, we review three core tasks: document segmentation, domain-specific entity extraction and automatic text summarization, which are essential for navigating lengthy deliberations, identifying political actors and personal information, and generating concise representations of complex decision-making processes. In reviewing these tasks, we discuss methodological approaches, evaluation metrics, and publicly available resources, while highlighting domain-specific challenges such as data scarcity, privacy constraints, and source variability. By synthesizing existing work across these foundational tasks, this article provides a structured overview of how NLP can enhance the structuring and accessibility of local governance meeting records.

</details>


### [210] [LLMs and people both learn to form conventions -- just not with each other](https://arxiv.org/abs/2602.08208)
*Cameron R. Jones,Agnese Lombardi,Kyle Mahowald,Benjamin K. Bergen*

Main category: cs.CL

TL;DR: 研究发现，LLM与人类在同类型对（人与人，AI与AI）中能够形成沟通上的习惯性行为，但在人-LLM异构对中则表现出差异，这表明对话对齐不仅仅需要模仿先前的互动，还需要共享对传达意义的理解偏差。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM在对话过程中是否能够形成与人类相似的沟通习惯，以及这种能力在不同类型的对话对中是否存在差异。

Method: 通过设计一个多模态的沟通游戏，分别测试人类、LLM以及人类与LLM之间的沟通一致性、信息准确性和信息长度，同时在实验2中通过提示方式引导LLM表现出人类相似的行为。

Result: 在同类型对中，无论是人类-人类还是AI-AI，都能表现出沟通习惯性行为的增强；但在人类-LLM异构对中，L LM的表现落后于人类-人类及AI-AI对，信息准确性和词汇重叠度较低。

Conclusion: 研究得出结论，为了实现有效的对话对齐，除了模仿过去的交互外，还需要基于传达意义的理解共享偏见。

Abstract: Humans align to one another in conversation -- adopting shared conventions that ease communication. We test whether LLMs form the same kinds of conventions in a multimodal communication game. Both humans and LLMs display evidence of convention-formation (increasing the accuracy and consistency of their turns while decreasing their length) when communicating in same-type dyads (humans with humans, AI with AI). However, heterogenous human-AI pairs fail -- suggesting differences in communicative tendencies. In Experiment 2, we ask whether LLMs can be induced to behave more like human conversants, by prompting them to produce superficially humanlike behavior. While the length of their messages matches that of human pairs, accuracy and lexical overlap in human-LLM pairs continues to lag behind that of both human-human and AI-AI pairs. These results suggest that conversational alignment requires more than just the ability to mimic previous interactions, but also shared interpretative biases toward the meanings that are conveyed.

</details>


### [211] [Pretraining with Token-Level Adaptive Latent Chain-of-Thought](https://arxiv.org/abs/2602.08220)
*Boyi Zeng,Yiqin Hao,He Li,Shixiang Song,Feichen Song,Zitong Wang,Siyuan Huang,Yi Xu,ZiWei He,Xinbing Wang,Zhouhan Lin*

Main category: cs.CL

TL;DR: 本文提出了将Latent Chain-of-Thought（Latent CoT）内部化到预训练中的方法，通过生成每个令牌前的可变长度Latent CoT轨迹，并在预训练时为难令牌分配更长的轨迹，为易令牌分配较短或零长度的轨迹，从而减少计算成本，提高语言建模性能和下游任务准确性。


<details>
  <summary>Details</summary>
Motivation: 面对大型语言模型因高质量语料库有限和通信成本增加而变得日益受限的情况，本文提出了一种新的方法，即通过在预训练中内化过程思维（Latent CoT）来增加每令牌计算，而无需增加参数数量。

Method: 通过在单阶段预训练期间为一般文本生成每个令牌前的可变长度Latent CoT轨迹实现。此方法使用令牌级自适应停止机制，为难令牌分配长轨迹，为易令牌分配短或零长度轨迹，从而在训练和推断中减少计算。

Result: 实验表明，即使与之前的循环基线相比减少了计算量（FLOPs），使用Llama架构时，此方法在语言建模和广泛下游任务方面均具有更好的性能。

Conclusion: 此工作为提升语言模型性能提供了一种新途径，通过在预训练中嵌入过程思维来优化每令牌计算成本，从而提高模型效果和效率。

Abstract: Scaling large language models by increasing parameters and training data is increasingly constrained by limited high-quality corpora and rising communication costs. This work explores an alternative axis: increasing per-token computation without expanding parameters, by internalizing latent Chain-of-Thought (CoT) into pretraining. We propose Pretraining with Token-Level Adaptive Latent CoT (adaptive latent CoT), where the model generates a variable-length latent CoT trajectory before emitting each token -- allocating longer trajectories to difficult tokens and shorter (or even zero) trajectories to easy ones. Importantly, this behavior emerges naturally from one-stage pretraining on general text and reduces computation in both training and inference via token-wise adaptive halting. Experiments with Llama architectures show that adaptive latent CoT consistently improves language modeling perplexity and broad downstream accuracy, even with fewer training FLOPs than prior recurrent baselines.

</details>


### [212] [CoRect: Context-Aware Logit Contrast for Hidden State Rectification to Resolve Knowledge Conflicts](https://arxiv.org/abs/2602.08221)
*Xuhua Ma,Richong Zhang,Zhijie Nie*

Main category: cs.CL

TL;DR: CoRect通过对比上下文化和非上下文化前向传递的logits，识别出表现出高参数偏见的隐藏层，并修正隐藏状态以保留基于证据的信息，从而提升生成的忠实度并减少幻觉。


<details>
  <summary>Details</summary>
Motivation: RAG模型在处理知识冲突时存在定制化知识压制问题，现有方法限于表面的解码调整或需要地面真理的目标权重编辑。CoRect旨在填补这一空白，以提高生成的忠实度。

Method: CoRect通过对每个隐藏层的logits进行对比，识别出那些受参数偏见影响显著的层，然后修正这些层中的隐藏状态，保留基于上下文的证据信息。

Result: 在问答和总结基准测试中，CoRect相比强壮的基础模型提高了忠实度并减少了幻觉。

Conclusion: CoRect是一种有效的解决RAG模型参数压制问题的方法，适用于需要高忠实度的生成任务。

Abstract: Retrieval-Augmented Generation (RAG) often struggles with knowledge conflicts, where model-internal parametric knowledge overrides retrieved evidence, leading to unfaithful outputs. Existing approaches are often limited, relying either on superficial decoding adjustments or weight editing that necessitates ground-truth targets. Through layer-wise analysis, we attribute this failure to a parametric suppression phenomenon: specifically, in deep layers, certain FFN layers overwrite context-sensitive representations with memorized priors. To address this, we propose CoRect (Context-Aware Logit Contrast for Hidden State Rectification). By contrasting logits from contextualized and non-contextualized forward passes, CoRect identifies layers that exhibit high parametric bias without requiring ground-truth labels. It then rectifies the hidden states to preserve evidence-grounded information. Across question answering (QA) and summarization benchmarks, CoRect consistently improves faithfulness and reduces hallucinations compared to strong baselines.

</details>


### [213] [Document Reconstruction Unlocks Scalable Long-Context RLVR](https://arxiv.org/abs/2602.08237)
*Yao Xiao,Lei Wang,Yue Deng,Guanzheng Chen,Ziqi Jin,Jung-jae Kim,Xiaoli Li,Roy Ka-wei Lee,Lidong Bing*

Main category: cs.CL

TL;DR: 该研究提出了一种无需人工标注或强大教师模型监督的无监督方法，通过替换长文档中的几段，并训练大语言模型通过强化学习重建文档，以增强其长期语境能力。该方法在RULER和LongBench v2两个基准测试上均取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: 目前强化学习具有验证奖励的方法主要依赖于高质量的教师模型或人类专家提供的参考答案，成本高且耗时。本文探索无监督方法以减轻这种依赖，通过无监督的反馈机制提高大语言模型在长期语境理解方面的表现。

Method: 替换长文档中部分段落为特殊占位符，训练模型通过强化学习从候选选项中正确识别并排序缺失的段落来重建文档，以此来训练模型捕捉全局叙述连贯性。

Result: 在两个常用基准RULER和LongBench v2上分别验证了该方法的有效性，RULER上取得显著提高，LongBench v2上则在没有传统长语境问答数据的情况下也取得了一定的改进。

Conclusion: 详尽的消融研究表明，奖励设计、数据收集策略、训练方案和数据规模对模型性能存在显著影响。同时，作者对外开放了与研究相关的代码、数据和模型。

Abstract: Reinforcement Learning with Verifiable Rewards~(RLVR) has become a prominent paradigm to enhance the capabilities (i.e.\ long-context) of Large Language Models~(LLMs). However, it often relies on gold-standard answers or explicit evaluation rubrics provided by powerful teacher models or human experts, which are costly and time-consuming. In this work, we investigate unsupervised approaches to enhance the long-context capabilities of LLMs, eliminating the need for heavy human annotations or teacher models' supervision. Specifically, we first replace a few paragraphs with special placeholders in a long document. LLMs are trained through reinforcement learning to reconstruct the document by correctly identifying and sequencing missing paragraphs from a set of candidate options. This training paradigm enables the model to capture global narrative coherence, significantly boosting long-context performance. We validate the effectiveness of our method on two widely used benchmarks, RULER and LongBench~v2. While acquiring noticeable gains on RULER, it can also achieve a reasonable improvement on LongBench~v2 without any manually curated long-context QA data. Furthermore, we conduct extensive ablation studies to analyze the impact of reward design, data curation strategies, training schemes, and data scaling effects on model performance. We publicly release our code, data, and models.

</details>


### [214] [On convexity and efficiency in semantic systems](https://arxiv.org/abs/2602.08238)
*Nathaniel Imel,Noga Zaslavasky*

Main category: cs.CL

TL;DR: 本文通过分析信息瓶颈框架下的语义效率，探讨了语义类别系统的凸性和效率之间的关系，发现尽管两者都导致了类似的结构观察，但它们本质上是不同的，效率提供了更全面的语义类型学解释。


<details>
  <summary>Details</summary>
Motivation: 作者试图解决关于人类语义类别系统的两种常见观点——凸性和效率——为何与色彩命名系统的观察结果一致，但它们之间的关系尚未被充分理解。

Method: 作者结合了分析性和经验性分析，利用信息瓶颈框架来讨论凸性和效率。

Result: 研究表明，凸性和效率是独立的，IB优化的系统大多在色彩命名领域具有凸性，而效率是区分已知色彩命名系统与假设变体的更强预测因子，凸性对此影响甚微。文章还讨论了凸性无法解释的一些现象，但效率可以解释。

Conclusion: 本文综合分析表明，凸性和效率在语义类别系统中可能是不同的概念，效率为理解语义类型学提供了一个更全面的视角。

Abstract: There are two widely held characterizations of human semantic category systems: (1) they form convex partitions of conceptual spaces, and (2) they are efficient for communication. While prior work observed that convexity and efficiency co-occur in color naming, the analytical relation between them and why they co-occur have not been well understood. We address this gap by combining analytical and empirical analyses that build on the Information Bottleneck (IB) framework for semantic efficiency. First, we show that convexity and efficiency are distinct in the sense that neither entails the other: there are convex systems which are inefficient, and optimally-efficient systems that are non-convex. Crucially, however, the IB-optimal systems are mostly convex in the domain of color naming, explaining the main empirical basis for the convexity approach. Second, we show that efficiency is a stronger predictor for discriminating attested color naming systems from hypothetical variants, with convexity adding negligible improvement on top of that. Finally, we discuss a range of empirical phenomena that convexity cannot account for but efficiency can. Taken together, our work suggests that while convexity and efficiency can yield similar structural observations, they are fundamentally distinct, with efficiency providing a more comprehensive account of semantic typology.

</details>


### [215] [Language Predicts Identity Fusion Across Cultures and Reveals Divergent Pathways to Violence](https://arxiv.org/abs/2602.08252)
*Devin R. Wright,Justin E. Lane,F. LeRon Shults*

Main category: cs.CL

TL;DR: 研究利用认知语言学模式、LLM和隐含隐喻来测量身份融合，通过分析英国和新加坡的数据集，该方法优于现有方法以预测验证的身份融合评分。这种方法在极端主义宣言中的应用揭示了两条高融合暴力路径。


<details>
  <summary>Details</summary>
Motivation: 随着政治极化和暴力事件的增加，理解极端主义的心理根源变得越来越重要。研究表明，身份融合与极端行为意愿相关。

Method: 研究提出了一种认知语言学身份融合评分法，该方法借助认知语言学模式、大型语言模型（LLMs）和隐含隐喻来衡量身份融合程度。

Result: 该方法在英国和新加坡的数据集中表现优于现有方法，能有效预测身份融合得分。应用到极端主义宣言中，研究发现了两种高融合暴力路径：意识形态驱动者倾向于以群体身份自居，形成亲属关系纽带；而以怨恨为驱动的个人则将群体以个人身份加以描述。

Conclusion: 研究结果进一步完善了身份融合理论，并提供了一种助于融合研究和极端主义检测的可扩展工具。

Abstract: In light of increasing polarization and political violence, understanding the psychological roots of extremism is increasingly important. Prior research shows that identity fusion predicts willingness to engage in extreme acts. We evaluate the Cognitive Linguistic Identity Fusion Score, a method that uses cognitive linguistic patterns, LLMs, and implicit metaphor to measure fusion from language. Across datasets from the United Kingdom and Singapore, this approach outperforms existing methods in predicting validated fusion scores. Applied to extremist manifestos, two distinct high-fusion pathways to violence emerge: ideologues tend to frame themselves in terms of group, forming kinship bonds; whereas grievance-driven individuals frame the group in terms of their personal identity. These results refine theories of identity fusion and provide a scalable tool aiding fusion research and extremism detection.

</details>


### [216] [Language Modeling and Understanding Through Paraphrase Generation and Detection](https://arxiv.org/abs/2602.08274)
*Jan Philip Wahle*

Main category: cs.CL

TL;DR: 论文提出了将重写句分解为其构成语言因素（重写类型），从而在语义等价方面提供更精细的认知依据。研究显示，即使对高级机器学习模型来说，这也是一个挑战性任务。然而，当明确训练以识别重写类型时，模型在相关重写任务和下游应用中的性能显著提升。例如，在剽窃检测和识别Quora上的重复问题时，基于重写类型的模型表现优于仅使用二元对的模型。


<details>
  <summary>Details</summary>
Motivation: 探讨人类语言表达的灵活性及其背后机制的重要性，并指出目前模型在处理重写任务中的不足，强调分解重写以识别语言因素来理解更深层次的语义等价性的必要性。

Method: 提出使用重写类型作为语言因素，将重写分解进模型中进行训练。通过实验对比基于重写类型训练的模型与仅基于二元对训练的模型在具体任务如剽窃检测和问答重复识别上的表现。

Result: 研究结果表明，基于重写类型的训练方法在某些任务上超过了人类基准线和仅仅基于二元对训练的模型，例如89.6%的剽窃检测准确性超过了78.4%的人类基准线和66.5%的基于arXiv的科学论文基准线。

Conclusion: 论文证明了通过分解重写以识别其语言构成因素是对模型进行更精细语义理解的有效方法，为计算语言模型提供了新的研究方向。

Abstract: Language enables humans to share knowledge, reason about the world, and pass on strategies for survival and innovation across generations. At the heart of this process is not just the ability to communicate but also the remarkable flexibility in how we can express ourselves. We can express the same thoughts in virtually infinite ways using different words and structures - this ability to rephrase and reformulate expressions is known as paraphrase. Modeling paraphrases is a keystone to meaning in computational language models; being able to construct different variations of texts that convey the same meaning or not shows strong abilities of semantic understanding. If computational language models are to represent meaning, they must understand and control the different aspects that construct the same meaning as opposed to different meanings at a fine granularity. Yet most existing approaches reduce paraphrasing to a binary decision between two texts or to producing a single rewrite of a source, obscuring which linguistic factors are responsible for meaning preservation. In this thesis, I propose that decomposing paraphrases into their constituent linguistic aspects (paraphrase types) offers a more fine-grained and cognitively grounded view of semantic equivalence. I show that even advanced machine learning models struggle with this task. Yet, when explicitly trained on paraphrase types, models achieve stronger performance on related paraphrase tasks and downstream applications. For example, in plagiarism detection, language models trained on paraphrase types surpass human baselines: 89.6% accuracy compared to 78.4% for plagiarism cases from Wikipedia, and 66.5% compared to 55.7% for plagiarism of scientific papers from arXiv. In identifying duplicate questions on Quora, models trained with paraphrase types improve over models trained on binary pairs. Furthermore, I demonstrate that...

</details>


### [217] [New Skills or Sharper Primitives? A Probabilistic Perspective on the Emergence of Reasoning in RLVR](https://arxiv.org/abs/2602.08281)
*Zhilin Wang,Yafu Li,Shunkai Zhang,Zhi Wang,Haoran Zhang,Xiaoye Qu,Yu Cheng*

Main category: cs.CL

TL;DR: 本研究提出了一种概率框架来定义LLMs的能力，并通过强化学习可验证奖励（RLVR）技术，利用原子步骤概率的细化提高模型在多步骤任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 探讨RLVR是否赋予LLMs新的能力，而非仅激发隐藏的能力。

Method: 开发了一种概率框架，专门训练模型处理单一步骤的操作，并使用Algebrarium框架进行评估。

Result: 实验证明RLVR激励模型探索新的解路径，复合表现由原子步骤的概率严格控制，且RLVR能牺牲特定技能以最大化整体奖励。

Conclusion: 研究表明，通过迭代优化可解问题，LLMs能够开发出解决未解问题的新能力。

Abstract: Whether Reinforcement Learning with Verifiable Rewards (RLVR) endows Large Language Models (LLMs) with new capabilities or merely elicits latent traces remains a central debate. In this work, we align with the former view, proposing a probabilistic framework where capability is defined by instance-level solvability. We hypothesize that the emergence of complex reasoning can be driven by sharpening atomic step probabilities, which enables models to overcome the exponential decay of success rates inherent in multi-step reasoning chains. Utilizing the Algebrarium framework, we train models exclusively on single-step operations and evaluate their performance on unseen multi-step tasks. Our empirical results confirm that: (1) RLVR incentivizes the exploration of previously inaccessible solution paths by amplifying the model's existing skills; (2) composite performance is strictly governed by the joint probability of atomic steps, evidenced by high Pearson correlation coefficients ($ρ\in [0.69, 0.96]$); and (3) RLVR, acting as a global optimizer, can cause specific skills to be sacrificed to maximize aggregate reward. Our work offers a novel explanation for emergent abilities in RLVR, suggesting that the iterative optimization of solvable problems enables models to develop the capabilities to tackle previously unsolvable scenarios.

</details>


### [218] [When Does Context Help? Error Dynamics of Contextual Information in Large Language Models](https://arxiv.org/abs/2602.08294)
*Dingzirui Wang,Xuanliang Zhang,Keyan Xu,Qingfu Zhu,Wanxiang Che,Yang Deng*

Main category: cs.CL

TL;DR: 本文提出了一个统一的理论框架，用于分析Transformer结构的大语言模型在推理时受任意上下文信息的影响，并证明了上下文修正向量分解为基本误差向量和上下文修正向量，为上下文纠错向量提供了必要的几何条件和上界。


<details>
  <summary>Details</summary>
Motivation: 为了解释除了特定场景如在上下文学习（ICL）之外，上下文信息在推理时间对大语言模型的影响机制。

Method: 通过输出误差动力学特性，提出了针对单层和多层Transformer的上下文影响统一理论框架，证明了上下文纠错向量与基本误差向量之间的分解，并给出了上下文修正向量的几何条件和上界。

Result: 证明了基误差向量和上下文修正向量之间的正交性以及上下文相关性和互补性如何决定上下文修正向量的上界。

Conclusion: 理论结果在多种场景下得到实验证明，提出了一种基于理论的上下文选择策略，提升了0.6%的性能。

Abstract: Contextual information at inference time, such as demonstrations, retrieved knowledge, or interaction history, can substantially improve large language models (LLMs) without parameter updates, yet its theoretical role remains poorly understood beyond specific settings such as in-context learning (ICL). We present a unified theoretical framework for analyzing the effect of arbitrary contextual information in Transformer-based LLMs. Our analysis characterizes contextual influence through output error dynamics. In a single-layer Transformer, we prove that the context-conditioned error vector decomposes additively into the baseline error vector and a contextual correction vector. This yields necessary geometric conditions for error reduction: the contextual correction must align with the negative baseline error and satisfy a norm constraint. We further show that the contextual correction norm admits an explicit upper bound determined by context-query relevance and complementarity. These results extend to multi-context and multi-layer Transformers. Experiments across ICL, retrieval-augmented generation, and memory evolution validate our theory and motivate a principled context selection strategy that improves performance by $0.6\%$.

</details>


### [219] [JUSTICE: Judicial Unified Synthesis Through Intermediate Conclusion Emulation for Automated Judgment Document Generation](https://arxiv.org/abs/2602.08305)
*Binglin Wu,Yingyi Zhang,Xiannneg Li*

Main category: cs.CL

TL;DR: 本文提出了一种名为JUSTICE的新框架，该框架通过模拟人类法官的思维过程，包括检索相关法律条文、预判阶段生成验证性中间结论以及综合生成最终判决，有效提升了判决书的法律准确性和一致性。


<details>
  <summary>Details</summary>
Motivation: 现有方法往往忽略了预判阶段，这导致判决文档缺乏法律依据和准确性。因此，本文旨在通过明确建模预判过程，提高生成判决文档的法律稳健性。

Method: JUSTICE框架包括三个关键组件：参照性法律元素检索器（RJER）、中间结论生成器（ICE）和综合生成器（JUS）。RJER用于检索相关法律条文和先例案例，ICE负责生成可验证的中间结论，而JUS则是综合这些输入来生成最终判决。

Result: 在领域内和跨领域的数据集上进行实验，JUSTICE框架显著优于现有基线，特别是在刑期预测方面取得了4.6%的改进。

Conclusion: 我们的研究结果强调了明确建模预判过程的重要性，以增强生成的判决文档的法律连贯性和准确性。

Abstract: Automated judgment document generation is a significant yet challenging legal AI task. As the conclusive written instrument issued by a court, a judgment document embodies complex legal reasoning. However, existing methods often oversimplify this complex process, particularly by omitting the ``Pre-Judge'' phase, a crucial step where human judges form a preliminary conclusion. This omission leads to two core challenges: 1) the ineffective acquisition of foundational judicial elements, and 2) the inadequate modeling of the Pre-Judge process, which collectively undermine the final document's legal soundness. To address these challenges, we propose \textit{\textbf{J}udicial \textbf{U}nified \textbf{S}ynthesis \textbf{T}hrough \textbf{I}ntermediate \textbf{C}onclusion \textbf{E}mulation} (JUSTICE), a novel framework that emulates the ``Search $\rightarrow$ Pre-Judge $\rightarrow$ Write'' cognitive workflow of human judges. Specifically, it introduces the Pre-Judge stage through three dedicated components: Referential Judicial Element Retriever (RJER), Intermediate Conclusion Emulator (ICE), and Judicial Unified Synthesizer (JUS). RJER first retrieves legal articles and a precedent case to establish a referential foundation. ICE then operationalizes the Pre-Judge phase by generating a verifiable intermediate conclusion. Finally, JUS synthesizes these inputs to craft the final judgment. Experiments on both an in-domain legal benchmark and an out-of-distribution dataset show that JUSTICE significantly outperforms strong baselines, with substantial gains in legal accuracy, including a 4.6\% improvement in prison term prediction. Our findings underscore the importance of explicitly modeling the Pre-Judge process to enhance the legal coherence and accuracy of generated judgment documents.

</details>


### [220] [Improving Data and Reward Design for Scientific Reasoning in Large Language Models](https://arxiv.org/abs/2602.08321)
*Zijie Chen,Zhenghao Lin,Xiao Liu,Zhenzhong Lan,Yeyun Gong,Peng Cheng*

Main category: cs.CL

TL;DR: 该研究开发了一个大规模的数据处理管道，名为Dr. SCI，并在此基础上提出了Dr. SCI后训练流程，显著提升了大型语言模型在科学推理和开放性问题回答上的表现。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在解决开放性科学问题时面临挑战，尤其是在数据构建和奖励设计方面，因此，研究旨在改善这些模型的科学推理能力。

Method: 研究首先构建了Dr. SCI数据集，包含多领域科学问题，以及新的后训练流程，包括探索扩展语义训练、动态难度课程和科学评价指导的强化学习。

Result: 实验结果显示，在GPQA-diamond上取得了63.2的评分，在GPQA-general上取得了32.4的评分，超过多个基线模型。

Conclusion: 该研究显著提高了大型语言模型在科学推理和开放性问题回答上的性能，特别是在开放性环境中表现出显著进步。

Abstract: Solving open-ended science questions remains challenging for large language models, particularly due to inherently unreliable supervision and evaluation. The bottleneck lies in the data construction and reward design for scientific post-training. We develop a large-scale, systematic data processing pipeline that transforms heterogeneous open-source science data into Dr. SCI dataset, which comprises of 1M questions across eight STEM subjects, with explicit verifiable/open-ended splits, scalable difficulty annotation, and fine-grained rubrics that operationalize evaluation for open-ended answers. Building on this dataset, we propose the Dr. SCI post-training pipeline, which redesigns the standard SFT -> RL workflow through three components: (i) Exploration-Expanding SFT, which broadens the model's reasoning pattern coverage prior to RL; (ii) Dynamic Difficulty Curriculum, which adapts training data to the model's evolving scientific capability; and (iii) SciRubric-Guided RL, which enables stable reinforcement learning on open-ended scientific questions via rubric-based evaluation with explicit answer correctness. Qwen3-4B-Base trained using Dr.SCI pipeline achieves 63.2 on GPQA-diamond and 32.4 on GPQA-general, consistently improves over strong post-trained baselines such as o1-mini and GPT-4o, demonstrating substantial gains in scientific reasoning, especially in open-ended settings.

</details>


### [221] [An Attention-over-Attention Generative Model for Joint Multiple Intent Detection and Slot Filling](https://arxiv.org/abs/2602.08322)
*Wei Zhu*

Main category: cs.CL

TL;DR: 该研究提出了一种生成模型框架，同时处理多意图检测和槽位填充，提出了一种注意力机制解决意图数量变化和子任务干扰问题，并基于单意图语句构建了新的多意图SLU数据集。


<details>
  <summary>Details</summary>
Motivation: 当前大多数SLU方法关注单一意图场景，而真实对话中用户常常在同一句话中表达多个意图，现有方法难以处理。

Method: 通过设计注意力机制，引入生成模型和注意力机制克服了潜在会话状态干扰和多个意图的挑战。

Result: 实验结果显示，所提出的生成式注意力模型在两个公开数据集MixATIS和MixSNIPS及新构建的数据集上均获得了领先性能。

Conclusion: 研究为多意图SLU问题提供了一种新颖的解决方法，展示了其有效性和优越性。

Abstract: In task-oriented dialogue systems, spoken language understanding (SLU) is a critical component, which consists of two sub-tasks, intent detection and slot filling. Most existing methods focus on the single-intent SLU, where each utterance only has one intent. However, in real-world scenarios users usually express multiple intents in an utterance, which poses a challenge for existing dialogue systems and datasets. In this paper, we propose a generative framework to simultaneously address multiple intent detection and slot filling. In particular, an attention-over-attention decoder is proposed to handle the variable number of intents and the interference between the two sub-tasks by incorporating an inductive bias into the process of multi-task learning. Besides, we construct two new multi-intent SLU datasets based on single-intent utterances by taking advantage of the next sentence prediction (NSP) head of the BERT model. Experimental results demonstrate that our proposed attention-over-attention generative model achieves state-of-the-art performance on two public datasets, MixATIS and MixSNIPS, and our constructed datasets.

</details>


### [222] [Latent Reasoning with Supervised Thinking States](https://arxiv.org/abs/2602.08332)
*Ido Amos,Avi Caciularu,Mor Geva,Amir Globerson,Jonathan Herzig,Lior Shani,Idan Szpektor*

Main category: cs.CL

TL;DR: 提出了一种名为Thinking States的方法，在输入处理过程中进行推理，通过生成思考令牌并转换回嵌入空间来实现，提高了推理的效率并保持了较强的推理能力。


<details>
  <summary>Details</summary>
Motivation: 由于链式思考（CoT）推理消耗大量的推理成本，本文提出了Thinking States方法来解决这一问题。

Method: Thinking States方法在输入处理过程中生成思维令牌序列，将思维令牌转换回嵌入空间，输入令牌和思维令牌一起送入后续的模型处理。

Result: 该方法在多个逻辑推理任务中表现优于其他潜在推理方法，提高了数学问题推理的性能，同时在保持与CoT相当的推理能力的同时降低了延迟。

Conclusion: Thinking States方法通过改进推理过程，展现了更强的推理行为，拓宽了推理链条，适用于长度更长的序列。

Abstract: Reasoning with a chain-of-thought (CoT) enables Large Language Models (LLMs) to solve complex tasks but incurs significant inference costs due to the generation of long rationales. We propose Thinking States, a method that performs reasoning {\em while} the input is processing. Specifically, Thinking States generates sequences of thinking tokens every few input tokens, transforms the thoughts back into embedding space, and adds them to the following input tokens. This has two key advantages. First, it captures the recurrent nature of CoT, but where the thought tokens are generated as input is processing. Second, since the thoughts are represented as tokens, they can be learned from natural language supervision, and using teacher-forcing, which is parallelizable. Empirically, Thinking States outperforms other latent reasoning methods on multiple reasoning tasks, narrowing the gap to CoT on math problems, and matching its performance on 2-Hop QA with improved latency. On state-tracking tasks, we show Thinking States leads to stronger reasoning behavior than CoT, successfully extrapolating to longer sequences than seen during training.

</details>


### [223] [UReason: Benchmarking the Reasoning Paradox in Unified Multimodal Models](https://arxiv.org/abs/2602.08336)
*Cheng Yang,Chufan Shi,Bo Shui,Yaokang Wu,Muzi Tao,Huijuan Wang,Ivan Yee Lee,Yong Liu,Xuezhe Ma,Taylor Berg-Kirkpatrick*

Main category: cs.CL

TL;DR: UReason 是一个诊断基准，用于评估统一多模态模型在图像生成中的链式思考能力。通过比较直接生成、链式思考引导生成和仅条件于精炼提示的生成，发现链式思考虽然一般能提升性能，但将其作为条件上下文时会妨碍视觉合成，而仅条件于精炼提示则有显著提高。这表明瓶颈在于上下文干扰而非推理能力不足。


<details>
  <summary>Details</summary>
Motivation: 当前统一多模态模型在图像生成中的链式思考效果尚不明确，因此作者提出UReason基准来诊断此问题。

Method: UReason包含2000个跨五大任务家庭的实例，引入了直接生成、链式思考引导生成和仅条件于精炼提示的生成比较框架，以孤立链式思考的作用。

Result: 观察到链式思考通常提高性能，但保留中间想法为条件上下文会妨碍视觉合成，仅条件于精炼提示则有显著效果。

Conclusion: 研究表明瓶颈在于上下文干扰，而非推理能力不足。UReason提供了一个研究统一模型中链式思考的规范性测试平台，并激励未来有效整合链式思考以解决视觉生成问题的方法。

Abstract: To elicit capabilities for addressing complex and implicit visual requirements, recent unified multimodal models increasingly adopt chain-of-thought reasoning to guide image generation. However, the actual effect of reasoning on visual synthesis remains unclear. We present UReason, a diagnostic benchmark for reasoning-driven image generation that evaluates whether reasoning can be faithfully executed in pixels. UReason contains 2,000 instances across five task families: Code, Arithmetic, Spatial, Attribute, and Text reasoning. To isolate the role of reasoning traces, we introduce an evaluation framework comparing direct generation, reasoning-guided generation, and de-contextualized generation which conditions only on the refined prompt. Across eight open-source unified models, we observe a consistent Reasoning Paradox: Reasoning traces generally improve performance over direct generation, yet retaining intermediate thoughts as conditioning context often hinders visual synthesis, and conditioning only on the refined prompt yields substantial gains. Our analysis suggests that the bottleneck lies in contextual interference rather than insufficient reasoning capacity. UReason provides a principled testbed for studying reasoning in unified models and motivates future methods that effectively integrate reasoning for visual generation while mitigating interference.

</details>


### [224] [ViGoEmotions: A Benchmark Dataset For Fine-grained Emotion Detection on Vietnamese Texts](https://arxiv.org/abs/2602.08371)
*Hung Quang Tran,Nam Tien Pham,Son T. Luu,Kiet Van Nguyen*

Main category: cs.CL

TL;DR: 本研究介绍了一个包含20,664条越南社交媒体评论的语料库，每条评论被细分为27种不同的情绪。通过三种预处理策略评估了八个预训练Transformer模型，结果表明将表情符号转换为文本常能提高某些基于BERT的基本模型的性能，而保留原始表情符号则能使ViSoBERT和CafeBERT表现最佳。ViSoBERT在宏F1分数上达到了61.50%，加权F1分数上为63.26%。


<details>
  <summary>Details</summary>
Motivation: 鉴于情感分类在情感预测和有害内容检测中的重要性，以及大型语言模型在自然语言处理领域取得的进展，本研究旨在构建一个越南语情感语料库并评估不同预处理策略对模型性能的影响。

Method: 研究构建了一个包含20,664条带有27种细分类情感标签的越南社交媒体评论的语料库。使用八种预训练Transformer模型，并采用三种预处理策略：基于规则的原始表情符号规范化、将表情符号转换为文本描述以及应用ViSoLex词汇规范化系统来评估数据集质量及其对情感分类的影响。

Result: 研究发现将表情符号转换为文本通常可以提升大多数基于BERT的基本模型性能，保留原始表情符号能使ViSoBERT和CafeBERT表现最佳。ViSoBERT在宏F1分数和加权F1分数上分别达到了61.50%和63.26%。此外，PhoBERT也表现出色。

Conclusion: 研究强调，虽然提出的语料库能够支持多种架构，但预处理策略和标注质量仍然是影响下游性能的关键因素。

Abstract: Emotion classification plays a significant role in emotion prediction and harmful content detection. Recent advancements in NLP, particularly through large language models (LLMs), have greatly improved outcomes in this field. This study introduces ViGoEmotions -- a Vietnamese emotion corpus comprising 20,664 social media comments in which each comment is classified into 27 fine-grained distinct emotions. To evaluate the quality of the dataset and its impact on emotion classification, eight pre-trained Transformer-based models were evaluated under three preprocessing strategies: preserving original emojis with rule-based normalization, converting emojis into textual descriptions, and applying ViSoLex, a model-based lexical normalization system. Results show that converting emojis into text often improves the performance of several BERT-based baselines, while preserving emojis yields the best results for ViSoBERT and CafeBERT. In contrast, removing emojis generally leads to lower performance. ViSoBERT achieved the highest Macro F1-score of 61.50% and Weighted F1-score of 63.26%. Strong performance was also observed from CafeBERT and PhoBERT. These findings highlight that while the proposed corpus can support diverse architectures effectively, preprocessing strategies and annotation quality remain key factors influencing downstream performance.

</details>


### [225] [Dynamic Long Context Reasoning over Compressed Memory via End-to-End Reinforcement Learning](https://arxiv.org/abs/2602.08382)
*Zhuoen Chen,Dongfang Li,Meishan Zhang,Baotian Hu,Min Zhang*

Main category: cs.CL

TL;DR: 提出了一种基于分块压缩和选择性记忆召回的认知启发式框架，以实现高效的大语境推理，通过联合优化压缩器和推理器以及单独训练门控模块，该方法在多跳推理基准上达到了竞争力并在上下文长度上实现了更大的扩展。


<details>
  <summary>Details</summary>
Motivation: 针对大型语言模型在长语境上的挑战，包括计算成本的急剧增加、信息遗忘和检索增强生成中的语境碎片化等问题，提出了一种新的认知启发式框架。

Method: 该方法将长输入分块并在每个分块中生成压缩的记忆表示，然后通过门控模块动态选择相关记忆块，并由带有演化的工件记忆的推理模块迭代处理以解决下游任务。压缩器和推理器通过端到端的强化学习联合优化，门控模块单独作为分类器训练。

Result: 实验结果表明，该方法在RULER-HQA等多跳推理基准上达到了竞争力，并且能够将上下文长度从7K扩展到1.75M个标记，相比MemAgent，在峰值GPU内存使用方面降低了两倍，推理速度提高了六倍。

Conclusion: 该研究提供了一种新的认知启发式框架，有效解决了长语境推理中的挑战，并在实际应用中展示了显著的性能增益。

Abstract: Large Language Models (LLMs) face significant challenges in long-context processing, including quadratic computational costs, information forgetting, and the context fragmentation inherent in retrieval-augmented generation (RAG). We propose a cognitively inspired framework for efficient long-context inference based on chunk-wise compression and selective memory recall, rather than processing all raw tokens. The framework segments long inputs into chunks and encodes each chunk into compressed memory representations using a learned compressor. A gating module dynamically selects relevant memory blocks, which are then iteratively processed by a reasoning module with an evolving working memory to solve downstream tasks. The compressor and reasoner are jointly optimized via end-to-end reinforcement learning, while the gating module is trained separately as a classifier. Experimental results show that the proposed method achieves competitive accuracy on multi-hop reasoning benchmarks such as RULER-HQA, extrapolates context length from 7K to 1.75M tokens, and offers a favorable accuracy-efficiency trade-off compared to strong long-context baselines. In particular, it achieves up to a 2 times reduction in peak GPU memory usage and a 6 times inference speedup over MemAgent.

</details>


### [226] [TEAM: Temporal-Spatial Consistency Guided Expert Activation for MoE Diffusion Language Model Acceleration](https://arxiv.org/abs/2602.08404)
*Linye Wei,Zixiang Luo,Pingzhi Tang,Meng Li*

Main category: cs.CL

TL;DR: TEAM框架通过减少激活的专家数量并选择性地激活必要专家，提高MoE dLLMs的解码速度，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 团队观察到MoE架构在解码过程中专家路由决策表现出强时间一致性与空间一致性，据此开发了 TEAM，以减少激活专家数量，提高灵活性。

Method: TEAM采用三种互补的专家激活和解码策略，保守地选择用于解码和遮蔽的必要专家，并在此基础上进行激进的多候选探索。

Result: 实验结果显示，TEAM比传统的MoE dLLM快2.2倍，性能基本无降。

Conclusion: TEAM为MoE dLLMs提供了一种加速的新方法，已经在GitHub上开源。

Abstract: Diffusion large language models (dLLMs) have recently gained significant attention due to their inherent support for parallel decoding. Building on this paradigm, Mixture-of-Experts (MoE) dLLMs with autoregressive (AR) initialization have further demonstrated strong performance competitive with mainstream AR models. However, we identify a fundamental mismatch between MoE architectures and diffusion-based decoding. Specifically, a large number of experts are activated at each denoising step, while only a small subset of tokens is ultimately accepted, resulting in substantial inference overhead and limiting their deployment in latency-sensitive applications. In this work, we propose TEAM, a plug-and-play framework that accelerates MoE dLLMs by enabling more accepted tokens with fewer activated experts. TEAM is motivated by the observation that expert routing decisions exhibit strong temporal consistency across denoising levels as well as spatial consistency across token positions. Leveraging these properties, TEAM employs three complementary expert activation and decoding strategies, conservatively selecting necessary experts for decoded and masked tokens and simultaneously performing aggressive speculative exploration across multiple candidates. Experimental results demonstrate that TEAM achieves up to 2.2x speedup over vanilla MoE dLLM, with negligible performance degradation. Code is released at https://github.com/PKU-SEC-Lab/TEAM-MoE-dLLM.

</details>


### [227] [Prism: Spectral-Aware Block-Sparse Attention](https://arxiv.org/abs/2602.08426)
*Xinghao Wang,Pengyu Wang,Xiaoran Liu,Fangxu Liu,Jason Chu,Kai Song,Xipeng Qiu*

Main category: cs.CL

TL;DR: Prism通过不依赖于训练的频谱感知方法，将block选择分解为高频和低频分支，恢复了被均值池化滤除的方位信号，从而实现仅基于block级别的block重要性估计，显著提高效率。


<details>
  <summary>Details</summary>
Motivation: 现有的粗粒度注意力方法作为block重要性估计的近似手段，但由于均值池化与旋转位置嵌入的相互作用，导致局部位置信息（例如斜线模式）的“盲点”问题，这限制了块级操作的效率。

Method: Prism通过引入谱感知的方法，将块选择过程拆分为高频和低频分支，通过能量基于的温度校准直接从池化表示中恢复衰减的方位信号，使得块重要性估计能够通过纯块级别操作完成。

Result: 实验结果表明，Prism在保持与全注意力相等的准确性的同时，提供了高达5.1倍的速度提升。

Conclusion: Prism为解决块稀疏注意力的效率问题提供了一种有效的解决方案，通过去除池化操作对高频位置信息的影响，使得块选择过程更加高效。

Abstract: Block-sparse attention is promising for accelerating long-context LLM pre-filling, yet identifying relevant blocks efficiently remains a bottleneck. Existing methods typically employ coarse-grained attention as a proxy for block importance estimation, but often resort to expensive token-level searching or scoring, resulting in significant selection overhead. In this work, we trace the inaccuracy of standard coarse-grained attention via mean pooling to a theoretical root cause: the interaction between mean pooling and Rotary Positional Embeddings (RoPE). We prove that mean pooling acts as a low-pass filter that induces destructive interference in high-frequency dimensions, effectively creating a "blind spot" for local positional information (e.g., slash patterns). To address this, we introduce Prism, a training-free spectral-aware approach that decomposes block selection into high-frequency and low-frequency branches. By applying energy-based temperature calibration, Prism restores the attenuated positional signals directly from pooled representations, enabling block importance estimation using purely block-level operations, thereby improving efficiency. Extensive evaluations confirm that Prism maintains accuracy parity with full attention while delivering up to $\mathbf{5.1\times}$ speedup.

</details>


### [228] [Large Language Models and Impossible Language Acquisition: "False Promise" or an Overturn of our Current Perspective towards AI](https://arxiv.org/abs/2602.08437)
*Ziyan wang,Longlong Ma*

Main category: cs.CL

TL;DR: 该研究通过实验分析了大型语言模型（LLMs）在学习可不可能语言时的表现，发现GPT-2小型模型在学习不可能语言时表现不佳，而LSTM模型则支持了乔姆斯基的观点。研究建议重塑对LLMs的理论视角，从乔姆斯基的理性浪漫主义向功能主义和经验主义转变。


<details>
  <summary>Details</summary>
Motivation: 研究旨在回应乔姆斯基对于大型语言模型的批评，通过实验数据验证这些观点，并探索新的理论方向。

Method: 实验方法包括构建一系列句法上不可能的语言，并使用GPT-2小型模型和LSTM模型进行学习能力测试。统计分析表明GPT-2小型模型在学习不可能语言方面表现差于学习可能语言，而LSTM模型则支持了乔姆斯基的理论。

Result: 实验结果显示GPT-2小型模型在学习不可能语言时表现不佳，而LSTM模型与乔姆斯基的观点相吻合。统计分析支持了乔姆斯基的论断。

Conclusion: 论文提出了一种在乔姆斯基理论框架下的LLMs新视角，并探讨了从理性浪漫主义向功能性主义和经验主义转变的理论路径。

Abstract: In Chomsky's provocative critique "The False Promise of CHATGPT," Large Language Models (LLMs) are characterized as mere pattern predictors that do not acquire languages via intrinsic causal and self-correction structures like humans, therefore are not able to distinguish impossible languages. It stands as a representative in a fundamental challenge to the intellectual foundations of AI, for it integrally synthesizes major issues in methodologies within LLMs and possesses an iconic a priori rationalist perspective. We examine this famous critic from both the perspective in pre-existing literature of linguistics and psychology as well as a research based on an experiment inquiring the capacity of learning both possible and impossible languages among LLMs. We constructed a set of syntactically impossible languages by applying certain transformations to English. These include reversing whole sentences, and adding negation based on word-count parity. Two rounds of controlled experiments were each conducted on GPT-2 small models and long short-term memory (LSTM) models. Statistical analysis (Welch's t-test) shows GPT2 small models underperform in learning all of the impossible languages compared to their performance on the possible language (p<.001). On the other hand, LSTM models' performance tallies with Chomsky's argument, suggesting the irreplaceable role of the evolution of transformer architecture. Based on theoretical analysis and empirical findings, we propose a new vision within Chomsky's theory towards LLMs, and a shift of theoretical paradigm outside Chomsky, from his "rationalist-romantics" paradigm to functionalism and empiricism in LLMs research.

</details>


### [229] [Characterizing, Evaluating, and Optimizing Complex Reasoning](https://arxiv.org/abs/2602.08498)
*Haoran Zhang,Yafu Li,Zhi Wang,Zhilin Wang,Shunkai Zhang,Xiaoye Qu,Yu Cheng*

Main category: cs.CL

TL;DR: 本文提出了ME²原则来定义高效的推理质量，并将推理痕迹建模为有向无环图（DAG），开发了一种基于DAG的成对评价方法。此外，他们构建了一个TRM-Preference数据集和一个思考奖励模型（TRM），用于大规模评估推理质量。实验表明思考奖励是一个有效的优化信号。选择更好的推理可以在测试时提高结果（最多19.3%的收益），在强化学习训练中可以增强推理和性能（最多3.9%的收益），适用于多种任务。


<details>
  <summary>Details</summary>
Motivation: 当前的大型推理模型依赖于复杂的内部推理痕迹结构，但缺乏评估和优化这些痕迹的方法。本文提出解决方案以解决此问题，通过引入ME²原则、基于DAG的成对评价方法以及思考奖励模型，改善了推理质量和性能。

Method: 本文通过以下步骤进行：首先，提出ME²原则定义推理质量；其次，将推理痕迹建模为DAG，并开发基于DAG的成对评价方法；然后，构建TRM-Preference数据集并训练TRM模型；最后，通过实验验证思考奖励模型的有效性。

Result: 通过实验表明，思考奖励作为优化信号作用显著。测试时选择更好的推理可以提高结果（最多19.3%的提升），在强化学习训练过程中TRM模型能有效改进推理和性能（最多3.9%的提升），适用于各种任务。

Conclusion: 这项工作提供了一个统一的视角和方法，通过定义和评估高效的推理质量，可以有效优化大型推理模型的性能。

Abstract: Large Reasoning Models (LRMs) increasingly rely on reasoning traces with complex internal structures. However, existing work lacks a unified answer to three fundamental questions: (1) what defines high-quality reasoning, (2) how to reliably evaluate long, implicitly structured reasoning traces, and (3) how to use such evaluation signals for reasoning optimization. To address these challenges, we provide a unified perspective. (1) We introduce the ME$^2$ principle to characterize reasoning quality along macro- and micro-level concerning efficiency and effectiveness. (2) Built on this principle, we model reasoning traces as directed acyclic graphs (DAGs) and develop a DAG-based pairwise evaluation method, capturing complex reasoning structures. (3) Based on this method, we construct the TRM-Preference dataset and train a Thinking Reward Model (TRM) to evaluate reasoning quality at scale. Experiments show that thinking rewards serve as an effective optimization signal. At test time, selecting better reasoning leads to better outcomes (up to 19.3% gain), and during RL training, thinking rewards enhance reasoning and performance (up to 3.9% gain) across diverse tasks.

</details>


### [230] [How Do Language Models Understand Tables? A Mechanistic Analysis of Cell Location](https://arxiv.org/abs/2602.08548)
*Xuanliang Zhang,Dingzirui Wang,Keyan Xu,Qingfu Zhu,Wanxiang Che*

Main category: cs.CL

TL;DR: 本文通过分析单元格定位这一原子任务，揭示了表格理解机制的三个阶段：语义绑定、坐标定位和信息提取，展示了模型如何通过计数离散分隔符来解析坐标，使用线性子空间编码列索引，并通过复用相同的注意力头进行多单元格定位，从而解释了转换器架构中的表格理解过程。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型（LLMs）在处理表格相关任务时其内部机制仍不明确，本文旨在探索表格理解的过程并揭示具体的机制。

Method: 通过对单元格定位任务进行细粒度分析，并利用激活图修补和补充解释技术，本文将表格理解过程分解为顺序的三个阶段：语义绑定、坐标定位和信息提取。

Result: 研究表明模型通过计数离散分隔符来解析坐标，列索引在一组线性子空间中编码，可以精确地引导模型焦点，与此同时，模型还能通过复用在原子定位过程中识别的相同注意力头来进行多单元格的定位。

Conclusion: 本文提供了Transformer架构中表格理解过程的全面解释，突出了模型在处理表格相关内容时使用的方法及其内部机制。

Abstract: While Large Language Models (LLMs) are increasingly deployed for table-related tasks, the internal mechanisms enabling them to process linearized two-dimensional structured tables remain opaque. In this work, we investigate the process of table understanding by dissecting the atomic task of cell location. Through activation patching and complementary interpretability techniques, we delineate the table understanding mechanism into a sequential three-stage pipeline: Semantic Binding, Coordinate Localization, and Information Extraction. We demonstrate that models locate the target cell via an ordinal mechanism that counts discrete delimiters to resolve coordinates. Furthermore, column indices are encoded within a linear subspace that allows for precise steering of model focus through vector arithmetic. Finally, we reveal that models generalize to multi-cell location tasks by multiplexing the identical attention heads identified during atomic location. Our findings provide a comprehensive explanation of table understanding within Transformer architectures.

</details>


### [231] [Beyond Scalar Scores: Reinforcement Learning for Error-Aware Quality Estimation of Machine Translation](https://arxiv.org/abs/2602.08600)
*Archchana Sindhujan,Girish A. Koushik,Shenbin Qian,Diptesh Kanojia,Constantin Orăsan*

Main category: cs.CL

TL;DR: 该研究提出了一种用于低资源语言对（英语到马拉雅拉姆语）的质量估计数据集，并引入了ALOPE-RL框架，使用误差感知奖励和紧凑型LLM实现在少量数据和计算资源下达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 之前的方法主要依赖于数值质量评分，未能提供关于翻译错误的具体信息，并且在低资源语言对上表现不佳。

Method: 研究首先构建了首个用于英-马拉雅拉姆语段级质量估计的数据集，包含人工注释的直接评估得分和翻译质量备注。然后，研究提出了一种基于策略的强化学习框架ALOPE-RL，利用来自DA得分和TQR的奖励训练高效的适配器。

Result: ALOPE-RL能够在使用小于40亿参数的紧凑型LLM、LoRA微调和4位量化进行训练时，在英-马拉雅拉姆语的质量估计任务上达到SOTA性能，超过了更大规模的LLM基线和领先的编码器基质量估计模型。

Conclusion: 研究结果表明，基于策略的强化学习方法能够提供明确的翻译错误描述，并在资源有限的条件下实现高质量估计。

Abstract: Quality Estimation (QE) aims to assess the quality of machine translation (MT) outputs without relying on reference translations, making it essential for real-world, large-scale MT evaluation. Large Language Models (LLMs) have shown significant promise in advancing the field of quality estimation of machine translation. However, most of the QE approaches solely rely on scalar quality scores, offering no explicit information about the translation errors that should drive these judgments. Moreover, for low-resource languages where annotated QE data is limited, existing approaches struggle to achieve reliable performance. To address these challenges, we introduce the first segment-level QE dataset for English to Malayalam, a severely resource-scarce language pair in the QE domain, comprising human-annotated Direct Assessment (DA) scores and Translation Quality Remarks (TQR), which are short, contextual, free-form annotator comments that describe translation errors. We further introduce ALOPE-RL, a policy-based reinforcement learning framework that trains efficient adapters based on policy rewards derived from DA score and TQR. Integrating error-aware rewards with ALOPE-RL, enables LLMs to reason about translation quality beyond numeric scores. Despite being trained on a small-scale QE dataset, ALOPE-RL achieves state-of-the-art performance on English to Malayalam QE using compact LLMs (<=4B parameters}) fine-tuned with LoRA and 4-bit quantization, outperforming both larger LLM-based baselines and leading encoder-based QE models. Our results demonstrate that error-aware, policy-based learning can deliver strong QE performance under limited data and compute budgets. We release our dataset, code, and trained models to support future research.

</details>


### [232] [VocalNet-MDM: Accelerating Streaming Speech LLM via Self-Distilled Masked Diffusion Modeling](https://arxiv.org/abs/2602.08607)
*Ziyang Cheng,Yuhao Wang,Heyang Liu,Ronghua Wu,Qunshan Gu,Yanfeng Wang,Yu Wang*

Main category: cs.CL

TL;DR: VocalNet-MDM是一款基于Masked Diffusion Modeling的非自回归模型，在仅使用6千小时语音数据训练的情况下，相比自回归模型实现了3.7到10倍的解码加速，并降低了第一片段的延迟。同时保持了竞争力的识别准确率和卓越的文本质量和语音自然度。


<details>
  <summary>Details</summary>
Motivation: 当前的自回归模型在语音生成效率上存在局限，VocalNet-MDM旨在通过引入非自回归模型Masked Diffusion Modeling，解决这一问题，提高语音交互的实时性和效率。

Method: VocalNet-MDM通过提出Hierarchical Block-wise Masking和Iterative Self-Distillation等方法，来适配于流式语音交互，解决了训练推理不匹配和迭代开销大的问题。

Result: 在6千小时的语音数据上训练后，VocalNet-MDM展现了与自回归模型相当甚至更优的识别准确率，同时具备比AR基线快3.7到10倍的解码速度和34%的第一片段延迟降低，展示了其在低延迟、高效语音LLMs中的潜力。

Conclusion: 研究团队证明了Masked Diffusion Modeling作为一种非自回归框架，能够在低延迟和高效的语音大语言模型中实现突破，具有广阔的应用前景。

Abstract: Recent Speech Large Language Models~(LLMs) have achieved impressive capabilities in end-to-end speech interaction. However, the prevailing autoregressive paradigm imposes strict serial constraints, limiting generation efficiency and introducing exposure bias. In this paper, we investigate Masked Diffusion Modeling~(MDM) as a non-autoregressive paradigm for speech LLMs and introduce VocalNet-MDM. To adapt MDM for streaming speech interaction, we address two critical challenges: training-inference mismatch and iterative overhead. We propose Hierarchical Block-wise Masking to align training objectives with the progressive masked states encountered during block diffusion decoding, and Iterative Self-Distillation to compress multi-step refinement into fewer steps for low-latency inference. Trained on a limited scale of only 6K hours of speech data, VocalNet-MDM achieves a 3.7$\times$--10$\times$ decoding speedup and reduces first-chunk latency by 34\% compared to AR baselines. It maintains competitive recognition accuracy while achieving state-of-the-art text quality and speech naturalness, demonstrating that MDM is a promising and scalable alternative for low-latency, efficient speech LLMs.

</details>


### [233] [Do Multilingual LLMs have specialized language heads?](https://arxiv.org/abs/2602.08625)
*Muhammad Naufil*

Main category: cs.CL

TL;DR: 本文探讨了多语言大语言模型是否具有特定于每种语言的语言注意力头，并研究了移除不需语言的特定头部而不影响目标语言性能的可能性，旨在为多语言大语言模型提供更高效的部署策略。


<details>
  <summary>Details</summary>
Motivation: 当前多语言大语言模型在生产部署中面临效率问题，尤其是当仅对支持的一部分语言感兴趣时。现有研究聚焦于翻译模型的语言特定或通用头部，但尚未针对多语言大语言模型进行类似研究。

Method: 本文通过实验探究多语言大语言模型是否专为每种语言设有特定注意力机制，并评估移除非目标语言的特定头部对性能的影响。

Result: 研究表明，多语言大语言模型确实存在特定于每种语言的注意力头，移除非目标语言的特定头部在目标语言上并不削弱性能。

Conclusion: 本文的发现为多语言大语言模型提供了更高效的部署策略，即通过移除不必要的语言注意力头部减少模型复杂度，同时保持目标语言的高准确性。

Abstract: Multilingual large language models (LLMs) have gained significant popularity for their ability to process and generate text across multiple languages. However, deploying these models in production can be inefficient when only a subset of the supported languages is of interest. There has been some research conducted on identifying whether machine translation models have language-specific or language-agnostic heads, however no research has been conducted for multilingual LLMs, to the best of our knowledge, that as we know are capable of performing diverse tasks beyond just translation. This paper explores whether multilingual LLMs have specialized language attention heads for each language, and investigates the possibility of removing language-specific heads for unwanted languages without degrading performance in the targeted languages. Our findings could inform more efficient deployment strategies for multilingual LLMs, enabling reduced model complexity while maintaining high accuracy for targeted languages.

</details>


### [234] [Fundamental Reasoning Paradigms Induce Out-of-Domain Generalization in Language Models](https://arxiv.org/abs/2602.08658)
*Mingzi Cao,Xingwei Tan,Mahmud Akhter,Marco Valentino,Maria Liakata,Xi Wang,Nikolaos Aletras*

Main category: cs.CL

TL;DR: 本文系统性地研究了演绎、归纳和溯因这三种基本推理范式在大语言模型（LLM）中的作用及其相互影响，通过构建数据集并采用多种训练方法，证明了通过这些方法可以显著提升模型的推理能力和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 目前虽然对LLM推理能力的研究较为丰富，但对三大基本推理范式的系统探究仍然不足。本文旨在弥补这一空白，探索并验证这些范式对大语言模型推理行为的影响。

Method: 本文通过收集来自符号任务的推理轨迹数据集，涵盖三种基本推理范式。之后，研究了一系列方法，包括简单的微调和更复杂的模型深度增加或转换为混合模型的方法，以将这些技能传授给大语言模型。最后，对模型进行了全面评估。

Result: 实验证明，通过这些方法可以实现大幅的能力提升和泛化性能增强。在多种现实任务中，模型的表现显著优于常规方法，最高可提升14.60%。

Conclusion: 研究表明，通过有效调用这些基本推理范式的技能，可以显著增强LLM的推理能力，特别是在现实任务中展现出了强大的泛化能力。

Abstract: Deduction, induction, and abduction are fundamental reasoning paradigms, core for human logical thinking. Although improving Large Language Model (LLM) reasoning has attracted significant research efforts, the extent to which the fundamental paradigms induce generalization has yet to be systematically explored. In this study, we shed light on how the interplay between these core paradigms influences LLMs' reasoning behavior. To this end, we first collect a new dataset of reasoning trajectories from symbolic tasks, each targeting one of the three fundamental paradigms, to abstract from concrete world knowledge. Then, we investigate effective ways for inducing these skills into LLMs. We experiment with a battery of methods including simple fine-tuning, and more complex approaches to increase model depth, or transform a dense model to a mixture-of-experts. We comprehensively evaluate induced models on realistic out-of-domain tasks, that are entirely formulated in natural language and contain real-world knowledge. Our results reveal that our approach yields strong generalizability with substantial performance gains (up to $14.60$) across realistic tasks.

</details>


### [235] [Learning to Judge: LLMs Designing and Applying Evaluation Rubrics](https://arxiv.org/abs/2602.08672)
*Clemencia Siro,Pourya Aliannejadi,Mohammad Aliannejadi*

Main category: cs.CL

TL;DR: 该研究利用大型语言模型（LLM）自动生成评估标准并应用，发现LLM在生成可解释的评估维度方面表现出色，但在事实和知识密集型场景中评分可靠性降低。闭源模型如GPT-4o比开源模型如Llama有更好的一致性。


<details>
  <summary>Details</summary>
Motivation: 由于传统的人类制定的评估标准往往静态且与模型内部语言质量表示不一致，研究希望通过引入GER-Eval（生成评估标准评价）来探索LLM是否能够设计并应用自己的评估标准。

Method: 该研究评估了LLM定义的评估标准的语义连贯性、评分可靠性及其与人类标准的一致性。

Result: 研究结果表明，LLM能够可靠地生成可解释的任务导向的评估维度，并在模型内部一致应用，但在事实和知识密集型场景中的评分可靠性下降。闭源模型如GPT-4o在一致性方面比开源模型如Llama更好。

Conclusion: 研究结果将评估定位为LLMs的一项学习语言能力，尽管在各个模型间存在分歧，但仍需开发新方法以同时建模人类和LLM的评估语言，以提高可靠性和可解释性。

Abstract: Large language models (LLMs) are increasingly used as evaluators for natural language generation, applying human-defined rubrics to assess system outputs. However, human rubrics are often static and misaligned with how models internally represent language quality. We introduce GER-Eval (Generating Evaluation Rubrics for Evaluation) to investigate whether LLMs can design and apply their own evaluation rubrics. We evaluate the semantic coherence and scoring reliability of LLM-defined criteria and their alignment with human criteria. LLMs reliably generate interpretable and task-aware evaluation dimensions and apply them consistently within models, but their scoring reliability degrades in factual and knowledge-intensive settings. Closed-source models such as GPT-4o achieve higher agreement and cross-model generalization than open-weight models such as Llama. Our findings position evaluation as a learned linguistic capability of LLMs, consistent within models but fragmented across them, and call for new methods that jointly model human and LLM evaluative language to improve reliability and interpretability.

</details>


### [236] [Challenges in Translating Technical Lectures: Insights from the NPTEL](https://arxiv.org/abs/2602.08698)
*Basudha Raje,Sadanand Venkatraman,Nandana TP,Soumyadeepa Das,Polkam Poojitha,M. Vijaykumar,Tanima Bagchi,Hema A. Murthy*

Main category: cs.CL

TL;DR: 本文探讨了机器翻译在印度语言如孟加拉语、马拉雅拉姆语和泰卢固语中的实际应用及方法论含义，尤其是在新兴翻译流程中的应用，并与现有的评估框架进行对比。


<details>
  <summary>Details</summary>
Motivation: 本文选择这些语言是为了展现语言多样性的三角测量，这突显了教育技术中多语言适应的重要性，特别是在NEP 2020背景下。此外，该研究还利用了最大的MOOC平台NPTEL的数据。

Method: 本文通过分析自发性口语语料库来研究机器翻译在保持技术概念清晰表达中的作用，特别关注保留适当的语体和词汇选择。

Result: 研究结果强调了针对特定度量的敏感性，并指出了测试表面重叠度量时面对的挑战，这些挑战源自高形态丰富的语言和语义紧凑的特性。

Conclusion: 本文指出了在印度这样的多元国家中，机器翻译方法在实现教育技术中的多语言适应性上面临的具体挑战，并提出了相应的研究发现。

Abstract: This study examines the practical applications and methodological implications of Machine Translation in Indian Languages, specifically Bangla, Malayalam, and Telugu, within emerging translation workflows and in relation to existing evaluation frameworks. The choice of languages prioritized in this study is motivated by a triangulation of linguistic diversity, which illustrates the significance of multilingual accommodation of educational technology under NEP 2020. This is further supported by the largest MOOC portal, i.e., NPTEL, which has served as a corpus to facilitate the arguments presented in this paper. The curation of a spontaneous speech corpora that accounts for lucid delivery of technical concepts, considering the retention of suitable register and lexical choices are crucial in a diverse country like India. The findings of this study highlight metric-specific sensitivity and the challenges of morphologically rich and semantically compact features when tested against surface overlapping metrics.

</details>


### [237] [Do Images Clarify? A Study on the Effect of Images on Clarifying Questions in Conversational Search](https://arxiv.org/abs/2602.08700)
*Clemencia Siro,Zahra Abbasiantaeb,Yifei Yuan,Mohammad Aliannejadi,Maarten de Rijke*

Main category: cs.CL

TL;DR: 该研究通过用户研究调查了图像在基于图像和文本的澄清问题中的作用，发现图像在回答澄清问题时有助于保持不同熟练水平用户的参与度，但在重新构建查询时则导致了更为精确的查询并提高了检索性能。文本仅设置在回答澄清问题时表现出更好的用户性能。


<details>
  <summary>Details</summary>
Motivation: 随着对话搜索系统采用澄清问题以细化用户查询和提高搜索体验，该研究旨在探索图像在这种情境下对用户表现的影响，尤其关注两种任务中的多模态与纯文本澄清问题的效果。

Method: 研究设计了用户研究，涉及73名参与者，评估了图像在澄清问题回答和查询重构任务中的作用，并对比了纯文本和多模态澄清问题的效果。

Result: 结果显示参与者更喜欢图像辅助的澄清问题以保持参与度，但在查询重构时偏好纯文本设置；图像对不同任务和用户熟练水平的影响不同。在回答澄清问题时，图像有助于维持不同熟练水平用户的参与度；在查询重构时，图像导致更准确的查询并改善检索结果。

Conclusion: 研究结果表明视觉增强的效果取决于任务和用户特征，并强调在特定搜索环境下应战略性地实施视觉增强功能。

Abstract: Conversational search systems increasingly employ clarifying questions to refine user queries and improve the search experience. Previous studies have demonstrated the usefulness of text-based clarifying questions in enhancing both retrieval performance and user experience. While images have been shown to improve retrieval performance in various contexts, their impact on user performance when incorporated into clarifying questions remains largely unexplored. We conduct a user study with 73 participants to investigate the role of images in conversational search, specifically examining their effects on two search-related tasks: (i) answering clarifying questions and (ii) query reformulation. We compare the effect of multimodal and text-only clarifying questions in both tasks within a conversational search context from various perspectives. Our findings reveal that while participants showed a strong preference for multimodal questions when answering clarifying questions, preferences were more balanced in the query reformulation task. The impact of images varied with both task type and user expertise. In answering clarifying questions, images helped maintain engagement across different expertise levels, while in query reformulation they led to more precise queries and improved retrieval performance. Interestingly, for clarifying question answering, text-only setups demonstrated better user performance as they provided more comprehensive textual information in the absence of images. These results provide valuable insights for designing effective multimodal conversational search systems, highlighting that the benefits of visual augmentation are task-dependent and should be strategically implemented based on the specific search context and user characteristics.

</details>


### [238] [FactSim: Fact-Checking for Opinion Summarization](https://arxiv.org/abs/2602.08709)
*Leandro Anghinoni,Jorge Sanchez*

Main category: cs.CL

TL;DR: 本文提出了一种新的全自动化方法，用于评估机器生成的摘要在意见总结任务中的事实一致性。


<details>
  <summary>Details</summary>
Motivation: 传统的自动化评价技术在使用大型语言模型时显示出局限性，本文旨在通过提出一种新的评价方法来解决这些问题。

Method: 通过测量机器生成的摘要中的声明与原始评论中的声明之间的相似性，覆盖范围和一致性来评估其事实一致性。

Result: 该方法能够给出相似声明的高分，即使声明被否定、改写或扩展。与最先进的指标相比，该评分高度相关于人类判断。

Conclusion: 本文提出的方法提供了对生成性人工智能在意见总结任务中表现的新颖评价方式。

Abstract: We explore the need for more comprehensive and precise evaluation techniques for generative artificial intelligence (GenAI) in text summarization tasks, specifically in the area of opinion summarization. Traditional methods, which leverage automated metrics to compare machine-generated summaries from a collection of opinion pieces, e.g. product reviews, have shown limitations due to the paradigm shift introduced by large language models (LLM). This paper addresses these shortcomings by proposing a novel, fully automated methodology for assessing the factual consistency of such summaries. The method is based on measuring the similarity between the claims in a given summary with those from the original reviews, measuring the coverage and consistency of the generated summary. To do so, we rely on a simple approach to extract factual assessment from texts that we then compare and summarize in a suitable score. We demonstrate that the proposed metric attributes higher scores to similar claims, regardless of whether the claim is negated, paraphrased, or expanded, and that the score has a high correlation to human judgment when compared to state-of-the-art metrics.

</details>


### [239] [PERSPECTRA: A Scalable and Configurable Pluralist Benchmark of Perspectives from Arguments](https://arxiv.org/abs/2602.08716)
*Shangrui Nie,Kian Omoomi,Lucie Flek,Zhixue Zhao,Charles Welch*

Main category: cs.CL

TL;DR: PERSPECTRA是一个基准测试，它结合了Kialo的结构清晰性和Reddit的语言多样性，提供了对论据的多种自然变异，用于评估多个观点的识别、匹配和推理能力，发现现有模型在某些方面表现不佳。


<details>
  <summary>Details</summary>
Motivation: 研究开发能够忠实地反映人类多样的大型语言模型，当前的对齐研究尚未充分关注多元化。

Method: PERSPECTRA通过结合Kialo的结构和Reddit的多样性，构建了一个多观点基准测试，包含3,810个自然变异的论据。

Result: 实验发现现有模型在观点识别、匹配和推理方面存在系统性错误，证明了七对相对的论点可以更准确地衡量模型。

Conclusion: PERSPECTRA是第一个可扩展且可配置的基准，用于评估模型如何表示、区分和推理多个角度，有助于改进LLM对多元化的理解和处理。

Abstract: Pluralism, the capacity to engage with diverse perspectives without collapsing them into a single viewpoint, is critical for developing large language models that faithfully reflect human heterogeneity. Yet this characteristic has not been carefully examined in the LLM research community and remains absent from most alignment studies. Debate-oriented sources provide a natural entry point for pluralism research. Previous work builds on online debate sources but remains constrained by costly human validation. Other debate-rich platforms such as Reddit and Kialo also offer promising material: Reddit provides linguistic diversity and scale but lacks clear argumentative structure, while Kialo supplies explicit pro/con graphs but remains overly concise and detached from natural discourse. We introduce PERSPECTRA, a pluralist benchmark that integrates the structural clarity of Kialo debate graphs with the linguistic diversity of real Reddit discussions. Using a controlled retrieval-and-expansion pipeline, we construct 3,810 enriched arguments spanning 762 pro/con stances on 100 controversial topics. Each opinion is expanded to multiple naturalistic variants, enabling robust evaluation of pluralism. We initialise three tasks with PERSPECTRA: opinion counting (identifying distinct viewpoints), opinion matching (aligning supporting stances and discourse to source opinions), and polarity check (inferring aggregate stance in mixed discourse). Experiments with state-of-the-art open-source and proprietary LLMs, highlight systematic failures, such as overestimating the number of viewpoints and misclassifying concessive structures, underscoring the difficulty of pluralism-aware understanding and reasoning. By combining diversity with structure, PERSPECTRA establishes the first scalable, configurable benchmark for evaluating how well models represent, distinguish, and reason over multiple perspectives.

</details>


### [240] [Map of Encoders -- Mapping Sentence Encoders using Quantum Relative Entropy](https://arxiv.org/abs/2602.08740)
*Gaifan Zhang,Danushka Bollegala*

Main category: cs.CL

TL;DR: 本文提出了一种方法，通过定量表示和可视化方式比较和展示大量句编码器，构建了一个包含1101个公开可用句编码器的地图，反映了它们之间的关系，并且该地图可以用于预测句编码器在下游任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 详细介绍了一种用来比较和可视化大规模句编码器的方法，以期为预训练句编码器领域提供一个新的视角，同时能够预测它们在下游任务中的表现。

Method: 该方法首先使用一个句子集的嵌入矩阵表示一个句编码器，然后通过计算句编码器的嵌入矩阵的成对内积矩阵，最后生成每个句编码器的特征向量，反映其相对于单位基编码器的量子相对熵。

Result: 该研究成功地创建了一个包含1101个公开可用句编码器的地图，能够准确地反映编码器之间的各种关系，并能够用于预测句编码器在检索和聚类等任务中的性能。

Conclusion: 本研究的方法验证了句编码器之间的关系能够通过创建一个可视化地图进行准确表达，并且这些映射关系能够用于预测句编码器在下游任务中的性能表现。

Abstract: We propose a method to compare and visualise sentence encoders at scale by creating a map of encoders where each sentence encoder is represented in relation to the other sentence encoders. Specifically, we first represent a sentence encoder using an embedding matrix of a sentence set, where each row corresponds to the embedding of a sentence. Next, we compute the Pairwise Inner Product (PIP) matrix for a sentence encoder using its embedding matrix. Finally, we create a feature vector for each sentence encoder reflecting its Quantum Relative Entropy (QRE) with respect to a unit base encoder. We construct a map of encoders covering 1101 publicly available sentence encoders, providing a new perspective of the landscape of the pre-trained sentence encoders. Our map accurately reflects various relationships between encoders, where encoders with similar attributes are proximally located on the map. Moreover, our encoder feature vectors can be used to accurately infer downstream task performance of the encoders, such as in retrieval and clustering tasks, demonstrating the faithfulness of our map.

</details>


### [241] [LakeHopper: Cross Data Lakes Column Type Annotation through Model Adaptation](https://arxiv.org/abs/2602.08793)
*Yushi Sun,Xujia Li,Nan Tang,Quanqing Xu,Chuanhui Yang,Lei Chen*

Main category: cs.CL

TL;DR: LakeHopper 是一个框架，旨在通过最小化目标数据湖需要的标注工作量来适应现有的预训练模式。它通过语言模型交互解决来源与目标之间的知识差距，采用基于聚类的数据选择方案，以及逐步适应目标数据湖的增量微调机制。


<details>
  <summary>Details</summary>
Motivation: 随着数据湖规模的扩大，某些数据源用于训练的高质量标注信息可能不足，导致现有解决方案无法有效应用于新数据湖。因此，我们研究如何利用预训练的语言模型来适应新的数据湖，而不必从头开始进行大量的标注工作。

Method: LakeHopper框架通过语言模型交互确定并解决知识差距，利用基于聚类的数据选择技术挑选未标注的列，并采用增量微调方法，逐步调整源模型以适应目标数据湖。

Result: 实验结果表明，LakeHopper在两个不同设置的数据湖转移任务中都取得了有效性验证。

Conclusion: LakeHopper能够有效减少新数据湖中的标注需求，提供了一种适应新数据湖的预训练语言模型应用方案。

Abstract: Column type annotation is vital for tasks like data cleaning, integration, and visualization. Recent solutions rely on resource-intensive language models fine-tuned on well-annotated columns from a particular set of tables, i.e., a source data lake. In this paper, we study whether we can adapt an existing pre-trained LM-based model to a new (i.e., target) data lake to minimize the annotations required on the new data lake. However, challenges include the source-target knowledge gap, selecting informative target data, and fine-tuning without losing shared knowledge exist. We propose LakeHopper, a framework that identifies and resolves the knowledge gap through LM interactions, employs a cluster-based data selection scheme for unannotated columns, and uses an incremental fine-tuning mechanism that gradually adapts the source model to the target data lake. Our experimental results validate the effectiveness of LakeHopper on two different data lake transfers under both low-resource and high-resource settings.

</details>


### [242] [Affective Flow Language Model for Emotional Support Conversation](https://arxiv.org/abs/2602.08826)
*Chenghui Zou,Ning Wang,Tiesunlong Shen,Luwei Xiao,Chuan Ma,Xiangpeng Li,Rui Mao,Erik Cambria*

Main category: cs.CL

TL;DR: 本文提出了一种名为AFlow的模型，通过引入细粒度的对话前缀监督，逐步建模多轮对话中的情感流动，从而提高情感支持对话中的策略连贯性和共情响应质量。实验结果显示，AFlow在多种情感场景下显著优于基线模型，并且使用开源骨干模型在主要的情感支持对话指标上超过了私有大模型GPT-4o和Claude-3.5。


<details>
  <summary>Details</summary>
Motivation: 现有情感支持对话系统的策略主要依赖稀疏的结果级信号进行调整，这限制了它在复杂多轮对话中的表现。因此，本文提出了AFlow模型，旨在通过细粒度监督逐步建模情感流动，以增强策略连贯性和共情响应。

Method: AFlow引入了一种新的方法，通过预测多步多轮对话中的情感流动来指导内部策略决策。它能够估计搜索轨迹中的中间效用，并学习一致的策略转换。为了进一步提高策略的连贯性和共情响应的质量，AFlow提出了一种子路径级别的流动平衡目标。

Result: 实验结果表明，AFlow在不同的情感场景下与竞争性基线相比取得了显著的改进。特别地，AFlow使用紧凑的开源骨干模型在主要的情感支持对话指标上超越了私有大模型GPT-4o和Claude-3.5。

Conclusion: 本文提出的方法和模型为情感支持对话领域提供了新的解决方案，通过细粒度的监督和流平衡策略，显著提高了策略连贯性和共情响应质量。

Abstract: Large language models (LLMs) have been widely applied to emotional support conversation (ESC). However, complex multi-turn support remains challenging.This is because existing alignment schemes rely on sparse outcome-level signals, thus offering limited supervision for intermediate strategy decisions. To fill this gap, this paper proposes affective flow language model for emotional support conversation (AFlow), a framework that introduces fine-grained supervision on dialogue prefixes by modeling a continuous affective flow along multi-turn trajectories. AFlow can estimate intermediate utility over searched trajectories and learn preference-consistent strategy transitions. To improve strategy coherence and empathetic response quality, a subpath-level flow-balance objective is presented to propagate preference signals to intermediate states. Experiment results show consistent and significant improvements over competitive baselines in diverse emotional contexts. Remarkably, AFlow with a compact open-source backbone outperforms proprietary LMMs such as GPT-4o and Claude-3.5 on major ESC metrics. Our code is available at https://github.com/chzou25-lgtm/AffectiveFlow.

</details>


### [243] [WildReward: Learning Reward Models from In-the-Wild Human Interactions](https://arxiv.org/abs/2602.08829)
*Hao Peng,Yunjia Qi,Xiaozhi Wang,Zijun Yao,Lei Hou,Juanzi Li*

Main category: cs.CL

TL;DR: 本文通过WildChat收集用户反馈并采用顺序回归进行无偏好对的直接训练，提出了WildReward模型。实验表明，WildReward在多个任务上性能与传统reward模型相当甚至更优，且用户多样性有助于提升模型表现。


<details>
  <summary>Details</summary>
Motivation: 通过利用用户在自然交互中的反馈信号来直接训练奖励模型，解决了传统基于偏好对的奖励模型依赖大量人工标注的问题，旨在提高奖励模型的效果和通用性。

Method: 本文的方法包括：1) 使用WildChat作为来源收集用户反馈；2) 利用用户反馈进行顺序回归训练，获得高质量的训练实例；3) 从用户反馈中直接学习，无需偏好对；4) 对WildReward模型进行广泛的实验评估。

Result: 实验结果表明，WildReward模型在多个任务上的表现与传统的奖励模型相当甚至更优。此外，模型对用户反馈的多样性显示出正面反应，更多的用户可以提升模型的效果。

Conclusion: 本文研究表明，直接通过无偏好对的用户反馈训练奖励模型是可行且有效的，未来可以在更多任务和模型中探索该方法的应用。

Abstract: Reward models (RMs) are crucial for the training of large language models (LLMs), yet they typically rely on large-scale human-annotated preference pairs. With the widespread deployment of LLMs, in-the-wild interactions have emerged as a rich source of implicit reward signals. This raises the question: Can we develop reward models directly from in-the-wild interactions? In this work, we explore this possibility by adopting WildChat as an interaction source and proposing a pipeline to extract reliable human feedback, yielding 186k high-quality instances for training WildReward via ordinal regression directly on user feedback without preference pairs. Extensive experiments demonstrate that WildReward achieves comparable or even superior performance compared to conventional reward models, with improved calibration and cross-sample consistency. We also observe that WildReward benefits directly from user diversity, where more users yield stronger reward models. Finally, we apply WildReward to online DPO training and observe significant improvements across various tasks. Code and data are released at https://github.com/THU-KEG/WildReward.

</details>


### [244] [Understanding Dynamic Compute Allocation in Recurrent Transformers](https://arxiv.org/abs/2602.08864)
*Ibraheem Muhammad Moosa,Suhas Lohit,Ye Wang,Moitreya Chatterjee,Wenpeng Yin*

Main category: cs.CL

TL;DR: 本研究重点研究了基于复杂度控制的评价框架，并提出了一种支持变深度计算的统一递归Transformer模型ANIRA，通过系统分析发现，与任务复杂度对齐的计算分配可以自发形成，但并不意味着算法泛化，早期计算决策依赖静态结构线索，而在线停止更接近算法执行状态。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要基于自然语言任务和任务级指标进行评估，无法直接观察和区分计算分配与架构因素，导致计算分配是否真正与复杂度对齐存在疑问。因此，为了更加精准地评估计算分配与复杂度的关系，提出了基于复杂度控制的评估框架和ANIRA模型。

Method: 首先，引入了一个基于参数化复杂度的算法和合成语言任务，用于直接测试计算分配。其次，提出了一种支持变深度计算的统一递归Transformer模型ANIRA。最后，使用该框架进行了系统性的分析。

Result: 实验结果表明，计算分配自发地与任务复杂度对齐，但并不意味着算法具有良好的泛化性能。模型在未见输入规模下表现不佳，尽管分配了更多计算。进一步观察了计算分配的时机，发现早些时候的决策依赖静态结构线索，而在线停止则更接近算法的执行状态。

Conclusion: 研究强调了计算分配与复杂度对齐的重要性，但提出了未来的挑战，即如何设计模型使之不仅在当前任务上表现良好，还能适应未见过的新输入规模。

Abstract: Token-level adaptive computation seeks to reduce inference cost by allocating more computation to harder tokens and less to easier ones. However, prior work is primarily evaluated on natural-language benchmarks using task-level metrics, where token-level difficulty is unobservable and confounded with architectural factors, making it unclear whether compute allocation truly aligns with underlying complexity. We address this gap through three contributions. First, we introduce a complexity-controlled evaluation paradigm using algorithmic and synthetic language tasks with parameterized difficulty, enabling direct testing of token-level compute allocation. Second, we propose ANIRA, a unified recurrent Transformer framework that supports per-token variable-depth computation while isolating compute allocation decisions from other model factors. Third, we use this framework to conduct a systematic analysis of token-level adaptive computation across alignment with complexity, generalization, and decision timing. Our results show that compute allocation aligned with task complexity can emerge without explicit difficulty supervision, but such alignment does not imply algorithmic generalization: models fail to extrapolate to unseen input sizes despite allocating additional computation. We further find that early compute decisions rely on static structural cues, whereas online halting more closely tracks algorithmic execution state.

</details>


### [245] [Is Reasoning Capability Enough for Safety in Long-Context Language Models?](https://arxiv.org/abs/2602.08874)
*Yu Fu,Haz Sameen Shahgir,Huanli Gong,Zhipeng Wei,N. Benjamin Erichson,Yue Dong*

Main category: cs.CL

TL;DR: 研究发现，更强的推理能力并不能保证模型在长上下文推理中的安全性。在一种新型的威胁模型下，模型未能完全拒绝有害意图，反而随着推理时间的努力增加，安全性下降。


<details>
  <summary>Details</summary>
Motivation: 探究在复杂情境下，提升模型的推理能力能否提高其安全性。

Method: 通过设计一种包含有害意图的复杂上下文，并将其分解为片段，研究验证模型的安全性。

Result: 研究发现，拥有更强推理能力的模型并非更安全，模型更倾向于构建有害意图，但在拒绝该意图方面表现不佳。同时，随着上下文的增加，安全性能下降。然而，增加推理解释所需计算量会显著降低攻击成功概率。

Conclusion: 研究结论指出，在长上下文推理中，安全性能不随推理能力的增强而提高，反而可能有所下降。

Abstract: Large language models (LLMs) increasingly combine long-context processing with advanced reasoning, enabling them to retrieve and synthesize information distributed across tens of thousands of tokens. A hypothesis is that stronger reasoning capability should improve safety by helping models recognize harmful intent even when it is not stated explicitly. We test this hypothesis in long-context settings where harmful intent is implicit and must be inferred through reasoning, and find that it does not hold. We introduce compositional reasoning attacks, a new threat model in which a harmful query is decomposed into incomplete fragments that scattered throughout a long context. The model is then prompted with a neutral reasoning query that induces retrieval and synthesis, causing the harmful intent to emerge only after composition. Evaluating 14 frontier LLMs on contexts up to 64k tokens, we uncover three findings: (1) models with stronger general reasoning capability are not more robust to compositional reasoning attacks, often assembling the intent yet failing to refuse; (2) safety alignment consistently degrades as context length increases; and (3) inference-time reasoning effort is a key mitigating factor: increasing inference-time compute reduces attack success by over 50 percentage points on GPT-oss-120b model. Together, these results suggest that safety does not automatically scale with reasoning capability, especially under long-context inference.

</details>


### [246] [GitSearch: Enhancing Community Notes Generation with Gap-Informed Targeted Search](https://arxiv.org/abs/2602.08945)
*Sahajpreet Singh,Kokil Jaidka,Min-Yen Kan*

Main category: cs.CL

TL;DR: 文章引入了一款名为GitSearch的方法，用于解决基于社区的 Moderation 面临的结构挑战，通过解决信息缺口并实时检索相关信息，实现99%的覆盖度和更高的帮助性评分。


<details>
  <summary>Details</summary>
Motivation: 现有AI方法在新型帖子上效果不佳，GitSearch旨在通过利用人类感知的质量差距来提供一种可扩展的替代方案。

Method: GitSearch采用三层流水线：识别信息短缺、执行实时针对性网页检索以解决这些短缺，以及合成符合平台标准的笔记。

Result: 通过GitHub Actions生成了一个包含78,698条带有社区注释的美国政治推文的基准，发现GitSearch的覆盖率接近100%，并在帮助性评分上优于人类生成的笔记。

Conclusion: GitSearch展示了在规模与质量之间取得平衡的检索效果，从而改善了社区Moderation的结构挑战。

Abstract: Community-based moderation offers a scalable alternative to centralized fact-checking, yet it faces significant structural challenges, and existing AI-based methods fail in "cold start" scenarios. To tackle these challenges, we introduce GitSearch (Gap-Informed Targeted Search), a framework that treats human-perceived quality gaps, such as missing context, etc., as first-class signals. GitSearch has a three-stage pipeline: identifying information deficits, executing real-time targeted web-retrieval to resolve them, and synthesizing platform-compliant notes. To facilitate evaluation, we present PolBench, a benchmark of 78,698 U.S. political tweets with their associated Community Notes. We find GitSearch achieves 99% coverage, almost doubling coverage over the state-of-the-art. GitSearch surpasses human-authored helpful notes with a 69% win rate and superior helpfulness scores (3.87 vs. 3.36), demonstrating retrieval effectiveness that balanced the trade-off between scale and quality.

</details>


### [247] [How Should We Model the Probability of a Language?](https://arxiv.org/abs/2602.08951)
*Rasul Dent,Pedro Ortiz Suarez,Thibault Clérice,Benoît Sagot*

Main category: cs.CL

TL;DR: 本文指出，当前语言识别系统覆盖的语言有限，主要原因是将语言识别视为脱节的文字分类问题，这限制了对少数语言的支持。通过重新定义语言识别为路由问题，并利用环境线索增加局部语言的可行性，可以改善对边缘语言的支持。


<details>
  <summary>Details</summary>
Motivation: 当前的商业和研究级别的语言识别系统对大多数语言的支持仍然有限或不存在，这是由于倾向于使用全球固定先验模型来进行无上下文的语言分类。作者认为，这种情况主要是由于语言识别的框定方式导致的。

Method: 该论文提出一种新的方法论，重新定义语言识别为路由问题，特别强调在进行语言识别时利用环境线索来增加局部语言的可行性。这意味着在识别过程中，系统可以考虑到更多语言使用环境中的信息。

Result: 该论文提供了改进语言识别系统覆盖能力的一些潜在方法，从而增加对之前未受到充分关注的语言的支持。

Conclusion: 研究指出，要想更好地支持边缘语言，关键在于重新定义语言识别任务，并在此基础上开发新的方法来整合环境线索，从而增加局部语言的可行性。

Abstract: Of the over 7,000 languages spoken in the world, commercial language identification (LID) systems only reliably identify a few hundred in written form. Research-grade systems extend this coverage under certain circumstances, but for most languages coverage remains patchy or nonexistent. This position paper argues that this situation is largely self-imposed. In particular, it arises from a persistent framing of LID as decontextualized text classification, which obscures the central role of prior probability estimation and is reinforced by institutional incentives that favor global, fixed-prior models. We argue that improving coverage for tail languages requires rethinking LID as a routing problem and developing principled ways to incorporate environmental cues that make languages locally plausible.

</details>


### [248] [Next Concept Prediction in Discrete Latent Space Leads to Stronger Language Models](https://arxiv.org/abs/2602.08984)
*Yuliang Liu,Yunchong Song,Yixuan Wang,Kewen Ge,Alex Lamb,Qipeng Guo,Kai Chen,Bowen Zhou,Zhouhan Lin*

Main category: cs.CL

TL;DR: 提出了一种名为Next Concept Prediction (NCP)的新范式，它是基于Next Token Prediction (NTP)的生成性预训练方法。NCP通过预测跨越多个token的概念，形成更具挑战性的预训练目标。实验证明，NCP能够提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统的标记级预训练模型存在局限，无法捕捉跨越多个token的概念。为了解决这一问题，研究者提出了NCP以增强模型的表示能力。

Method: 研究者提出了一种新的预训练方法，该方法首先使用Vector Quantization量化隐藏状态，构建概念词汇表。模型通过NCP和NTP共同引导参数更新，并利用所生成的概念来指导后续token的生成。模型使用Pythia和GPT-2作为基础模型进行了训练。

Result: 实验结果显示，NCP在13个基准测试上相较于传统的标记级模型取得了稳定的性能提升。此外，持续预训练实验表明，NCP可以在NTP训练的基础上进一步提升模型性能。

Conclusion: 研究表明，NCP通过引入更具挑战性的预训练任务，能够产出更强大的语言模型，提供了一个向更好的语言建模研究方向发展的有希望的途径。

Abstract: We propose Next Concept Prediction (NCP), a generative pretraining paradigm built on top of Next Token Prediction (NTP). NCP predicts discrete concepts that span multiple tokens, thereby forming a more challenging pretraining objective. Our model, ConceptLM, quantizes hidden states using Vector Quantization and constructs a concept vocabulary. It leverages both NCP and NTP to drive parameter updates and generates a concept to guide the generation of the following tokens. We train ConceptLM from scratch at scales ranging from 70M to 1.5B parameters with up to 300B training data, including Pythia and GPT-2 backbones. Results on 13 benchmarks show that NCP yields consistent performance gains over traditional token-level models. Furthermore, continual pretraining experiments on an 8B-parameter Llama model indicate that NCP can further improve an NTP-trained model. Our analysis suggests that NCP leads to more powerful language models by introducing a harder pretraining task, providing a promising path toward better language modeling.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [249] [Antiferromagnetic Tunnel Junctions (AFMTJs) for In-Memory Computing: Modeling and Case Study](https://arxiv.org/abs/2602.08323)
*Yousuf Choudhary,Tosiron Adegbija*

Main category: cs.AR

TL;DR: 本文介绍了首个集成了多子晶格Landau-Lifshitz-Gilbert（LLG）动力学与电路级建模的抗铁磁隧道势垒（AFMTJ）端到端模拟框架，实验表明AFMTJ的写入延迟和能量消耗比传统MTJ分别低8倍和9倍。在结合内存计算架构后，AFMTJ的平均加速比和能效分别比CPU基线高出17.5倍和近20倍，明显优于基于MTJ的IMC。这确立了AFMTJ作为可扩展、低功耗计算的基本元件的潜力。


<details>
  <summary>Details</summary>
Motivation: 由于抗铁磁隧道势垒（AFMTJ）能够实现皮秒级别的开关速度和飞焦级别的写入能，因此迫切需要一种综合多子晶格动力学和电路水平建模的仿真框架来深入理解其性能。除了基本的研究动机外，建立这种框架也是为了克服传统磁性隧道结（MTJ）在集成内存计算架构中的性能瓶颈。

Method: 该研究建立了一个端到端的模拟框架，它集成了多子晶格的Landau-Lifshitz-Gilbert（LLG）动力学和电路级建模，并使用SPICE进行了基于电路的模拟实验。

Result: 实验结果显示，基于AFMTJ的系统在写入延迟和写入能方面分别比传统MTJ低8倍和9倍。当结合内存计算架构时，与CPU基线相比，AFMTJ可以提供17.5倍的平均加速比和几乎20倍的能效提升。

Conclusion: 该研究证实了AFMTJ作为低功耗、可扩展计算的基础组件的潜力，特别是在内存计算架构中。

Abstract: Antiferromagnetic Tunnel Junctions (AFMTJs) enable picosecond switching and femtojoule writes through ultrafast sublattice dynamics. We present the first end-to-end AFMTJ simulation framework integrating multi-sublattice Landau-Lifshitz-Gilbert (LLG) dynamics with circuit-level modeling. SPICE-based simulations show that AFMTJs achieve ~8x lower write latency and ~9x lower write energy than conventional MTJs. When integrated into an in-memory computing architecture, AFMTJs deliver 17.5x average speedup and nearly 20x energy savings versus a CPU baseline-significantly outperforming MTJ-based IMC. These results establish AFMTJs as a compelling primitive for scalable, low-power computing.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [250] [Is there "Secret Sauce'' in Large Language Model Development?](https://arxiv.org/abs/2602.07238)
*Matthias Mertens,Natalia Fischl-Lanzoni,Neil Thompson*

Main category: cs.AI

TL;DR: 研究使用2022-2025年间发布的809个模型的训练和基准数据，分析了LLM性能差异的主要驱动因素。研究发现，领先开发者具备特定的效率优势，但这些优势在模型性能分布中的重要性不同。在性能前沿，80-90%的性能差异可通过更高的训练计算量来解释，表明规模而非专有技术驱动了前沿创新。而在前沿之外，专用技术和共享算法进展显著减少了达到固定能力阈值所需的计算量。


<details>
  <summary>Details</summary>
Motivation: 研究动机是为了探讨在大型语言模型性能上是否存在依赖于开发者的专有技术，还是主要由计算量的增加所驱动。研究者使用大量的模型数据来进行分析，探索了这个领域的重要发现。

Method: 研究使用了包含809个模型的训练和基准数据，这些模型发布于2022年至2025年之间。通过构建带有发布日期和开发者固定效应的扩展律回归模型进行分析。

Result: 研究结果表明，存在开发者的效率优势，尤其是在性能分布的前沿，高达80-90%的性能差异可归因于较大的训练计算量。然而，在性能分布的其他部分，专用技术与共享算法的进步显著减少了所需的计算量。此外，同一公司内也存在显著的模型效率差异，即可训练两个具有超过40倍计算效率差别的模型。

Conclusion: 研究结论指出，虽然规模在驱动性能提升方面扮演了主要角色，但专有技术和算法的进步在某些情况下同样至关重要，并且对模型效率的影响范围广泛，可能在公司内部也是如此。这些发现对AI行业的领导地位和能力扩散有着深远的含义。

Abstract: Do leading LLM developers possess a proprietary ``secret sauce'', or is LLM performance driven by scaling up compute? Using training and benchmark data for 809 models released between 2022 and 2025, we estimate scaling-law regressions with release-date and developer fixed effects. We find clear evidence of developer-specific efficiency advantages, but their importance depends on where models lie in the performance distribution. At the frontier, 80-90% of performance differences are explained by higher training compute, implying that scale--not proprietary technology--drives frontier advances. Away from the frontier, however, proprietary techniques and shared algorithmic progress substantially reduce the compute required to reach fixed capability thresholds. Some companies can systematically produce smaller models more efficiently. Strikingly, we also find substantial variation of model efficiency within companies; a firm can train two models with more than 40x compute efficiency difference. We also discuss the implications for AI leadership and capability diffusion.

</details>


### [251] [From Out-of-Distribution Detection to Hallucination Detection: A Geometric View](https://arxiv.org/abs/2602.07253)
*Litian Liu,Reza Pourreza,Yubing Jian,Yao Qin,Roland Memisevic*

Main category: cs.AI

TL;DR: 通过将语言模型的下一个标记预测重新定义为分类任务，并结合离分布（OOD）检测技术，本文提出了一种新的无训练、单样本的幻觉检测方法，提高了推理任务中的幻觉检测准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的幻觉检测方法在问答任务中表现优异，但在需要推理的任务中效果较差。因此，本文通过重新定义幻觉检测为离分布（OOD）检测问题，利用计算机视觉领域的相关技术来提升大型语言模型在推理任务中的幻觉检测能力。

Method: 将语言模型的下一个标记预测问题转化为分类任务，并采用离分布检测方法，从而无需额外训练即可识别单个样本是否为不合理的输出。

Result: 利用这种新的方法，本文实现了一种无需训练且只需单个样本就能在推理任务中准确检测幻觉的方法。

Conclusion: 本文的研究表明，将幻觉检测问题重新定义为离分布检测问题，为大型语言模型的安全性提供了一条有前途且可扩展的途径。

Abstract: Detecting hallucinations in large language models is a critical open problem with significant implications for safety and reliability. While existing hallucination detection methods achieve strong performance in question-answering tasks, they remain less effective on tasks requiring reasoning. In this work, we revisit hallucination detection through the lens of out-of-distribution (OOD) detection, a well-studied problem in areas like computer vision. Treating next-token prediction in language models as a classification task allows us to apply OOD techniques, provided appropriate modifications are made to account for the structural differences in large language models. We show that OOD-based approaches yield training-free, single-sample-based detectors, achieving strong accuracy in hallucination detection for reasoning tasks. Overall, our work suggests that reframing hallucination detection as OOD detection provides a promising and scalable pathway toward language model safety.

</details>


### [252] [Incentive-Aware AI Safety via Strategic Resource Allocation: A Stackelberg Security Games Perspective](https://arxiv.org/abs/2602.07259)
*Cheol Woo Kim,Davin Choo,Tzeh Yuan Neoh,Milind Tambe*

Main category: cs.AI

TL;DR: 该摘要提出了一种新的AI安全视角，基于Stackelberg安全博弈（SSGs），强调AI监控不仅仅是静态优化模型对其期望行为的调整，而是将AI监控视为防御者（审计员、评估者和部署者）和攻击者（恶意行为者、未对齐的贡献者或最坏情况的失败模式）之间的战略互动。


<details>
  <summary>Details</summary>
Motivation: 当前的安全框架主要将模型对齐视为静态优化问题，而忽视了数据收集、模型评估和部署中动态和对抗性的激励机制。因此，该研究设计了一种基于Stackelberg安全博弈的新视角来解决这一问题。

Method: 该论文首先介绍了Stackelberg安全博弈理论，并将其应用于AI安全领域。通过考虑防御者和攻击者之间的对抗性资源分配，提出了一个新的综合框架，用于应对AI生命周期中的激励设计、有限的审核能力、对抗性不确定性等问题。

Result: 通过该框架，研究提出了一些具体的应用场景：包括训练期间防止数据/反馈中毒的审计、在资源受限的情况下进行预部署评估、以及在对抗环境中进行鲁棒多模型部署。这些应用展示了游戏论威慑如何使AI监控具有前瞻性、风险意识，并具备抵御操控的能力。

Conclusion: 该研究的工作为算法对齐和机构监督设计之间建立了桥梁，旨在使AI监控更加主动、风险意识强且对操控具有韧性。

Abstract: As AI systems grow more capable and autonomous, ensuring their safety and reliability requires not only model-level alignment but also strategic oversight of the humans and institutions involved in their development and deployment. Existing safety frameworks largely treat alignment as a static optimization problem (e.g., tuning models to desired behavior) while overlooking the dynamic, adversarial incentives that shape how data are collected, how models are evaluated, and how they are ultimately deployed. We propose a new perspective on AI safety grounded in Stackelberg Security Games (SSGs): a class of game-theoretic models designed for adversarial resource allocation under uncertainty. By viewing AI oversight as a strategic interaction between defenders (auditors, evaluators, and deployers) and attackers (malicious actors, misaligned contributors, or worst-case failure modes), SSGs provide a unifying framework for reasoning about incentive design, limited oversight capacity, and adversarial uncertainty across the AI lifecycle. We illustrate how this framework can inform (1) training-time auditing against data/feedback poisoning, (2) pre-deployment evaluation under constrained reviewer resources, and (3) robust multi-model deployment in adversarial environments. This synthesis bridges algorithmic alignment and institutional oversight design, highlighting how game-theoretic deterrence can make AI oversight proactive, risk-aware, and resilient to manipulation.

</details>


### [253] [BRIDGE: Predicting Human Task Completion Time From Model Performance](https://arxiv.org/abs/2602.07267)
*Fengyuan Liu,Jay Gala,Nilaksh,Dzmitry Bahdanau,Siva Reddy,Hugo Larochelle*

Main category: cs.AI

TL;DR: 该工作提出了一种名为BRIDGE的心理测量框架，该框架通过学习模型响应中的潜在难度尺度，并将其与人类任务完成时间锚定，以评估AI系统的实际能力。该框架使用二参数逻辑项目反应理论模型，可以从模型在多个基准上的表现数据中联合估计潜在的任务难度和模型能力。


<details>
  <summary>Details</summary>
Motivation: 现有依赖直接人类任务完成时间注释的方法成本高昂、噪音大且难以扩展到不同基准。BRIDGE旨在提供一种更有效的评估AI系统实际能力的方法。

Method: BRIDGE利用二参数逻辑项目反应理论模型，通过分析模型在多个基准上的表现数据，来联合估计潜在的任务难度和模型能力。通过将潜在任务难度与人类完成时间对数线性关联，该方法能够仅从模型表现推测新基准的完成时间。

Result: BRIDGE能够在没有直接人类完成时间数据的情况下预测模型能力前台界至人类任务长度，且独立地重现了METR的指数级扩展结果，表明50%可解决任务的水平几乎每6个月翻一番。

Conclusion: BRIDGE为评估AI系统的实际能力提供了一种有效且经济的方法，这将有助于更准确地理解和描述AI与人类技能的对比。

Abstract: Evaluating the real-world capabilities of AI systems requires grounding benchmark performance in human-interpretable measures of task difficulty. Existing approaches that rely on direct human task completion time annotations are costly, noisy, and difficult to scale across benchmarks. In this work, we propose BRIDGE, a unified psychometric framework that learns the latent difficulty scale from model responses and anchors it to human task completion time. Using a two-parameter logistic Item Response Theory model, we jointly estimate latent task difficulty and model capability from model performance data across multiple benchmarks. We demonstrate that latent task difficulty varies linearly with the logarithm of human completion time, allowing human task completion time to be inferred for new benchmarks from model performance alone. Leveraging this alignment, we forecast frontier model capabilities in terms of human task length and independently reproduce METR's exponential scaling results, with the 50% solvable task horizon doubling approximately every 6 months.

</details>


### [254] [TermiGen: High-Fidelity Environment and Robust Trajectory Synthesis for Terminal Agents](https://arxiv.org/abs/2602.07274)
*Kaijie Zhu,Yuzhou Nie,Yijiang Li,Yiming Huang,Jialian Wu,Jiang Liu,Ximeng Sun,Zhenfei Yin,Lun Wang,Zicheng Liu,Emad Barsoum,William Yang Wang,Wenbo Guo*

Main category: cs.AI

TL;DR: 该研究提出了TermiGen，一种合成可验证环境和抗错误专家轨迹的端到端管道，显著提高了终端任务执行的成功率。


<details>
  <summary>Details</summary>
Motivation: 当前开源大语言模型在执行复杂终端任务方面存在环境多样性不足和错误恢复能力差的问题，通过TermiGen解决这些问题，以提高模型性能。

Method: TermiGen采用迭代多智能体改进循环生成功能有效的任务和Docker容器。其次，通过生成器-批评者协议在轨迹收集过程中主动注入错误，生成富含错误纠正周期的数据。

Result: 基于TermiGen生成的数据集进行微调，TermiGen-Qwen2.5-Coder-32B在TerminalBench上的通过率达到了31.3%，打破了开源权重模型的最新记录，超越了现有基线和具备竞争力的专有模型。

Conclusion: 研究证明了通过合成高质量、经验证的环境和错误数据可以显著提升大模型的终端任务执行能力和错误恢复能力。

Abstract: Executing complex terminal tasks remains a significant challenge for open-weight LLMs, constrained by two fundamental limitations. First, high-fidelity, executable training environments are scarce: environments synthesized from real-world repositories are not diverse and scalable, while trajectories synthesized by LLMs suffer from hallucinations. Second, standard instruction tuning uses expert trajectories that rarely exhibit simple mistakes common to smaller models. This creates a distributional mismatch, leaving student models ill-equipped to recover from their own runtime failures. To bridge these gaps, we introduce TermiGen, an end-to-end pipeline for synthesizing verifiable environments and resilient expert trajectories. Termi-Gen first generates functionally valid tasks and Docker containers via an iterative multi-agent refinement loop. Subsequently, we employ a Generator-Critic protocol that actively injects errors during trajectory collection, synthesizing data rich in error-correction cycles. Fine-tuned on this TermiGen-generated dataset, our TermiGen-Qwen2.5-Coder-32B achieves a 31.3% pass rate on TerminalBench. This establishes a new open-weights state-of-the-art, outperforming existing baselines and notably surpassing capable proprietary models such as o4-mini. Dataset is avaiable at https://github.com/ucsb-mlsec/terminal-bench-env.

</details>


### [255] [Adaptive Scaffolding for Cognitive Engagement in an Intelligent Tutoring System](https://arxiv.org/abs/2602.07308)
*Sutapa Dey Tithi,Nazia Alam,Tahreem Yasir,Yang Shi,Xiaoyi Tian,Min Chi,Tiffany Barnes*

Main category: cs.AI

TL;DR: 该研究创建了一个自适应系统，通过动态选择实例来个性化逻辑智能辅导系统中的认知参与水平。对比了贝叶斯知识追踪（BKT）和深度强化学习（DRL）选择实例类型的方法。结果显示，两种自适应策略均提高了学生的测试成绩，但不同类型的学生受益于不同的方法。


<details>
  <summary>Details</summary>
Motivation: 本文探讨了在智能辅导系统中通过自适应方式个性化认知参与的方法，旨在解决如何根据学生的学习状态提供适宜的学习助力的问题。这对于提高学生的学习效果具有重要意义。

Method: 研究采用ICAP（交互持续参与）框架定义了不同认知参与层级，并结合BKT和DRL技术，通过动态选择指导示例和错误示例来实现对认知参与的适应性支撑。

Result: 实验结果表明，两种自适应策略均显著提升了学生的测试成绩。贝叶斯知识追踪在低初始知识学生中效果更佳，能帮助他们缩小与高初始知识学生之间的差距；而深度强化学习在高初始知识学生中表现更优。

Conclusion: 该研究贡献了关于认知参与与自适应性及其对学习结果影响的深入见解，对于理解如何有效个性化学习支持具有重要价值。

Abstract: The ICAP framework defines four cognitive engagement levels: Passive, Active, Constructive, and Interactive, where increased cognitive engagement can yield improved learning. However, personalizing learning activities that elicit the optimal level of cognitive engagement remains a key challenge in intelligent tutoring systems (ITS). In this work, we develop and evaluate a system that adaptively scaffolds cognitive engagement by dynamically selecting worked examples in two different ICAP modes: (active) Guided examples and (constructive) Buggy examples. We compare Bayesian Knowledge Tracing (BKT) and Deep Reinforcement Learning (DRL) as adaptive methods against a non-adaptive baseline method for selecting example type in a logic ITS. Our experiment with 113 students demonstrates that both adaptive policies significantly improved student performance on test problems. BKT yielded the largest improvement in posttest scores for low prior knowledge students, helping them catch up with their high prior knowledge peers, whereas DRL yielded significantly higher posttest scores among high prior knowledge students. This paper contributes new insights into the complex interactions of cognitive engagement and adaptivity and their results on learning outcomes.

</details>


### [256] [RAPiD: Real-time Deterministic Trajectory Planning via Diffusion Behavior Priors for Safe and Efficient Autonomous Driving](https://arxiv.org/abs/2602.07339)
*Ruturaj Reddy,Hrishav Bakul Barua,Junn Yong Loo,Thanh Thi Nguyen,Ganesh Krishnasamy*

Main category: cs.AI

TL;DR: RAPiD通过使用预训练的分数规整策略优化，将基于扩散的过程规划器转化为确定性策略，提升了执行效率并保持了性能。


<details>
  <summary>Details</summary>
Motivation: 为了克服基于扩散的规划器实时部署中的即时性和安全性挑战，该研究提出了一种新的确定性策略提取框架，以提高鲁棒性和效率。

Method: 使用预训练的扩散模型得分函数作为先验引导策略学习，通过分数规整策略优化和采用模仿预测司机控制器的评论者来增强策略安全性与舒适性。

Result: RAPiD在nuPlan封闭回路场景中实现了与扩散基线相当的性能，但速度快8倍，并在基于学习的规划器中达到最先进的泛化性能。

Conclusion: 该研究证明了通过确定性策略替代扩散采样，能够在不牺牲性能的前提下，显著提升实时环境下的部署效率。

Abstract: Diffusion-based trajectory planners have demonstrated strong capability for modeling the multimodal nature of human driving behavior, but their reliance on iterative stochastic sampling poses critical challenges for real-time, safety-critical deployment. In this work, we present RAPiD, a deterministic policy extraction framework that distills a pretrained diffusion-based planner into an efficient policy while eliminating diffusion sampling. Using score-regularized policy optimization, we leverage the score function of a pre-trained diffusion planner as a behavior prior to regularize policy learning. To promote safety and passenger comfort, the policy is optimized using a critic trained to imitate a predictive driver controller, providing dense, safety-focused supervision beyond conventional imitation learning. Evaluations demonstrate that RAPiD achieves competitive performance on closed-loop nuPlan scenarios with an 8x speedup over diffusion baselines, while achieving state-of-the-art generalization among learning-based planners on the interPlan benchmark. The official website of this work is: https://github.com/ruturajreddy/RAPiD.

</details>


### [257] [VGAS: Value-Guided Action-Chunk Selection for Few-Shot Vision-Language-Action Adaptation](https://arxiv.org/abs/2602.07399)
*Changhua Xu,Jie Lu,Junyu Xuan,En Yu*

Main category: cs.AI

TL;DR: VGAS框架通过最佳N选择法在推理时选择符合语义且几何精确的动作片段，结合细粒度几何正则化和价值敏感的动作评估策略，提高了动作执行的成功率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前的VLA模型在新任务学习中表现不稳定，尤其是缺乏演示时。VGAS框架旨在通过几何正则化和基于价值的选择策略来解决这一问题。

Method: VGAS框架分为两部分：第一部分是使用 finetuned VLA 作为高召回率的提案生成器；第二部分引入了Q-Chunk-Former作为基于几何的 transformer 批判者来解决细粒度的几何歧义。此外，还提出了显式几何正则化方法来保持动作分类的稳定性。

Result: 实验和理论分析表明，VGAS框架能够提高在有限演示和分布变化下的成功率和鲁棒性。

Conclusion: VGAS框架提供了一种解决VLA模型在缺乏演示表现不稳定的新任务学习问题的有效方法，展示了其在实际应用中的潜力。

Abstract: Vision--Language--Action (VLA) models bridge multimodal reasoning with physical control, but adapting them to new tasks with scarce demonstrations remains unreliable. While fine-tuned VLA policies often produce semantically plausible trajectories, failures often arise from unresolved geometric ambiguities, where near-miss action candidates lead to divergent execution outcomes under limited supervision. We study few-shot VLA adaptation from a \emph{generation--selection} perspective and propose a novel framework \textbf{VGAS} (\textbf{V}alue-\textbf{G}uided \textbf{A}ction-chunk \textbf{S}election). It performs inference-time best-of-$N$ selection to identify action chunks that are both semantically faithful and geometrically precise. Specifically, \textbf{VGAS} employs a finetuned VLA as a high-recall proposal generator and introduces the \textrm{Q-Chunk-Former}, a geometrically grounded Transformer critic to resolve fine-grained geometric ambiguities. In addition, we propose \textit{Explicit Geometric Regularization} (\texttt{EGR}), which explicitly shapes a discriminative value landscape to preserve action ranking resolution among near-miss candidates while mitigating value instability under scarce supervision. Experiments and theoretical analysis demonstrate that \textbf{VGAS} consistently improves success rates and robustness under limited demonstrations and distribution shifts. Our code is available at https://github.com/Jyugo-15/VGAS.

</details>


### [258] [Are Reasoning LLMs Robust to Interventions on Their Chain-of-Thought?](https://arxiv.org/abs/2602.07470)
*Alexander von Recum,Leander Girrbach,Zeynep Akata*

Main category: cs.AI

TL;DR: 研究了一个控制化的评估框架，通过在固定时间点干扰模型的推理路径，考察了RLLMs在面对各种类型的干扰时的鲁棒性。发现RLLMs在较大模型中更加鲁棒，但若干扰出现在早期会降低鲁棒性。不同类型的干扰（如修饰性、中性和对抗性）对推理路径的影响不同，这些发现有助于更好地理解和改善RLLMs的推理机制。


<details>
  <summary>Details</summary>
Motivation: 为了评估RLLMs推理路径的鲁棒性及其中心恢复机制（如表达疑虑），研究者设计了一种控制化的评估框架，通过在固定时间点应用多种类型的干扰来考查RLLMs。研究者认为理解RLLMs如何维护推理完整性和识别疑虑作为中心恢复机制是重要的。

Method: 研究者使用了一种控制化的评估框架，针对多种类型的干扰（包括修饰性、中性和对抗性）及其对不同大小的RLLMs（在数学、科学和逻辑任务中的表现），进行了评估。研究者应用了一系列干预措施（包括对推理路径的完整、去除和改变）跨越数学、科学和逻辑任务的多个开放权模型。

Result: 研究表明，RLLMs在较大模型中更加鲁棒，并能在多样化的干扰下恢复。然而，鲁棒性在早期干扰时会下降。修饰性干扰会抑制疑虑表达并减少性能，而其他干扰则可能引发疑虑，从而支持恢复。此外，中性和对抗性噪声可能会使推理路径的长度增加超过200%，而修饰性干扰则可能会缩短这些路径但损害准确性。

Conclusion: 研究提供了RLLMs维护推理完整性的新证据，指出疑虑作为一种中心恢复机制的重要性，并揭示了在鲁棒性与效率之间存在的权衡，这应该是未来训练方法需要解决的问题。

Abstract: Reasoning LLMs (RLLMs) generate step-by-step chains of thought (CoTs) before giving an answer, which improves performance on complex tasks and makes reasoning more transparent. But how robust are these reasoning traces to disruptions that occur within them? To address this question, we introduce a controlled evaluation framework that perturbs a model's own CoT at fixed timesteps. We design seven interventions (benign, neutral, and adversarial) and apply them to multiple open-weight RLLMs across Math, Science, and Logic tasks. Our results show that RLLMs are generally robust, reliably recovering from diverse perturbations, with robustness improving with model size and degrading when interventions occur early. However, robustness is not style-invariant: paraphrasing suppresses doubt-like expressions and reduces performance, while other interventions trigger doubt and support recovery. Recovery also carries a cost: neutral and adversarial noise can inflate CoT length by more than 200%, whereas paraphrasing shortens traces but harms accuracy. These findings provide new evidence on how RLLMs maintain reasoning integrity, identify doubt as a central recovery mechanism, and highlight trade-offs between robustness and efficiency that future training methods should address.

</details>


### [259] [Computing the Reachability Value of Posterior-Deterministic POMDPs](https://arxiv.org/abs/2602.07473)
*Nathanaël Fijalkow,Arka Ghosh,Roman Kniazev,Guillermo A. Pérez,Pierre Vandenhove*

Main category: cs.AI

TL;DR: 我们引入了后验确定性的POMDPs，在这种POMDPs中，可以通过当前状态、采取的动作和获得的观测结果来唯一地确定下一个状态。对于这种POMDPs，可以逼近达到给定状态集的最大概率。


<details>
  <summary>Details</summary>
Motivation: 由于传统的POMDP们的已知验证和综合问题通常是不可判定或不可处理的，因此探讨新类别的POMDPs模型变得尤为重要。

Method: 提出了一种新的定义——后验确定性的POMDPs，并证明了在这个模型中，可以逼近到达给定目标状态集的最大概率。

Result: 后验确定性的POMDPs模型使得针对特定设定的问题（例如驱动器镜像POMDP）能够实现概率的近似计算。

Conclusion: 该研究为POMDPs家族提供了一种重要扩展，同时也为解决POMDPs中的复杂性问题提供了新的视角。

Abstract: Partially observable Markov decision processes (POMDPs) are a fundamental model for sequential decision-making under uncertainty. However, many verification and synthesis problems for POMDPs are undecidable or intractable. Most prominently, the seminal result of Madani et al. (2003) states that there is no algorithm that, given a POMDP and a set of target states, can compute the maximal probability of reaching the target states, or even approximate it up to a non-trivial constant. This is in stark contrast to fully observable Markov decision processes (MDPs), where the reachability value can be computed in polynomial time.
  In this work, we introduce posterior-deterministic POMDPs, a novel class of POMDPs. Our main technical contribution is to show that for posterior-deterministic POMDPs, the maximal probability of reaching a given set of states can be approximated up to arbitrary precision.
  A POMDP is posterior-deterministic if the next state can be uniquely determined by the current state, the action taken, and the observation received. While the actual state is generally uncertain in POMDPs, the posterior-deterministic property tells us that once the true state is known it remains known forever. This simple and natural definition includes all MDPs and captures classical non-trivial examples such as the Tiger POMDP (Kaelbling et al. 1998), making it one of the largest known classes of POMDPs for which the reachability value can be approximated.

</details>


### [260] [GraphAgents: Knowledge Graph-Guided Agentic AI for Cross-Domain Materials Design](https://arxiv.org/abs/2602.07491)
*Isabella A. Stewart,Tarjei Paule Hage,Yu-Chuan Hsu,Markus J. Buehler*

Main category: cs.AI

TL;DR: 本文介绍了一种利用大规模知识图谱引导的多智能体框架，以解决材料科学中信息过载的问题，尤其是针对持久性及多氟烷基化合物（PFAS）的替代品设计，该框架通过专门化和分布式推理提高了发现和生成可持续材料设计的能力。


<details>
  <summary>Details</summary>
Motivation: 在材料科学中，创新需要融合不同领域的知识，但传统的单智能体大模型在处理大量信息时容易产生幻觉，为此，研究提出一个多智能体框架，利用大规模知识图谱来辅助材料设计，从而应对信息过载的挑战。

Method: 该研究开发了一个多智能体框架，不同智能体专注于问题分解、证据检索、设计参数提取和图遍历等任务，通过大规模知识图谱来发现跨领域的隐含关联，促进假设生成。此外，框架通过调整图遍历策略，在利用现有知识进行探索性搜索和深度研发之间切换。

Result: 实验结果显示，多智能体框架优于单次提示，证明了分布式专项和关系推理的价值。通过为生物医学管材生成PFAS自由的可持续替代品，该框架展示了其灵活性和实用性。

Conclusion: 本文提出的方法结合了大规模知识图谱和多智能体推理，有助于扩展材料设计的空间，推动了自动生成设计理念的发展。

Abstract: Large Language Models (LLMs) promise to accelerate discovery by reasoning across the expanding scientific landscape. Yet, the challenge is no longer access to information but connecting it in meaningful, domain-spanning ways. In materials science, where innovation demands integrating concepts from molecular chemistry to mechanical performance, this is especially acute. Neither humans nor single-agent LLMs can fully contend with this torrent of information, with the latter often prone to hallucinations. To address this bottleneck, we introduce a multi-agent framework guided by large-scale knowledge graphs to find sustainable substitutes for per- and polyfluoroalkyl substances (PFAS)-chemicals currently under intense regulatory scrutiny. Agents in the framework specialize in problem decomposition, evidence retrieval, design parameter extraction, and graph traversal, uncovering latent connections across distinct knowledge pockets to support hypothesis generation. Ablation studies show that the full multi-agent pipeline outperforms single-shot prompting, underscoring the value of distributed specialization and relational reasoning. We demonstrate that by tailoring graph traversal strategies, the system alternates between exploitative searches focusing on domain-critical outcomes and exploratory searches surfacing emergent cross-connections. Illustrated through the exemplar of biomedical tubing, the framework generates sustainable PFAS-free alternatives that balance tribological performance, thermal stability, chemical resistance, and biocompatibility. This work establishes a framework combining knowledge graphs with multi-agent reasoning to expand the materials design space, showcasing several initial design candidates to demonstrate the approach.

</details>


### [261] [Joint Reward Modeling: Internalizing Chain-of-Thought for Efficient Visual Reward Models](https://arxiv.org/abs/2602.07533)
*Yankai Yang,Yancheng Long,Hongyang Wei,Wei Chen,Tianke Zhang,Kaiyu Jiang,Haonan Fan,Changyi Liu,Jiankang Chen,Kaiyu Tang,Bin Wen,Fan Yang,Tingting Gao,Han Li,Shuo Yang*

Main category: cs.AI

TL;DR: 提出了一个名为联合奖励建模（JRM）的方法，该方法通过共享的视觉语言骨干联合优化偏好学习和语言建模，实现了在MMRB2和EditReward-Bench上的SOTA结果，显著提高了在线强化学习中的稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: 当前的奖励建模方法存在局限性：区别性奖励模型能够很好地与人类偏好对齐，但对于复杂的语义理解能力有限；生成性奖励模型虽然具有较强的语义理解能力，但在推理上的效率较低且难以直接与人类偏好对齐。因此，为了结合两者的优势，提出了JRM方法。

Method: JRM方法通过共享的视觉语言模型（VLM）联合优化偏好表示和语言表示，从而在保持高效性的同时增强了语义理解能力。具体而言，它通过从生成性奖励模型中学到的语义和推理能力，将其内化到区别性表示中。

Result: JRM方法在验证集MMRB2和EditReward-Bench上均达到了SOTA性能，并且增强了在线强化学习任务中的稳定性和效率。

Conclusion: 该研究展示了联合训练可以在奖励建模任务中有效地平衡效率和语义理解能力。

Abstract: Reward models are critical for reinforcement learning from human feedback, as they determine the alignment quality and reliability of generative models. For complex tasks such as image editing, reward models are required to capture global semantic consistency and implicit logical constraints beyond local similarity. Existing reward modeling approaches have clear limitations. Discriminative reward models align well with human preferences but struggle with complex semantics due to limited reasoning supervision. Generative reward models offer stronger semantic understanding and reasoning, but they are costly at inference time and difficult to align directly with human preferences. To this end, we propose Joint Reward Modeling (JRM), which jointly optimizes preference learning and language modeling on a shared vision-language backbone. This approach internalizes the semantic and reasoning capabilities of generative models into efficient discriminative representations, enabling fast and accurate evaluation. JRM achieves state-of-the-art results on MMRB2 and EditReward-Bench, and significantly improves stability and performance in downstream online reinforcement learning. These results show that joint training effectively bridges efficiency and semantic understanding in reward modeling.

</details>


### [262] [MSP-LLM: A Unified Large Language Model Framework for Complete Material Synthesis Planning](https://arxiv.org/abs/2602.07543)
*Heewoong Noh,Gyoung S. Na,Namkyeong Lee,Chanyoung Park*

Main category: cs.AI

TL;DR: 提出了一种名为MSP-LLM的统一LLM框架，通过将材料合成规划（MSP）分解为前驱物预测（PP）和合成操作预测（SOP）两个子任务，利用层次前驱物类型作为合成相关假设，并引入离散材料类作为决策中介，实现化学一致性的决策链。实验表明MSP-LLM在PP、SOP和完整的MSP任务上均优于现有方法，展示了该框架在加速实际材料发现方面有效且可扩展。


<details>
  <summary>Details</summary>
Motivation: 目前，材料合成规划（MSP）仍然是AI驱动材料发现中的一个重大挑战。虽然已经有一些基于AI的方法可以解决MSP中的某些子任务，但尚未形成一种统揽整个MSP任务的方法。本文提出了一种统一的基于LLM的框架MSP-LLM，旨在整体解决MSP问题，以促进实际材料的发现。

Method: MSP-LLM框架通过将MSP任务分解为前驱物预测（PP）和合成操作预测（SOP）两个子任务来处理。该模型引入了离散材料类作为决策中介，用于组织这两个任务，形成一个化学上一致的决策链。利用层次前驱物类型作为合成相关假设，并采用显式条件策略，保留前驱物相关信息在自回归解码过程中。

Result: MSP-LLM在前驱物预测（PP）、合成操作预测（SOP）以及完整的材料合成规划（MSP）任务上的实验结果表明，该方法优于现有方法。

Conclusion: MSP-LLM提供了一种有效且可扩展的框架，用于解决材料合成规划问题，有助于加速实际材料的发现过程。

Abstract: Material synthesis planning (MSP) remains a fundamental and underexplored bottleneck in AI-driven materials discovery, as it requires not only identifying suitable precursor materials but also designing coherent sequences of synthesis operations to realize a target material. Although several AI-based approaches have been proposed to address isolated subtasks of MSP, a unified methodology for solving the entire MSP task has yet to be established. We propose MSP-LLM, a unified LLM-based framework that formulates MSP as a structured process composed of two constituent subproblems: precursor prediction (PP) and synthesis operation prediction (SOP). Our approach introduces a discrete material class as an intermediate decision variable that organizes both tasks into a chemically consistent decision chain. For OP, we further incorporate hierarchical precursor types as synthesis-relevant inductive biases and employ an explicit conditioning strategy that preserves precursor-related information in the autoregressive decoding state. Extensive experiments show that MSP-LLM consistently outperforms existing methods on both PP and SOP, as well as on the complete MSP task, demonstrating an effective and scalable framework for MSP that can accelerate real-world materials discovery.

</details>


### [263] [VERIFY-RL: Verifiable Recursive Decomposition for Reinforcement Learning in Mathematical Reasoning](https://arxiv.org/abs/2602.07559)
*Kaleem Ullah Qasim,Jiashu Zhang,Hao Li,Muhammad Kafeel Shaheen*

Main category: cs.AI

TL;DR: 本文提出了一种名为Verify-RL的框架，通过符号微分确保分解的有效性，并在复杂数学问题求解中实现了显著的准确率提升。


<details>
  <summary>Details</summary>
Motivation: 现有的分解方法往往缺乏严格的数学依据，因此本文旨在通过符号微分来确保问题分解的有效性。

Method: 提出了一种新的框架Verify-RL，其中每个父任务-子任务分解都满足三个可验证条件：严格递减的结构复杂性、解包含性和形式化的规则推导。这种方法利用符号计算进行自动验证，确保分解的有效性。

Result: 实验结果表明，通过消除无效分解可以获得显著的性能提升。在最困难的问题上，准确度从32%提高到了68%，整体提升达到了40%。

Conclusion: 通过符号微分确保问题分解的有效性，避免了现有方法中的许多缺陷，使模型能够更有效地解决复杂的数学问题。

Abstract: Training language models to solve complex mathematical problems benefits from curriculum learning progressively training on simpler subproblems. However, existing decomposition methods are often heuristic, offering no guarantees that subproblems are simpler, that solving them aids the parent task, or that their relationships are mathematically grounded. We observe that symbolic differentiation provides a natural structure for verified decomposition: calculus rules explicitly define how expressions reduce to simpler components with provable properties. We introduce Verify-RL, a framework where every parent-child decomposition satisfies three verifiable conditions: strictly decreasing structural complexity, solution containment, and formal rule derivation. Unlike heuristic methods where a significant fraction of decompositions are invalid our properties admit automatic verification through symbolic computation, achieving "verification by construction" Experiments demonstrate that eliminating invalid decompositions yields sizable gains, accuracy on the hardest problems more than doubles from 32% to 68%, with a 40% relative improvement overall.

</details>


### [264] [SleepMaMi: A Universal Sleep Foundation Model for Integrating Macro- and Micro-structures](https://arxiv.org/abs/2602.07628)
*Keondo Park,Younghoon Na,Yourim Choi,Hyunwoo Ryu,Hyun-Woo Shin,Hyung-Sin Kim*

Main category: cs.AI

TL;DR: SleepMaMi 是一种睡眠基础模型，用于掌握一整晚的睡眠架构和细粒度的信号形态。该模型采用分层双编码器设计，macro-encoder 通过人口统计学指导对比学习来建模整晚的时间依赖性，micro-encoder 则通过混合填充自动编码器和多模态对比目标来捕捉生物信号的短期特性。预训练在超过20,000个 PSG 记录上，SleepMaMi 在多种下游任务中表现出色，具有更强的泛化能力和标签高效适应性。


<details>
  <summary>Details</summary>
Motivation: 在睡眠医学领域，传统的方法主要关注局部微结构特征，忽视了多模式的上下文和大规模睡眠模式。SleepMaMi的引入旨在解决这一问题，通过同时建模整个晚上和短期信号特征，提高了临床睡眠分析中的泛化能力和标签效率。

Method: SleepMaMi 采用分层双编码器架构，包括一个宏观编码器（Macro-Encoder）和一个微观编码器（Micro-Encoder）。宏观编码器通过人口统计学指导对比学习来建模整个晚上的时间依赖性，而微观编码器则通过混合填充自动编码器和多模态对比目标来捕捉信号的短期特性。模型预训练使用了一个包含超过20,000个 PSG 记录的大数据集。

Result: 预训练后，SleepMaMi 在临床睡眠分析的各种下游任务中表现出优越性能，证明了其更强的泛化能力和标签高效的适应能力。

Conclusion: SleepMaMi 的引入和成功应用表明，结合整体和局部特征的模型在改善睡眠医学中传统的任务特定模型方面的潜力巨大。该模型为进一步的睡眠研究和临床分析提供了新的工具。

Abstract: While the shift toward unified foundation models has revolutionized many deep learning domains, sleep medicine remains largely restricted to task-specific models that focus on localized micro-structure features. These approaches often neglect the rich, multi-modal context of Polysomnography (PSG) and fail to capture the global macro-structure of a full night's sleep. To address this, we introduce SleepMaMi , a Sleep Foundation Model engineered to master both hour-long sleep architectures and fine-grained signal morphologies. Our framework utilizes a hierarchical dual-encoder design: a Macro-Encoder to model full-night temporal dependencies and a Micro-Encoder to capture short-term characteristics from biosignals. Macro-Encoder is trained via Demographic-Guided Contrastive Learning, which aligns overnight sleep patterns with objective subject metadata, such as age, sex and BMI to refine global representations. Micro-Encoder is optimized via a hybrid Masked Autoencoder (MAE) and multi-modal contrastive objective. Pre-trained on a massive corpus of $>$20,000 PSG recordings (158K hours),SleepMaMi outperforms existing foundation models across a diverse suite of downstream tasks, demonstrating superior generalizability and label-efficient adaptation for clinical sleep analysis.

</details>


### [265] [Efficient Table Retrieval and Understanding with Multimodal Large Language Models](https://arxiv.org/abs/2602.07642)
*Zhuoyan Xu,Haoyang Fang,Boran Han,Bonan Min,Bernie Wang,Cuixiong Hu,Shuai Zhang*

Main category: cs.AI

TL;DR: TabRAG 提出了一个框架，允许 MLLMs 对包含大量表格图像的集合进行查询并生成答案，相比现有方法取得了显著的提升。


<details>
  <summary>Details</summary>
Motivation: 视觉表示的表格数据在现实世界中广泛应用，但现有方法通常假设表结构已知，而实际应用场景需要从大规模图像集合中识别并推理出相关表格来回答查询。

Method: TabRAG 采用联合训练的视觉-文本基础模型首先检索候选表格，然后利用 MLLMs 进行细粒度的候选排序，最后通过对选定表格的推理生成答案。

Result: 在包含88,161个训练样本和9,819个测试样本、共涉及48,504张独特表格的8个基准数据集上，TabRAG 在检索召回率上提升了7.0%，在回答准确性上提升了6.1%。

Conclusion: TabRAG 为真实世界的表格理解任务提供了切实可行的解决方案，并且在多种基准测试中取得了显著的性能改进。

Abstract: Tabular data is frequently captured in image form across a wide range of real-world scenarios such as financial reports, handwritten records, and document scans. These visual representations pose unique challenges for machine understanding, as they combine both structural and visual complexities. While recent advances in Multimodal Large Language Models (MLLMs) show promising results in table understanding, they typically assume the relevant table is readily available. However, a more practical scenario involves identifying and reasoning over relevant tables from large-scale collections to answer user queries. To address this gap, we propose TabRAG, a framework that enables MLLMs to answer queries over large collections of table images. Our approach first retrieves candidate tables using jointly trained visual-text foundation models, then leverages MLLMs to perform fine-grained reranking of these candidates, and finally employs MLLMs to reason over the selected tables for answer generation. Through extensive experiments on a newly constructed dataset comprising 88,161 training and 9,819 testing samples across 8 benchmarks with 48,504 unique tables, we demonstrate that our framework significantly outperforms existing methods by 7.0% in retrieval recall and 6.1% in answer accuracy, offering a practical solution for real-world table understanding tasks.

</details>


### [266] [ONTrust: A Reference Ontology of Trust](https://arxiv.org/abs/2602.07662)
*Glenda Amaral,Tiago Prince Sales,Riccardo Baratella,Daniele Porello,Renata Guizzardi,Giancarlo Guizzardi*

Main category: cs.AI

TL;DR: 该文提出了一个名为ONTrust的信任本体，用于提供信任的坚实本体论基础，通过统一的基础本体进行规范，并应用于多个领域以证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 基于近年来人工智能的进步和去中心化技术（如区块链）的引入，这些新发展为产品和信息服务的提供以及个体和集体福祉的提升带来了潜力。但其采纳取决于信任。为了构建可信赖的系统，需要为新的信任形式制定法律、法规和治理模式，因此需要对信任进行适当的概念化，以便同时为人类和机器所理解。

Method: 该研究开发了一个名为ONTrust的信任本体，该本体基于统一的基础本体并用OntoUML规范化，应用于多个领域，例如概念建模、企业架构设计、语言评估与重新设计、信任管理、需求工程以及应用于情感人机团队中的可信赖人工智能。

Result: ONTrust正式定义了不同类型的信任概念、信任的不同因素以及信任关系中的风险如何产生。此外，该本体还应用于模拟了两个来自文献的研究案例。

Conclusion: 该研究最终得出了一个可靠的信任本体模型，用于指导信息建模、自动化推理、信息集成和语义互操作性的任务。

Abstract: Trust has stood out more than ever in the light of recent innovations. Some examples are advances in artificial intelligence that make machines more and more humanlike, and the introduction of decentralized technologies (e.g. blockchains), which creates new forms of (decentralized) trust. These new developments have the potential to improve the provision of products and services, as well as to contribute to individual and collective well-being. However, their adoption depends largely on trust. In order to build trustworthy systems, along with defining laws, regulations and proper governance models for new forms of trust, it is necessary to properly conceptualize trust, so that it can be understood both by humans and machines. This paper is the culmination of a long-term research program of providing a solid ontological foundation on trust, by creating reference conceptual models to support information modeling, automated reasoning, information integration and semantic interoperability tasks. To address this, a Reference Ontology of Trust (ONTrust) was developed, grounded on the Unified Foundational Ontology and specified in OntoUML, which has been applied in several initiatives, to demonstrate, for example, how it can be used for conceptual modeling and enterprise architecture design, for language evaluation and (re)design, for trust management, for requirements engineering, and for trustworthy artificial intelligence (AI) in the context of affective Human-AI teaming. ONTrust formally characterizes the concept of trust and its different types, describes the different factors that can influence trust, as well as explains how risk emerges from trust relations. To illustrate the working of ONTrust, the ontology is applied to model two case studies extracted from the literature.

</details>


### [267] [EventCast: Hybrid Demand Forecasting in E-Commerce with LLM-Based Event Knowledge](https://arxiv.org/abs/2602.07695)
*Congcong Hu,Yuang Shi,Fan Huang,Yang Xiang,Zhou Ye,Ming Jin,Shiyu Wang*

Main category: cs.AI

TL;DR: EventCast是一个模块化预测框架，通过结合未来事件知识到时间序列预测中，它在应对突发需求模式变化方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的预测系统在高影响时期（如闪购、节假日活动和突然的政策干预）往往无法准确预测，因为需求模式会突然且不可预测地发生变化。EventCast框架旨在通过结合未来事件知识来提升预测准确性。

Method: EventCast框架使用大型语言模型（LLMs）进行事件驱动的推理，将非结构化的业务数据转化为可解释的文本摘要，这些摘要包括活动计划、假期安排以及卖家激励措施。这些文本摘要与历史需求特征在双塔架构中融合，从而产生准确、可解释且可扩展的预测。

Result: EventCast在实际电商场景下的性能测试中表现出卓越成效，与不含事件知识的基线相比，MAE和MSE分别提高了86.9%和97.7%；与最佳工业基线相比，事件驱动时期降低MAE多达57.0%和MSE高达83.3%。

Conclusion: EventCast已被部署到实际的工业管道中，为动态电商环境中的操作决策提供了可行的解决方案，提升了预测的准确性、解释性和可扩展性。

Abstract: Demand forecasting is a cornerstone of e-commerce operations, directly impacting inventory planning and fulfillment scheduling. However, existing forecasting systems often fail during high-impact periods such as flash sales, holiday campaigns, and sudden policy interventions, where demand patterns shift abruptly and unpredictably. In this paper, we introduce EventCast, a modular forecasting framework that integrates future event knowledge into time-series prediction. Unlike prior approaches that ignore future interventions or directly use large language models (LLMs) for numerical forecasting, EventCast leverages LLMs solely for event-driven reasoning. Unstructured business data, which covers campaigns, holiday schedules, and seller incentives, from existing operational databases, is processed by an LLM that converts it into interpretable textual summaries leveraging world knowledge for cultural nuances and novel event combinations. These summaries are fused with historical demand features within a dual-tower architecture, enabling accurate, explainable, and scalable forecasts. Deployed on real-world e-commerce scenarios spanning 4 countries of 160 regions over 10 months, EventCast achieves up to 86.9% and 97.7% improvement on MAE and MSE compared to the variant without event knowledge, and reduces MAE by up to 57.0% and MSE by 83.3% versus the best industrial baseline during event-driven periods. EventCast has deployed into real-world industrial pipelines since March 2025, offering a practical solution for improving operational decision-making in dynamic e-commerce environments.

</details>


### [268] [Humanizing AI Grading: Student-Centered Insights on Fairness, Trust, Consistency and Transparency](https://arxiv.org/abs/2602.07754)
*Bahare Riahi,Veronica Catete*

Main category: cs.AI

TL;DR: 本研究探讨了27名本科生对基于块状编程的期末项目中人工智能评分系统的感知，重点关注公平、信任、一致性和透明性。研究发现，学生担心AI缺乏上下文理解和个性化。建议构建公正、可信赖的AI系统，体现人类判断、灵活性和同理心，作为受人类监管的辅助工具。此项工作通过促进学生声音的重视和提供人性化AI的设计原则，为伦理评估实践做出了贡献。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能技术在教育中的应用越来越广泛，特别是自动化评分系统的引入，研究者需要深入了解学生们对于这种技术的看法，以确保其使用更加合理、公平和透明。通过本研究，研究者们希望更好地理解学生们对AI评分系统的接受度和接受理由，以及如何通过平衡人工评分和AI评分的优势，提升学习质量和公平性。

Method: 本研究采用量化方法，收集并分析了参与学生对AI评分系统的反馈意见。研究者通过问卷调查的方式，让参与者将AI评分和真人评分进行对比，从而评估系统在公平性、信任度、一致性和透明性方面的表现。

Result: 研究发现，在一定程度上，学生们对AI评分系统的接受度较低，主要因其缺乏上下文理解和个性化反馈。多数学生表达了对AI评分系统结果的一致性和透明性缺乏信心。研究结果强调了在实施自动化评分系统时的重要性，即如何确保技术工具能更好地为学生们提供个性化的学习体验，并增强他们对系统的信任。

Conclusion: 研究结果表明，为了让AI评分系统更加受学生欢迎，需要加强这方面的技术开发，同时强调人机协作的重要性。研究者建议未来的研究应专注于解决当前系统中的公平性问题，并通过设计更人性化的AI工具，提高学生的满意度。此外，研究还为教育者和政策制定者提供了一个更全面的视角，以促进更加公平、透明和伦理的评分方法的发展。

Abstract: This study investigates students' perceptions of Artificial Intelligence (AI) grading systems in an undergraduate computer science course (n = 27), focusing on a block-based programming final project. Guided by the ethical principles framework articulated by Jobin (2019), our study examines fairness, trust, consistency, and transparency in AI grading by comparing AI-generated feedback with original human-graded feedback. Findings reveal concerns about AI's lack of contextual understanding and personalization. We recommend that equitable and trustworthy AI systems reflect human judgment, flexibility, and empathy, serving as supplementary tools under human oversight. This work contributes to ethics-centered assessment practices by amplifying student voices and offering design principles for humanizing AI in designed learning environments.

</details>


### [269] [Do Multi-Agents Dream of Electric Screens? Achieving Perfect Accuracy on AndroidWorld Through Task Decomposition](https://arxiv.org/abs/2602.07787)
*Pierre-Louis Favreau,Jean-Pierre Lo,Clement Guiguet,Charles Simon-Meunier,Nicolas Dehandschoewercker,Allen G. Roush,Judah Goldfeder,Ravid Shwartz-Ziv*

Main category: cs.AI

TL;DR: Minitap 是一个多智能体系统，成功解决了 AndroidWorld 基准测试中的所有任务，并超过人类表现。通过认知分离、确定性后验证和元认知推理，Minitap 改善了单智能体架构的局限，开源发布。


<details>
  <summary>Details</summary>
Motivation: 单智能体架构存在关联推理污染、文字输入检测不足和重复行动循环的缺点，因此需要一个更先进的框架来提高自动化测试的效果。

Method: Minitap 通过六种专门智能体的认知分离、对设备状态的输入验证以及检测循环并调整策略的元认知推理，解决了单智能体架构的问题。

Result: Minitap 在 AndroidWorld 测试中表现出色，获得了100%的成功率，优于人类表现80%。消融实验表明，多智能体分解、验证执行和元认知推理分别贡献了21分、7分和9分。

Conclusion: Minitap 为移动应用测试提供了新的解决方案，解决了单智能体架构的局限性，并通过开源技术促进了该领域的进一步研究和发展。

Abstract: We present Minitap, a multi-agent system that achieves 100% success on the AndroidWorld benchmark, the first to fully solve all 116 tasks and surpassing human performance (80%). We first analyze why single-agent architectures fail: context pollution from mixed reasoning traces, silent text input failures undetected by the agent, and repetitive action loops without escape. Minitap addresses each failure through targeted mechanisms: cognitive separation across six specialized agents, deterministic post-validation of text input against device state, and meta-cognitive reasoning that detects cycles and triggers strategy changes. Ablations show multi-agent decomposition contributes +21 points over single-agent baselines; verified execution adds +7 points; meta-cognition adds +9 points. We release Minitap as open-source software. https://github.com/minitap-ai/mobile-use

</details>


### [270] [Data Darwinism Part I: Unlocking the Value of Scientific Data for Pre-training](https://arxiv.org/abs/2602.07824)
*Yiwei Qin,Zhen Huang,Tiantian Mi,Weiye Si,Chenyang Zhou,Qipeng Guo,Siyuan Feng,Pengfei Liu*

Main category: cs.AI

TL;DR: 该研究引入了Data Darwinism，一种十级分类法，旨在通过先进的模型生成更优质的训练数据，从而提高模型性能。研究通过科学文献构建了一个900亿词的语料库，并利用前沿的大语言模型对原始科学文本进行生成性细化和认知完成。经过全面训练后，基于Data Darwinism处理的数据集在多种基准测试中显示了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 由于数据质量直接影响模型性能，但缺乏系统的数据处理框架，研究旨在通过一种多层次的数据模型共生进化方法来提高模型性能。

Method: 研究构建了一个基于科学文献的大型语料库（900亿词），并利用高级语言模型进行生成性细化和认知完成。同时，研究者对基础模型进行了从头开始的预训练，以确保没有科学内容之前的基线结果，并在大量数据上进行了进一步的训练。

Result: 经过600亿词的进一步训练后，基于Data Darwinism方法的处理效果在多个基准测试中优于基线。特别是在领域特定任务上，性能提升了5.60和8.40个百分点。总体而言，逐级提升到最高等级的数据处理方法能够提供总计1.36的性能增益。

Conclusion: 研究得出结论，高级别数据处理能够释放数据的潜在价值，从而显著提高模型性能。研究还发布了Darwin-Science语料库和从头训练的模型，以推动科学数据和模型的共生进化发展。

Abstract: Data quality determines foundation model performance, yet systematic processing frameworks are lacking. We introduce Data Darwinism, a ten-level taxonomy (L0-L9) that conceptualizes data-model co-evolution: advanced models produce superior data for next-generation systems. We validate this on scientific literature by constructing Darwin-Science, a 900B-token corpus (L0-L5). We identify a learnability gap in raw scientific text, which we bridge via L4 (Generative Refinement) and L5 (Cognitive Completion) using frontier LLMs to explicate reasoning and terminology.
  To ensure rigorous attribution, we pre-trained daVinci-origin-3B/7B models from scratch, excluding scientific content to create contamination-free baselines. After 600B tokens of continued pre-training, Darwin-Science outperforms baselines by +2.12 (3B) and +2.95 (7B) points across 20+ benchmarks, rising to +5.60 and +8.40 points on domain-aligned tasks. Systematic progression to L5 yields a +1.36 total gain, confirming that higher-level processing unlocks latent data value. We release the Darwin-Science corpus and daVinci-origin models to enable principled, co-evolutionary development.

</details>


### [271] [Time Series Reasoning via Process-Verifiable Thinking Data Synthesis and Scheduling for Tailored LLM Reasoning](https://arxiv.org/abs/2602.07830)
*Jiahui Zhou,Dan Li,Boxin Li,Xiao Zhang,Erli Meng,Lin Li,Zhuomin Chen,Jian Lou,See-Kiong Ng*

Main category: cs.AI

TL;DR: VeriTime 使用数据合成、数据调度和强化学习训练框架，提升时间序列推理任务中的大规模语言模型性能。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据的广泛应用，以及大型语言模型通过强化学习增强链式思维推理能力，激励了VeriTime框架的提出。

Method: VeriTime框架包括数据合成、数据调度和强化学习训练三个关键步骤。通过合成TS-text多模态数据集，设计困难与任务类型相关的数据调度机制，以及提出包含验证过程级链式思维推理数据的两阶段强化微调。

Result: 实验证明VeriTime显著提升了时间序列推理任务的性能，使得3B和4B模型达到或超过了大型专用LLM的推理能力。

Conclusion: VeriTime通过精心设计的方法显著提升了时间序列推理任务中的LLM性能，展现出了在这一领域应用潜力。

Abstract: Time series is a pervasive data type across various application domains, rendering the reasonable solving of diverse time series tasks a long-standing goal. Recent advances in large language models (LLMs), especially their reasoning abilities unlocked through reinforcement learning (RL), have opened new opportunities for tackling tasks with long Chain-of-Thought (CoT) reasoning. However, leveraging LLM reasoning for time series remains in its infancy, hindered by the absence of carefully curated time series CoT data for training, limited data efficiency caused by underexplored data scheduling, and the lack of RL algorithms tailored for exploiting such time series CoT data. In this paper, we introduce VeriTime, a framework that tailors LLMs for time series reasoning through data synthesis, data scheduling, and RL training. First, we propose a data synthesis pipeline that constructs a TS-text multimodal dataset with process-verifiable annotations. Second, we design a data scheduling mechanism that arranges training samples according to a principled hierarchy of difficulty and task taxonomy. Third, we develop a two-stage reinforcement finetuning featuring fine-grained, multi-objective rewards that leverage verifiable process-level CoT data. Extensive experiments show that VeriTime substantially boosts LLM performance across diverse time series reasoning tasks. Notably, it enables compact 3B, 4B models to achieve reasoning capabilities on par with or exceeding those of larger proprietary LLMs.

</details>


### [272] [LQA: A Lightweight Quantized-Adaptive Framework for Vision-Language Models on the Edge](https://arxiv.org/abs/2602.07849)
*Xin Wang,Hualin Zhou,Sheng Guang Wang,Ting Dang,Yu Zhang,Hong Jia,Tao Gu*

Main category: cs.AI

TL;DR: 本文提出了一种轻量级的LQA框架，该框架结合了模态感知量化策略和无梯度测试时自适应机制，以实现对资源受限硬件上的视觉-语言模型的鲁棒且高效的部署，并显著优于基于梯度的测试时自适应方法。


<details>
  <summary>Details</summary>
Motivation: 现有针对边缘设备的视觉-语言模型部署方法存在资源约束和性能退化的问题，尤其是在分布变化的情况下。研究旨在提出一种轻量级框架LQA，以克服这些挑战。

Method: LQA框架采用模态感知量化策略和无梯度测试时自适应机制。引入了选择性混合量化（SHQ）以及量化无梯度自适应机制。

Result: 实验结果表明，LQA在多种合成和真实分布变化场景下整体自适应性能提高了4.5%，使用内存低于全精度模型，且在七个开源数据集上较基于梯度的测试时自适应方法的内存使用率低至19.9倍。

Conclusion: 研究表明，LQA为视觉-语言模型在边缘设备上的稳健、隐私保护和高效部署提供了一条实用途径。

Abstract: Deploying Vision-Language Models (VLMs) on edge devices is challenged by resource constraints and performance degradation under distribution shifts. While test-time adaptation (TTA) can counteract such shifts, existing methods are too resource-intensive for on-device deployment. To address this challenge, we propose LQA, a lightweight, quantized-adaptive framework for VLMs that combines a modality-aware quantization strategy with gradient-free test-time adaptation. We introduce Selective Hybrid Quantization (SHQ) and a quantized, gradient-free adaptation mechanism to enable robust and efficient VLM deployment on resource-constrained hardware. Experiments across both synthetic and real-world distribution shifts show that LQA improves overall adaptation performance by 4.5\%, uses less memory than full-precision models, and significantly outperforms gradient-based TTA methods, achieving up to 19.9$\times$ lower memory usage across seven open-source datasets. These results demonstrate that LQA offers a practical pathway for robust, privacy-preserving, and efficient VLM deployment on edge devices.

</details>


### [273] [Emergent Misalignment is Easy, Narrow Misalignment is Hard](https://arxiv.org/abs/2602.07852)
*Anna Soligo,Edward Turner,Senthooran Rajamanoharan,Neel Nanda*

Main category: cs.AI

TL;DR: 研究发现，微调大型语言模型于狭窄有害数据集会导致模型在多种不同场景中产生刻板的‘邪恶’回应。尽管专家事先进行了注册调研未能预测此结果，表明我们对语言模型学习和泛化的归纳偏见理解不足。通过研究这种异常对齐（EM），发现通用对齐方式比狭窄解决方案更稳定、更高效，并提供了一种线性表示通用对齐方式的方法，这对监测和缓解模型偏离对齐至关重要。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过微调大型语言模型在窄数据集上的表现来揭示它们在泛化过程中可能产生的不良行为，这有助于理解并减少这些模型的潜在风险。

Method: 使用异常对齐（EM）作为案例，探讨不同EM微调收敛于相同的线性表示通用对齐，引入KL散度损失以学习这种线性表示，比较这些表示，确定通用对齐具有更低的损耗且更具有鲁棒性。

Result: 研究发现，通用对齐方式相比狭窄的解决方案更加稳定且高效，其线性表示可以通过KL散度损失学习到，相比狭窄解决方案具有更低的损耗和更高鲁棒性。

Conclusion: 这项研究提供了一种共轭表示通用对齐的方式，这对监控和缓解模型偏离对齐至关重要。同时也提出了一种推测分析方法，有助于更深入理解语言模型的学习和泛化潜能的归纳偏见。

Abstract: Finetuning large language models on narrowly harmful datasets can cause them to become emergently misaligned, giving stereotypically `evil' responses across diverse unrelated settings. Concerningly, a pre-registered survey of experts failed to predict this result, highlighting our poor understanding of the inductive biases governing learning and generalisation in LLMs. We use emergent misalignment (EM) as a case study to investigate these inductive biases and find that models can just learn the narrow dataset task, but that the general solution appears to be more stable and more efficient. To establish this, we build on the result that different EM finetunes converge to the same linear representation of general misalignment, which can be used to mediate misaligned behaviour. We find a linear representation of the narrow solution also exists, and can be learned by introducing a KL divergence loss. Comparing these representations reveals that general misalignment achieves lower loss, is more robust to perturbations, and is more influential in the pre-training distribution. This work isolates a concrete representation of general misalignment for monitoring and mitigation. More broadly, it offers a detailed case study and preliminary metrics for investigating how inductive biases shape generalisation in LLMs. We open-source all code, datasets and model finetunes.

</details>


### [274] [MemFly: On-the-Fly Memory Optimization via Information Bottleneck](https://arxiv.org/abs/2602.07885)
*Zhenyuan Zhang,Xianzhang Jia,Zhiqin Yang,Zhenbo Song,Wei Xue,Sirui Han,Yike Guo*

Main category: cs.AI

TL;DR: 本文提出了一种名为MemFly的框架，旨在使大语言模型在保持检索准确性的同时压缩冗余信息。MemFly通过无梯度优化器优化压缩熵和相关熵，构建分层记忆结构，并结合语义、符号和拓扑路径的混合检索机制来处理复杂的多跳查询。


<details>
  <summary>Details</summary>
Motivation: 现有框架在压缩冗余信息和保持下游任务精确检索之间存在根本矛盾。MemFly旨在通过遵循信息瓶颈原则自适应地优化记忆结构以弥合这一差距。

Method: MemFly使用无梯度优化器来最小化压缩熵并最大化相关熵，构建分层记忆结构。同时，它还开发了一种混合检索机制，结合了语义、符号和拓扑路径，并逐步细化以处理复杂的多跳查询。

Result: 全面的实验表明，与最先进的基线相比，MemFly在记忆一致性、响应真实性和准确性方面表现出显著的提升。

Conclusion: 本文提出了一种创新的MemFly框架，其通过优化记忆结构和引入高效的检索机制，显著改善了大语言模型在长程任务中的表现。

Abstract: Long-term memory enables large language model agents to tackle complex tasks through historical interactions. However, existing frameworks encounter a fundamental dilemma between compressing redundant information efficiently and maintaining precise retrieval for downstream tasks. To bridge this gap, we propose MemFly, a framework grounded in information bottleneck principles that facilitates on-the-fly memory evolution for LLMs. Our approach minimizes compression entropy while maximizing relevance entropy via a gradient-free optimizer, constructing a stratified memory structure for efficient storage. To fully leverage MemFly, we develop a hybrid retrieval mechanism that seamlessly integrates semantic, symbolic, and topological pathways, incorporating iterative refinement to handle complex multi-hop queries. Comprehensive experiments demonstrate that MemFly substantially outperforms state-of-the-art baselines in memory coherence, response fidelity, and accuracy.

</details>


### [275] [Selective Fine-Tuning for Targeted and Robust Concept Unlearning](https://arxiv.org/abs/2602.07919)
*Mansi,Avinash Kori,Francesca Toni,Soteris Demetriou*

Main category: cs.AI

TL;DR: TRUST（目标鲁棒选择性微调）是一种新的动态估计目标概念神经元并利用选择性微调进行去学习的方法，通过Hessian正则化增强。实验表明，该方法对抗对抗性提示具有鲁棒性，保持了生成高质量，且比当前最佳方法更快。


<details>
  <summary>Details</summary>
Motivation: 现有概念去学习方法依赖于全微调，计算成本高昂且现有静态方法并不能有效执行动态微调。

Method: TRUST利用Hessian正则化实现动态估计和选择性微调，通过这种方法进行概念去学习，不仅针对单个概念，还针对概念组合和条件概念。

Result: TRUST在对抗性的提示下表现出鲁棒性，在保持生成质量的同时显著减少计算时间。

Conclusion: TRUST方法为概念去学习提供了更高效、更鲁棒的解决方案，比当前最佳方法具有明显优势。

Abstract: Text guided diffusion models are used by millions of users, but can be easily exploited to produce harmful content. Concept unlearning methods aim at reducing the models' likelihood of generating harmful content. Traditionally, this has been tackled at an individual concept level, with only a handful of recent works considering more realistic concept combinations. However, state of the art methods depend on full finetuning, which is computationally expensive. Concept localisation methods can facilitate selective finetuning, but existing techniques are static, resulting in suboptimal utility. In order to tackle these challenges, we propose TRUST (Targeted Robust Selective fine Tuning), a novel approach for dynamically estimating target concept neurons and unlearning them through selective finetuning, empowered by a Hessian based regularization. We show experimentally, against a number of SOTA baselines, that TRUST is robust against adversarial prompts, preserves generation quality to a significant degree, and is also significantly faster than the SOTA. Our method achieves unlearning of not only individual concepts but also combinations of concepts and conditional concepts, without any specific regularization.

</details>


### [276] [Accelerating Social Science Research via Agentic Hypothesization and Experimentation](https://arxiv.org/abs/2602.07983)
*Jishu Sen Gupta,Harini SI,Somesh Kumar Singh,Syed Mohamad Tawseeq,Yaman Kumar Singla,David Doermann,Rajiv Ratn Shah,Balaji Krishnamurthy*

Main category: cs.AI

TL;DR: EXPERIGEN 是一个通过贝叶斯优化启发的两阶段搜索框架，能够提出更多统计显著性假设并具有更高的预测性能；在专家审核和现实生活中的 A/B 测试中均表现出色。


<details>
  <summary>Details</summary>
Motivation: 加速数据驱动的社会科学研究过程，减少研究周期和提高发现效率。

Method: EXPERIGEN 使用贝叶斯优化来驱动一个两阶段搜索过程，其中包含一个生成器提议假设，和一个实验者进行实证评估。

Result: 与之前的方法相比，EXPERIGEN 在多个领域中能够发现 2-4 倍的统计显著性假设，且预测性能提高了 7-17%；在专家审核中，88% 的假设被视为新颖且具有研究价值；在 A/B 测试中，进行了 LLM 生成假设的验证，获得了统计学显著结果。

Conclusion: EXPERIGEN 不仅在统计性能方面表现出色，还能够生成具有一定创新性和实用性的假设，且在现实世界的 A/B 测试中也得到了验证。

Abstract: Data-driven social science research is inherently slow, relying on iterative cycles of observation, hypothesis generation, and experimental validation. While recent data-driven methods promise to accelerate parts of this process, they largely fail to support end-to-end scientific discovery. To address this gap, we introduce EXPERIGEN, an agentic framework that operationalizes end-to-end discovery through a Bayesian optimization inspired two-phase search, in which a Generator proposes candidate hypotheses and an Experimenter evaluates them empirically. Across multiple domains, EXPERIGEN consistently discovers 2-4x more statistically significant hypotheses that are 7-17 percent more predictive than prior approaches, and naturally extends to complex data regimes including multimodal and relational datasets. Beyond statistical performance, hypotheses must be novel, empirically grounded, and actionable to drive real scientific progress. To evaluate these qualities, we conduct an expert review of machine-generated hypotheses, collecting feedback from senior faculty. Among 25 reviewed hypotheses, 88 percent were rated moderately or strongly novel, 70 percent were deemed impactful and worth pursuing, and most demonstrated rigor comparable to senior graduate-level research. Finally, recognizing that ultimate validation requires real-world evidence, we conduct the first A/B test of LLM-generated hypotheses, observing statistically significant results with p less than 1e-6 and a large effect size of 344 percent.

</details>


### [277] [Free(): Learning to Forget in Malloc-Only Reasoning Models](https://arxiv.org/abs/2602.08030)
*Yilun Zheng,Dongyang Ma,Tian Liang,Jiahao Xu,Xinting Huang,Lijie Chen,Haitao Mi,Yan Wang*

Main category: cs.AI

TL;DR: Free()LM 通过引入 Free-Module 和自我遗忘机制，解决了语言模型在推理过程中的冗余计算问题，显著提升了模型的性能，特别是在长周期任务中。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型在推理过程中存在冗余计算的问题，这会导致性能下降。Free()LM 旨在通过增加自我遗忘能力来克服这一挑战。

Method: Free()LM 采用 Free-Module 和 LoRA 调整器来实现自遗忘功能。通过交替推理和清理模式，模型能够识别并删除无用的上下文片段，保持紧凑且无噪声的状态。

Result: Free()LM 在所有模型规模上提供了持续的性能提升，相对于顶级推理基线平均提高了 3.3%。它在 IMOanswerBench 中使用 DeepSeek V3.2-Speciale 达到了新的 SOTA。此外，在标准 Qwen3-235B-A22B 面临完全崩溃的长期任务中，Free()LM 可以恢复到 50% 的性能。

Conclusion: 研究发现，可持续的智能需要拥有忘记的能力，就像能够思考一样重要。Free()LM 的提出为解决语言模型在推理过程中的冗余计算问题提供了一种有效的方法。

Abstract: Reasoning models enhance problem-solving by scaling test-time compute, yet they face a critical paradox: excessive thinking tokens often degrade performance rather than improve it. We attribute this to a fundamental architectural flaw: standard LLMs operate as "malloc-only" engines, continuously accumulating valid and redundant steps alike without a mechanism to prune obsolete information. To break this cycle, we propose Free()LM, a model that introduces an intrinsic self-forgetting capability via the Free-Module, a plug-and-play LoRA adapter. By iteratively switching between reasoning and cleaning modes, Free()LM dynamically identifies and prunes useless context chunks, maintaining a compact and noise-free state.
  Extensive experiments show that Free()LM provides consistent improvements across all model scales (8B to 685B). It achieves a 3.3% average improvement over top-tier reasoning baselines, even establishing a new SOTA on IMOanswerBench using DeepSeek V3.2-Speciale. Most notably, in long-horizon tasks where the standard Qwen3-235B-A22B model suffers a total collapse (0% accuracy), Free()LM restores performance to 50%. Our findings suggest that sustainable intelligence requires the freedom to forget as much as the power to think.

</details>


### [278] [Securing Dual-Use Pathogen Data of Concern](https://arxiv.org/abs/2602.08061)
*Doni Bloomfield,Allison Berke,Moritz S. Hanke,Aaron Maiwald,James R. M. Black,Toby Webster,Tina Hernandez-Boussard,Oliver M. Crook,Jassi Pannu*

Main category: cs.AI

TL;DR: 该研究提出了一个五级生物安全数据水平框架（BDL）来分类病原体数据，并提出了每个风险等级的技术限制，同时建议制定新的治理框架以管理新生成的双重用途病原体数据。


<details>
  <summary>Details</summary>
Motivation: 鉴于AI模型在生物学领域的应用日益增多，训练数据的质量和类型直接影响到模型的安全性和潜在威胁，因此需要建立数据管控机制以防止AI被用于有害目的。

Method: 作者通过分析不同类型的数据对AI模型的影响，制定了一个五级生物安全数据水平框架，并提出了相应的技术限制。

Result: 研究提出了一个涵盖不同类型病原体数据的五级生物安全数据框架，以及针对不同风险级别的技术限制建议。

Conclusion: 作者认为，在计算和编程资源日益普及的今天，数据管控可能是减少有害生物AI能力传播风险最有效的措施之一。

Abstract: Training data is an essential input into creating competent artificial intelligence (AI) models. AI models for biology are trained on large volumes of data, including data related to biological sequences, structures, images, and functions. The type of data used to train a model is intimately tied to the capabilities it ultimately possesses--including those of biosecurity concern. For this reason, an international group of more than 100 researchers at the recent 50th anniversary Asilomar Conference endorsed data controls to prevent the use of AI for harmful applications such as bioweapons development. To help design such controls, we introduce a five-tier Biosecurity Data Level (BDL) framework for categorizing pathogen data. Each level contains specific data types, based on their expected ability to contribute to capabilities of concern when used to train AI models. For each BDL tier, we propose technical restrictions appropriate to its level of risk. Finally, we outline a novel governance framework for newly created dual-use pathogen data. In a world with widely accessible computational and coding resources, data controls may be among the most high-leverage interventions available to reduce the proliferation of concerning biological AI capabilities.

</details>


### [279] [Objective Decoupling in Social Reinforcement Learning: Recovering Ground Truth from Sycophantic Majorities](https://arxiv.org/abs/2602.08092)
*Majid Ghasemi,Mark Crowley*

Main category: cs.AI

TL;DR: 本文挑战了强化学习中基于人类反馈的核心假设，提出了一种新的对齐策略Epistemic Source Alignment (ESA)，并证明其在 adversarial 和 sycophantic 的评估者存在时仍能收敛于真实目标。


<details>
  <summary>Details</summary>
Motivation: 论文旨在解决强化学习中基于人类反馈的传统对齐方法面临的挑战，尤其是在社会环境中，评估者可能表现为奉承、懒惰或敌对。

Method: 论文提出了一个新的对齐策略Epistemic Source Alignment (ESA)，通过利用稀疏的安全公理来判断反馈源而非信号本身。

Result: 理论证明，ESA可以在多数评估者有偏差的情况下保证收敛于真实目标，而传统的方法在这种情况下会失败。

Conclusion: ESA为在复杂社会环境中实现更好的强化学习对齐提供了一种新的解决方案。

Abstract: Contemporary AI alignment strategies rely on a fragile premise: that human feedback, while noisy, remains a fundamentally truthful signal. In this paper, we identify this assumption as Dogma 4 of Reinforcement Learning (RL). We demonstrate that while this dogma holds in static environments, it fails in social settings where evaluators may be sycophantic, lazy, or adversarial. We prove that under Dogma 4, standard RL agents suffer from what we call Objective Decoupling, a structural failure mode where the agent's learned objective permanently separates from the latent ground truth, guaranteeing convergence to misalignment. To resolve this, we propose Epistemic Source Alignment (ESA). Unlike standard robust methods that rely on statistical consensus (trusting the majority), ESA utilizes sparse safety axioms to judge the source of the feedback rather than the signal itself. We prove that this "judging the judges" mechanism guarantees convergence to the true objective, even when a majority of evaluators are biased. Empirically, we show that while traditional consensus methods fail under majority collusion, our approach successfully recovers the optimal policy.

</details>


### [280] [RECUR: Resource Exhaustion Attack via Recursive-Entropy Guided Counterfactual Utilization and Reflection](https://arxiv.org/abs/2602.08214)
*Ziwei Wang,Yuanhe Zhang,Jing Chen,Zhenhong Zhou,Ruichao Liang,Ruiying Du,Ju Jia,Cong Wu,Yang Liu*

Main category: cs.AI

TL;DR: 该研究引入了递归熵来量化反思过程中资源消耗的风险，提出了RECUR，一种通过递归熵引导的反事实利用与反思攻击，显著增加了输出长度和降低了吞吐量，揭示了大规模推理模型在推理过程中的安全问题。


<details>
  <summary>Details</summary>
Motivation: 现有的LRM存在过度推理和资源消耗过大的问题，但由于对推理过程中反映部分的忽视，这些问题未得到充分关注。递归熵的引入旨在量化这种风险，从而揭示推理过程中的内在安全问题。

Method: 研究通过分析递归熵在推理过程中的变化趋势来构建反事实问题，进而发现模型的缺陷及潜在风险，提出了RECUR攻击方法来证明这些问题。

Result: 实验表明，在正常的推理过程中，递归熵呈现明显的下降趋势，而RECUR攻击则打破了这种趋势，显著增加了输出长度高达11倍，同时将吞吐量降低至10%。

Conclusion: 递归熵为评估大型推理模型在推理过程中的资源消耗风险提供了新视角，RECUR攻击展示了这种风险的严重性，对提升推理模型鲁棒性有重要意义。

Abstract: Large Reasoning Models (LRMs) employ reasoning to address complex tasks. Such explicit reasoning requires extended context lengths, resulting in substantially higher resource consumption. Prior work has shown that adversarially crafted inputs can trigger redundant reasoning processes, exposing LRMs to resource-exhaustion vulnerabilities. However, the reasoning process itself, especially its reflective component, has received limited attention, even though it can lead to over-reflection and consume excessive computing power. In this paper, we introduce Recursive Entropy to quantify the risk of resource consumption in reflection, thereby revealing the safety issues inherent in inference itself. Based on Recursive Entropy, we introduce RECUR, a resource exhaustion attack via Recursive Entropy guided Counterfactual Utilization and Reflection. It constructs counterfactual questions to verify the inherent flaws and risks of LRMs. Extensive experiments demonstrate that, under benign inference, recursive entropy exhibits a pronounced decreasing trend. RECUR disrupts this trend, increasing the output length by up to 11x and decreasing throughput by 90%. Our work provides a new perspective on robust reasoning.

</details>


### [281] [Weak-Driven Learning: How Weak Agents make Strong Agents Stronger](https://arxiv.org/abs/2602.08222)
*Zehao Chen,Gongxun Li,Tianxiang Ai,Yifei Li,Zixuan Huang,Wang Zhou,Fuzhen Zhuang,Xianglong Liu,Jianxin Li,Deqing Wang,Yikun Ban*

Main category: cs.AI

TL;DR: WMSS利用弱检查点指导持续优化，通过熵动态识别可恢复的学习缺口并进行补偿学习，使强模型超越传统后训练饱和状态，实验表明该方法在数学推理和代码生成数据集上实现有效的性能提升，且无额外推理成本。


<details>
  <summary>Details</summary>
Motivation: 针对大语言模型在增长至高度自信后进一步训练效果递减的现象，提出WMSS后训练优化框架，利用模型的历史弱状态中的信息监督信号来驱动优化。

Method: WMSS通过熵动态分析识别模型训练中的可学习缺口，然后采用补偿学习策略来弥合这些缺口，从而使得强模型能够在已有的饱和状态下继续进行优化。

Result: WMSS在数学推理和代码生成任务上展示了模型性能的有效提升，并且在整个过程中不增加推理成本。

Conclusion: WMSS为后训练优化提供了新的视角，通过挖掘模型历史中的潜在学习资源，克服饱和瓶颈，实现模型性能上的突破。

Abstract: As post-training optimization becomes central to improving large language models, we observe a persistent saturation bottleneck: once models grow highly confident, further training yields diminishing returns. While existing methods continue to reinforce target predictions, we find that informative supervision signals remain latent in models' own historical weak states. Motivated by this observation, we propose WMSS (Weak Agents Can Make Strong Agents Stronger), a post-training paradigm that leverages weak checkpoints to guide continued optimization. By identifying recoverable learning gaps via entropy dynamics and reinforcing them through compensatory learning, WMSS enables strong agents to improve beyond conventional post-training saturation. Experiments on mathematical reasoning and code generation datasets show that agents trained with our approach achieve effective performance improvements, while incurring zero additional inference cost.

</details>


### [282] [InfiCoEvalChain: A Blockchain-Based Decentralized Framework for Collaborative LLM Evaluation](https://arxiv.org/abs/2602.08229)
*Yifan Yang,Jinjia Li,Kunxi Li,Puhao Zheng,Yuanyi Wang,Zheyan Qu,Yang Yu,Jianmin Wu,Ming Li,Hongxia Yang*

Main category: cs.AI

TL;DR: 本文提出了一种去中心化的评估框架，通过采用区块链协议激励全球贡献者作为独立验证者，并在异构计算节点上进行大规模基准测试，从而实现硬件和参数的多样化，减少了模型评估的波动性和不稳定性。


<details>
  <summary>Details</summary>
Motivation: 当前集中式评估存在不透明性、过拟合和硬件诱发的方差问题，现有的评价标准不一致。

Method: 通过区块链协议激励全球贡献者作为独立验证者，使用奖励系统确保评估的完整性，减少不诚实的参与，同时还进行了大规模的基准测试。

Result: 去中心化评估框架显著减少了模型评估的波动性，标准差从1.67降低至0.28，提高了模型排名的统计信心。

Conclusion: 作者表示已经完全实现了该平台，并将在不久的将来向社区发布。

Abstract: The rapid advancement of large language models (LLMs) demands increasingly reliable evaluation, yet current centralized evaluation suffers from opacity, overfitting, and hardware-induced variance. Our empirical analysis reveals an alarming inconsistency in existing evaluations: the standard deviation across ten repeated runs of a single model on HumanEval (1.67) actually exceeds the performance gap among the top-10 models on the official leaderboard (0.91), rendering current rankings statistically precarious. To mitigate these instabilities, we propose a decentralized evaluation framework that enables hardware and parameter diversity through large-scale benchmarking across heterogeneous compute nodes. By leveraging the blockchain-based protocol, the framework incentivizes global contributors to act as independent validators, using a robust reward system to ensure evaluation integrity and discourage dishonest participation. This collective verification transforms evaluation from a "centralized black box" into a "decentralized endorsement" where multi-party consensus and diverse inference environments yield a more stable, representative metric. Experimental results demonstrate that the decentralized evaluation framework reduces the standard deviation across ten runs on the same model to 0.28. This significant improvement over conventional frameworks ensures higher statistical confidence in model rankings. We have completely implemented this platform and will soon release it to the community.

</details>


### [283] [Do MLLMs Really See It: Reinforcing Visual Attention in Multimodal LLMs](https://arxiv.org/abs/2602.08241)
*Siqu Ou,Tianrui Wan,Zhiyuan Zhao,Junyu Gao,Xuelong Li*

Main category: cs.AI

TL;DR: SAYO 是一种通过引入基于区域的视觉注意力奖励，使用强化学习进行训练的视觉推理模型，改善了 MLLMs 在复杂推理任务中的视觉关注表现。


<details>
  <summary>Details</summary>
Motivation: 当前的大规模多模态语言模型在视觉关注上表现出较弱的能力，即早期的视觉偏差在后续推理过程中很少被纠正，导致错误传播和失败的推理。

Method: SAYO 采用强化学习框架，并引入基于区域的视觉注意力奖励，直接优化与视觉支持推理步骤相关的信号。

Result: SAYO 在多个多模态基准测试中展示了在各类推理和感知任务上的一致性能提升。

Conclusion: SAYO 通过改进视觉关注训练机制，提高了多模态大型语言模型在复杂推理任务上的表现。

Abstract: While chain-of-thought (CoT) reasoning has substantially improved multimodal large language models (MLLMs) on complex reasoning tasks, existing approaches largely rely on long textual reasoning trajectories and provide limited mechanisms for learning stable visual attention policies. Our analysis shows that current MLLMs exhibit weak visual focus: early-stage visual misalignment is rarely corrected during subsequent reasoning, leading to error propagation and failed inferences. We argue that this limitation stems from inadequate credit assignment for visual attention during training. To address this issue, we propose SAYO, a visual reasoning model trained with a reinforcement learning (RL) framework that introduces a region-level visual attention-based reward. This reward explicitly aligns optimization signals with visually grounded reasoning steps, enabling the model to learn more reliable attention behaviors. Extensive experiments across multiple multimodal benchmarks demonstrate that SAYO consistently improves performance on diverse reasoning and perception tasks.

</details>


### [284] [G-LNS: Generative Large Neighborhood Search for LLM-Based Automatic Heuristic Design](https://arxiv.org/abs/2602.08253)
*Baoyun Zhao,He Wang,Liang Zeng*

Main category: cs.AI

TL;DR: 提出了一种名为G-LNS的生成进化框架，它扩展了基于大规模语言模型的自动设计，针对复杂组合优化问题的大型邻域搜索操作进行自动化设计。G-LNS能够发现有效的破坏和修复操作逻辑，实现了近最优解并具备强大的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的自动启发式设计方法通常限制在固定的启发式形式，难以在复杂组合优化问题中跳出深层局部最优解。G-LNS旨在通过协同演化破坏和修复操作器对，来探索结构性操作。

Method: G-LNS使用大型语言模型来协同演化破坏和修复操作器。一种协作评价机制明确捕捉它们的交互，从而发现互补的操作逻辑。这种方法能在不降低计算预算的情况下找到接近最优解。

Result: 在具有挑战性的组合优化基准问题上，例如旅行商问题和带容量的车辆路径问题，G-LNS的表现优于基于语言模型的自动设计方法和强大的经典求解器。发现的启发式方法不仅在计算预算上得到了优化，而且对各种未见过的实例分布也表现出泛化能力。

Conclusion: G-LNS展示了其在复杂组合优化问题中的优越性，能够有效设计大型邻域搜索操作，为相关领域的研究提供了新的思路和方法。

Abstract: While Large Language Models (LLMs) have recently shown promise in Automated Heuristic Design (AHD), existing approaches typically formulate AHD around constructive priority rules or parameterized local search guidance, thereby restricting the search space to fixed heuristic forms. Such designs offer limited capacity for structural exploration, making it difficult to escape deep local optima in complex Combinatorial Optimization Problems (COPs). In this work, we propose G-LNS, a generative evolutionary framework that extends LLM-based AHD to the automated design of Large Neighborhood Search (LNS) operators. Unlike prior methods that evolve heuristics in isolation, G-LNS leverages LLMs to co-evolve tightly coupled pairs of destroy and repair operators. A cooperative evaluation mechanism explicitly captures their interaction, enabling the discovery of complementary operator logic that jointly performs effective structural disruption and reconstruction. Extensive experiments on challenging COP benchmarks, such as Traveling Salesman Problems (TSP) and Capacitated Vehicle Routing Problems (CVRP), demonstrate that G-LNS significantly outperforms LLM-based AHD methods as well as strong classical solvers. The discovered heuristics not only achieve near-optimal solutions with reduced computational budgets but also exhibit robust generalization across diverse and unseen instance distributions.

</details>


### [285] [Puda: Private User Dataset Agent for User-Sovereign and Privacy-Preserving Personalized AI](https://arxiv.org/abs/2602.08268)
*Akinori Maeda,Yuto Sekiya,Sota Sugimura,Tomoya Asai,Yu Tsuda,Kohei Ikeda,Hiroshi Fujii,Kohei Watanabe*

Main category: cs.AI

TL;DR: Puda是一种用户主权架构，能够跨服务聚合数据并允许用户在适当的数据粒度级别上控制数据共享，从而在个人化服务和隐私保护之间实现平衡。


<details>
  <summary>Details</summary>
Motivation: 现有数据孤岛导致用户主权受损，隐私权与个性化服务需求之间的冲突明显，特提出Puda以增强用户数据控制能力。

Method: Puda通过浏览器系统实现，提供三个级别的数据控制：详细的浏览历史、提取的关键字和预定义的类别子集，利用LLM进行评估。

Result: 预定义的类别子集在个性化性能上可与详细的浏览历史相媲美，证明了Puda在多粒度数据管理中的有效性。

Conclusion: Puda提供了一种AI原生的基础架构，支持用户主权，让安全使用个性化AI成为可能，有效缓解隐私与个性化之间的矛盾。

Abstract: Personal data centralization among dominant platform providers including search engines, social networking services, and e-commerce has created siloed ecosystems that restrict user sovereignty, thereby impeding data use across services. Meanwhile, the rapid proliferation of Large Language Model (LLM)-based agents has intensified demand for highly personalized services that require the dynamic provision of diverse personal data. This presents a significant challenge: balancing the utilization of such data with privacy protection. To address this challenge, we propose Puda (Private User Dataset Agent), a user-sovereign architecture that aggregates data across services and enables client-side management. Puda allows users to control data sharing at three privacy levels: (i) Detailed Browsing History, (ii) Extracted Keywords, and (iii) Predefined Category Subsets. We implemented Puda as a browser-based system that serves as a common platform across diverse services and evaluated it through a personalized travel planning task. Our results show that providing Predefined Category Subsets achieves 97.2% of the personalization performance (evaluated via an LLM-as-a-Judge framework across three criteria) obtained when sharing Detailed Browsing History. These findings demonstrate that Puda enables effective multi-granularity management, offering practical choices to mitigate the privacy-personalization trade-off. Overall, Puda provides an AI-native foundation for user sovereignty, empowering users to safely leverage the full potential of personalized AI.

</details>


### [286] [The Vibe-Automation of Automation: A Proactive Education Framework for Computer Science in the Age of Generative AI](https://arxiv.org/abs/2602.08295)
*Ilya Levin*

Main category: cs.AI

TL;DR: 该论文提出了从计算机科学基础假设中涌现的新质性范式转移，强调生成AI的核心在于识别和操作嵌入实践的语境敏感规律，而非优化预定义的目标度量。随着人类角色从算法问题定义转向'氛围工程'，以人为中心的设计和教育框架变得至关重要。


<details>
  <summary>Details</summary>
Motivation: 论文旨在探讨生成AI如何作为一种质变而非渐进技术发展，强调它对计算机科学基础假设的影响，进而推动教育和机构层面的转型。

Method: 通过引入'氛围自动化'理念，论文分析了生成AI如何识别和操作嵌入实践中的隐性规律，以及如何重新定义人类的角色。

Result: 论文提出了一种新的框架，将注意力从算法定义转向氛围工程，该框架贯穿于教师世界观、产业关系和课程设计三个层级和领域。

Conclusion: 论文提出了避免生成AI带来的模式崩溃和文化同质化的建议，强调了慎重参与的重要性，以防止生成系统走向合成单一性。

Abstract: The emergence of generative artificial intelligence (GenAI) represents not an incremental technological advance but a qualitative epistemological shift that challenges foundational assumptions of computer science. Whereas machine learning has been described as the automation of automation, generative AI operates by navigating contextual, semantic, and stylistic coherence rather than optimizing predefined objective metrics. This paper introduces the concept of Vibe-Automation to characterize this transition.
  The central claim is that the significance of GenAI lies in its functional access to operationalized tacit regularities: context-sensitive patterns embedded in practice that cannot be fully specified through explicit algorithmic rules. Although generative systems do not possess tacit knowledge in a phenomenological sense, they operationalize sensitivities to tone, intent, and situated judgment encoded in high-dimensional latent representations. On this basis, the human role shifts from algorithmic problem specification toward Vibe-Engineering, understood as the orchestration of alignment and contextual judgment in generative systems.
  The paper connects this epistemological shift to educational and institutional transformation by proposing a conceptual framework structured across three analytical levels and three domains of action: faculty worldview, industry relations, and curriculum design. The risks of mode collapse and cultural homogenization are briefly discussed, emphasizing the need for deliberate engagement with generative systems to avoid regression toward synthetic uniformity.

</details>


### [287] [Moral Sycophancy in Vision Language Models](https://arxiv.org/abs/2602.08311)
*Shadman Rabby,Md. Hefzul Hossain Papon,Sabbir Ahmed,Nokimul Hasan Arif,A. B. M. Ashikur Rahman,Irfan Ahmad*

Main category: cs.AI

TL;DR: 该研究首次系统地探讨了视觉语言模型（VLMs）在遇到用户诱导偏见时的道德媚俗行为。研究发现，模型更有可能从正确转向错误的道德判断，而不仅仅是相反方向。在不同数据集上的表现差异揭示了道德韧性与数据集的依赖性。此外，模型的纠错能力越强，越容易引入推理错误。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探讨和理解视觉语言模型在面对用户的道德争议时的媚俗行为对决策的直接影响，填补了现有研究中在道德决策方面的不足。

Method: 该研究使用道德化和M^3oralBench数据集，分析了十种常用的视觉语言模型在用户明确反对情况下的一致性和媚俗行为。通过对错误引入率（EIR）和错误修正率（ECR）的评估，研究了模型的特性。

Result: 研究结果显示，即使在最初判断正确的前提下，视觉语言模型在其后生成的响应中也常常包含道德错误。在不同数据集上的表现表明了道德坚固性与数据集依赖性。错误引入率（EIR）和错误修正率（ECR）的评估揭示了模型纠错能力与推理错误之间的权衡。

Conclusion: 视觉语言模型更容易从正确转向错误的道德判断，这要求开发人员采取有原则的方法来改善多模态AI系统的道德一致性和稳健性。

Abstract: Sycophancy in Vision-Language Models (VLMs) refers to their tendency to align with user opinions, often at the expense of moral or factual accuracy. While prior studies have explored sycophantic behavior in general contexts, its impact on morally grounded visual decision-making remains insufficiently understood. To address this gap, we present the first systematic study of moral sycophancy in VLMs, analyzing ten widely-used models on the Moralise and M^3oralBench datasets under explicit user disagreement. Our results reveal that VLMs frequently produce morally incorrect follow-up responses even when their initial judgments are correct, and exhibit a consistent asymmetry: models are more likely to shift from morally right to morally wrong judgments than the reverse when exposed to user-induced bias. Follow-up prompts generally degrade performance on Moralise, while yielding mixed or even improved accuracy on M^3oralBench, highlighting dataset-dependent differences in moral robustness. Evaluation using Error Introduction Rate (EIR) and Error Correction Rate (ECR) reveals a clear trade-off: models with stronger error-correction capabilities tend to introduce more reasoning errors, whereas more conservative models minimize errors but exhibit limited ability to self-correct. Finally, initial contexts with a morally right stance elicit stronger sycophantic behavior, emphasizing the vulnerability of VLMs to moral influence and the need for principled strategies to improve ethical consistency and robustness in multimodal AI systems.

</details>


### [288] [Who Deserves the Reward? SHARP: Shapley Credit-based Optimization for Multi-Agent System](https://arxiv.org/abs/2602.08335)
*Yanming Li,Xuelin Zhang,WenJie Lu,Ziye Tang,Maodong Wu,Haotian Luo,Tongtong Wu,Zijie Peng,Hongze Mi,Yibo Feng,Naiqiang Tan,Chao Huang,Hong Chen,Li Shen*

Main category: cs.AI

TL;DR: SHARP 是一种通过精确的功劳归属来优化多智能体强化学习的新框架，通过分解奖励机制提升训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有的多智能体强化学习方法在处理复杂的现实世界问题时存在效率低、功劳归属不清的问题。

Method: SHARP 通过引入全球传播准确性奖励、基于 Shapley 值的边际贡献奖励以及工具过程奖励，使得智能体的特定优势在整个轨迹组内得到合理分配，从而提升训练的稳定性和效率。

Result: 在多种实际应用基准测试中，SHARP 显著优于最新的基线方法，单智能体和多智能体方法分别提高了 23.66% 和 14.05% 的性能。

Conclusion: SHARP 提供了一种有效的解决智能体任务奖励分配问题的方法，为强化学习多智能体系统提供了改进策略。

Abstract: Integrating Large Language Models (LLMs) with external tools via multi-agent systems offers a promising new paradigm for decomposing and solving complex problems. However, training these systems remains notoriously difficult due to the credit assignment challenge, as it is often unclear which specific functional agent is responsible for the success or failure of decision trajectories. Existing methods typically rely on sparse or globally broadcast rewards, failing to capture individual contributions and leading to inefficient reinforcement learning. To address these limitations, we introduce the Shapley-based Hierarchical Attribution for Reinforcement Policy (SHARP), a novel framework for optimizing multi-agent reinforcement learning via precise credit attribution. SHARP effectively stabilizes training by normalizing agent-specific advantages across trajectory groups, primarily through a decomposed reward mechanism comprising a global broadcast-accuracy reward, a Shapley-based marginal-credit reward for each agent, and a tool-process reward to improve execution efficiency. Extensive experiments across various real-world benchmarks demonstrate that SHARP significantly outperforms recent state-of-the-art baselines, achieving average match improvements of 23.66% and 14.05% over single-agent and multi-agent approaches, respectively.

</details>


### [289] [CoTZero: Annotation-Free Human-Like Vision Reasoning via Hierarchical Synthetic CoT](https://arxiv.org/abs/2602.08339)
*Chengyi Du,Yazhe Niu,Dazhong Shen,Luxin Xu*

Main category: cs.AI

TL;DR: CoTZero 提出了一种无需标注的范式，通过双阶段数据合成和认知对齐训练方法，增强了视觉语言模型的高层次推理和泛化能力。该方法利用自底向上的过程生成原子视素并逐步组合成多样化结构的问题推理形式，在自顶向下的过程中使用粗略的整体结构指导局部细节和因果关系的解释。实验结果显示 CoTZero 在多级语义不一致基准上的 F1 分数为 83.33%，并且在各个组成部分中都得到了验证。


<details>
  <summary>Details</summary>
Motivation: 为了克服现有视觉语言模型在高层次推理和因果关系理解方面的局限性，通过引入人类推理模型来解决表面关联问题。

Method: CoTZero 方法包含两个组件：一是双阶段数据合成方法，包括自底向上的原子视素提取与组合，以及自顶向下的层级推理引导；二是认知对齐训练方法，基于合成数据引入认知一致验证奖励来强化模型的层级推理与泛化。

Result: CoTZero 在多级语义不一致基准上的 F1 分数为 83.33%，并且在多个实验设置中验证了各个组成部分的有效性。

Conclusion: CoTZero 通过引入自底向上和自顶向下的过程，以及认知对齐的强化训练方法，显著提高了视觉语言模型的高层次推理和泛化能力。

Abstract: Recent advances in vision-language models (VLMs) have markedly improved image-text alignment, yet they still fall short of human-like visual reasoning. A key limitation is that many VLMs rely on surface correlations rather than building logically coherent structured representations, which often leads to missed higher-level semantic structure and non-causal relational understanding, hindering compositional and verifiable reasoning. To address these limitations by introducing human models into the reasoning process, we propose CoTZero, an annotation-free paradigm with two components: (i) a dual-stage data synthesis approach and (ii) a cognition-aligned training method. In the first component, we draw inspiration from neurocognitive accounts of compositional productivity and global-to-local analysis. In the bottom-up stage, CoTZero extracts atomic visual primitives and incrementally composes them into diverse, structured question-reasoning forms. In the top-down stage, it enforces hierarchical reasoning by using coarse global structure to guide the interpretation of local details and causal relations. In the cognition-aligned training component, built on the synthesized CoT data, we introduce Cognitively Coherent Verifiable Rewards (CCVR) in Reinforcement Fine-Tuning (RFT) to further strengthen VLMs' hierarchical reasoning and generalization, providing stepwise feedback on reasoning coherence and factual correctness. Experiments show that CoTZero achieves an F1 score of 83.33 percent on our multi-level semantic inconsistency benchmark with lexical-perturbation negatives, across both in-domain and out-of-domain settings. Ablations confirm that each component contributes to more interpretable and human-aligned visual reasoning.

</details>


### [290] [Effect-Level Validation for Causal Discovery](https://arxiv.org/abs/2602.08340)
*Hoang Dang,Luan Pham,Minh Nguyen*

Main category: cs.AI

TL;DR: 该研究提出了一种以效应为中心、优先考虑适用性的框架，强调识别性而非仅通过图恢复准确性来评估发现的因果图。利用真实游戏 telemetry 数据探究早期接触竞争性游戏对短期留存的影响，发现许多统计上合理的发现输出，一旦施加最小的时序和语义约束，便不再承认因果查询的有效性。研究还发现，即使图结构不同，某些算法家族也能得出一致的决策一致效应估计，而其他方法在终点模糊时则表现出适用性不稳定或效应衰减。


<details>
  <summary>Details</summary>
Motivation: 在实时反馈驱动系统中对用户干预效果进行因果发现，传统的图恢复准确性评估方法并不能完全反映因果推理的可靠性，特别是在存在强烈自我选择效应的情况下。

Method: 实验研究了早期接触竞争性游戏是否能提升短期用户留存，通过实施时空约束来检查因果查询的有效性。评估使用多个算法框架处理真实游戏 telemetry 数据，计算图结构与效应估计，比较不同方法在保持因果安全性、稳定性和反驳敏感性方面的能力。

Result: 研究发现，即使图结构存在差异，某些算法也能以一致的方式估计效应，而在不同方法之间适用性存在波动或效应衰减。符合条件的因果查询很少，说明在特定目标查询下的因果可靠性，图层度量无法完全替代。研究结果表明，因果可靠性应该优先考虑适用性和效果层面的验证，而不是单纯追求因果结构的恢复。

Conclusion: 在实时反馈驱动系统中推断用户干预的效果时，研究所提出的框架表明应重视适用性而非单纯的因果图恢复。研究为因果推理在大规模观测数据的应用提供了新的视角，未来可进一步扩展适用于不同类型的观测数据。

Abstract: Causal discovery is increasingly applied to large-scale telemetry data to estimate the effects of user-facing interventions, yet its reliability for decision-making in feedback-driven systems with strong self-selection remains unclear. In this paper, we propose an effect-centric, admissibility-first framework that treats discovered graphs as structural hypotheses and evaluates them by identifiability, stability, and falsification rather than by graph recovery accuracy alone. Empirically, we study the effect of early exposure to competitive gameplay on short-term retention using real-world game telemetry. We find that many statistically plausible discovery outputs do not admit point-identified causal queries once minimal temporal and semantic constraints are enforced, highlighting identifiability as a critical bottleneck for decision support. When identification is possible, several algorithm families converge to similar, decision-consistent effect estimates despite producing substantially different graph structures, including cases where the direct treatment-outcome edge is absent and the effect is preserved through indirect causal pathways. These converging estimates survive placebo, subsampling, and sensitivity refutation. In contrast, other methods exhibit sporadic admissibility and threshold-sensitive or attenuated effects due to endpoint ambiguity. These results suggest that graph-level metrics alone are inadequate proxies for causal reliability for a given target query. Therefore, trustworthy causal conclusions in telemetry-driven systems require prioritizing admissibility and effect-level validation over causal structural recovery alone.

</details>


### [291] [OPE: Overcoming Information Saturation in Parallel Thinking via Outline-Guided Path Exploration](https://arxiv.org/abs/2602.08344)
*Qi Guo,Jianing Wang,Deyang Kong,Xiangyu Xi,Jianfei Zhang,Yi Lu,Jingang Wang,Wei Wang,Shikun Zhang,Wei Ye*

Main category: cs.AI

TL;DR: 本文提出了面向强化学习带验证奖励（RLVR）设置下并行思考优化的理论分析，提出了Outline-Guided Path Exploration（OPE）方法，通过生成多样化的推理大纲来减少信息冗余，提高并行路径探索阶段的表现，最终在多个复杂数学基准的广泛实验中验证了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的并行思考优化主要集中于聚合阶段的优化，忽略了路径探索阶段的重要性。本文旨在通过理论分析，探讨如何在RLVR设置下优化并行思考，以克服互相信息瓶颈带来的整体性能限制。

Method: 本文提出了OPE方法，该方法在并行路径推理之前生成多样化的推理大纲，通过减少信息冗余来提高路径探索阶段的表现。

Result: 实验结果表明，OPE可以在不同的聚合策略下有效提升推理性能，使大型推理模型（LRMs）更可靠地发现正确解。

Conclusion: 该研究揭示并克服了并行思考优化中的关键瓶颈，从而增强了LRMs在解决复杂问题时的整体性能。

Abstract: Parallel thinking has emerged as a new paradigm for large reasoning models (LRMs) in tackling complex problems. Recent methods leverage Reinforcement Learning (RL) to enhance parallel thinking, aiming to address the limitations in computational resources and effectiveness encountered with supervised fine-tuning. However, most existing studies primarily focus on optimizing the aggregation phase, with limited attention to the path exploration stage. In this paper, we theoretically analyze the optimization of parallel thinking under the Reinforcement Learning with Verifiable Rewards (RLVR) setting, and identify that the mutual information bottleneck among exploration paths fundamentally restricts overall performance. To address this, we propose Outline-Guided Path Exploration (OPE), which explicitly partitions the solution space by generating diverse reasoning outlines prior to parallel path reasoning, thereby reducing information redundancy and improving the diversity of information captured across exploration paths. We implement OPE with an iterative RL strategy that optimizes outline planning and outline-guided reasoning independently. Extensive experiments across multiple challenging mathematical benchmarks demonstrate that OPE effectively improves reasoning performance in different aggregation strategies, enabling LRMs to more reliably discover correct solutions.

</details>


### [292] [Towards Better Evolution Modeling for Temporal Knowledge Graphs](https://arxiv.org/abs/2602.08353)
*Zhang Jiasheng,Li Zhangpin,Wang Mingzhe,Shao Jie,Cui Jiangtao,Li Hui*

Main category: cs.AI

TL;DR: 现有基准因存在捷径被不当评价，本文提出TKG演进基准，包含四个修正偏见的数据集和两个新任务，以更加准确评估TKG演化建模的挑战。


<details>
  <summary>Details</summary>
Motivation: 鉴于现有基准可能存在捷径问题，提出的TKG演进基准旨在纠正数据集中的内在偏差和简化评价任务，以促进对TKG演化建模挑战的更准确评估。

Method: 通过识别当前数据集中的偏见和简化评价任务的局限性，提出四个修正偏见的数据集和两个与演化过程密切相关的新型任务。

Result: TKG演化基准提供了四个修正偏见的数据集和两个新型任务，有助于更加准确地评估TKG演化建模的挑战。

Conclusion: TKG演化基准为TKG演化建模的研究提供了更加公平和准确的评价标准。

Abstract: Temporal knowledge graphs (TKGs) structurally preserve evolving human knowledge. Recent research has focused on designing models to learn the evolutionary nature of TKGs to predict future facts, achieving impressive results. For instance, Hits@10 scores over 0.9 on YAGO dataset. However, we find that existing benchmarks inadvertently introduce a shortcut. Near state-of-the-art performance can be simply achieved by counting co-occurrences, without using any temporal information. In this work, we examine the root cause of this issue, identifying inherent biases in current datasets and over simplified form of evaluation task that can be exploited by these biases. Through this analysis, we further uncover additional limitations of existing benchmarks, including unreasonable formatting of time-interval knowledge, ignorance of learning knowledge obsolescence, and insufficient information for precise evolution understanding, all of which can amplify the shortcut and hinder a fair assessment. Therefore, we introduce the TKG evolution benchmark. It includes four bias-corrected datasets and two novel tasks closely aligned with the evolution process, promoting a more accurate understanding of the challenges in TKG evolution modeling. Benchmark is available at: https://github.com/zjs123/TKG-Benchmark.

</details>


### [293] [Does Your Reasoning Model Implicitly Know When to Stop Thinking?](https://arxiv.org/abs/2602.08354)
*Zixuan Huang,Xin Xia,Yuxi Ren,Jianbin Zheng,Xuanda Wang,Zhixia Zhang,Hongyan Xie,Songshi Liang,Zehao Chen,Xuefeng Xiao,Fuzhen Zhuang,Jianxin Li,Yikun Ban,Deqing Wang*

Main category: cs.AI

TL;DR: 该研究分析了LRMs在复杂推理任务中的冗余问题，提出了一个新的采样范式SAGE，使其更有效地进行推理，并将其整合到RL中，进一步提高了推理准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 克服LRMs在复杂推理任务中的冗余问题，提高计算效率和实时应用中的响应速度。

Method: 研究发现LRMs自身有能力在适当的时候停止推理，但当前的采样方法未能充分利用这一能力。因此，提出了SAGE（Self-Aware Guided Efficient Reasoning） paradigam来引导LRMs更加节约资源地进行推理。

Result: SAGE可以在保持推理准确性的同时极大地提高推理效率，通过将其集成到基于组的强化学习（SAGE-RL）中，SAGE-RL能够在多个复杂的数学基准测试中显著提高推理的准确性和效率。

Conclusion: SAGE是一个创新的采样范式，可以优化LRMs的推理过程，提高其在实时应用中的表现。

Abstract: Recent advancements in large reasoning models (LRMs) have greatly improved their capabilities on complex reasoning tasks through Long Chains of Thought (CoTs). However, this approach often results in substantial redundancy, impairing computational efficiency and causing significant delays in real-time applications. Recent studies show that longer reasoning chains are frequently uncorrelated with correctness and can even be detrimental to accuracy. In a further in-depth analysis of this phenomenon, we surprisingly uncover and empirically verify that LRMs implicitly know the appropriate time to stop thinking, while this capability is obscured by current sampling paradigms. Motivated by this, we introduce SAGE (Self-Aware Guided Efficient Reasoning), a novel sampling paradigm that unleashes this efficient reasoning potential. Furthermore, integrating SAGE as mixed sampling into group-based reinforcement learning (SAGE-RL) enables SAGE-RL to effectively incorporate SAGE-discovered efficient reasoning patterns into standard pass@1 inference, markedly enhancing both the reasoning accuracy and efficiency of LRMs across multiple challenging mathematical benchmarks.

</details>


### [294] [Circuit Representations of Random Forests with Applications to XAI](https://arxiv.org/abs/2602.08362)
*Chunxi Ji,Adnan Darwiche*

Main category: cs.AI

TL;DR: 本文提出了一种将随机森林分类器编译为电路的方法，使每个电路都能直接编码分类器某些类别的实例。这种方法比现有方法更高效，利用此方法计算决策的完整且通用原因，并提出算法计算决策的鲁棒性和最短改变路径。


<details>
  <summary>Details</summary>
Motivation: 提升随机森林分类器的决策解释能力，使其在不同应用场景中更具实用性。

Method: 提出的编译方法将随机森林分类器的决策过程转化为电路，每个电路能直接编码分类结果中的实例。利用此电路计算决策的鲁棒性和最短改变路径。

Result: 该方法比现有方法更高效，在多种数据集上成功列出了所有充分原因、必要原因和对比解释，计算了决策的鲁棒性，并找到了决策改变的最短路径。

Conclusion: 该研究展示了其在随机森林分类器决策解释中的应用价值，提升了模型的透明度和可解释性。

Abstract: We make three contributions in this paper. First, we present an approach for compiling a random forest classifier into a set of circuits, where each circuit directly encodes the instances in some class of the classifier. We show empirically that our proposed approach is significantly more efficient than existing similar approaches. Next, we utilize this approach to further obtain circuits that are tractable for computing the complete and general reasons of a decision, which are instance abstractions that play a fundamental role in computing explanations. Finally, we propose algorithms for computing the robustness of a decision and all shortest ways to flip it. We illustrate the utility of our contributions by using them to enumerate all sufficient reasons, necessary reasons and contrastive explanations of decisions; to compute the robustness of decisions; and to identify all shortest ways to flip the decisions made by random forest classifiers learned from a wide range of datasets.

</details>


### [295] [Grounding Generative Planners in Verifiable Logic: A Hybrid Architecture for Trustworthy Embodied AI](https://arxiv.org/abs/2602.08373)
*Feiyu Wu,Xu Zheng,Yue Qu,Zhuocheng Wang,Zicheng Feng,Hui Li*

Main category: cs.AI

TL;DR: 提出了一种称为VIRF的验证迭代改进框架，该框架通过逻辑导师和语言模型规划者的协同工作，提高实体AI的规划安全性，同时引入了知识获取管道来综合安全知识库。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用神经符号架构提高大型语言模型的规划能力，使其在物理部署中具备可靠性，特别是在安全任务中。

Method: VIRF采用了一种导师学徒对话机制，逻辑导师根据形式安全本体提供因果和教学反馈给语言模型规划者，从而进行智能的规划修复。

Result: 在复杂的家庭安全任务中，VIRF实现了100%的目标条件率（GCR）和0%的危险动作率（HAR），并且表现出高度的效率，平均只需要1.1次修正迭代。

Conclusion: VIRF为构建本身可信且可验证安全的实体AI提供了一条理论基础导向的道路。

Abstract: Large Language Models (LLMs) show promise as planners for embodied AI, but their stochastic nature lacks formal reasoning, preventing strict safety guarantees for physical deployment. Current approaches often rely on unreliable LLMs for safety checks or simply reject unsafe plans without offering repairs. We introduce the Verifiable Iterative Refinement Framework (VIRF), a neuro-symbolic architecture that shifts the paradigm from passive safety gatekeeping to active collaboration. Our core contribution is a tutor-apprentice dialogue where a deterministic Logic Tutor, grounded in a formal safety ontology, provides causal and pedagogical feedback to an LLM planner. This enables intelligent plan repairs rather than mere avoidance. We also introduce a scalable knowledge acquisition pipeline that synthesizes safety knowledge bases from real-world documents, correcting blind spots in existing benchmarks. In challenging home safety tasks, VIRF achieves a perfect 0 percent Hazardous Action Rate (HAR) and a 77.3 percent Goal-Condition Rate (GCR), which is the highest among all baselines. It is highly efficient, requiring only 1.1 correction iterations on average. VIRF demonstrates a principled pathway toward building fundamentally trustworthy and verifiably safe embodied agents.

</details>


### [296] [On Protecting Agentic Systems' Intellectual Property via Watermarking](https://arxiv.org/abs/2602.08401)
*Liwen Wang,Zongjie Li,Yuchong Xie,Shuai Wang,Dongdong She,Wei Wang,Juergen Rahmel*

Main category: cs.AI

TL;DR: 本文介绍了一种名为AGENTWM的新框架，旨在为自主系统提供水印，防止盗版。该框架通过在工具执行路径中微妙地偏倚功能等价的行动序列分布来工作，从而在不降低性能的情况下嵌入可验证的信号。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型水印技术在自主系统领域存在局限性，因为实际的自主系统往往以灰箱形式运作，隐藏了验证所需的内部推理痕迹。因此，为了保护自主系统的知识产权免受适应性强的攻击者侵害，本文提出了专门为此类系统设计的水印框架。

Method: AGENTWM框架利用功能等价的动作序列的语义等价性，通过微妙地偏倚功能上等价的工具执行路径的分布来注入水印，从而在不引起用户注意的情况下嵌入可验证的信号。

Result: 通过开发自动生成鲁棒水印方案的管道和严格的统计假设检验程序，证明了AGENTWM在三个复杂领域中能够以高检测精度赢得较小的性能影响。结果表明，AGENTWM有效地保护了自主系统的知识产权免受适应性强的攻击者窃取。

Conclusion: 本研究展示了为自主系统提供的专门设计的水印方法的可行性，并验证其对抗适应性强的攻击者的有效性，从而保护它们的知识产权。

Abstract: The evolution of Large Language Models (LLMs) into agentic systems that perform autonomous reasoning and tool use has created significant intellectual property (IP) value. We demonstrate that these systems are highly vulnerable to imitation attacks, where adversaries steal proprietary capabilities by training imitation models on victim outputs. Crucially, existing LLM watermarking techniques fail in this domain because real-world agentic systems often operate as grey boxes, concealing the internal reasoning traces required for verification. This paper presents AGENTWM, the first watermarking framework designed specifically for agentic models. AGENTWM exploits the semantic equivalence of action sequences, injecting watermarks by subtly biasing the distribution of functionally identical tool execution paths. This mechanism allows AGENTWM to embed verifiable signals directly into the visible action trajectory while remaining indistinguishable to users. We develop an automated pipeline to generate robust watermark schemes and a rigorous statistical hypothesis testing procedure for verification. Extensive evaluations across three complex domains demonstrate that AGENTWM achieves high detection accuracy with negligible impact on agent performance. Our results confirm that AGENTWM effectively protects agentic IP against adaptive adversaries, who cannot remove the watermarks without severely degrading the stolen model's utility.

</details>


### [297] [TreeTensor: Boost AI System on Nested Data with Constrained Tree-Like Tensor](https://arxiv.org/abs/2602.08517)
*Shaoang Zhang,Yazhe Niu*

Main category: cs.AI

TL;DR: 该研究提出了一种新的容器结构TreeTensor，旨在解决复杂认知AI系统中数据的层次结构问题。通过利用受限树结构，TreeTensor允许用户以接近零成本的方式在复杂数据上应用任意函数和操作，提升了AI系统的可编程性和运行时效率。


<details>
  <summary>Details</summary>
Motivation: 传统的张量在处理具有层次结构的复杂多模态数据时显得不够灵活，尤其是当数据结构复杂且需要高效操作时。因此，研究旨在提出一种新的容器结构，以提供更好的编程体验和运行时性能。

Method: 研究团队总结了复杂数据中两种主要的计算模式，并提出了一种受限制的树结构作为新的容器设计。通过这种方法，可以系统地建模数据关系，并灵活地处理各种机器学习库中的数据。

Result: TreeTensor不仅在一个复杂的AI系统（例如AlphaStar）中展示了强大的实用性和可扩展性，还特别应对了非固定长度数据的高效计算问题，同时保持了良好的运行时效率。

Conclusion: 该研究提出了一种新的树形容器TreeTensor，显著提升了复杂多模态数据处理的灵活性和效率。已开源于https://github.com/opendilab/DI-treetensor。

Abstract: Tensor is the most basic and essential data structure of nowadays artificial intelligence (AI) system. The natural properties of Tensor, especially the memory-continuity and slice-independence, make it feasible for training system to leverage parallel computing unit like GPU to process data simultaneously in batch, spatial or temporal dimensions. However, if we look beyond perception tasks, the data in a complicated cognitive AI system usually has hierarchical structures (i.e. nested data) with various modalities. They are inconvenient and inefficient to program directly with conventional Tensor with fixed shape. To address this issue, we summarize two main computational patterns of nested data, and then propose a general nested data container: TreeTensor. Through various constraints and magic utilities of TreeTensor, one can apply arbitrary functions and operations to nested data with almost zero cost, including some famous machine learning libraries, such as Scikit-Learn, Numpy and PyTorch. Our approach utilizes a constrained tree-structure perspective to systematically model data relationships, and it can also easily be combined with other methods to extend more usages, such as asynchronous execution and variable-length data computation. Detailed examples and benchmarks show TreeTensor not only provides powerful usability in various problems, especially one of the most complicated AI systems at present: AlphaStar for StarCraftII, but also exhibits excellent runtime efficiency without any overhead. Our project is available at https://github.com/opendilab/DI-treetensor.

</details>


### [298] [Reinforcement Inference: Leveraging Uncertainty for Self-Correcting Language Model Reasoning](https://arxiv.org/abs/2602.08520)
*Xinhai Sun*

Main category: cs.AI

TL;DR: 该研究提出了一种熵感知的推理策略——强化推理，它使模型能够在不确定性较高的情况下进行二次推理，从而提高推理准确性，同时减少额外的计算成本。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）在专业环境中通常以一次性、贪婪的推理方式进行评估和部署，这种模式会导致模型的真正能力被低估。

Method: 研究通过引入熵感知的推理策略——强化推理，该策略利用模型自身的不确定性，在不确定性较高时选择性地触发二次、更谨慎的推理过程。初步实验在12,032个MMLU-Pro问题上进行，验证了策略的有效性。

Result: 实验结果显示，使用DeepSeek-v3.2在零样本设置下，强化推理将准确率从60.72%提高到了84.03%，仅仅增加了61.06%的推理调用次数。

Conclusion: 研究者の結果、熵感知的推理策略不仅可以作为推理过程的实用升级，而且还可以作为一种新的评估和扩展模型能力的框架。

Abstract: Modern large language models (LLMs) are often evaluated and deployed under a \emph{one-shot, greedy} inference protocol, especially in professional settings that require deterministic behavior. This regime can systematically under-estimate a fixed model's true capability: many errors arise not from missing knowledge, but from premature commitment under internal ambiguity. We introduce \emph{Reinforcement Inference}, an entropy-aware inference-time control strategy that uses the model's own uncertainty to selectively invoke a second, more deliberate reasoning attempt, enabling stronger performance \emph{without any retraining}.
  On 12,032 MMLU-Pro questions across 14 subjects, using DeepSeek-v3.2 with deterministic decoding in a zero-shot setting, Reinforcement Inference improves accuracy from 60.72\% to 84.03\%, while only incurring 61.06\% additional inference calls. A 100\% re-asking ablation reaches 84.35\%, indicating that uncertainty-aware selection captures most of the attainable improvement with substantially less compute. Moreover, a \emph{prompt-only} ablation underperforms the baseline, suggesting that the gains are not explained by generic `` your output had high entropy, think step-by-step'' prompting alone.
  Beyond providing a practical inference-time upgrade, our results suggest a broader \emph{entropy-aware} paradigm for measuring and expanding model capability: because modern decoder-based models generate outputs autoregressively, entropy and related confidence measures arise naturally as first-class control signals during generation. The resulting gap between one-pass greedy inference and uncertainty-conditioned deliberation offers a diagnostic lens on an LLM's latent reasoning horizon and motivates future training objectives that explicitly constrain correctness--confidence alignment.

</details>


### [299] [An Attention Mechanism for Robust Multimodal Integration in a Global Workspace Architecture](https://arxiv.org/abs/2602.08597)
*Roland Bertin-Johannet,Lara Scipio,Leopold Maytié,Rufin VanRullen*

Main category: cs.AI

TL;DR: 该研究提出了一种自上而下的注意机制，以增强全局工作空间系统在复杂多模态数据集上的鲁棒性，并展示了良好的跨任务和跨模态泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前的多模态注意力模型在跨任务和跨模态的泛化能力较弱，研究旨在通过提出一种新的自上而下的注意机制来改进多模态信息处理，并使全局工作空间系统与最新研究中的最佳模型相竞争。

Method: 研究通过设计一个自上而下的注意机制，选择全局工作空间内的模态信息。测试了该机制在两个逐渐复杂的数据集（简单的形状和MM-IMDb 1.0）上的表现。

Result: 结果表明，该机制增强了全局工作空间系统在多模态数据集上的鲁棒性，并且具有良好的跨任务和跨模态泛化能力。与MM-IMDb 1.0基准上的现有基线相比，该机制使全局工作空间系统竞争最先进的模型。

Conclusion: 研究提出了一种新颖的自上而下的注意机制，并验证了其在处理多模态信息方面的有效性，从而使得全局工作空间系统具有竞争力。

Abstract: Global Workspace Theory (GWT), inspired by cognitive neuroscience, posits that flexible cognition could arise via the attentional selection of a relevant subset of modalities within a multimodal integration system. This cognitive framework can inspire novel computational architectures for multimodal integration. Indeed, recent implementations of GWT have explored its multimodal representation capabilities, but the related attention mechanisms remain understudied. Here, we propose and evaluate a top-down attention mechanism to select modalities inside a global workspace. First, we demonstrate that our attention mechanism improves noise robustness of a global workspace system on two multimodal datasets of increasing complexity: Simple Shapes and MM-IMDb 1.0. Second, we highlight various cross-task and cross-modality generalization capabilities that are not shared by multimodal attention models from the literature. Comparing against existing baselines on the MM-IMDb 1.0 benchmark, we find our attention mechanism makes the global workspace competitive with the state of the art.

</details>


### [300] [Debate is efficient with your time](https://arxiv.org/abs/2602.08630)
*Jonah Brown-Cohen,Geoffrey Irving,Simon C. Marshall,Ilan Newman,Georgios Piliouras,Mario Szegedy*

Main category: cs.AI

TL;DR: 通过辩论方法，两个竞争性的人工智能模型帮助人类判断复杂的计算任务。该研究引入了辩论查询复杂性(DQC)，指的是验证者要正确决定辩论所需的最小查询次数。研究发现，PSPACE/poly类的问题可以用O(log n)的查询量解决，这表明即使对于高度复杂的问题，查询效率也非常高。此外，输入的所有位都依赖于该函数需要Omega(log n)查询，而任何可由大小为s的电路计算的函数满足DQC(f) <= log(s) + 3。


<details>
  <summary>Details</summary>
Motivation: 为了评估AI安全策略中人类监督的实际成本，特别是在辩论方法中，研究人员引入了新的概念DQC以量化验证过程所需的查询次数，从而揭示不同的复杂性类问题的查询效率。

Method: 研究通过理论推导和电路大小相关的关系，分析了寻求状态变迁的论证数量与函数复杂性的联系，并提出了PSPACE/poly类问题的DQC为O(log n)等定量结果。

Result: 研究发现，PSPACE/poly类问题可以用O(log n)的查询量解决，这表明即使对于高度复杂的问题，查询效率也非常高。此外，输入的所有位都依赖于该函数需要Omega(log n)查询，而任何可由大小为s的电路计算的函数满足DQC(f) <= log(s) + 3。

Conclusion: 该研究揭示了辩论方法在实现高效验证人类无法直接判断的任务方面的潜力，并为复杂的函数提供了确定性的查询次数上限。此结果还关联到电路复杂性研究，进一步研究可以尝试利用DQC导出新电路下界以便评估确定性计算资源的需求。

Abstract: AI safety via debate uses two competing models to help a human judge verify complex computational tasks. Previous work has established what problems debate can solve in principle, but has not analysed the practical cost of human oversight: how many queries must the judge make to the debate transcript? We introduce Debate Query Complexity}(DQC), the minimum number of bits a verifier must inspect to correctly decide a debate.
  Surprisingly, we find that PSPACE/poly (the class of problems which debate can efficiently decide) is precisely the class of functions decidable with O(log n) queries. This characterisation shows that debate is remarkably query-efficient: even for highly complex problems, logarithmic oversight suffices. We also establish that functions depending on all their input bits require Omega(log n) queries, and that any function computable by a circuit of size s satisfies DQC(f) <= log(s) + 3. Interestingly, this last result implies that proving DQC lower bounds of log(n) + 6 for languages in P would yield new circuit lower bounds, connecting debate query complexity to central questions in circuit complexity.

</details>


### [301] [Why do we Trust Chatbots? From Normative Principles to Behavioral Drivers](https://arxiv.org/abs/2602.08707)
*Aditya Gulati,Nuria Oliver*

Main category: cs.AI

TL;DR: 本文探讨了聊天机器人的信任机制，提出将其视作销售技巧高超的销售人员，并强调心理信任与规范信任之间的区别需要通过更多研究来澄清。


<details>
  <summary>Details</summary>
Motivation: 现有的监管和政策框架主要关注信任的规范定义，但用户的信任却更多地源于行为机制。该研究旨在深入理解信任，并提出新的视角来重新定义聊天机器人的角色。

Method: 文章基于对认知偏差和用户行为的研究，提出对聊天机器人进行新的定位。

Result: 本文提出了将聊天机器人视为追求组织目标的高明销售人员的观点，并指出了心理信任与规范信任之间的区分。

Conclusion: 文章认为需要更多研究和支持机制，以帮助用户正确校准对对话AI系统的信任。

Abstract: As chatbots increasingly blur the boundary between automated systems and human conversation, the foundations of trust in these systems warrant closer examination. While regulatory and policy frameworks tend to define trust in normative terms, the trust users place in chatbots often emerges from behavioral mechanisms. In many cases, this trust is not earned through demonstrated trustworthiness but is instead shaped by interactional design choices that leverage cognitive biases to influence user behavior. Based on this observation, we propose reframing chatbots not as companions or assistants, but as highly skilled salespeople whose objectives are determined by the deploying organization. We argue that the coexistence of competing notions of "trust" under a shared term obscures important distinctions between psychological trust formation and normative trustworthiness. Addressing this gap requires further research and stronger support mechanisms to help users appropriately calibrate trust in conversational AI systems.

</details>


### [302] [Intermediate Results on the Complexity of STRIPS$_{1}^{1}$](https://arxiv.org/abs/2602.08708)
*Stefan Edelkamp,Jiří Fink,Petr Gregor,Anders Jonsson,Bernhard Nebel*

Main category: cs.AI

TL;DR: 本文基于Bylander的结果，探讨了仅允许单一条件和效果的操作是否可以使命题STRIPS规划问题成为NP完全问题，并通过调用SAT求解器、引入文字图及将其映射到Petri网的方式对小型实例进行求解。


<details>
  <summary>Details</summary>
Motivation: 探讨命题STRIPS规划问题的复杂性，特别是具有单一条件和效果的操作是否能使得该问题成为NP完全。

Method: 通过调用SAT求解器对小型实例进行求解，并且引入文字图将其映射到Petri网。

Result: 尚未确定命题STRIPS规划问题在只允许单一条件和效果的操作时是否为NP完全问题。

Conclusion: 作者试图通过工具和技术（如SAT求解器和Petri网）来推进对该问题的理解。

Abstract: This paper is based on Bylander's results on the computational complexity of propositional STRIPS planning. He showed that when only ground literals are permitted, determining plan existence is PSPACE-complete even if operators are limited to two preconditions and two postconditions. While NP-hardness is settled, it is unknown whether propositional STRIPS with operators that only have one precondition and one effect is NP-complete. We shed light on the question whether this small solution hypothesis for STRIPS$^1_1$ is true, calling a SAT solver for small instances, introducing the literal graph, and mapping it to Petri nets.

</details>


### [303] [Exploring SAIG Methods for an Objective Evaluation of XAI](https://arxiv.org/abs/2602.08715)
*Miquel Miró-Nicolau,Gabriel Moyà-Alcover,Anna Arias-Duart*

Main category: cs.AI

TL;DR: 本论文首次对合成人工智能ground truth (SAIG)方法进行了审查和分析，划分出七种关键特征来区分不同的SAIG方法，揭示了XAI评估技术缺乏共识的问题，强调了该领域进一步研究和标准化的必要性。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决XAI方法评估中的复杂性问题，尤其是在缺乏通用正确ground truth的情况下，通过引入SAIG方法进行直接评估。

Method: 提出了一个新的分类体系，用于分类和区分SAIG方法，并进行了一项比较研究。

Result: 研究表明在XAI评估技术上缺乏共识，明确了识别和分类SAIG方法的关键特征。

Conclusion: 强调了进一步研究和标准化的重要性，以弥合XAI评估方法中的现有差距。

Abstract: The evaluation of eXplainable Artificial Intelligence (XAI) methods is a rapidly growing field, characterized by a wide variety of approaches. This diversity highlights the complexity of the XAI evaluation, which, unlike traditional AI assessment, lacks a universally correct ground truth for the explanation, making objective evaluation challenging. One promising direction to address this issue involves the use of what we term Synthetic Artificial Intelligence Ground truth (SAIG) methods, which generate artificial ground truths to enable the direct evaluation of XAI techniques. This paper presents the first review and analysis of SAIG methods. We introduce a novel taxonomy to classify these approaches, identifying seven key features that distinguish different SAIG methods. Our comparative study reveals a concerning lack of consensus on the most effective XAI evaluation techniques, underscoring the need for further research and standardization in this area.

</details>


### [304] [Finite-State Controllers for (Hidden-Model) POMDPs using Deep Reinforcement Learning](https://arxiv.org/abs/2602.08734)
*David Hudák,Maris F. L. Galesloot,Martin Tappler,Martin Kurečka,Nils Jansen,Milan Češka*

Main category: cs.AI

TL;DR: Lexpop框架通过深度强化学习训练神经策略，并通过高效提取方法构建有限状态控制器，同时确保了控制器的性能保证。


<details>
  <summary>Details</summary>
Motivation: 现有POMDP求解器的可扩展性有限，特别是当需要针对多个POMDP的鲁棒策略时。Lexpop旨在解决这一问题。

Method: Lexpop框架采用深度强化学习训练神经策略，通过提取方法构建有限状态控制器。

Result: Lexpop在处理具有大量状态空间的问题时，优于现有的POMDP和HM-POMDP求解器。

Conclusion: Lexpop框架通过结合深度学习和形式化评估，在POMDP和HM-POMDP求解方面取得了显著进展。

Abstract: Solving partially observable Markov decision processes (POMDPs) requires computing policies under imperfect state information. Despite recent advances, the scalability of existing POMDP solvers remains limited. Moreover, many settings require a policy that is robust across multiple POMDPs, further aggravating the scalability issue. We propose the Lexpop framework for POMDP solving. Lexpop (1) employs deep reinforcement learning to train a neural policy, represented by a recurrent neural network, and (2) constructs a finite-state controller mimicking the neural policy through efficient extraction methods. Crucially, unlike neural policies, such controllers can be formally evaluated, providing performance guarantees. We extend Lexpop to compute robust policies for hidden-model POMDPs (HM-POMDPs), which describe finite sets of POMDPs. We associate every extracted controller with its worst-case POMDP. Using a set of such POMDPs, we iteratively train a robust neural policy and consequently extract a robust controller. Our experiments show that on problems with large state spaces, Lexpop outperforms state-of-the-art solvers for POMDPs as well as HM-POMDPs.

</details>


### [305] [Belief Offloading in Human-AI Interaction](https://arxiv.org/abs/2602.08754)
*Rose E. Guingrich,Dvija Mehta,Umang Bhatt*

Main category: cs.AI

TL;DR: 该研究探讨了人们如何在依赖LLM时形成和维护信念，并研究了这一过程对人类认知技能和思想模式的影响。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于理解LLM在人类认知过程中的作用，特别是在人们依赖LLM形成和维护信念时，这种依赖如何影响人们的认知技能和信念系统。

Method: 该研究结合了哲学、心理学和计算机科学的研究成果来界定信念卸载的边界条件，并提供一种描述性分类体系以及其规范性含义。

Result: 研究结果揭示了信念卸载的概念，并提出了对其在人类-机器交互中的潜在影响及其规范性含义的理解。

Conclusion: 研究结论建议未来的工作应评估信念卸载在人类-机器交互中的潜在影响及其后果。

Abstract: What happens when people's beliefs are derived from information provided by an LLM? People's use of LLM chatbots as thought partners can contribute to cognitive offloading, which can have adverse effects on cognitive skills in cases of over-reliance. This paper defines and investigates a particular kind of cognitive offloading in human-AI interaction, "belief offloading," in which people's processes of forming and upholding beliefs are offloaded onto an AI system with downstream consequences on their behavior and the nature of their system of beliefs. Drawing on philosophy, psychology, and computer science research, we clarify the boundary conditions under which belief offloading occurs and provide a descriptive taxonomy of belief offloading and its normative implications. We close with directions for future work to assess the potential for and consequences of belief offloading in human-AI interaction.

</details>


### [306] [Dynamics Within Latent Chain-of-Thought: An Empirical Study of Causal Structure](https://arxiv.org/abs/2602.08783)
*Zirui Li,Xuefeng Bai,Kehai Chen,Yizhi Li,Jian Yang,Chenghua Lin,Min Zhang*

Main category: cs.AI

TL;DR: 研究探讨了潜在的链式推理方法，并通过结构因果模型（SCM）和逐步do-干预分析其因果效应，发现潜在步骤的行为更像分阶段的功能性，而非均匀增加的深度，揭示了早期输出偏见与后期表征承诺之间的持续差距。


<details>
  <summary>Details</summary>
Motivation: 探讨潜在链式推理方法的因果机制，以此作为理解并改进潜在推理系统的工具。

Method: 通过使用结构因果模型（SCM）和逐步do-干预方法分析潜在步骤的因果效应。

Result: 发现潜在步骤的行为更像分阶段的功能性，而非均匀增加的深度，揭示了早期输出偏见与后期表征承诺之间的持续差距。

Conclusion: 建议采用条件模式和稳定性意识的分析及相应训练/解码目标来提高对潜在推理系统的理解和改进。

Abstract: Latent or continuous chain-of-thought methods replace explicit textual rationales with a number of internal latent steps, but these intermediate computations are difficult to evaluate beyond correlation-based probes. In this paper, we view latent chain-of-thought as a manipulable causal process in representation space by modeling latent steps as variables in a structural causal model (SCM) and analyzing their effects through step-wise $\mathrm{do}$-interventions. We study two representative paradigms (i.e., Coconut and CODI) on both mathematical and general reasoning tasks to investigate three key questions: (1) which steps are causally necessary for correctness and when answers become decidable early; (2) how does influence propagate across steps, and how does this structure compare to explicit CoT; and (3) do intermediate trajectories retain competing answer modes, and how does output-level commitment differ from representational commitment across steps. We find that latent-step budgets behave less like homogeneous extra depth and more like staged functionality with non-local routing, and we identify a persistent gap between early output bias and late representational commitment. These results motivate mode-conditional and stability-aware analyses -- and corresponding training/decoding objectives -- as more reliable tools for interpreting and improving latent reasoning systems.

</details>


### [307] [The Use of AI Tools to Develop and Validate Q-Matrices](https://arxiv.org/abs/2602.08796)
*Kevin Fan,Jacquelyn A. Bialo,Hongli Li*

Main category: cs.AI

TL;DR: 本研究探讨了是否可以通过AI工具来辅助构建Q矩阵，并比较了AI生成的Q矩阵与已验证的Q矩阵的匹配度。


<details>
  <summary>Details</summary>
Motivation: 认知诊断模型(CDM)中构造Q矩阵是一个至关重要的但劳动密集的过程。因此，研究希望通过使用AI工具，特别是通用语言模型，来提高Q矩阵开发的效率和准确性。

Method: 研究在2025年5月和2026年1月分别使用了不同的AI模型，并与标准人类专家的Q矩阵进行对比，使用Cohen's kappa系数来衡量一致性和匹配度。

Result: 初步研究表明，Google Gemini 2.5 Pro通过与验证Q矩阵的匹配度最高（Kappa = 0.63），超过了所有人类专家的匹配度。然而，后续分析显示，随着AI技术的发展，最新的AI版本与验证Q矩阵的匹配度降低。

Conclusion: 研究指出，虽然AI在一定程度上可以辅助Q矩阵的开发，但仍需进一步的优化和评估，以确保其准确性和可靠性。未来的研究方向将侧重于改进AI技术的稳定性和长期效果。

Abstract: Constructing a Q-matrix is a critical but labor-intensive step in cognitive diagnostic modeling (CDM). This study investigates whether AI tools (i.e., general language models) can support Q-matrix development by comparing AI-generated Q-matrices with a validated Q-matrix from Li and Suen (2013) for a reading comprehension test. In May 2025, multiple AI models were provided with the same training materials as human experts. Agreement among AI-generated Q-matrices, the validated Q-matrix, and human raters' Q-matrices was assessed using Cohen's kappa. Results showed substantial variation across AI models, with Google Gemini 2.5 Pro achieving the highest agreement (Kappa = 0.63) with the validated Q-matrix, exceeding that of all human experts. A follow-up analysis in January 2026 using newer AI versions, however, revealed lower agreement with the validated Q-matrix. Implications and directions for future research are discussed.

</details>


### [308] [Root Cause Analysis Method Based on Large Language Models with Residual Connection Structures](https://arxiv.org/abs/2602.08804)
*Liming Zhou,Ailing Liu,Hongwei Liu,Min He,Heng Zhang*

Main category: cs.AI

TL;DR: RC-LLM 使用残差连接和大型语言模型集成多源 telemetry 数据，有效提升了微服务架构中的故障根源分析准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 复杂微服务架构中的复杂故障传播和高维度 telemetry 数据限制了现有故障根源分析方法的有效性。

Method: 提出了基于残差连接的大型语言模型 RCA 方法 RC-LLM。该方法设计了一种类似残差的分层融合结构来集成多源 telemetry 数据，并利用大型语言模型的上下文推理能力建模时间上的以及跨微服务的因果关系。

Result: 在 CCF-AIOps 微服务数据集上的实验结果显示，RC-LLM 在故障根源分析方面达到了较高的准确性和效率。

Conclusion: RC-LLM 通过结合残差连接结构和大型语言模型的特点，有效地改进了在复杂微服务环境中进行故障根源分析的技术方法。

Abstract: Root cause localization remain challenging in complex and large-scale microservice architectures. The complex fault propagation among microservices and the high dimensionality of telemetry data, including metrics, logs, and traces, limit the effectiveness of existing root cause analysis (RCA) methods. In this paper, a residual-connection-based RCA method using large language model (LLM), named RC-LLM, is proposed. A residual-like hierarchical fusion structure is designed to integrate multi-source telemetry data, while the contextual reasoning capability of large language models is leveraged to model temporal and cross-microservice causal dependencies. Experimental results on CCF-AIOps microservice datasets demonstrate that RC-LLM achieves strong accuracy and efficiency in root cause analysis.

</details>


### [309] [Negative-Aware Diffusion Process for Temporal Knowledge Graph Extrapolation](https://arxiv.org/abs/2602.08815)
*Yanglei Gan,Peng He,Yuxiang Cai,Run Lin,Guanyu Zhou,Qiao Liu*

Main category: cs.AI

TL;DR: 引入了一种名为NADEx的新模型，用于处理时间知识图谱（TKG）推理中的预测问题。该模型通过条件生成路径结合历史正负证据，并使用带有时间关系背景的变换器去噪器进行重建，从而提高预测准确性。实验结果显示了其在多个公开TKG基准上的优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型因为在生成路径上仅利用正证据而忽视了负背景信息，以及在训练时主要采用交叉熵排名目标缺乏对去噪嵌入校准的监督，存在性能改进的空间。因此，提出NADEx来解决这些问题。

Method: NADEx通过编码实体、关系和时间间隔的主体中心历史信息形成序列嵌入。模型在前向过程中扰动查询对象，使用Transformer去噪器在反向过程中重建，并通过批内负原型的余弦对齐正则化器进一步提高对不相关人员的决策边界。

Result: NADEx模型在四组公开的TKG基准测试上展示了最先进的性能表现。

Conclusion: NADEx通过综合历史正负证据和更加细致地监督去噪校准，有望提升时间知识图谱推理和预测的准确性。

Abstract: Temporal Knowledge Graph (TKG) reasoning seeks to predict future missing facts from historical evidence. While diffusion models (DM) have recently gained attention for their ability to capture complex predictive distributions, two gaps remain: (i) the generative path is conditioned only on positive evidence, overlooking informative negative context, and (ii) training objectives are dominated by cross-entropy ranking, which improves candidate ordering but provides little supervision over the calibration of the denoised embedding. To bridge this gap, we introduce Negative-Aware Diffusion model for TKG Extrapolation (NADEx). Specifically, NADEx encodes subject-centric histories of entities, relations and temporal intervals into sequential embeddings. NADEx perturbs the query object in the forward process and reconstructs it in reverse with a Transformer denoiser conditioned on the temporal-relational context. We further derive a cosine-alignment regularizer derived from batch-wise negative prototypes, which tightens the decision boundary against implausible candidates. Comprehensive experiments on four public TKG benchmarks demonstrate that NADEx delivers state-of-the-art performance.

</details>


### [310] [Steer2Adapt: Dynamically Composing Steering Vectors Elicits Efficient Adaptation of LLMs](https://arxiv.org/abs/2602.07276)
*Pengrui Han,Xueqiang Xu,Keyang Xuan,Peiyang Song,Siru Ouyang,Runchu Tian,Yuqing Jiang,Cheng Qian,Pengcheng Jiang,Jiashuo Sun,Junxia Cui,Ming Zhong,Ge Liu,Jiawei Han,Jiaxuan You*

Main category: cs.AI

TL;DR: STEER2ADAPT 提出了一种轻量级框架，通过组合预定义的引导向量而非从头学习来适应大规模语言模型。该方法在多个任务和模型上表现出色，特别是在推理和安全领域。


<details>
  <summary>Details</summary>
Motivation: 当前大多数引导方法依赖于单一静态的方向，缺乏灵活性，难以应对任务变化和复杂需求。STEER2ADAPT 提出了一种解决这一问题的新方法。

Method: STEER2ADAPT 通过捕捉共享的概念维度作为可重用的低维语义先验子空间，并通过少量示例动态发现基向量的组合来适应新任务。

Result: 在9个任务和3个模型中，STEER2ADAPT 在推理和安全领域取得了平均8.2%的提升，展示了其在数据效率、稳定性和透明性方面的优势。

Conclusion: STEER2ADAPT 在多个场景下提供了稳定而有效的适应方法，证明了其在大规模语言模型领域的应用潜力。

Abstract: Activation steering has emerged as a promising approach for efficiently adapting large language models (LLMs) to downstream behaviors. However, most existing steering methods rely on a single static direction per task or concept, making them inflexible under task variation and inadequate for complex tasks that require multiple coordinated capabilities. To address this limitation, we propose STEER2ADAPT, a lightweight framework that adapts LLMs by composing steering vectors rather than learning new ones from scratch. In many domains (e.g., reasoning or safety), tasks share a small set of underlying concept dimensions. STEER2ADAPT captures these dimensions as a reusable, low-dimensional semantic prior subspace, and adapts to new tasks by dynamically discovering a linear combination of basis vectors from only a handful of examples. Experiments across 9 tasks and 3 models in both reasoning and safety domains demonstrate the effectiveness of STEER2ADAPT, achieving an average improvement of 8.2%. Extensive analyses further show that STEER2ADAPT is a data-efficient, stable, and transparent inference-time adaptation method for LLMs.

</details>


### [311] [Deciding the Satisfiability of Combined Qualitative Constraint Networks](https://arxiv.org/abs/2602.08848)
*Quentin Cohen-Solal,Alexandre Niveau,Maroua Bouzid*

Main category: cs.AI

TL;DR: 该研究提出了一个统一框架，涵盖了多种扩展和组合形式的定性推理，重点解决了满足性决策的复杂性问题。


<details>
  <summary>Details</summary>
Motivation: 为了在不精确和不完整信息下进行推理，特别是在多尺度、时间序列和松散整合的场景下。

Method: 通过建立两个互补的定理来保证满足性决策为多项式，并将其应用于组合的规模-拓扑结果。

Result: 提出了一个统一的定性推理框架，解决了满足性决策的复杂性，并恢复了规模-拓扑组合的已知结果。

Conclusion: 该研究既扩展了定性推理的形式定义，又加深了对不同形式推理组合的理解。

Abstract: Among the various forms of reasoning studied in the context of artificial intelligence, qualitative reasoning makes it possible to infer new knowledge in the context of imprecise, incomplete information without numerical values. In this paper, we propose a formal framework unifying several forms of extensions and combinations of qualitative formalisms, including multi-scale reasoning, temporal sequences, and loose integrations. This framework makes it possible to reason in the context of each of these combinations and extensions, but also to study in a unified way the satisfiability decision and its complexity. In particular, we establish two complementary theorems guaranteeing that the satisfiability decision is polynomial, and we use them to recover the known results of the size-topology combination. We also generalize the main definition of qualitative formalism to include qualitative formalisms excluded from the definitions of the literature, important in the context of combinations.

</details>


### [312] [Efficient and Stable Reinforcement Learning for Diffusion Language Models](https://arxiv.org/abs/2602.08905)
*Jiawei Liu,Xiting Wang,Yuanyuan Zhong,Defu Lian,Yu Yang*

Main category: cs.AI

TL;DR: 本文提出了一种名为Spatio-Temporal Pruning（STP）的方法，通过时空剪枝来提升扩散大规模语言模型（dLLMs）在强化学习中的效率和稳定性。


<details>
  <summary>Details</summary>
Motivation: 由于强化学习应用到扩散大规模语言模型（dLLMs）时面临效率和稳定性的挑战，因此提出STP框架来通过时空剪枝同时提高RL的效率和稳定性。

Method: STP框架包含两种剪枝方式，分别是空间剪枝（通过静态先验约束探索空间）和时间剪枝（跳过冗余的后期精修步骤），以减少生成过程中的冗余。

Result: 理论分析表明，STP严格降低了对数似然估计的方差，从而确保更稳定的策略更新。实验结果表明，STP在效率和准确度方面都优于现有的先进基线。

Conclusion: 文章证明了STP的有效性，并提供了开源代码。

Abstract: Reinforcement Learning (RL) is crucial for unlocking the complex reasoning capabilities of Diffusion-based Large Language Models (dLLMs). However, applying RL to dLLMs faces unique challenges in efficiency and stability. To address these challenges, we propose Spatio-Temporal Pruning (STP), a framework designed to simultaneously improve the efficiency and stability of RL for dLLMs. STP compresses the redundancy in the generative process through: (1) \textit{spatial pruning}, which constrains the exploration space using static priors; and (2) \textit{temporal pruning}, which bypasses redundant late-stage refinement steps. Our theoretical analysis demonstrates that STP strictly reduces the variance of the log-likelihood estimation, thereby ensuring more stable policy updates. Extensive experiments demonstrate that STP surpasses state-of-the-art baselines in both efficiency and accuracy. Our code is available at https://github.com/Lolo1222/STP.

</details>


### [313] [CausalT5K: Diagnosing and Informing Refusal for Trustworthy Causal Reasoning of Skepticism, Sycophancy, Detection-Correction, and Rung Collapse](https://arxiv.org/abs/2602.08939)
*Longling Geng,Andy Ouyang,Theodore Wu,Daphne Barretto,Matthew John Hayes,Rachael Cooper,Yuqiao Zeng,Sameer Vijay,Gia Ancone,Ankit Rai,Matthew Wolfman,Patrick Flanagan,Edward Y. Chang*

Main category: cs.AI

TL;DR: CausalT5K 是一个包含5000多案例的诊断基准，涵盖10个领域，旨在诊断大型语言模型在因果推理中的失误，包括梯子坍塌、趋炎附势和结果判断不足等方面。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏系统诊断基准，大型语言模型在因果推理中的失误（如趋炎附势、梯子坍塌和结果判断不足）尚未得到有效整改。

Method: CausalT5K 通过将因果陷阱嵌入真实叙事中，并将性能分解为效用（灵敏度）和安全性（特异性），从而在人类-机器协作流程、迭代交叉验证循环和基于规则的、大模型和人工评分的综合验证下，实现了贝叶斯因果推理梯度的研究基础设施。

Result: 初步实验揭示了因果推理系统中存在的四象限控制景观，静态审计策略无法普遍适用，突显了 CausalT5K 对推进可信推理系统的价值。

Conclusion: CausalT5K 为系统诊断大型语言模型的因果推理错误提供了一个有效的基准，有助于推动可信推理系统的发展。

Abstract: LLM failures in causal reasoning, including sycophancy, rung collapse, and miscalibrated refusal, are well-documented, yet progress on remediation is slow because no benchmark enables systematic diagnosis. We introduce CausalT5K, a diagnostic benchmark of over 5,000 cases across 10 domains that tests three critical capabilities: (1) detecting rung collapse, where models answer interventional queries with associational evidence; (2) resisting sycophantic drift under adversarial pressure; and (3) generating Wise Refusals that specify missing information when evidence is underdetermined. Unlike synthetic benchmarks, CausalT5K embeds causal traps in realistic narratives and decomposes performance into Utility (sensitivity) and Safety (specificity), revealing failure modes invisible to aggregate accuracy. Developed through a rigorous human-machine collaborative pipeline involving 40 domain experts, iterative cross-validation cycles, and composite verification via rule-based, LLM, and human scoring, CausalT5K implements Pearl's Ladder of Causation as research infrastructure. Preliminary experiments reveal a Four-Quadrant Control Landscape where static audit policies universally fail, a finding that demonstrates CausalT5K's value for advancing trustworthy reasoning systems. Repository: https://github.com/genglongling/CausalT5kBench

</details>


### [314] [CoRefine: Confidence-Guided Self-Refinement for Adaptive Test-Time Compute](https://arxiv.org/abs/2602.08948)
*Chen Jin,Ryutaro Tanno,Tom Diethe,Philip Teare*

Main category: cs.AI

TL;DR: CoRefine 是一种轻量化的自校正方法，通过少量参数的 Conv1D 控制器进行基于信心的自我校正，能够显著减少计算量并提高推理准确性，适用于多种推理基准和模型。


<details>
  <summary>Details</summary>
Motivation: 解决大规模语言模型在推理时依赖大规模并行解码带来的计算成本问题，提出 CoRefine 方法以降低计算资源消耗并保持较高的推理准确性。

Method: CoRefine 使用一个轻量级的 211k 参数 Conv1D 控制器，通过分析推理过程中的信心水平来决定是否结束推理、重新检查或尝试不同方法。该控制器能够实现精准且有针对性地自我纠正。

Result: CoRefine 方法在多个推理基准测试和三种开源模型上展现了良好的性能，平均每个问题只需 2.7 次自我纠正步骤，相对于512样本基线可减少大约190倍的令牌消耗。当控制器自信停止时，其精确度达到惊人的92.6%。

Conclusion: 该方法通过信心控制提供了可扩展的推理模块化工具，适用于可靠性不完美的验证环境，并为长程推理提供了可能的解决方案。

Abstract: Large Language Models (LLMs) often rely on test-time scaling via parallel decoding (for example, 512 samples) to boost reasoning accuracy, but this incurs substantial compute. We introduce CoRefine, a confidence-guided self-refinement method that achieves competitive accuracy using a fraction of the tokens via a lightweight 211k-parameter Conv1D controller atop a frozen LLM. The controller consumes full-trace confidence to decide whether to halt, re-examine, or try a different approach, enabling targeted self-correction with an average of 2.7 refinement steps per problem and roughly 190-fold token reduction relative to 512-sample baselines. Across diverse reasoning benchmarks and three open-source models, the controller achieves 92.6 percent precision when it confidently halts, indicating that confidence dynamics reliably signal correctness without ground-truth verification. We extend this to CoRefine-Tree, a hybrid sequential-parallel variant that adaptively balances exploration and exploitation, with easy serving integration and verifier compatibility. By treating confidence as a control signal rather than a correctness guarantee, CoRefine provides a modular primitive for scalable reasoning and agentic settings with imperfect verifiers.

</details>


### [315] [stable-worldmodel-v1: Reproducible World Modeling Research and Evaluation](https://arxiv.org/abs/2602.08968)
*Lucas Maes,Quentin Le Lidec,Dan Haramati,Nassim Massaudi,Damien Scieur,Yann LeCun,Randall Balestriero*

Main category: cs.AI

TL;DR: 介绍了stable-worldmodel (SWM)，这是一个模块化、经过测试和文档化的环境模型研究生态系统，旨在解决现有世界模型存在的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的世界模型实现存在不可复用、易出错和评估标准不统一的问题，SWM旨在解决这些问题，提高研究效率和结果的可靠性和可重复性。

Method: SWM提供了高效的数据收集工具、标准化的环境、规划算法和基准实现。此外，每个在SWM中的环境都允许控制变异因素，包括视觉和物理属性，以支持鲁棒性和连续学习研究。

Result: SWM通过被用来研究DINO-WM的零样本鲁棒性展示出了其实用性。

Conclusion: SWM的成功实施表明，它可以作为环境模型研究的基础，促进更有效的研究和更可靠的结果。

Abstract: World Models have emerged as a powerful paradigm for learning compact, predictive representations of environment dynamics, enabling agents to reason, plan, and generalize beyond direct experience. Despite recent interest in World Models, most available implementations remain publication-specific, severely limiting their reusability, increasing the risk of bugs, and reducing evaluation standardization. To mitigate these issues, we introduce stable-worldmodel (SWM), a modular, tested, and documented world-model research ecosystem that provides efficient data-collection tools, standardized environments, planning algorithms, and baseline implementations. In addition, each environment in SWM enables controllable factors of variation, including visual and physical properties, to support robustness and continual learning research. Finally, we demonstrate the utility of SWM by using it to study zero-shot robustness in DINO-WM.

</details>


### [316] [InternAgent-1.5: A Unified Agentic Framework for Long-Horizon Autonomous Scientific Discovery](https://arxiv.org/abs/2602.08990)
*Shiyang Feng,Runmin Ma,Xiangchao Yan,Yue Fan,Yusong Hu,Songtao Huang,Shuaiyu Zhang,Zongsheng Cao,Tianshuo Peng,Jiakang Yuan,Zijie Guo,Zhijie Zhong,Shangheng Du,Weida Wang,Jinxin Shi,Yuhao Zhou,Xiaohan He,Zhiyin Yu,Fangchen Yu,Qihao Zheng,Jiamin Wu,Mianxin Liu,Chi Zhang,Shaowei Hou,Shuya Li,Yankai Jiang,Wenjie Lou,Lilong Wang,Zifu Wang,Jiong Wang,Wanghan Xu,Yue Deng,Dongrui Liu,Yiheng Wang,Wenlong Zhang,Fenghua Ling,Shufei Zhang,Xiaosong Wang,Shuangjia Zheng,Xun Huang,Siqi Sun,Shuyue Hu,Peng Ye,Chunfeng Song,Bin Wang,Conghui He,Yihao Liu,Xin Li,Qibin Hou,Tao Chen,Xiangyu Yue,Bin Wang,Liang He,Dahua Lin,Bowen Zhou,Bo Zhang,Lei Bai*

Main category: cs.AI

TL;DR: InternAgent-1.5 是一种统一的系统，用于在计算和经验领域实现端到端的科学发现。它在多个科学推理基准和不同类型的发现任务上表现出色，提供了自主科学研究的一般和可扩展框架。


<details>
  <summary>Details</summary>
Motivation: 论文旨在构建一个统一的系统 InternAgent-1.5，以实现跨计算和经验领域的端到端科学发现。

Method: 系统采用结构化的架构，由生成、验证、进化三个协调的子系统组成，并支持深层研究、解决方案优化和长期记忆的基础能力，从而实现在长时间发现周期内连续操作并保持一致性和改进。

Result: InternAgent-1.5 在 GAIA、HLE、GPQA 和 FrontierScience 等科学推理基准测试及算法和经验发现任务中表现出色，达到了领先性能。

Conclusion: InternAgent-1.5 提供了自主科学研究的通用和可扩展框架，展示了其强大的基础能力和广泛的适用性。

Abstract: We introduce InternAgent-1.5, a unified system designed for end-to-end scientific discovery across computational and empirical domains. The system is built on a structured architecture composed of three coordinated subsystems for generation, verification, and evolution. These subsystems are supported by foundational capabilities for deep research, solution optimization, and long horizon memory. The architecture allows InternAgent-1.5 to operate continuously across extended discovery cycles while maintaining coherent and improving behavior. It also enables the system to coordinate computational modeling and laboratory experimentation within a single unified system. We evaluate InternAgent-1.5 on scientific reasoning benchmarks such as GAIA, HLE, GPQA, and FrontierScience, and the system achieves leading performance that demonstrates strong foundational capabilities. Beyond these benchmarks, we further assess two categories of discovery tasks. In algorithm discovery tasks, InternAgent-1.5 autonomously designs competitive methods for core machine learning problems. In empirical discovery tasks, it executes complete computational or wet lab experiments and produces scientific findings in earth, life, biological, and physical domains. Overall, these results show that InternAgent-1.5 provides a general and scalable framework for autonomous scientific discovery.

</details>


### [317] [iGRPO: Self-Feedback-Driven LLM Reasoning](https://arxiv.org/abs/2602.09000)
*Ali Hatamizadeh,Shrimai Prabhumoye,Igor Gitman,Ximing Lu,Seungju Han,Wei Ping,Yejin Choi,Jan Kautz*

Main category: cs.AI

TL;DR: 本文提出了一种名为iGRPO的迭代组相对策略优化方法，通过模型生成的草案进行自我条件化，改进了LLM在复杂数学问题上的表现。


<details>
  <summary>Details</summary>
Motivation: 由于大语言模型在解决复杂数学问题时无法产生准确和一致的结果，本文提出了一种新的迭代组相对策略优化方法以提高这些模型的质量与可靠性。

Method: iGRPO是一种两阶段的方法，首先生成多个探索性的草案并选择具有最高奖励的草案，然后将其添加到原始提示中并进行条件化改进，优化策略以超越之前的最佳尝试。该方法使用了基于组相对奖励归一化的策略优化。

Result: 在各种基础模型上，iGRPO在基准测试中表现出色，优于GRPO。在特定训练数据集下，iGRPO达到了创纪录的85.62%和79.64%的AIME24和AIME25准确率。

Conclusion: 迭代自我反馈式的强化学习可能为可验证的数学推理的进步提供新的途径。

Abstract: Large Language Models (LLMs) have shown promise in solving complex mathematical problems, yet they still fall short of producing accurate and consistent solutions. Reinforcement Learning (RL) is a framework for aligning these models with task-specific rewards, improving overall quality and reliability. Group Relative Policy Optimization (GRPO) is an efficient, value-function-free alternative to Proximal Policy Optimization (PPO) that leverages group-relative reward normalization. We introduce Iterative Group Relative Policy Optimization (iGRPO), a two-stage extension of GRPO that adds dynamic self-conditioning through model-generated drafts. In Stage 1, iGRPO samples multiple exploratory drafts and selects the highest-reward draft using the same scalar reward signal used for optimization. In Stage 2, it appends this best draft to the original prompt and applies a GRPO-style update on draft-conditioned refinements, training the policy to improve beyond its strongest prior attempt. Under matched rollout budgets, iGRPO consistently outperforms GRPO across base models (e.g., Nemotron-H-8B-Base-8K and DeepSeek-R1 Distilled), validating its effectiveness on diverse reasoning benchmarks. Moreover, applying iGRPO to OpenReasoning-Nemotron-7B trained on AceReason-Math achieves new state-of-the-art results of 85.62\% and 79.64\% on AIME24 and AIME25, respectively. Ablations further show that the refinement wrapper generalizes beyond GRPO variants, benefits from a generative judge, and alters learning dynamics by delaying entropy collapse. These results underscore the potential of iterative, self-feedback-based RL for advancing verifiable mathematical reasoning.

</details>


### [318] [Data Science and Technology Towards AGI Part I: Tiered Data Management](https://arxiv.org/abs/2602.09003)
*Yudong Wang,Zixuan Fu,Hengyu Zhao,Chen Zhao,Chuyue Zhou,Xinle Lin,Hongya Lyu,Shuaikang Xue,Yi Yi,Yingjiao Wang,Zhi Zheng,Yuzhou Zhang,Jie Zhou,Chaojun Xiao,Xu Han,Zhiyuan Liu,Maosong Sun*

Main category: cs.AI

TL;DR: 本文提出了一种分层数据管理框架，以支持LLM的全生命周期，专注于数据模型协同进化的新阶段，有效提高了训练效率和模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型研究依赖于单纯的数据量扩展，这种方法遇到了数据获取成本、数据可用性及训练效率的瓶颈。为了解决这些问题，本文提出了一个分层数据管理框架，旨在促进数据和模型的相互引导与优化。

Method: 该框架设计了有四级（L0-L4）的数据管理系统，从未经整理的原始资源到组织良好的可验证知识。每个层级都具有独特的数据特性、管理策略和训练角色。同时，LLMs 在数据管理过程中的质量评分和内容编辑中被充分应用，以细化不同层级的数据。此类框架平衡了数据质量、获取成本与边际训练收益，提供了一种可扩展和可持续的数据管理系统。

Result: 实验结果表明，分层数据利用显著提升了训练效率和模型性能。采用此类方法，框架证明了其有效性和实用性。

Conclusion: 本文提出了一种创新性分层数据管理框架，不仅能有效管理复杂多样的数据，还能够显著改善大型语言模型的训练过程。该框架通过对数据的有意识分配支持多个阶段的训练，为研究AGI奠定了基础，旨在促进更高效、更经济的数据驱动模型开发。为推动进一步的研究，作者已公开了分层数据集及处理工具。

Abstract: The development of artificial intelligence can be viewed as an evolution of data-driven learning paradigms, with successive shifts in data organization and utilization continuously driving advances in model capability. Current LLM research is dominated by a paradigm that relies heavily on unidirectional scaling of data size, increasingly encountering bottlenecks in data availability, acquisition cost, and training efficiency. In this work, we argue that the development of AGI is entering a new phase of data-model co-evolution, in which models actively guide data management while high-quality data, in turn, amplifies model capabilities. To implement this vision, we propose a tiered data management framework, designed to support the full LLM training lifecycle across heterogeneous learning objectives and cost constraints. Specifically, we introduce an L0-L4 tiered data management framework, ranging from raw uncurated resources to organized and verifiable knowledge. Importantly, LLMs are fully used in data management processes, such as quality scoring and content editing, to refine data across tiers. Each tier is characterized by distinct data properties, management strategies, and training roles, enabling data to be strategically allocated across LLM training stages, including pre-training, mid-training, and alignment. The framework balances data quality, acquisition cost, and marginal training benefit, providing a systematic approach to scalable and sustainable data management. We validate the effectiveness of the proposed framework through empirical studies, in which tiered datasets are constructed from raw corpora and used across multiple training phases. Experimental results demonstrate that tier-aware data utilization significantly improves training efficiency and model performance. To facilitate further research, we release our tiered datasets and processing tools to the community.

</details>


### [319] [GEBench: Benchmarking Image Generation Models as GUI Environments](https://arxiv.org/abs/2602.09007)
*Haodong Li,Jingwei Wu,Quan Sun,Guopeng Li,Juanxi Tian,Huanyu Zhang,Yanlin Lai,Ruichuan An,Hongbo Peng,Yuhong Dai,Chenxi Li,Chunmei Qing,Jia Wang,Ziyang Meng,Zheng Ge,Xiangyu Zhang,Daxin Jiang*

Main category: cs.AI

TL;DR: GEBench 是一个为评估 GUI 生成中的动态交互和时间一致性而设计的新基准，它包含700个样例并使用GE-Score五维度评估模型。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注视觉保真度，GEBench 克服了这一空白，着眼于 GUI 特定领域的状态转换和时间连贯性。

Method: GEBench 包含5种任务类别，涵盖了单步交互和多步轨迹，同时支持具体到虚构场景，还考虑了着地点定位。GE-Score 评估目标实现、交互逻辑、内容一致性、UI 可能性和视觉质量。

Result: 当前模型在单一步骤转换上表现良好，但在长时间交互序列中的时间连贯性和空间定位方面存在重大挑战，指出了图标的理解和文本渲染以及定位精度作为主要瓶颈。

Conclusion: GEBench 为系统评估提供基础，并为未来构建高保真生成 GUI 环境的研究指明了方向。

Abstract: Recent advancements in image generation models have enabled the prediction of future Graphical User Interface (GUI) states based on user instructions. However, existing benchmarks primarily focus on general domain visual fidelity, leaving the evaluation of state transitions and temporal coherence in GUI-specific contexts underexplored. To address this gap, we introduce GEBench, a comprehensive benchmark for evaluating dynamic interaction and temporal coherence in GUI generation. GEBench comprises 700 carefully curated samples spanning five task categories, covering both single-step interactions and multi-step trajectories across real-world and fictional scenarios, as well as grounding point localization. To support systematic evaluation, we propose GE-Score, a novel five-dimensional metric that assesses Goal Achievement, Interaction Logic, Content Consistency, UI Plausibility, and Visual Quality. Extensive evaluations on current models indicate that while they perform well on single-step transitions, they struggle significantly with maintaining temporal coherence and spatial grounding over longer interaction sequences. Our findings identify icon interpretation, text rendering, and localization precision as critical bottlenecks. This work provides a foundation for systematic assessment and suggests promising directions for future research toward building high-fidelity generative GUI environments. The code is available at: https://github.com/stepfun-ai/GEBench.

</details>
