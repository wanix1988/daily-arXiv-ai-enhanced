<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 141]
- [cs.CL](#cs.CL) [Total: 55]
- [cs.AI](#cs.AI) [Total: 34]
- [cs.AR](#cs.AR) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Characterizing Motion Encoding in Video Diffusion Timesteps](https://arxiv.org/abs/2512.22175)
*Vatsal Baherwani,Yixuan Ren,Abhinav Shrivastava*

Main category: cs.CV

TL;DR: 通过大规模定量研究，该研究揭示了视频扩散模型在时间步中运动编码和外观编码之间的竞争边界，并简化了一次性运动定制的范式。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到视频的扩散模型在时间步中如何编码运动仍未得到系统地表征，该研究旨在量化这一过程。

Method: 通过在指定时间步范围内注入新条件来引入运动编码和外观保真之间的权衡，通过大规模定量研究来表征这一时间步。

Result: 发现了一个早期的运动主导阶段和一个后来的外观主导阶段，界定了时间和空间中的运动和外观分离的原则。

Conclusion: 新的方法在训练和推理阶段仅限制在运动主导阶段，实现了强大的运动转移，无需额外的去偏差模块或特定目标。

Abstract: Text-to-video diffusion models synthesize temporal motion and spatial appearance through iterative denoising, yet how motion is encoded across timesteps remains poorly understood. Practitioners often exploit the empirical heuristic that early timesteps mainly shape motion and layout while later ones refine appearance, but this behavior has not been systematically characterized. In this work, we proxy motion encoding in video diffusion timesteps by the trade-off between appearance editing and motion preservation induced when injecting new conditions over specified timestep ranges, and characterize this proxy through a large-scale quantitative study. This protocol allows us to factor motion from appearance by quantitatively mapping how they compete along the denoising trajectory. Across diverse architectures, we consistently identify an early, motion-dominant regime and a later, appearance-dominant regime, yielding an operational motion-appearance boundary in timestep space. Building on this characterization, we simplify current one-shot motion customization paradigm by restricting training and inference to the motion-dominant regime, achieving strong motion transfer without auxiliary debiasing modules or specialized objectives. Our analysis turns a widely used heuristic into a spatiotemporal disentanglement principle, and our timestep-constrained recipe can serve as ready integration into existing motion transfer and editing methods.

</details>


### [2] [Enhancing Medical Data Analysis through AI-Enhanced Locally Linear Embedding: Applications in Medical Point Location and Imagery](https://arxiv.org/abs/2512.22182)
*Hassan Khalid,Muhammad Mahad Khaliq,Muhammad Jawad Bashir*

Main category: cs.CV

TL;DR: 该论文提出了一种将人工智能与局部线性嵌入（LLE）结合的新颖方法，旨在提高医疗计费和转录系统的准确性和效率。实验结果显示，该模型显著提高了数据处理精度和运营效率。


<details>
  <summary>Details</summary>
Motivation: 医学领域的快速发展引发了对提高医疗计费和转录系统准确性和效率的需求，尤其是在处理高维度的医疗数据时。因此，研究者提出了一种结合人工智与LLE的方法，以减少人为错误并优化操作流程。

Method: 该论文提出了一个AI增强的LLE模型，通过数学建模来实现，其目的是在医疗计费和转录服务中提高准确性和效率。该模型通过自动化处理流程来说，减少人为错误并快速准确地生成患者护理记录和财务交易。

Result: 实验表明，该AI增强的LLE模型在数据处理准确性和运营效率方面有了显著提升。

Conclusion: 该研究不仅强调了AI增强的LLE在医疗数据分析中的潜力，还为未来的更广泛医疗领域应用奠定了基础。

Abstract: The rapid evolution of Artificial intelligence in healthcare has opened avenues for enhancing various processes, including medical billing and transcription. This paper introduces an innovative approach by integrating AI with Locally Linear Embedding (LLE) to revolutionize the handling of high-dimensional medical data. This AI-enhanced LLE model is specifically tailored to improve the accuracy and efficiency of medical billing systems and transcription services. By automating these processes, the model aims to reduce human error and streamline operations, thereby facilitating faster and more accurate patient care documentation and financial transactions. This paper provides a comprehensive mathematical model of AI-enhanced LLE, demonstrating its application in real-world healthcare scenarios through a series of experiments. The results indicate a significant improvement in data processing accuracy and operational efficiency. This study not only underscores the potential of AI-enhanced LLE in medical data analysis but also sets a foundation for future research into broader healthcare applications.

</details>


### [3] [Unbiased Visual Reasoning with Controlled Visual Inputs](https://arxiv.org/abs/2512.22183)
*Zhaonan Li,Shijie Lu,Fei Wang,Jacob Dineen,Xiao Ye,Zhikun Xu,Siyi Liu,Young Min Cho,Bangzheng Li,Daniel Chang,Kenny Nguyen,Qizheng Yang,Muhao Chen,Ben Zhou*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: End-to-end Vision-language Models (VLMs) often answer visual questions by exploiting spurious correlations instead of causal visual evidence, and can become more shortcut-prone when fine-tuned. We introduce VISTA (Visual-Information Separation for Text-based Analysis), a modular framework that decouples perception from reasoning via an explicit information bottleneck. A frozen VLM sensor is restricted to short, objective perception queries, while a text-only LLM reasoner decomposes each question, plans queries, and aggregates visual facts in natural language. This controlled interface defines a reward-aligned environment for training unbiased visual reasoning with reinforcement learning. Instantiated with Qwen2.5-VL and Llama3.2-Vision sensors, and trained with GRPO from only 641 curated multi-step questions, VISTA significantly improves robustness to real-world spurious correlations on SpuriVerse (+16.29% with Qwen-2.5-VL-7B and +6.77% with Llama-3.2-Vision-11B), while remaining competitive on MMVP and a balanced SeedBench subset. VISTA transfers robustly across unseen VLM sensors and is able to recognize and recover from VLM perception failures. Human analysis further shows that VISTA's reasoning traces are more neutral, less reliant on spurious attributes, and more explicitly grounded in visual evidence than end-to-end VLM baselines.

</details>


### [4] [SAMM2D: Scale-Aware Multi-Modal 2D Dual-Encoder for High-Sensitivity Intracrania Aneurysm Screening](https://arxiv.org/abs/2512.22185)
*Antara Titikhsha,Divyanshu Tak*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Effective aneurysm detection is essential to avert life-threatening hemorrhages, but it remains challenging due to the subtle morphology of the aneurysm, pronounced class imbalance, and the scarcity of annotated data. We introduce SAMM2D, a dual-encoder framework that achieves an AUC of 0.686 on the RSNA intracranial aneurysm dataset; an improvement of 32% over the clinical baseline. In a comprehensive ablation across six augmentation regimes, we made a striking discovery: any form of data augmentation degraded performance when coupled with a strong pretrained backbone. Our unaugmented baseline model outperformed all augmented variants by 1.75--2.23 percentage points (p < 0.01), overturning the assumption that "more augmentation is always better" in low-data medical settings. We hypothesize that ImageNet-pretrained features already capture robust invariances, rendering additional augmentations both redundant and disruptive to the learned feature manifold. By calibrating the decision threshold, SAMM2D reaches 95% sensitivity, surpassing average radiologist performance, and translates to a projected \$13.9M in savings per 1,000 patients in screening applications. Grad-CAM visualizations confirm that 85% of true positives attend to relevant vascular regions (62% IoU with expert annotations), demonstrating the model's clinically meaningful focus. Our results suggest that future medical imaging workflows could benefit more from strong pretraining than from increasingly complex augmentation pipelines.

</details>


### [5] [HookMIL: Revisiting Context Modeling in Multiple Instance Learning for Computational Pathology](https://arxiv.org/abs/2512.22188)
*Xitong Ling,Minxi Ouyang,Xiaoxiao Li,Jiawen Li,Ying Chen,Yuxuan Sun,Xinrui Chen,Tian Guan,Xiaoping Liu,Yonghong He*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Multiple Instance Learning (MIL) has enabled weakly supervised analysis of whole-slide images (WSIs) in computational pathology. However, traditional MIL approaches often lose crucial contextual information, while transformer-based variants, though more expressive, suffer from quadratic complexity and redundant computations. To address these limitations, we propose HookMIL, a context-aware and computationally efficient MIL framework that leverages compact, learnable hook tokens for structured contextual aggregation. These tokens can be initialized from (i) key-patch visual features, (ii) text embeddings from vision-language pathology models, and (iii) spatially grounded features from spatial transcriptomics-vision models. This multimodal initialization enables Hook Tokens to incorporate rich textual and spatial priors, accelerating convergence and enhancing representation quality. During training, Hook tokens interact with instances through bidirectional attention with linear complexity. To further promote specialization, we introduce a Hook Diversity Loss that encourages each token to focus on distinct histopathological patterns. Additionally, a hook-to-hook communication mechanism refines contextual interactions while minimizing redundancy. Extensive experiments on four public pathology datasets demonstrate that HookMIL achieves state-of-the-art performance, with improved computational efficiency and interpretability. Codes are available at https://github.com/lingxitong/HookMIL.

</details>


### [6] [Tiny-YOLOSAM: Fast Hybrid Image Segmentation](https://arxiv.org/abs/2512.22193)
*Kenneth Xu,Songhan Wu*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The Segment Anything Model (SAM) enables promptable, high-quality segmentation but is often too computationally expensive for latency-critical settings. TinySAM is a lightweight, distilled SAM variant that preserves strong zero-shot mask quality, yet its "segment-everything" mode still requires hundreds of prompts and remains slow in practice. We first replicate TinySAM on COCO val2017 using official checkpoints, matching the reported AP within 0.03%, establishing a reliable experimental baseline. Building on this, we propose Tiny-YOLOSAM, a fast hybrid pipeline that uses a recent YOLO detector (YOLOv12) to generate box prompts for TinySAM on salient foreground objects, and supplements uncovered regions with sparse point prompts sampled only where YOLO-guided masks provide no coverage. On COCO val2017, the hybrid system substantially improves class-agnostic coverage (AR from 16.4% to 77.1%, mIoU from 19.2% to 67.8%) while reducing end-to-end runtime from 49.20s/image to 10.39s/image (4.7x) on an Apple M1 Pro CPU. These results suggest detector-guided prompting combined with targeted sparse sampling as an effective alternative to dense "segment-everything" prompting for practical full-scene segmentation.

</details>


### [7] [Quadrant Segmentation VLM with Few-Shot Adaptation and OCT Learning-based Explainability Methods for Diabetic Retinopathy](https://arxiv.org/abs/2512.22197)
*Shivum Telang*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Diabetic Retinopathy (DR) is a leading cause of vision loss worldwide, requiring early detection to preserve sight. Limited access to physicians often leaves DR undiagnosed. To address this, AI models utilize lesion segmentation for interpretability; however, manually annotating lesions is impractical for clinicians. Physicians require a model that explains the reasoning for classifications rather than just highlighting lesion locations. Furthermore, current models are one-dimensional, relying on a single imaging modality for explainability and achieving limited effectiveness. In contrast, a quantitative-detection system that identifies individual DR lesions in natural language would overcome these limitations, enabling diverse applications in screening, treatment, and research settings. To address this issue, this paper presents a novel multimodal explainability model utilizing a VLM with few-shot learning, which mimics an ophthalmologist's reasoning by analyzing lesion distributions within retinal quadrants for fundus images. The model generates paired Grad-CAM heatmaps, showcasing individual neuron weights across both OCT and fundus images, which visually highlight the regions contributing to DR severity classification. Using a dataset of 3,000 fundus images and 1,000 OCT images, this innovative methodology addresses key limitations in current DR diagnostics, offering a practical and comprehensive tool for improving patient outcomes.

</details>


### [8] [TCFormer: A 5M-Parameter Transformer with Density-Guided Aggregation for Weakly-Supervised Crowd Counting](https://arxiv.org/abs/2512.22203)
*Qiang Guo,Rubo Zhang,Bingbing Zhang,Junjie Liu,Jianqing Liu*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Crowd counting typically relies on labor-intensive point-level annotations and computationally intensive backbones, restricting its scalability and deployment in resource-constrained environments. To address these challenges, this paper proposes the TCFormer, a tiny, ultra-lightweight, weakly-supervised transformer-based crowd counting framework with only 5 million parameters that achieves competitive performance. Firstly, a powerful yet efficient vision transformer is adopted as the feature extractor, the global context-aware capabilities of which provides semantic meaningful crowd features with a minimal memory footprint. Secondly, to compensate for the lack of spatial supervision, we design a feature aggregation mechanism termed the Learnable Density-Weighted Averaging module. This module dynamically re-weights local tokens according to predicted density scores, enabling the network to adaptively modulate regional features based on their specific density characteristics without the need for additional annotations. Furthermore, this paper introduces a density-level classification loss, which discretizes crowd density into distinct grades, thereby regularizing the training process and enhancing the model's classification power across varying levels of crowd density. Therefore, although TCformer is trained under a weakly-supervised paradigm utilizing only image-level global counts, the joint optimization of count and density-level losses enables the framework to achieve high estimation accuracy. Extensive experiments on four benchmarks including ShanghaiTech A/B, UCF-QNRF, and NWPU datasets demonstrate that our approach strikes a superior trade-off between parameter efficiency and counting accuracy and can be a good solution for crowd counting tasks in edge devices.

</details>


### [9] [VLM-PAR: A Vision Language Model for Pedestrian Attribute Recognition](https://arxiv.org/abs/2512.22217)
*Abdellah Zakaria Sellam,Salah Eddine Bekhouche,Fadi Dornaika,Cosimo Distante,Abdenour Hadid*

Main category: cs.CV

TL;DR: VLM-PAR 使用冻结的 SigLIP 2 多语言编码器，通过紧凑的视觉特征交叉注意融合对图像和提示嵌入进行对齐，显著提高了 PA100K、PETA 和 Market-1501 的 Pedestrian Attribute Recognition（PAR）基准测试性能。


<details>
  <summary>Details</summary>
Motivation: PAR 遭遇类别不平衡、复杂属性共生关系和领域迁移的问题，因此需要一种能够有效处理这些挑战的方法。

Method: VLM-PAR 是一个模块化的视觉-语言框架，基于冻结的 SigLIP 2 多语言编码器。通过紧凑的视觉特征交叉注意融合，它对图像和提示嵌入进行了对齐。

Result: VLM-PAR 在严重不平衡的 PA100K 基准测试上实现了显著的准确性改进，达到了新的最佳性能，同时在 PETA 和 Market-1501 基准测试中也取得了显著的性能跃升。

Conclusion: 实验证明，将大规模的视觉-语言预训练与跨模态精炼相结合可以有效地克服PAR中的类别不平衡和泛化挑战。

Abstract: Pedestrian Attribute Recognition (PAR) involves predicting fine-grained attributes such as clothing color, gender, and accessories from pedestrian imagery, yet is hindered by severe class imbalance, intricate attribute co-dependencies, and domain shifts. We introduce VLM-PAR, a modular vision-language framework built on frozen SigLIP 2 multilingual encoders. By first aligning image and prompt embeddings via refining visual features through a compact cross-attention fusion, VLM-PAR achieves significant accuracy improvement on the highly imbalanced PA100K benchmark, setting a new state-of-the-art performance, while also delivering significant gains in mean accuracy across PETA and Market-1501 benchmarks. These results underscore the efficacy of integrating large-scale vision-language pretraining with targeted cross-modal refinement to overcome imbalance and generalization challenges in PAR.

</details>


### [10] [On Extending Semantic Abstraction for Efficient Search of Hidden Objects](https://arxiv.org/abs/2512.22220)
*Tasha Pais,Nikhilesh Belulkar*

Main category: cs.CV

TL;DR: 该论文提出了基于语义抽象的2D视觉Transformer在检测隐藏物体方面表现出色，能够通过历史数据更高效地定位隐藏物体，显著提高了查找速度。


<details>
  <summary>Details</summary>
Motivation: 通过更好地理解和利用2D视觉Transformer的激活图，该研究旨在使家用机器人能在快速定位隐藏物体方面更加高效。

Method: 论文利用2D视觉Transformer的激活图来构建“抽象物体”表示，并应用该方法进行隐藏物体的3D定位与补全。通过对历史数据中的物体常见位置进行学习，利用语义抽象框架来提高查找效率。

Result: 该方法能够准确且快速地确定完全隐藏物体的3D位置，相比随机搜索大大缩短了寻找时间。

Conclusion: 该研究提供了在隐藏物体识别方面的技术扩展，增强了家用机器人的物体查找能力，能够在实际应用场景中节约时间和提高效率。

Abstract: Semantic Abstraction's key observation is that 2D VLMs' relevancy activations roughly correspond to their confidence of whether and where an object is in the scene. Thus, relevancy maps are treated as "abstract object" representations. We use this framework for learning 3D localization and completion for the exclusive domain of hidden objects, defined as objects that cannot be directly identified by a VLM because they are at least partially occluded. This process of localizing hidden objects is a form of unstructured search that can be performed more efficiently using historical data of where an object is frequently placed. Our model can accurately identify the complete 3D location of a hidden object on the first try significantly faster than a naive random search. These extensions to semantic abstraction hope to provide household robots with the skills necessary to save time and effort when looking for lost objects.

</details>


### [11] [VideoScaffold: Elastic-Scale Visual Hierarchies for Streaming Video Understanding in MLLMs](https://arxiv.org/abs/2512.22226)
*Naishan Zheng,Jie Huang,Qingpei Guo,Feng Zhao*

Main category: cs.CV

TL;DR: VideoScaffold 是一种适用于流媒体视频理解的动态表征框架，它通过弹性扩展事件分割（EES）和层次事件聚合（HEC）机制，自动调整事件粒度，保持细粒度视觉语义，从而实现从精细帧级理解到抽象事件推理的平滑过渡，并且在多种基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前方法在处理长时间视频时存在诸多挑战，如帧间冗余较大、需要时空一致的表示。因此，开发一种能够适应视频时长变化、保持细粒度视觉语义并能进行时空信息整合的流媒体视频理解框架变得必要。

Method: VideoScaffold 包括两个关键模块：弹性扩展事件分割（EES）和层次事件聚合（HEC）。EES 借助预测引导的分割机制动态优化事件边界；HEC 则逐步整合具有语义关联的片段形成多级抽象。两者配合使 VideoScaffold 能够随着视频流的变化从细粒度帧级理解平滑过渡到抽象事件推理。

Result: 在多种基准测试中，VideoScaffold 获得了最先进的性能，证明了其高效性和有效性。同时，该框架具备模块化和插件式特性，可以无缝扩展现有的基于图片的 MLLMs 以实现连续视频理解。

Conclusion: VideoScaffold 在流媒体视频理解领域展现出巨大的潜力，能够在保持视觉细粒度的同时进行时空信息的有效整合，为动态视频内容的理解提供了一种新的框架思路。

Abstract: Understanding long videos with multimodal large language models (MLLMs) remains challenging due to the heavy redundancy across frames and the need for temporally coherent representations. Existing static strategies, such as sparse sampling, frame compression, and clustering, are optimized for offline settings and often produce fragmented or over-compressed outputs when applied to continuous video streams. We present VideoScaffold, a dynamic representation framework designed for streaming video understanding. It adaptively adjusts event granularity according to video duration while preserving fine-grained visual semantics. VideoScaffold introduces two key components: Elastic-Scale Event Segmentation (EES), which performs prediction-guided segmentation to dynamically refine event boundaries, and Hierarchical Event Consolidation (HEC), which progressively aggregates semantically related segments into multi-level abstractions. Working in concert, EES and HEC enable VideoScaffold to transition smoothly from fine-grained frame understanding to abstract event reasoning as the video stream unfolds. Extensive experiments across both offline and streaming video understanding benchmarks demonstrate that VideoScaffold achieves state-of-the-art performance. The framework is modular and plug-and-play, seamlessly extending existing image-based MLLMs to continuous video comprehension. The code is available at https://github.com/zheng980629/VideoScaffold.

</details>


### [12] [KAN-FPN-Stem:A KAN-Enhanced Feature Pyramid Stem for Boosting ViT-based Pose Estimation](https://arxiv.org/abs/2512.22228)
*HaoNan Tang*

Main category: cs.CV

TL;DR: 本文提出了一种新的KAN增强FPN茎架构，通过在多尺度融合过程中的终端替代标准线性3x3平滑卷积为KAN卷积层，显著提升了基于ViT的姿态估计模型性能。


<details>
  <summary>Details</summary>
Motivation: ViT在密集预测任务中表现出色，但其性能经常受限于过于简化的前端设计，特别是Naive Patchification机制处理多尺度变化能力不足，导致信息不可逆丢失。因此，需要一种新的方法来改善ViT前端的特征融合，提高模型性能。

Method: 通过去除多尺度融合后的非线性平滑步骤中的标准线性3x3卷积，使用KAN增强的卷积层替代，以适应性学习并校正多尺度融合产生的‘伪像’。这一方法具体包括引入KAN-增强的FPN茎架构，该架构保留了FPN的经典“上采样和添加”融合机制。

Result: 在COCO数据集上，KAN-FPN茎架构在轻量级ViTPose-S基线下取得了显著的性能提升，AP增益高达2.0。

Conclusion: 论文不仅提供了一个即插即用且高性能的模块，更重要的是揭示了ViT前端性能瓶颈往往不在特征精炼（注意力）而在特征融合质量（融合），并且提出了一种通过引入KAN操作有效解决这一问题的路径。

Abstract: Vision Transformers (ViT) have demonstrated significant promise in dense prediction tasks such as pose estimation. However, their performance is frequently constrained by the overly simplistic front-end designs employed in models like ViTPose. This naive patchification mechanism struggles to effectively handle multi-scale variations and results in irreversible information loss during the initial feature extraction phase. To overcome this limitation, we introduce a novel KAN-enhanced FPN-Stem architecture. Through rigorous ablation studies, we first identified that the true bottleneck for performance improvement lies not in plug-and-play attention modules (e.g., CBAM), but in the post-fusion non-linear smoothing step within the FPN. Guided by this insight, our core innovation is to retain the classic "upsample-and-add" fusion stream of the FPN, but replace its terminal, standard linear 3x3 smoothing convolution with a powerful KAN-based convolutional layer. Leveraging its superior non-linear modeling capabilities, this KAN-based layer adaptively learns and rectifies the "artifacts" generated during the multi-scale fusion process. Extensive experiments on the COCO dataset demonstrate that our KAN-FPN-Stem achieves a significant performance boost of up to +2.0 AP over the lightweight ViTPose-S baseline. This work not only delivers a plug-and-play, high-performance module but, more importantly, reveals that: the performance bottleneck in ViT front-end often lies not in 'feature refinement' (Attention), but in the quality of 'feature fusion' (Fusion). Furthermore, it provides an effective path to address this bottleneck through the introduction of the KAN operator.

</details>


### [13] [Meta-information Guided Cross-domain Synergistic Diffusion Model for Low-dose PET Reconstruction](https://arxiv.org/abs/2512.22237)
*Mengxiao Geng,Ran Hong,Xiaoling Xu,Bingxuan Li,Qiegen Liu*

Main category: cs.CV

TL;DR: 该研究提出了一种指导元信息的跨域协同扩散模型（MiG-DM），通过综合跨模态先验知识和考虑患者特性、剂量相关信息等元信息，生成高质量的PET图像，实验结果表明其在提高PET图像质量和保留生理细节方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有低剂量PET成像方法忽视了投影领域物理知识和患者特定元信息的重要性，这限制了功能语义关联的挖掘。文献旨在通过引入能够综合跨模态先验知识并与图像重建进行语义对齐的模型，克服传统方法的不足。

Method: MiG-DM通过元信息编码模块将临床参数转换为语义提示，结合考虑患者特征、剂量相关信息和半定量参数，实现文本元信息与图像重建的跨模态对齐。模型通过在投影域和图像域的协同处理，利用特定的sinogram适配器捕捉全局物理结构。

Result: 实验结果证明，MiG-DM在公共UDPET数据集和不同剂量水平的临床数据集上优于现有方法，在提高PET图像质量和保留生理细节方面表现出色。

Conclusion: MiG-DM通过整合患者特定元信息和多模态先验知识生成高质量PET图像，为低剂量PET成像提供了新的解决方案。

Abstract: Low-dose PET imaging is crucial for reducing patient radiation exposure but faces challenges like noise interference, reduced contrast, and difficulty in preserving physiological details. Existing methods often neglect both projection-domain physics knowledge and patient-specific meta-information, which are critical for functional-semantic correlation mining. In this study, we introduce a meta-information guided cross-domain synergistic diffusion model (MiG-DM) that integrates comprehensive cross-modal priors to generate high-quality PET images. Specifically, a meta-information encoding module transforms clinical parameters into semantic prompts by considering patient characteristics, dose-related information, and semi-quantitative parameters, enabling cross-modal alignment between textual meta-information and image reconstruction. Additionally, the cross-domain architecture combines projection-domain and image-domain processing. In the projection domain, a specialized sinogram adapter captures global physical structures through convolution operations equivalent to global image-domain filtering. Experiments on the UDPET public dataset and clinical datasets with varying dose levels demonstrate that MiG-DM outperforms state-of-the-art methods in enhancing PET image quality and preserving physiological details.

</details>


### [14] [Evaluating an Adaptive Multispectral Turret System for Autonomous Tracking Across Variable Illumination Conditions](https://arxiv.org/abs/2512.22263)
*Aahan Sachdeva,Dhanvinkumar Ganeshkumar,James E. Gallagher,Tyler Treat,Edward J. Oughton*

Main category: cs.CV

TL;DR: 本文提出了一种自适应框架，结合了RGB和长波红外（LWIR）视频流，并根据光照条件动态选择最佳检测模型，显著提高了低光条件下的检测性能。


<details>
  <summary>Details</summary>
Motivation: 传统的RGB检测管道在低光照环境中表现不佳，而基于热的系统缺乏颜色和纹理信息。为了解决这些问题，本研究开发了一种自适应融合框架，能够在不同光照条件下结合RGB和LWIR视频流，并根据光照条件选择最优检测模型。

Method: 研究团队训练了33个YOLO模型，并通过将RGB和LWIR帧在10%增量下以11种融合比集成，来探索不同光线条件下的最佳融合策略。然后，根据这些模型在不同光照条件下的表现来选择最优模型。

Result: 在所有光照条件下，自适应融合框架都能显著提高检测性能。满光条件（80/20 RGB-LWIR）和低光条件（90/10融合）的模型分别达到了92.8%和92.0%的平均置信度，远高于YOLOv5 nano和YOLOv11 nano基线。在无光条件下，40/60融合达到了71.0%的置信度，也超过了基线标准，尽管没有统计学显著性。

Conclusion: 该研究提出的方法通过结合多种带宽的感知信息，有效地提高了自主机器人在不同光照条件下的视觉性能，从而增强了搜索和救援任务的效率和可靠性。

Abstract: Autonomous robotic platforms are playing a growing role across the emergency services sector, supporting missions such as search and rescue operations in disaster zones and reconnaissance. However, traditional red-green-blue (RGB) detection pipelines struggle in low-light environments, and thermal-based systems lack color and texture information. To overcome these limitations, we present an adaptive framework that fuses RGB and long-wave infrared (LWIR) video streams at multiple fusion ratios and dynamically selects the optimal detection model for each illumination condition. We trained 33 You Only Look Once (YOLO) models on over 22,000 annotated images spanning three light levels: no-light (<10 lux), dim-light (10-1000 lux), and full-light (>1000 lux). To integrate both modalities, fusion was performed by blending aligned RGB and LWIR frames at eleven ratios, from full RGB (100/0) to full LWIR (0/100) in 10% increments. Evaluation showed that the best full-light model (80/20 RGB-LWIR) and dim-light model (90/10 fusion) achieved 92.8% and 92.0% mean confidence; both significantly outperformed the YOLOv5 nano (YOLOv5n) and YOLOv11 nano (YOLOv11n) baselines. Under no-light conditions, the top 40/60 fusion reached 71.0%, exceeding baselines though not statistically significant. Adaptive RGB-LWIR fusion improved detection confidence and reliability across all illumination conditions, enhancing autonomous robotic vision performance.

</details>


### [15] [Human-Aligned Generative Perception: Bridging Psychophysics and Generative Models](https://arxiv.org/abs/2512.22272)
*Antara Titikhsha,Om Kulkarni,Dharun Muthaiah*

Main category: cs.CV

TL;DR: 研究开发了一种无需专门训练即可引入几何理解的方法，通过使用轻量级的现成鉴别器作为外部指导信号，将几何和风格分离并控制生成过程。实验表明，这种引导生成的方法相较于未引导的基本线相比，语义对齐度提高了约80%。


<details>
  <summary>Details</summary>
Motivation: 为了弥合人类感知与当前生成模型之间的语义差距，并提高文字到图像模型的几何控制能力。

Method: 提出了一个基于THINGS三元组数据集训练的Human Perception Embedding (HPE)教师模型，通过在潜扩散过程中注入这个教师模型的梯度，将几何和风格分离，实现了可控的生成过程。

Result: 实验表明，使用这种引导方法的生成模型在面对材料冲突时，仍能准确生成复杂三维形状，如伊姆斯椅子。与未引导的基本线相比，语义对齐度提高了约80%。此外，流动模型在缺乏持续指导时倾向于回归默认轨迹。

Conclusion: 研究表明小型教师模型可以可靠地引导大型生成系统，增强几何控制，并扩展文字到图像合成的创意思维范围。

Abstract: Text-to-image diffusion models generate highly detailed textures, yet they often rely on surface appearance and fail to follow strict geometric constraints, particularly when those constraints conflict with the style implied by the text prompt. This reflects a broader semantic gap between human perception and current generative models. We investigate whether geometric understanding can be introduced without specialized training by using lightweight, off-the-shelf discriminators as external guidance signals. We propose a Human Perception Embedding (HPE) teacher trained on the THINGS triplet dataset, which captures human sensitivity to object shape. By injecting gradients from this teacher into the latent diffusion process, we show that geometry and style can be separated in a controllable manner. We evaluate this approach across three architectures: Stable Diffusion v1.5 with a U-Net backbone, the flow-matching model SiT-XL/2, and the diffusion transformer PixArt-Σ. Our experiments reveal that flow models tend to drift back toward their default trajectories without continuous guidance, and we demonstrate zero-shot transfer of complex three-dimensional shapes, such as an Eames chair, onto conflicting materials such as pink metal. This guided generation improves semantic alignment by about 80 percent compared to unguided baselines. Overall, our results show that small teacher models can reliably guide large generative systems, enabling stronger geometric control and broadening the creative range of text-to-image synthesis.

</details>


### [16] [GeCo: A Differentiable Geometric Consistency Metric for Video Generation](https://arxiv.org/abs/2512.22274)
*Leslie Gu,Junhwa Hur,Charles Herrmann,Fangneng Zhan,Todd Zickler,Deqing Sun,Hanspeter Pfister*

Main category: cs.CV

TL;DR: GeCo 是一种基于几何的度量方法，用于同时检测静态场景中的几何变形和遮挡不一致性，通过融合残差运动和深度先验生成可解释的密集一致图，用于系统地评估视频生成模型，并作为无训练指导损失来减少变形 artefacts。


<details>
  <summary>Details</summary>
Motivation: 常见的视频生成模型在处理几何变形和遮挡不一致性时存在局限性，GeCo 方法通过引入基于几何的度量，旨在填补这一空白。

Method: GeCo 方法融合了残差运动和深度先验信息，生成可解释的密集一致性图，用于检测和评估视频生成模型中的几何变形和遮挡不一致性问题。

Result: 利用 GeCo 方法，研究人员发现了一些常见的视频生成模型在处理动态场景时的失败模式，并进一步将该方法作为无训练指导损失应用于视频生成，有效减少了变形 artefacts。

Conclusion: GeCo 在检测和指导减少视频生成中的几何变形和遮挡问题方面展示出了潜力，为进一步改进视频生成模型提供了新的方向。

Abstract: We introduce GeCo, a geometry-grounded metric for jointly detecting geometric deformation and occlusion-inconsistency artifacts in static scenes. By fusing residual motion and depth priors, GeCo produces interpretable, dense consistency maps that reveal these artifacts. We use GeCo to systematically benchmark recent video generation models, uncovering common failure modes, and further employ it as a training-free guidance loss to reduce deformation artifacts during video generation.

</details>


### [17] [The Illusion of Clinical Reasoning: A Benchmark Reveals the Pervasive Gap in Vision-Language Models for Clinical Competency](https://arxiv.org/abs/2512.22275)
*Dingyu Wang,Zimu Yuan,Jiajun Liu,Shanggui Liu,Nan Zhou,Tianxing Xu,Di Huang,Dong Jiang*

Main category: cs.CV

TL;DR: 本研究开发了B&J基准测试，包含1245个来自骨科和运动医学真实病例的问题，评估了模型在不同任务中的表现，发现最先进的模型在结构化问题上表现优异，但在开放型任务和结合多种模态的问题上表现较差，强调了现有人工智能模型尚不具备复杂的跨模态推理能力。


<details>
  <summary>Details</summary>
Motivation: 背景显示，现有的基准测试方法不能捕获临床情景下的综合、多模态推理能力，而实际医疗护理需要这种能力。因此，有必要开发一个更加全面的评估框架，以更真实地反映模型的临床推理能力。

Method: 研究团队开发了B&J基准测试，包含1245个从骨科和运动医学实际病例中提取的问题，覆盖7个与临床推理路径对应的任务，包括知识回忆、文本和图像解释、诊断生成、治疗计划制定和提供理由等。评估了11种视觉-语言模型和6种大型语言模型。

Result: 研究结果显示，在结构化问题上，最先进的模型达到了超过90%的准确性，但在开放性任务和多模态整合问题上的表现明显下降，准确性甚至未能达到60%。视觉-语言模型在医学图像解释方面显示出显著局限性，经常出现严重的文字驱动幻觉，常常忽视矛盾的视觉证据。值得注意的是，专门为医疗应用微调的模型在性能上与通用模型没有显著优势。

Conclusion: 当前的人工智能模型尚未达到临床所需的复杂多模态推理能力。它们的临床部署应严格限于辅助性、基于文本的角色，直至在核心临床任务上实现基本的突破性进展，特别是在多模态理解和视觉理解方面的提升。

Abstract: Background: The rapid integration of foundation models into clinical practice and public health necessitates a rigorous evaluation of their true clinical reasoning capabilities beyond narrow examination success. Current benchmarks, typically based on medical licensing exams or curated vignettes, fail to capture the integrated, multimodal reasoning essential for real-world patient care. Methods: We developed the Bones and Joints (B&J) Benchmark, a comprehensive evaluation framework comprising 1,245 questions derived from real-world patient cases in orthopedics and sports medicine. This benchmark assesses models across 7 tasks that mirror the clinical reasoning pathway, including knowledge recall, text and image interpretation, diagnosis generation, treatment planning, and rationale provision. We evaluated eleven vision-language models (VLMs) and six large language models (LLMs), comparing their performance against expert-derived ground truth. Results: Our results demonstrate a pronounced performance gap between task types. While state-of-the-art models achieved high accuracy, exceeding 90%, on structured multiple-choice questions, their performance markedly declined on open-ended tasks requiring multimodal integration, with accuracy scarcely reaching 60%. VLMs demonstrated substantial limitations in interpreting medical images and frequently exhibited severe text-driven hallucinations, often ignoring contradictory visual evidence. Notably, models specifically fine-tuned for medical applications showed no consistent advantage over general-purpose counterparts. Conclusions: Current artificial intelligence models are not yet clinically competent for complex, multimodal reasoning. Their safe deployment should currently be limited to supportive, text-based roles. Future advancement in core clinical tasks awaits fundamental breakthroughs in multimodal integration and visual understanding.

</details>


### [18] [A Three-Level Alignment Framework for Large-Scale 3D Retrieval and Controlled 4D Generation](https://arxiv.org/abs/2512.22294)
*Philip Xu,David Elizondo,Raouf Hamzaoui*

Main category: cs.CV

TL;DR: Uni4D 提出了一种统一框架，用于大规模开放词汇3D检索和基于三维结构多级对齐的可控4D生成，通过改进语义对齐提升了多模态准确性，并在实验中展示了高质的3D检索和可控4D生成能力。


<details>
  <summary>Details</summary>
Motivation: 当前的3D检索和4D生成方法大多独立进行，缺乏统一框架来实现精准的跨模态对齐。Uni4D旨在通过统一框架提供更高质量的3D检索和可控4D生成，推动动态多模态理解和实际应用。

Method: Uni4D采用了基于Align3D 130数据集的3D文本多头注意力和搜索模型，优化了语义对齐，并结合精确的文本到3D检索、多视图3D到图像对齐和图像到文本对齐三个组件，实现了跨模态对齐。

Result: 实验结果显示，Uni4D在大规模开放词汇3D检索和可控4D生成中表现出高质量的能力，提高了动态多模态理解和实际应用水平。

Conclusion: Uni4D提供了一种先进的方法，以实现更准确的跨模态对齐，解决了当前存在的问题，并展示了在3D检索和4D生成领域的应用潜力。

Abstract: We introduce Uni4D, a unified framework for large scale open vocabulary 3D retrieval and controlled 4D generation based on structured three level alignment across text, 3D models, and image modalities. Built upon the Align3D 130 dataset, Uni4D employs a 3D text multi head attention and search model to optimize text to 3D retrieval through improved semantic alignment. The framework further strengthens cross modal alignment through three components: precise text to 3D retrieval, multi view 3D to image alignment, and image to text alignment for generating temporally consistent 4D assets. Experimental results demonstrate that Uni4D achieves high quality 3D retrieval and controllable 4D generation, advancing dynamic multimodal understanding and practical applications.

</details>


### [19] [Real-Time In-Cabin Driver Behavior Recognition on Low-Cost Edge Hardware](https://arxiv.org/abs/2512.22298)
*Vesal Ahsani,Babak Hossein Khalaj*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In-cabin Driver Monitoring Systems (DMS) must recognize distraction- and drowsiness-related behaviors with low latency under strict constraints on compute, power, and cost. We present a single-camera in-cabin driver behavior recognition system designed for deployment on two low-cost edge platforms: Raspberry Pi 5 (CPU-only) and Google Coral Edge TPU. The proposed pipeline combines (i) a compact per-frame vision model, (ii) a confounder-aware label design to reduce visually similar false positives, and (iii) a temporal decision head that triggers alerts only when predictions are both confident and sustained. The system covers 17 behavior classes, including multiple phone-use modes, eating/drinking, smoking, reaching behind, gaze/attention shifts, passenger interaction, grooming, control-panel interaction, yawning, and eyes-closed sleep. Training and evaluation use licensed datasets spanning diverse drivers, vehicles, and lighting conditions (details in Section 6), and we further validate runtime behavior in real in-vehicle tests. The optimized deployments achieve about 16 FPS on Raspberry Pi 5 with INT8 inference (per-frame latency under 60 ms) and about 25 FPS on Coral Edge TPU, enabling real-time monitoring and stable alert generation on inexpensive hardware. Finally, we discuss how reliable in-cabin human-state perception can serve as an upstream input for human-centered vehicle intelligence, including emerging agentic vehicle concepts.

</details>


### [20] [VideoZoomer: Reinforcement-Learned Temporal Focusing for Long Video Reasoning](https://arxiv.org/abs/2512.22315)
*Yang Ding,Yizhen Zhang,Xin Lai,Ruihang Chu,Yujiu Yang*

Main category: cs.CV

TL;DR: 该研究提出了一种名为VideoZoomer的新框架，通过动态调整视觉焦点帮助多模态大型语言模型在长视频理解任务中取得更好的表现。


<details>
  <summary>Details</summary>
Motivation: 当前的多模态大语言模型在长视频理解中受限于有限的上下文窗口，依赖于均匀取帧或静态预选方法，可能会忽视关键信息且难以修正错误的选择。因此，研究希望开发一种能够自动调整视觉焦点的框架，以提高模型的理解能力。

Method: 研究采用了双阶段训练策略，首先是基于提取的代表样例和反思轨迹的受监督细调阶段，其次是使用强化学习进一步优化操作策略。VideoZoomer框架从粗略的低帧率概览开始，利用时间缩放工具在自主选择的时刻获取高帧率片段，逐步以多轮交互的方式收集细粒度证据。

Result: 实验结果显示，7B模型展示了多样且复杂的推理模式，能够在长视频理解和推理基准测试中取得出色表现，甚至在具有挑战性的任务上超过开源模型，并能以较少的帧预算实现更高的效率。

Conclusion: 研究证明，VideoZoomer框架能够有效提升多模态大语言模型的长视频理解能力，并且在性能和效率上具有明显优势。

Abstract: Multimodal Large Language Models (MLLMs) have achieved remarkable progress in vision-language tasks yet remain limited in long video understanding due to the limited context window. Consequently, prevailing approaches tend to rely on uniform frame sampling or static pre-selection, which might overlook critical evidence and unable to correct its initial selection error during its reasoning process. To overcome these limitations, we propose VideoZoomer, a novel agentic framework that enables MLLMs to dynamically control their visual focus during reasoning. Starting from a coarse low-frame-rate overview, VideoZoomer invokes a temporal zoom tool to obtain high-frame-rate clips at autonomously chosen moments, thereby progressively gathering fine-grained evidence in a multi-turn interactive manner. Accordingly, we adopt a two-stage training strategy: a cold-start supervised fine-tuning phase on a curated dataset of distilled exemplar and reflection trajectories, followed by reinforcement learning to further refine the agentic policy. Extensive experiments demonstrate that our 7B model delivers diverse and complex reasoning patterns, yielding strong performance across a broad set of long video understanding and reasoning benchmarks. These emergent capabilities allow it to consistently surpass existing open-source models and even rival proprietary systems on challenging tasks, while achieving superior efficiency under reduced frame budgets.

</details>


### [21] [SpotEdit: Selective Region Editing in Diffusion Transformers](https://arxiv.org/abs/2512.22323)
*Zhibin Qin,Zhenxiong Tan,Zeqing Wang,Songhua Liu,Xinchao Wang*

Main category: cs.CV

TL;DR: SpotEdit 是一种无需训练的扩散编辑框架，仅选择性地更新被修改的区域，通过 SpotSelector 跳过稳定区域的计算并使用 SpotFusion 机制动态融合条件图像特征，从而减少冗余计算并保持未修改区域的高保真度。


<details>
  <summary>Details</summary>
Motivation: 当前的扩散模型在所有时间步均匀处理和去噪所有令牌，这会导致不必要的计算并在未修改区域可能造成质量下降。SpotEdit 学术探索的核心问题是否真的需要在每次编辑过程中重新生成每一个区域。

Method: SpotEdit 由 SpotSelector 和 SpotFusion 两部分组成。SpotSelector 通过感知相似度识别稳定区域并利用这些区域的特征来跳过计算。SpotFusion 通过动态融合机制将这些特征与编辑的令牌结合，以保持上下文连贯性和编辑质量。

Result: 通过减少不必要的计算并保持未修改区域的高保真度，SpotEdit 实现了高效且精确的图像编辑。

Conclusion: 本文提出了一种创新的 SpotEdit 框架，可以在保证高保真度的同时提高图像编辑的效率，特别是在处理局部修改时表现出色。

Abstract: Diffusion Transformer models have significantly advanced image editing by encoding conditional images and integrating them into transformer layers. However, most edits involve modifying only small regions, while current methods uniformly process and denoise all tokens at every timestep, causing redundant computation and potentially degrading unchanged areas. This raises a fundamental question: Is it truly necessary to regenerate every region during editing? To address this, we propose SpotEdit, a training-free diffusion editing framework that selectively updates only the modified regions. SpotEdit comprises two key components: SpotSelector identifies stable regions via perceptual similarity and skips their computation by reusing conditional image features; SpotFusion adaptively blends these features with edited tokens through a dynamic fusion mechanism, preserving contextual coherence and editing quality. By reducing unnecessary computation and maintaining high fidelity in unmodified areas, SpotEdit achieves efficient and precise image editing.

</details>


### [22] [DeMoGen: Towards Decompositional Human Motion Generation with Energy-Based Diffusion Models](https://arxiv.org/abs/2512.22324)
*Jianrong Zhang,Hehe Fan,Yi Yang*

Main category: cs.CV

TL;DR: 该研究引入了一种名为DeMoGen的方法，它采用能量基于的扩散模型进行分解学习，目的是从复杂的运动序列中提取出可重用的运动基元。


<details>
  <summary>Details</summary>
Motivation: 目前的人类运动建模主要集中在前向建模上，即从文本到整体运动的学习，或从一组运动概念重组复杂运动。该研究提出了一个逆向视角，即如何将整体运动分解为具有语义意义的子组件。通过这种方式，研究希望能够发现和利用多个运动概念之间的组合分布。

Method: 该方法采用了能量基于的扩散模型，提出了三种训练变种，分别是显式分解文本提示训练的DeMoGen-Exp，进行正交自我监督分解的DeMoGen-OSS，以及强制原始文本嵌入与分解文本嵌入之间语义一致性的DeMoGen-SC。通过这种方法，可以直接从整体运动中发现并提取出多个运动概念。

Result: 该方法能够有效地分解复杂的运动序列，从中提取出可重用的运动基元，并可以灵活重组这些概念生成新的和多样化的运动。此外，研究还构建了一个文本分解的数据集，以支持这种分解训练。

Conclusion: 该研究为从整体运动中提取运动基元提供了一种新的方法，论证了这种分解学习的有效性和实用性。

Abstract: Human motions are compositional: complex behaviors can be described as combinations of simpler primitives. However, existing approaches primarily focus on forward modeling, e.g., learning holistic mappings from text to motion or composing a complex motion from a set of motion concepts. In this paper, we consider the inverse perspective: decomposing a holistic motion into semantically meaningful sub-components. We propose DeMoGen, a compositional training paradigm for decompositional learning that employs an energy-based diffusion model. This energy formulation directly captures the composed distribution of multiple motion concepts, enabling the model to discover them without relying on ground-truth motions for individual concepts. Within this paradigm, we introduce three training variants to encourage a decompositional understanding of motion: 1. DeMoGen-Exp explicitly trains on decomposed text prompts; 2. DeMoGen-OSS performs orthogonal self-supervised decomposition; 3. DeMoGen-SC enforces semantic consistency between original and decomposed text embeddings. These variants enable our approach to disentangle reusable motion primitives from complex motion sequences. We also demonstrate that the decomposed motion concepts can be flexibly recombined to generate diverse and novel motions, generalizing beyond the training distribution. Additionally, we construct a text-decomposed dataset to support compositional training, serving as an extended resource to facilitate text-to-motion generation and motion composition.

</details>


### [23] [The Multi-View Paradigm Shift in MRI Radiomics: Predicting MGMT Methylation in Glioblastoma](https://arxiv.org/abs/2512.22331)
*Mariya Miteva,Maria Nisheva-Pavlova*

Main category: cs.CV

TL;DR: 文章介绍了一种基于变分自编码器的多视图潜在表示学习框架，用于从MRI中集成T1Gd和FLAIR模态的互补放射组学特征，以预测MGMT启动子甲基化状态。


<details>
  <summary>Details</summary>
Motivation: 在胶质母细胞瘤（GBM）中，MGMT启动子甲基化具有重要的预后和治疗意义。由于传统单一模态和早期融合方法存在着特征冗余和对模态特定信息建模不完整的局限性，因此亟需一种能够保留模态特定结构和实现有效多模态集成的方法。

Method: 本文提出了一种基于变分自编码器的多视图潜在表示学习框架。通过独立的概率编码器编码每种模态，并在紧凑的潜在空间中进行融合，从而实现了模态特定结构的保留和有效的多模态集成，最后利用潜在嵌入进行MGMT启动子甲基化状态分类。

Result: 该模型通过多模态集成，提高了MGMT启动子甲基化状态分类的准确性。

Conclusion: 研究结果表明，基于变分自编码器的多视图潜在表示学习框架在胶质母细胞瘤MGMT启动子甲基化状态分类任务中具有潜在应用价值。

Abstract: Non-invasive inference of molecular tumor characteristics from medical imaging is a central goal of radiogenomics, particularly in glioblastoma (GBM), where O6-methylguanine-DNA methyltransferase (MGMT) promoter methylation carries important prognostic and therapeutic significance. Although radiomics-based machine learning methods have shown promise for this task, conventional unimodal and early-fusion approaches are often limited by high feature redundancy and an incomplete modeling of modality-specific information. In this work, we introduce a multi-view latent representation learning framework based on variational autoencoders (VAE) to integrate complementary radiomic features derived from post-contrast T1-weighted (T1Gd) and Fluid-Attenuated Inversion Recovery (FLAIR) magnetic resonance imaging (MRI). By encoding each modality through an independent probabilistic encoder and performing fusion in a compact latent space, the proposed approach preserves modality-specific structure while enabling effective multimodal integration. The resulting latent embeddings are subsequently used for MGMT promoter methylation classification.

</details>


### [24] [Feature Learning with Multi-Stage Vision Transformers on Inter-Modality HER2 Status Scoring and Tumor Classification on Whole Slides](https://arxiv.org/abs/2512.22335)
*Olaide N. Oyelade,Oliver Hoxey,Yulia Humrye*

Main category: cs.CV

TL;DR: 本研究提出了一种基于视觉变换器的端到端管道，用于在HE和IHC染色全组织切片图像中自动预测和评分HER2状态。该方法准确地识别肿瘤区域，并在4级评分法中实现了94%的分类精度和93.3%的特异性，与人类病理科医生的结果相当。


<details>
  <summary>Details</summary>
Motivation: 现有的多种深度学习方法预测HER2表达时，并未提供像素级别的HER2状态定位，阻碍了肿瘤治疗的进一步实施。因此，该研究旨在开发一个能够自动预测和评分HER2状态的端到端系统，以支持更为精确的治疗方法选择。

Method: 该方法包括通过对HE全组织切片图像（WSI）进行像素级别的处理来定位肿瘤区域。提出了一种新的映射函数，将相关的IHC WSI区域对应到HE WSI的恶性区域。嵌入了一种临床启发的HER2评分机制，实现了4级评分（0, 1+, 2+, 3+）的自动像素级别注释。采用了包含13个病例的私有数据集进行实验。

Result: 实验结果表明，在肿瘤定位中具有良好的分类准确性。HER2状态预测中，4级评分法的分类精度为0.94，敏感性为93.3%，与人类病理科医生的结果相当。

Conclusion: 基于视觉变换器的管道能够有效地识别HER2表达，并在4级评分法中取得了高精度和特异性，显示出其在实际临床应用中的潜力。

Abstract: The popular use of histopathology images, such as hematoxylin and eosin (H&E), has proven to be useful in detecting tumors. However, moving such cancer cases forward for treatment requires accurate on the amount of the human epidermal growth factor receptor 2 (HER2) protein expression. Predicting both the lower and higher levels of HER2 can be challenging. Moreover, jointly analyzing H&E and immunohistochemistry (IHC) stained images for HER2 scoring is difficult. Although several deep learning methods have been investigated to address the challenge of HER2 scoring, they suffer from providing a pixel-level localization of HER2 status. In this study, we propose a single end-to-end pipeline using a system of vision transformers with HER2 status scoring on whole slide images of WSIs. The method includes patch-wise processing of H&E WSIs for tumor localization. A novel mapping function is proposed to correspondingly identify correlated IHC WSIs regions with malignant regions on H&E. A clinically inspired HER2 scoring mechanism is embedded in the pipeline and allows for automatic pixel-level annotation of 4-way HER2 scoring (0, 1+, 2+, and 3+). Also, the proposed method accurately returns HER2-negative and HER2-positive. Privately curated datasets were collaboratively extracted from 13 different cases of WSIs of H&E and IHC. A thorough experiment is conducted on the proposed method. Results obtained showed a good classification accuracy during tumor localization. Also, a classification accuracy of 0.94 and a specificity of 0.933 were returned for the prediction of HER2 status, scoring in the 4-way methods. The applicability of the proposed pipeline was investigated using WSIs patches as comparable to human pathologists. Findings from the study showed the usability of jointly evaluated H&E and IHC images on end-to-end ViTs-based models for HER2 scoring

</details>


### [25] [VULCAN: Tool-Augmented Multi Agents for Iterative 3D Object Arrangement](https://arxiv.org/abs/2512.22351)
*Zhengfei Kuang,Rui Lin,Long Zhao,Gordon Wetzstein,Saining Xie,Sanghyun Woo*

Main category: cs.CV

TL;DR: 本文通过引入MCP基API、增强3D场景理解以及提出多智能体协作框架，解决了MLLMs在3D对象排列任务中的关键挑战，显著提升了多步骤指令的执行效果。


<details>
  <summary>Details</summary>
Motivation: 鉴于Multimodal Large Language Models (MLLMs)在2D视觉语言任务中的显著进步，但其在3D场景操作中的应用仍待探索。本文旨在填补这一空白。

Method: 作者引入了MCP基API以改善MLLMs的视觉关联能力，开发了一套专用的视觉工具来增强对3D场景的理解，并提出了多智能体协作框架来管理多步指令并能从中间错误中恢复。

Result: 本文方法在25个复杂的3D对象排列任务上表现出色，显著优于现有基线。

Conclusion: 本文提出的方法有力地推动了MLLMs在3D场景操作任务中的应用，展示了他们在精确3D感知和精准操作方面的潜力。

Abstract: Despite the remarkable progress of Multimodal Large Language Models (MLLMs) in 2D vision-language tasks, their application to complex 3D scene manipulation remains underexplored. In this paper, we bridge this critical gap by tackling three key challenges in 3D object arrangement task using MLLMs. First, to address the weak visual grounding of MLLMs, which struggle to link programmatic edits with precise 3D outcomes, we introduce an MCP-based API. This shifts the interaction from brittle raw code manipulation to more robust, function-level updates. Second, we augment the MLLM's 3D scene understanding with a suite of specialized visual tools to analyze scene state, gather spatial information, and validate action outcomes. This perceptual feedback loop is critical for closing the gap between language-based updates and precise 3D-aware manipulation. Third, to manage the iterative, error-prone updates, we propose a collaborative multi-agent framework with designated roles for planning, execution, and verification. This decomposition allows the system to robustly handle multi-step instructions and recover from intermediate errors. We demonstrate the effectiveness of our approach on a diverse set of 25 complex object arrangement tasks, where it significantly outperforms existing baselines. Website: vulcan-3d.github.io

</details>


### [26] [Self-Evaluation Unlocks Any-Step Text-to-Image Generation](https://arxiv.org/abs/2512.22374)
*Xin Yu,Xiaojuan Qi,Zhengqi Li,Kai Zhang,Richard Zhang,Zhe Lin,Eli Shechtman,Tianyu Wang,Yotam Nitzan*

Main category: cs.CV

TL;DR: Self-E是一种新型的从零开始训练的文本到图像生成模型，能够支持任意步骤的推理。它通过结合即时局部学习和自我驱动的整体匹配，实现了高质量文本到图像的生成，性能在少量步骤和大量步骤之间表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像生成模型通常需要预训练教师或依赖局部监督，导致在较少的推理步骤中表现不佳。Self-E通过引入自我评估机制，能够在不依赖预训练模型和不只依赖局部监督的情况下，实现高效且可扩展的生成。

Method: Self-E采用了一种新颖的自我评估机制，通过当前生成样本的得分估计来进行自我评价。这种机制使模型能够作为一个动态自我教师，即时学习，并与一种类似于流匹配的方法结合，实现从零开始的高质量文本到图像生成。

Result: 实验结果表明，Self-E在少量步骤生成方面表现优异，且在50步时与最先进的流匹配模型相当。此外，其性能随着推理步骤的增加而单调提高，能够在单个统一模型中提供超快速少量步骤生成和高质量长轨迹采样。

Conclusion: Self-E为一种全新的，任意步骤的文本到图像模型，为高效和可扩展的生成提供了统一框架。

Abstract: We introduce the Self-Evaluating Model (Self-E), a novel, from-scratch training approach for text-to-image generation that supports any-step inference. Self-E learns from data similarly to a Flow Matching model, while simultaneously employing a novel self-evaluation mechanism: it evaluates its own generated samples using its current score estimates, effectively serving as a dynamic self-teacher. Unlike traditional diffusion or flow models, it does not rely solely on local supervision, which typically necessitates many inference steps. Unlike distillation-based approaches, it does not require a pretrained teacher. This combination of instantaneous local learning and self-driven global matching bridges the gap between the two paradigms, enabling the training of a high-quality text-to-image model from scratch that excels even at very low step counts. Extensive experiments on large-scale text-to-image benchmarks show that Self-E not only excels in few-step generation, but is also competitive with state-of-the-art Flow Matching models at 50 steps. We further find that its performance improves monotonically as inference steps increase, enabling both ultra-fast few-step generation and high-quality long-trajectory sampling within a single unified model. To our knowledge, Self-E is the first from-scratch, any-step text-to-image model, offering a unified framework for efficient and scalable generation.

</details>


### [27] [iOSPointMapper: RealTime Pedestrian and Accessibility Mapping with Mobile AI](https://arxiv.org/abs/2512.22392)
*Himanshu Naidu,Yuxiang Zhang,Sachin Mehta,Anat Caspi*

Main category: cs.CV

TL;DR: iOSPointMapper 是一种利用 iPhone 和 iPad 进行实时、隐私友好型人行道测绘的移动应用，通过集成语义分割、LiDAR 深度估计和 GPS/IMU 综合定位，为交通数据交换倡议提供准确且可扩展的人行道数据。


<details>
  <summary>Details</summary>
Motivation: 现有的人行道数据采集方法成本高、碎片化且难以规模化，因此开发 iOSPointMapper 旨在为城市建设无障碍、包容性更强的人行道基础设施提供准确且易于获取的数据。

Method: iOSPointMapper 利用 iPhone 和 iPad 的在设备端语义分割、LiDAR 深度估算及 GPS/IMU 综合定位技术，通过用户指导注释界面验证系统输出，从而实现人行道关键特征的检测和定位。

Result: 经详细评估，该系统的功能发现能力和空间地理绘图性能表明，iOSPointMapper 具有增强行人测绘的潜力，为解决行人道路数据缺口提供了一种可扩展且用户中心的方法。

Conclusion: 本文展示了 iOSPointMapper 作为一种新型的人行道数据收集和共享解决方案，其能够支持交通数据交换倡议，助力交通决策过程，并为城市规划提供支持。

Abstract: Accurate, up-to-date sidewalk data is essential for building accessible and inclusive pedestrian infrastructure, yet current approaches to data collection are often costly, fragmented, and difficult to scale. We introduce iOSPointMapper, a mobile application that enables real-time, privacy-conscious sidewalk mapping on the ground, using recent-generation iPhones and iPads. The system leverages on-device semantic segmentation, LiDAR-based depth estimation, and fused GPS/IMU data to detect and localize sidewalk-relevant features such as traffic signs, traffic lights and poles. To ensure transparency and improve data quality, iOSPointMapper incorporates a user-guided annotation interface for validating system outputs before submission. Collected data is anonymized and transmitted to the Transportation Data Exchange Initiative (TDEI), where it integrates seamlessly with broader multimodal transportation datasets. Detailed evaluations of the system's feature detection and spatial mapping performance reveal the application's potential for enhanced pedestrian mapping. Together, these capabilities offer a scalable and user-centered approach to closing critical data gaps in pedestrian

</details>


### [28] [DeFloMat: Detection with Flow Matching for Stable and Efficient Generative Object Localization](https://arxiv.org/abs/2512.22406)
*Hansang Lee,Chaelin Lee,Nieun Seo,Joon Seok Lim,Helen Hong*

Main category: cs.CV

TL;DR: DeFloMat是一个新颖的生成式目标检测框架，通过引入条件流动匹配，将扩散模型的多步 stochastic 模型转换为一个快速的确定性流程场，适用于需要高时间敏感性的临床应用。


<details>
  <summary>Details</summary>
Motivation: 目前的扩散模型在临床应用中速度较慢，而DeFloMat通过使用确定性的流动场解决了这一问题，能够在保证精度的同时提高检测速度。

Method: DeFloMat通过引入条件最优传输理论中的 Rectified Flow 来替代扩散模型中的 stochastic 过程，实现通过简单的 ODE 求解器进行快速推理。

Result: 在 MRE 临床数据集上的实验表明，DeFloMat 在仅 3 步推理中达到了 43.32% 的 AP_{10:50} 精度，比 DiffusionDet 的 31.03% 提高了四分之一的性能，同时显著提高了定位特性并提高了召回率和稳定性。

Conclusion: DeFloMat 为稳定和快速的对象定位设定了新的标准，成功地在生成精度和临床效率之间找到了平衡。

Abstract: We propose DeFloMat (Detection with Flow Matching), a novel generative object detection framework that addresses the critical latency bottleneck of diffusion-based detectors, such as DiffusionDet, by integrating Conditional Flow Matching (CFM). Diffusion models achieve high accuracy by formulating detection as a multi-step stochastic denoising process, but their reliance on numerous sampling steps ($T \gg 60$) makes them impractical for time-sensitive clinical applications like Crohn's Disease detection in Magnetic Resonance Enterography (MRE). DeFloMat replaces this slow stochastic path with a highly direct, deterministic flow field derived from Conditional Optimal Transport (OT) theory, specifically approximating the Rectified Flow. This shift enables fast inference via a simple Ordinary Differential Equation (ODE) solver. We demonstrate the superiority of DeFloMat on a challenging MRE clinical dataset. Crucially, DeFloMat achieves state-of-the-art accuracy ($43.32\% \text{ } AP_{10:50}$) in only $3$ inference steps, which represents a $1.4\times$ performance improvement over DiffusionDet's maximum converged performance ($31.03\% \text{ } AP_{10:50}$ at $4$ steps). Furthermore, our deterministic flow significantly enhances localization characteristics, yielding superior Recall and stability in the few-step regime. DeFloMat resolves the trade-off between generative accuracy and clinical efficiency, setting a new standard for stable and rapid object localization.

</details>


### [29] [Bright 4B: Scaling Hyperspherical Learning for Segmentation in 3D Brightfield Microscopy](https://arxiv.org/abs/2512.22423)
*Amil Khan,Matheus Palhares Viana,Suraj Mishra,B. S. Manjunath*

Main category: cs.CV

TL;DR: 本文提出了一种名为Bright-4B的40亿参数基础模型，该模型可以在无需荧光标记或额外处理的情况下，直接从3D明场成像中分割亚细胞结构。该模型在多个共聚焦数据集上表现出对深度和细胞类型的细微结构细节的保留，优于当前的CNN和Transformer基线。


<details>
  <summary>Details</summary>
Motivation: 在传统的3D明场显微镜下，实现精确的亚细胞结构分割通常依赖于荧光标记或复杂的后处理步骤。为解决这一问题，本文提出了Bright-4B模型，旨在无需荧光或其他辅助通道的情况下，直接从3D明场数据中自动进行精确分割。

Method: Bright-4B模型融合了硬件对齐的本征稀疏注意力机制、深度宽度残差超连接，以及软Mixture-of-Experts自适应容量机制。进一步采用了可插拔的各向异性补丁嵌入，以尊重共聚焦点扩展和轴向变薄，确保3D标记的几何保真度。

Result: 实验结果显示，Bright-4B模型在多个共聚焦数据集上表现出了对深度和细胞类型中的细微结构细节的保真度。此外，该模型在多个方面优于当前的CNN和Transformer基线，包括精确度和效率。

Conclusion: 该研究通过提供一种从3D明场图像中自动进行亚细胞结构分割的解决方案，展示了复杂模型在生物医学成像中的应用潜力，并且将代码、预训练权重和模型发布，以便于更大规模的无标签3D细胞映射。

Abstract: Label-free 3D brightfield microscopy offers a fast and noninvasive way to visualize cellular morphology, yet robust volumetric segmentation still typically depends on fluorescence or heavy post-processing. We address this gap by introducing Bright-4B, a 4 billion parameter foundation model that learns on the unit hypersphere to segment subcellular structures directly from 3D brightfield volumes. Bright-4B combines a hardware-aligned Native Sparse Attention mechanism (capturing local, coarse, and selected global context), depth-width residual HyperConnections that stabilize representation flow, and a soft Mixture-of-Experts for adaptive capacity. A plug-and-play anisotropic patch embed further respects confocal point-spread and axial thinning, enabling geometry-faithful 3D tokenization. The resulting model produces morphology-accurate segmentations of nuclei, mitochondria, and other organelles from brightfield stacks alone--without fluorescence, auxiliary channels, or handcrafted post-processing. Across multiple confocal datasets, Bright-4B preserves fine structural detail across depth and cell types, outperforming contemporary CNN and Transformer baselines. All code, pretrained weights, and models for downstream finetuning will be released to advance large-scale, label-free 3D cell mapping.

</details>


### [30] [EmoCtrl: Controllable Emotional Image Content Generation](https://arxiv.org/abs/2512.22437)
*Jingyuan Yang,Weibin Luo,Hui Huang*

Main category: cs.CV

TL;DR: 本文提出了Controllable Emotional Image Content Generation (C-EICG)，结合内容描述生成具有目标情绪的图像，兼顾内容一致性与情感表达，通过集成文本和视觉情感增强模块，实验表明该方法在多个方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像模型注重内容一致而缺乏情感意识，而情感驱动模型则会导致内容失真。本文旨在弥合并解决这两种方法之间的差距。

Method: 提出了EmoCtrl方法，基于标注了内容、情感和感受性提示的数据集，将抽象情感与视觉线索联系起来。EmoCtrl包括文本和视觉情感增强模块，通过描述性语义和感知线索丰富情感表达。

Result: 定量和定性实验表明，EmoCtrl在多个方面超越了现有方法，实现了内容与情感表达的忠实度，用户研究也证明了其与人类偏好的高度一致性。此外，该方法还适用于创意应用，展示了学习到的情感令牌的稳健性和适应性。

Conclusion: EmoCtrl方法有效地实现了一个具有目标情绪的图像的生成，同时保持了内容的准确性，为后续研究提供了一个新的方向。

Abstract: An image conveys meaning through both its visual content and emotional tone, jointly shaping human perception. We introduce Controllable Emotional Image Content Generation (C-EICG), which aims to generate images that remain faithful to a given content description while expressing a target emotion. Existing text-to-image models ensure content consistency but lack emotional awareness, whereas emotion-driven models generate affective results at the cost of content distortion. To address this gap, we propose EmoCtrl, supported by a dataset annotated with content, emotion, and affective prompts, bridging abstract emotions to visual cues. EmoCtrl incorporates textual and visual emotion enhancement modules that enrich affective expression via descriptive semantics and perceptual cues. The learned emotion tokens exhibit complementary effects, as demonstrated through ablations and visualizations. Quantatitive and qualatitive experiments demonstrate that EmoCtrl achieves faithful content and expressive emotion control, outperforming existing methods across multiple aspects. User studies confirm EmoCtrl's strong alignment with human preference. Moreover, EmoCtrl generalizes well to creative applications, further demonstrating the robustness and adaptability of the learned emotion tokens.

</details>


### [31] [LECalib: Line-Based Event Camera Calibration](https://arxiv.org/abs/2512.22441)
*Zibin Liu,Banglei Guana,Yang Shanga,Zhenbao Yu,Yifei Bian,Qifeng Yu*

Main category: cs.CV

TL;DR: 提出了一种基于几何线的事件相机标定框架，能够从事件流中直接检测几何线并利用事件线标定模型生成初始镜头参数猜测值，适用于平面和非平面线，进一步通过非线性优化进行参数优化。


<details>
  <summary>Details</summary>
Motivation: 现有事件相机标定方法通常需要使用闪光图案、重建强度图像并利用事件提取的特征。然而，这些方法通常耗时且需要手动放置校准对象，无法适应快速变化的场景。本文提出了一种新颖的方法，利用常见人工环境中的几何线进行标定，解决了现有方法的上述问题。

Method: 该方法通过直接从事件流中检测几何线，并利用事件线校准模型生成初始镜头参数猜测值，适用于平面和非平面线。随后采用非线性优化进一步精简镜头参数。

Result: 通过仿真和真实世界的实验验证了该方法的有效性和准确性。该方法已在单目和立体事件相机上进行了验证。

Conclusion: 这种基于几何线的事件相机标定方法能够高效、准确地完成标定任务，适用于快速变化的场景。

Abstract: Camera calibration is an essential prerequisite for event-based vision applications. Current event camera calibration methods typically involve using flashing patterns, reconstructing intensity images, and utilizing the features extracted from events. Existing methods are generally time-consuming and require manually placed calibration objects, which cannot meet the needs of rapidly changing scenarios. In this paper, we propose a line-based event camera calibration framework exploiting the geometric lines of commonly-encountered objects in man-made environments, e.g., doors, windows, boxes, etc. Different from previous methods, our method detects lines directly from event streams and leverages an event-line calibration model to generate the initial guess of camera parameters, which is suitable for both planar and non-planar lines. Then, a non-linear optimization is adopted to refine camera parameters. Both simulation and real-world experiments have demonstrated the feasibility and accuracy of our method, with validation performed on monocular and stereo event cameras. The source code is released at https://github.com/Zibin6/line_based_event_camera_calib.

</details>


### [32] [Towards Robust Optical-SAR Object Detection under Missing Modalities: A Dynamic Quality-Aware Fusion Framework](https://arxiv.org/abs/2512.22447)
*Zhicheng Zhao,Yuancheng Xu,Andong Lu,Chenglong Li,Jin Tang*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Optical and Synthetic Aperture Radar (SAR) fusion-based object detection has attracted significant research interest in remote sensing, as these modalities provide complementary information for all-weather monitoring. However, practical deployment is severely limited by inherent challenges. Due to distinct imaging mechanisms, temporal asynchrony, and registration difficulties, obtaining well-aligned optical-SAR image pairs remains extremely difficult, frequently resulting in missing or degraded modality data. Although recent approaches have attempted to address this issue, they still suffer from limited robustness to random missing modalities and lack effective mechanisms to ensure consistent performance improvement in fusion-based detection. To address these limitations, we propose a novel Quality-Aware Dynamic Fusion Network (QDFNet) for robust optical-SAR object detection. Our proposed method leverages learnable reference tokens to dynamically assess feature reliability and guide adaptive fusion in the presence of missing modalities. In particular, we design a Dynamic Modality Quality Assessment (DMQA) module that employs learnable reference tokens to iteratively refine feature reliability assessment, enabling precise identification of degraded regions and providing quality guidance for subsequent fusion. Moreover, we develop an Orthogonal Constraint Normalization Fusion (OCNF) module that employs orthogonal constraints to preserve modality independence while dynamically adjusting fusion weights based on reliability scores, effectively suppressing unreliable feature propagation. Extensive experiments on the SpaceNet6-OTD and OGSOD-2.0 datasets demonstrate the superiority and effectiveness of QDFNet compared to state-of-the-art methods, particularly under partial modality corruption or missing data scenarios.

</details>


### [33] [SonoVision: A Computer Vision Approach for Helping Visually Challenged Individuals Locate Objects with the Help of Sound Cues](https://arxiv.org/abs/2512.22449)
*Md Abu Obaida Zishan,Annajiat Alim Rasel*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Locating objects for the visually impaired is a significant challenge and is something no one can get used to over time. However, this hinders their independence and could push them towards risky and dangerous scenarios. Hence, in the spirit of making the visually challenged more self-sufficient, we present SonoVision, a smart-phone application that helps them find everyday objects using sound cues through earphones/headphones. This simply means, if an object is on the right or left side of a user, the app makes a sinusoidal sound in a user's respective ear through ear/headphones. However, to indicate objects located directly in front, both the left and right earphones are rung simultaneously. These sound cues could easily help a visually impaired individual locate objects with the help of their smartphones and reduce the reliance on people in their surroundings, consequently making them more independent. This application is made with the flutter development platform and uses the Efficientdet-D2 model for object detection in the backend. We believe the app will significantly assist the visually impaired in a safe and user-friendly manner with its capacity to work completely offline. Our application can be accessed here https://github.com/MohammedZ666/SonoVision.git.

</details>


### [34] [SAM 3D for 3D Object Reconstruction from Remote Sensing Images](https://arxiv.org/abs/2512.22452)
*Junsheng Yao,Lichao Mou,Qingyu Li*

Main category: cs.CV

TL;DR: 本研究介绍了首个用于遥感单目建筑重建的通用图像到3D模型SAM 3D，实验表明其在建筑屋顶几何结构的连贯性和边界清晰度方面优于现有方法，且提出了城市场景重建的方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法往往需要特定的任务架构和剧烈的监督，本文通过引入通用的SAM 3D模型进行系统评测，旨在为城市三维重建提供有效的基础模型，并解决现有模型的不足。

Method: 本研究使用了Frechet Inception Distance (FID) 和 CLIP-based Maximum Mean Discrepancy (CMMD) 作为评估指标，对SAM 3D和TRELLIS模型进行了对比实验。

Result: 实验结果表明SAM 3D在屋顶几何结构连贯性和边界清晰度方面优于TRELLIS，进一步证明了它在城市场景重建中的潜力。

Conclusion: 研究还分析了实际应用中的限制，并讨论了未来的研究方向，为在城市3D重建中部署模型提供了实用的指导，同时也促进了场景级结构先验的整合。

Abstract: Monocular 3D building reconstruction from remote sensing imagery is essential for scalable urban modeling, yet existing methods often require task-specific architectures and intensive supervision. This paper presents the first systematic evaluation of SAM 3D, a general-purpose image-to-3D foundation model, for monocular remote sensing building reconstruction. We benchmark SAM 3D against TRELLIS on samples from the NYC Urban Dataset, employing Frechet Inception Distance (FID) and CLIP-based Maximum Mean Discrepancy (CMMD) as evaluation metrics. Experimental results demonstrate that SAM 3D produces more coherent roof geometry and sharper boundaries compared to TRELLIS. We further extend SAM 3D to urban scene reconstruction through a segment-reconstruct-compose pipeline, demonstrating its potential for urban scene modeling. We also analyze practical limitations and discuss future research directions. These findings provide practical guidance for deploying foundation models in urban 3D reconstruction and motivate future integration of scene-level structural priors.

</details>


### [35] [Pose-Guided Residual Refinement for Interpretable Text-to-Motion Generation and Editing](https://arxiv.org/abs/2512.22464)
*Sukhyun Jeong,Yong-Hoon Choi*

Main category: cs.CV

TL;DR: PGR$^2$M提出了一种混合表示方法，通过残差向量量化（RVQ）令牌化器来增强可理解的姿势代码，改善了细粒度时序变化的捕捉能力。该方法在HumanML3D和KIT-ML上的实验表明，其在生成和编辑方面的提高超过了CoMo以及最近的扩散和令牌化基线。


<details>
  <summary>Details</summary>
Motivation: 现有的基于姿势码的方法在捕捉细微的时序动态和高频细节方面存在局限，这限制了其重构精度和局部可控性。

Method: PGR$^2$M采用了一种混合表示方法，引入了残差码并通过残差向量量化（RVQ）令牌化器来增强姿势码。该令牌化器可以分解运动为粗略的整体结构编码的姿势latent和模型细微时间变化的残差latent。此外，使用残差丢弃机制来防止过度依赖残差，维持姿势码的语义对齐和可编辑性。之后，基于此令牌化器，使用基础Transformer和精炼Transformer分别从文本自回归预测姿势码和在文本、姿势码和量化阶段条件下预测残差码。

Result: PGR$^2$M在HumanML3D和KIT-ML的实验中显示出与CoMo以及最近的扩散和令牌化基线相比，生成和编辑方面表现更佳，提高了Fréchet引申距离和重构指标。

Conclusion: PGR$^2$M方法能够进行直观且能保持结构的运动编辑，并具备在生成和编辑场景中提供更好的重构精度和重构指标。

Abstract: Text-based 3D motion generation aims to automatically synthesize diverse motions from natural-language descriptions to extend user creativity, whereas motion editing modifies an existing motion sequence in response to text while preserving its overall structure. Pose-code-based frameworks such as CoMo map quantifiable pose attributes into discrete pose codes that support interpretable motion control, but their frame-wise representation struggles to capture subtle temporal dynamics and high-frequency details, often degrading reconstruction fidelity and local controllability. To address this limitation, we introduce pose-guided residual refinement for motion (PGR$^2$M), a hybrid representation that augments interpretable pose codes with residual codes learned via residual vector quantization (RVQ). A pose-guided RVQ tokenizer decomposes motion into pose latents that encode coarse global structure and residual latents that model fine-grained temporal variations. Residual dropout further discourages over-reliance on residuals, preserving the semantic alignment and editability of the pose codes. On top of this tokenizer, a base Transformer autoregressively predicts pose codes from text, and a refine Transformer predicts residual codes conditioned on text, pose codes, and quantization stage. Experiments on HumanML3D and KIT-ML show that PGR$^2$M improves Fréchet inception distance and reconstruction metrics for both generation and editing compared with CoMo and recent diffusion- and tokenization-based baselines, while user studies confirm that it enables intuitive, structure-preserving motion edits.

</details>


### [36] [Event-based high temporal resolution measurement of shock wave motion field](https://arxiv.org/abs/2512.22474)
*Taihang Lei,Banglei Guan,Minzu Liang,Pengju Sun,Jing Tao,Yang Shang,Qifeng Yu*

Main category: cs.CV

TL;DR: 本研究提出了一种利用多事件摄像头的新框架，用于高时空分辨率测量冲击波的运动参数。通过极坐标系统和事件偏移计算，提取特征并处理跨帧信息，实现多角度冲击波的测量和运动场重构，精度达到5.20%-0.06%。


<details>
  <summary>Details</summary>
Motivation: 高精度测量冲击波的参数对应用领域极为重要，但传统的测量方法难以应对冲击波快速不均匀传播和不稳定测试环境的挑战。因此，本研究旨在提出一种新型框架来解决这些问题。

Method: 1. 构建极坐标系统进行事件编码以揭示冲击波传播模式。2. 通过事件偏移计算，进行自适应区域提取。3. 迭代斜率分析提取冲击波前沿事件。4. 根据基于事件的光学成像模型推导事件几何模型和冲击波运动参数，同时建立3D重构模型。

Result: 速度测量结果与压力传感器和经验公式对比，误差最大为5.20%，最小为0.06%，展示了高时空分辨率下的高精度冲击波运动场测量。

Conclusion: 该方法实现了多角度冲击波测量、运动场重构及爆炸当量逆向工程，显著增强了精确度。

Abstract: Accurate measurement of shock wave motion parameters with high spatiotemporal resolution is essential for applications such as power field testing and damage assessment. However, significant challenges are posed by the fast, uneven propagation of shock waves and unstable testing conditions. To address these challenges, a novel framework is proposed that utilizes multiple event cameras to estimate the asymmetry of shock waves, leveraging its high-speed and high-dynamic range capabilities. Initially, a polar coordinate system is established, which encodes events to reveal shock wave propagation patterns, with adaptive region-of-interest (ROI) extraction through event offset calculations. Subsequently, shock wave front events are extracted using iterative slope analysis, exploiting the continuity of velocity changes. Finally, the geometric model of events and shock wave motion parameters is derived according to event-based optical imaging model, along with the 3D reconstruction model. Through the above process, multi-angle shock wave measurement, motion field reconstruction, and explosive equivalence inversion are achieved. The results of the speed measurement are compared with those of the pressure sensors and the empirical formula, revealing a maximum error of 5.20% and a minimum error of 0.06%. The experimental results demonstrate that our method achieves high-precision measurement of the shock wave motion field with both high spatial and temporal resolution, representing significant progress.

</details>


### [37] [Scalpel-SAM: A Semi-Supervised Paradigm for Adapting SAM to Infrared Small Object Detection](https://arxiv.org/abs/2512.22483)
*Zihan Liu,Xiangning Ren,Dezhang Kong,Yipeng Zhang,Meng Han*

Main category: cs.CV

TL;DR: 本文提出了一种基于多任务混合的半监督框架，通过先验知识蒸馏和部署导向的知识迁移两个阶段，实现红外小目标检测的半监督学习。


<details>
  <summary>Details</summary>
Motivation: 红外小目标检测在标注成本高昂的情况下急需半监督学习方法，现有方法如SAM存在领域差异、无法编码物理先验和架构复杂等挑战。

Method: 提出了一种分层次的MoE适配器，基于此核心组件设计了两阶段知识蒸馏和迁移：先验知识蒸馏阶段使用MoE适配器和少量完全监督数据对SAM进行蒸馏生成专家教师Scalpel-SAM；部署导向的知识迁移阶段使用Scalpel-SAM生成伪标签，用于训练轻量高效的下游模型。

Result: 实验表明，在少量标注数据的情况下，本文提出的框架可以使下游模型达到或超过完全监督模型的效果。这是第一个系统地使用SAM作为教师模型解决红外小目标检测中数据稀缺问题的半监督框架。

Conclusion: 本文提出的方法能有效应对红外小目标检测中的半监督学习挑战，并且在实验中展示了其有效性和优势。

Abstract: Infrared small object detection urgently requires semi-supervised paradigms due to the high cost of annotation. However, existing methods like SAM face significant challenges of domain gaps, inability of encoding physical priors, and inherent architectural complexity. To address this, we designed a Hierarchical MoE Adapter consisting of four white-box neural operators. Building upon this core component, we propose a two-stage paradigm for knowledge distillation and transfer: (1) Prior-Guided Knowledge Distillation, where we use our MoE adapter and 10% of available fully supervised data to distill SAM into an expert teacher (Scalpel-SAM); and (2) Deployment-Oriented Knowledge Transfer, where we use Scalpel-SAM to generate pseudo labels for training lightweight and efficient downstream models. Experiments demonstrate that with minimal annotations, our paradigm enables downstream models to achieve performance comparable to, or even surpassing, their fully supervised counterparts. To our knowledge, this is the first semi-supervised paradigm that systematically addresses the data scarcity issue in IR-SOT using SAM as the teacher model.

</details>


### [38] [SCAFusion: A Multimodal 3D Detection Framework for Small Object Detection in Lunar Surface Exploration](https://arxiv.org/abs/2512.22503)
*Xin Chen,Kang Luo,Yangyi Xiao,Hesheng Wang*

Main category: cs.CV

TL;DR: SCFuseion通过增强多模态特征一致性和引入新颖的Section aware Coordinate Attention机制，在月球环境下实现了小型不规则目标的可靠检测。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态3D感知方法在月球等非地球环境中表现不佳，为了解决这个问题，提出了SCFuseion，一种专门针对月球无人探测任务的多模态3D目标检测模型。

Method: 基于BEVFusion框架，SCFuseion模型引入了认知适配器、对比校准模块、视觉辅助训练分支以及Section aware Coordinate Attention机制等模块，来提升模型对小型不规则目标的检测能力。

Result: SCFuseion模型在nuScenes验证集上的mAP取得了69.7%，NDS取得了72.1%，分别比基线提升了5.0%和2.7%。在模拟月球环境下的Isaac Sim中，其mAP达到了90.93%，比基线高出11.5%，特别在检测小型陨石等小而不规则的障碍物方面表现出色。

Conclusion: SCFuseion模型通过减少参数量和计算量提升，改进了小型不规则目标在月球环境下的检测性能。

Abstract: Reliable and precise detection of small and irregular objects, such as meteor fragments and rocks, is critical for autonomous navigation and operation in lunar surface exploration. Existing multimodal 3D perception methods designed for terrestrial autonomous driving often underperform in off world environments due to poor feature alignment, limited multimodal synergy, and weak small object detection. This paper presents SCAFusion, a multimodal 3D object detection model tailored for lunar robotic missions. Built upon the BEVFusion framework, SCAFusion integrates a Cognitive Adapter for efficient camera backbone tuning, a Contrastive Alignment Module to enhance camera LiDAR feature consistency, a Camera Auxiliary Training Branch to strengthen visual representation, and most importantly, a Section aware Coordinate Attention mechanism explicitly designed to boost the detection performance of small, irregular targets. With negligible increase in parameters and computation, our model achieves 69.7% mAP and 72.1% NDS on the nuScenes validation set, improving the baseline by 5.0% and 2.7%, respectively. In simulated lunar environments built on Isaac Sim, SCAFusion achieves 90.93% mAP, outperforming the baseline by 11.5%, with notable gains in detecting small meteor like obstacles.

</details>


### [39] [CoAgent: Collaborative Planning and Consistency Agent for Coherent Video Generation](https://arxiv.org/abs/2512.22536)
*Qinglin Zeng,Kaitong Cai,Ruiqi Chen,Qinhan Lv,Keze Wang*

Main category: cs.CV

TL;DR: CoAgent 是一种协作性和闭环框架，通过计划-合成-验证的流水线流程，显著提高开放域视频生成的叙事连贯性、视觉一致性及叙事质量。


<details>
  <summary>Details</summary>
Motivation: 当前文本到视频模型在处理开放域视频生成时存在信息不一致性和时间结构不稳定的问题，因此需要一种能够维持叙事连贯性和视觉一致性的框架。

Method: CoAgent 框架包括 Storyboard Planner, Global Context Manager, Synthesis Module, Visual Consistency Controller, Verifier Agent和Pacing-Aware Editor，形成了计划-合成-验证的流程。

Result: 实验表明，CoAgent 显著提高了长视频生成的连贯性、视觉一致性和叙事质量。

Conclusion: CoAgent 通过其协作性和闭环设计解决了现有模型的局限性，为开放域视频生成提供了新的解决方案。

Abstract: Maintaining narrative coherence and visual consistency remains a central challenge in open-domain video generation. Existing text-to-video models often treat each shot independently, resulting in identity drift, scene inconsistency, and unstable temporal structure. We propose CoAgent, a collaborative and closed-loop framework for coherent video generation that formulates the process as a plan-synthesize-verify pipeline. Given a user prompt, style reference, and pacing constraints, a Storyboard Planner decomposes the input into structured shot-level plans with explicit entities, spatial relations, and temporal cues. A Global Context Manager maintains entity-level memory to preserve appearance and identity consistency across shots. Each shot is then generated by a Synthesis Module under the guidance of a Visual Consistency Controller, while a Verifier Agent evaluates intermediate results using vision-language reasoning and triggers selective regeneration when inconsistencies are detected. Finally, a pacing-aware editor refines temporal rhythm and transitions to match the desired narrative flow. Extensive experiments demonstrate that CoAgent significantly improves coherence, visual consistency, and narrative quality in long-form video generation.

</details>


### [40] [Self-Rewarded Multimodal Coherent Reasoning Across Diverse Visual Domains](https://arxiv.org/abs/2512.22545)
*Jesen Zhang,Ningyuan Liu,Kaitong Cai,Sidi Liu,Jing Yang,Ziliang Chen,Xiaofei Sun,Keze Wang*

Main category: cs.CV

TL;DR: SR-MCR 是一个轻量级且无标签的框架，通过利用模型输出的内在过程信号来对齐推理过程，从而改进多模态大模型的中间推理与视觉一致性。


<details>
  <summary>Details</summary>
Motivation: 目前多模态大语言模型（LLM）在推理方面表现出流畅但不可靠的特点，现有方法大多仅监督最终答案而忽略了中间推理过程的可靠性。

Method: SR-MCR 引入了五个自参照线索：语义对齐、词汇保真度、非冗余性、视觉接地和步骤一致性，整合成一个可靠性加权的奖励，以提供细粒度的过程级指导。此外，还使用了一个无评论家的目标函数，并增强了一个信心感知冷却机制，更稳定地训练并抑制简单的或过于自信的生成。

Result: SR-MCR 在 Qwen2.5-VL 上构建，显著提高了答案精度与推理连贯性，特别是在视觉基准测试上。SR-MCR-7B 达到开源模型中的最佳性能，平均精度为 81.4%。

Conclusion: 该工作通过 SR-MCR 框架相较于现有方法在多个视觉基准上的表现有了显著提升，证明了该方法的有效性。

Abstract: Multimodal LLMs often produce fluent yet unreliable reasoning, exhibiting weak step-to-step coherence and insufficient visual grounding, largely because existing alignment approaches supervise only the final answer while ignoring the reliability of the intermediate reasoning process. We introduce SR-MCR, a lightweight and label-free framework that aligns reasoning by exploiting intrinsic process signals derived directly from model outputs. Five self-referential cues -- semantic alignment, lexical fidelity, non-redundancy, visual grounding, and step consistency -- are integrated into a normalized, reliability-weighted reward that provides fine-grained process-level guidance. A critic-free GRPO objective, enhanced with a confidence-aware cooling mechanism, further stabilizes training and suppresses trivial or overly confident generations. Built on Qwen2.5-VL, SR-MCR improves both answer accuracy and reasoning coherence across a broad set of visual benchmarks; among open-source models of comparable size, SR-MCR-7B achieves state-of-the-art performance with an average accuracy of 81.4%. Ablation studies confirm the independent contributions of each reward term and the cooling module.

</details>


### [41] [PTalker: Personalized Speech-Driven 3D Talking Head Animation via Style Disentanglement and Modality Alignment](https://arxiv.org/abs/2512.22602)
*Bin Wang,Yang Xu,Huan Zhao,Hao Zhang,Zixing Zhang*

Main category: cs.CV

TL;DR: PTalker框架通过风格解缠和多层次对齐机制生成逼真的个性化3D说话头部动画。


<details>
  <summary>Details</summary>
Motivation: 当前3D说话头部生成方法忽视了个体说话风格的细微差别，导致风格和个人化的现实感不强。

Method: PTalker通过设计风格解缠约束来分离驱动音频和动作序列中的风格和内容空间，并采用多级对齐机制（空间、时间、特征对齐）来提高唇同步精度。

Result: PTalker在公共数据集上的实验证明能够生成与个体特定说话风格匹配的逼真3D说话头部，并且超越了现有最佳方法。

Conclusion: PTalker框架提升了3D说话头部的个性化和真实性，为该领域带来了改进。

Abstract: Speech-driven 3D talking head generation aims to produce lifelike facial animations precisely synchronized with speech. While considerable progress has been made in achieving high lip-synchronization accuracy, existing methods largely overlook the intricate nuances of individual speaking styles, which limits personalization and realism. In this work, we present a novel framework for personalized 3D talking head animation, namely "PTalker". This framework preserves speaking style through style disentanglement from audio and facial motion sequences and enhances lip-synchronization accuracy through a three-level alignment mechanism between audio and mesh modalities. Specifically, to effectively disentangle style and content, we design disentanglement constraints that encode driven audio and motion sequences into distinct style and content spaces to enhance speaking style representation. To improve lip-synchronization accuracy, we adopt a modality alignment mechanism incorporating three aspects: spatial alignment using Graph Attention Networks to capture vertex connectivity in the 3D mesh structure, temporal alignment using cross-attention to capture and synchronize temporal dependencies, and feature alignment by top-k bidirectional contrastive losses and KL divergence constraints to ensure consistency between speech and mesh modalities. Extensive qualitative and quantitative experiments on public datasets demonstrate that PTalker effectively generates realistic, stylized 3D talking heads that accurately match identity-specific speaking styles, outperforming state-of-the-art methods. The source code and supplementary videos are available at: PTalker.

</details>


### [42] [Enhancing Noise Resilience in Face Clustering via Sparse Differential Transformer](https://arxiv.org/abs/2512.22612)
*Dafeng Zhang,Yongqi Song,Shizhuo Liu*

Main category: cs.CV

TL;DR: 本文提出了一种基于预测的Top-K Jaccard相似性系数，并采用Sparse Differential Transformer (SDT)消除了噪声，提高了邻居节点的纯度，从而改进了面部聚类的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用Jaccard相似系数来衡量人脸嵌入之间的关系，但由于引入了大量无关节点，导致相似性测量的区分力有限，从而降低了聚类性能。需要一种能够提高邻居节点纯度并增强相似性测量可靠性的新方法。

Method: 本文首先提出了一种预测驱动的Top-K Jaccard相似性系数来提高邻居节点的纯度。接着，为了准确预测最优邻居数量（Top-K），引入了一种基于Transformer的预测模型。然而， Vanilla Transformer在处理节点关系时容易引入噪声。因此，本文提出了Sparse Differential Transformer (SDT)来减轻这一问题。

Result: 实验表明，SDT模型在多个数据集（如MS-Celeb-1M）上的表现优于现有方法，表现出更具鲁棒性的面部聚类解决方案。

Conclusion: 本文提出的方法能够显著提升面部聚类的性能，具有较高的实际应用价值。

Abstract: The method used to measure relationships between face embeddings plays a crucial role in determining the performance of face clustering. Existing methods employ the Jaccard similarity coefficient instead of the cosine distance to enhance the measurement accuracy. However, these methods introduce too many irrelevant nodes, producing Jaccard coefficients with limited discriminative power and adversely affecting clustering performance. To address this issue, we propose a prediction-driven Top-K Jaccard similarity coefficient that enhances the purity of neighboring nodes, thereby improving the reliability of similarity measurements. Nevertheless, accurately predicting the optimal number of neighbors (Top-K) remains challenging, leading to suboptimal clustering results. To overcome this limitation, we develop a Transformer-based prediction model that examines the relationships between the central node and its neighboring nodes near the Top-K to further enhance the reliability of similarity estimation. However, vanilla Transformer, when applied to predict relationships between nodes, often introduces noise due to their overemphasis on irrelevant feature relationships. To address these challenges, we propose a Sparse Differential Transformer (SDT), instead of the vanilla Transformer, to eliminate noise and enhance the model's anti-noise capabilities. Extensive experiments on multiple datasets, such as MS-Celeb-1M, demonstrate that our approach achieves state-of-the-art (SOTA) performance, outperforming existing methods and providing a more robust solution for face clustering.

</details>


### [43] [Dream-VL & Dream-VLA: Open Vision-Language and Vision-Language-Action Models with Diffusion Language Model Backbone](https://arxiv.org/abs/2512.22615)
*Jiacheng Ye,Shansan Gong,Jiahui Gao,Junming Fan,Shuang Wu,Wei Bi,Haoli Bai,Lifeng Shang,Lingpeng Kong*

Main category: cs.CV

TL;DR: Dream-VL和Dream-VLA是基于扩散的视-图语言模型，展现了在视觉规划任务上的优越性能，同时证明了这类模型在下游任务中优于自回归基线模型。


<details>
  <summary>Details</summary>
Motivation: 当前自回归大型视-图语言模型在视觉规划和动态机器人控制中的表现受限于其顺序生成过程，因此研究者探索了基于扩散的大语言模型来解决这一问题。

Method: 研究引入了Dream-VL和Dream-VLA，这两种模型均基于扩散机制，并在开放数据集上进行了预训练。Dream-VL专注于视-图语言建模，而Dream-VLA在此基础上增加了动作预测功能。

Result: Dream-VL在多项基准测试中达到新的性能里程碑，Dream-VLA则在视-图语言-动任务上实现了97.2%的平均成功率，并在多个环境上优于现有领先模型。

Conclusion: 研究表明扩散基的视-图语言模型更适合处理视觉规划任务，同时为未来的视-图语言-动模型研究提供了新的视角和工具。

Abstract: While autoregressive Large Vision-Language Models (VLMs) have achieved remarkable success, their sequential generation often limits their efficacy in complex visual planning and dynamic robotic control. In this work, we investigate the potential of constructing Vision-Language Models upon diffusion-based large language models (dLLMs) to overcome these limitations. We introduce Dream-VL, an open diffusion-based VLM (dVLM) that achieves state-of-the-art performance among previous dVLMs. Dream-VL is comparable to top-tier AR-based VLMs trained on open data on various benchmarks but exhibits superior potential when applied to visual planning tasks. Building upon Dream-VL, we introduce Dream-VLA, a dLLM-based Vision-Language-Action model (dVLA) developed through continuous pre-training on open robotic datasets. We demonstrate that the natively bidirectional nature of this diffusion backbone serves as a superior foundation for VLA tasks, inherently suited for action chunking and parallel generation, leading to significantly faster convergence in downstream fine-tuning. Dream-VLA achieves top-tier performance of 97.2% average success rate on LIBERO, 71.4% overall average on SimplerEnv-Bridge, and 60.5% overall average on SimplerEnv-Fractal, surpassing leading models such as $π_0$ and GR00T-N1. We also validate that dVLMs surpass AR baselines on downstream tasks across different training objectives. We release both Dream-VL and Dream-VLA to facilitate further research in the community.

</details>


### [44] [Rethinking Memory Design in SAM-Based Visual Object Tracking](https://arxiv.org/abs/2512.22624)
*Mohamad Alansari,Muzammal Naseer,Hasan Al Marzouqi,Naoufel Werghi,Sajid Javed*

Main category: cs.CV

TL;DR: 本研究系统地研究了基于SAM（Segment Anything Model）的记忆在视觉目标跟踪中的应用，提出了一个统一的混合记忆框架，该框架对短时外观记忆和长时干扰解决记忆进行了明确分解，显著提升了目标跟踪的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前基于SAM的记忆机制在不同模型间存在差异且机制不清晰，本文的目的在于探索统一的、设计良好的记忆机制对SAM跟踪及其下一代模型的适用性。

Method: 作者分析了基于SAM2的目标跟踪器，重点在于记忆机制的不同实现，然后在SAM3框架中重新实现了这些记忆机制，并通过大规模评估来独立分析记忆设计的影响。最终提出了一种混合记忆框架，通过实验证明其在SAM2和SAM3上的有效性和优越性。

Result: 提出的混合记忆框架在SAM2和SAM3的基础上，显著提高了在遮挡、复杂运动以及干扰场景下的目标跟踪鲁棒性。

Conclusion: 本文提出了一种混合记忆框架，通过实验证明了该框架的有效性，为SAM及其下一代模型的目标跟踪设计提供了新颖的思路和参考。

Abstract: \noindent Memory has become the central mechanism enabling robust visual object tracking in modern segmentation-based frameworks. Recent methods built upon Segment Anything Model 2 (SAM2) have demonstrated strong performance by refining how past observations are stored and reused. However, existing approaches address memory limitations in a method-specific manner, leaving the broader design principles of memory in SAM-based tracking poorly understood. Moreover, it remains unclear how these memory mechanisms transfer to stronger, next-generation foundation models such as Segment Anything Model 3 (SAM3). In this work, we present a systematic memory-centric study of SAM-based visual object tracking. We first analyze representative SAM2-based trackers and show that most methods primarily differ in how short-term memory frames are selected, while sharing a common object-centric representation. Building on this insight, we faithfully reimplement these memory mechanisms within the SAM3 framework and conduct large-scale evaluations across ten diverse benchmarks, enabling a controlled analysis of memory design independent of backbone strength. Guided by our empirical findings, we propose a unified hybrid memory framework that explicitly decomposes memory into short-term appearance memory and long-term distractor-resolving memory. This decomposition enables the integration of existing memory policies in a modular and principled manner. Extensive experiments demonstrate that the proposed framework consistently improves robustness under long-term occlusion, complex motion, and distractor-heavy scenarios on both SAM2 and SAM3 backbones. Code is available at: https://github.com/HamadYA/SAM3_Tracking_Zoo. \textbf{This is a preprint. Some results are being finalized and may be updated in a future revision.}

</details>


### [45] [Envision: Embodied Visual Planning via Goal-Imagery Video Diffusion](https://arxiv.org/abs/2512.22626)
*Yuming Gu,Yizhi Wang,Yining Hong,Yipeng Gao,Hao Jiang,Angtian Wang,Bo Liu,Nathaniel S. Dennler,Zhengfei Kuang,Hao Li,Gordon Wetzstein,Chongyang Ma*

Main category: cs.CV

TL;DR: Envision是一个基于扩散模型的框架，用于开发具备视觉规划能力的机器人，通过明确的指令和目标图像生成连贯且物理上合理的场景轨迹，显著提高了任务目标的一致性和物体保真度。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉规划方法大多依赖于向前预测，缺乏对目标的明确建模，导致路径在空间上发生漂移，无法保证目标一致性。因此，Envision通过结合目标图像生成连贯的视频轨迹，从而在视觉规划中提供了更高的目标可控性和路径连贯性。

Method: Envision的方法包括两个阶段：首先是Goal Imagery Model通过识别任务相关区域并进行场景与指令的区域感知交叉注意力处理，生成与目标紧密相关的图像。接着，Env-Goal Video Model基于一个从前一个帧和最后一个帧条件化的视频扩散模型（FL2V），在初始观察与目标图像之间生成平滑且物理上合理的视频轨迹。

Result: Envision在对象操作和图像编辑基准测试中，展示了在目标一致性和空间连贯性上的优越表现，且在对象保真度方面也取得了显著进步，具有直接支持机器人规划和控制的能力。

Conclusion: Envision通过明确的目标图像和合理的扩散生成过程，实现了视觉连贯性和物理合理性的有效结合，为机器人提供了可靠的视觉规划能力。

Abstract: Embodied visual planning aims to enable manipulation tasks by imagining how a scene evolves toward a desired goal and using the imagined trajectories to guide actions. Video diffusion models, through their image-to-video generation capability, provide a promising foundation for such visual imagination. However, existing approaches are largely forward predictive, generating trajectories conditioned on the initial observation without explicit goal modeling, thus often leading to spatial drift and goal misalignment. To address these challenges, we propose Envision, a diffusion-based framework that performs visual planning for embodied agents. By explicitly constraining the generation with a goal image, our method enforces physical plausibility and goal consistency throughout the generated trajectory. Specifically, Envision operates in two stages. First, a Goal Imagery Model identifies task-relevant regions, performs region-aware cross attention between the scene and the instruction, and synthesizes a coherent goal image that captures the desired outcome. Then, an Env-Goal Video Model, built upon a first-and-last-frame-conditioned video diffusion model (FL2V), interpolates between the initial observation and the goal image, producing smooth and physically plausible video trajectories that connect the start and goal states. Experiments on object manipulation and image editing benchmarks demonstrate that Envision achieves superior goal alignment, spatial consistency, and object preservation compared to baselines. The resulting visual plans can directly support downstream robotic planning and control, providing reliable guidance for embodied agents.

</details>


### [46] [FinPercep-RM: A Fine-grained Reward Model and Co-evolutionary Curriculum for RL-based Real-world Super-Resolution](https://arxiv.org/abs/2512.22647)
*Yidi Liu,Zihao Fan,Jie Huang,Jie Xiao,Dong Li,Wenlong Zhang,Lei Bai,Xueyang Fu,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: 本文提出了一种基于编码器-解码器架构的细粒度感知奖励模型（FinPercep-RM），它不仅能提供全局质量评分，还能生成感知降级图，量化局部缺陷。同时引入了FGR-30k数据集，包含各种真实的超分辨率模型带来的细微降级。为此，提出了一种协同进化课程学习机制（CCL），促进奖励模型和超分辨率模型的同步学习，提高训练稳定性。实验表明我们的方法在多个超分辨率模型上能够提升整体质量和局部真实感。


<details>
  <summary>Details</summary>
Motivation: 现有的基于奖励模型的强化学习与人类反馈（RLHF）方法在图像生成领域表现出色，但传统的图像质量评估（IQA）模型对局部细微差异不够敏感，导致了感知质量的不准确优化。

Method: 提出了细粒度感知奖励模型（FinPercep-RM），结合增强学习与人类反馈技术，通过生成感知降级图量化局部缺陷，解决传统IQA模型的不足。此外，引入了FGR-30k数据集进行训练，并提出了协同进化课程学习机制（CCL）来稳定训练过程。

Result: 实验结果显示，FinPercep-RM在性能评估上优于传统IQA模型，并且通过CCL机制实现了超分辨率模型的优化与稳定训练。

Conclusion: 本文提出的细粒度感知奖励模型结合课程学习机制有效地解决了超分辨率模型训练中的复杂性问题，提升了基于RLHF方法的超分辨率模型的感知质量和局部真实性。

Abstract: Reinforcement Learning with Human Feedback (RLHF) has proven effective in image generation field guided by reward models to align human preferences. Motivated by this, adapting RLHF for Image Super-Resolution (ISR) tasks has shown promise in optimizing perceptual quality with Image Quality Assessment (IQA) model as reward models. However, the traditional IQA model usually output a single global score, which are exceptionally insensitive to local and fine-grained distortions. This insensitivity allows ISR models to produce perceptually undesirable artifacts that yield spurious high scores, misaligning optimization objectives with perceptual quality and results in reward hacking. To address this, we propose a Fine-grained Perceptual Reward Model (FinPercep-RM) based on an Encoder-Decoder architecture. While providing a global quality score, it also generates a Perceptual Degradation Map that spatially localizes and quantifies local defects. We specifically introduce the FGR-30k dataset to train this model, consisting of diverse and subtle distortions from real-world super-resolution models. Despite the success of the FinPercep-RM model, its complexity introduces significant challenges in generator policy learning, leading to training instability. To address this, we propose a Co-evolutionary Curriculum Learning (CCL) mechanism, where both the reward model and the ISR model undergo synchronized curricula. The reward model progressively increases in complexity, while the ISR model starts with a simpler global reward for rapid convergence, gradually transitioning to the more complex model outputs. This easy-to-hard strategy enables stable training while suppressing reward hacking. Experiments validates the effectiveness of our method across ISR models in both global quality and local realism on RLHF methods.

</details>


### [47] [Visual Autoregressive Modelling for Monocular Depth Estimation](https://arxiv.org/abs/2512.22653)
*Amir El-Ghoussani,André Kaup,Nassir Navab,Gustavo Carneiro,Vasileios Belagiannis*

Main category: cs.CV

TL;DR: 该研究提出了一种基于视觉自回归（VAR）先验的单目深度估计方法，该方法在室内基准测试中表现出优越性能，同时在室外数据集上也表现出良好的适应性。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如基于扩散的方法）在单目深度估计任务中表现不佳，该研究旨在提供一种竞争性的替代方案。

Method: 该研究使用了一个大规模的文本到图像自回归模型，引入了一种尺度级条件上采样机制和无分类指导。整个推理过程在十个固定自回归阶段进行。

Result: 该方法只需要74000个合成样本即可微调，取得了有竞争力的结果。在室内基准测试中表现尤为出色，并在室外数据集上也具有较强的适用性。

Conclusion: 该工作确立了自回归先验作为深度估计任务中几何感知生成模型的一种补充家庭的地位，特别是在数据规模和3D视觉任务上的适应性方面显示出优势。

Abstract: We propose a monocular depth estimation method based on visual autoregressive (VAR) priors, offering an alternative to diffusion-based approaches. Our method adapts a large-scale text-to-image VAR model and introduces a scale-wise conditional upsampling mechanism with classifier-free guidance. Our approach performs inference in ten fixed autoregressive stages, requiring only 74K synthetic samples for fine-tuning, and achieves competitive results. We report state-of-the-art performance in indoor benchmarks under constrained training conditions, and strong performance when applied to outdoor datasets. This work establishes autoregressive priors as a complementary family of geometry-aware generative models for depth estimation, highlighting advantages in data scalability, and adaptability to 3D vision tasks. Code available at "https://github.com/AmirMaEl/VAR-Depth".

</details>


### [48] [Investigating Deep Learning Models for Ejection Fraction Estimation from Echocardiography Videos](https://arxiv.org/abs/2512.22657)
*Shravan Saranyan,Pramit Saha*

Main category: cs.CV

TL;DR: 本文研究了几种深度学习架构在超声心动图视频进行左心室射血分数（LVEF）估计中的有效性，发现修改后的3D Inception架构表现最佳，但模型倾向于过拟合，且性能对超参数选择易变。


<details>
  <summary>Details</summary>
Motivation: 目前临床实践中，手动评估心脏功能费时且结果存在较大的观察者间变异，而深度学习方法可能提供一种替代方案，有潜力达到经验丰富的人类专家水平。

Method: 本研究采用了3D Inception、两流和CNN-RNN三种模型架构，并通过EchoNet-Dynamic数据集（10,030个超声心动图视频）对模型进行训练和评估，同时系统地评估了架构修改和融合策略以寻找提高预测精确度的配置。

Result: 经过训练和评估，修改后的3D Inception架构表现最佳，其均方根误差（RMSE）为6.79%，但发现模型存在着过拟合的现象，小型且简单的模型通常有更好的泛化能力。此外，模型性能对于卷积核大小和规范化策略等超参数的选择非常敏感。

Conclusion: 研究得出，修改后的3D Inception架构适用于超声心动图视频中的LVEF估计，但需注意模型的过拟合问题和超参数的选择对性能的影响。所获得的设计模型架构和训练策略的见解可能会应用于更广泛的医学和非医学视频分析任务。

Abstract: Left ventricular ejection fraction (LVEF) is a key indicator of cardiac function and plays a central role in the diagnosis and management of cardiovascular disease. Echocardiography, as a readily accessible and non-invasive imaging modality, is widely used in clinical practice to estimate LVEF. However, manual assessment of cardiac function from echocardiograms is time-consuming and subject to considerable inter-observer variability. Deep learning approaches offer a promising alternative, with the potential to achieve performance comparable to that of experienced human experts. In this study, we investigate the effectiveness of several deep learning architectures for LVEF estimation from echocardiography videos, including 3D Inception, two-stream, and CNN-RNN models. We systematically evaluate architectural modifications and fusion strategies to identify configurations that maximize prediction accuracy. Models were trained and evaluated on the EchoNet-Dynamic dataset, comprising 10,030 echocardiogram videos. Our results demonstrate that modified 3D Inception architectures achieve the best overall performance, with a root mean squared error (RMSE) of 6.79%. Across architectures, we observe a tendency toward overfitting, with smaller and simpler models generally exhibiting improved generalization. Model performance was also found to be highly sensitive to hyperparameter choices, particularly convolutional kernel sizes and normalization strategies. While this study focuses on echocardiography-based LVEF estimation, the insights gained regarding architectural design and training strategies may be applicable to a broader range of medical and non-medical video analysis tasks.

</details>


### [49] [Unleashing Foundation Vision Models: Adaptive Transfer for Diverse Data-Limited Scientific Domains](https://arxiv.org/abs/2512.22664)
*Qiankun Li,Feng He,Huabao Chen,Xin Ning,Kun Wang,Zengfu Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为CLAdapter的新型聚类注意力适配器，它通过增强从大规模数据中学习到的丰富表示以适应各种数据受限的下游任务，从而提高了基础视觉模型在不同领域的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管已有大量数据集（如LAION和ImageNet）和预训练模型（如ViT和ConvNeXt）的广泛应用使计算机视觉领域取得了重要进展，但在某些数据受限的专业科学领域中，仍然面临众多挑战。

Method: CLAdapter通过引入注意力机制和聚类中心来个性化增强变换特征，并通过分布相关性和转换矩阵实现特征的定制化表示学习，从而实现从丰富的预训练特征到各种下游场景的有效适应。

Result: 通过在多个领域的10个数据集上进行实验，CLAdapter在这些受限数据科学领域中实现了最先进的性能，展示了其通过自适应转移来释放基础视觉模型潜力的有效性。

Conclusion: CLAdapter凭借其统一的接口设计，使得它能够无缝集成到多种模型架构中，有望在不同下游任务中获得更广泛的应用。

Abstract: In the big data era, the computer vision field benefits from large-scale datasets such as LAION-2B, LAION-400M, and ImageNet-21K, Kinetics, on which popular models like the ViT and ConvNeXt series have been pre-trained, acquiring substantial knowledge. However, numerous downstream tasks in specialized and data-limited scientific domains continue to pose significant challenges. In this paper, we propose a novel Cluster Attention Adapter (CLAdapter), which refines and adapts the rich representations learned from large-scale data to various data-limited downstream tasks. Specifically, CLAdapter introduces attention mechanisms and cluster centers to personalize the enhancement of transformed features through distribution correlation and transformation matrices. This enables models fine-tuned with CLAdapter to learn distinct representations tailored to different feature sets, facilitating the models' adaptation from rich pre-trained features to various downstream scenarios effectively. In addition, CLAdapter's unified interface design allows for seamless integration with multiple model architectures, including CNNs and Transformers, in both 2D and 3D contexts. Through extensive experiments on 10 datasets spanning domains such as generic, multimedia, biological, medical, industrial, agricultural, environmental, geographical, materials science, out-of-distribution (OOD), and 3D analysis, CLAdapter achieves state-of-the-art performance across diverse data-limited scientific domains, demonstrating its effectiveness in unleashing the potential of foundation vision models via adaptive transfer. Code is available at https://github.com/qklee-lz/CLAdapter.

</details>


### [50] [INTERACT-CMIL: Multi-Task Shared Learning and Inter-Task Consistency for Conjunctival Melanocytic Intraepithelial Lesion Grading](https://arxiv.org/abs/2512.22666)
*Mert Ikinci,Luna Toma,Karin U. Loeffler,Leticia Ussem,Daniela Süsskind,Julia M. Weller,Yousef Yeganeh,Martina C. Herwig-Carl,Shadi Albarqouni*

Main category: cs.CV

TL;DR: INTERACT-CMIL 是一个多头深度学习框架，能够通过联合预测五种组织病理学轴（WHO4、WHO5、横向扩展、纵向扩展和细胞异性）来改进CMIL（结膜黑色素细胞上皮内病变）的分级，具有显著的F1得分提升。


<details>
  <summary>Details</summary>
Motivation: 由于CMIL的细微形态学提示和相互关联的诊断标准，准确评估它们的分级对治疗和黑色素瘤预测至关重要。现有方法难以满足这一需求。

Method: INTERACT-CMIL 使用共特征学习与组合部分监督相结合的方法，并通过交叉任务一致性损失来强化任务间的关联性，从而实现多任务预测。

Result: 该框架在来自三个大学医院的486个专家标注的结膜活检片段新数据集上进行了训练和评估，并相对于CNN和基础模型基准线取得了显著改进，具体表现为WHO4亚类别的相对宏F1增益高达55.1%，纵向扩展分类的增益为25.0%。

Conclusion: 该方法不仅提供了与专家评估一致的多标准预测，还有望成为CMIL诊断的可重复计算基准，并向标准化数字化眼病理学迈进。

Abstract: Accurate grading of Conjunctival Melanocytic Intraepithelial Lesions (CMIL) is essential for treatment and melanoma prediction but remains difficult due to subtle morphological cues and interrelated diagnostic criteria. We introduce INTERACT-CMIL, a multi-head deep learning framework that jointly predicts five histopathological axes; WHO4, WHO5, horizontal spread, vertical spread, and cytologic atypia, through Shared Feature Learning with Combinatorial Partial Supervision and an Inter-Dependence Loss enforcing cross-task consistency. Trained and evaluated on a newly curated, multi-center dataset of 486 expert-annotated conjunctival biopsy patches from three university hospitals, INTERACT-CMIL achieves consistent improvements over CNN and foundation-model (FM) baselines, with relative macro F1 gains up to 55.1% (WHO4) and 25.0% (vertical spread). The framework provides coherent, interpretable multi-criteria predictions aligned with expert grading, offering a reproducible computational benchmark for CMIL diagnosis and a step toward standardized digital ocular pathology.

</details>


### [51] [CritiFusion: Semantic Critique and Spectral Alignment for Faithful Text-to-Image Generation](https://arxiv.org/abs/2512.22681)
*ZhenQi Chen,TsaiChing Ni,YuanFu Yang*

Main category: cs.CV

TL;DR: CriticFusion引入了一种新的文本到图像的生成框架，结合了多模态语义批判机制和频域精炼，提升了生成图像的一致性和细节，实验结果表明其在人对齐度和视觉质量上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前的文本到图像模型虽然在视觉保真度上表现出色，但在复杂指令下的语义对齐效果不佳。CriticFusion旨在改进这一问题。

Method: CriticFusion使用CriticCore模块通过视觉语言模型和语言模型丰富提示上下文，并在频域融合中间生成状态，确保生成内容与指令意图更精确对齐。

Result: CriticFusion在标准基准测试中显著提高了文本到图像的一致性和视觉质量的人类对齐度评分，尤其是在人偏好评分和美学评价上。

Conclusion: 实验和定性结果证明了CriticFusion在细节、现实感和指令忠实度方面的优越性能，显示了语义批判和频域对齐策略的有效性。

Abstract: Recent text-to-image diffusion models have achieved remarkable visual fidelity but often struggle with semantic alignment to complex prompts. We introduce CritiFusion, a novel inference-time framework that integrates a multimodal semantic critique mechanism with frequency-domain refinement to improve text-to-image consistency and detail. The proposed CritiCore module leverages a vision-language model and multiple large language models to enrich the prompt context and produce high-level semantic feedback, guiding the diffusion process to better align generated content with the prompt's intent. Additionally, SpecFusion merges intermediate generation states in the spectral domain, injecting coarse structural information while preserving high-frequency details. No additional model training is required. CritiFusion serves as a plug-in refinement stage compatible with existing diffusion backbones. Experiments on standard benchmarks show that our method notably improves human-aligned metrics of text-to-image correspondence and visual quality. CritiFusion consistently boosts performance on human preference scores and aesthetic evaluations, achieving results on par with state-of-the-art reward optimization approaches. Qualitative results further demonstrate superior detail, realism, and prompt fidelity, indicating the effectiveness of our semantic critique and spectral alignment strategy.

</details>


### [52] [Autoregressive Flow Matching for Motion Prediction](https://arxiv.org/abs/2512.22688)
*Johnathan Xie,Stefan Stojanov,Cristobal Eyzaguirre,Daniel L. K. Yamins,Jiajun Wu*

Main category: cs.CV

TL;DR: ARFM 是一种新的用于序列连续数据的概率建模方法，它依靠多种视频数据集训练来预测未来点轨迹位置，适用于长时间预测。


<details>
  <summary>Details</summary>
Motivation: 当前视频预测方法在捕捉复杂运动方面存在局限，而人类或机器人的运动预测对复杂运动建模要求较高。因此，本文旨在开发一种新的概率建模方法 ARFM，以弥补视频预测在复杂运动建模上的不足。

Method: ARFM 通过自回归流匹配技术，基于多种视频数据集训练模型，输出未来点轨迹位置。

Result: ARFM 模型能够有效预测复杂的运动轨迹，并且在机器人动作预测和人类运动预测任务上，利用预测的未来轨迹可以显著提升下游任务的表现。

Conclusion: ARFM 为连续序列数据的概率建模开辟了新的途径，并通过实验证明其在复杂运动建模上的优势。

Abstract: Motion prediction has been studied in different contexts with models trained on narrow distributions and applied to downstream tasks in human motion prediction and robotics. Simultaneously, recent efforts in scaling video prediction have demonstrated impressive visual realism, yet they struggle to accurately model complex motions despite massive scale. Inspired by the scaling of video generation, we develop autoregressive flow matching (ARFM), a new method for probabilistic modeling of sequential continuous data and train it on diverse video datasets to generate future point track locations over long horizons. To evaluate our model, we develop benchmarks for evaluating the ability of motion prediction models to predict human and robot motion. Our model is able to predict complex motions, and we demonstrate that conditioning robot action prediction and human motion prediction on predicted future tracks can significantly improve downstream task performance. Code and models publicly available at: https://github.com/Johnathan-Xie/arfm-motion-prediction.

</details>


### [53] [Multimodal Diffeomorphic Registration with Neural ODEs and Structural Descriptors](https://arxiv.org/abs/2512.22689)
*Salvador Rodriguez-Sanz,Monica Hernandez*

Main category: cs.CV

TL;DR: 该研究提出了一种使用神经常微分方程（Neural ODEs）的多模态 diffeomorphic 注册方法，旨在解决非刚性算法在准确性和计算复杂度之间的权衡问题，并能适用多模态环境，同时保持高效的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的非刚性注册算法在准确性和计算复杂度之间存在权衡，并且通常假设图像对中同构区域的强度相关性，限制了其在多模态环境中的应用。这篇文章提出了一种新颖的方法来突破这些限制。

Method: 该方法采用了基于神经常微分方程的模型，结合结构描述符和非结构图像相似性来实现多模态 diffeomorphic 注册。

Result: 通过广泛的实验评估，该方法在定性和定量上都超过了最先进的基准模型，并展示了在不同数据集组合中的优良性能，即使在不同的缩放级别下注册也能保持低误差。

Conclusion: 该研究证明了基于 Neural ODE 的方法在多模态非刚性注册中的适用性和有效性，同时强调了其对不同正则化水平的鲁棒性。

Abstract: This work proposes a multimodal diffeomorphic registration method using Neural Ordinary Differential Equations (Neural ODEs). Nonrigid registration algorithms exhibit tradeoffs between their accuracy, the computational complexity of their deformation model, and its proper regularization. In addition, they also assume intensity correlation in anatomically homologous regions of interest among image pairs, limiting their applicability to the monomodal setting. Unlike learning-based models, we propose an instance-specific framework that is not subject to high scan requirements for training and does not suffer performance degradation at inference time on modalities unseen during training. Our method exploits the potential of continuous-depth networks in the Neural ODE paradigm with structural descriptors, widely adopted as modality-agnostic metric models which exploit self-similarities on parameterized neighborhood geometries. We propose three different variants that integrate image-based or feature-based structural descriptors and nonstructural image similarities computed by local mutual information. We conduct extensive evaluations on different experiments formed by scan dataset combinations and show surpassing qualitative and quantitative results compared to state-of-the-art baselines adequate for large or small deformations, and specific of multimodal registration. Lastly, we also demonstrate the underlying robustness of the proposed framework to varying levels of explicit regularization while maintaining low error, its suitability for registration at varying scales, and its efficiency with respect to other methods targeted to large-deformation registration.

</details>


### [54] [SCPainter: A Unified Framework for Realistic 3D Asset Insertion and Novel View Synthesis](https://arxiv.org/abs/2512.22706)
*Paul Dobre,Jackson Cooper,Xin Wang,Hongzhou Yang*

Main category: cs.CV

TL;DR: SCPainter 提供了一个统一的框架，结合了 3D 车辆资产表示和 3D 场景点云的扩散生成方法，以实现基于新颖视角的 3D 车辆实景插入和新型视图合成。该框架在 Waymo 开放数据集上的评估展示了其在生成多样化和逼真驾驶数据方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 当前 3D 资产重建技术在重建动态角色时虽然在结构和外观上很准确，但在光照或阴影上仍然难以捕捉真实感，而现有的观点合成方法大多将资产插入和新视角合成能力作为独立处理。为了创建更丰富的训练场景和提升模型的鲁棒性和安全性，需要结合 3D 车辆插入和新视角合成。

Method: SCPainter 使用 3D 高斯点表示法车资产和 3D 场景点云相结合，采用基于扩散的生成方法来生成高质图像。它首先将 3D 车辆资产和 3D 场景点云投影到新颖视图，然后利用这些投影作为条件来训练扩散模型。

Result: SCPainter 在 Waymo 开放数据集上的评估表明其能够实现 3D 车辆实景插入和新视角合成，有助于创建多样化的和现实的驾驶数据。

Conclusion: SCPainter 提供了一个集成 3D 车辆实景插入和新视角合成的统一框架，有望显著提高自动驾驶模型的训练数据质量，增强其鲁棒性和安全性。

Abstract: 3D Asset insertion and novel view synthesis (NVS) are key components for autonomous driving simulation, enhancing the diversity of training data. With better training data that is diverse and covers a wide range of situations, including long-tailed driving scenarios, autonomous driving models can become more robust and safer. This motivates a unified simulation framework that can jointly handle realistic integration of inserted 3D assets and NVS. Recent 3D asset reconstruction methods enable reconstruction of dynamic actors from video, supporting their re-insertion into simulated driving scenes. While the overall structure and appearance can be accurate, it still struggles to capture the realism of 3D assets through lighting or shadows, particularly when inserted into scenes. In parallel, recent advances in NVS methods have demonstrated promising results in synthesizing viewpoints beyond the originally recorded trajectories. However, existing approaches largely treat asset insertion and NVS capabilities in isolation. To allow for interaction with the rest of the scene and to enable more diverse creation of new scenarios for training, realistic 3D asset insertion should be combined with NVS. To address this, we present SCPainter (Street Car Painter), a unified framework which integrates 3D Gaussian Splat (GS) car asset representations and 3D scene point clouds with diffusion-based generation to jointly enable realistic 3D asset insertion and NVS. The 3D GS assets and 3D scene point clouds are projected together into novel views, and these projections are used to condition a diffusion model to generate high quality images. Evaluation on the Waymo Open Dataset demonstrate the capability of our framework to enable 3D asset insertion and NVS, facilitating the creation of diverse and realistic driving data.

</details>


### [55] [Improved cystic hygroma detection from prenatal imaging using ultrasound-specific self-supervised representation learning](https://arxiv.org/abs/2512.22730)
*Youssef Megahed,Robin Ducharme,Inok Lee,Inbal Willner,Olivier X. Miguel,Kevin Dick,Adrian D. C. Chan,Mark Walker,Steven Hawken*

Main category: cs.CV

TL;DR: 本研究通过使用自监督预训练的Ultrasound Self-Supervised Foundation Model with Masked Autoencoding (USF-MAE)模型，提高了在胎超图像中准确检测产前透明隔囊肿的能力，相比标准的DenseNet-169模型，USF-MAE表现出更高的准确性和区分度。


<details>
  <summary>Details</summary>
Motivation: 产前透明隔囊肿是一个高风险的超声发现，预示着染色体异常、结构畸形以及不良妊娠结果的高风险。传统的监督深度学习方法受限于小规模标注数据集，因此本研究评估了超声特定的自监督预训练方法是否可以促进在关键一期超声图像中准确、稳健地检测透明隔囊肿。

Method: 本研究使用了预训练在370,000张未标注超声图像上的Ultrasound Self-Supervised Foundation Model with Masked Autoencoding (USF-MAE)，并在控制的超声影像数据集上进行了二分类微调，采用4折交叉验证方法进行评估。

Result: USF-MAE在所有评估指标上都优于基线DenseNet-169模型：准确率为0.96，敏感性为0.94，特异性为0.98，受试者操作特征曲线下面积（ROC-AUC）为0.98，而基线模型分别为0.93、0.92、0.94和0.94；并用Score-CAM可视化分析显示了模型预测的临床相关性。

Conclusion: USF-MAE模型在评估指标中取得了显著提升，证明了自监督预训练方法在胎儿透明隔囊肿的超声图像检测中的有效性，并且图像解释性分析支持了其临床适用性。

Abstract: Cystic hygroma is a high-risk prenatal ultrasound finding that portends high rates of chromosomal abnormalities, structural malformations, and adverse pregnancy outcomes. Automated detection can increase reproducibility and support scalable early screening programs, but supervised deep learning methods are limited by small labelled datasets. This study assesses whether ultrasound-specific self-supervised pretraining can facilitate accurate, robust deep learning detection of cystic hygroma in first-trimester ultrasound images. We fine-tuned the Ultrasound Self-Supervised Foundation Model with Masked Autoencoding (USF-MAE), pretrained on over 370,000 unlabelled ultrasound images, for binary classification of normal controls and cystic hygroma cases used in this study. Performance was evaluated on the same curated ultrasound dataset, preprocessing pipeline, and 4-fold cross-validation protocol as for the DenseNet-169 baseline, using accuracy, sensitivity, specificity, and the area under the receiver operating characteristic curve (ROC-AUC). Model interpretability was analyzed qualitatively using Score-CAM visualizations. USF-MAE outperformed the DenseNet-169 baseline on all evaluation metrics. The proposed model yielded a mean accuracy of 0.96, sensitivity of 0.94, specificity of 0.98, and ROC-AUC of 0.98 compared to 0.93, 0.92, 0.94, and 0.94 for the DenseNet-169 baseline, respectively. Qualitative Score-CAM visualizations of model predictions demonstrated clinical relevance by highlighting expected regions in the fetal neck for both positive and negative cases. Paired statistical analysis using a Wilcoxon signed-rank test confirmed that performance improvements achieved by USF-MAE were statistically significant (p = 0.0057).

</details>


### [56] [Split4D: Decomposed 4D Scene Reconstruction Without Video Segmentation](https://arxiv.org/abs/2512.22745)
*Yongzhen Hu,Yihui Yang,Haotong Lin,Yifan Wang,Junting Dong,Yifu Deng,Xinyu Zhu,Fan Jia,Hujun Bao,Xiaowei Zhou,Sida Peng*

Main category: cs.CV

TL;DR: 该论文提出了一种基于Freetime FeatureGS和流式特征学习策略的4D场景重建方法，提高了多视角视频4D重建质量。


<details>
  <summary>Details</summary>
Motivation: 当前方法依赖于不稳定的视频分割图，导致重建结果不可靠，因此需提出新的方法以克服此问题。

Method: 该方法使用Freetime FeatureGS模型将动态场景表示为具有可学习特征和线性运动能力的高斯原语集合，通过对比损失优化，根据2D分割图上的投影信息决定特征是否靠近或远离。同时，通过时间上有序的特征训练样本采样，实现实时特征传播。

Result: 实验结果表明，该方法在多个数据集中的重建质量显著优于现有方法。

Conclusion: 该论文提出的方法为提高4D场景重建质量提供了一种新的解决方案，具有重要研究价值。

Abstract: This paper addresses the problem of decomposed 4D scene reconstruction from multi-view videos. Recent methods achieve this by lifting video segmentation results to a 4D representation through differentiable rendering techniques. Therefore, they heavily rely on the quality of video segmentation maps, which are often unstable, leading to unreliable reconstruction results. To overcome this challenge, our key idea is to represent the decomposed 4D scene with the Freetime FeatureGS and design a streaming feature learning strategy to accurately recover it from per-image segmentation maps, eliminating the need for video segmentation. Freetime FeatureGS models the dynamic scene as a set of Gaussian primitives with learnable features and linear motion ability, allowing them to move to neighboring regions over time. We apply a contrastive loss to Freetime FeatureGS, forcing primitive features to be close or far apart based on whether their projections belong to the same instance in the 2D segmentation map. As our Gaussian primitives can move across time, it naturally extends the feature learning to the temporal dimension, achieving 4D segmentation. Furthermore, we sample observations for training in a temporally ordered manner, enabling the streaming propagation of features over time and effectively avoiding local minima during the optimization process. Experimental results on several datasets show that the reconstruction quality of our method outperforms recent methods by a large margin.

</details>


### [57] [TrimTokenator-LC: Towards Adaptive Visual Token Pruning for Large Multimodal Models with Long Contexts](https://arxiv.org/abs/2512.22748)
*Hao Zhang,Mengsi Lyu,Bo Huang,Yulong Ao,Yonghua Lin*

Main category: cs.CV

TL;DR: 本文提出了一种适应于长上下文和多张图像场景的自适应视觉标记剪枝方法，该方法将冗余分解为图像内和跨图像冗余，并通过动态预算分配维持在长上下文设置中的强性能，同时显著减少了视觉标记的数量。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉标记剪枝方法在处理长上下文和多张图像的场景时往往效果不佳，本文旨在解决这一问题。

Method: 分解冗余为图像内和跨图像冗余，并通过量化这些冗余来动态分配预算。分为两个阶段：第一阶段为每张图像分配内容感知的标记预算，并贪婪地选择最具有代表性的标记；第二阶段进行全局多样性筛选以形成候选池，并采用帕累托选择程序来平衡多样性与文本对齐。

Result: 实验表明，本文提出的自适应剪枝方法在长上下文设置中保持了良好的性能，同时显著减少了视觉标记的数量。

Conclusion: 本文提出的方法能够有效解决视觉标记剪枝在长上下文和多图像设置中的挑战，其方法在实际应用中具有一定的潜力。

Abstract: Large Multimodal Models (LMMs) have proven effective on various tasks. They typically encode visual inputs into Original Model sequences of tokens, which are then concatenated with textual tokens and jointly processed by the language model. However, the growing number of visual tokens greatly increases inference cost. Visual token pruning has emerged as a promising solution. However, existing methods often overlook scenarios involving long context inputs with multiple images. In this paper, we analyze the challenges of visual token pruning in long context, multi-image settings and introduce an adaptive pruning method tailored for such scenarios. We decompose redundancy into intra-image and inter-image components and quantify them through intra-image diversity and inter-image variation, which jointly guide dynamic budget allocation. Our approach consists of two stages. The intra-image stage allocates each image a content-aware token budget and greedily selects its most representative tokens. The inter-image stage performs global diversity filtering to form a candidate pool and then applies a Pareto selection procedure that balances diversity with text alignment. Extensive experiments show that our approach maintains strong performance in long context settings while significantly cutting down the number of visual tokens.

</details>


### [58] [Neighbor-Aware Token Reduction via Hilbert Curve for Vision Transformers](https://arxiv.org/abs/2512.22760)
*Yunge Li,Lanyu Xu*

Main category: cs.CV

TL;DR: 该研究提出了一种基于希尔伯特曲线重新排序的新型邻域感知token缩减方法，通过显式地在2D空间中保留邻域结构，实现与现有方法相比更高的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有token合并和剪枝策略往往忽略了空域连续性和邻域关系，导致局部上下文的丢失。本文旨在通过新方法保留邻域结构，提高ViTs的计算效率。

Method: 该方法包括两种关键策略：邻域感知剪枝(NAP)实现选择性token保留，邻域相似度合并(MAT)实现局部token聚集。利用希尔伯特曲线进行1D序列表示。

Result: 实验表明，该方法在准确性和效率之间达到了当前最先进的平衡。

Conclusion: 该工作强调了空域连续性和邻域结构的重要性，并为ViTs的架构优化提供了新的见解。

Abstract: Vision Transformers (ViTs) have achieved remarkable success in visual recognition tasks, but redundant token representations limit their computational efficiency. Existing token merging and pruning strategies often overlook spatial continuity and neighbor relationships, resulting in the loss of local context. This paper proposes novel neighbor-aware token reduction methods based on Hilbert curve reordering, which explicitly preserves the neighbor structure in a 2D space using 1D sequential representations. Our method introduces two key strategies: Neighbor-Aware Pruning (NAP) for selective token retention and Merging by Adjacent Token similarity (MAT) for local token aggregation. Experiments demonstrate that our approach achieves state-of-the-art accuracy-efficiency trade-offs compared to existing methods. This work highlights the importance of spatial continuity and neighbor structure, offering new insights for the architectural optimization of ViTs.

</details>


### [59] [Plug In, Grade Right: Psychology-Inspired AGIQA](https://arxiv.org/abs/2512.22780)
*Zhicheng Liao,Baoliang Chen,Hanwei Zhu,Lingyu Zhu,Shiqi Wang,Weisi Lin*

Main category: cs.CV

TL;DR: 该研究指出现有的AGIQA模型存在语义漂移的问题，并提出了一种改进的Graded Response Model (GRM)，称为Arithmetic GRM，用于克服这一问题。


<details>
  <summary>Details</summary>
Motivation: 现有的AGIQA模型通过计算图像嵌入与文本嵌入之间的相似度来估计图像质量，但观察到这些相似度分布通常是非单模态的，存在语义不一致的问题。

Method: 研究基于心理测量学的Graded Response Model (GRM)，设计了一个两分支的质量评级模块：一个分支估计图像的能力，另一个分支构建多级难度。进一步使用算术方式建模难度生成，确保难度的单调性和释义性。

Result: 提出的Arithmetic GRM 基于质量分级模块（AGQG）模块在多种先进AGIQA框架中表现出了增强性能，并能有效适应自然和屏幕内容的图像质量评估。

Conclusion: 研究认为该模型能作为未来IQA模型的核心组件之一，有广阔的潜力。

Abstract: Existing AGIQA models typically estimate image quality by measuring and aggregating the similarities between image embeddings and text embeddings derived from multi-grade quality descriptions. Although effective, we observe that such similarity distributions across grades usually exhibit multimodal patterns. For instance, an image embedding may show high similarity to both "excellent" and "poor" grade descriptions while deviating from the "good" one. We refer to this phenomenon as "semantic drift", where semantic inconsistencies between text embeddings and their intended descriptions undermine the reliability of text-image shared-space learning. To mitigate this issue, we draw inspiration from psychometrics and propose an improved Graded Response Model (GRM) for AGIQA. The GRM is a classical assessment model that categorizes a subject's ability across grades using test items with various difficulty levels. This paradigm aligns remarkably well with human quality rating, where image quality can be interpreted as an image's ability to meet various quality grades. Building on this philosophy, we design a two-branch quality grading module: one branch estimates image ability while the other constructs multiple difficulty levels. To ensure monotonicity in difficulty levels, we further model difficulty generation in an arithmetic manner, which inherently enforces a unimodal and interpretable quality distribution. Our Arithmetic GRM based Quality Grading (AGQG) module enjoys a plug-and-play advantage, consistently improving performance when integrated into various state-of-the-art AGIQA frameworks. Moreover, it also generalizes effectively to both natural and screen content image quality assessment, revealing its potential as a key component in future IQA models.

</details>


### [60] [Parallel Diffusion Solver via Residual Dirichlet Policy Optimization](https://arxiv.org/abs/2512.22796)
*Ruoyu Wang,Ziyu Li,Beier Zhu,Liangyu Yuan,Hanwang Zhang,Xun Yang,Xiaojun Chang,Chi Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种新的ODE求解器EPD-Solver，通过在每步中结合多个并行梯度评估来降低累积截断误差，从而提高采样效率。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型具有较高的生成性能，但由于其逐段去噪的特性，导致采样延迟较高。EPD-Solver旨在通过利用几何洞察，即采样轨迹主要限制在低维流形上，来缓解这种问题。

Method: EPD-Solver通过引入两个阶段的优化框架来改进现有的ODE样本生成器。第一阶段，通过基于蒸馏的方法优化少量可学习参数；第二阶段，提出了参数高效的强化学习微调方案，将求解器重新定义为一个随机Dirichlet策略。这种方法在不影响低延迟采样特性的同时，增强了复杂文本到图像任务的表现。

Result: EPD-Solver显著减少了公式积分过程中的累积截断误差，同时保持了低延迟特性，在复杂的文本到图像生成任务中取得了优异的性能。

Conclusion: EPD-Solver提供了一种灵活的方法，可以直接插件化到现有的ODE采样器中，以提升其性能。这种方法在保留低延迟的同时，有效提高了生成质量。

Abstract: Diffusion models (DMs) have achieved state-of-the-art generative performance but suffer from high sampling latency due to their sequential denoising nature. Existing solver-based acceleration methods often face significant image quality degradation under a low-latency budget, primarily due to accumulated truncation errors arising from the inability to capture high-curvature trajectory segments. In this paper, we propose the Ensemble Parallel Direction solver (dubbed as EPD-Solver), a novel ODE solver that mitigates these errors by incorporating multiple parallel gradient evaluations in each step. Motivated by the geometric insight that sampling trajectories are largely confined to a low-dimensional manifold, EPD-Solver leverages the Mean Value Theorem for vector-valued functions to approximate the integral solution more accurately. Importantly, since the additional gradient computations are independent, they can be fully parallelized, preserving low-latency sampling nature. We introduce a two-stage optimization framework. Initially, EPD-Solver optimizes a small set of learnable parameters via a distillation-based approach. We further propose a parameter-efficient Reinforcement Learning (RL) fine-tuning scheme that reformulates the solver as a stochastic Dirichlet policy. Unlike traditional methods that fine-tune the massive backbone, our RL approach operates strictly within the low-dimensional solver space, effectively mitigating reward hacking while enhancing performance in complex text-to-image (T2I) generation tasks. In addition, our method is flexible and can serve as a plugin (EPD-Plugin) to improve existing ODE samplers.

</details>


### [61] [VPTracker: Global Vision-Language Tracking via Visual Prompt and MLLM](https://arxiv.org/abs/2512.22799)
*Jingchao Wang,Kaiwen Zhou,Zhijian Wu,Kunhua Ji,Dingjiang Huang,Yefeng Zheng*

Main category: cs.CV

TL;DR: VPTracker 使用大规模多模态语言模型和位置感知视觉提示，实现了全局跟踪，提高了跟踪稳定性和目标消歧歧义性。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言跟踪方法通常局限于局部搜索，容易在视角变化、遮挡和快速目标移动时失败。VPTracker 通过引入全局搜索和位置感知视觉提示，以建立在大规模多模态语言模型上的全球跟踪框架，增强系统在这些挑战性场景下的表现。

Method: VPTracker 基于大规模多模态语言模型，通过位置感知视觉提示机制融合空间优先级，构建基于目标先前位置的区域级提示，优先区域级识别并在必要时进行全局推理。

Result: VPTracker 在各种具有挑战性的场景下，显著提高了跟踪稳定性和目标消歧歧义性。与现有方法相比，VPTracker 表现出更强大的鲁棒性和跟踪性能。

Conclusion: VPTracker 打开了将大规模多模态语言模型集成到视觉跟踪中的新途径，标志着视觉语言跟踪领域的新进展。

Abstract: Vision-Language Tracking aims to continuously localize objects described by a visual template and a language description. Existing methods, however, are typically limited to local search, making them prone to failures under viewpoint changes, occlusions, and rapid target movements. In this work, we introduce the first global tracking framework based on Multimodal Large Language Models (VPTracker), exploiting their powerful semantic reasoning to locate targets across the entire image space. While global search improves robustness and reduces drift, it also introduces distractions from visually or semantically similar objects. To address this, we propose a location-aware visual prompting mechanism that incorporates spatial priors into the MLLM. Specifically, we construct a region-level prompt based on the target's previous location, enabling the model to prioritize region-level recognition and resort to global inference only when necessary. This design retains the advantages of global tracking while effectively suppressing interference from distracting visual content. Extensive experiments show that our approach significantly enhances tracking stability and target disambiguation under challenging scenarios, opening a new avenue for integrating MLLMs into visual tracking. Code is available at https://github.com/jcwang0602/VPTracker.

</details>


### [62] [Medical Scene Reconstruction and Segmentation based on 3D Gaussian Representation](https://arxiv.org/abs/2512.22800)
*Bin Liu,Wenyan Tian,Huangxin Fu,Zizheng Li,Zhifen He,Bo Li*

Main category: cs.CV

TL;DR: 本文提出了一种基于3D高斯和三平面表示的高效医学图像3D重建方法，能够在稀疏切片条件下保持结构连续性和语义一致性，同时提高重建效率。


<details>
  <summary>Details</summary>
Motivation: 传统3D重建方法计算成本高且在稀疏切片条件下容易出现结构不连续和细节丢失的问题，无法满足临床所需的精度要求。为了克服这些挑战，本文提出了一种新的3D重建方法。

Method: 本文方法结合了3D高斯表示的高效渲染和几何表示优势，同时在稀疏切片条件下显著增强了结构连续性和语义一致性。

Result: 实验结果表明，本文方法在多模态医学数据集如超声和MRI中能够生成高质量、解剖学上连贯且语义稳定的医学图像，同时大幅提高重建效率。

Conclusion: 本文提出的方法提供了一种高效可靠的医学图像3D可视化和临床分析的新途径。

Abstract: 3D reconstruction of medical images is a key technology in medical image analysis and clinical diagnosis, providing structural visualization support for disease assessment and surgical planning. Traditional methods are computationally expensive and prone to structural discontinuities and loss of detail in sparse slices, making it difficult to meet clinical accuracy requirements.To address these challenges, we propose an efficient 3D reconstruction method based on 3D Gaussian and tri-plane representations. This method not only maintains the advantages of Gaussian representation in efficient rendering and geometric representation but also significantly enhances structural continuity and semantic consistency under sparse slicing conditions. Experimental results on multimodal medical datasets such as US and MRI show that our proposed method can generate high-quality, anatomically coherent, and semantically stable medical images under sparse data conditions, while significantly improving reconstruction efficiency. This provides an efficient and reliable new approach for 3D visualization and clinical analysis of medical images.

</details>


### [63] [Evaluating the Performance of Open-Vocabulary Object Detection in Low-quality Image](https://arxiv.org/abs/2512.22801)
*Po-Chih Wu*

Main category: cs.CV

TL;DR: 本研究引入了一个模拟现实世界低质量图像的新数据集，评估现有模型在低质量图像条件下的开放词汇对象检测性能。结果表明，尽管在低水平退化下模型的mAP得分没有显著下降，但高水平退化导致所有模型性能急剧下降；OWLv2模型在不同类型的退化中表现更稳定。


<details>
  <summary>Details</summary>
Motivation: 当前开放词汇对象检测模型主要在高质量图像上进行评估，本研究旨在填补低质量图像环境下模型性能评估的空白。

Method: 构建了一个模拟低质量图像的新数据集，并在此数据集上评估了多个现有开放词汇对象检测模型在不同图像退化程度下的表现。

Result: 结果显示，OWLv2模型在低水平退化下表现稳定，而其他模型（如OWL-ViT、GroundingDINO、Detic）在高水平退化时性能显著下降。

Conclusion: 此研究强调了在低质量图像条件下对开放词汇对象检测模型进行全面评估的重要性，并建议未来研究者可以使用新引入的数据集进行进一步研究。

Abstract: Open-vocabulary object detection enables models to localize and recognize objects beyond a predefined set of categories and is expected to achieve recognition capabilities comparable to human performance. In this study, we aim to evaluate the performance of existing models on open-vocabulary object detection tasks under low-quality image conditions. For this purpose, we introduce a new dataset that simulates low-quality images in the real world. In our evaluation experiment, we find that although open-vocabulary object detection models exhibited no significant decrease in mAP scores under low-level image degradation, the performance of all models dropped sharply under high-level image degradation. OWLv2 models consistently performed better across different types of degradation, while OWL-ViT, GroundingDINO, and Detic showed significant performance declines. We will release our dataset and codes to facilitate future studies.

</details>


### [64] [EgoReAct: Egocentric Video-Driven 3D Human Reaction Generation](https://arxiv.org/abs/2512.22808)
*Libo Zhang,Zekun Li,Tianyu Li,Zeyu Cao,Rui Xu,Xiaoxiao Long,Wenjia Wang,Jingbo Wang,Yuan Liu,Wenping Wang,Daquan Zhou,Taku Komura,Zhiyang Dou*

Main category: cs.CV

TL;DR: 该研究构建了Human Reaction Dataset (HRD)以解决现有数据集中的空间不一致性问题，并提出了一种名为EgoReAct的自回归框架，该框架能够实时生成与眼球动视频对齐的人类反应动作。


<details>
  <summary>Details</summary>
Motivation: 目前，准确表示人类对主体视觉输入的适应性反应一直是挑战性的任务。现有数据集往往无法满足实时精确对齐的要求，而EgoReAct旨在填补这一空白。

Method: 该方法首先通过Vector Quantised-Variational AutoEncoder压缩反应动作到一个紧凑但具备表达能力的潜在空间，然后使用生成预训练变换器从视觉输入生成反应动作。此外，EgoReAct在生成过程中引入了3D动态特性，如度量深度和头部动态，以增强空间定位。

Result: 实验结果表明，与之前的方法相比，EgoReAct在真实感、空间一致性及生成效率方面表现出显著的优势，同时在生成过程中严格遵循因果关系。

Conclusion: EgoReAct 是一个在实时场景下能够精确生成与主体视频对齐的人类反应动作的自回归框架，提供了更高的性能和数据集，为该领域树立了新的标杆。

Abstract: Humans exhibit adaptive, context-sensitive responses to egocentric visual input. However, faithfully modeling such reactions from egocentric video remains challenging due to the dual requirements of strictly causal generation and precise 3D spatial alignment. To tackle this problem, we first construct the Human Reaction Dataset (HRD) to address data scarcity and misalignment by building a spatially aligned egocentric video-reaction dataset, as existing datasets (e.g., ViMo) suffer from significant spatial inconsistency between the egocentric video and reaction motion, e.g., dynamically moving motions are always paired with fixed-camera videos. Leveraging HRD, we present EgoReAct, the first autoregressive framework that generates 3D-aligned human reaction motions from egocentric video streams in real-time. We first compress the reaction motion into a compact yet expressive latent space via a Vector Quantised-Variational AutoEncoder and then train a Generative Pre-trained Transformer for reaction generation from the visual input. EgoReAct incorporates 3D dynamic features, i.e., metric depth, and head dynamics during the generation, which effectively enhance spatial grounding. Extensive experiments demonstrate that EgoReAct achieves remarkably higher realism, spatial consistency, and generation efficiency compared with prior methods, while maintaining strict causality during generation. We will release code, models, and data upon acceptance.

</details>


### [65] [KANO: Kolmogorov-Arnold Neural Operator for Image Super-Resolution](https://arxiv.org/abs/2512.22822)
*Chenyu Li,Danfeng Hong,Bing Zhang,Zhaojie Pan,Jocelyn Chanussot*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The highly nonlinear degradation process, complex physical interactions, and various sources of uncertainty render single-image Super-resolution (SR) a particularly challenging task. Existing interpretable SR approaches, whether based on prior learning or deep unfolding optimization frameworks, typically rely on black-box deep networks to model latent variables, which leaves the degradation process largely unknown and uncontrollable. Inspired by the Kolmogorov-Arnold theorem (KAT), we for the first time propose a novel interpretable operator, termed Kolmogorov-Arnold Neural Operator (KANO), with the application to image SR. KANO provides a transparent and structured representation of the latent degradation fitting process. Specifically, we employ an additive structure composed of a finite number of B-spline functions to approximate continuous spectral curves in a piecewise fashion. By learning and optimizing the shape parameters of these spline functions within defined intervals, our KANO accurately captures key spectral characteristics, such as local linear trends and the peak-valley structures at nonlinear inflection points, thereby endowing SR results with physical interpretability. Furthermore, through theoretical modeling and experimental evaluations across natural images, aerial photographs, and satellite remote sensing data, we systematically compare multilayer perceptrons (MLPs) and Kolmogorov-Arnold networks (KANs) in handling complex sequence fitting tasks. This comparative study elucidates the respective advantages and limitations of these models in characterizing intricate degradation mechanisms, offering valuable insights for the development of interpretable SR techniques.

</details>


### [66] [3D Scene Change Modeling With Consistent Multi-View Aggregation](https://arxiv.org/abs/2512.22830)
*Zirui Zhou,Junfeng Ni,Shujie Zhang,Yixin Chen,Siyuan Huang*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Change detection plays a vital role in scene monitoring, exploration, and continual reconstruction. Existing 3D change detection methods often exhibit spatial inconsistency in the detected changes and fail to explicitly separate pre- and post-change states. To address these limitations, we propose SCaR-3D, a novel 3D scene change detection framework that identifies object-level changes from a dense-view pre-change image sequence and sparse-view post-change images. Our approach consists of a signed-distance-based 2D differencing module followed by multi-view aggregation with voting and pruning, leveraging the consistent nature of 3DGS to robustly separate pre- and post-change states. We further develop a continual scene reconstruction strategy that selectively updates dynamic regions while preserving the unchanged areas. We also contribute CCS3D, a challenging synthetic dataset that allows flexible combinations of 3D change types to support controlled evaluations. Extensive experiments demonstrate that our method achieves both high accuracy and efficiency, outperforming existing methods.

</details>


### [67] [A Minimal Solver for Relative Pose Estimation with Unknown Focal Length from Two Affine Correspondences](https://arxiv.org/abs/2512.22833)
*Zhenbao Yu,Shirong Ye,Ronghe Jin,Shunkun Liang,Zibin Liu,Huiyun Zhang,Banglei Guan*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In this paper, we aim to estimate the relative pose and focal length between two views with known intrinsic parameters except for an unknown focal length from two affine correspondences (ACs). Cameras are commonly used in combination with inertial measurement units (IMUs) in applications such as self-driving cars, smartphones, and unmanned aerial vehicles. The vertical direction of camera views can be obtained by IMU measurements. The relative pose between two cameras is reduced from 5DOF to 3DOF. We propose a new solver to estimate the 3DOF relative pose and focal length. First, we establish constraint equations from two affine correspondences when the vertical direction is known. Then, based on the properties of the equation system with nontrivial solutions, four equations can be derived. These four equations only involve two parameters: the focal length and the relative rotation angle. Finally, the polynomial eigenvalue method is utilized to solve the problem of focal length and relative rotation angle. The proposed solver is evaluated using synthetic and real-world datasets. The results show that our solver performs better than the existing state-of-the-art solvers.

</details>


### [68] [ByteLoom: Weaving Geometry-Consistent Human-Object Interactions through Progressive Curriculum Learning](https://arxiv.org/abs/2512.22854)
*Bangya Liu,Xinyu Gong,Zelin Zhao,Ziyang Song,Yulei Lu,Suhui Wu,Jun Zhang,Suman Banerjee,Hao Zhang*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Human-object interaction (HOI) video generation has garnered increasing attention due to its promising applications in digital humans, e-commerce, advertising, and robotics imitation learning. However, existing methods face two critical limitations: (1) a lack of effective mechanisms to inject multi-view information of the object into the model, leading to poor cross-view consistency, and (2) heavy reliance on fine-grained hand mesh annotations for modeling interaction occlusions. To address these challenges, we introduce ByteLoom, a Diffusion Transformer (DiT)-based framework that generates realistic HOI videos with geometrically consistent object illustration, using simplified human conditioning and 3D object inputs. We first propose an RCM-cache mechanism that leverages Relative Coordinate Maps (RCM) as a universal representation to maintain object's geometry consistency and precisely control 6-DoF object transformations in the meantime. To compensate HOI dataset scarcity and leverage existing datasets, we further design a training curriculum that enhances model capabilities in a progressive style and relaxes the demand of hand mesh. Extensive experiments demonstrate that our method faithfully preserves human identity and the object's multi-view geometry, while maintaining smooth motion and object manipulation.

</details>


### [69] [MUSON: A Reasoning-oriented Multimodal Dataset for Socially Compliant Navigation in Urban Environments](https://arxiv.org/abs/2512.22867)
*Zhuonan Liu,Xinyu Zhang,Zishuo Wang,Tomohito Kawabata,Xuesu Xiao,Ling Xiao*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Socially compliant navigation requires structured reasoning over dynamic pedestrians and physical constraints to ensure safe and interpretable decisions. However, existing social navigation datasets often lack explicit reasoning supervision and exhibit highly long-tailed action distributions, limiting models' ability to learn safety-critical behaviors. To address these issues, we introduce MUSON, a multimodal dataset for short-horizon social navigation collected across diverse indoor and outdoor campus scenes. MUSON adopts a structured five-step Chain-of-Thought annotation consisting of perception, prediction, reasoning, action, and explanation, with explicit modeling of static physical constraints and a rationally balanced discrete action space. Compared to SNEI, MUSON provides consistent reasoning, action, and explanation. Benchmarking multiple state-of-the-art Small Vision Language Models on MUSON shows that Qwen2.5-VL-3B achieves the highest decision accuracy of 0.8625, demonstrating that MUSON serves as an effective and reusable benchmark for socially compliant navigation. The dataset is publicly available at https://huggingface.co/datasets/MARSLab/MUSON

</details>


### [70] [Learning Anatomy from Multiple Perspectives via Self-supervision in Chest Radiographs](https://arxiv.org/abs/2512.22872)
*Ziyu Zhou,Haozhe Luo,Mohammad Reza Hosseinzadeh Taher,Jiaxuan Pang,Xiaowei Ding,Michael B. Gotway,Jianming Liang*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Foundation models have been successful in natural language processing and computer vision because they are capable of capturing the underlying structures (foundation) of natural languages. However, in medical imaging, the key foundation lies in human anatomy, as these images directly represent the internal structures of the body, reflecting the consistency, coherence, and hierarchy of human anatomy. Yet, existing self-supervised learning (SSL) methods often overlook these perspectives, limiting their ability to effectively learn anatomical features. To overcome the limitation, we built Lamps (learning anatomy from multiple perspectives via self-supervision) pre-trained on large-scale chest radiographs by harmoniously utilizing the consistency, coherence, and hierarchy of human anatomy as the supervision signal. Extensive experiments across 10 datasets evaluated through fine-tuning and emergent property analysis demonstrate Lamps' superior robustness, transferability, and clinical potential when compared to 10 baseline models. By learning from multiple perspectives, Lamps presents a unique opportunity for foundation models to develop meaningful, robust representations that are aligned with the structure of human anatomy.

</details>


### [71] [Let Samples Speak: Mitigating Spurious Correlation by Exploiting the Clusterness of Samples](https://arxiv.org/abs/2512.22874)
*Weiwei Li,Junzhuo Liu,Yuanyuan Ren,Yuchen Zheng,Yahao Liu,Wen Li*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Deep learning models are known to often learn features that spuriously correlate with the class label during training but are irrelevant to the prediction task. Existing methods typically address this issue by annotating potential spurious attributes, or filtering spurious features based on some empirical assumptions (e.g., simplicity of bias). However, these methods may yield unsatisfactory performance due to the intricate and elusive nature of spurious correlations in real-world data. In this paper, we propose a data-oriented approach to mitigate the spurious correlation in deep learning models. We observe that samples that are influenced by spurious features tend to exhibit a dispersed distribution in the learned feature space. This allows us to identify the presence of spurious features. Subsequently, we obtain a bias-invariant representation by neutralizing the spurious features based on a simple grouping strategy. Then, we learn a feature transformation to eliminate the spurious features by aligning with this bias-invariant representation. Finally, we update the classifier by incorporating the learned feature transformation and obtain an unbiased model. By integrating the aforementioned identifying, neutralizing, eliminating and updating procedures, we build an effective pipeline for mitigating spurious correlation. Experiments on image and NLP debiasing benchmarks show an improvement in worst group accuracy of more than 20% compared to standard empirical risk minimization (ERM). Codes and checkpoints are available at https://github.com/davelee-uestc/nsf_debiasing .

</details>


### [72] [M-ErasureBench: A Comprehensive Multimodal Evaluation Benchmark for Concept Erasure in Diffusion Models](https://arxiv.org/abs/2512.22877)
*Ju-Hsuan Weng,Jia-Wei Liao,Cheng-Fu Chou,Jun-Cheng Chen*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Text-to-image diffusion models may generate harmful or copyrighted content, motivating research on concept erasure. However, existing approaches primarily focus on erasing concepts from text prompts, overlooking other input modalities that are increasingly critical in real-world applications such as image editing and personalized generation. These modalities can become attack surfaces, where erased concepts re-emerge despite defenses. To bridge this gap, we introduce M-ErasureBench, a novel multimodal evaluation framework that systematically benchmarks concept erasure methods across three input modalities: text prompts, learned embeddings, and inverted latents. For the latter two, we evaluate both white-box and black-box access, yielding five evaluation scenarios. Our analysis shows that existing methods achieve strong erasure performance against text prompts but largely fail under learned embeddings and inverted latents, with Concept Reproduction Rate (CRR) exceeding 90% in the white-box setting. To address these vulnerabilities, we propose IRECE (Inference-time Robustness Enhancement for Concept Erasure), a plug-and-play module that localizes target concepts via cross-attention and perturbs the associated latents during denoising. Experiments demonstrate that IRECE consistently restores robustness, reducing CRR by up to 40% under the most challenging white-box latent inversion scenario, while preserving visual quality. To the best of our knowledge, M-ErasureBench provides the first comprehensive benchmark of concept erasure beyond text prompts. Together with IRECE, our benchmark offers practical safeguards for building more reliable protective generative models.

</details>


### [73] [Guided Path Sampling: Steering Diffusion Models Back on Track with Principled Path Guidance](https://arxiv.org/abs/2512.22881)
*Haosen Li,Wenshuo Chen,Shaofeng Liang,Lei Wang,Haozhe Jia,Yutao Yue*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Iterative refinement methods based on a denoising-inversion cycle are powerful tools for enhancing the quality and control of diffusion models. However, their effectiveness is critically limited when combined with standard Classifier-Free Guidance (CFG). We identify a fundamental limitation: CFG's extrapolative nature systematically pushes the sampling path off the data manifold, causing the approximation error to diverge and undermining the refinement process. To address this, we propose Guided Path Sampling (GPS), a new paradigm for iterative refinement. GPS replaces unstable extrapolation with a principled, manifold-constrained interpolation, ensuring the sampling path remains on the data manifold. We theoretically prove that this correction transforms the error series from unbounded amplification to strictly bounded, guaranteeing stability. Furthermore, we devise an optimal scheduling strategy that dynamically adjusts guidance strength, aligning semantic injection with the model's natural coarse-to-fine generation process. Extensive experiments on modern backbones like SDXL and Hunyuan-DiT show that GPS outperforms existing methods in both perceptual quality and complex prompt adherence. For instance, GPS achieves a superior ImageReward of 0.79 and HPS v2 of 0.2995 on SDXL, while improving overall semantic alignment accuracy on GenEval to 57.45%. Our work establishes that path stability is a prerequisite for effective iterative refinement, and GPS provides a robust framework to achieve it.

</details>


### [74] [Hash Grid Feature Pruning](https://arxiv.org/abs/2512.22882)
*Yangzhi Ma,Bojun Liu,Jie Li,Li Li,Dong Liu*

Main category: cs.CV

TL;DR: 提出了一种基于高斯点坐标的哈希网格特征剪枝方法，用于识别并移除无效特征，从而减少存储大小且不降低模型性能，实验表明平均码率降低了8%。


<details>
  <summary>Details</summary>
Motivation: 当前哈希网格用于隐式神经场学习，特别是对于Gaussian splatting，但在3D空间的稀疏区域导致无效特征，增加存储和传输开销。

Method: 通过分析输入高斯点的坐标来识别并移除无效特征，仅编码有效特征。

Result: 该方法在不牺牲模型性能的前提下减少了哈希网格的存储大小，相比基准方法实现了平均8%的码率降低。

Conclusion: 所提出的方法提升了率失真性能，并通过CTC标准验证了效果。

Abstract: Hash grids are widely used to learn an implicit neural field for Gaussian splatting, serving either as part of the entropy model or for inter-frame prediction. However, due to the irregular and non-uniform distribution of Gaussian splats in 3D space, numerous sparse regions exist, rendering many features in the hash grid invalid. This leads to redundant storage and transmission overhead. In this work, we propose a hash grid feature pruning method that identifies and prunes invalid features based on the coordinates of the input Gaussian splats, so that only the valid features are encoded. This approach reduces the storage size of the hash grid without compromising model performance, leading to improved rate-distortion performance. Following the Common Test Conditions (CTC) defined by the standardization committee, our method achieves an average bitrate reduction of 8% compared to the baseline approach.

</details>


### [75] [OpenGround: Active Cognition-based Reasoning for Open-World 3D Visual Grounding](https://arxiv.org/abs/2512.23020)
*Wenyuan Huang,Zhao Wang,Zhou Wei,Ting Huang,Fang Zhao,Jian Yang,Zhenyu Zhang*

Main category: cs.CV

TL;DR: 介绍了OpenGround，一种用于开放世界3D视觉定位的零样本框架。通过Active Cognition-based Reasoning（ACR）模块动态扩展视觉语言模型的认知范围，OpenGround能够处理预定义和开放世界的分类，表现出色，特别是在OpenTarget数据集上提升了17.6%。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于预定义的Object Lookup Table，限制了在未定义或不可预见目标场景中的应用，因此需要一种新的解决方案来克服这一限制。

Method: OpenGround引入了Active Cognition-based Reasoning（ACR）模块，该模块通过认知任务链实现类似人类的感知，主动推理与上下文相关的目标，并通过动态更新的OLT扩展VLM的认知范围，从而能够处理预定义和开放世界的分类。

Result: OpenGround在Nr3D上表现竞争，在ScanRefer上达到最新水平，并在OpenTarget数据集上实现了17.6%的显著提升。

Conclusion: OpenGround框架通过ACR模块展示了在开放世界3D视觉定位方面的能力，通过动态扩展OLT来增强VLM的认知能力，提升了在不同场景下的性能。

Abstract: 3D visual grounding aims to locate objects based on natural language descriptions in 3D scenes. Existing methods rely on a pre-defined Object Lookup Table (OLT) to query Visual Language Models (VLMs) for reasoning about object locations, which limits the applications in scenarios with undefined or unforeseen targets. To address this problem, we present OpenGround, a novel zero-shot framework for open-world 3D visual grounding. Central to OpenGround is the Active Cognition-based Reasoning (ACR) module, which is designed to overcome the fundamental limitation of pre-defined OLTs by progressively augmenting the cognitive scope of VLMs. The ACR module performs human-like perception of the target via a cognitive task chain and actively reasons about contextually relevant objects, thereby extending VLM cognition through a dynamically updated OLT. This allows OpenGround to function with both pre-defined and open-world categories. We also propose a new dataset named OpenTarget, which contains over 7000 object-description pairs to evaluate our method in open-world scenarios. Extensive experiments demonstrate that OpenGround achieves competitive performance on Nr3D, state-of-the-art on ScanRefer, and delivers a substantial 17.6% improvement on OpenTarget. Project Page at [this https URL](https://why-102.github.io/openground.io/).

</details>


### [76] [JavisGPT: A Unified Multi-modal LLM for Sounding-Video Comprehension and Generation](https://arxiv.org/abs/2512.22905)
*Kai Liu,Jungang Li,Yuchong Sun,Shengqiong Wu,Jianzhang Gao,Daoan Zhang,Wei Zhang,Sheng Jin,Sicheng Yu,Geng Zhan,Jiayi Ji,Fan Zhou,Liang Zheng,Shuicheng Yan,Hao Fei,Tat-Seng Chua*

Main category: cs.CV

TL;DR: JavisGPT 是一种新颖的音视频统一的大语言模型，它采用简洁的编码器-LLM解码器架构，结合了时空音视频融合模块和时间同步的学习查询，通过多层次的训练和大量高质量的音视频文本对话数据集，显著提高了在复杂和时间同步设置下的音视频理解和生成性能。


<details>
  <summary>Details</summary>
Motivation: 当前的大语言模型主要集中在单一模态的文本理解与生成上，而忽视了音视频的融合理解。为了解决这一问题，JavisGPT引入了一个新的架构，旨在提高联合音视频理解与生成的能力。

Method: JavisGPT 采用了编码器-LLM-解码器结构，包含了一个同步融合模块 (SyncFusion module) 用于时空音视频融合，以及时间同步学习查询以连接预训练的JAV-DiT生成器。此外，还设计了一个有效地分阶段训练管道，包括多模态预训练、音视频微调和指令调优。

Result: JavisGPT 在 JAV 理解和生成基准测试中表现出色，特别是在复杂或时间同步的环境中，展示了与其他现有的多模态大语言模型相比的优越性。

Conclusion: 总的来说，JavisGPT 是一种创新的多模态大语言模型，通过改进的架构设计和数据集构建，提高了复杂场景下的音视频理解和生成能力。

Abstract: This paper presents JavisGPT, the first unified multimodal large language model (MLLM) for Joint Audio-Video (JAV) comprehension and generation. JavisGPT adopts a concise encoder-LLM-decoder architecture, featuring a SyncFusion module for spatio-temporal audio-video fusion and synchrony-aware learnable queries to bridge a pretrained JAV-DiT generator. This design enables temporally coherent video-audio understanding and generation from multimodal instructions. We design an effective three-stage training pipeline consisting of multimodal pretraining, audio-video fine-tuning, and large-scale instruction-tuning, to progressively build multimodal comprehension and generation from existing vision-language models. To support this, we further construct JavisInst-Omni, a high-quality instruction dataset with over 200K GPT-4o-curated audio-video-text dialogues that span diverse and multi-level comprehension and generation scenarios. Extensive experiments on JAV comprehension and generation benchmarks show that JavisGPT outperforms existing MLLMs, particularly in complex and temporally synchronized settings.

</details>


### [77] [ColaVLA: Leveraging Cognitive Latent Reasoning for Hierarchical Parallel Trajectory Planning in Autonomous Driving](https://arxiv.org/abs/2512.22939)
*Qihang Peng,Xuesong Chen,Chenye Yang,Shaoshuai Shi,Hongsheng Li*

Main category: cs.CV

TL;DR: ColaVLA 提出了一种统一的视觉-语言-动作框架，将推理从文本转移到统一的潜在空间，并与分层并行轨迹解码器耦合，能够在单次前向传播中生成多尺度、因果关系一致的轨迹。


<details>
  <summary>Details</summary>
Motivation: 解决基于 VLM 的自主摆渡面临的关键挑战，即文本离散推理与连续控制之间的不匹配、自回归链式推理的高延迟以及不高效或非因果推理的限制。

Method: ColaVLA 采用了认知潜在推理器和分层并行规划器，将场景理解压缩成决策导向的元动作嵌入，并通过两次 VLM 前向传递，在单次前向传播中生成多尺度且因果关系一致的轨迹。

Result: ColaVLA 在 nuScenes 标准上实现了最先进的性能，同时拥有有利的效率和鲁棒性，在开环和闭环设置中均表现良好。

Conclusion: ColaVLA 推动了基于 VLM 的自主摆渡的发展，提供了一种高效、准确且安全的轨迹生成方法。

Abstract: Autonomous driving requires generating safe and reliable trajectories from complex multimodal inputs. Traditional modular pipelines separate perception, prediction, and planning, while recent end-to-end (E2E) systems learn them jointly. Vision-language models (VLMs) further enrich this paradigm by introducing cross-modal priors and commonsense reasoning, yet current VLM-based planners face three key challenges: (i) a mismatch between discrete text reasoning and continuous control, (ii) high latency from autoregressive chain-of-thought decoding, and (iii) inefficient or non-causal planners that limit real-time deployment. We propose ColaVLA, a unified vision-language-action framework that transfers reasoning from text to a unified latent space and couples it with a hierarchical, parallel trajectory decoder. The Cognitive Latent Reasoner compresses scene understanding into compact, decision-oriented meta-action embeddings through ego-adaptive selection and only two VLM forward passes. The Hierarchical Parallel Planner then generates multi-scale, causality-consistent trajectories in a single forward pass. Together, these components preserve the generalization and interpretability of VLMs while enabling efficient, accurate and safe trajectory generation. Experiments on the nuScenes benchmark show that ColaVLA achieves state-of-the-art performance in both open-loop and closed-loop settings with favorable efficiency and robustness.

</details>


### [78] [CLIP-Joint-Detect: End-to-End Joint Training of Object Detectors with Contrastive Vision-Language Supervision](https://arxiv.org/abs/2512.22969)
*Behnam Raoufi,Hossein Sharify,Mohamad Mahdee Ramezanee,Khosrow Hajsadeghi,Saeed Bagheri Shouraki*

Main category: cs.CV

TL;DR: CLIP-Joint-Detect 提出了一种新的检测框架，通过端到端联合训练将 CLIP 风格的对比视觉语言监督融入检测中，适用于多种检测模型，实验表明能提高检测性能。


<details>
  <summary>Details</summary>
Motivation: 传统的基于交叉熵分类的物体检测器容易受到类别不平衡和标签噪声的影响。CLIP-Joint-Detect 框架旨在通过引入类别的对比学习增强物体检测的鲁棒性。

Method: CLIP-Joint-Detect 框架通过端到端联合训练，将 CLIP 风格的对比视觉语言监督集成，使用 InfoNCE 对比损失和辅助的交叉熵项，同时优化标准检测损失。特征通过一个轻量级并行头被投影到 CLIP 语义空间中。

Result: CLIP-Joint-Detect 在Pascal VOC 2007+2012 和 MS COCO 2017 上取得了显著的检测性能提升，保持了实时推理速度。其性能在不同架构和数据集上得到了验证。

Conclusion: CLIP-Joint-Detect 通过结合视觉与语言信息，改善了传统检测器的检测性能，并且此框架能够兼容多种检测模型。

Abstract: Conventional object detectors rely on cross-entropy classification, which can be vulnerable to class imbalance and label noise. We propose CLIP-Joint-Detect, a simple and detector-agnostic framework that integrates CLIP-style contrastive vision-language supervision through end-to-end joint training. A lightweight parallel head projects region or grid features into the CLIP embedding space and aligns them with learnable class-specific text embeddings via InfoNCE contrastive loss and an auxiliary cross-entropy term, while all standard detection losses are optimized simultaneously. The approach applies seamlessly to both two-stage and one-stage architectures. We validate it on Pascal VOC 2007+2012 using Faster R-CNN and on the large-scale MS COCO 2017 benchmark using modern YOLO detectors (YOLOv11), achieving consistent and substantial improvements while preserving real-time inference speed. Extensive experiments and ablations demonstrate that joint optimization with learnable text embeddings markedly enhances closed-set detection performance across diverse architectures and datasets.

</details>


### [79] [PathoSyn: Imaging-Pathology MRI Synthesis via Disentangled Deviation Diffusion](https://arxiv.org/abs/2512.23130)
*Jian Wang,Sixing Rong,Jiarui Xing,Yuling Xu,Weide Liu*

Main category: cs.CV

TL;DR: PathoSyn 是一个统一的生成框架，用于合成磁共振成像 (MRI) 图像，它通过解耦解剖和病理变化来解决现有生成模型中的特征缠绕问题，从而生成高质量、真实且解剖学准确的合成数据，支持疾病进展建模和临床决策系统评估。


<details>
  <summary>Details</summary>
Motivation: 面对现有深生成模型在像素全局域或二元掩膜域中生成图像时出现的特征纠缠问题，导致解剖结构损坏和结构不连续，PathoSyn 旨在通过解多步骤歧分的任务分解，共同捕捉图像细节同时保持全局解剖结构的完整性来解决这些问题。

Method: PathoSyn 通过将合成任务分解为确定性的解剖学重建和随机偏差建模，利用 Deviation-Space Diffusion Model 学习病理残差的条件分布，并通过耦合空间一致性的缝合策略和推理时的稳定模块来保证空间一致性，从而生成高保真度的患者特异性合成数据。

Result: PathoSyn 在肿瘤成像基准测试上的定量和定性评估表明，与整体扩散和条件掩膜基线相比，在感知真实性和解剖学保真度方面表现更优。

Conclusion: PathoSyn 提供了一个强大的机制来生成高质量的患者特异性合成数据，对于在数据稀缺的情况下构建鲁棒诊断算法具有重要意义并支持疾病进展建模和临床决策系统基准测试。

Abstract: We present PathoSyn, a unified generative framework for Magnetic Resonance Imaging (MRI) image synthesis that reformulates imaging-pathology as a disentangled additive deviation on a stable anatomical manifold. Current generative models typically operate in the global pixel domain or rely on binary masks, these paradigms often suffer from feature entanglement, leading to corrupted anatomical substrates or structural discontinuities. PathoSyn addresses these limitations by decomposing the synthesis task into deterministic anatomical reconstruction and stochastic deviation modeling. Central to our framework is a Deviation-Space Diffusion Model designed to learn the conditional distribution of pathological residuals, thereby capturing localized intensity variations while preserving global structural integrity by construction. To ensure spatial coherence, the diffusion process is coupled with a seam-aware fusion strategy and an inference-time stabilization module, which collectively suppress boundary artifacts and produce high-fidelity internal lesion heterogeneity. PathoSyn provides a mathematically principled pipeline for generating high-fidelity patient-specific synthetic datasets, facilitating the development of robust diagnostic algorithms in low-data regimes. By allowing interpretable counterfactual disease progression modeling, the framework supports precision intervention planning and provides a controlled environment for benchmarking clinical decision-support systems. Quantitative and qualitative evaluations on tumor imaging benchmarks demonstrate that PathoSyn significantly outperforms holistic diffusion and mask-conditioned baselines in both perceptual realism and anatomical fidelity. The source code of this work will be made publicly available.

</details>


### [80] [YOLO-IOD: Towards Real Time Incremental Object Detection](https://arxiv.org/abs/2512.22973)
*Shizhou Zhang,Xueqiang Lv,Yinghui Xing,Qirui Wu,Di Xu,Chen Zhao,Yanning Zhang*

Main category: cs.CV

TL;DR: YOLO-IOD 是一种新的 YOLO 基础增量对象检测框架，通过三种机制—冲突感知伪标签精炼、重要内核选择和跨阶段非对称知识蒸馏—解决 YOLO 模型在增量学习过程中的知识冲突问题，从而实现高效且低遗忘的增量对象检测。


<details>
  <summary>Details</summary>
Motivation: 当前的增量对象检测方法大多依赖于 Faster R-CNN 或 DETR 系列检测器，但未能充分利用 YOLO 的实时性能。本文旨在解决 YOLO 在增量学习过程中面临的知识冲突问题，提出了一种新的增量对象检测方法 YOLO-IOD。

Method: YOLO-IOD 通过引入冲突感知伪标签精炼（CPR）、重要内核选择（IKS）和跨阶段非对称知识蒸馏（CAKD）三种机制，分别解决前景背景混淆、参数干扰和知识蒸馏不一致的问题。此外，还引入了 LoCo COCO 数据集以提高增量学习的现实性。

Result: 在传统和 LoCo COCO 两个基准数据集上的实验结果表明，YOLO-IOD 能够实现高效且低遗忘的增量对象检测。

Conclusion: YOLO-IOD 成功地解决了 YOLO 增量学习过程中存在的主要知识冲突问题，为实时增量对象检测提供了新的解决方案。

Abstract: Current methods for incremental object detection (IOD) primarily rely on Faster R-CNN or DETR series detectors; however, these approaches do not accommodate the real-time YOLO detection frameworks. In this paper, we first identify three primary types of knowledge conflicts that contribute to catastrophic forgetting in YOLO-based incremental detectors: foreground-background confusion, parameter interference, and misaligned knowledge distillation. Subsequently, we introduce YOLO-IOD, a real-time Incremental Object Detection (IOD) framework that is constructed upon the pretrained YOLO-World model, facilitating incremental learning via a stage-wise parameter-efficient fine-tuning process. Specifically, YOLO-IOD encompasses three principal components: 1) Conflict-Aware Pseudo-Label Refinement (CPR), which mitigates the foreground-background confusion by leveraging the confidence levels of pseudo labels and identifying potential objects relevant to future tasks. 2) Importancebased Kernel Selection (IKS), which identifies and updates the pivotal convolution kernels pertinent to the current task during the current learning stage. 3) Cross-Stage Asymmetric Knowledge Distillation (CAKD), which addresses the misaligned knowledge distillation conflict by transmitting the features of the student target detector through the detection heads of both the previous and current teacher detectors, thereby facilitating asymmetric distillation between existing and newly introduced categories. We further introduce LoCo COCO, a more realistic benchmark that eliminates data leakage across stages. Experiments on both conventional and LoCo COCO benchmarks show that YOLO-IOD achieves superior performance with minimal forgetting.

</details>


### [81] [RealCamo: Boosting Real Camouflage Synthesis with Layout Controls and Textual-Visual Guidance](https://arxiv.org/abs/2512.22974)
*Chunyuan Chen,Yunuo Cai,Shujuan Li,Weiyun Liang,Bin Wang,Jing Xu*

Main category: cs.CV

TL;DR: 该研究提出了一种名为 ReamCamo 的统一出图框架，旨在提高生成的伪装图像的真实性和视觉保真度。


<details>
  <summary>Details</summary>
Motivation: 现有伪装图像生成方法难以真实地表现伪装效果，尤其是伪装与背景之间的语义一致性不够。

Method: 提出了一个新型的出图框架，通过引入全局布局控制来增强前景与背景的语义一致性，并结合语义化的背景检索和多模态的文本-视觉条件指导生成过程。

Result: 通过 ReamCamo 生成的图像在背景-前景分布差异度方面表现出更高的伪装质量，并且生成的图像更具真实性和视觉保真度。

Conclusion: 本文证明了 ReamCamo 在伪装图像生成方面取得了显著效果，为伪装目标检测提供了更高质量的数据支持。

Abstract: Camouflaged image generation (CIG) has recently emerged as an efficient alternative for acquiring high-quality training data for camouflaged object detection (COD). However, existing CIG methods still suffer from a substantial gap to real camouflaged imagery: generated images either lack sufficient camouflage due to weak visual similarity, or exhibit cluttered backgrounds that are semantically inconsistent with foreground targets. To address these limitations, we propose ReamCamo, a unified out-painting based framework for realistic camouflaged image generation. ReamCamo explicitly introduces additional layout controls to regulate global image structure, thereby improving semantic coherence between foreground objects and generated backgrounds. Moreover, we construct a multi-modal textual-visual condition by combining a unified fine-grained textual task description with texture-oriented background retrieval, which jointly guides the generation process to enhance visual fidelity and realism. To quantitatively assess camouflage quality, we further introduce a background-foreground distribution divergence metric that measures the effectiveness of camouflage in generated images. Extensive experiments and visualizations demonstrate the effectiveness of our proposed framework.

</details>


### [82] [ForCM: Forest Cover Mapping from Multispectral Sentinel-2 Image by Integrating Deep Learning with Object-Based Image Analysis](https://arxiv.org/abs/2512.23196)
*Maisha Haque,Israt Jahan Ayshi,Sadaf M. Anis,Nahian Tasnim,Mithila Moontaha,Md. Sabbir Ahmed,Muhammad Iqbal Hossain,Mohammad Zavid Parvez,Subrata Chakraborty,Biswajeet Pradhan,Biswajit Banik*

Main category: cs.CV

TL;DR: 该研究提出了一种名为ForCM的新方法，结合了基于对象的图像分析（OBIA）和深度学习（DL），使用多光谱的Sentinel-2图像对亚马逊雨林的森林覆盖进行映射。通过验证不同DL模型，结果表明，结合OBIA的DL模型能够提高森林覆盖率的精度。


<details>
  <summary>Details</summary>
Motivation: 鉴于传统的基于对象的图像分析方法在处理复杂地理环境时的局限性，研究旨在探索将深度学习模型与基于对象的图像分析相结合的新方法，以提升森林覆盖映射的准确性。这种方法还探索了五种不同的DL模型，为环境监测和保护提供技术支撑。

Method: 本研究使用了多光谱的Sentinel-2图像，并测试了UNet、UNet++、ResUNet、AttentionUNet和ResNet50-Segnet五种DL模型。在验证不同DL模型的效果后，将最有效的DL模型与OBIA结合，以提高映射精度。

Result: 研究结果表明，将DL模型与OBIA结合后，森林覆盖映射的总体精度分别提高了至94.54%（使用ResUNet-OBIA）和95.64%（使用AttentionUNet-OBIA），而传统的OBIA方法的总体精度仅为92.91%。

Conclusion: 研究结果证明了将DL模型与OBIA结合的方法对于提高森林覆盖映射的准确性是有效的，且为环境监测和保护提供了一种新的手段。

Abstract: This research proposes "ForCM", a novel approach to forest cover mapping that combines Object-Based Image Analysis (OBIA) with Deep Learning (DL) using multispectral Sentinel-2 imagery. The study explores several DL models, including UNet, UNet++, ResUNet, AttentionUNet, and ResNet50-Segnet, applied to high-resolution Sentinel-2 Level 2A satellite images of the Amazon Rainforest. The datasets comprise three collections: two sets of three-band imagery and one set of four-band imagery. After evaluation, the most effective DL models are individually integrated with the OBIA technique to enhance mapping accuracy. The originality of this work lies in evaluating different deep learning models combined with OBIA and comparing them with traditional OBIA methods. The results show that the proposed ForCM method improves forest cover mapping, achieving overall accuracies of 94.54 percent with ResUNet-OBIA and 95.64 percent with AttentionUNet-OBIA, compared to 92.91 percent using traditional OBIA. This research also demonstrates the potential of free and user-friendly tools such as QGIS for accurate mapping within their limitations, supporting global environmental monitoring and conservation efforts.

</details>


### [83] [PoseStreamer: A Multi-modal Framework for 6DoF Pose Estimation of Unseen Moving Objects](https://arxiv.org/abs/2512.22979)
*Huiming Yang,Linglin Liao,Fei Ding,Sibo Wang,Zijian Zeng*

Main category: cs.CV

TL;DR: PoseStreamer 是一种用于高速移动场景下的六自由度姿态估计的鲁棒多模态框架，通过整合自适应姿态记忆队列、基于对象的二维跟踪器以及射线姿态滤波器来提高估计精度。


<details>
  <summary>Details</summary>
Motivation: 现有的6DoF姿态估计方法在高速移动场景下表现不佳，且标准RGB相机在高速和低光环境下面临运动模糊的问题，因此需要提出一种新的方法来解决这些挑战。

Method: PoseStreamer采用了自适应姿态记忆队列、基于对象的二维跟踪器和射线姿态滤波器三种核心组件，分别用于时间一致性、2D先验增强和几何细化，以提高精度。

Result: 通过PoseStreamer，展示了其在高速运动场景下的优越准确性，并且作为一种无需模板的框架，对于未见过的移动物体具有较强的泛化能力。

Conclusion: 该研究提出了PoseStreamer框架，并通过MoCapCube6D多模态数据集验证了其在高速运动场景中的表现，证明了其在姿态估计领域的潜力。

Abstract: Six degree of freedom (6DoF) pose estimation for novel objects is a critical task in computer vision, yet it faces significant challenges in high-speed and low-light scenarios where standard RGB cameras suffer from motion blur. While event cameras offer a promising solution due to their high temporal resolution, current 6DoF pose estimation methods typically yield suboptimal performance in high-speed object moving scenarios. To address this gap, we propose PoseStreamer, a robust multi-modal 6DoF pose estimation framework designed specifically on high-speed moving scenarios. Our approach integrates three core components: an Adaptive Pose Memory Queue that utilizes historical orientation cues for temporal consistency, an Object-centric 2D Tracker that provides strong 2D priors to boost 3D center recall, and a Ray Pose Filter for geometric refinement along camera rays. Furthermore, we introduce MoCapCube6D, a novel multi-modal dataset constructed to benchmark performance under rapid motion. Extensive experiments demonstrate that PoseStreamer not only achieves superior accuracy in high-speed moving scenarios, but also exhibits strong generalizability as a template-free framework for unseen moving objects.

</details>


### [84] [Spatial-aware Symmetric Alignment for Text-guided Medical Image Segmentation](https://arxiv.org/abs/2512.22981)
*Linglin Liao,Qichuan Geng,Yu Liu*

Main category: cs.CV

TL;DR: 本文提出了一种名为Spatial-aware Symmetric Alignment (SSA)的框架，以解决现有方法无法同时处理诊断和描述性文本的问题，并通过空间感知对齐机制和区域级别的方向引导策略，增强了对具有空间关系约束的病变的分割能力。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决现有技术在处理医疗图像分割时的两个关键瓶颈：第一个瓶颈是无法同时处理诊断和描述性文本，导致难以识别病灶和与图像区域建立联系。第二个瓶颈是现有的方法未能捕捉到位置约束，导致分割结果出现关键偏差。

Method: 本文提出了Spatial-aware Symmetric Alignment (SSA)框架，包含两个主要部分：1. 对称最优传输对齐机制来加强图像区域与多种相关表达之间的联系，建立双向细粒度多模态对应；2. 组合方向引导策略，通过构建区域级别的方向引导掩膜显式引入文本中的空间约束。

Result: 在公开基准上的广泛实验表明，SSA在准确分割具有空间关系约束的病灶方面达到了当前最佳水平。

Conclusion: 本文提出的SSA框架通过解决现有方法在处理富文本信息与空间约束方面的不足，显著提升了医疗图像分割的精度，特别是对具有空间关系的病灶分割具有重要作用。

Abstract: Text-guided Medical Image Segmentation has shown considerable promise for medical image segmentation, with rich clinical text serving as an effective supplement for scarce data. However, current methods have two key bottlenecks. On one hand, they struggle to process diagnostic and descriptive texts simultaneously, making it difficult to identify lesions and establish associations with image regions. On the other hand, existing approaches focus on lesions description and fail to capture positional constraints, leading to critical deviations. Specifically, with the text "in the left lower lung", the segmentation results may incorrectly cover both sides of the lung. To address the limitations, we propose the Spatial-aware Symmetric Alignment (SSA) framework to enhance the capacity of referring hybrid medical texts consisting of locational, descriptive, and diagnostic information. Specifically, we propose symmetric optimal transport alignment mechanism to strengthen the associations between image regions and multiple relevant expressions, which establishes bi-directional fine-grained multimodal correspondences. In addition, we devise a composite directional guidance strategy that explicitly introduces spatial constraints in the text by constructing region-level guidance masks. Extensive experiments on public benchmarks demonstrate that SSA achieves state-of-the-art (SOTA) performance, particularly in accurately segmenting lesions characterized by spatial relational constraints.

</details>


### [85] [Exploring Syn-to-Real Domain Adaptation for Military Target Detection](https://arxiv.org/abs/2512.23208)
*Jongoh Jeong,Youngjin Oh,Gyeongrae Nam,Jeongeun Lee,Kuk-Jin Yoon*

Main category: cs.CV

TL;DR: 该研究旨在通过使用Unreal Engine生成 RGB 基础合成数据，以应对军事领域中跨域目标检测的挑战。


<details>
  <summary>Details</summary>
Motivation: 军事目标检测需要在多种复杂环境中进行，SAR 数据虽然性能优越但成本高昂且数据集有限。因此，希望通过生成合成数据降低数据收集和处理成本，同时提升跨域目标检测的效果。

Method: 本文提出了一种基于合成数据的方法，通过使用Unreal Engine生成高保真度的RGB图像，用于军事目标检测。通过合成到真实的转换实验，验证了数据集的有效性，并与最新的监督程度较轻的域适应方法进行了比较。

Result: 结果显示，当前依赖少量图像提示（如目标类别）的半监督或轻监督域适应方法在该合成训练集上表现显著优于无监督方法。

Conclusion: 本文指出了目前军事目标检测研究中面临的挑战，并提出了一种有前景的方法来应对跨域目标检测的困境。

Abstract: Object detection is one of the key target tasks of interest in the context of civil and military applications. In particular, the real-world deployment of target detection methods is pivotal in the decision-making process during military command and reconnaissance. However, current domain adaptive object detection algorithms consider adapting one domain to another similar one only within the scope of natural or autonomous driving scenes. Since military domains often deal with a mixed variety of environments, detecting objects from multiple varying target domains poses a greater challenge. Several studies for armored military target detection have made use of synthetic aperture radar (SAR) data due to its robustness to all weather, long range, and high-resolution characteristics. Nevertheless, the costs of SAR data acquisition and processing are still much higher than those of the conventional RGB camera, which is a more affordable alternative with significantly lower data processing time. Furthermore, the lack of military target detection datasets limits the use of such a low-cost approach. To mitigate these issues, we propose to generate RGB-based synthetic data using a photorealistic visual tool, Unreal Engine, for military target detection in a cross-domain setting. To this end, we conducted synthetic-to-real transfer experiments by training our synthetic dataset and validating on our web-collected real military target datasets. We benchmark the state-of-the-art domain adaptation methods distinguished by the degree of supervision on our proposed train-val dataset pair, and find that current methods using minimal hints on the image (e.g., object class) achieve a substantial improvement over unsupervised or semi-supervised DA methods. From these observations, we recognize the current challenges that remain to be overcome.

</details>


### [86] [Reverse Personalization](https://arxiv.org/abs/2512.22984)
*Han-Wei Kung,Tuomas Varanka,Nicu Sebe*

Main category: cs.CV

TL;DR: 该研究提出了一个逆个性化框架，用于面部匿名化，通过条件扩散反向操作直接修改图像，而无需使用文本提示，同时 incorporation 了身份引导的条件支路以超越模型训练数据中的主体，实现可控制属性的匿名化。


<details>
  <summary>Details</summary>
Motivation: 现有的基于提示的方法在去除或修改身份特异性特征时依赖于个体在预训练模型中有良好的表征，或者需要针对特定身份进行模型微调，这限制了应用范围。

Method: 该研究提出了一个逆个性化框架，通过引入条件扩散反向操作和身份引导的条件支路，实现直接对图片的操纵，同时支持可控制属性的匿名化。

Result: 实验结果表明，该方法在身份移除、属性保留和图片质量之间取得了最先进的平衡。

Conclusion: 该研究提供了一种新的方法，可以有效实现面部匿名化，同时保留了可调整的面部属性控制，并且结果优于现有的方法。

Abstract: Recent text-to-image diffusion models have demonstrated remarkable generation of realistic facial images conditioned on textual prompts and human identities, enabling creating personalized facial imagery. However, existing prompt-based methods for removing or modifying identity-specific features rely either on the subject being well-represented in the pre-trained model or require model fine-tuning for specific identities. In this work, we analyze the identity generation process and introduce a reverse personalization framework for face anonymization. Our approach leverages conditional diffusion inversion, allowing direct manipulation of images without using text prompts. To generalize beyond subjects in the model's training data, we incorporate an identity-guided conditioning branch. Unlike prior anonymization methods, which lack control over facial attributes, our framework supports attribute-controllable anonymization. We demonstrate that our method achieves a state-of-the-art balance between identity removal, attribute preservation, and image quality. Source code and data are available at https://github.com/hanweikung/reverse-personalization .

</details>


### [87] [A Low-Cost UAV Deep Learning Pipeline for Integrated Apple Disease Diagnosis,Freshness Assessment, and Fruit Detection](https://arxiv.org/abs/2512.22990)
*Soham Dutta,Soham Banerjee,Sneha Mahata,Anindya Sen,Sayantani Datta*

Main category: cs.CV

TL;DR: 该研究提出了一种基于单色RGB摄像头的低成本无人机智能系统，用于苹果园的叶片病害检测、苹果新鲜度评估和实时苹果检测与定位，系统在本地运行无需云支持。


<details>
  <summary>Details</summary>
Motivation: 当前的无人机系统通常是独立处理病害检测、果实质量评估和产量估计等任务，且依赖昂贵的多光谱传感器。本研究旨在通过整合ResNet50、VGG16和YOLOv8模型，提供一种成本更低、功能更全面的解决方案，以支持精确农业。

Method: 该系统利用单色RGB摄像头，采用ResNet50进行叶片病害检测，VGG16进行苹果新鲜度评估，YOLOv8进行实时苹果检测与定位。系统部署在ESP32-CAM和Raspberry Pi上，实现了离线本地推理。

Result: 实验结果表明，叶子病害分类准确率为98.9%，苹果新鲜度分类准确率为97.4%，苹果检测的F1分数为0.857。

Conclusion: 该框架提供了一种更可访问且可扩展的替代方案，无需昂贵的多光谱传感器，支持低成本的精确农业实践。

Abstract: Apple orchards require timely disease detection, fruit quality assessment, and yield estimation, yet existing UAV-based systems address such tasks in isolation and often rely on costly multispectral sensors. This paper presents a unified, low-cost RGB-only UAV-based orchard intelligent pipeline integrating ResNet50 for leaf disease detection, VGG 16 for apple freshness determination, and YOLOv8 for real-time apple detection and localization. The system runs on an ESP32-CAM and Raspberry Pi, providing fully offline on-site inference without cloud support. Experiments demonstrate 98.9% accuracy for leaf disease classification, 97.4% accuracy for freshness classification, and 0.857 F1 score for apple detection. The framework provides an accessible and scalable alternative to multispectral UAV solutions, supporting practical precision agriculture on affordable hardware.

</details>


### [88] [Holi-DETR: Holistic Fashion Item Detection Leveraging Contextual Information](https://arxiv.org/abs/2512.23221)
*Youngchae Kwon,Jinyoung Choi,Injung Kim*

Main category: cs.CV

TL;DR: Holi-DETR 提出了一种新颖的全貌检测变换器，通过利用服装项之间的共现关系、相对位置和大小以及与人体关键点的空间关系，来克服服装检测中的模糊性。


<details>
  <summary>Details</summary>
Motivation: 现有的检测器通常独立地检测每个项，忽略了项之间的上下文信息，这可能导致模糊性。Holi-DETR 通过整合三种类型的上下文信息解决了这一问题。

Method: Holi-DETR 架构将三种不同类型的上下文信息（共现概率、相对位置和大小、项与人体关键点的空间关系）整合到 DETR 及其后续模型中。

Result: Holi-DETR 在平均精度（AP）方面比原始的 DETR 和 Co-DETR 分别提高了 3.6 个和 1.1 个百分点。

Conclusion: Holi-DETR 在处理服装检测任务时表现出色，通过利用复杂的上下文信息显著提高了精度。

Abstract: Fashion item detection is challenging due to the ambiguities introduced by the highly diverse appearances of fashion items and the similarities among item subcategories. To address this challenge, we propose a novel Holistic Detection Transformer (Holi-DETR) that detects fashion items in outfit images holistically, by leveraging contextual information. Fashion items often have meaningful relationships as they are combined to create specific styles. Unlike conventional detectors that detect each item independently, Holi-DETR detects multiple items while reducing ambiguities by leveraging three distinct types of contextual information: (1) the co-occurrence relationship between fashion items, (2) the relative position and size based on inter-item spatial arrangements, and (3) the spatial relationships between items and human body key-points. %Holi-DETR explicitly incorporates three types of contextual information: (1) the co-occurrence probability between fashion items, (2) the relative position and size based on inter-item spatial arrangements, and (3) the spatial relationships between items and human body key-points. To this end, we propose a novel architecture that integrates these three types of heterogeneous contextual information into the Detection Transformer (DETR) and its subsequent models. In experiments, the proposed methods improved the performance of the vanilla DETR and the more recently developed Co-DETR by 3.6 percent points (pp) and 1.1 pp, respectively, in terms of average precision (AP).

</details>


### [89] [Anomaly Detection by Effectively Leveraging Synthetic Images](https://arxiv.org/abs/2512.23227)
*Sungho Kang,Hyunkyu Park,Yeonho Lee,Hanbyul Lee,Mijoo Jeong,YeongHyeon Park,Injae Lee,Juneho Yi*

Main category: cs.CV

TL;DR: 本文提出了一种利用预训练的文本指导图像到图像转换模型和图像检索模型高效生成合成缺陷图像的新框架。通过两阶段训练策略，首先在基于规则的合成图像上预训练模型，然后在高质量图像上进行微调，以提高异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 鉴于实际缺陷图像稀缺，传统基于正常图像的无监督方法不一定有效。本文旨在提出一种使用合成图像提高异常检测性能且成本较低的方法。

Method: 本文首先利用预训练的文本指导图像到图像转化模型生成初始合成缺陷图像，然后通过图像检索模型进一步筛选，确保生成的图像更具真实性和相关性。此外，提出了两阶段训练策略，即先在基于规则的合成图像上预训练，再在高质量合成图像上进行微调。

Result: 实验表明，该方法在MVTec AD数据集上的异常检测性能得到了显著提升，同时降低了数据收集的成本。

Conclusion: 该研究提供了一种有效利用合成图像提高异常检测性能的方法，通过文本指导的图像合成和图像检索以及两阶段训练策略，成功解决了合成图像的真实性和相关性问题，为工业制造领域的异常检测提供了新的解决方案。

Abstract: Anomaly detection plays a vital role in industrial manufacturing. Due to the scarcity of real defect images, unsupervised approaches that rely solely on normal images have been extensively studied. Recently, diffusion-based generative models brought attention to training data synthesis as an alternative solution. In this work, we focus on a strategy to effectively leverage synthetic images to maximize the anomaly detection performance. Previous synthesis strategies are broadly categorized into two groups, presenting a clear trade-off. Rule-based synthesis, such as injecting noise or pasting patches, is cost-effective but often fails to produce realistic defect images. On the other hand, generative model-based synthesis can create high-quality defect images but requires substantial cost. To address this problem, we propose a novel framework that leverages a pre-trained text-guided image-to-image translation model and image retrieval model to efficiently generate synthetic defect images. Specifically, the image retrieval model assesses the similarity of the generated images to real normal images and filters out irrelevant outputs, thereby enhancing the quality and relevance of the generated defect images. To effectively leverage synthetic images, we also introduce a two stage training strategy. In this strategy, the model is first pre-trained on a large volume of images from rule-based synthesis and then fine-tuned on a smaller set of high-quality images. This method significantly reduces the cost for data collection while improving the anomaly detection performance. Experiments on the MVTec AD dataset demonstrate the effectiveness of our approach.

</details>


### [90] [With Great Context Comes Great Prediction Power: Classifying Objects via Geo-Semantic Scene Graphs](https://arxiv.org/abs/2512.23024)
*Ciprian Constantinescu,Marius Leordeanu*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Humans effortlessly identify objects by leveraging a rich understanding of the surrounding scene, including spatial relationships, material properties, and the co-occurrence of other objects. In contrast, most computational object recognition systems operate on isolated image regions, devoid of meaning in isolation, thus ignoring this vital contextual information. This paper argues for the critical role of context and introduces a novel framework for contextual object classification. We first construct a Geo-Semantic Contextual Graph (GSCG) from a single monocular image. This rich, structured representation is built by integrating a metric depth estimator with a unified panoptic and material segmentation model. The GSCG encodes objects as nodes with detailed geometric, chromatic, and material attributes, and their spatial relationships as edges. This explicit graph structure makes the model's reasoning process inherently interpretable. We then propose a specialized graph-based classifier that aggregates features from a target object, its immediate neighbors, and the global scene context to predict its class. Through extensive ablation studies, we demonstrate that our context-aware model achieves a classification accuracy of 73.4%, dramatically outperforming context-agnostic versions (as low as 38.4%). Furthermore, our GSCG-based approach significantly surpasses strong baselines, including fine-tuned ResNet models (max 53.5%) and a state-of-the-art multimodal Large Language Model (LLM), Llama 4 Scout, which, even when given the full image alongside a detailed description of objects, maxes out at 42.3%. These results on COCO 2017 train/val splits highlight the superiority of explicitly structured and interpretable context for object recognition tasks.

</details>


### [91] [Physics-Inspired Modeling and Content Adaptive Routing in an Infrared Gas Leak Detection Network](https://arxiv.org/abs/2512.23234)
*Dongsheng Li,Chaobo Chen,Siling Wang,Song Gao*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Detecting infrared gas leaks is critical for environmental monitoring and industrial safety, yet remains difficult because plumes are faint, small, semitransparent, and have weak, diffuse boundaries. We present physics-edge hybrid gas dynamic routing network (PEG-DRNet). First, we introduce the Gas Block, a diffusion-convection unit modeling gas transport: a local branch captures short-range variations, while a large-kernel branch captures long-range propagation. An edge-gated learnable fusion module balances local detail and global context, strengthening weak-contrast plume and contour cues. Second, we propose the adaptive gradient and phase edge operator (AGPEO), computing reliable edge priors from multi-directional gradients and phase-consistent responses. These are transformed by a multi-scale edge perception module (MSEPM) into hierarchical edge features that reinforce boundaries. Finally, the content-adaptive sparse routing path aggregation network (CASR-PAN), with adaptive information modulation modules for fusion and self, selectively propagates informative features across scales based on edge and content cues, improving cross-scale discriminability while reducing redundancy. Experiments on the IIG dataset show that PEG-DRNet achieves an overall AP of 29.8\%, an AP$_{50}$ of 84.3\%, and a small-object AP of 25.3\%, surpassing the RT-DETR-R18 baseline by 3.0\%, 6.5\%, and 5.3\%, respectively, while requiring only 43.7 Gflops and 14.9 M parameters. The proposed PEG-DRNet achieves superior overall performance with the best balance of accuracy and computational efficiency, outperforming existing CNN and Transformer detectors in AP and AP$_{50}$ on the IIG and LangGas dataset.

</details>


### [92] [ViLaCD-R1: A Vision-Language Framework for Semantic Change Detection in Remote Sensing](https://arxiv.org/abs/2512.23244)
*Xingwei Ma,Shiyang Feng,Bo Zhang,Bin Wang*

Main category: cs.CV

TL;DR: 该研究提出了一种名为ViLaCD-R1的两阶段框架，通过结合多模态和视觉语言模型（VLM）来提高遥感变化检测的准确性和鲁棒性。该框架包括一个多图像推理器（MIR）和一个掩码引导解码器（MGD），能够更好地识别和定位语义变化，抑制非语义变化，实现在复杂现实场景中的顶级准确度。


<details>
  <summary>Details</summary>
Motivation: 传统的遥感变化检测方法在语义捕捉和非语义噪声抗扰方面存在不足，而现有的视觉语言模型虽有所改进但仍然面临准确的空间定位和像素级边界划分等挑战。因此，需要一种新的方法来提升遥感变化检测的性能。

Method: 研究提出了一个两阶段框架：首先是通过监督微调（SFT）和强化学习（RL）训练视觉语言模型（VLM），用于输入双时间图像块并输出粗略的变化掩码；其次，解码器将双时间图像特征与粗略掩码结合，预测精确的二元变化图。

Result: 在多个遥感变化检测基准上的综合评估表明，ViLaCD-R1能够显著提高真正的语义变化识别和定位，有效地抑制非语义变化，并在复杂现实场景中达到最先进的准确度。

Conclusion: ViLaCD-R1框架在遥感变化检测领域具有重要的研究和应用价值，特别是在提升变化检测的鲁棒性和准确性方面表现出色。

Abstract: Remote sensing change detection (RSCD), a complex multi-image inference task, traditionally uses pixel-based operators or encoder-decoder networks that inadequately capture high-level semantics and are vulnerable to non-semantic perturbations. Although recent multimodal and vision-language model (VLM)-based approaches enhance semantic understanding of change regions by incorporating textual descriptions, they still suffer from challenges such as inaccurate spatial localization, imprecise pixel-level boundary delineation, and limited interpretability. To address these issues, we propose ViLaCD-R1, a two-stage framework comprising a Multi-Image Reasoner (MIR) and a Mask-Guided Decoder (MGD). Specifically, the VLM is trained through supervised fine-tuning (SFT) and reinforcement learning (RL) on block-level dual-temporal inference tasks, taking dual-temporal image patches as input and outputting a coarse change mask. Then, the decoder integrates dual-temporal image features with this coarse mask to predict a precise binary change map. Comprehensive evaluations on multiple RSCD benchmarks demonstrate that ViLaCD-R1 substantially improves true semantic change recognition and localization, robustly suppresses non-semantic variations, and achieves state-of-the-art accuracy in complex real-world scenarios.

</details>


### [93] [Toward Stable Semi-Supervised Remote Sensing Segmentation via Co-Guidance and Co-Fusion](https://arxiv.org/abs/2512.23035)
*Yi Zhou,Xuechao Zou,Shun Zhang,Kai Li,Shiying Wang,Jingming Chen,Congyan Lang,Tengfei Cao,Pin Tao,Yuanchun Shi*

Main category: cs.CV

TL;DR: 本文提出了一种名为Co2S的稳定半监督遥感图像分割框架，融合了来自视觉-语言模型和自监督模型的先验知识，通过引入显式-隐式语义共导引机制和全局-局部特征协同融合策略，解决了伪标签漂移问题，实验表明该方法在多个常用数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的半监督遥感图像语义分割方法（如Semi-supervised RS图像分割）面临伪标签漂移的问题，导致模型的精度下降。为了解决这一问题，本文提出了Co2S框架。

Method: 该方法基于一种异质双学生架构，包含了两个不同的ViT基础视觉模型，分别初始化为预训练的CLIP和DINOv3。此外，引入了显式-隐式语义共导引机制和全局-局部特征协同融合策略，以有效利用来自视觉-语言模型和自监督模型的信息，从而提高语义一致性。

Result: 本文提出的Co2S方法在六个常用数据集上进行了实验，展示了其优越性，尤其是在不同分割协议和多种场景下，取得了领先的表现。

Conclusion: 本文提出了一种有效的半监督遥感图像分割方法，通过融合视觉-语言模型和自监督模型的先验知识，显著降低了伪标签漂移问题，提高了分割精度。

Abstract: Semi-supervised remote sensing (RS) image semantic segmentation offers a promising solution to alleviate the burden of exhaustive annotation, yet it fundamentally struggles with pseudo-label drift, a phenomenon where confirmation bias leads to the accumulation of errors during training. In this work, we propose Co2S, a stable semi-supervised RS segmentation framework that synergistically fuses priors from vision-language models and self-supervised models. Specifically, we construct a heterogeneous dual-student architecture comprising two distinct ViT-based vision foundation models initialized with pretrained CLIP and DINOv3 to mitigate error accumulation and pseudo-label drift. To effectively incorporate these distinct priors, an explicit-implicit semantic co-guidance mechanism is introduced that utilizes text embeddings and learnable queries to provide explicit and implicit class-level guidance, respectively, thereby jointly enhancing semantic consistency. Furthermore, a global-local feature collaborative fusion strategy is developed to effectively fuse the global contextual information captured by CLIP with the local details produced by DINOv3, enabling the model to generate highly precise segmentation results. Extensive experiments on six popular datasets demonstrate the superiority of the proposed method, which consistently achieves leading performance across various partition protocols and diverse scenarios. Project page is available at https://xavierjiezou.github.io/Co2S/.

</details>


### [94] [SoulX-LiveTalk Technical Report](https://arxiv.org/abs/2512.23379)
*Le Shen,Qiao Qian,Tan Yu,Ke Zhou,Tianhang Yu,Yu Zhan,Zhenjie Wang,Ming Tao,Shunshun Yin,Siyuan Liu*

Main category: cs.CV

TL;DR: SoulX-LiveTalk 是一种新型框架，通过引入双向注意力机制和多步回顾自校正机制，结合混合序列并行、平行VAE及内核级优化，实现了大规模扩散模型在实时光线追踪下的高保真实时流媒体生成，启动延迟仅为0.87秒，实时帧率为32帧。


<details>
  <summary>Details</summary>
Motivation: 为解决大规模扩散模型在实时生成高保真音频驱动的头像时面临的计算负荷与严格延迟限制之间的冲突，提出了一种新的双向注意力策略和多步回顾自校正机制，以提高运动连贯性和视觉细节，同时实现极低的启动延迟和高的帧率。

Method: 该方法采用了Self-correcting Bidirectional Distillation策略，通过保持视频片段内的双向注意力，确保了时空相关性的保留。此外，引入了Multi-step Retrospective Self-Correction Mechanism来自动恢复累计错误，增强了模型的稳定性。在优化方面，开发了一整套全栈推理加速工具，包括混合序列并行、平行VAE和内核级优化。

Result: SoulX-LiveTalk 实现了14B参数规模系统下的亚秒级启动延迟（0.87秒）和32 FPS的实时吞吐量，显著提升了高保真互动数字人类合成的标准。

Conclusion: SoulX-LiveTalk 通过创新机制和优化策略，成功提高了大规模扩散模型在实时生成高质量音频驱动头像时的性能，为该领域的研究提供了新的参考。

Abstract: Deploying massive diffusion models for real-time, infinite-duration, audio-driven avatar generation presents a significant engineering challenge, primarily due to the conflict between computational load and strict latency constraints. Existing approaches often compromise visual fidelity by enforcing strictly unidirectional attention mechanisms or reducing model capacity. To address this problem, we introduce \textbf{SoulX-LiveTalk}, a 14B-parameter framework optimized for high-fidelity real-time streaming. Diverging from conventional unidirectional paradigms, we use a \textbf{Self-correcting Bidirectional Distillation} strategy that retains bidirectional attention within video chunks. This design preserves critical spatiotemporal correlations, significantly enhancing motion coherence and visual detail. To ensure stability during infinite generation, we incorporate a \textbf{Multi-step Retrospective Self-Correction Mechanism}, enabling the model to autonomously recover from accumulated errors and preventing collapse. Furthermore, we engineered a full-stack inference acceleration suite incorporating hybrid sequence parallelism, Parallel VAE, and kernel-level optimizations. Extensive evaluations confirm that SoulX-LiveTalk is the first 14B-scale system to achieve a \textbf{sub-second start-up latency (0.87s)} while reaching a real-time throughput of \textbf{32 FPS}, setting a new standard for high-fidelity interactive digital human synthesis.

</details>


### [95] [Fuzzy-Logic and Deep Learning for Environmental Condition-Aware Road Surface Classification](https://arxiv.org/abs/2512.23436)
*Mustafa Demetgul,Sanja Lazarova Molnar*

Main category: cs.CV

TL;DR: 该研究提出了一种基于 weather 条件数据和路面状态数据的实时监控系统。利用移动电话摄像头在卡尔斯鲁厄理工学院周边的道路上收集数据，采用了多种基于图像的深度学习算法进行道路分类，并提出利用模糊逻辑对不同天气和时间下的道路表面进行分类。


<details>
  <summary>Details</summary>
Motivation: 传统道路监测方法成本高昂且缺乏系统性，而本文提出的基于图像的实时系统能够更快速、高效地获取道路状态信息，对于车辆管理和主动驾驶系统规划具有重要意义。

Method: 研究采用移动电话摄像头采集道路图像数据，同时结合路感加速度数据。在收集到的数据基础上，测试了多种基于图像的深度学习算法（如 Alexnet、LeNet、VGG、Resnet），并通过模糊逻辑对不同天气及时间段下的道路进行分类。

Result: 研究通过图像数据分类实现了超过95%的路面状态识别准确率，表明基于图像的方法在道路识别方面具有显著优势。同时，使用模糊逻辑分类方法可以有效地根据不同天气条件和时间段识别道路类型。

Conclusion: 该研究有效构建了实时路面监测系统，突破了传统路面监测方法的限制，并证明了基于图像和模糊逻辑的方法在道路识别和分类方面的高效性。

Abstract: Monitoring states of road surfaces provides valuable information for the planning and controlling vehicles and active vehicle control systems. Classical road monitoring methods are expensive and unsystematic because they require time for measurements. This article proposes an real time system based on weather conditional data and road surface condition data. For this purpose, we collected data with a mobile phone camera on the roads around the campus of the Karlsruhe Institute of Technology. We tested a large number of different image-based deep learning algorithms for road classification. In addition, we used road acceleration data along with road image data for training by using them as images. We compared the performances of acceleration-based and camera image-based approaches. The performances of the simple Alexnet, LeNet, VGG, and Resnet algorithms were compared as deep learning algorithms. For road condition classification, 5 classes were considered: asphalt, damaged asphalt, gravel road, damaged gravel road, pavement road and over 95% accuracy performance was achieved. It is also proposed to use the acceleration or the camera image to classify the road surface according to the weather and the time of day using fuzzy logic.

</details>


### [96] [CoFi-Dec: Hallucination-Resistant Decoding via Coarse-to-Fine Generative Feedback in Large Vision-Language Models](https://arxiv.org/abs/2512.23453)
*Zongsheng Cao,Yangfan He,Anran Liu,Jun Xie,Feng Chen,Zepeng Wang*

Main category: cs.CV

TL;DR: CoFi-Dec 是一种无需训练的解码框架，通过结合生成式的自我反馈和从粗糙到精细的视觉条件，减轻大规模多模态模型的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 减轻多模态模型在生成过程中出现的不可信内容（如幻觉）问题，以提高其实用性。

Method: 通过生成自反馈和多级视觉条件生成中间文本响应，再利用文本到图像模型生成合成图像，使用Wasserstein融合机制对不同视觉条件下的预测分布进行对齐。

Result: CoFi-Dec 在六个幻觉关注基准测试上表现出色，大幅减少了实体级和语义级的幻觉，优于现有解码策略。

Conclusion: CoFi-Dec 提供了一种无需额外训练的解决方案，能够广泛应用于各种大视觉语言模型，增强它们的可靠性和准确性。

Abstract: Large Vision-Language Models (LVLMs) have achieved impressive progress in multi-modal understanding and generation. However, they still tend to produce hallucinated content that is inconsistent with the visual input, which limits their reliability in real-world applications. We propose \textbf{CoFi-Dec}, a training-free decoding framework that mitigates hallucinations by integrating generative self-feedback with coarse-to-fine visual conditioning. Inspired by the human visual process from global scene perception to detailed inspection, CoFi-Dec first generates two intermediate textual responses conditioned on coarse- and fine-grained views of the original image. These responses are then transformed into synthetic images using a text-to-image model, forming multi-level visual hypotheses that enrich grounding cues. To unify the predictions from these multiple visual conditions, we introduce a Wasserstein-based fusion mechanism that aligns their predictive distributions into a geometrically consistent decoding trajectory. This principled fusion reconciles high-level semantic consistency with fine-grained visual grounding, leading to more robust and faithful outputs. Extensive experiments on six hallucination-focused benchmarks show that CoFi-Dec substantially reduces both entity-level and semantic-level hallucinations, outperforming existing decoding strategies. The framework is model-agnostic, requires no additional training, and can be seamlessly applied to a wide range of LVLMs. The implementation is available at https://github.com/AI-Researcher-Team/CoFi-Dec.

</details>


### [97] [GeoTeacher: Geometry-Guided Semi-Supervised 3D Object Detection](https://arxiv.org/abs/2512.23147)
*Jingyu Li,Xiaolong Zhao,Zhe Liu,Wenxiao Wu,Li Zhang*

Main category: cs.CV

TL;DR: GeoTeacher 提出了一种关键点基的几何关系监督模块，以及一种顾及距离衰减的体素级数据增强策略，用于提升学生模型在有限标注数据下的几何信息捕捉能力。


<details>
  <summary>Details</summary>
Motivation: 当前的半监督3D物体检测方法往往忽视了模型在有限标注数据下，对物体几何信息的低敏感度，这影响了学生模型在物体感知和定位上的能力。

Method: GeoTeacher 方法设计了一个基于关键点的几何关系监督模块，通过 Teacher 模型的知识迁移来增强学生模型的几何关系理解能力。同时，引入了考虑距离衰减的体素级数据增强策略来增加物体几何结构的多样性。

Result: 在 ONCE 和 Waymo 数据集上的实验表明，GeoTeacher 能够有效提升半监督3D物体检测的性能，并达到了新的 SOTA 结果。

Conclusion: GeoTeacher 提出的几何关系监督模块和数据增强策略在半监督3D物体检测中表现出显著效果，证明了其在现实应用中的良好泛化能力。

Abstract: Semi-supervised 3D object detection, aiming to explore unlabeled data for boosting 3D object detectors, has emerged as an active research area in recent years. Some previous methods have shown substantial improvements by either employing heterogeneous teacher models to provide high-quality pseudo labels or enforcing feature-perspective consistency between the teacher and student networks. However, these methods overlook the fact that the model usually tends to exhibit low sensitivity to object geometries with limited labeled data, making it difficult to capture geometric information, which is crucial for enhancing the student model's ability in object perception and localization. In this paper, we propose GeoTeacher to enhance the student model's ability to capture geometric relations of objects with limited training data, especially unlabeled data. We design a keypoint-based geometric relation supervision module that transfers the teacher model's knowledge of object geometry to the student, thereby improving the student's capability in understanding geometric relations. Furthermore, we introduce a voxel-wise data augmentation strategy that increases the diversity of object geometries, thereby further improving the student model's ability to comprehend geometric structures. To preserve the integrity of distant objects during augmentation, we incorporate a distance-decay mechanism into this strategy. Moreover, GeoTeacher can be combined with different SS3D methods to further improve their performance. Extensive experiments on the ONCE and Waymo datasets indicate the effectiveness and generalization of our method and we achieve the new state-of-the-art results. Code will be available at https://github.com/SII-Whaleice/GeoTeacher

</details>


### [98] [HY-Motion 1.0: Scaling Flow Matching Models for Text-To-Motion Generation](https://arxiv.org/abs/2512.23464)
*Yuxin Wen,Qing Shuai,Di Kang,Jing Li,Cheng Wen,Yue Qian,Ningxin Jiao,Changhai Chen,Weijie Chen,Yiran Wang,Jinkun Guo,Dongyue An,Han Liu,Yanyu Tong,Chao Zhang,Qing Guo,Juan Chen,Qiao Zhang,Youyi Zhang,Zihao Yao,Cheng Zhang,Hong Duan,Xiaoping Wu,Qi Chen,Fei Cheng,Liang Dong,Peng He,Hao Zhang,Jiaxin Lin,Chao Zhang,Zhongyi Fan,Yifan Li,Zhichao Hu,Yuhong Liu,Linus,Jie Jiang,Xiaolong Li,Linchao Bao*

Main category: cs.CV

TL;DR: HY-Motion 1.0 是一个大型运动生成模型系列，能够根据文本描述生成3D人类运动，通过大规模预训练、高精度微调和强化学习实现精确指令跟随能力，并涵盖200多种运动类别。


<details>
  <summary>Details</summary>
Motivation: 开放源代码社区目前缺乏能够出色指令跟随的大型运动生成模型，HY-Motion 1.0 尝试通过引入全面的全阶段训练范式来填补这一空白。

Method: HY-Motion 1.0 采用扩散转换器（DiT）为基础的流动匹配模型，大规模预训练超过3000小时的运动数据，高质微调400小时的精选数据，并结合人类反馈和奖励模型进行强化学习。

Result: HY-Motion 1.0 实现了比当前开源基准显著更高的指令跟随能力，覆盖200多种运动类别，包括细腻的运动清理和标注。

Conclusion: 该模型框架通过严格的运动清理和标注流程，以及全面的全阶段训练，为3D人类运动生成模型向商业成熟迈进奠定了基础，并向开放社区开放源代码以促进未来研究。

Abstract: We present HY-Motion 1.0, a series of state-of-the-art, large-scale, motion generation models capable of generating 3D human motions from textual descriptions. HY-Motion 1.0 represents the first successful attempt to scale up Diffusion Transformer (DiT)-based flow matching models to the billion-parameter scale within the motion generation domain, delivering instruction-following capabilities that significantly outperform current open-source benchmarks. Uniquely, we introduce a comprehensive, full-stage training paradigm -- including large-scale pretraining on over 3,000 hours of motion data, high-quality fine-tuning on 400 hours of curated data, and reinforcement learning from both human feedback and reward models -- to ensure precise alignment with the text instruction and high motion quality. This framework is supported by our meticulous data processing pipeline, which performs rigorous motion cleaning and captioning. Consequently, our model achieves the most extensive coverage, spanning over 200 motion categories across 6 major classes. We release HY-Motion 1.0 to the open-source community to foster future research and accelerate the transition of 3D human motion generation models towards commercial maturity.

</details>


### [99] [REVEALER: Reinforcement-Guided Visual Reasoning for Element-Level Text-Image Alignment Evaluation](https://arxiv.org/abs/2512.23169)
*Fulin Shi,Wenyi Xiao,Bin Chen,Liang Din,Leilei Gan*

Main category: cs.CV

TL;DR: REVEALER 提出了一种基于强化学习引导的可视化推理统一框架，用于文本到图像生成模型的元素级对齐评估。


<details>
  <summary>Details</summary>
Motivation: 现有的评估方法主要依赖于粗粒度的指标或静态QA管道，缺乏细粒度的可解释性，无法充分反映人类偏好。REVEALER 设计的目的是为了弥补这一缺陷。

Method: REVEALER 采用了一个结构化的“grounding-reasoning-conclusion”范式，利用组相对策略优化(GRPO)方法，并结合结构格式、定位准确性和对齐 fidelity 的复合奖励函数进行模型优化。

Result: REVEALER 在四个基准测试（EvalMuse-40K, RichHF, MHaluBench, GenAI-Bench）上均取得了最先进的性能，且在推断效率方面优于现有的迭代可视化推理方法。

Conclusion: REVEALER 能够使多模态大语言模型明确地定位语义元素并得出可解释的对齐判断，且在多个基准测试中表现出色。

Abstract: Evaluating the alignment between textual prompts and generated images is critical for ensuring the reliability and usability of text-to-image (T2I) models. However, most existing evaluation methods rely on coarse-grained metrics or static QA pipelines, which lack fine-grained interpretability and struggle to reflect human preferences. To address this, we propose REVEALER, a unified framework for element-level alignment evaluation based on reinforcement-guided visual reasoning. Adopting a structured "grounding-reasoning-conclusion" paradigm, our method enables Multimodal Large Language Models (MLLMs) to explicitly localize semantic elements and derive interpretable alignment judgments. We optimize the model via Group Relative Policy Optimization(GRPO) using a composite reward function that incorporates structural format, grounding accuracy, and alignment fidelity. Extensive experiments across four benchmarks-EvalMuse-40K, RichHF, MHaluBench, and GenAI-Bench-demonstrate that REVEALER achieves state-of-the-art performance. Our approach consistently outperforms both strong proprietary models and supervised baselines while demonstrating superior inference efficiency compared to existing iterative visual reasoning methods.

</details>


### [100] [GVSynergy-Det: Synergistic Gaussian-Voxel Representations for Multi-View 3D Object Detection](https://arxiv.org/abs/2512.23176)
*Yi Zhang,Yi Wang,Lei Yao,Lap-Pui Chau*

Main category: cs.CV

TL;DR: GVSynergy-Det通过结合连续高斯表示和离散体素表示，提高了基于图像的3D检测精度，无需依赖密集的3D监督和深度传感器，展现出了优越的室内场景检测性能。


<details>
  <summary>Details</summary>
Motivation: 通过将连续的高斯表示和离散的体素表示结合起来，解决传统基于图像的3D检测方法所面临的高精度需要密集3D监督的挑战。

Method: 引入了一种双重表示架构，将通用化的高斯斑点技术应用于提取补充几何特征，并开发了一种跨表示增强机制，这种机制通过学习集成将几何细节从高斯场注入体素特征中。

Result: 该方法在多个室内场景基准（ScanNetV2和ARKitScenes）中达到了最先进的结果，表现出显著优于现有方法的性能，并且不需要任何深度或密集3D几何监督。

Conclusion: GVSynergy-Det提出了一种新颖的框架，通过联合学习高斯和体素表示，提高了3D目标检测的准确性，展示了在实际应用中的潜力。

Abstract: Image-based 3D object detection aims to identify and localize objects in 3D space using only RGB images, eliminating the need for expensive depth sensors required by point cloud-based methods. Existing image-based approaches face two critical challenges: methods achieving high accuracy typically require dense 3D supervision, while those operating without such supervision struggle to extract accurate geometry from images alone. In this paper, we present GVSynergy-Det, a novel framework that enhances 3D detection through synergistic Gaussian-Voxel representation learning. Our key insight is that continuous Gaussian and discrete voxel representations capture complementary geometric information: Gaussians excel at modeling fine-grained surface details while voxels provide structured spatial context. We introduce a dual-representation architecture that: 1) adapts generalizable Gaussian Splatting to extract complementary geometric features for detection tasks, and 2) develops a cross-representation enhancement mechanism that enriches voxel features with geometric details from Gaussian fields. Unlike previous methods that either rely on time-consuming per-scene optimization or utilize Gaussian representations solely for depth regularization, our synergistic strategy directly leverages features from both representations through learnable integration, enabling more accurate object localization. Extensive experiments demonstrate that GVSynergy-Det achieves state-of-the-art results on challenging indoor benchmarks, significantly outperforming existing methods on both ScanNetV2 and ARKitScenes datasets, all without requiring any depth or dense 3D geometry supervision (e.g., point clouds or TSDF).

</details>


### [101] [GaussianDWM: 3D Gaussian Driving World Model for Unified Scene Understanding and Multi-Modal Generation](https://arxiv.org/abs/2512.23180)
*Tianchen Deng,Xuefeng Chen,Yi Chen,Qu Chen,Yuyao Xu,Lijin Yang,Le Xu,Yu Zhang,Bo Zhang,Wuxiong Huang,Hesheng Wang*

Main category: cs.CV

TL;DR: 该研究提出了一种基于3D高斯场景表示的新型统一世界模型框架，能够同时进行3D场景理解和多模态场景生成，并能增强上下文理解与生成任务。该方法通过将丰富的语言特征嵌入到每个高斯原语中，实现了早态模态对齐，设计了一种任务感知的语言导向采样策略，并构建了一个多模态生成模型。


<details>
  <summary>Details</summary>
Motivation: 现有的驾驶世界模型在3D场景理解能力上存在局限，且无法准确地将文本等语言信息与底层3D场景对齐。本文旨在通过引入基于3D高斯场景表示的统一框架，增强模型的场景理解和生成能力。

Method: 通过将语言特征嵌入3D高斯场景表示，设计了一种新的任务感知的语言导向采样策略，构建了多模态生成模型。

Result: 实验在nuScenes、NuInteract数据集上验证了该方法的有效性，实现了当前最好的性能。

Conclusion: 该研究提出的方法在3D场景理解和多模态场景生成方面表现出色。

Abstract: Driving World Models (DWMs) have been developing rapidly with the advances of generative models. However, existing DWMs lack 3D scene understanding capabilities and can only generate content conditioned on input data, without the ability to interpret or reason about the driving environment. Moreover, current approaches represent 3D spatial information with point cloud or BEV features do not accurately align textual information with the underlying 3D scene. To address these limitations, we propose a novel unified DWM framework based on 3D Gaussian scene representation, which enables both 3D scene understanding and multi-modal scene generation, while also enabling contextual enrichment for understanding and generation tasks. Our approach directly aligns textual information with the 3D scene by embedding rich linguistic features into each Gaussian primitive, thereby achieving early modality alignment. In addition, we design a novel task-aware language-guided sampling strategy that removes redundant 3D Gaussians and injects accurate and compact 3D tokens into LLM. Furthermore, we design a dual-condition multi-modal generation model, where the information captured by our vision-language model is leveraged as a high-level language condition in combination with a low-level image condition, jointly guiding the multi-modal generation process. We conduct comprehensive studies on the nuScenes, and NuInteract datasets to validate the effectiveness of our framework. Our method achieves state-of-the-art performance. We will release the code publicly on GitHub https://github.com/dtc111111/GaussianDWM.

</details>


### [102] [AnyMS: Bottom-up Attention Decoupling for Layout-guided and Training-free Multi-subject Customization](https://arxiv.org/abs/2512.23537)
*Binhe Yu,Zhen Wang,Kexin Li,Yuqian Yuan,Wenqiao Zhang,Long Chen,Juncheng Li,Jun Xiao,Yueting Zhuang*

Main category: cs.CV

TL;DR: AnyMS 提供了一种无需额外训练的布局指导多主体定制框架，通过底上双级注意力脱耦机制确保文本对齐、主体身份保留和布局控制。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理多主体定制中的文本对齐、主体身份保留和布局控制这三个关键目标方面存在困难，并且依赖额外训练限制了其扩展性和效率。

Method: AnyMS 引入了底上双级注意力脱耦机制，通过全局脱耦确保文本对齐，通过局部脱耦防止主体之间的冲突，保证了主体的身份保留和布局控制。此外，它使用预训练图像适配器来提取与扩散模型相兼容的主体特异性特征。

Result: 实验结果表明，AnyMS 达到了最先进的技术水平，支持复杂组合并能扩展到更多的主体。

Conclusion: AnyMS 提供了一种有效的解决方案来改善多主体定制中的关键问题，并且具有高效的性能和扩展性。

Abstract: Multi-subject customization aims to synthesize multiple user-specified subjects into a coherent image. To address issues such as subjects missing or conflicts, recent works incorporate layout guidance to provide explicit spatial constraints. However, existing methods still struggle to balance three critical objectives: text alignment, subject identity preservation, and layout control, while the reliance on additional training further limits their scalability and efficiency. In this paper, we present AnyMS, a novel training-free framework for layout-guided multi-subject customization. AnyMS leverages three input conditions: text prompt, subject images, and layout constraints, and introduces a bottom-up dual-level attention decoupling mechanism to harmonize their integration during generation. Specifically, global decoupling separates cross-attention between textual and visual conditions to ensure text alignment. Local decoupling confines each subject's attention to its designated area, which prevents subject conflicts and thus guarantees identity preservation and layout control. Moreover, AnyMS employs pre-trained image adapters to extract subject-specific features aligned with the diffusion model, removing the need for subject learning or adapter tuning. Extensive experiments demonstrate that AnyMS achieves state-of-the-art performance, supporting complex compositions and scaling to a larger number of subjects.

</details>


### [103] [PathFound: An Agentic Multimodal Model Activating Evidence-seeking Pathological Diagnosis](https://arxiv.org/abs/2512.23545)
*Shengyi Hua,Jianfeng Wu,Tianle Shen,Kangzhe Hu,Zhongzhen Huang,Shujuan Ni,Zhihong Zhang,Yuan Li,Zhe Wang,Xiaofan Zhang*

Main category: cs.CV

TL;DR: PathFound 是一种多模态模型，通过整合病理性视觉基础模型、视觉-语言模型和基于强化学习的推理模型，实现积极的信息获取和诊断精炼，促进证据寻求推理过程。


<details>
  <summary>Details</summary>
Motivation: 当前大多数病理模型采用静态推理范式，缺乏对不明确诊断的重新评估和证据收集，与临床诊断工作流程不符。PathFound 模型旨在填补这一空白，通过证据寻求推理优化病理解剖诊断。

Method: PathFound 结合了病理性视觉基础模型、视觉-语言模型和基于强化学习的推理模型。该模型分为三阶段：初始诊断、证据寻求和最终决定，通过积极的信息获取来改进诊断准确性。

Result: 在多个大型多模态模型中，PathFound 策略的一致性改进表明在计算病理中证据寻求流程的有效性，并在多种临床场景中实现了最先进的诊断性能。

Conclusion: PathFound 在病理解剖诊断中具有显著优势，能够发现细微的特征，如核特征和局部侵袭，并且在多模态模型中显示出良好的诊断性能。

Abstract: Recent pathological foundation models have substantially advanced visual representation learning and multimodal interaction. However, most models still rely on a static inference paradigm in which whole-slide images are processed once to produce predictions, without reassessment or targeted evidence acquisition under ambiguous diagnoses. This contrasts with clinical diagnostic workflows that refine hypotheses through repeated slide observations and further examination requests. We propose PathFound, an agentic multimodal model designed to support evidence-seeking inference in pathological diagnosis. PathFound integrates the power of pathological visual foundation models, vision-language models, and reasoning models trained with reinforcement learning to perform proactive information acquisition and diagnosis refinement by progressing through the initial diagnosis, evidence-seeking, and final decision stages. Across several large multimodal models, adopting this strategy consistently improves diagnostic accuracy, indicating the effectiveness of evidence-seeking workflows in computational pathology. Among these models, PathFound achieves state-of-the-art diagnostic performance across diverse clinical scenarios and demonstrates strong potential to discover subtle details, such as nuclear features and local invasions.

</details>


### [104] [Task-oriented Learnable Diffusion Timesteps for Universal Few-shot Learning of Dense Tasks](https://arxiv.org/abs/2512.23210)
*Changgyoon Oh,Jongoh Jeong,Jegyeong Cho,Kuk-Jin Yoon*

Main category: cs.CV

TL;DR: 提出了Task-aware Timestep Selection (TTS) 和 Timestep Feature Consolidation (TFC) 两个模块，通过自适应选择最优的扩散时间步，提高密集预测任务的性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于扩散模型的应用在任务特定解码器的引导下利用多步正向-反向马尔可夫过程学习到的视觉表示，但扩散时间步的选择仍依赖于经验直觉，偏向某些任务。本研究旨在探索更具适应性的扩散时间步特征，以改善零样本密集预测任务的性能。

Method: 引入了两个模块：1) Task-aware Timestep Selection (TTS)，基于时间步损失和相似性分数选择理想的时间步；2) Timestep Feature Consolidation (TFC)，整合选择的时间步特征，以提高零样本设置下的密集预测性能。此外，还提出了一种参数效率高的微调适配器。

Result: 在大型具有挑战性的Taskonomy数据集上，结合参数高效的微调适配器，该框架仅使用少量的支持查询，便在密集预测任务中获得了优越的性能。

Conclusion: 本研究表明，通过自适应选择最适合的任务相关时间步以及整合这些时间步特征，可以获得更好的零样本密集预测效果，为未来的研究提供了新的改进方向。

Abstract: Denoising diffusion probabilistic models have brought tremendous advances in generative tasks, achieving state-of-the-art performance thus far. Current diffusion model-based applications exploit the power of learned visual representations from multistep forward-backward Markovian processes for single-task prediction tasks by attaching a task-specific decoder. However, the heuristic selection of diffusion timestep features still heavily relies on empirical intuition, often leading to sub-optimal performance biased towards certain tasks. To alleviate this constraint, we investigate the significance of versatile diffusion timestep features by adaptively selecting timesteps best suited for the few-shot dense prediction task, evaluated on an arbitrary unseen task. To this end, we propose two modules: Task-aware Timestep Selection (TTS) to select ideal diffusion timesteps based on timestep-wise losses and similarity scores, and Timestep Feature Consolidation (TFC) to consolidate the selected timestep features to improve the dense predictive performance in a few-shot setting. Accompanied by our parameter-efficient fine-tuning adapter, our framework effectively achieves superiority in dense prediction performance given only a few support queries. We empirically validate our learnable timestep consolidation method on the large-scale challenging Taskonomy dataset for dense prediction, particularly for practical universal and few-shot learning scenarios.

</details>


### [105] [RxnBench: A Multimodal Benchmark for Evaluating Large Language Models on Chemical Reaction Understanding from Scientific Literature](https://arxiv.org/abs/2512.23565)
*Hanzheng Li,Xi Fang,Yixuan Li,Chaozheng Huang,Junjie Wang,Xi Wang,Hongzhe Bai,Bojun Hao,Shenyu Lin,Huiqi Liang,Linfeng Zhang,Guolin Ke*

Main category: cs.CV

TL;DR: RxnBench 是一个多层次基准，用于评估 MLLMs 的化学反应理解能力。该基准包括针对视觉感知和机制推理的单图问答任务，以及需要跨模态整合的全文档问答任务。研究发现，尽管模型在提取显式文本方面表现出色，但在深奥的化学逻辑和精确结构识别方面存在能力差距。


<details>
  <summary>Details</summary>
Motivation: 当前的 MLLMs 虽然能够在文献中提取文字信息，但在理解和处理复杂的化学反应图示以及从中推理出所需的化学信息方面存在困难。RxnBench 的提出旨在弥补这一不足，促进 MLLMs 在化学领域的应用和发展。

Method: RxnBench 设计了两个主要的问答任务，即单图问答（SF-QA）和全文档问答（FD-QA），以测试 MLLMs 的化学反应理解能力。通过使用从305个具体化学反应方案中生成的1,525个问题和涵盖108篇出版物的复杂文献内容，该基准从视觉感知和跨模态整合等多个方面评估了模型的表现。

Result: 研究结果显示，尽管标准架构的模型在提取显式信息方面表现良好，但在深奥的化学逻辑推理和精确结构识别方面却存在明显不足。而通过推理时间内的增强推理能力，某些模型的表现有了显著提升，但对于 FD-QA 任务，没有模型能达到50%的准确率。

Conclusion: 研究呼吁开发针对化学领域的专有视觉编码器和更为强大的推理引擎，以更好地推动自主 AI 化学家的发展。

Abstract: The integration of Multimodal Large Language Models (MLLMs) into chemistry promises to revolutionize scientific discovery, yet their ability to comprehend the dense, graphical language of reactions within authentic literature remains underexplored. Here, we introduce RxnBench, a multi-tiered benchmark designed to rigorously evaluate MLLMs on chemical reaction understanding from scientific PDFs. RxnBench comprises two tasks: Single-Figure QA (SF-QA), which tests fine-grained visual perception and mechanistic reasoning using 1,525 questions derived from 305 curated reaction schemes, and Full-Document QA (FD-QA), which challenges models to synthesize information from 108 articles, requiring cross-modal integration of text, schemes, and tables. Our evaluation of MLLMs reveals a critical capability gap: while models excel at extracting explicit text, they struggle with deep chemical logic and precise structural recognition. Notably, models with inference-time reasoning significantly outperform standard architectures, yet none achieve 50\% accuracy on FD-QA. These findings underscore the urgent need for domain-specific visual encoders and stronger reasoning engines to advance autonomous AI chemists.

</details>


### [106] [MM-UAVBench: How Well Do Multimodal Large Language Models See, Think, and Plan in Low-Altitude UAV Scenarios?](https://arxiv.org/abs/2512.23219)
*Shiqi Dai,Zizhi Ma,Zhicong Luo,Xuesong Yang,Yibin Huang,Wanyue Zhang,Chi Chen,Zonghao Guo,Wang Xu,Yufei Sun,Maosong Sun*

Main category: cs.CV

TL;DR: 本文介绍了MM-UAVBench，这是一个全面评估多模态大语言模型在低空无人机场景下感知、认知和规划能力的基准。该基准涵盖了19个子任务，通过大量真实无人机数据的手动标注问题进行了实验，揭示了当前模型适应低空场景复杂视觉和认知需求的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基准较少涵盖低空无人机场景的独特挑战，MM-UAVBench旨在填补这一空白，通过统一评估多模态大语言模型在感知、认知和规划等核心能力方面的能力，促进这一领域的发展。

Method: MM-UAVBench 包含 19 个子任务，并通过手工标注的问题进行了全面实验，这些问题基于公共数据集收集的真实无人机数据，涵盖了感知、认知和规划等三个核心能力维度。

Result: 实验结果显示现有的多模态大语言模型在应对低空无人机场景下的复杂视觉和认知需求上表现不佳，揭示了诸如空间偏差和多视角理解等关键瓶颈。

Conclusion: 该基准希望通过展示当前模型的瓶颈，促进对未来适用于真实无人机环境的大规模语言模型的研究与发展，以实现更可靠和强大的无人机智能。

Abstract: While Multimodal Large Language Models (MLLMs) have exhibited remarkable general intelligence across diverse domains, their potential in low-altitude applications dominated by Unmanned Aerial Vehicles (UAVs) remains largely underexplored. Existing MLLM benchmarks rarely cover the unique challenges of low-altitude scenarios, while UAV-related evaluations mainly focus on specific tasks such as localization or navigation, without a unified evaluation of MLLMs'general intelligence. To bridge this gap, we present MM-UAVBench, a comprehensive benchmark that systematically evaluates MLLMs across three core capability dimensions-perception, cognition, and planning-in low-altitude UAV scenarios. MM-UAVBench comprises 19 sub-tasks with over 5.7K manually annotated questions, all derived from real-world UAV data collected from public datasets. Extensive experiments on 16 open-source and proprietary MLLMs reveal that current models struggle to adapt to the complex visual and cognitive demands of low-altitude scenarios. Our analyses further uncover critical bottlenecks such as spatial bias and multi-view understanding that hinder the effective deployment of MLLMs in UAV scenarios. We hope MM-UAVBench will foster future research on robust and reliable MLLMs for real-world UAV intelligence.

</details>


### [107] [Bridging Your Imagination with Audio-Video Generation via a Unified Director](https://arxiv.org/abs/2512.23222)
*Jiaxu Zhang,Tianshu Hu,Yuan Zhang,Zenan Li,Linjie Luo,Guosheng Lin,Xin Chen*

Main category: cs.CV

TL;DR: UniMAGE 是一种统一的导演模型，结合文本和图像生成，通过先交错学习概念，再分离专家学习，提升叙事逻辑和关键帧一致性，从而辅助非专家制作长场景多镜头影片。


<details>
  <summary>Details</summary>
Motivation: 现有AI驱动的视频创作系统将剧本撰写和关键镜头设计拆分为独立任务，UniMAGE 提出了统一框架的理念，即将这两个任务结合在一起，支持非专家通过现有音频视频生成模型创作长场景多镜头影片。

Method: UniMAGE 采用 Mixture-of-Transformers 架构结合文本和图像生成，通过 Interleaved Concept Learning 和 Disentangled Expert Learning 两种训练方式增强叙事逻辑和关键帧一致性。

Result: 实验证明，UniMAGE 在开源模型中达到最先进的性能，生成逻辑连贯的视频脚本和视觉一致的关键帧图像。

Conclusion: UniMAGE 提出了一种创新的方法来统一剧本撰写和关键帧设计，显著提升了视频创作的自动化和创造性，为非专业用户提供了一种新的创作途径。

Abstract: Existing AI-driven video creation systems typically treat script drafting and key-shot design as two disjoint tasks: the former relies on large language models, while the latter depends on image generation models. We argue that these two tasks should be unified within a single framework, as logical reasoning and imaginative thinking are both fundamental qualities of a film director. In this work, we propose UniMAGE, a unified director model that bridges user prompts with well-structured scripts, thereby empowering non-experts to produce long-context, multi-shot films by leveraging existing audio-video generation models. To achieve this, we employ the Mixture-of-Transformers architecture that unifies text and image generation. To further enhance narrative logic and keyframe consistency, we introduce a ``first interleaving, then disentangling'' training paradigm. Specifically, we first perform Interleaved Concept Learning, which utilizes interleaved text-image data to foster the model's deeper understanding and imaginative interpretation of scripts. We then conduct Disentangled Expert Learning, which decouples script writing from keyframe generation, enabling greater flexibility and creativity in storytelling. Extensive experiments demonstrate that UniMAGE achieves state-of-the-art performance among open-source models, generating logically coherent video scripts and visually consistent keyframe images.

</details>


### [108] [SURE Guided Posterior Sampling: Trajectory Correction for Diffusion-Based Inverse Problems](https://arxiv.org/abs/2512.23232)
*Minwoo Kim,Hongki Lim*

Main category: cs.CV

TL;DR: SGPS为了解决反问题中的迭代求解方法因累积误差导致的高质量重建需要大量步骤的问题，通过使用Stein's Unbiased Risk Estimate（SURE）梯度更新和PCA噪声估计来纠正采样轨迹偏差，从而减少错误累积，使得在少于100个Neural Function Evaluations（NFEs）的情况下也能保持高质量重建。


<details>
  <summary>Details</summary>
Motivation: 现有的迭代求解方法在解决反问题时需要大量步骤，且会因累积误差而影响重建质量。

Method: SGPS方法通过使用SURE梯度更新和PCA噪声估计来纠正采样轨迹偏差，从而减少错误累积，提高后验采样精度。

Result: SGPS能够在少于100个NFEs的情况下保持高质量重建，并在多种反问题中表现出色，优于现有方法。

Conclusion: SGPS为解决反问题中高效高质量重建提供了一种新的有效方法。

Abstract: Diffusion models have emerged as powerful learned priors for solving inverse problems. However, current iterative solving approaches which alternate between diffusion sampling and data consistency steps typically require hundreds or thousands of steps to achieve high quality reconstruction due to accumulated errors. We address this challenge with SURE Guided Posterior Sampling (SGPS), a method that corrects sampling trajectory deviations using Stein's Unbiased Risk Estimate (SURE) gradient updates and PCA based noise estimation. By mitigating noise induced errors during the critical early and middle sampling stages, SGPS enables more accurate posterior sampling and reduces error accumulation. This allows our method to maintain high reconstruction quality with fewer than 100 Neural Function Evaluations (NFEs). Our extensive evaluation across diverse inverse problems demonstrates that SGPS consistently outperforms existing methods at low NFE counts.

</details>


### [109] [RS-Prune: Training-Free Data Pruning at High Ratios for Efficient Remote Sensing Diffusion Foundation Models](https://arxiv.org/abs/2512.23239)
*Fan Wei,Runmin Dong,Yushan Lai,Yixiang Yang,Zhaoyang Luo,Jinxiao Zhang,Miao Yang,Shuai Yuan,Jiyao Zhao,Bin Luo,Haohuan Fu*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的两阶段数据精简方法，能够在高剪裁比下快速选择高质量子集，从而提高扩散模型的训练效率和生成质量，并在多种下游任务中取得领先性能。


<details>
  <summary>Details</summary>
Motivation: 当前的遥感（RS）扩散基础模型依赖于大量全球代表性数据，这些数据可能包含冗余、噪声和类别不平衡，这降低了训练效率并阻碍了模型收敛。本文旨在通过一种无需训练的两阶段数据精简方法解决这一问题。

Method: 本文提出的方法包括两阶段数据精简过程。首先，通过基于熵的标准高效去除低信息量的样本。其次，利用参考遥感场景分类数据集进行场景感知聚类，并采用分层抽样策略来提高聚类效果同时减少大规模未标记数据上的计算成本。最后，通过平衡聚类级别的一致性与样本代表性，在高剪裁比下实现精细的样本选择，同时保持整体多样性和代表性。

Result: 通过该方法在训练数据中进行85%的剪裁后，模型能够显著提高收敛速度和生成质量。与传统方法相比，该方法训练出的扩散基础模型在包括超分辨率和语义图像合成在内的多种下游任务中实现了最先进的性能。

Conclusion: 本文提出的数据精简方法为开发遥感生成基础模型提供了实际指导。

Abstract: Diffusion-based remote sensing (RS) generative foundation models are cruial for downstream tasks. However, these models rely on large amounts of globally representative data, which often contain redundancy, noise, and class imbalance, reducing training efficiency and preventing convergence. Existing RS diffusion foundation models typically aggregate multiple classification datasets or apply simplistic deduplication, overlooking the distributional requirements of generation modeling and the heterogeneity of RS imagery. To address these limitations, we propose a training-free, two-stage data pruning approach that quickly select a high-quality subset under high pruning ratios, enabling a preliminary foundation model to converge rapidly and serve as a versatile backbone for generation, downstream fine-tuning, and other applications. Our method jointly considers local information content with global scene-level diversity and representativeness. First, an entropy-based criterion efficiently removes low-information samples. Next, leveraging RS scene classification datasets as reference benchmarks, we perform scene-aware clustering with stratified sampling to improve clustering effectiveness while reducing computational costs on large-scale unlabeled data. Finally, by balancing cluster-level uniformity and sample representativeness, the method enables fine-grained selection under high pruning ratios while preserving overall diversity and representativeness. Experiments show that, even after pruning 85\% of the training data, our method significantly improves convergence and generation quality. Furthermore, diffusion foundation models trained with our method consistently achieve state-of-the-art performance across downstream tasks, including super-resolution and semantic image synthesis. This data pruning paradigm offers practical guidance for developing RS generative foundation models.

</details>


### [110] [Multimodal Interpretation of Remote Sensing Images: Dynamic Resolution Input Strategy and Multi-scale Vision-Language Alignment Mechanism](https://arxiv.org/abs/2512.23243)
*Siyu Zhang,Ying Chen,Lianlei Shan,Runhe Qiu*

Main category: cs.CV

TL;DR: 本文提出了一种结合动态分辨率输入策略(DRIS)和多尺度视觉语言对齐机制(MS-VLAM)的Vision-language模型(VLM)框架。该框架通过自适应分配计算资源来提高图像处理的细致度和效率，以及跨模态语义对齐的准确性。


<details>
  <summary>Details</summary>
Motivation: 针对现有方法的局限性，如固定分辨率难以平衡效率和细节、单尺度对齐缺乏语义层次等问题，提出了一种新的多模态遥感影像融合框架，旨在提高语义理解和计算效率。

Method: 该研究提出了一种整合了动态分辨率输入策略(DRIS)和多尺度视觉语言对齐机制(MS-VLAM)的Vision-language模型框架。DRIS采用由粗到细的方法，根据图像内容的复杂性自适应分配计算资源，保留关键的细粒度特征，减少冗余计算开销。MS-VLAM构建了一个包含对象、局部区域和全局三个层次的对齐机制，系统地捕捉跨模态语义一致性，缓解语义错配和粒度失衡问题。

Result: 在RS-GPT4V数据集上的实验结果表明，所提出的框架显著提高了语义理解和计算效率。相对于传统方法，在图像描述和跨模态检索任务中，BLEU-4和CIDEr评价指标上表现出更优的性能，R@10指标也有所提升。

Conclusion: 该技术框架提供了一种构建高效和稳健的多模态遥感系统的新型方法，为智能遥感解释的应用工程提供了一定的理论基础和技术指导。

Abstract: Multimodal fusion of remote sensing images serves as a core technology for overcoming the limitations of single-source data and improving the accuracy of surface information extraction, which exhibits significant application value in fields such as environmental monitoring and urban planning. To address the deficiencies of existing methods, including the failure of fixed resolutions to balance efficiency and detail, as well as the lack of semantic hierarchy in single-scale alignment, this study proposes a Vision-language Model (VLM) framework integrated with two key innovations: the Dynamic Resolution Input Strategy (DRIS) and the Multi-scale Vision-language Alignment Mechanism (MS-VLAM).Specifically, the DRIS adopts a coarse-to-fine approach to adaptively allocate computational resources according to the complexity of image content, thereby preserving key fine-grained features while reducing redundant computational overhead. The MS-VLAM constructs a three-tier alignment mechanism covering object, local-region and global levels, which systematically captures cross-modal semantic consistency and alleviates issues of semantic misalignment and granularity imbalance.Experimental results on the RS-GPT4V dataset demonstrate that the proposed framework significantly improves the accuracy of semantic understanding and computational efficiency in tasks including image captioning and cross-modal retrieval. Compared with conventional methods, it achieves superior performance in evaluation metrics such as BLEU-4 and CIDEr for image captioning, as well as R@10 for cross-modal retrieval. This technical framework provides a novel approach for constructing efficient and robust multimodal remote sensing systems, laying a theoretical foundation and offering technical guidance for the engineering application of intelligent remote sensing interpretation.

</details>


### [111] [ASemConsist: Adaptive Semantic Feature Control for Training-Free Identity-Consistent Generation](https://arxiv.org/abs/2512.23245)
*Shin seong Kim,Minjung Shin,Hyunin Cho,Youngjung Uh*

Main category: cs.CV

TL;DR: 本文提出了一个名为ASemconsist的新框架，通过选择性修改文本嵌入来实现对角色身份的显式语义控制，同时不牺牲图像提示的一致性。该框架还包括一个自适应特征共享策略和一种统一的评估协议，称为一致性质量评分(CQS)。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像生成方法在保持角色身份一致性的同时，很难保证每个图像提示的一致性，存在贸易off。本文旨在解决这个问题。

Method: 本文提出了一种新颖的方法，ASemconsist，它通过选择性修改文本嵌入来实现显式的语义控制，以便在不牺牲图像提示一致性的情况下保护角色的身份。

Result: 该方法在保持角色身份一致性的同时，能够保证图像提示的一致性。并通过自适应特征共享策略，对该方法进行了优化。还提出了一种统一的评估协议，称为一致性质量评分(CQS)，该协议将角色身份保护与图像提示一致性集成到单一综合性指标中。

Conclusion: 该框架在实验中取得了卓越的性能，成功解决了早期方法中的贸易off问题，确保了角色身份的一致性以及对文本指令的响应一致性。

Abstract: Recent text-to-image diffusion models have significantly improved visual quality and text alignment. However, generating a sequence of images while preserving consistent character identity across diverse scene descriptions remains a challenging task. Existing methods often struggle with a trade-off between maintaining identity consistency and ensuring per-image prompt alignment. In this paper, we introduce a novel framework, ASemconsist, that addresses this challenge through selective text embedding modification, enabling explicit semantic control over character identity without sacrificing prompt alignment. Furthermore, based on our analysis of padding embeddings in FLUX, we propose a semantic control strategy that repurposes padding embeddings as semantic containers. Additionally, we introduce an adaptive feature-sharing strategy that automatically evaluates textual ambiguity and applies constraints only to the ambiguous identity prompt. Finally, we propose a unified evaluation protocol, the Consistency Quality Score (CQS), which integrates identity preservation and per-image text alignment into a single comprehensive metric, explicitly capturing performance imbalances between the two metrics. Our framework achieves state-of-the-art performance, effectively overcoming prior trade-offs. Project page: https://minjung-s.github.io/asemconsist

</details>


### [112] [Contour Information Aware 2D Gaussian Splatting for Image Representation](https://arxiv.org/abs/2512.23255)
*Masaya Takabe,Hiroshi Watanabe,Sujun Hong,Tomohiro Ikai,Zheming Fan,Ryo Ishimoto,Kakeru Sugimoto,Ruri Imichi*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Image representation is a fundamental task in computer vision. Recently, Gaussian Splatting has emerged as an efficient representation framework, and its extension to 2D image representation enables lightweight, yet expressive modeling of visual content. While recent 2D Gaussian Splatting (2DGS) approaches provide compact storage and real-time decoding, they often produce blurry or indistinct boundaries when the number of Gaussians is small due to the lack of contour awareness. In this work, we propose a Contour Information-Aware 2D Gaussian Splatting framework that incorporates object segmentation priors into Gaussian-based image representation. By constraining each Gaussian to a specific segmentation region during rasterization, our method prevents cross-boundary blending and preserves edge structures under high compression. We also introduce a warm-up scheme to stabilize training and improve convergence. Experiments on synthetic color charts and the DAVIS dataset demonstrate that our approach achieves higher reconstruction quality around object edges compared to existing 2DGS methods. The improvement is particularly evident in scenarios with very few Gaussians, while our method still maintains fast rendering and low memory usage.

</details>


### [113] [Plug-and-Play Fidelity Optimization for Diffusion Transformer Acceleration via Cumulative Error Minimization](https://arxiv.org/abs/2512.23258)
*Tong Shao,Yusen Fu,Guoying Sun,Jingde Kong,Zhuotao Tian,Jingyong Su*

Main category: cs.CV

TL;DR: 本文提出了一种名为CEM的新颖保真度优化插件，旨在通过累积误差最小化补偿现有加速方法中的固定缓存策略缺陷，从而大幅提高生成保真度。


<details>
  <summary>Details</summary>
Motivation: 当前的缓存基加速方法在不影响精度的情况下无法获得足够的速度提升，而现有的补救策略如剪枝或预测虽能修正错误但固定策略难以适应复杂错误变种；本文通过提出CEM插件来优化现有加速模型的误差补偿机制。

Method: CEM通过定义累积误差进行动态编程优化策略，无需额外计算开销，适用于任意缓存预算的加速度模型，同时能够无缝集成到现有的加速模型中。

Result: 在九个生成模型和量化方法上的实验表明，CEM显著提高了现有加速模型的生成保真度，并在FLUX.1-dev，PixArt-$α$，StableDiffusion1.5和Hunyuan上超越了原始生成性能。

Conclusion: 本文提出的CEM为加速扩散变换器模型的通用加速策略提供了一个有效的解决方案，具有模型无关性且具有广泛应用潜力。

Abstract: Although Diffusion Transformer (DiT) has emerged as a predominant architecture for image and video generation, its iterative denoising process results in slow inference, which hinders broader applicability and development. Caching-based methods achieve training-free acceleration, while suffering from considerable computational error. Existing methods typically incorporate error correction strategies such as pruning or prediction to mitigate it. However, their fixed caching strategy fails to adapt to the complex error variations during denoising, which limits the full potential of error correction. To tackle this challenge, we propose a novel fidelity-optimization plugin for existing error correction methods via cumulative error minimization, named CEM. CEM predefines the error to characterize the sensitivity of model to acceleration jointly influenced by timesteps and cache intervals. Guided by this prior, we formulate a dynamic programming algorithm with cumulative error approximation for strategy optimization, which achieves the caching error minimization, resulting in a substantial improvement in generation fidelity. CEM is model-agnostic and exhibits strong generalization, which is adaptable to arbitrary acceleration budgets. It can be seamlessly integrated into existing error correction frameworks and quantized models without introducing any additional computational overhead. Extensive experiments conducted on nine generation models and quantized methods across three tasks demonstrate that CEM significantly improves generation fidelity of existing acceleration models, and outperforms the original generation performance on FLUX.1-dev, PixArt-$α$, StableDiffusion1.5 and Hunyuan. The code will be made publicly available.

</details>


### [114] [Multi-Track Multimodal Learning on iMiGUE: Micro-Gesture and Emotion Recognition](https://arxiv.org/abs/2512.23291)
*Arman Martirosyan,Shahane Tigranyan,Maria Razzhivina,Artak Aslanyan,Nazgul Salikhova,Ilya Makarov,Andrey Savchenko,Aram Avetisyan*

Main category: cs.CV

TL;DR: 该研究提出两个多模态框架来解决微手势分类和基于行为的情绪预测问题。通过结合RGB和3D姿态表示，以及通过跨模态Token融合模块和瞬时融合模块来集成视频和骨骼嵌入，研究展示了其方法在基于行为的情绪预测任务中的鲁棒性能。


<details>
  <summary>Details</summary>
Motivation: 提出了两个多模态框架，旨在解决微手势分类和基于行为的情绪预测这两个高度挑战性的问题，尤其是在处理细粒度的人类行为时。

Method: 研究通过探索RGB和3D姿态表示之间的互补优势，以及使用MViTv2-S和2s-AGCN从视频和骨架中提取嵌入，实现了复杂的时空模式捕捉。进一步通过跨模态Token融合模块和瞬时融合模块来集成不同的模态信息，实现了对微手势和情绪状态的识别。

Result: 研究在iMiGUE数据集上的实验表明，其方法在基于行为的情绪预测任务中的性能和准确性都很强，并成功获得了第2名。

Conclusion: 研究展示了在处理细粒度的微手势和基于行为的情绪预测时，通过多模态融合的方法可以显著提高模型的鲁棒性和准确性。

Abstract: Micro-gesture recognition and behavior-based emotion prediction are both highly challenging tasks that require modeling subtle, fine-grained human behaviors, primarily leveraging video and skeletal pose data. In this work, we present two multimodal frameworks designed to tackle both problems on the iMiGUE dataset. For micro-gesture classification, we explore the complementary strengths of RGB and 3D pose-based representations to capture nuanced spatio-temporal patterns. To comprehensively represent gestures, video, and skeletal embeddings are extracted using MViTv2-S and 2s-AGCN, respectively. Then, they are integrated through a Cross-Modal Token Fusion module to combine spatial and pose information. For emotion recognition, our framework extends to behavior-based emotion prediction, a binary classification task identifying emotional states based on visual cues. We leverage facial and contextual embeddings extracted using SwinFace and MViTv2-S models and fuse them through an InterFusion module designed to capture emotional expressions and body gestures. Experiments conducted on the iMiGUE dataset, within the scope of the MiGA 2025 Challenge, demonstrate the robust performance and accuracy of our method in the behavior-based emotion prediction task, where our approach secured 2nd place.

</details>


### [115] [CME-CAD: Heterogeneous Collaborative Multi-Expert Reinforcement Learning for CAD Code Generation](https://arxiv.org/abs/2512.23333)
*Ke Niu,Haiyang Yu,Zhuofan Chen,Zhengtao Yao,Weitao Jia,Xiaodong Ge,Jingqun Tang,Benlei Cui,Bin Li,Xiangyang Xue*

Main category: cs.CV

TL;DR: 该论文提出了Heterogeneous Collaborative Multi-Expert Reinforcement Learning (CME-CAD)框架，旨在通过结合专家模型的优势，改进CAD代码生成，以期生成精准、可编辑的CAD模型。


<details>
  <summary>Details</summary>
Motivation: 目前CAD模型的重建方法在精度和可编辑性上存在局限性，且依赖于手动标注，难以满足工业设计的需求。

Method: 提出了CME-CAD框架，包含两个阶段：Multi-Expert Fine-Tuning (MEFT) 和 Multi-Expert Reinforcement Learning (MERL)。同时，还开发了CADExpert数据集，用于评估方法的有效性。

Result: 通过CME-CAD框架生成的CAD模型准确且完全可编辑，表明该方法在工业设计中的潜在应用。

Conclusion: CME-CAD框架成功地解决了传统CAD模型在精度和可编辑性上的问题，展示了其在工业设计中的价值和潜力。

Abstract: Computer-Aided Design (CAD) is essential in industrial design, but the complexity of traditional CAD modeling and workflows presents significant challenges for automating the generation of high-precision, editable CAD models. Existing methods that reconstruct 3D models from sketches often produce non-editable and approximate models that fall short of meeting the stringent requirements for precision and editability in industrial design. Moreover, the reliance on text or image-based inputs often requires significant manual annotation, limiting their scalability and applicability in industrial settings. To overcome these challenges, we propose the Heterogeneous Collaborative Multi-Expert Reinforcement Learning (CME-CAD) paradigm, a novel training paradigm for CAD code generation. Our approach integrates the complementary strengths of these models, facilitating collaborative learning and improving the model's ability to generate accurate, constraint-compatible, and fully editable CAD models. We introduce a two-stage training process: Multi-Expert Fine-Tuning (MEFT), and Multi-Expert Reinforcement Learning (MERL). Additionally, we present CADExpert, an open-source benchmark consisting of 17,299 instances, including orthographic projections with precise dimension annotations, expert-generated Chain-of-Thought (CoT) processes, executable CADQuery code, and rendered 3D models.

</details>


### [116] [Visual Language Hypothesis](https://arxiv.org/abs/2512.23335)
*Xiu Li*

Main category: cs.CV

TL;DR: 本文从结构和拓扑的角度研究了视觉表示学习，提出了视觉理解需要一个语义语言的观点，并从这一假设出发，推导出两个理论结论：一是语义商空间不是原空间的子流形，需要非同胚的、区分性的目标；二是表示机制需要支持拓扑变化。


<details>
  <summary>Details</summary>
Motivation: 本文的动机在于探索基于结构和拓扑视角的视觉理解机制，从而提供一种对大规模判别和多模态模型的拓扑视角解释，并与经典统计学习理论中的原则相一致。

Method: 本文从一个假设出发，即视觉理解需要一个语义语言，并结合转移性和抽象性的假设，推导出两个关键理论结论。

Result: 研究结果表明：语义商空间不是原空间的子流形，需要非同胚的、区分性的目标；并且这种语义抽象需要支持拓扑变化的表示机制。

Conclusion: 本文提出了一种新的视觉表示学习的拓扑框架，并表明这种框架能够解释大量判别和多模态模型中的某些观察到的规律性，并与统计学习理论中的经典原则相吻合。

Abstract: We study visual representation learning from a structural and topological perspective. We begin from a single hypothesis: that visual understanding presupposes a semantic language for vision, in which many perceptual observations correspond to a small number of discrete semantic states. Together with widely assumed premises on transferability and abstraction in representation learning, this hypothesis implies that the visual observation space must be organized in a fiber bundle like structure, where nuisance variation populates fibers and semantics correspond to a quotient base space. From this structure we derive two theoretical consequences. First, the semantic quotient $X/G$ is not a submanifold of $X$ and cannot be obtained through smooth deformation alone, semantic invariance requires a non-homeomorphic, discriminative target, for example, supervision via labels, cross instance identification, or multimodal alignment that supplies explicit semantic equivalence. Second, we show that approximating the quotient also places structural demands on the model architecture. Semantic abstraction requires not only an external semantic target, but a representation mechanism capable of supporting topology change: an expand-and-snap process in which the manifold is first geometrically expanded to separate structure and then collapsed to form discrete semantic regions. We emphasize that these results are interpretive rather than prescriptive: the framework provides a topological lens that aligns with empirical regularities observed in large-scale discriminative and multimodal models, and with classical principles in statistical learning theory.

</details>


### [117] [SpatialMosaic: A Multiview VLM Dataset for Partial Visibility](https://arxiv.org/abs/2512.23365)
*Kanghee Lee,Injae Lee,Minseok Kwak,Kwonyoung Ryu,Jungi Hong,Jaesik Park*

Main category: cs.CV

TL;DR: 该研究提出了一种大规模多视角数据生成和标注管道，构建了SpatialMosaic数据集，并设计了一个新的基准测试SpatialMosaic-Bench，以及一个结合3D重建模型以增强视觉语言模型在多视角条件下的空间推理能力的框架SpatialMosaicVLM。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖预构建的3D表示或现成的重建管道，限制了其在实际环境中的规模和应用，该研究旨在通过直接从多视角图像学习空间推理来解决这一问题。

Method: 研究团队提出了一种多视角数据生成和标注管道，用于构建现实且多样化的空间推理问答对（SpatialMosaic数据集），并设计了SpatialMosaic-Bench基准测试，以及一个将3D重建模型整合到视觉语言模型中的混合框架（SpatialMosaicVLM）。

Result: 通过研究团队开发的数据集和VQA任务，空间推理在多视角条件下的表现得到了显著提升，证明了数据生成管道的有效性。

Conclusion: 研究结论表明，通过创建真实且具有挑战性的数据集，可以有效地提高视觉语言模型在处理空间推理任务时的能力。

Abstract: The rapid progress of Multimodal Large Language Models (MLLMs) has unlocked the potential for enhanced 3D scene understanding and spatial reasoning. However, existing approaches often rely on pre-constructed 3D representations or off-the-shelf reconstruction pipelines, which constrain scalability and real-world applicability. A recent line of work explores learning spatial reasoning directly from multi-view images, enabling Vision-Language Models (VLMs) to understand 3D scenes without explicit 3D reconstructions. Nevertheless, key challenges that frequently arise in real-world environments, such as partial visibility, occlusion, and low-overlap conditions that require spatial reasoning from fragmented visual cues, remain under-explored. To address these limitations, we propose a scalable multi-view data generation and annotation pipeline that constructs realistic spatial reasoning QAs, resulting in SpatialMosaic, a comprehensive instruction-tuning dataset featuring 2M QA pairs. We further introduce SpatialMosaic-Bench, a challenging benchmark for evaluating multi-view spatial reasoning under realistic and challenging scenarios, consisting of 1M QA pairs across 6 tasks. In addition, we present SpatialMosaicVLM, a hybrid framework that integrates 3D reconstruction models as geometry encoders within VLMs for robust spatial reasoning. Extensive experiments demonstrate that our proposed dataset and VQA tasks effectively enhance spatial reasoning under challenging multi-view conditions, validating the effectiveness of our data generation pipeline in constructing realistic and diverse QA pairs. Code and dataset will be available soon.

</details>


### [118] [NeXT-IMDL: Build Benchmark for NeXT-Generation Image Manipulation Detection & Localization](https://arxiv.org/abs/2512.23374)
*Yifei Li,Haoyuan He,Yu Zheng,Bingyao Yu,Wenzhao Zheng,Lei Chen,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: 本文提出了一种名为NeXT-IMDL的新基准，旨在通过系统地探索当前检测器的边界来检测和定位图像操作，从而发现当前模型在面对多样化的AI生成内容时存在的普遍缺陷。


<details>
  <summary>Details</summary>
Motivation: 当前的图像操纵检测方法过于依赖跨数据集评估，未能全面揭示其在处理各种AI生成内容时的脆弱性。本文通过提出一个新的大型诊断基准NeXT-IMDL，旨在提供一个全面的评估工具，以促进下一代图像操纵检测模型的发展。

Method: NeXT-IMDL基于四个基本维度（编辑模型、操作类型、内容语义、伪造粒度）分类AI生成内容，并实施了五种严格的跨维度评估协议。

Result: 广泛的实验揭示，当前的11款代表性模型在模拟真实世界的多种泛化场景下表现不佳，暴露出系统性失败和显著性能下降。

Conclusion: 通过NeXT-IMDL提供的诊断工具和新发现，本文推动了建立真正具有抗脆弱性的下一代图像操纵检测模型的发展。

Abstract: The accessibility surge and abuse risks of user-friendly image editing models have created an urgent need for generalizable, up-to-date methods for Image Manipulation Detection and Localization (IMDL). Current IMDL research typically uses cross-dataset evaluation, where models trained on one benchmark are tested on others. However, this simplified evaluation approach conceals the fragility of existing methods when handling diverse AI-generated content, leading to misleading impressions of progress. This paper challenges this illusion by proposing NeXT-IMDL, a large-scale diagnostic benchmark designed not just to collect data, but to probe the generalization boundaries of current detectors systematically. Specifically, NeXT-IMDL categorizes AIGC-based manipulations along four fundamental axes: editing models, manipulation types, content semantics, and forgery granularity. Built upon this, NeXT-IMDL implements five rigorous cross-dimension evaluation protocols. Our extensive experiments on 11 representative models reveal a critical insight: while these models perform well in their original settings, they exhibit systemic failures and significant performance degradation when evaluated under our designed protocols that simulate real-world, various generalization scenarios. By providing this diagnostic toolkit and the new findings, we aim to advance the development towards building truly robust, next-generation IMDL models.

</details>


### [119] [SOFTooth: Semantics-Enhanced Order-Aware Fusion for Tooth Instance Segmentation](https://arxiv.org/abs/2512.23411)
*Xiaolan Li,Wanquan Liu,Pengcheng Li,Pengyu Jie,Chenqiang Gao*

Main category: cs.CV

TL;DR: SOFTooth 提出了一种增强语义、有序感知的 2D-3D 融合框架，通过无监督的方式将 2D 点残差门控模块的咬合视图 SAM 特征注入 3D 点特征，优化牙齿边缘。中心引导的掩膜精细化可调节实例掩膜与几何质心的一致性，避免中心漂移。有序感知匈牙利匹配策略将牙齿解剖顺序和中心距离整合到基于相似性的分配中，确保在牙齿丢失或拥挤的情况下也能实现一致的标记。


<details>
  <summary>Details</summary>
Motivation: 当前牙科实例分割中存在三个主要挑战：拥挤的牙弓、模糊的牙龈边界、缺失的牙齿和罕见但重要的第三磨牙。传统的基于几何线索的 3D 方法往往会出现边界泄漏、中心漂移和不一致的牙体标识，特别是在少数类和复杂解剖结构中。另一方面，基础 2D 模型如 Segment Anything Model (SAM) 能够提供强大的边界感知语义，但由于直接应用于 3D 在临床工作中是不可行的。

Method: SOFTooth 方法包括一个点级残差门控模块，将 OCC 视图 SAM 插值嵌入到 3D 点特征中，以细化牙龈边缘和牙齿间边界。中心引导掩膜精细调节实例掩膜与几何质心的一致性，减少中心漂移。有序感知匈牙利匹配策略整合牙齿解剖顺序和中心距离，确保在牙齿缺失或密集排列的情况下标签一致性。

Result: SOFTooth 方法在 3DTeethSeg‘22 数据集上取得了最先进的总体准确度和平均交并比。特别是在涉及第三磨牙的情况下，表现明显优于其他方法，证明了丰富的 2D 语义可以在不进行 2D 微调的情况下有效地转移到 3D 牙体实例分割。

Conclusion: SOFTooth 通过结合 2D 语义和 3D 语义增强框架，成功解决了牙科实例分割中的挑战，特别适用于稀有且重要的牙齿类型，展示了其潜在的临床应用价值。

Abstract: Three-dimensional (3D) tooth instance segmentation remains challenging due to crowded arches, ambiguous tooth-gingiva boundaries, missing teeth, and rare yet clinically important third molars. Native 3D methods relying on geometric cues often suffer from boundary leakage, center drift, and inconsistent tooth identities, especially for minority classes and complex anatomies. Meanwhile, 2D foundation models such as the Segment Anything Model (SAM) provide strong boundary-aware semantics, but directly applying them in 3D is impractical in clinical workflows. To address these issues, we propose SOFTooth, a semantics-enhanced, order-aware 2D-3D fusion framework that leverages frozen 2D semantics without explicit 2D mask supervision. First, a point-wise residual gating module injects occlusal-view SAM embeddings into 3D point features to refine tooth-gingiva and inter-tooth boundaries. Second, a center-guided mask refinement regularizes consistency between instance masks and geometric centroids, reducing center drift. Furthermore, an order-aware Hungarian matching strategy integrates anatomical tooth order and center distance into similarity-based assignment, ensuring coherent labeling even under missing or crowded dentitions. On 3DTeethSeg'22, SOFTooth achieves state-of-the-art overall accuracy and mean IoU, with clear gains on cases involving third molars, demonstrating that rich 2D semantics can be effectively transferred to 3D tooth instance segmentation without 2D fine-tuning.

</details>


### [120] [DriveLaW:Unifying Planning and Video Generation in a Latent Driving World](https://arxiv.org/abs/2512.23421)
*Tianze Xia,Yongkang Li,Lijun Zhou,Jingfeng Yao,Kaixin Xiong,Haiyang Sun,Bing Wang,Kun Ma,Hangjun Ye,Wenyu Liu,Xinggang Wang*

Main category: cs.CV

TL;DR: DriveLaW 提出了一种全新的范式，将视频生成与运动规划统一起来，通过直接将 DriveLaW-Video 的潜在表示注入规划器，实现高保真度的未来预测和可靠轨迹规划的一致性。


<details>
  <summary>Details</summary>
Motivation: 当前世界模型的应用主要局限于泛化的预测和独立的运动规划过程，缺乏直接的关联，而 DriveLaW 通过统一视图生成和运动规划，旨在解决这一问题。

Method: DriveLaW 由两个核心部分组成：DriveLaW-Video 和 DriveLaW-Act。前者通过具有表现力的潜在表示生成高保真度预测，后者则根据来自 DriveLaW-Video 的潜在表示生成一致且可靠的轨迹。训练策略采用三阶段递进式。

Result: DriveLaW 在两个任务上均取得前所未有的成绩，特别是在视图预测方面，相比最佳工作在 FID 上提高了 33.3%，在 FVD 上提高了 1.8%，在 NAVSIM 规划基准测试中也取得了新纪录。

Conclusion: DriveLaW 的统一方法不仅显著推动视图预测的进步，还实现了在规划基准测试中新的里程碑。

Abstract: World models have become crucial for autonomous driving, as they learn how scenarios evolve over time to address the long-tail challenges of the real world. However, current approaches relegate world models to limited roles: they operate within ostensibly unified architectures that still keep world prediction and motion planning as decoupled processes. To bridge this gap, we propose DriveLaW, a novel paradigm that unifies video generation and motion planning. By directly injecting the latent representation from its video generator into the planner, DriveLaW ensures inherent consistency between high-fidelity future generation and reliable trajectory planning. Specifically, DriveLaW consists of two core components: DriveLaW-Video, our powerful world model that generates high-fidelity forecasting with expressive latent representations, and DriveLaW-Act, a diffusion planner that generates consistent and reliable trajectories from the latent of DriveLaW-Video, with both components optimized by a three-stage progressive training strategy. The power of our unified paradigm is demonstrated by new state-of-the-art results across both tasks. DriveLaW not only advances video prediction significantly, surpassing best-performing work by 33.3% in FID and 1.8% in FVD, but also achieves a new record on the NAVSIM planning benchmark.

</details>


### [121] [Direct Diffusion Score Preference Optimization via Stepwise Contrastive Policy-Pair Supervision](https://arxiv.org/abs/2512.23426)
*Dohyun Kim,Seungwoo Lyu,Seung Wook Kim,Paul Hongsuck Seo*

Main category: cs.CV

TL;DR: 提出了Direct Diffusion Score Preference Optimization (DDSPO)，通过使用预训练模型生成偏好信号，直接从获胜和失败的策略中提取每一时间步的监督信息，实现了在不需要明确奖励建模或人工注释的情况下改善文本图像生成的一致性和视觉质量。


<details>
  <summary>Details</summary>
Motivation: 现有的基于偏好训练的方法在保留用户意图和一致的美学质量方面效果有限，且需要昂贵且可能有噪声的人类标注数据。DDSPO通过利用预训练模型自动生成偏好信号，提供了一种轻量级的方法来优化扩散模型。

Method: DDSPO方法从已有的胜利和失败策略中直接获取每一时间步的监督信号，而不仅仅是最终的样本。它通过比较预训练模型在原始提示和语义降级变体之间的输出差异来自动生成偏好信号。

Result: 实验结果表明，DDSPO在文本图像生成的一致性和视觉质量方面优于或匹配了现有的偏好优化方法，同时所需监督信号更少。

Conclusion: DDSPO提供了一种无需明确奖励建模或人工注释的有效策略，来支持扩散模型的偏好监督，从而提升其生成效果。

Abstract: Diffusion models have achieved impressive results in generative tasks such as text-to-image synthesis, yet they often struggle to fully align outputs with nuanced user intent and maintain consistent aesthetic quality. Existing preference-based training methods like Diffusion Direct Preference Optimization help address these issues but rely on costly and potentially noisy human-labeled datasets. In this work, we introduce Direct Diffusion Score Preference Optimization (DDSPO), which directly derives per-timestep supervision from winning and losing policies when such policies are available. Unlike prior methods that operate solely on final samples, DDSPO provides dense, transition-level signals across the denoising trajectory. In practice, we avoid reliance on labeled data by automatically generating preference signals using a pretrained reference model: we contrast its outputs when conditioned on original prompts versus semantically degraded variants. This practical strategy enables effective score-space preference supervision without explicit reward modeling or manual annotations. Empirical results demonstrate that DDSPO improves text-image alignment and visual quality, outperforming or matching existing preference-based methods while requiring significantly less supervision. Our implementation is available at: https://dohyun-as.github.io/DDSPO

</details>


### [122] [Towards Integrating Uncertainty for Domain-Agnostic Segmentation](https://arxiv.org/abs/2512.23427)
*Jesse Brouwers,Xiaoyan Xing,Alexander Timans*

Main category: cs.CV

TL;DR: 该研究通过构建UncertSAM基准，评估了一系列轻量级后处理不确定性评估方法，并提出了一种初步的基于不确定性指导的预测精炼步骤，以探索在没有领域知识的情况下量化不确定性是否能增强空间分割模型的通用性。


<details>
  <summary>Details</summary>
Motivation: 当前的语义分割模型，如Segment Anything Model（SAM）家庭，尽管在零样本设置下表现出色，但在数据分布偏移或知识有限的域中仍存在脆弱性。因此，本文旨在研究如何通过不确定性量化来缓解这些挑战，从而提高模型的通用性。

Method: 1. 构建了一个包含八大数据集的UncertSAM基准，旨在在包含阴影、透明性及迷彩等挑战性分割条件下的SAM模型进行压力测试；2. 评估了一系列轻量级后处理不确定性估计方法；3. 考察了一种初步的基于不确定性指导的预测精炼步骤。

Result: 最后一层Laplace近似的方法获得了与分割错误高度相关的不确定性估计，表明存在有意义的信号。尽管预测精炼的效果有限，但结果表明将不确定性纳入分割模型可能有助于提高其稳健性。

Conclusion: 研究中得到的初步结果显示，将不确定性纳入分割模型中支持了其在域外的鲁棒性，并且未来的工作将继续探索将不确定性指导纳入实际部署中的可能性。

Abstract: Foundation models for segmentation such as the Segment Anything Model (SAM) family exhibit strong zero-shot performance, but remain vulnerable in shifted or limited-knowledge domains. This work investigates whether uncertainty quantification can mitigate such challenges and enhance model generalisability in a domain-agnostic manner. To this end, we (1) curate UncertSAM, a benchmark comprising eight datasets designed to stress-test SAM under challenging segmentation conditions including shadows, transparency, and camouflage; (2) evaluate a suite of lightweight, post-hoc uncertainty estimation methods; and (3) assess a preliminary uncertainty-guided prediction refinement step. Among evaluated approaches, a last-layer Laplace approximation yields uncertainty estimates that correlate well with segmentation errors, indicating a meaningful signal. While refinement benefits are preliminary, our findings underscore the potential of incorporating uncertainty into segmentation models to support robust, domain-agnostic performance. Our benchmark and code are made publicly available.

</details>


### [123] [RealX3D: A Physically-Degraded 3D Benchmark for Multi-view Visual Restoration and Reconstruction](https://arxiv.org/abs/2512.23437)
*Shuhong Liu,Chenyu Bao,Ziteng Cui,Yun Liu,Xuangeng Chu,Lin Gu,Marcos V. Conde,Ryo Umagami,Tomohiro Hashimoto,Zijian Hu,Tianhan Xu,Yuan Gan,Yusuke Kurose,Tatsuya Harada*

Main category: cs.CV

TL;DR: RealX3D 提供了一个基准来评估多视图视觉恢复和 3D 重建在不同物理退化条件下的效果。


<details>
  <summary>Details</summary>
Motivation: 当前的多视图管道在现实世界的挑战性环境中表现脆弱，因此需要一个新的基准来评估其在真实退化条件下的表现。

Method: RealX3D 通过收集不同严重程度的多种类型的物理退化图像，提供了一个统一的采集协议来创建 LQ/GT 视图对。

Result: 通过最近几年的多种优化方法和前馈方法，发现这些方法在物理退化下的重建质量存在显著下降。

Conclusion: 此研究强调了在现实退化条件下评估多视图重建技术的重要性，并展示了RealX3D的优势。

Abstract: We introduce RealX3D, a real-capture benchmark for multi-view visual restoration and 3D reconstruction under diverse physical degradations. RealX3D groups corruptions into four families, including illumination, scattering, occlusion, and blurring, and captures each at multiple severity levels using a unified acquisition protocol that yields pixel-aligned LQ/GT views. Each scene includes high-resolution capture, RAW images, and dense laser scans, from which we derive world-scale meshes and metric depth. Benchmarking a broad range of optimization-based and feed-forward methods shows substantial degradation in reconstruction quality under physical corruptions, underscoring the fragility of current multi-view pipelines in real-world challenging environments.

</details>


### [124] [Automated river gauge plate reading using a hybrid object detection and generative AI framework in the Limpopo River Basin](https://arxiv.org/abs/2512.23454)
*Kayathri Vigneswaran,Hugo Retief,Jai Clifford Holmes,Mariangel Garcia Andarcia,Hansaka Tennakoon*

Main category: cs.CV

TL;DR: 提出了一种结合基于视觉的水位检测、YOLOv8姿态尺取和大型多模态语言模型（GPT 4o和Gemini 2.0 Flash）的混合框架，用于自动化河流水位计读数。该系统在实验中表现良好，特别是在几何元数据的帮助下，提高了自动读数的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统水文观测方法受到手动测量误差和环境限制的限制，本文提出的方法旨在提供一种规模更大、更高效、更可靠的自动化水文监测解决方案，以提高洪水预报、水资源管理和生态保护的效果。

Method: 该方法包括图像预处理、标注、水位检测、尺度缺口估计和数字读数提取等阶段。使用YOLOv8进行姿态尺取，基于视觉检测水位线，结合大型多模态语言模型（GPT 4o和Gemini 2.0 Flash）实现自动读数。

Result: 水位检测的精确度为94.24%，F1分数为83.64%，尺度缺口检测提供了准确的几何校准，Gemini Stage 2实现了最高的准确性，平均绝对误差为5.43厘米，均方根误差为8.58厘米，R平方为0.84，在最佳图像条件下。

Conclusion: 结果表明，图像质量对大规模语言模型（LLM）的效果有重要影响，且结合几何元数据与多模态人工智能对于稳健的水位估计非常重要。提出的系统为实时河流水位计数字化和改进水资源管理提供了潜在的可能性。

Abstract: Accurate and continuous monitoring of river water levels is essential for flood forecasting, water resource management, and ecological protection. Traditional hydrological observation methods are often limited by manual measurement errors and environmental constraints. This study presents a hybrid framework integrating vision based waterline detection, YOLOv8 pose scale extraction, and large multimodal language models (GPT 4o and Gemini 2.0 Flash) for automated river gauge plate reading. The methodology involves sequential stages of image preprocessing, annotation, waterline detection, scale gap estimation, and numeric reading extraction. Experiments demonstrate that waterline detection achieved high precision of 94.24 percent and an F1 score of 83.64 percent, while scale gap detection provided accurate geometric calibration for subsequent reading extraction. Incorporating scale gap metadata substantially improved the predictive performance of LLMs, with Gemini Stage 2 achieving the highest accuracy, with a mean absolute error of 5.43 cm, root mean square error of 8.58 cm, and R squared of 0.84 under optimal image conditions. Results highlight the sensitivity of LLMs to image quality, with degraded images producing higher errors, and underscore the importance of combining geometric metadata with multimodal artificial intelligence for robust water level estimation. Overall, the proposed approach offers a scalable, efficient, and reliable solution for automated hydrological monitoring, demonstrating potential for real time river gauge digitization and improved water resource management.

</details>


### [125] [TV-RAG: A Temporal-aware and Semantic Entropy-Weighted Framework for Long Video Retrieval and Understanding](https://arxiv.org/abs/2512.23483)
*Zongsheng Cao,Yangfan He,Anran Liu,Feng Chen,Zepeng Wang,Jun Xie*

Main category: cs.CV

TL;DR: TV-RAG 是一种无需训练的架构，通过引入时间衰减检索模块和熵权关键帧采样器机制，解决长视频推理中的时间关注点狭窄和信息冗余问题，实现在大型视频语言模型上的双层推理，提升长视频理解能力。


<details>
  <summary>Details</summary>
Motivation: 为了解决大型视频语言模型在处理长视频时遇到的时间窗口狭窄和未能捕捉细粒度语义变化的问题，以及主流基于文本的检索框架忽略多模态内容间丰富的时间依赖性的问题。

Method: TV-RAG 采用时间衰减检索模块和熵权关键帧采样器两个主要机制，时间衰减检索模块通过引入时间偏移来调整相似度计算，更好地匹配多媒体上下文；熵权关键帧采样器选择均匀间隔且信息密集的关键帧，减少冗余并保持代表性。

Result: TV-RAG 在多种长视频基准测试（如 Video-MME、MLVU 和 LongVideoBench）上表现出色，超越了许多主流基线系统，证明了该模型的有效性。

Conclusion: TV-RAG 提供了一种低成本的系统升级路径，可以无缝集成到现有大型视频语言模型中，增强长视频理解能力，具有广泛的应用前景。

Abstract: Large Video Language Models (LVLMs) have rapidly emerged as the focus of multimedia AI research. Nonetheless, when confronted with lengthy videos, these models struggle: their temporal windows are narrow, and they fail to notice fine-grained semantic shifts that unfold over extended durations. Moreover, mainstream text-based retrieval pipelines, which rely chiefly on surface-level lexical overlap, ignore the rich temporal interdependence among visual, audio, and subtitle channels. To mitigate these limitations, we propose TV-RAG, a training-free architecture that couples temporal alignment with entropy-guided semantics to improve long-video reasoning. The framework contributes two main mechanisms: \emph{(i)} a time-decay retrieval module that injects explicit temporal offsets into the similarity computation, thereby ranking text queries according to their true multimedia context; and \emph{(ii)} an entropy-weighted key-frame sampler that selects evenly spaced, information-dense frames, reducing redundancy while preserving representativeness. By weaving these temporal and semantic signals together, TV-RAG realises a dual-level reasoning routine that can be grafted onto any LVLM without re-training or fine-tuning. The resulting system offers a lightweight, budget-friendly upgrade path and consistently surpasses most leading baselines across established long-video benchmarks such as Video-MME, MLVU, and LongVideoBench, confirming the effectiveness of our model. The code can be found at https://github.com/AI-Researcher-Team/TV-RAG.

</details>


### [126] [Multi-label Classification with Panoptic Context Aggregation Networks](https://arxiv.org/abs/2512.23486)
*Mingyuan Jiu,Hailong Zhu,Wenchuan Wei,Hichem Sahbi,Rongrong Ji,Mingliang Xu*

Main category: cs.CV

TL;DR: PanCAN提出了一种新的方法，通过多层次几何上下文的跨尺度特征聚合，有效建模跨尺度上下文交互，从而提升复杂场景理解。实验结果表明，PanCAN在多种基准测试上优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 当前的方法大多关注基本的几何关系或局部特征，而忽略了跨尺度对象间的上下文交互，导致对复杂场景的理解不够全面。

Method: PanCAN采用了一种新颖的方法，通过跨尺度特征聚合的方式，在高维希尔伯特空间中整合多层次的几何上下文。利用随机游走与注意力机制结合，PanCAN能够在不同尺度下学习多层次的邻域关系，并在不同尺度的模块之间进行级联，通过注意力机制动态融合重要锚点的邻域特征。

Result: 在NUS-WIDE、PASCAL VOC2007和MS-COCO等多项基准测试上的多项标签分类实验表明，PanCAN取得了竞争性的结果，并在定量和定性评价中均优于现有的技术。

Conclusion: PanCAN能够显著提高复杂场景的理解，通过结合多尺度、多层次的上下文感知特征。

Abstract: Context modeling is crucial for visual recognition, enabling highly discriminative image representations by integrating both intrinsic and extrinsic relationships between objects and labels in images. A limitation in current approaches is their focus on basic geometric relationships or localized features, often neglecting cross-scale contextual interactions between objects. This paper introduces the Deep Panoptic Context Aggregation Network (PanCAN), a novel approach that hierarchically integrates multi-order geometric contexts through cross-scale feature aggregation in a high-dimensional Hilbert space. Specifically, PanCAN learns multi-order neighborhood relationships at each scale by combining random walks with an attention mechanism. Modules from different scales are cascaded, where salient anchors at a finer scale are selected and their neighborhood features are dynamically fused via attention. This enables effective cross-scale modeling that significantly enhances complex scene understanding by combining multi-order and cross-scale context-aware features. Extensive multi-label classification experiments on NUS-WIDE, PASCAL VOC2007, and MS-COCO benchmarks demonstrate that PanCAN consistently achieves competitive results, outperforming state-of-the-art techniques in both quantitative and qualitative evaluations, thereby substantially improving multi-label classification performance.

</details>


### [127] [IdentityStory: Taming Your Identity-Preserving Generator for Human-Centric Story Generation](https://arxiv.org/abs/2512.23519)
*Donghao Zhou,Jingyu Lin,Guibao Shen,Quande Liu,Jialin Gao,Lihao Liu,Lan Du,Cunjian Chen,Chi-Wing Fu,Xiaowei Hu,Pheng-Ann Heng*

Main category: cs.CV

TL;DR: IdentityStory 是一个通过保持一致的角色身份来生成多个人物跨图故事的框架，通过迭代身份发现和重新降噪身份注入两个组件实现。


<details>
  <summary>Details</summary>
Motivation: 当前的视觉生成模型可以在文本中生成一致的角色故事，但人本故事生成面临额外挑战，如保持高度多样性和详细的人脸一致性以及跨多个图像协调多人物。

Method: IdentityStory框架包含迭代身份发现和重新降噪身份注入两个关键组件。迭代身份发现用于提取一致的角色身份，而重新降噪身份注入则用于在保持所需上下文的同时重置角色身份。

Result: IdentityStory在ConsiStory-Human基准上优于现有方法，特别是在人脸一致性方面，并支持多人物组合。此外，框架还展示了生成无限长度故事和动态人物组合的应用潜力。

Conclusion: IdentityStory框架为解决人本故事生成中的关键挑战提供了一种创新的方法，特别是在保持角色身份的一致性方面表现出色。

Abstract: Recent visual generative models enable story generation with consistent characters from text, but human-centric story generation faces additional challenges, such as maintaining detailed and diverse human face consistency and coordinating multiple characters across different images. This paper presents IdentityStory, a framework for human-centric story generation that ensures consistent character identity across multiple sequential images. By taming identity-preserving generators, the framework features two key components: Iterative Identity Discovery, which extracts cohesive character identities, and Re-denoising Identity Injection, which re-denoises images to inject identities while preserving desired context. Experiments on the ConsiStory-Human benchmark demonstrate that IdentityStory outperforms existing methods, particularly in face consistency, and supports multi-character combinations. The framework also shows strong potential for applications such as infinite-length story generation and dynamic character composition.

</details>


### [128] [Iterative Inference-time Scaling with Adaptive Frequency Steering for Image Super-Resolution](https://arxiv.org/abs/2512.23532)
*Hexin Zhang,Dong Li,Jie Huang,Bingzhou Wang,Xueyang Fu,Zhengjun Zha*

Main category: cs.CV

TL;DR: IAFS 提出了一种训练无需的框架，通过迭代细化和频率敏感的粒子融合来平衡感知质量和结构一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的图像生成方法难以同时保证高频率感知质量和低频率结构保真度。

Method: IAFS 通过迭代细化逐步矫正结构偏差，并通过自适应地结合高频率感知线索和低频率结构信息来确保频率融合的有效性。

Result: 实验表明，IAFS 能有效地解决感知与结构的矛盾，提升图像的感知细节和结构准确性，超越了现有的推断时缩放方法。

Conclusion: IAFS 为图像超分辨率领域提供了一种新的解决方案，有望在未来进一步研究和应用中发挥重要作用。

Abstract: Diffusion models have become a leading paradigm for image super-resolution (SR), but existing methods struggle to guarantee both the high-frequency perceptual quality and the low-frequency structural fidelity of generated images. Although inference-time scaling can theoretically improve this trade-off by allocating more computation, existing strategies remain suboptimal: reward-driven particle optimization often causes perceptual over-smoothing, while optimal-path search tends to lose structural consistency. To overcome these difficulties, we propose Iterative Diffusion Inference-Time Scaling with Adaptive Frequency Steering (IAFS), a training-free framework that jointly leverages iterative refinement and frequency-aware particle fusion. IAFS addresses the challenge of balancing perceptual quality and structural fidelity by progressively refining the generated image through iterative correction of structural deviations. Simultaneously, it ensures effective frequency fusion by adaptively integrating high-frequency perceptual cues with low-frequency structural information, allowing for a more accurate and balanced reconstruction across different image details. Extensive experiments across multiple diffusion-based SR models show that IAFS effectively resolves the perception-fidelity conflict, yielding consistently improved perceptual detail and structural accuracy, and outperforming existing inference-time scaling methods.

</details>


### [129] [PurifyGen: A Risk-Discrimination and Semantic-Purification Model for Safe Text-to-Image Generation](https://arxiv.org/abs/2512.23546)
*Zongsheng Cao,Yangfan He,Anran Liu,Jun Xie,Feng Chen,Zepeng Wang*

Main category: cs.CV

TL;DR: PurifyGen 提出了一种无需训练的新型安全文本到图像生成方法，通过双重策略净化提示，有效减少潜在有害内容，同时保持生成内容的原意和连贯性。


<details>
  <summary>Details</summary>
Motivation: 当前文本到图像生成方法的质量提升伴随着安全问题，传统的安全方法存在明显的局限性。

Method: PurifyGen 使用双重净化策略。首先，通过计算每个提示词与预定义的有害和干净概念列表之间的互补语义距离，标记潜在的危险词；其次，对危险的提示进行双重空间转换，去除有害语义，强化安全语义，确保原始意图和连贯性。

Result: PurifyGen 在五个数据集上测试，表现出比现有方法更好的有害内容减少效果，并能达到与依赖训练的方法相当的效果。

Conclusion: PurifyGen 提供了一个即插即用的解决方案，理论上完备且泛化能力强，是一个有效安全增强技术。

Abstract: Recent advances in diffusion models have notably enhanced text-to-image (T2I) generation quality, but they also raise the risk of generating unsafe content. Traditional safety methods like text blacklisting or harmful content classification have significant drawbacks: they can be easily circumvented or require extensive datasets and extra training. To overcome these challenges, we introduce PurifyGen, a novel, training-free approach for safe T2I generation that retains the model's original weights. PurifyGen introduces a dual-stage strategy for prompt purification. First, we evaluate the safety of each token in a prompt by computing its complementary semantic distance, which measures the semantic proximity between the prompt tokens and concept embeddings from predefined toxic and clean lists. This enables fine-grained prompt classification without explicit keyword matching or retraining. Tokens closer to toxic concepts are flagged as risky. Second, for risky prompts, we apply a dual-space transformation: we project toxic-aligned embeddings into the null space of the toxic concept matrix, effectively removing harmful semantic components, and simultaneously align them into the range space of clean concepts. This dual alignment purifies risky prompts by both subtracting unsafe semantics and reinforcing safe ones, while retaining the original intent and coherence. We further define a token-wise strategy to selectively replace only risky token embeddings, ensuring minimal disruption to safe content. PurifyGen offers a plug-and-play solution with theoretical grounding and strong generalization to unseen prompts and models. Extensive testing shows that PurifyGen surpasses current methods in reducing unsafe content across five datasets and competes well with training-dependent approaches. The code can refer to https://github.com/AI-Researcher-Team/PurifyGen.

</details>


### [130] [ThinkGen: Generalized Thinking for Visual Generation](https://arxiv.org/abs/2512.23568)
*Siyu Jiao,Yiheng Lin,Yujie Zhong,Qi She,Wei Zhou,Xiaohan Lan,Zilong Huang,Fei Yu,Yingchen Yu,Yunqing Zhao,Yao Zhao,Yunchao Wei*

Main category: cs.CV

TL;DR: 提出ThinkGen框架，它是一种混合了Mullm的CoT推理机制的视觉生成框架，通过分离GRPO训练方案，在多种生成场景中表现出了强健且先进的生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有技术在将CoT推理扩展到生成任务方面存在局限性，特别是缺乏通用机制来促进跨场景的有效学习和应用。

Method: 采用了一种解耦的架构，预训练的Mullm负责基于用户意图生成定制化的指令，而基于DiT的模块则生成高质量的图像。此外，还提出了一种分离的GRPO训练方案，交替强化学习以优化MLLM和DiT模块之间的协同工作。

Result: 广泛的实验证明了ThinkGen在多个生成基准上实现了强大的、最先进的生成性能。

Conclusion: 该工作证明了将MLLM的CoT推理机制应用于视觉生成场景的有效性，并提出了提升模型跨场景适应能力的方法。

Abstract: Recent progress in Multimodal Large Language Models (MLLMs) demonstrates that Chain-of-Thought (CoT) reasoning enables systematic solutions to complex understanding tasks. However, its extension to generation tasks remains nascent and limited by scenario-specific mechanisms that hinder generalization and adaptation. In this work, we present ThinkGen, the first think-driven visual generation framework that explicitly leverages MLLM's CoT reasoning in various generation scenarios. ThinkGen employs a decoupled architecture comprising a pretrained MLLM and a Diffusion Transformer (DiT), wherein the MLLM generates tailored instructions based on user intent, and DiT produces high-quality images guided by these instructions. We further propose a separable GRPO-based training paradigm (SepGRPO), alternating reinforcement learning between the MLLM and DiT modules. This flexible design enables joint training across diverse datasets, facilitating effective CoT reasoning for a wide range of generative scenarios. Extensive experiments demonstrate that ThinkGen achieves robust, state-of-the-art performance across multiple generation benchmarks. Code is available: https://github.com/jiaosiyuu/ThinkGen

</details>


### [131] [ProGuard: Towards Proactive Multimodal Safeguard](https://arxiv.org/abs/2512.23573)
*Shaohan Yu,Lijun Li,Chenyang Si,Lu Sheng,Jing Shao*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The rapid evolution of generative models has led to a continuous emergence of multimodal safety risks, exposing the limitations of existing defense methods. To address these challenges, we propose ProGuard, a vision-language proactive guard that identifies and describes out-of-distribution (OOD) safety risks without the need for model adjustments required by traditional reactive approaches. We first construct a modality-balanced dataset of 87K samples, each annotated with both binary safety labels and risk categories under a hierarchical multimodal safety taxonomy, effectively mitigating modality bias and ensuring consistent moderation across text, image, and text-image inputs. Based on this dataset, we train our vision-language base model purely through reinforcement learning (RL) to achieve efficient and concise reasoning. To approximate proactive safety scenarios in a controlled setting, we further introduce an OOD safety category inference task and augment the RL objective with a synonym-bank-based similarity reward that encourages the model to generate concise descriptions for unseen unsafe categories. Experimental results show that ProGuard achieves performance comparable to closed-source large models on binary safety classification, substantially outperforms existing open-source guard models on unsafe content categorization. Most notably, ProGuard delivers a strong proactive moderation ability, improving OOD risk detection by 52.6% and OOD risk description by 64.8%.

</details>


### [132] [LiveTalk: Real-Time Multimodal Interactive Video Diffusion via Improved On-Policy Distillation](https://arxiv.org/abs/2512.23576)
*Ethan Chern,Zhulin Hu,Bohao Tang,Jiadi Su,Steffi Chern,Zhijie Deng,Pengfei Liu*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Real-time video generation via diffusion is essential for building general-purpose multimodal interactive AI systems. However, the simultaneous denoising of all video frames with bidirectional attention via an iterative process in diffusion models prevents real-time interaction. While existing distillation methods can make the model autoregressive and reduce sampling steps to mitigate this, they focus primarily on text-to-video generation, leaving the human-AI interaction unnatural and less efficient. This paper targets real-time interactive video diffusion conditioned on a multimodal context, including text, image, and audio, to bridge the gap. Given the observation that the leading on-policy distillation approach Self Forcing encounters challenges (visual artifacts like flickering, black frames, and quality degradation) with multimodal conditioning, we investigate an improved distillation recipe with emphasis on the quality of condition inputs as well as the initialization and schedule for the on-policy optimization. On benchmarks for multimodal-conditioned (audio, image, and text) avatar video generation including HDTF, AVSpeech, and CelebV-HQ, our distilled model matches the visual quality of the full-step, bidirectional baselines of similar or larger size with 20x less inference cost and latency. Further, we integrate our model with audio language models and long-form video inference technique Anchor-Heavy Identity Sinks to build LiveTalk, a real-time multimodal interactive avatar system. System-level evaluation on our curated multi-turn interaction benchmark shows LiveTalk outperforms state-of-the-art models (Sora2, Veo3) in multi-turn video coherence and content quality, while reducing response latency from 1 to 2 minutes to real-time generation, enabling seamless human-AI multimodal interaction.

</details>


### [133] [Same or Not? Enhancing Visual Perception in Vision-Language Models](https://arxiv.org/abs/2512.23592)
*Damiano Marsili,Aditya Mehta,Ryan Y. Lin,Georgia Gkioxari*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Vision-language models (VLMs) excel at broad visual understanding but remain coarse-grained, exhibit visual biases, and miss subtle visual details. Existing training corpora reinforce this limitation by emphasizing general recognition ("Is it a cat or a dog?") over fine-grained perception. To address this, we introduce a new training corpus and task designed to enhance the perceptual abilities of VLMs. TWIN is a large-scale dataset of 561,000 image-pair queries that task models to determine whether two visually similar images depict the same object, encouraging attention to nuanced visual cues. The dataset spans a diverse range of everyday objects across contexts, viewpoints, and appearances. Fine-tuning VLMs on TWIN yields notable gains in fine-grained recognition, even on unseen domains such as art, animals, plants, and landmarks. To quantify these gains, we introduce FGVQA, a benchmark suite of 12,000 queries that repurposes fine-grained recognition and retrieval datasets from multiple domains. While existing VLMs struggle on FGVQA, when fine-tuned on TWIN they improve by up to 19.3%, without compromising performance on general VQA benchmarks. Finally, our TWIN dataset scales favorably with object annotations, and our analysis shows that scale is key to performance. We envision TWIN as a drop-in addition to open-source VLM training corpora, advancing perceptual precision of future models. Project webpage: https://glab-caltech.github.io/twin/

</details>


### [134] [Detection Fire in Camera RGB-NIR](https://arxiv.org/abs/2512.23594)
*Nguyen Truong Khai,Luong Duc Vinh*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Improving the accuracy of fire detection using infrared night vision cameras remains a challenging task. Previous studies have reported strong performance with popular detection models. For example, YOLOv7 achieved an mAP50-95 of 0.51 using an input image size of 640 x 1280, RT-DETR reached an mAP50-95 of 0.65 with an image size of 640 x 640, and YOLOv9 obtained an mAP50-95 of 0.598 at the same resolution. Despite these results, limitations in dataset construction continue to cause issues, particularly the frequent misclassification of bright artificial lights as fire.
  This report presents three main contributions: an additional NIR dataset, a two-stage detection model, and Patched-YOLO. First, to address data scarcity, we explore and apply various data augmentation strategies for both the NIR dataset and the classification dataset. Second, to improve night-time fire detection accuracy while reducing false positives caused by artificial lights, we propose a two-stage pipeline combining YOLOv11 and EfficientNetV2-B0. The proposed approach achieves higher detection accuracy compared to previous methods, particularly for night-time fire detection. Third, to improve fire detection in RGB images, especially for small and distant objects, we introduce Patched-YOLO, which enhances the model's detection capability through patch-based processing. Further details of these contributions are discussed in the following sections.

</details>


### [135] [Scalable Residual Feature Aggregation Framework with Hybrid Metaheuristic Optimization for Robust Early Pancreatic Neoplasm Detection in Multimodal CT Imaging](https://arxiv.org/abs/2512.23597)
*Janani Annur Thiruvengadam,Kiran Mayee Nabigaru,Anusha Kovi*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The early detection of pancreatic neoplasm is a major clinical dilemma, and it is predominantly so because tumors are likely to occur with minimal contrast margins and a large spread anatomy-wide variation amongst patients on a CT scan. These complexities require to be addressed with an effective and scalable system that can assist in enhancing the salience of the subtle visual cues and provide a high level of the generalization on the multimodal imaging data. A Scalable Residual Feature Aggregation (SRFA) framework is proposed to be used to meet these conditions in this study. The framework integrates a pipeline of preprocessing followed by the segmentation using the MAGRes-UNet that is effective in making the pancreatic structures and isolating regions of interest more visible. DenseNet-121 performed with residual feature storage is used to extract features to allow deep hierarchical features to be aggregated without properties loss. To go further, hybrid HHO-BA metaheuristic feature selection strategy is used, which guarantees the best feature subset refinement. To be classified, the system is trained based on a new hybrid model that integrates the ability to pay attention on the world, which is the Vision Transformer (ViT) with the high representational efficiency of EfficientNet-B3. A dual optimization mechanism incorporating SSA and GWO is used to fine-tune hyperparameters to enhance greater robustness and less overfitting. Experimental results support the significant improvement in performance, with the suggested model reaching 96.23% accuracy, 95.58% F1-score and 94.83% specificity, the model is significantly better than the traditional CNNs and contemporary transformer-based models. Such results highlight the possibility of the SRFA framework as a useful instrument in the early detection of pancreatic tumors.

</details>


### [136] [Memorization in 3D Shape Generation: An Empirical Study](https://arxiv.org/abs/2512.23628)
*Shu Pu,Boya Zeng,Kaichen Zhou,Mengyu Wang,Zhuang Liu*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Generative models are increasingly used in 3D vision to synthesize novel shapes, yet it remains unclear whether their generation relies on memorizing training shapes. Understanding their memorization could help prevent training data leakage and improve the diversity of generated results. In this paper, we design an evaluation framework to quantify memorization in 3D generative models and study the influence of different data and modeling designs on memorization. We first apply our framework to quantify memorization in existing methods. Next, through controlled experiments with a latent vector-set (Vecset) diffusion model, we find that, on the data side, memorization depends on data modality, and increases with data diversity and finer-grained conditioning; on the modeling side, it peaks at a moderate guidance scale and can be mitigated by longer Vecsets and simple rotation augmentation. Together, our framework and analysis provide an empirical understanding of memorization in 3D generative models and suggest simple yet effective strategies to reduce it without degrading generation quality. Our code is available at https://github.com/zlab-princeton/3d_mem.

</details>


### [137] [Rethinking the Spatio-Temporal Alignment of End-to-End 3D Perception](https://arxiv.org/abs/2512.23635)
*Xiaoyu Li,Peidong Li,Xian Wu,Long Shi,Dedong Liu,Yitao Wu,Jiajia Fu,Dixiao Cui,Lijun Zhao,Lining Sun*

Main category: cs.CV

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Spatio-temporal alignment is crucial for temporal modeling of end-to-end (E2E) perception in autonomous driving (AD), providing valuable structural and textural prior information. Existing methods typically rely on the attention mechanism to align objects across frames, simplifying the motion model with a unified explicit physical model (constant velocity, etc.). These approaches prefer semantic features for implicit alignment, challenging the importance of explicit motion modeling in the traditional perception paradigm. However, variations in motion states and object features across categories and frames render this alignment suboptimal. To address this, we propose HAT, a spatio-temporal alignment module that allows each object to adaptively decode the optimal alignment proposal from multiple hypotheses without direct supervision. Specifically, HAT first utilizes multiple explicit motion models to generate spatial anchors and motion-aware feature proposals for historical instances. It then performs multi-hypothesis decoding by incorporating semantic and motion cues embedded in cached object queries, ultimately providing the optimal alignment proposal for the target frame. On nuScenes, HAT consistently improves 3D temporal detectors and trackers across diverse baselines. It achieves state-of-the-art tracking results with 46.0% AMOTA on the test set when paired with the DETR3D detector. In an object-centric E2E AD method, HAT enhances perception accuracy (+1.3% mAP, +3.1% AMOTA) and reduces the collision rate by 32%. When semantics are corrupted (nuScenes-C), the enhancement of motion modeling by HAT enables more robust perception and planning in the E2E AD.

</details>


### [138] [OmniAgent: Audio-Guided Active Perception Agent for Omnimodal Audio-Video Understanding](https://arxiv.org/abs/2512.23646)
*Keda Tao,Wenjie Du,Bohan Yu,Weiqiang Wang,Jian Liu,Huan Wang*

Main category: cs.CV

TL;DR: Omnimodal large language models currently lack fine-grained cross-modal understanding and multimodal alignment. OmniAgent, an audio-guided active perception agent, overcomes these limitations by dynamically orchestrating tools for fine-grained reasoning and achieving state-of-the-art performance in three benchmarks.


<details>
  <summary>Details</summary>
Motivation: 先前的多媒体模型在跨模态理解和多模态对齐方面存在不足，OmniAgent旨在通过动态规划和工具调度来增强音频-视觉推理的精细度。

Method: OmniAgent利用音频线索局部化时间事件，并引导后续推理。它采用从粗到细的音频指导感知框架，动态规划并自主调用工具以任务相关线索为焦点。

Result: OmniAgent在三个音频-视频理解基准测试中表现出色，其性能超过现有的开源和专有模型，准确性提高了10% - 20%。

Conclusion: OmniAgent提出了一个新的主动多模态查询范式，提升了音频-视觉推理的精细度，展现出卓越的性能。

Abstract: Omnimodal large language models have made significant strides in unifying audio and visual modalities; however, they often lack the fine-grained cross-modal understanding and have difficulty with multimodal alignment. To address these limitations, we introduce OmniAgent, a fully audio-guided active perception agent that dynamically orchestrates specialized tools to achieve more fine-grained audio-visual reasoning. Unlike previous works that rely on rigid, static workflows and dense frame-captioning, this paper demonstrates a paradigm shift from passive response generation to active multimodal inquiry. OmniAgent employs dynamic planning to autonomously orchestrate tool invocation on demand, strategically concentrating perceptual attention on task-relevant cues. Central to our approach is a novel coarse-to-fine audio-guided perception paradigm, which leverages audio cues to localize temporal events and guide subsequent reasoning. Extensive empirical evaluations on three audio-video understanding benchmarks demonstrate that OmniAgent achieves state-of-the-art performance, surpassing leading open-source and proprietary models by substantial margins of 10% - 20% accuracy.

</details>


### [139] [IDT: A Physically Grounded Transformer for Feed-Forward Multi-View Intrinsic Decomposition](https://arxiv.org/abs/2512.23667)
*Kang Du,Yirui Guan,Zeyu Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为IDT的前馈框架，用于多视图内在图像分解，通过基于变压器的注意力机制联合考虑多个输入图像，不需要迭代生成采样即可产生视图一致的内在因素。


<details>
  <summary>Details</summary>
Motivation: 现有的单视图反卷积方法在多视图设置中难以扩展，往往导致严重的视图不一致性。在此背景下，本文提出了IDT，旨在解决多视图中内在图像分解的视图一致性问题。

Method: IDT采用物理上合理的图像形成模型，显式分解图像为漫反射、漫反射阴影和镜面反射等组成部分，通过基于变压器的注意力机制，仅在一次前向传递中生成视图一致的内在因素。

Result: 实验结果表明，IDT在合成和真实数据集上都能够产生更清晰的漫反射、更一致的漫反射阴影和更隔离的镜面反射。

Conclusion: IDT在多视图一致性的提高上具有显著优势，有效解决了视图不一致的问题，有助于实现内在成像中的材料和照明效果的可解释和可控的分解。

Abstract: Intrinsic image decomposition is fundamental for visual understanding, as RGB images entangle material properties, illumination, and view-dependent effects. Recent diffusion-based methods have achieved strong results for single-view intrinsic decomposition; however, extending these approaches to multi-view settings remains challenging, often leading to severe view inconsistency. We propose \textbf{Intrinsic Decomposition Transformer (IDT)}, a feed-forward framework for multi-view intrinsic image decomposition. By leveraging transformer-based attention to jointly reason over multiple input images, IDT produces view-consistent intrinsic factors in a single forward pass, without iterative generative sampling. IDT adopts a physically grounded image formation model that explicitly decomposes images into diffuse reflectance, diffuse shading, and specular shading. This structured factorization separates Lambertian and non-Lambertian light transport, enabling interpretable and controllable decomposition of material and illumination effects across views. Experiments on both synthetic and real-world datasets demonstrate that IDT achieves cleaner diffuse reflectance, more coherent diffuse shading, and better-isolated specular components, while substantially improving multi-view consistency compared to prior intrinsic decomposition methods.

</details>


### [140] [Diffusion Knows Transparency: Repurposing Video Diffusion for Transparent Object Depth and Normal Estimation](https://arxiv.org/abs/2512.23705)
*Shaocong Xu,Songlin Wei,Qizhe Wei,Zheng Geng,Hong Li,Licheng Shen,Qianpu Sun,Shu Han,Bin Ma,Bohan Li,Chongjie Ye,Yuhang Zheng,Nan Wang,Saining Zhang,Hao Zhao*

Main category: cs.CV

TL;DR: 该研究通过利用现代视频扩散模型生成透明现象的能力，开发了TransPhy3D合成视频数据集，并以此为基础训练一个轻量级的视频到视频深度和法线估计模型DKT，实现了零样本SOTA性能，特别是在涉及透明性和反射性的基准测试中。


<details>
  <summary>Details</summary>
Motivation: 本研究希望将视频扩散模型在透明现象生成方面的强大能力应用于感知系统，从而解决透明物体的感知难题。

Method: 研究首先构建了TransPhy3D合成视频数据集，该数据集结合了静态和程序化资产并渲染为具有真实感的RGB、深度和法线数据。然后，基于一个大型视频扩散模型，使用轻量级的LoRA适配器训练了一个视频到视频的深度和法线估计模型。在训练过程中，还将RGB和噪声深度的潜在变量进行联合训练。

Result: 研究结果表明，DKT模型在透明性及反射性基准测试任务上展现了卓越的零样本性能，并且在ClearPose和DREDS（CatKnown/CatNovel）任务中提高了准确性和时间一致性。该模型性能优于现有的图像和视频基线，并且其一个紧凑版本的运行速度接近每帧0.17秒。

Conclusion: 该研究支持了“扩散模型懂得透明性”的广泛观点，表明生成视频先验可以被高效且无标签地重新用于现实世界操作中的鲁棒且具时序一致性感知。

Abstract: Transparent objects remain notoriously hard for perception systems: refraction, reflection and transmission break the assumptions behind stereo, ToF and purely discriminative monocular depth, causing holes and temporally unstable estimates. Our key observation is that modern video diffusion models already synthesize convincing transparent phenomena, suggesting they have internalized the optical rules. We build TransPhy3D, a synthetic video corpus of transparent/reflective scenes: 11k sequences rendered with Blender/Cycles. Scenes are assembled from a curated bank of category-rich static assets and shape-rich procedural assets paired with glass/plastic/metal materials. We render RGB + depth + normals with physically based ray tracing and OptiX denoising. Starting from a large video diffusion model, we learn a video-to-video translator for depth (and normals) via lightweight LoRA adapters. During training we concatenate RGB and (noisy) depth latents in the DiT backbone and co-train on TransPhy3D and existing frame-wise synthetic datasets, yielding temporally consistent predictions for arbitrary-length input videos. The resulting model, DKT, achieves zero-shot SOTA on real and synthetic video benchmarks involving transparency: ClearPose, DREDS (CatKnown/CatNovel), and TransPhy3D-Test. It improves accuracy and temporal consistency over strong image/video baselines, and a normal variant sets the best video normal estimation results on ClearPose. A compact 1.3B version runs at ~0.17 s/frame. Integrated into a grasping stack, DKT's depth boosts success rates across translucent, reflective and diffuse surfaces, outperforming prior estimators. Together, these results support a broader claim: "Diffusion knows transparency." Generative video priors can be repurposed, efficiently and label-free, into robust, temporally coherent perception for challenging real-world manipulation.

</details>


### [141] [Stream-DiffVSR: Low-Latency Streamable Video Super-Resolution via Auto-Regressive Diffusion](https://arxiv.org/abs/2512.23709)
*Hau-Shiang Shiu,Chin-Yang Lin,Zhixiang Wang,Chi-Wei Hsiao,Po-Fan Yu,Yu-Chih Chen,Yu-Lun Liu*

Main category: cs.CV

TL;DR: Stream-DiffVSR提出了一种用于高效在线视频超分辨率的因果条件化扩散框架，通过仅使用过去帧、快速去噪器、运动对齐的线索注入模块和时间感知解码器，显著提高了延迟敏感设置下的性能。


<details>
  <summary>Details</summary>
Motivation: 传统的基于扩散的视频超分辨率（VSR）方法在延迟敏感的应用中不实际，因为它们依赖于未来帧和昂贵的多步去噪过程。为了克服这个问题，作者提出了Stream-DiffVSR，旨在提供高效的在线视频超分辨率，同时保持高质量。

Method: Stream-DiffVSR使用四步提炼去噪器进行快速推理，结合Auto-regressive Temporal Guidance (ARTG)模块以在潜空间解噪期间注入运动对齐的线索，并采用带有Temporal Processor Module (TPM)的轻量级时间感知解码器来增强细节和时间连贯性。

Result: Stream-DiffVSR在一张RTX4090 GPU上以0.328秒的速度处理720p帧，并相比之前的基于扩散的方法显著提高了感知质量（LPIPS +0.095），同时降低了超过130倍的延迟。与在线SOTA TMP方法相比，有效减少了初始延迟从超过4600秒缩短至0.328秒，成为第一个适用于低延迟在线部署的扩散VSR方法。

Conclusion: 总之，Stream-DiffVSR通过高效利用过去信息，提升了延迟敏感设置下的视频超分辨率性能，适用于实时应用。

Abstract: Diffusion-based video super-resolution (VSR) methods achieve strong perceptual quality but remain impractical for latency-sensitive settings due to reliance on future frames and expensive multi-step denoising. We propose Stream-DiffVSR, a causally conditioned diffusion framework for efficient online VSR. Operating strictly on past frames, it combines a four-step distilled denoiser for fast inference, an Auto-regressive Temporal Guidance (ARTG) module that injects motion-aligned cues during latent denoising, and a lightweight temporal-aware decoder with a Temporal Processor Module (TPM) that enhances detail and temporal coherence. Stream-DiffVSR processes 720p frames in 0.328 seconds on an RTX4090 GPU and significantly outperforms prior diffusion-based methods. Compared with the online SOTA TMP, it boosts perceptual quality (LPIPS +0.095) while reducing latency by over 130x. Stream-DiffVSR achieves the lowest latency reported for diffusion-based VSR, reducing initial delay from over 4600 seconds to 0.328 seconds, thereby making it the first diffusion VSR method suitable for low-latency online deployment. Project page: https://jamichss.github.io/stream-diffvsr-project-page/

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [142] [Open-Source Multimodal Moxin Models with Moxin-VLM and Moxin-VLA](https://arxiv.org/abs/2512.22208)
*Pu Zhao,Xuan Shen,Zhenglun Kong,Yixin Shen,Sung-En Chang,Arash Akbari,Timothy Rupprecht,Lei Lu,Enfu Nan,Changdi Yang,Yumei He,Weiyan Shi,Xingchen Xu,Yu Huang,Wei Jiang,Wei Wang,Yue Chen,Yong He,Yanzhi Wang*

Main category: cs.CL

TL;DR: 本文介绍了一种全开源的大规模语言模型Moxin及其三个变体。Moxin遵循模型开放框架，不仅提供模型权重，还公开了训练数据和实现细节，以促进更加包容和合作的研究环境。其三个变体分别针对视觉语言、视觉语言动作以及中文能力进行优化，并在多项评估中表现出优越的性能。


<details>
  <summary>Details</summary>
Motivation: 为了推进大规模语言模型的开放性和透明度，促进科研领域的创新合作，作者团队提出了Moxin及其三个变体。项目基于开源框架进行，旨在提高模型的多功能性和灵活性。

Method: 开发团队采用了开源的框架和数据集进行训练，并通过开放源代码、数据和模型的方式提供给社区。他们开发了针对特定任务的模型变体，如Moxin-VLM、Moxin-VLA和Moxin-Chinese。

Result: 实验结果显示，Moxin及其变体在各种评估中表现卓越，特别是在视觉语言、视觉语言动作以及中文处理任务上。

Conclusion: 本文通过发布Moxin及其变体模型，促进了开放源码社区的大规模语言模型的发展，同时提供了透明的数据和训练过程，促进了共研合作的生态系统。

Abstract: Recently, Large Language Models (LLMs) have undergone a significant transformation, marked by a rapid rise in both their popularity and capabilities. Leading this evolution are proprietary LLMs like GPT-4 and GPT-o1, which have captured widespread attention in the AI community due to their remarkable performance and versatility. Simultaneously, open-source LLMs, such as LLaMA and Mistral, have made great contributions to the ever-increasing popularity of LLMs due to the ease to customize and deploy the models across diverse applications. Moxin 7B is introduced as a fully open-source LLM developed in accordance with the Model Openness Framework, which moves beyond the simple sharing of model weights to embrace complete transparency in training, datasets, and implementation detail, thus fostering a more inclusive and collaborative research environment that can sustain a healthy open-source ecosystem. To further equip Moxin with various capabilities in different tasks, we develop three variants based on Moxin, including Moxin-VLM, Moxin-VLA, and Moxin-Chinese, which target the vision-language, vision-language-action, and Chinese capabilities, respectively. Experiments show that our models achieve superior performance in various evaluations. We adopt open-source framework and open data for the training. We release our models, along with the available data and code to derive these models.

</details>


### [143] [Hierarchical Geometry of Cognitive States in Transformer Embedding Spaces](https://arxiv.org/abs/2512.22227)
*Sophie Zhao*

Main category: cs.CL

TL;DR: 研究发现，基于变换器的语言模型的嵌入空间具有与人类可解释的认知属性相一致的分级层次结构，同时这些模型对表面词汇统计不敏感。


<details>
  <summary>Details</summary>
Motivation: 为了探索句子嵌入中是否存在与人类可理解的认知或心理属性相一致的分级层次结构。

Method: 构建了一个注释数据集，包含自然语言句子及其连续的能量分数和离散的层级标签，并使用固定句子嵌入的多个变换器模型来评估通过线性和浅层非线性探针的这些标注信息可恢复性。同时使用非参数置换测试进一步验证探针的表现超过了随机标签的情况。

Result: 线性探针和浅层非线性探针均能可靠地恢复这些标注信息，浅层非线性探针在性能上优于线性探针。TF-IDF 基准较低，表明观测到的结构不是由表面词汇统计决定的。此外，非参数置换测试还进一步证实了探针表现超越了随机标签的假设。

Conclusion: 变换器嵌入空间表现出了层次化的几何结构，与人类定义的认知属性相一致，但模型对于内在意识或现象学方面保持了中立。

Abstract: Recent work has shown that transformer-based language models learn rich geometric structure in their embedding spaces, yet the presence of higher-level cognitive organization within these representations remains underexplored. In this work, we investigate whether sentence embeddings encode a graded, hierarchical structure aligned with human-interpretable cognitive or psychological attributes. We construct a dataset of 480 natural-language sentences annotated with continuous ordinal energy scores and discrete tier labels spanning seven ordered cognitive categories. Using fixed sentence embeddings from multiple transformer models, we evaluate the recoverability of these annotations via linear and shallow nonlinear probes. Across models, both continuous scores and tier labels are reliably decodable, with shallow nonlinear probes providing consistent performance gains over linear probes. Lexical TF-IDF baselines perform substantially worse, indicating that the observed structure is not attributable to surface word statistics alone. Nonparametric permutation tests further confirm that probe performance exceeds chance under label-randomization nulls. Qualitative analyses using UMAP visualizations and confusion matrices reveal smooth low-to-high gradients and predominantly adjacent-tier confusions in embedding space. Taken together, these results provide evidence that transformer embedding spaces exhibit a hierarchical geometric organization aligned with human-defined cognitive attributes, while remaining agnostic to claims of internal awareness or phenomenology.

</details>


### [144] [SmartSnap: Proactive Evidence Seeking for Self-Verifying Agents](https://arxiv.org/abs/2512.22322)
*Shaofei Cai,Yulei Qin,Haojia Lin,Zihan Xu,Gang Li,Yuchen Shi,Zongyi Li,Yong Mao,Siqi Cai,Xiaoyu Tan,Yitao Liang,Ke Li,Xing Sun*

Main category: cs.CL

TL;DR: 文章提出了一种名为SmartSnap的新方法，通过让智能体在执行任务时进行自我验证，减少了传统任务验证过程中繁琐的背景分析，从而提高了验证效率和可靠性。实验结果表明，这种新方法可以提升LLM驱动智能体的性能，尤其对于大型模型效果显著。


<details>
  <summary>Details</summary>
Motivation: 现有的任务验证方法效率低下且可靠性不足，SmartSnap通过引导智能体自我验证来改进这一问题，旨在解决大规模和复杂GUI任务中智能体验证的挑战。

Method: SmartSnap方法基于两种核心机制：（1）一个具有双重任务的智能体，不仅要完成任务还要提供证明；（2）遵循3C原则（完整、简洁、创造）的自验证过程，利用在线环境生成关键的最小化证据快照，以及通用大语言模型作为裁判验证这些证据的有效性和相关性。

Result: 在移动任务中进行的实验显示，SmartSnap方法使得LLM驱动的智能体训练变得更具可扩展性，8B和30B模型分别取得了高达26.08%和16.66%的性能提升。结合解解决方案和寻找证据的过程，还培养出了高效、自我验证的智能体，与其竞争者相比，性能也非常突出。

Conclusion: SmartSnap为理解智能体是否成功完成任务提供了一种新的、更具主动性和效率的方法，对于开发自主智能体具有重要意义。

Abstract: Agentic reinforcement learning (RL) holds great promise for the development of autonomous agents under complex GUI tasks, but its scalability remains severely hampered by the verification of task completion. Existing task verification is treated as a passive, post-hoc process: a verifier (i.e., rule-based scoring script, reward or critic model, and LLM-as-a-Judge) analyzes the agent's entire interaction trajectory to determine if the agent succeeds. Such processing of verbose context that contains irrelevant, noisy history poses challenges to the verification protocols and therefore leads to prohibitive cost and low reliability. To overcome this bottleneck, we propose SmartSnap, a paradigm shift from this passive, post-hoc verification to proactive, in-situ self-verification by the agent itself. We introduce the Self-Verifying Agent, a new type of agent designed with dual missions: to not only complete a task but also to prove its accomplishment with curated snapshot evidences. Guided by our proposed 3C Principles (Completeness, Conciseness, and Creativity), the agent leverages its accessibility to the online environment to perform self-verification on a minimal, decisive set of snapshots. Such evidences are provided as the sole materials for a general LLM-as-a-Judge verifier to determine their validity and relevance. Experiments on mobile tasks across model families and scales demonstrate that our SmartSnap paradigm allows training LLM-driven agents in a scalable manner, bringing performance gains up to 26.08% and 16.66% respectively to 8B and 30B models. The synergizing between solution finding and evidence seeking facilitates the cultivation of efficient, self-verifying agents with competitive performance against DeepSeek V3.1 and Qwen3-235B-A22B.

</details>


### [145] [The Syntax of qulk-clauses in Yemeni Ibbi Arabic: A Minimalist Approach](https://arxiv.org/abs/2512.22376)
*Zubaida Mohammed Albadani,Mohammed Q. Shormani*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: This study investigates the syntax of qulk-clauses in Yemeni Ibbi Arabic (YIA) within the Minimalist Program. The construction qulk-clause, a morphologically fused form meaning 'I said,' introduces embedded declarative interrogative, and imperative clauses, often eithout complementizer. The central proposal of this paper is that qulk-clauses are biclausal structures in which qulk functions a clause-embedding predicate sec;ecting a dull CP complement. By applying core minimalist operations, viz., Merge, Move, Agree, and Spell-out, the study provides a layered syntactic analysis of qulk-clauses, for illustrating how their derivation proceeds through standard computational steps and post-syntactic processes such as Morphological Merger. The proposal also accounts for dialect-specific features like bipartite negation, cliticization, and CP embedding. The findings offer theoretical contributions to generative syntax, specifically minimalism. The study concludes raising theoretical questions concerning extending the analysis to the addressee-clause kil-k 'you said'. It also provides insights into the possibility of the universality of minimalism.

</details>


### [146] [Towards Efficient Post-Training via Fourier-Driven Adapter Architectures](https://arxiv.org/abs/2512.22378)
*Donggyun Bae,Jongil Park*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: We propose a novel framework, termed Fourier-Activated Adapter (FAA), for parameter-efficient fine-tuning of large pre-trained language models. By incorporating random Fourier features into lightweight adapter modules, FAA decomposes intermediate representations into complementary low- and high-frequency components, enabling frequency-aware modulation of semantic information. This design allows the model to selectively emphasize informative frequency bands during adaptation while preserving the representational capacity of the frozen backbone. Extensive experiments on GLUE, E2E NLG, and instruction-tuning benchmarks demonstrate that FAA consistently achieves competitive or superior performance compared to existing parameter-efficient fine-tuning methods, while maintaining low computational and memory overhead. Ablation studies further verify the effectiveness of frequency-aware activation and adaptive weighting mechanisms, highlighting FAA as a robust and efficient approach for post-training large language models.

</details>


### [147] [LLM-Guided Exemplar Selection for Few-Shot Wearable-Sensor Human Activity Recognition](https://arxiv.org/abs/2512.22385)
*Elsen Ronando,Sozo Inoue*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: In this paper, we propose an LLM-Guided Exemplar Selection framework to address a key limitation in state-of-the-art Human Activity Recognition (HAR) methods: their reliance on large labeled datasets and purely geometric exemplar selection, which often fail to distinguish similar weara-ble sensor activities such as walking, walking upstairs, and walking downstairs. Our method incorporates semantic reasoning via an LLM-generated knowledge prior that captures feature importance, inter-class confusability, and exemplar budget multipliers, and uses it to guide exemplar scoring and selection. These priors are combined with margin-based validation cues, PageRank centrality, hubness penalization, and facility-location optimization to obtain a compact and informative set of exemplars. Evaluated on the UCI-HAR dataset under strict few-shot conditions, the framework achieves a macro F1-score of 88.78%, outperforming classical approaches such as random sampling, herding, and $k$-center. The results show that LLM-derived semantic priors, when integrated with structural and geometric cues, provide a stronger foundation for selecting representative sensor exemplars in few-shot wearable-sensor HAR.

</details>


### [148] [Hallucination Detection and Evaluation of Large Language Model](https://arxiv.org/abs/2512.22416)
*Chenggong Zhang,Haopeng Wang*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Hallucinations in Large Language Models (LLMs) pose a significant challenge, generating misleading or unverifiable content that undermines trust and reliability. Existing evaluation methods, such as KnowHalu, employ multi-stage verification but suffer from high computational costs. To address this, we integrate the Hughes Hallucination Evaluation Model (HHEM), a lightweight classification-based framework that operates independently of LLM-based judgments, significantly improving efficiency while maintaining high detection accuracy. We conduct a comparative analysis of hallucination detection methods across various LLMs, evaluating True Positive Rate (TPR), True Negative Rate (TNR), and Accuracy on question-answering (QA) and summarization tasks. Our results show that HHEM reduces evaluation time from 8 hours to 10 minutes, while HHEM with non-fabrication checking achieves the highest accuracy \(82.2\%\) and TPR \(78.9\%\). However, HHEM struggles with localized hallucinations in summarization tasks. To address this, we introduce segment-based retrieval, improving detection by verifying smaller text components. Additionally, our cumulative distribution function (CDF) analysis indicates that larger models (7B-9B parameters) generally exhibit fewer hallucinations, while intermediate-sized models show higher instability. These findings highlight the need for structured evaluation frameworks that balance computational efficiency with robust factual validation, enhancing the reliability of LLM-generated content.

</details>


### [149] [HiFi-RAG: Hierarchical Content Filtering and Two-Pass Generation for Open-Domain RAG](https://arxiv.org/abs/2512.22442)
*Cattalyya Nuengsigkapian*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Retrieval-Augmented Generation (RAG) in open-domain settings faces significant challenges regarding irrelevant information in retrieved documents and the alignment of generated answers with user intent. We present HiFi-RAG (Hierarchical Filtering RAG), the winning closed-source system in the Text-to-Text static evaluation of the MMU-RAGent NeurIPS 2025 Competition. Our approach moves beyond standard embedding-based retrieval via a multi-stage pipeline. We leverage the speed and cost-efficiency of Gemini 2.5 Flash (4-6x cheaper than Pro) for query formulation, hierarchical content filtering, and citation attribution, while reserving the reasoning capabilities of Gemini 2.5 Pro for final answer generation. On the MMU-RAGent validation set, our system outperformed the baseline, improving ROUGE-L to 0.274 (+19.6%) and DeBERTaScore to 0.677 (+6.2%). On Test2025, our custom dataset evaluating questions that require post-cutoff knowledge (post January 2025), HiFi-RAG outperforms the parametric baseline by 57.4% in ROUGE-L and 14.9% in DeBERTaScore.

</details>


### [150] [Exploring the Vertical-Domain Reasoning Capabilities of Large Language Models](https://arxiv.org/abs/2512.22443)
*Jie Zhou,Xin Chen,Jie Zhang,Zhe Li*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large Language Models (LLMs) are reshaping learning paradigms, cognitive processes, and research methodologies across a wide range of domains. Integrating LLMs with professional fields and redefining the relationship between LLMs and domain-specific applications has become a critical challenge for promoting enterprise digital transformation and broader social development. To effectively integrate LLMs into the accounting domain, it is essential to understand their domain-specific reasoning capabilities. This study introduces the concept of vertical-domain accounting reasoning and establishes evaluation criteria by analyzing the training data characteristics of representative GLM-series models. These criteria provide a foundation for subsequent research on reasoning paradigms and offer benchmarks for improving accounting reasoning performance. Based on this framework, we evaluate several representative models, including GLM-6B, GLM-130B, GLM-4, and OpenAI GPT-4, on a set of accounting reasoning tasks. Experimental results show that different prompt engineering strategies lead to varying degrees of performance improvement across models, with GPT-4 achieving the strongest accounting reasoning capability. However, current LLMs still fall short of real-world application requirements. In particular, further optimization is needed for deployment in enterprise-level accounting scenarios to fully realize the potential value of LLMs in this domain.

</details>


### [151] [Constituency Structure over Eojeol in Korean Treebanks](https://arxiv.org/abs/2512.22487)
*Jungyeul Park,Chulwoo Park*

Main category: cs.CL

TL;DR: 本文讨论了韩语成分树库设计中终端单位的选择，提出使用eojeol为基础的成分表示，同时在独立层次中编码形态学分段和细颗粒度词性信息，为跨树库比较和成分依赖转换提供了支持。


<details>
  <summary>Details</summary>
Motivation: 韩语词汇具有复杂的形态学特征，将形态学单元作为成分终端会混淆单词内部形态结构与短语层次的句法结构，导致与基于eojeol的依赖资源不匹配。

Method: 提出了使用eojeol为基础的成分表示方法，通过形态学分段和细颗粒度词性信息的独立层次表示来解决上述问题，并通过比较分析验证了这种方法的有效性。

Result: 在明确定义的规范化假设下，Sejong和Penn韩语树库在基于eojeol的成分层次上可以视为等价的表示。

Conclusion: 提出了一种基于eojeol的注释方案，能够保留可解释的成分信息，并支持跨树库比较和成分依赖转换。

Abstract: The design of Korean constituency treebanks raises a fundamental representational question concerning the choice of terminal units. Although Korean words are morphologically complex, treating morphemes as constituency terminals conflates word internal morphology with phrase level syntactic structure and creates mismatches with eojeol based dependency resources. This paper argues for an eojeol based constituency representation, with morphological segmentation and fine grained part of speech information encoded in a separate, non constituent layer. A comparative analysis shows that, under explicit normalization assumptions, the Sejong and Penn Korean treebanks can be treated as representationally equivalent at the eojeol based constituency level. Building on this result, we outline an eojeol based annotation scheme that preserves interpretable constituency and supports cross treebank comparison and constituency dependency conversion.

</details>


### [152] [Learning When Not to Attend Globally](https://arxiv.org/abs/2512.22562)
*Xuan Luo,Kailai Zhang,Xifeng Yan*

Main category: cs.CL

TL;DR: All-or-Here Attention (AHA) 提出了一种新的注意力机制，通过动态选择滑动窗口注意力或全局注意，显著降低了全注意力操作的消耗，潜在地提高了模型的效率。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探索大型语言模型在处理长文本时如何有效地重用信息，而不是每次处理都要全局检索所有之前的信息，从而提高性能

Method: 该研究提出了一种名为 AHA 的新机制。AHA 使用二元路由器动态切换每个注意力头的局部滑动窗口注意力和全局全注意力。研究通过不同窗口大小的评估来探索上下文依赖的分布。

Result: 实验结果显示，使用256个令牌的窗口，约有93%的原始全注意力操作可以通过滑动窗口注意力替代而不会影响性能。在不同窗口大小的评估中，发现上下文依赖分布呈现出长尾分布，随着局部窗口的扩大，全注意力的需求快速降低。

Conclusion: 研究表明，全注意力在许多情况下是冗余的，仅需按需访问全局上下文即可实现高效的推理。AHA 可以显著减少全注意力操作的消耗，从而提高模型的效率。

Abstract: When reading books, humans focus primarily on the current page, flipping back to recap prior context only when necessary. Similarly, we demonstrate that Large Language Models (LLMs) can learn to dynamically determine when to attend to global context. We propose All-or-Here Attention (AHA), which utilizes a binary router per attention head to dynamically toggle between full attention and local sliding window attention for each token. Our results indicate that with a window size of 256 tokens, up to 93\% of the original full attention operations can be replaced by sliding window attention without performance loss. Furthermore, by evaluating AHA across various window sizes, we identify a long-tail distribution in context dependency, where the necessity for full attention decays rapidly as the local window expands. By decoupling local processing from global access, AHA reveals that full attention is largely redundant, and that efficient inference requires only on-demand access to the global context.

</details>


### [153] [Structured Prompting and LLM Ensembling for Multimodal Conversational Aspect-based Sentiment Analysis](https://arxiv.org/abs/2512.22603)
*Zhiqiang Gao,Shihao Gao,Zixing Zhang,Yihao Guo,Hongyu Chen,Jing Han*

Main category: cs.CL

TL;DR: 本文针对多模态对话中的小样本多方面情感分析（MCABSA）挑战，设计了逐步提示管道和三个人工智能模型的集成方法，分别完成了情感组件的提取和情感切换的检测。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能系统越来越注重情感智能，理解多模态对话中的情感成为了一个关键问题。因此，本文参与了MCABSA挑战，旨在提高情感智能AI系统的性能。

Method: 首先，设计了一个逐步提示管道，以指导大语言模型（LLMs）按顺序提取情感组件，增强上下文理解。其次，通过集成三种不同的人工智能模型，利用它们各自的优点，稳健地识别情感转变及其触发因素。

Result: 该系统在子任务I中取得了47.38%的平均得分，在子任务II中实现了74.12%的精确匹配F1分数，展示了逐步细化策略和集成方法在丰富多模态情感分析任务中的有效性。

Conclusion: 本文提出了逐步提示管道和模型集成方法，有效地解决了MCABSA挑战的两个亚任务，为未来多模态情感分析的研究提供了有益的方法和思路。

Abstract: Understanding sentiment in multimodal conversations is a complex yet crucial challenge toward building emotionally intelligent AI systems. The Multimodal Conversational Aspect-based Sentiment Analysis (MCABSA) Challenge invited participants to tackle two demanding subtasks: (1) extracting a comprehensive sentiment sextuple, including holder, target, aspect, opinion, sentiment, and rationale from multi-speaker dialogues, and (2) detecting sentiment flipping, which detects dynamic sentiment shifts and their underlying triggers. For Subtask-I, in the present paper, we designed a structured prompting pipeline that guided large language models (LLMs) to sequentially extract sentiment components with refined contextual understanding. For Subtask-II, we further leveraged the complementary strengths of three LLMs through ensembling to robustly identify sentiment transitions and their triggers. Our system achieved a 47.38% average score on Subtask-I and a 74.12% exact match F1 on Subtask-II, showing the effectiveness of step-wise refinement and ensemble strategies in rich, multimodal sentiment analysis tasks.

</details>


### [154] [Chain-of-thought Reviewing and Correction for Time Series Question Answering](https://arxiv.org/abs/2512.22627)
*Chen Su,Yuanhe Tian,Yan Song*

Main category: cs.CL

TL;DR: T3LLM 提出了一种利用三个语言模型进行多步推理和错误修正的时间序列问题回答框架。


<details>
  <summary>Details</summary>
Motivation: 现有的 LLM 方法在处理复杂数值序列时容易出错，因此提出 T3LLM 以利用时间序列数据可验证的特性进行多步推理和自我修正。

Method: T3LLM 包含三个模型：生成器（负责生成思维链）、审阅者（检查推理并提供改正意见）和学生（根据协作生成的改正后的思维链进行微调）。

Result: 在多个时间序列问题回答基准上的实验结果表明，T3LLM 在与强大 LLM 基线模型的比较中达到了最先进的性能。

Conclusion: T3LLM 通过引入一个完善的自我纠正机制提供了处理时间序列数据的有效方法。

Abstract: With the advancement of large language models (LLMs), diverse time series analysis tasks are reformulated as time series question answering (TSQA) through a unified natural language interface. However, existing LLM-based approaches largely adopt general natural language processing techniques and are prone to reasoning errors when handling complex numerical sequences. Different from purely textual tasks, time series data are inherently verifiable, enabling consistency checking between reasoning steps and the original input. Motivated by this property, we propose T3LLM, which performs multi-step reasoning with an explicit correction mechanism for time series question answering. The T3LLM framework consists of three LLMs, namely, a worker, a reviewer, and a student, that are responsible for generation, review, and reasoning learning, respectively. Within this framework, the worker generates step-wise chains of thought (CoT) under structured prompts, while the reviewer inspects the reasoning, identifies erroneous steps, and provides corrective comments. The collaboratively generated corrected CoT are used to fine-tune the student model, internalizing multi-step reasoning and self-correction into its parameters. Experiments on multiple real-world TSQA benchmarks demonstrate that T3LLM achieves state-of-the-art performance over strong LLM-based baselines.

</details>


### [155] [M2G-Eval: Enhancing and Evaluating Multi-granularity Multilingual Code Generation](https://arxiv.org/abs/2512.22628)
*Fanglin Xu,Wei Zhang,Jian Yang,Guo Chen,Aishan Liu,Zhoujun Li,Xianglong Liu,Bryan Dai*

Main category: cs.CL

TL;DR: M2G-Eval 是一个多粒度、多语言框架，用于评估代码生成能力，跨四个级别：类、函数、代码块和行。该框架通过详细的任务和评测实例区分了不同代码范围的细粒度能力差异，并揭示了编程概念的跨语言传递性。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要评估单一结构粒度和有限编程语言的模型，未能全面覆盖代码范围和多语言场景。M2G-Eval 旨在填补这一空白，提供更精细和全面的代码生成能力评估。

Method: M2G-Eval 包含 18 种编程语言的 17000 多个训练任务和 1286 个人工注释、无污染的评测实例。通过监督微调 Qwen3-8B 和组相对策略优化开发了 M2G-Eval-Coder 模型。评测了 30 种模型以揭示代码生成难度等级、性能差距和跨语言相关性。

Result: 研究揭示了三个主要发现：(1) 从行级到类级逐渐增加的难度等级；(2) 随着任务复杂度增加，全粒度和部分粒度语言之间的性能差距扩大；(3) 强烈的跨语言相关性表明模型学习到可迁移的编程概念。

Conclusion: M2G-Eval 增强了对代码生成能力的细化诊断和对复杂长代码合成问题持续挑战的理解。

Abstract: The rapid advancement of code large language models (LLMs) has sparked significant research interest in systematically evaluating their code generation capabilities, yet existing benchmarks predominantly assess models at a single structural granularity and focus on limited programming languages, obscuring fine-grained capability variations across different code scopes and multilingual scenarios. We introduce M2G-Eval, a multi-granularity, multilingual framework for evaluating code generation in large language models (LLMs) across four levels: Class, Function, Block, and Line. Spanning 18 programming languages, M2G-Eval includes 17K+ training tasks and 1,286 human-annotated, contamination-controlled test instances. We develop M2G-Eval-Coder models by training Qwen3-8B with supervised fine-tuning and Group Relative Policy Optimization. Evaluating 30 models (28 state-of-the-art LLMs plus our two M2G-Eval-Coder variants) reveals three main findings: (1) an apparent difficulty hierarchy, with Line-level tasks easiest and Class-level most challenging; (2) widening performance gaps between full- and partial-granularity languages as task complexity increases; and (3) strong cross-language correlations, suggesting that models learn transferable programming concepts. M2G-Eval enables fine-grained diagnosis of code generation capabilities and highlights persistent challenges in synthesizing complex, long-form code.

</details>


### [156] [On the Role of Discreteness in Diffusion LLMs](https://arxiv.org/abs/2512.22630)
*Ziqi Jin,Bin Wang,Xiang Lin,Lidong Bing,Aixin Sun*

Main category: cs.CL

TL;DR: 此研究重新审视了从扩散过程视角的语言模型，并提出了五个区分扩散机制与语言特定需求的属性。研究指出现有方法在连续嵌入空间和标记级扩散上的局限，并提出应更紧密地与文本结构对齐，以解决信息分布不均和多标记依赖的问题。


<details>
  <summary>Details</summary>
Motivation: 作者认为扩散模型在语言生成中具有吸引人的特性，如并行解码和迭代 refinement，但由于文本的离散和高度结构化，直接应用扩散模型存在挑战。因此，该研究旨在改善扩散语言模型。

Method: 研究首先将现有方法分为嵌入空间中的连续扩散和标记级扩散，然后分析了大量现代的扩散语言模型，明确了两个关键问题，并提出了改进策略。

Result: 该研究指出了现有扩散语言模型的两个核心问题，并提出了改善策略，鼓励未来研究开发更一致的扩散语言模型。

Conclusion: 研究承认现有扩散模型之间的结构性权衡，提出应更紧密地与文本结构对齐，解决信息分布不均和多标记依赖问题，以提高模型性能和一致性。

Abstract: Diffusion models offer appealing properties for language generation, such as parallel decoding and iterative refinement, but the discrete and highly structured nature of text challenges the direct application of diffusion principles. In this paper, we revisit diffusion language modeling from the view of diffusion process and language modeling, and outline five properties that separate diffusion mechanics from language-specific requirements. We first categorize existing approaches into continuous diffusion in embedding space and discrete diffusion over tokens. We then show that each satisfies only part of the five essential properties and therefore reflects a structural trade-off. Through analyses of recent large diffusion language models, we identify two central issues: (i) uniform corruption does not respect how information is distributed across positions, and (ii) token-wise marginal training cannot capture multi-token dependencies during parallel decoding. These observations motivate diffusion processes that align more closely with the structure of text, and encourage future work toward more coherent diffusion language models.

</details>


### [157] [Evaluating GRPO and DPO for Faithful Chain-of-Thought Reasoning in LLMs](https://arxiv.org/abs/2512.22631)
*Hadi Mohammadi,Tamas Kozak,Anastasia Giachanou*

Main category: cs.CL

TL;DR: 研究评估了两种优化方法（Group Relative Policy Optimization 和 Direct Preference Optimization）来提高因果推理（CoT）的可靠性，结果显示GRPO在大型模型中表现更优。


<details>
  <summary>Details</summary>
Motivation: 研究旨在更好地理解chain-of-thought推理的限制，并评估不同的优化方法以提高其可靠性，从而为大型语言模型的安全监督和对齐监控提供更透明和可信的推理。

Method: 实验使用了两种优化方法，组相对策略优化（GRPO）和直接偏好优化（DPO），对大型语言模型进行评估。

Result: GRPO在更大规模的模型中表现出更高的性能，特别是在Qwen2.5-14B-Instruct模型上取得了最佳结果。方法与模型规模之间存在正相关关系，但GRPO在提高可靠性指标方面表现出更大的潜力，并在较小规模模型中表现出不稳定行为。

Conclusion: 研究结果表明，GRPO为开发更透明和可信的大型语言模型推理提供了有前景的方向。

Abstract: Chain-of-thought (CoT) reasoning has emerged as a powerful technique for improving the problem-solving capabilities of large language models (LLMs), particularly for tasks requiring multi-step reasoning. However, recent studies show that CoT explanations often fail to reflect the model's actual reasoning process, as models may produce coherent yet misleading justifications or modify answers without acknowledging external cues. Such discrepancies undermine the reliability of CoT-based methods for safety supervision and alignment monitoring, as models can generate plausible but deceptive rationales for incorrect answers. To better understand this limitation, we evaluate two optimization methods, Group Relative Policy Optimization (GRPO) and Direct Preference Optimization (DPO), in their ability to improve CoT faithfulness. Our experiments show that GRPO achieves higher performance than DPO in larger models, with the Qwen2.5-14B-Instruct model attaining the best results across all evaluation metrics. Both approaches exhibit positive correlations between model size and performance, but GRPO shows greater potential for improving faithfulness metrics, albeit with less stable behavior at smaller scales. These results suggest that GRPO offers a promising direction for developing more transparent and trustworthy reasoning in LLMs.

</details>


### [158] [Conformal Prediction Sets for Next-Token Prediction in Large Language Models: Balancing Coverage Guarantees with Set Efficiency](https://arxiv.org/abs/2512.22682)
*Yoshith Roy Kotla,Varshith Roy Kotla*

Main category: cs.CL

TL;DR: 本文研究了在大规模词汇量的变压器模型中应用自适应预测集（APS）进行下一个标记预测的问题。提出了词汇意识自适应预测框架（VACP），通过语义掩码和温度调整评分来有效减少预测空间，同时保持边际覆盖率。实验表明，VACP为SQUAD和WikiText基准数据集达到了89.7%的经验覆盖率（目标90%），并且预测集大小显著减小，实现效率提高了197倍。


<details>
  <summary>Details</summary>
Motivation: 在高风险领域部署大规模语言模型需要对不确定性进行严格的量化，而传统的softmax概率往往校准不良。本文的动机在于探索一种新的方法，即应用自适应预测集（APS）来改进预测集的校准性和信息性。

Method: 本文提出了一种新的方法——词汇意识自适应预测框架（VACP），结合语义掩码和温度调整评分，这种方法在维持边际覆盖率的同时有效减少了预测集的大小。

Result: 实验结果表明，使用VACP方法在Gemma-2B上进行预测时，能达到90%的目标覆盖率，同时将预测集的平均大小从847个词核减到4.3个词，效率提升了197倍。

Conclusion: 本文展示了VACP方法在保持高覆盖率的同时，极大地提高了预测集的效率，证明了其在改进语言模型预测集的有效性和实用性。

Abstract: Deploying large language models (LLMs) in high-stakes domains requires rigorous uncertainty quantification, yet standard softmax probabilities are often poorly calibrated. We present a systematic study of Adaptive Prediction Sets (APS) applied to next-token prediction in transformer-based models with large vocabularies (greater than 250,000 tokens). Our central contribution is the identification of a coverage-efficiency tradeoff: while naive conformal prediction achieves valid coverage, it produces prediction sets of hundreds of tokens, rendering them uninformative. We propose Vocabulary-Aware Conformal Prediction (VACP), a framework that leverages semantic masking and temperature-adjusted scoring to reduce the effective prediction space while provably maintaining marginal coverage. Experiments on Gemma-2B using SQUAD and WikiText benchmarks demonstrate that VACP achieves 89.7 percent empirical coverage (90 percent target) while reducing the mean prediction set size from 847 tokens to 4.3 tokens -- a 197x improvement in efficiency. We provide a theoretical analysis of vocabulary reduction and release our implementation for reproducibility.

</details>


### [159] [GHaLIB: A Multilingual Framework for Hope Speech Detection in Low-Resource Languages](https://arxiv.org/abs/2512.22705)
*Ahmed Abdullah,Sana Fatima,Haroon Mahmood*

Main category: cs.CL

TL;DR: 该论文提出了一个针对低资源语言如乌尔都语的多语言希望言论检测框架，利用预训练的变压器模型实现了较好的性能。


<details>
  <summary>Details</summary>
Motivation: 自然语言处理领域希望言论的研究相对较少，而针对低资源语言如乌尔都语的相关资源更是稀缺。这限制了开发支持积极在线沟通的工具。现有基于变压器的模型虽然在检测仇恨言论和冒犯言论方面具有有效性，但在希望言论检测领域的应用还未广泛展开。

Method: 该研究利用了XLM-RoBERTa、mBERT、EuroBERT和UrduBERT等预训练的变压器模型，并对其进行了简单的预处理和训练分类器。

Result: 该研究在PolyHope-M 2025基准测试中表现良好，乌尔都语二分类的F1分数为95.2%，多分类的F1分数为65.2%，西班牙语、德语和英语表现也较为竞争。

Conclusion: 该研究证明了使用现有跨语言模型在低资源环境中应用的可能性，有助于更有效地识别希望言论，从而促进建设性的数字交流。

Abstract: Hope speech has been relatively underrepresented in Natural Language Processing (NLP). Current studies are largely focused on English, which has resulted in a lack of resources for low-resource languages such as Urdu. As a result, the creation of tools that facilitate positive online communication remains limited. Although transformer-based architectures have proven to be effective in detecting hate and offensive speech, little has been done to apply them to hope speech or, more generally, to test them across a variety of linguistic settings. This paper presents a multilingual framework for hope speech detection with a focus on Urdu. Using pretrained transformer models such as XLM-RoBERTa, mBERT, EuroBERT, and UrduBERT, we apply simple preprocessing and train classifiers for improved results. Evaluations on the PolyHope-M 2025 benchmark demonstrate strong performance, achieving F1-scores of 95.2% for Urdu binary classification and 65.2% for Urdu multi-class classification, with similarly competitive results in Spanish, German, and English. These results highlight the possibility of implementing existing multilingual models in low-resource environments, thus making it easier to identify hope speech and helping to build a more constructive digital discourse.

</details>


### [160] [Beg to Differ: Understanding Reasoning-Answer Misalignment Across Languages](https://arxiv.org/abs/2512.22712)
*Anaelia Ovalle,Candace Ross,Sebastian Ruder,Adina Williams,Karen Ullrich,Mark Ibrahim,Levent Sagun*

Main category: cs.CL

TL;DR: 研究发现，尽管多语言模型在任务准确率上表现出色，但它们的推理链在非拉丁语系语言中与结论之间的逻辑对齐程度较低，主要由于证据不足和不合理的推理步骤。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在多语言环境下的推理能力是否能够跨语言保持高质量，当前的多语言评估方法是否提供了模型推理能力的完整图景。

Method: 通过人类验证的框架，对65000个推理链进行了跨6种语言和6个前沿模型的分析。

Result: 揭示了模型推理链在非拉丁语系语言与结论之间的逻辑对齐较差的问题，尤其是证据不足和不合理的推理步骤。

Conclusion: 表明当前的多语言评估方式并不能完整反映模型的推理能力，提出了需要构建新的推理意识评估框架的需求。

Abstract: Large language models demonstrate strong reasoning capabilities through chain-of-thought prompting, but whether this reasoning quality transfers across languages remains underexplored. We introduce a human-validated framework to evaluate whether model-generated reasoning traces logically support their conclusions across languages. Analyzing 65k reasoning traces from GlobalMMLU questions across 6 languages and 6 frontier models, we uncover a critical blind spot: while models achieve high task accuracy, their reasoning can fail to support their conclusions. Reasoning traces in non-Latin scripts show at least twice as much misalignment between their reasoning and conclusions than those in Latin scripts. We develop an error taxonomy through human annotation to characterize these failures, finding they stem primarily from evidential errors (unsupported claims, ambiguous facts) followed by illogical reasoning steps. Our findings demonstrate that current multilingual evaluation practices provide an incomplete picture of model reasoning capabilities and highlight the need for reasoning-aware evaluation frameworks.

</details>


### [161] [Mitigating Social Desirability Bias in Random Silicon Sampling](https://arxiv.org/abs/2512.22725)
*Sashank Chapala,Maksym Mironov,Songgaojun Deng*

Main category: cs.CL

TL;DR: 本研究通过使用不同的提示方法，以减少大语言模型在模拟人群响应时出现的社会可接受性偏见，并通过詹森-沙恩堡分散度评估与真实人类数据的接近程度。


<details>
  <summary>Details</summary>
Motivation: 由于大语言模型在模拟社会敏感问题时表现出社会可接受性偏见，本研究旨在探索减少这种偏见的提示方法，以使模拟样本更接近真实人类数据。

Method: 研究使用来自美国全国选举研究（ANES）的数据，在三种不同类型的大小语言模型（包括开源Llama-3.1系列和GPT-4.1-mini）上测试了四种不同类型的提示方法来减小程序样本的社会可接受性偏见。评估方法是使用双曲夹更广偏差和启动置信区间进行衡量。

Result: 研究发现重新表述的提示方法最有效地降低了社会可接受性答案的分布集中度，并实现了更接近ANES的分布。反编码在符合条件的项目中取得了混合效果，而提示和前言鼓励了一致性回答但没有表现出系统性的去偏见效果。

Conclusion: 研究验证了基于提示的框架控制措施的有效性，以减轻大语言模型中固有的社会可接受性偏见，为更具有代表性的程序样本提供了实用路线。

Abstract: Large Language Models (LLMs) are increasingly used to simulate population responses, a method known as ``Silicon Sampling''. However, responses to socially sensitive questions frequently exhibit Social Desirability Bias (SDB), diverging from real human data toward socially acceptable answers. Existing studies on social desirability bias in LLM-based sampling remain limited. In this work, we investigate whether minimal, psychologically grounded prompt wording can mitigate this bias and improve alignment between silicon and human samples. We conducted a study using data from the American National Election Study (ANES) on three LLMs from two model families: the open-source Llama-3.1 series and GPT-4.1-mini. We first replicate a baseline silicon sampling study, confirming the persistent Social Desirability Bias. We then test four prompt-based mitigation methods: \emph{reformulated} (neutral, third-person phrasing), \emph{reverse-coded} (semantic inversion), and two meta-instructions, \emph{priming} and \emph{preamble}, respectively encouraging analytics and sincerity. Alignment with ANES is evaluated using Jensen-Shannon Divergence with bootstrap confidence intervals. Our results demonstrate that reformulated prompts most effectively improve alignment by reducing distribution concentration on socially acceptable answers and achieving distributions closer to ANES. Reverse-coding produced mixed results across eligible items, while the Priming and Preamble encouraged response uniformity and showed no systematic benefit for bias mitigation. Our findings validate the efficacy of prompt-based framing controls in mitigating inherent Social Desirability Bias in LLMs, providing a practical path toward more representative silicon samples.

</details>


### [162] [Data Augmentation for Classification of Negative Pregnancy Outcomes in Imbalanced Data](https://arxiv.org/abs/2512.22732)
*Md Badsha Biswas*

Main category: cs.CL

TL;DR: 该研究利用公开社交媒体数据，特别是来自Twitter的数据，通过自然语言处理方法，识别并分类分享怀孕经历的女性，旨在改善对不良妊娠结果的研究，并评估特定干预措施的影响。


<details>
  <summary>Details</summary>
Motivation: 由于天然死亡（如流产、死产、出生缺陷和早产）是婴儿死亡的主要原因，而且当前对不良妊娠结果的研究和干预策略仍有改进空间，因此利用社交媒体数据增强研究来源变得尤为重要。

Method: 该研究通过构建自然语言处理（NLP）流水线，自动识别分享怀孕经历的女性，并基于报告结果对他们进行分类（完全妊娠和正常出生体重定义为阳性案例；报告不良妊娠结果的定义为负面案例）。同时，研究还探索了特定干预措施对母婴健康结果的因果影响，并提出了未来健康研究的框架。

Result: 该方法能够提供潜在的干预措施对母婴健康结果的评估，展示社交媒体数据在流行病学调查中作为辅助资源的可行性。

Conclusion: 研究认为，利用社交媒体数据增强了不良妊娠结果研究的广度和深度，提供了新的视角和工具，为未来的研究奠定了基础。

Abstract: Infant mortality remains a significant public health concern in the United States, with birth defects identified as a leading cause. Despite ongoing efforts to understand the causes of negative pregnancy outcomes like miscarriage, stillbirths, birth defects, and premature birth, there is still a need for more comprehensive research and strategies for intervention. This paper introduces a novel approach that uses publicly available social media data, especially from platforms like Twitter, to enhance current datasets for studying negative pregnancy outcomes through observational research. The inherent challenges in utilizing social media data, including imbalance, noise, and lack of structure, necessitate robust preprocessing techniques and data augmentation strategies. By constructing a natural language processing (NLP) pipeline, we aim to automatically identify women sharing their pregnancy experiences, categorizing them based on reported outcomes. Women reporting full gestation and normal birth weight will be classified as positive cases, while those reporting negative pregnancy outcomes will be identified as negative cases. Furthermore, this study offers potential applications in assessing the causal impact of specific interventions, treatments, or prenatal exposures on maternal and fetal health outcomes. Additionally, it provides a framework for future health studies involving pregnant cohorts and comparator groups. In a broader context, our research showcases the viability of social media data as an adjunctive resource in epidemiological investigations about pregnancy outcomes.

</details>


### [163] [WeDLM: Reconciling Diffusion Language Models with Standard Causal Attention for Fast Inference](https://arxiv.org/abs/2512.22737)
*Aiwei Liu,Minghua He,Shaoxun Zeng,Sijun Zhang,Linhao Zhang,Chuhan Wu,Wei Jia,Yuan Liu,Xiao Zhou,Jie Zhou*

Main category: cs.CL

TL;DR: WeDLM通过基于标准因果注意力的平行生成框架，解决了Diffusion语言模型的效率问题，实现了与AR引擎相当的推理速度，某些情况下甚至快3倍。


<details>
  <summary>Details</summary>
Motivation: 现有的Diffusion语言模型虽然能够实现并行推理，但依赖双向注意机制导致无法有效利用标准的前缀缓存机制，降低了实际部署效果。

Method: WeDLM提出了一种基于标准因果注意力的解码框架，通过顶学重排序法，使得观察到的令牌保持逻辑位置同时被物理移动到前缀上，进而提出了一种流式解码方法，持续将自信的令牌添加到从左到右增长的前缀中并保持固定的并行工作负载。

Result: WeDLM在保持高质量推理的同时，为强AR基线模型提供了显著的速度提升，特别是在低熵生成环境中，速度可提高至10倍。

Conclusion: WeDLM通过创新的缓存友好解码方法，展示了在优化的AR引擎面前，Diffusion解码方法在实际应用中也能表现出优越的性能。

Abstract: Autoregressive (AR) generation is the standard decoding paradigm for Large Language Models (LLMs), but its token-by-token nature limits parallelism at inference time. Diffusion Language Models (DLLMs) offer parallel decoding by recovering multiple masked tokens per step; however, in practice they often fail to translate this parallelism into deployment speed gains over optimized AR engines (e.g., vLLM). A key reason is that many DLLMs rely on bidirectional attention, which breaks standard prefix KV caching and forces repeated contextualization, undermining efficiency. We propose WeDLM, a diffusion decoding framework built entirely on standard causal attention to make parallel generation prefix-cache friendly. The core idea is to let each masked position condition on all currently observed tokens while keeping a strict causal mask, achieved by Topological Reordering that moves observed tokens to the physical prefix while preserving their logical positions. Building on this property, we introduce a streaming decoding procedure that continuously commits confident tokens into a growing left-to-right prefix and maintains a fixed parallel workload, avoiding the stop-and-wait behavior common in block diffusion methods. Experiments show that WeDLM preserves the quality of strong AR backbones while delivering substantial speedups, approaching 3x on challenging reasoning benchmarks and up to 10x in low-entropy generation regimes; critically, our comparisons are against AR baselines served by vLLM under matched deployment settings, demonstrating that diffusion-style decoding can outperform an optimized AR engine in practice.

</details>


### [164] [Fake News Classification in Urdu: A Domain Adaptation Approach for a Low-Resource Language](https://arxiv.org/abs/2512.22778)
*Muhammad Zain Ali,Bernhard Pfahringer,Tony Smith*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Misinformation on social media is a widely acknowledged issue, and researchers worldwide are actively engaged in its detection. However, low-resource languages such as Urdu have received limited attention in this domain. An obvious approach is to utilize a multilingual pretrained language model and fine-tune it for a downstream classification task, such as misinformation detection. However, these models struggle with domain-specific terms, leading to suboptimal performance. To address this, we investigate the effectiveness of domain adaptation before fine-tuning for fake news classification in Urdu, employing a staged training approach to optimize model generalization. We evaluate two widely used multilingual models, XLM-RoBERTa and mBERT, and apply domain-adaptive pretraining using a publicly available Urdu news corpus. Experiments on four publicly available Urdu fake news datasets show that domain-adapted XLM-R consistently outperforms its vanilla counterpart, while domain-adapted mBERT exhibits mixed results.

</details>


### [165] [CNSight: Evaluation of Clinical Note Segmentation Tools](https://arxiv.org/abs/2512.22795)
*Risha Surana,Adrian Law,Sunwoo Kim,Rishab Sridhar,Angxiao Han,Peiyu Hong*

Main category: cs.CL

TL;DR: 该研究利用一个包含1000份MIMIC-IV笔记的定制数据集，评估了基于规则的基线、领域特定的transformer模型以及大型语言模型在临床笔记分段任务上的表现，表明大型API驱动的模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 当前临床记录多以非结构化或半结构化的形式存储，这对二次分析和临床应用构成挑战。可靠地识别段落边界对结构化该记录至关重要。因此，该研究旨在提升临床笔记段落识别的准确性。

Method: 研究团队使用了一个含1000张MIMIC-IV笔记的定制数据集，来评估基于规则的基线模型、领域特定的变压器模型及大型语言模型在临床笔记分段任务上的性能。

Result: 大型API驱动的语言模型在分段任务中表现最佳，尤其是GPT-5-mini，在句子级别和自由文本分段上的平均F1值达到了72.4。轻量级基线在有结构的句子级别任务中表现不错，但在无结构的自由文本任务上表现欠佳。

Conclusion: 研究结果支持选择适当的分段方法，并为下游任务如信息提取、患者组识别和自动摘要提供了基础。

Abstract: Clinical notes are often stored in unstructured or semi-structured formats after extraction from electronic medical record (EMR) systems, which complicates their use for secondary analysis and downstream clinical applications. Reliable identification of section boundaries is a key step toward structuring these notes, as sections such as history of present illness, medications, and discharge instructions each provide distinct clinical contexts. In this work, we evaluate rule-based baselines, domain-specific transformer models, and large language models for clinical note segmentation using a curated dataset of 1,000 notes from MIMIC-IV. Our experiments show that large API-based models achieve the best overall performance, with GPT-5-mini reaching a best average F1 of 72.4 across sentence-level and freetext segmentation. Lightweight baselines remain competitive on structured sentence-level tasks but falter on unstructured freetext. Our results provide guidance for method selection and lay the groundwork for downstream tasks such as information extraction, cohort identification, and automated summarization.

</details>


### [166] [AutoForge: Automated Environment Synthesis for Agentic Reinforcement Learning](https://arxiv.org/abs/2512.22857)
*Shihao Cai,Runnan Fang,Jialong Wu,Baixuan Li,Xinyu Wang,Yong Jiang,Liangcai Su,Liwen Zhang,Wenbiao Yin,Zhen Zhang,Fuli Feng,Pengjun Xie,Xiaobin Wang*

Main category: cs.CL

TL;DR: 本文提出了一种统一管道，用于自动化和可扩展地合成含有高难度但易于验证任务的模拟环境，并且还提出了一种环境层面的RL算法，该算法有效地缓解了模拟用户的不稳定性，同时还能够在环境层面进行优势估计，提高了训练效率和稳定性。


<details>
  <summary>Details</summary>
Motivation: 由于现有的RL方法在模拟环境中所面临的任务难度不足且合成环境不稳定，本研究旨在解决这些挑战，提供一个更加统一和高效的强化学习框架。

Method: 该方法包含两个主要部分：一是设计了一个自动化和可扩展的合成高难度任务的模拟环境的管道；二是开发了一个环境层面的RL算法，该算法可以在环境层面上实现优势估计，同时帮助缓解模拟用户不稳定性带来的问题。

Result: 研究的结果表明，提出的管道可以生成高质量的模拟环境和任务，环境层面的RL算法可以有效的提高训练效率和稳定性能。

Conclusion: 本研究提出的管道和环境层面的RL算法有效解决了模拟环境中RL任务的挑战，使得基于模拟环境进行的RL研究更能具有实际应用意义，并且具有跨领域推广的潜力。

Abstract: Conducting reinforcement learning (RL) in simulated environments offers a cost-effective and highly scalable way to enhance language-based agents. However, previous work has been limited to semi-automated environment synthesis or tasks lacking sufficient difficulty, offering little breadth or depth. In addition, the instability of simulated users integrated into these environments, along with the heterogeneity across simulated environments, poses further challenges for agentic RL. In this work, we propose: (1) a unified pipeline for automated and scalable synthesis of simulated environments associated with high-difficulty but easily verifiable tasks; and (2) an environment level RL algorithm that not only effectively mitigates user instability but also performs advantage estimation at the environment level, thereby improving training efficiency and stability. Comprehensive evaluations on agentic benchmarks, including tau-bench, tau2-Bench, and VitaBench, validate the effectiveness of our proposed method. Further in-depth analyses underscore its out-of-domain generalization.

</details>


### [167] [Diversity or Precision? A Deep Dive into Next Token Prediction](https://arxiv.org/abs/2512.22955)
*Haoyuan Wu,Hai Wang,Jiajia Wu,Jinxiang Ou,Keyao Wang,Weile Chen,Zihao Zheng,Bei Yu*

Main category: cs.CL

TL;DR: 该研究重新审视了预先标准化交叉熵损失，并将其视为单步骤时期望值下的策略梯度优化的一种特定实例。通过提出一种广义预训练目标，结合监督学习和在线策略优化的原则，并设计了一个奖励重塑策略，该策略平衡了多样性和精确性。实验表明，相对于增强探索性的直觉，精度导向的先验为强化学习提供了更好的探索空间。


<details>
  <summary>Details</summary>
Motivation: 近年来，强化学习（RL）在大型语言模型（LLMs）的推理能力改进方面取得了显著进展。然而，这种RL训练的有效性取决于预训练模型的令牌输出分布。为此，研究提出了一种广义预训练目标，旨在优化LLMs的推理性能。

Method: 研究通过将下一个令牌预测重新形成本分优势策略下的随机决策过程，并设计了奖励重塑策略来平衡多样性和精确性。该策略通过正向奖励调整因子控制对真实令牌概率的集中，并从排名感知机制不对等地对待高排名和低排名的负令牌。该方法允许重新调整预训练的令牌-输出分布，从而调查如何为强化学习提供更有利的探索空间。

Result: 实验结果表明，采用精度导向先验进行了广泛的探索实验，其性能优于熵导向的均衡方案。这意味着增强RL训练效果的关键不是增加分布的不确定性，而是利用精确性的先验信息来优化模型的推理空间。

Conclusion: 研究结论表明，广义预训练可以将监督学习（SL）和在线策略优化（PPO）集成，通过平衡多样性与精度，提供一种更适合强化学习的探索空间，从而显著增强最终端到端的推理性能。

Abstract: Recent advancements have shown that reinforcement learning (RL) can substantially improve the reasoning abilities of large language models (LLMs). The effectiveness of such RL training, however, depends critically on the exploration space defined by the pre-trained model's token-output distribution. In this paper, we revisit the standard cross-entropy loss, interpreting it as a specific instance of policy gradient optimization applied within a single-step episode. To systematically study how the pre-trained distribution shapes the exploration potential for subsequent RL, we propose a generalized pre-training objective that adapts on-policy RL principles to supervised learning. By framing next-token prediction as a stochastic decision process, we introduce a reward-shaping strategy that explicitly balances diversity and precision. Our method employs a positive reward scaling factor to control probability concentration on ground-truth tokens and a rank-aware mechanism that treats high-ranking and low-ranking negative tokens asymmetrically. This allows us to reshape the pre-trained token-output distribution and investigate how to provide a more favorable exploration space for RL, ultimately enhancing end-to-end reasoning performance. Contrary to the intuition that higher distribution entropy facilitates effective exploration, we find that imposing a precision-oriented prior yields a superior exploration space for RL.

</details>


### [168] [Prompt engineering does not universally improve Large Language Model performance across clinical decision-making tasks](https://arxiv.org/abs/2512.22966)
*Mengdi Chai,Ali R. Zomorrodi*

Main category: cs.CL

TL;DR: 本研究评估了三种最先进的语言模型在临床决策支持中的表现，并探索通过提示工程优化其性能的方法。


<details>
  <summary>Details</summary>
Motivation: 探索大规模语言模型在实际临床决策中的应用潜力，以及提示工程是否能有效提升模型性能。

Method: 研究在5个关键的临床决策任务上评估了三个最先进的语言模型的表现，并通过提示工程技术尝试提升模型性能。

Result: 研究发现，不同模型在不同任务上的表现存在很大差异，提示工程技术对于提升性能的效果取决于任务和模型特性。

Conclusion: 提示工程在提升模型性能方面并非通用方案，需结合具体任务和模型特性定制策略。

Abstract: Large Language Models (LLMs) have demonstrated promise in medical knowledge assessments, yet their practical utility in real-world clinical decision-making remains underexplored. In this study, we evaluated the performance of three state-of-the-art LLMs-ChatGPT-4o, Gemini 1.5 Pro, and LIama 3.3 70B-in clinical decision support across the entire clinical reasoning workflow of a typical patient encounter. Using 36 case studies, we first assessed LLM's out-of-the-box performance across five key sequential clinical decision-making tasks under two temperature settings (default vs. zero): differential diagnosis, essential immediate steps, relevant diagnostic testing, final diagnosis, and treatment recommendation. All models showed high variability by task, achieving near-perfect accuracy in final diagnosis, poor performance in relevant diagnostic testing, and moderate performance in remaining tasks. Furthermore, ChatGPT performed better under the zero temperature, whereas LIama showed stronger performance under the default temperature. Next, we assessed whether prompt engineering could enhance LLM performance by applying variations of the MedPrompt framework, incorporating targeted and random dynamic few-shot learning. The results demonstrate that prompt engineering is not a one-size-fit-all solution. While it significantly improved the performance on the task with lowest baseline accuracy (relevant diagnostic testing), it was counterproductive for others. Another key finding was that the targeted dynamic few-shot prompting did not consistently outperform random selection, indicating that the presumed benefits of closely matched examples may be counterbalanced by loss of broader contextual diversity. These findings suggest that the impact of prompt engineering is highly model and task-dependent, highlighting the need for tailored, context-aware strategies for integrating LLMs into healthcare.

</details>


### [169] [Improving Generalization in LLM Structured Pruning via Function-Aware Neuron Grouping](https://arxiv.org/abs/2512.23014)
*Tao Yu,Yongqi An,Kuan Zhu,Guibo Zhu,Ming Tang,Jinqiao Wang*

Main category: cs.CL

TL;DR: FANG是一种后训练剪枝框架，旨在通过识别并保留对特定功能至关重要的神经元来减轻校准偏差，从而提高下游任务的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的后训练结构化剪枝方法在小样本校准集不能很好地反映预训练数据分布时，显示出有限的下游任务泛化能力。

Method: FANG方法基于处理的语义上下文类型对具有相似功能的神经元进行分组，并独立修剪每个组。在每个组内的重要性估计过程中，与神经元组功能角色关联性强的标记被赋予更高的权重。此外，FANG还保留了在多种上下文类型中做出贡献的神经元。它通过根据功能复杂性适应性地分配稀疏性来实现稀疏性和性能之间的更好权衡。

Result: 实验表明，FANG在保持语言建模性能的同时提高了下游准确性。与其他剪枝方法结合使用时，FANG在30%和40%稀疏性下的平均准确性分别优于FLAP和OBC 1.5%-8.5%。

Conclusion: FANG证明了在特定功能关键的神经元中减轻校准偏差的能力，并在结合FLAP和OBC后实现了最先进的性能。

Abstract: Large Language Models (LLMs) demonstrate impressive performance across natural language tasks but incur substantial computational and storage costs due to their scale. Post-training structured pruning offers an efficient solution. However, when few-shot calibration sets fail to adequately reflect the pretraining data distribution, existing methods exhibit limited generalization to downstream tasks. To address this issue, we propose Function-Aware Neuron Grouping (FANG), a post-training pruning framework that alleviates calibration bias by identifying and preserving neurons critical to specific function. FANG groups neurons with similar function based on the type of semantic context they process and prunes each group independently. During importance estimation within each group, tokens that strongly correlate with the functional role of the neuron group are given higher weighting. Additionally, FANG also preserves neurons that contribute across multiple context types. To achieve a better trade-off between sparsity and performance, it allocates sparsity to each block adaptively based on its functional complexity. Experiments show that FANG improves downstream accuracy while preserving language modeling performance. It achieves the state-of-the-art (SOTA) results when combined with FLAP and OBC, two representative pruning methods. Specifically, FANG outperforms FLAP and OBC by 1.5%--8.5% in average accuracy under 30% and 40% sparsity.

</details>


### [170] [LENS: LLM-Enabled Narrative Synthesis for Mental Health by Aligning Multimodal Sensing with Language Models](https://arxiv.org/abs/2512.23025)
*Wenxuan Xu,Arvind Pillai,Subigya Nepal,Amanda C Collins,Daniel M Mackin,Michael V Heinz,Tess Z Griffin,Nicholas C Jacobson,Andrew Campbell*

Main category: cs.CL

TL;DR: LENS框架通过将生态时刻评估（EMA）响应转换为自然语言描述，构建了一个大规模的传感器-文本数据集，并训练了一个切片级编码器，将其直接投影到LLM的表示空间中，以解决长时序传感器数据处理问题，从而改善了LLM对行为信号的理解，并展示了其在精神健康评估中的应用。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型难以直接处理长时间的传感器数据流，且缺乏传感器-文本数据集。为解决这些挑战，开发LENS框架。

Method: 1. 将EMA响应转换为自然语言描述，构建大规模传感器-文本数据集；2. 训练切片级编码器，将原始传感器信号直接投影到LLM的表示空间。

Result: LENS在标准NLP指标和特定症状严重性准确性方面优于基准模型，并在用户的临床专家研究中显示出生成详尽且临床相关的叙事。

Conclusion: 该研究方法推进了LLM作为健康传感接口的发展，提供了一条将模型应用于处理原生行为信号并支持临床决策的道路。

Abstract: Multimodal health sensing offers rich behavioral signals for assessing mental health, yet translating these numerical time-series measurements into natural language remains challenging. Current LLMs cannot natively ingest long-duration sensor streams, and paired sensor-text datasets are scarce. To address these challenges, we introduce LENS, a framework that aligns multimodal sensing data with language models to generate clinically grounded mental-health narratives. LENS first constructs a large-scale dataset by transforming Ecological Momentary Assessment (EMA) responses related to depression and anxiety symptoms into natural-language descriptions, yielding over 100,000 sensor-text QA pairs from 258 participants. To enable native time-series integration, we train a patch-level encoder that projects raw sensor signals directly into an LLM's representation space. Our results show that LENS outperforms strong baselines on standard NLP metrics and task-specific measures of symptom-severity accuracy. A user study with 13 mental-health professionals further indicates that LENS-produced narratives are comprehensive and clinically meaningful. Ultimately, our approach advances LLMs as interfaces for health sensing, providing a scalable path toward models that can reason over raw behavioral signals and support downstream clinical decision-making.

</details>


### [171] [Is Chain-of-Thought Really Not Explainability? Chain-of-Thought Can Be Faithful without Hint Verbalization](https://arxiv.org/abs/2512.23032)
*Kerem Zaman,Shashank Srivastava*

Main category: cs.CL

TL;DR: 该研究质疑了Biasing Features使用的偏置特征衡量模型思维过程的忠实性，并提出使用新的 faithful@k 指标来评估。研究结果表明，更宽松的推理时长限制可以显著增加思维过程对提示摘要的表达，暗示了先前被认为不忠实的思维过程可能是由于剪枝导致的。研究还应用因果中介分析，揭示非语言化的提示仍能通过思维过程影响预测结果。因此，研究建议避免仅依赖基于提示的评估方法，而是采用更全面的可解释性工具包。


<details>
  <summary>Details</summary>
Motivation: 研究者认为现有的不忠实性评估方法会被模型用来压缩分布计算的推理过程错误地标记为不忠实，而这些过程至关重要，变化可以通过因果中介分析验证。

Method: 研究者提出了一个新的 faithful@k 指标来评估模型的思维过程，并通过对比基于提示的方法与其他指标的评估结果，研究者发现使用更宽松的推理限制可以改善思维过程的表达。此外，研究还利用因果中介分析验证了提示在模型预测中的潜在影响。

Result: 实验结果表明，更宽松的推理限制可以显著增加思维过程对提示摘要的表达，特别是在Llama-3和Gemma-3模型上。通过因果中介分析，研究还证明了即使非语言化的提示也可以通过思维过程影响预测结果。

Conclusion: 研究结论为避免仅依赖基于提示的评估手段和推荐采用更广泛的可解释性工具包提供了证据。

Abstract: Recent work, using the Biasing Features metric, labels a CoT as unfaithful if it omits a prompt-injected hint that affected the prediction. We argue this metric confuses unfaithfulness with incompleteness, the lossy compression needed to turn distributed transformer computation into a linear natural language narrative. On multi-hop reasoning tasks with Llama-3 and Gemma-3, many CoTs flagged as unfaithful by Biasing Features are judged faithful by other metrics, exceeding 50% in some models. With a new faithful@k metric, we show that larger inference-time token budgets greatly increase hint verbalization (up to 90% in some settings), suggesting much apparent unfaithfulness is due to tight token limits. Using Causal Mediation Analysis, we further show that even non-verbalized hints can causally mediate prediction changes through the CoT. We therefore caution against relying solely on hint-based evaluations and advocate a broader interpretability toolkit, including causal mediation and corruption-based metrics.

</details>


### [172] [Accelerating Language Model Workflows with Prompt Choreography](https://arxiv.org/abs/2512.23049)
*TJ Bai,Jason Eisner*

Main category: cs.CL

TL;DR: 该研究提出了一种名为Prompt Choreography的框架，通过维护动态全局的键值缓存来高效执行大语言模型的工作流。这显著减少了消息的延迟，并在一些工作流中实现了端到端的加速。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在多智能体工作流中的广泛应用，提高这类模型的工作流效率变得非常重要。因此，本文提出了一种新的机制——Prompt Choreography来提高大语言模型的工作流执行效率。

Method: Prompt Choreography 通过维护一个动态的、全局的键值缓存来实现其功能。每个语言模型调用可以处理之前编码的消息中任意重组的子集。此外，框架还支持并发调用。研究中指出，虽然缓存消息编码有时会与重新编码的结果不同，但通过微调语言模型以适应缓存，可以使模型模仿原始结果。

Result: 实验结果显示，使用 Prompt Choreography 可以显著减少每条消息的延迟（速度快2.0到6.2倍），并在某些主要由冗余计算支配的工作流中实现了端到端速度提高（超过2.2倍）。

Conclusion: Prompt Choreography 通过优化大语言模型的工作流执行过程，在多智能体工作流中有巨大应用潜力。

Abstract: Large language models are increasingly deployed in multi-agent workflows. We introduce Prompt Choreography, a framework that efficiently executes LLM workflows by maintaining a dynamic, global KV cache. Each LLM call can attend to an arbitrary, reordered subset of previously encoded messages. Parallel calls are supported. Though caching messages' encodings sometimes gives different results from re-encoding them in a new context, we show in diverse settings that fine-tuning the LLM to work with the cache can help it mimic the original results. Prompt Choreography significantly reduces per-message latency (2.0--6.2$\times$ faster time-to-first-token) and achieves substantial end-to-end speedups ($>$2.2$\times$) in some workflows dominated by redundant computation.

</details>


### [173] [TabiBERT: A Large-Scale ModernBERT Foundation Model and Unified Benchmarking Framework for Turkish](https://arxiv.org/abs/2512.23065)
*Melikşah Türker,A. Ebrar Kızıloğlu,Onur Güngör,Susan Üsküdarlı*

Main category: cs.CL

TL;DR: TabiBERT是一个基于ModernBERT架构的源生土耳其语编码器，在大规模语料上微调，具有较长的上下文长度和高效性，实现了在多个任务上的领先性能。


<details>
  <summary>Details</summary>
Motivation: 当前土耳其NLP缺乏一个基于现代架构（如RoPE、FlashAttention等）的源生训练模型，TabiBERT填补了这个空白。

Method: TabiBERT采用了ModernBERT架构并融入了RoPE等技术，微调数据涵盖多个领域，支持大上下文长度，并通过优化提升推理速度和减少GPU内存使用。

Result: TabiBERT在TabiBench上达到了77.58的得分，超过BERTurk 1.62分，在五个任务上达到SOTA；相较于特定任务的最佳结果，TabiBERT的平均改进为+1.47，显示了广泛的领域泛化能力。

Conclusion: 该研究展示了TabiBERT的优越性能，并开放了模型权重、训练配置和评估代码，促进了透明和可重复的土耳其语编码器研究。

Abstract: Since the inception of BERT, encoder-only Transformers have evolved significantly in computational efficiency, training stability, and long-context modeling. ModernBERT consolidates these advances by integrating Rotary Positional Embeddings (RoPE), FlashAttention, and refined normalization. Despite these developments, Turkish NLP lacks a monolingual encoder trained from scratch incorporating such modern architectural paradigms. This work introduces TabiBERT, a monolingual Turkish encoder based on ModernBERT architecture trained from scratch on a large, curated corpus. TabiBERT is pre-trained on one trillion tokens sampled from an 84.88B token multi-domain corpus: web text (73%), scientific publications (20%), source code (6%), and mathematical content (0.3%). The model supports 8,192-token context length (16x original BERT), achieves up to 2.65x inference speedup, and reduces GPU memory consumption, enabling larger batch sizes. We introduce TabiBench with 28 datasets across eight task categories with standardized splits and protocols, evaluated using GLUE-style macro-averaging. TabiBERT attains 77.58 on TabiBench, outperforming BERTurk by 1.62 points and establishing state-of-the-art on five of eight categories: question answering (+9.55), code retrieval (+2.41), and document retrieval (+0.60). Compared with task-specific prior best results, including specialized models like TurkishBERTweet, TabiBERT achieves +1.47 average improvement, indicating robust cross-domain generalization. We release model weights, training configurations, and evaluation code for transparent, reproducible Turkish encoder research.

</details>


### [174] [Reservoir Computing inspired Matrix Multiplication-free Language Model](https://arxiv.org/abs/2512.23145)
*Takumi Shiratsuchi,Yuichiro Tanaka,Hakaru Tamukoh*

Main category: cs.CL

TL;DR: 本文提出了一种矩阵乘法自由的语言模型，并通过借鉴水库计算架构进一步减少了训练成本。实验结果表明，该架构减少了参数数量、降低了训练时间和推理时间，同时保持了与基线模型相当的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然在自然语言处理任务中表现出色，但高昂的计算成本是其应用的重要瓶颈。为解决这一问题，本文通过构建矩阵乘法自由的模型并通过引入水库层来减少计算开销。

Method: 本文采用了一种部分固定并共享部分层权重的方法，并结合多个操作减少内存访问。同时引入了基于水库计算的架构来获得丰富的动态表示。

Result: 实验结果表明，与基线模型相比，该架构减少了参数数量高达19%，训练时间减少了9.9%，推理时间减少了8.0%，同时保持了相当的性能。

Conclusion: 本文提出的方法在保证性能的前提下有效降低了计算成本，为大规模语言模型的广泛应用提供了新的思路。

Abstract: Large language models (LLMs) have achieved state-of-the-art performance in natural language processing; however, their high computational cost remains a major bottleneck. In this study, we target computational efficiency by focusing on a matrix multiplication free language model (MatMul-free LM) and further reducing the training cost through an architecture inspired by reservoir computing. Specifically, we partially fix and share the weights of selected layers in the MatMul-free LM and insert reservoir layers to obtain rich dynamic representations without additional training overhead. Additionally, several operations are combined to reduce memory accesses. Experimental results show that the proposed architecture reduces the number of parameters by up to 19%, training time by 9.9%, and inference time by 8.0%, while maintaining comparable performance to the baseline model.

</details>


### [175] [Not too long do read: Evaluating LLM-generated extreme scientific summaries](https://arxiv.org/abs/2512.23206)
*Zhuoqi Lyu,Qing Ke*

Main category: cs.CL

TL;DR: 本研究提出了一种称为BiomedTLDR的新数据集，包含大量科研人员撰写的高质量科学总结，旨在评估LLM在生成摘要方面的性能。实验结果显示，尽管某些LLM能够生成类人化的摘要，但整体而言，LLM倾向于更加提取性，而不是抽象性，它们对原始文本的词汇选择和修辞结构表现出了更大的偏好。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏高质量的科学极端总结数据集，阻碍了LLM总结能力的发展和评估，因此提出了BiomedTLDR数据集，以促进科学知识的有效传播。

Method: 本研究提出了BiomedTLDR数据集，并使用流行的开源LLM基于论文摘要生成TLDR。通过分析生成的总结与人类撰写的总结，研究者评估了LLM的总结能力。

Result: 研究发现，尽管部分LLM能够生成类人的TLDR，但总体而言，它们更偏向于提取性，更多地保留原始摘要的词汇和结构，较少展现出人类撰写的总结的抽象性和创造力。

Conclusion: 该研究强调了构建高质量的数据集对于评估LLM性能的重要性，并指出未来的改进方向应关注使LLM生成更具抽象性和创新性的总结。

Abstract: High-quality scientific extreme summary (TLDR) facilitates effective science communication. How do large language models (LLMs) perform in generating them? How are LLM-generated summaries different from those written by human experts? However, the lack of a comprehensive, high-quality scientific TLDR dataset hinders both the development and evaluation of LLMs' summarization ability. To address these, we propose a novel dataset, BiomedTLDR, containing a large sample of researcher-authored summaries from scientific papers, which leverages the common practice of including authors' comments alongside bibliography items. We then test popular open-weight LLMs for generating TLDRs based on abstracts. Our analysis reveals that, although some of them successfully produce humanoid summaries, LLMs generally exhibit a greater affinity for the original text's lexical choices and rhetorical structures, hence tend to be more extractive rather than abstractive in general, compared to humans. Our code and datasets are available at https://github.com/netknowledge/LLM_summarization (Lyu and Ke, 2025).

</details>


### [176] [Scoring, Reasoning, and Selecting the Best! Ensembling Large Language Models via a Peer-Review Process](https://arxiv.org/abs/2512.23213)
*Zhijun Chen,Zeyu Ji,Qianren Mao,Junhang Cheng,Bangjie Qin,Hao Wu,Zhuoran Li,Jingzheng Li,Kai Sun,Zizhe Wang,Yikun Ban,Zhu Sun,Xiangyang Ji,Hailong Sun*

Main category: cs.CL

TL;DR: LLM-PeerReview利用多个不同强项的LLM生成的候选响应，通过评估、推理和最终选择最高分的响应来增强模型性能，且无需监督，具有灵活性和概括性。


<details>
  <summary>Details</summary>
Motivation: 在自然语言处理领域，LLM在生成高质量响应方面显得力不从心，特别是在面对多个生成结果时，难以选择最符合要求的响应。本文提出了一种无需监督、灵活适应且具有概括性的LLM-PeerReview方法，以改善这一问题。

Method: LLM-PeerReview首先使用新兴的LLM-as-a-Judge技术重新利用现有LLM评估每个响应，然后通过原理性的图模型式真推理算法或简单的平均策略聚合多个评分来生成最终评分，最后选择最高分的响应作为最佳群体输出。

Result: 经过四个数据集的实验证明，LLM-PeerReview两种变体均取得了优异的结果，相较于最近的先进模型Smoothie-Global，分别提高了6.9%和7.3%的效果。

Conclusion: 本文提出的LLM-PeerReview方法具有简单而强大的概念，在多个数据集上表现出色，为改进LLM生成响应提供了新的思路。

Abstract: We propose LLM-PeerReview, an unsupervised LLM Ensemble method that selects the most ideal response from multiple LLM-generated candidates for each query, harnessing the collective wisdom of multiple models with diverse strengths. LLM-PeerReview is built on a novel, peer-review-inspired framework that offers a clear and interpretable mechanism, while remaining fully unsupervised for flexible adaptability and generalization. Specifically, it operates in three stages: For scoring, we use the emerging LLM-as-a-Judge technique to evaluate each response by reusing multiple LLMs at hand; For reasoning, we can apply a principled graphical model-based truth inference algorithm or a straightforward averaging strategy to aggregate multiple scores to produce a final score for each response; Finally, the highest-scoring response is selected as the best ensemble output. LLM-PeerReview is conceptually simple and empirically powerful. The two variants of the proposed approach obtain strong results across four datasets, including outperforming the recent advanced model Smoothie-Global by 6.9% and 7.3% points, respectively.

</details>


### [177] [Anka: A Domain-Specific Language for Reliable LLM Code Generation](https://arxiv.org/abs/2512.23214)
*Saif Khalfan Saif Al Mazrouei*

Main category: cs.CL

TL;DR: 大型语言模型在复杂多步骤编程任务中表现不佳，这研究表明，一种具有明确语法结构的数据转换管道专用语言（Anka）能够显著提高生成代码的准确性，尤其是针对多步骤任务。通过这种方法，LLMs可以从上下文提示中学习新型DSL，实现接近于人类的准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）已经展示了在代码生成方面的出色能力，但在复杂的多步骤编程任务上仍表现出系统性的错误。研究者推测这种错误源于通用编程语言的灵活性，这种语言允许多种有效的解决方案，同时需要隐式的状态管理。为验证这一假设，研究者开发了Anka，一种专门为数据转换管道设计的领域特定语言，具有明确的语法约束。

Method: 研究者通过引入具有明确语法约束的领域特定语言Anka并利用大型语言模型Claude 3.5 Haiku进行实验，评估其在复杂的多步骤编程任务上的表现。此外，还利用GPT-4o-mini作为对照组进行交叉验证。

Result: 大型语言模型在使用Anka设计的专用语言后，对多步骤任务的准确度从60%提高到了100%，表现出显著提高。而相比之下，使用普通编程语言Python的表现不佳，仅能达到60%的准确度。研究进一步验证了Anka在准确度上的表现优于普通语言。

Conclusion: 研究结果表明，大型语言模型可以在仅通过上下文提示的情况下学习全新的DSL，从而实现接近原生的准确性。此外，具有明确语法约束的DSL显著减少了复杂任务中的错误数量。这一发现还表明，专门设计的领域特定语言能够在某些任务上优于通用编程语言，尤其是当这些通用语言对于复杂的任务表现不佳时。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in code generation, yet they exhibit systematic errors on complex, multi-step programming tasks. We hypothesize that these errors stem from the flexibility of general-purpose languages, which permits multiple valid approaches and requires implicit state management. To test this hypothesis, we introduce Anka, a domain-specific language (DSL) for data transformation pipelines designed with explicit, constrained syntax that reduces ambiguity in code generation. Despite having zero prior training exposure to Anka, Claude 3.5 Haiku achieves 99.9% parse success and 95.8% overall task accuracy across 100 benchmark problems. Critically, Anka demonstrates a 40 percentage point accuracy advantage over Python on multi-step pipeline tasks (100% vs. 60%), where Python's flexible syntax leads to frequent errors in operation sequencing and variable management. Cross-model validation with GPT-4o-mini confirms this advantage (+26.7 percentage points on multi-step tasks). Our results demonstrate that: (1) LLMs can learn novel DSLs entirely from in-context prompts, achieving near-native accuracy; (2) constrained syntax significantly reduces errors on complex tasks; and (3) domain-specific languages purposefully designed for LLM generation can outperform general-purpose languages on which the LLM has extensive training. We release the complete language implementation, benchmark suite, and evaluation framework to facilitate further research.

</details>


### [178] [Interpretable Safety Alignment via SAE-Constructed Low-Rank Subspace Adaptation](https://arxiv.org/abs/2512.23260)
*Dianyun Wang,Qingsen Ma,Yuhu Shang,Zhifeng Lu,Lechen Ning,Zhenbo Xu,Huijia Wu,Zhaofeng He*

Main category: cs.CL

TL;DR: 本文提出了一种通过预训练稀疏自编码器（SAEs）识别任务相关特征并构造显式低秩子空间的方法，用于引导适配器初始化。该方法在保持高安全率的同时，仅更新少量参数，并提供对学习到的对齐子空间的可解释性见解。


<details>
  <summary>Details</summary>
Motivation: 传统低秩适应方法（如LoRA）虽然有效，但参数更新是黑盒方式进行的，缺乏可解释性。本文旨在解决这一限制，并提供了理论分析证明了在单义性假设下的SAE基适应空间识别方法能够实现几乎无误差的恢复，而在多义性空间直接识别会面临不可克服的误差下限。

Method: 本文利用预训练的稀疏自编码器识别任务相关特征，并在解耦特征空间中构造显式低秩子空间来引导适配器的初始化。

Result: 本文的方法在安全对齐方面达到了99.6%的安全率，相较于全量微调高出7.4个百分点，接近于基于RLHF的方法，同时仅更新了0.19-0.24%的参数。

Conclusion: 本文证明了通过引入机制上的可解释性到微调过程中，可以在提升性能的同时增强透明度。

Abstract: Parameter-efficient fine-tuning has become the dominant paradigm for adapting large language models to downstream tasks. Low-rank adaptation methods such as LoRA operate under the assumption that task-relevant weight updates reside in a low-rank subspace, yet this subspace is learned implicitly from data in a black-box manner, offering no interpretability or direct control. We hypothesize that this difficulty stems from polysemanticity--individual dimensions encoding multiple entangled concepts. To address this, we leverage pre-trained Sparse Autoencoders (SAEs) to identify task-relevant features in a disentangled feature space, then construct an explicit, interpretable low-rank subspace to guide adapter initialization. We provide theoretical analysis proving that under monosemanticity assumptions, SAE-based subspace identification achieves arbitrarily small recovery error, while direct identification in polysemantic space suffers an irreducible error floor. On safety alignment, our method achieves up to 99.6% safety rate--exceeding full fine-tuning by 7.4 percentage points and approaching RLHF-based methods--while updating only 0.19-0.24% of parameters. Crucially, our method provides interpretable insights into the learned alignment subspace through the semantic grounding of SAE features. Our work demonstrates that incorporating mechanistic interpretability into the fine-tuning process can simultaneously improve both performance and transparency.

</details>


### [179] [Chinese Morph Resolution in E-commerce Live Streaming Scenarios](https://arxiv.org/abs/2512.23280)
*Jiahao Zhu,Jipeng Qiang,Ran Bai,Chenyu Liu,Xiaoye Ouyang*

Main category: cs.CL

TL;DR: 本文介绍了一个名为Live Auditory Morph Resolution (LiveAMR)的任务，旨在检测电商直播平台上的语音欺骗行为，提出了首个包含86,790个样本的LiveAMR数据集，并将任务转化为文本生成问题，利用大规模语言模型生成额外训练数据以提高性能。


<details>
  <summary>Details</summary>
Motivation: 随着短视频平台如抖音在中国电子商务直播中的重要性日益增加，主播们通过语音变形来逃避监管和进行虚假广告的情况也愈发普遍。因此，需要研究新的方法来识别这些行为，以改进直播平台的监管措施。

Method: 研究人员构建了一个包含大量样本的LiveAMR数据集，并将检测任务转化为文本生成问题，通过利用大规模语言模型生成额外的训练数据来提升模型性能。

Result: 本研究提出了首个LiveAMR数据集，并通过大规模语言模型生成大量训练数据，展示了语音欺骗识别技术在电商直播平台监管中的有效性。

Conclusion: 通过LiveAMR任务的研究，证实了语音欺骗识别技术对于监管电商直播具有重要意义，将为未来的直播平台监管提供新的解决方案和技术支持。

Abstract: E-commerce live streaming in China, particularly on platforms like Douyin, has become a major sales channel, but hosts often use morphs to evade scrutiny and engage in false advertising. This study introduces the Live Auditory Morph Resolution (LiveAMR) task to detect such violations. Unlike previous morph research focused on text-based evasion in social media and underground industries, LiveAMR targets pronunciation-based evasion in health and medical live streams. We constructed the first LiveAMR dataset with 86,790 samples and developed a method to transform the task into a text-to-text generation problem. By leveraging large language models (LLMs) to generate additional training data, we improved performance and demonstrated that morph resolution significantly enhances live streaming regulation.

</details>


### [180] [AI Meets Brain: Memory Systems from Cognitive Neuroscience to Autonomous Agents](https://arxiv.org/abs/2512.23343)
*Jiafeng Liang,Hao Li,Chang Li,Jiaqi Zhou,Shixin Jiang,Zekun Wang,Changkai Ji,Zhihao Zhu,Runxuan Liu,Tao Ren,Jinlan Fu,See-Kiong Ng,Xia Liang,Ming Liu,Bing Qin*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Memory serves as the pivotal nexus bridging past and future, providing both humans and AI systems with invaluable concepts and experience to navigate complex tasks. Recent research on autonomous agents has increasingly focused on designing efficient memory workflows by drawing on cognitive neuroscience. However, constrained by interdisciplinary barriers, existing works struggle to assimilate the essence of human memory mechanisms. To bridge this gap, we systematically synthesizes interdisciplinary knowledge of memory, connecting insights from cognitive neuroscience with LLM-driven agents. Specifically, we first elucidate the definition and function of memory along a progressive trajectory from cognitive neuroscience through LLMs to agents. We then provide a comparative analysis of memory taxonomy, storage mechanisms, and the complete management lifecycle from both biological and artificial perspectives. Subsequently, we review the mainstream benchmarks for evaluating agent memory. Additionally, we explore memory security from dual perspectives of attack and defense. Finally, we envision future research directions, with a focus on multimodal memory systems and skill acquisition.

</details>


### [181] [A Stepwise-Enhanced Reasoning Framework for Large Language Models Based on External Subgraph Generation](https://arxiv.org/abs/2512.23356)
*Xin Zhang,Yang Cao,Baoxing Wu,Xinyi Chen,Kai Song,Siying Li*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large Language Models (LLMs) have achieved strong performance across a wide range of natural language processing tasks in recent years, including machine translation, text generation, and question answering. As their applications extend to increasingly complex scenarios, however, LLMs continue to face challenges in tasks that require deep reasoning and logical inference. In particular, models trained on large scale textual corpora may incorporate noisy or irrelevant information during generation, which can lead to incorrect predictions or outputs that are inconsistent with factual knowledge. To address this limitation, we propose a stepwise reasoning enhancement framework for LLMs based on external subgraph generation, termed SGR. The proposed framework dynamically constructs query relevant subgraphs from external knowledge bases and leverages their semantic structure to guide the reasoning process. By performing reasoning in a step by step manner over structured subgraphs, SGR reduces the influence of noisy information and improves reasoning accuracy. Specifically, the framework first generates an external subgraph tailored to the input query, then guides the model to conduct multi step reasoning grounded in the subgraph, and finally integrates multiple reasoning paths to produce the final answer. Experimental results on multiple benchmark datasets demonstrate that SGR consistently outperforms strong baselines, indicating its effectiveness in enhancing the reasoning capabilities of LLMs.

</details>


### [182] [Entropy-Guided Token Dropout: Training Autoregressive Language Models with Limited Domain Data](https://arxiv.org/abs/2512.23422)
*Jiapeng Wang,Yiwen Hu,Yanzipeng Gao,Haoyu Wang,Shuo Wang,Hongyu Lu,Jiaxin Mao,Wayne Xin Zhao,Junyi Li,Xiao Zhang*

Main category: cs.CL

TL;DR: 本文通过熵引导的令牌dropout方法，EntroDrop，解决大规模语言模型在多轮训练中的性能退化问题，验证了这种策略在数据稀缺领域下提升LLM适应性的潜力。


<details>
  <summary>Details</summary>
Motivation: 鉴于高质量领域特定数据的稀缺性，多阶段训练成为调整大型语言模型的实用策略。然而，自回归模型在重复数据暴露下常常出现性能下降，熵分布不平衡导致低熵令牌过快学习从而导致模型泛化能力下降。

Method: EntroDrop方法通过在训练过程中选择性地屏蔽低熵令牌，运用课程学习安排以适应训练进度调整正则化强度，来解决上述问题。

Result: 实验结果显示，EntroDrop在不同参数量的模型上都优于标准正则化基线，并在整个多轮训练过程中保持稳定的性能。

Conclusion: 该研究强调了在有限数据下，正则化策略需与令牌级学习动力学相匹配的重要性，并表明这为在数据受限领域中更好地调整LLM提供了前景。

Abstract: As access to high-quality, domain-specific data grows increasingly scarce, multi-epoch training has become a practical strategy for adapting large language models (LLMs). However, autoregressive models often suffer from performance degradation under repeated data exposure, where overfitting leads to a marked decline in model capability. Through empirical analysis, we trace this degradation to an imbalance in learning dynamics: predictable, low-entropy tokens are learned quickly and come to dominate optimization, while the model's ability to generalize on high-entropy tokens deteriorates with continued training. To address this, we introduce EntroDrop, an entropy-guided token dropout method that functions as structured data regularization. EntroDrop selectively masks low-entropy tokens during training and employs a curriculum schedule to adjust regularization strength in alignment with training progress. Experiments across model scales from 0.6B to 8B parameters show that EntroDrop consistently outperforms standard regularization baselines and maintains robust performance throughout extended multi-epoch training. These findings underscore the importance of aligning regularization with token-level learning dynamics when training on limited data. Our approach offers a promising pathway toward more effective adaptation of LLMs in data-constrained domains.

</details>


### [183] [The Effect of Gender Diversity on Scientific Team Impact: A Team Roles Perspective](https://arxiv.org/abs/2512.23429)
*Yi Zhao,Yongjun Zhu,Donghun Kim,Yuzhuo Wang,Heng Zhang,Chao Lu,Chengzhi Zhang*

Main category: cs.CL

TL;DR: 研究通过分析超过130,000篇来自PLOS期刊的文章，发现领导和辅助角色中的性别多样性与科研团队影响力之间存在倒U形关系。小型团队中全部女性领导会提升团队影响力，而大型团队中这种关系不显著；对于辅助角色，性别多样性始终正面影响团队影响力。


<details>
  <summary>Details</summary>
Motivation: 当前关于性别多样性如何影响科研团队成功的研究结果不一致，主要原因是大多研究将多样性单一化，忽略了内部角色的差异。本文通过更多角色分类研究，决心提供更全面的理解。

Method: 利用超过130,000篇来自PLOS期刊的文章，定义科学团队为所有合著者，并通过五年的引文数量来衡量团队影响力。根据作者贡献声明，将成员分为领导和辅助角色。使用多变量回归分析领导和辅助角色中性别多样性与团队影响力之间的关联，并应用阈值回归模型探讨团队规模对这种关系的影响。

Result: 研究发现领导和辅助角色中的性别多样性与团队影响力之间呈现倒U形关系。所有女性作为领导、所有男性作为辅助作用的团队影响力最高。对于领导角色，小型团队中全部女性领导者提高了团队影响力，而在大型团队中这种关系不显著；对于辅助角色，性别多样性对所有大小团队的影响力都是显著且正向的。

Conclusion: 研究表明性别多样性在不同团队角色中的影响作用具有复杂性，特别是在团队规模不同的场景下，性别多样性对领导和辅助角色的影响具有显著差异。

Abstract: The influence of gender diversity on the success of scientific teams is of great interest to academia. However, prior findings remain inconsistent, and most studies operationalize diversity in aggregate terms, overlooking internal role differentiation. This limitation obscures a more nuanced understanding of how gender diversity shapes team impact. In particular, the effect of gender diversity across different team roles remains poorly understood. To this end, we define a scientific team as all coauthors of a paper and measure team impact through five-year citation counts. Using author contribution statements, we classified members into leadership and support roles. Drawing on more than 130,000 papers from PLOS journals, most of which are in biomedical-related disciplines, we employed multivariable regression to examine the association between gender diversity in these roles and team impact. Furthermore, we apply a threshold regression model to investigate how team size moderates this relationship. The results show that (1) the relationship between gender diversity and team impact follows an inverted U-shape for both leadership and support groups; (2) teams with an all-female leadership group and an all-male support group achieve higher impact than other team types. Interestingly, (3) the effect of leadership-group gender diversity is significantly negative for small teams but becomes positive and statistically insignificant in large teams. In contrast, the estimates for support-group gender diversity remain significant and positive, regardless of team size.

</details>


### [184] [Semantic Tree Inference on Text Corpa using a Nested Density Approach together with Large Language Model Embeddings](https://arxiv.org/abs/2512.23471)
*Thomas Haschka,Joseph Bakarji*

Main category: cs.CL

TL;DR: 本文提出了一种嵌套密度聚类方法，用于在大型语言模型嵌入空间中识别和构建文本的层次语义关系树。


<details>
  <summary>Details</summary>
Motivation: 当前使用大型语言模型（LLM）嵌入来存储和检索文本，但文本语料库中的全局结构语义关系仍然不透明。为了解决这个问题，本文提出了一种嵌套密度聚类方法。

Method: 该方法通过在LLM嵌入空间中寻找密集簇来确定具有强烈语义相似性的文本，然后随着密度标准逐步放松，这些密集簇会合并成更具扩散性的簇，最终形成一个层次树结构。

Result: 该方法成功地用于多个数据集，包括科学摘要、20 Newsgroups和IMDB 50k电影评论，并展示了其在不同领域的广泛应用和鲁棒性。

Conclusion: 嵌套密度聚类方法不仅有助于揭示文本数据集中的语义结构，还能揭示领域及其子领域的演变。

Abstract: Semantic text classification has undergone significant advances in recent years due to the rise of large language models (LLMs) and their high dimensional embeddings. While LLM-embeddings are frequently used to store and retrieve text by semantic similarity in vector databases, the global structure semantic relationships in text corpora often remains opaque. Herein we propose a nested density clustering approach, to infer hierarchical trees of semantically related texts. The method starts by identifying texts of strong semantic similarity as it searches for dense clusters in LLM embedding space. As the density criterion is gradually relaxed, these dense clusters merge into more diffuse clusters, until the whole dataset is represented by a single cluster - the root of the tree. By embedding dense clusters into increasingly diffuse ones, we construct a tree structure that captures hierarchical semantic relationships among texts. We outline how this approach can be used to classify textual data for abstracts of scientific abstracts as a case study. This enables the data-driven discovery research areas and their subfields without predefined categories. To evaluate the general applicability of the method, we further apply it to established benchmark datasets such as the 20 News- groups and IMDB 50k Movie Reviews, demonstrating its robustness across domains. Finally we discuss possible applications on scientometrics, topic evolution, highlighting how nested density trees can reveal semantic structure and evolution in textual datasets.

</details>


### [185] [Automatic Detection of Complex Quotation Patterns in Aggadic Literature](https://arxiv.org/abs/2512.23504)
*Hadar Miller,Tsvi Kuflik,Moshe Lavee*

Main category: cs.CL

TL;DR: ACT是一种新颖的三阶段算法，用于自动检测拉比文学中的圣经引文，特别适用于短且结构嵌入的引文。它通过与形态学感知对齐算法结合上下文敏感增强阶段来识别复杂的引文模式，在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 当前现有的文本重用框架在检测短、改写或结构嵌入的引文方面面临挑战，而AC T算法通过对拉比文献中圣经引文的自动检测，填补了这一空缺，有助于提高对历史文本分析的理解，尤其是在形态学丰富和引文密集的传统领域。

Method: AC T算法包括三个阶段：形态学感知对齐、上下文敏感增强和引用类型检测。具有形态学感知对比算法，以及上下文敏感增强来识别复杂的引文模式。

Result: AC T算法在与领先系统 Dicta、Passim、Text-Matcher 的比较中表现出色，尤其是在F1得分、召回率和精度方面。配置模式如ACT-2和ACT-3也展示了权衡与-tradeoff-之间在精度和召回率之间的平衡。

Conclusion: AC T算法证明了其在自动检测拉比文献中的圣经引文方面的优势，提出了一种新的方法来检测文本中的引用模式，并为历史文本分析提供了一种新的工具。

Abstract: This paper presents ACT (Allocate Connections between Texts), a novel three-stage algorithm for the automatic detection of biblical quotations in Rabbinic literature. Unlike existing text reuse frameworks that struggle with short, paraphrased, or structurally embedded quotations, ACT combines a morphology-aware alignment algorithm with a context-sensitive enrichment stage that identifies complex citation patterns such as "Wave" and "Echo" quotations.
  Our approach was evaluated against leading systems, including Dicta, Passim, Text-Matcher, as well as human-annotated critical editions. We further assessed three ACT configurations to isolate the contribution of each component. Results demonstrate that the full ACT pipeline (ACT-QE) outperforms all baselines, achieving an F1 score of 0.91, with superior Recall (0.89) and Precision (0.94). Notably, ACT-2, which lacks stylistic enrichment, achieves higher Recall (0.90) but suffers in Precision, while ACT-3, using longer n-grams, offers a tradeoff between coverage and specificity.
  In addition to improving quotation detection, ACT's ability to classify stylistic patterns across corpora opens new avenues for genre classification and intertextual analysis. This work contributes to digital humanities and computational philology by addressing the methodological gap between exhaustive machine-based detection and human editorial judgment. ACT lays a foundation for broader applications in historical textual analysis, especially in morphologically rich and citation-dense traditions like Aggadic literature.

</details>


### [186] [UniHetero: Could Generation Enhance Understanding for Vision-Language-Model at Large Data Scale?](https://arxiv.org/abs/2512.23512)
*Fengjiao Chen,Minhao Jing,Weitao Lu,Yan Feng,Xiaoyu Li,Xuezhi Cao*

Main category: cs.CL

TL;DR: 该研究探讨了大规模预训练下的统一模型UniHetero，发现生成任务能提升理解但需生成语义而非像素；生成任务展示了更优的数据扩展趋势及更高的数据利用率；输入嵌入的自回归有助于捕捉视觉细节。


<details>
  <summary>Details</summary>
Motivation: 探索视觉-语言大模型中生成任务对理解能力的提升能力，尤其是在大规模数据预训练的背景下。

Method: 采用大规模预训练方法，使用包含语义生成和视觉生成任务的统一模型UniHetero进行实验。

Result: 实验发现生成任务可以提高理解能力，但只有生成语义而非像素；生成任务显示了更好的数据扩展趋势和更高的数据利用率；输入嵌入的自回归有助于捕捉视觉细节。

Conclusion: 这项工作表明，在视觉-语言大模型中，生成任务特别是生成语义在提升理解能力和数据利用上具有重要意义；输入嵌入自回归方法有效提升模型性能。

Abstract: Vision-language large models are moving toward the unification of visual understanding and visual generation tasks. However, whether generation can enhance understanding is still under-explored on large data scale. In this work, we analysis the unified model with a concise structure, UniHetero, under large-scale pretraining (>200M samples). Our key observations are: (1) Generation can improve understanding, but Only if you generate Semantics, Not Pixels. (2) Generation reveals a superior Data Scaling trend and higher Data Utilization. (3) Autoregression on Input Embedding is effective to capture visual details.

</details>


### [187] [Single LLM Debate, MoLaCE: Mixture of Latent Concept Experts Against Confirmation Bias](https://arxiv.org/abs/2512.23518)
*Hazel Kim,Philip Torr*

Main category: cs.CL

TL;DR: MoLaCE 是一种轻量级的推理时框架，通过混合不同激活强度的概念专家，来减轻大型语言模型中的确认偏见问题，并在保持计算效率的同时提升模型的鲁棒性和多样性。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型中的确认偏见问题，特别在多智能体辩论场景中，由于回声室效应会加剧偏见，需要一种能够减轻偏见的方法。

Method: MoLaCE框架通过即时混合不同激活强度的概念专家，在模型响应中引入多样性和不确定性，具体通过重塑不同表述提示的潜在概念权重来影响事实准确性，从而改善模型的反应。

Result: 实验结果表明，MoLaCE有效减少了确认偏见，提高了模型的鲁棒性，并且在需要的计算资源远少于多智能体辩论的情况下，仍然能匹敌甚至超越多智能体辩论的效果。

Conclusion: MoLaCE是一种有潜力的、能够减轻大型语言模型确认偏见的框架，它能够在保持计算效率的同时增强模型的内部辩论能力，对提高模型性能具有重要意义。

Abstract: Large language models (LLMs) are highly vulnerable to input confirmation bias. When a prompt implies a preferred answer, models often reinforce that bias rather than explore alternatives. This phenomenon remains underexplored, yet it is already harmful in base models and poses an even greater risk in multi-agent debate, where echo chambers reinforce bias instead of correction. We introduce Mixture of Latent Concept Experts (MoLaCE), a lightweight inference-time framework that addresses confirmation bias by mixing experts instantiated as different activation strengths over latent concepts that shape model responses. Our key insight is that, due to the compositional nature of language, differently phrased prompts reweight latent concepts in prompt-specific ways that affect factual correctness, so no single fixed intervention can be applied universally across inputs. This design enables a single LLM to emulate the benefits of debate internally while remaining computationally efficient and scalable. It can also be integrated into multi-agent debate frameworks to diversify perspectives and reduce correlated errors. We empirically show that it consistently reduces confirmation bias, improves robustness, and matches or surpasses multi-agent debate while requiring only a fraction of the computation.

</details>


### [188] [Lie to Me: Knowledge Graphs for Robust Hallucination Self-Detection in LLMs](https://arxiv.org/abs/2512.23547)
*Sahil Kale,Antonio Luca Alfeo*

Main category: cs.CL

TL;DR: 本文提出了一种简单但强大的方法，通过将LLM的响应转换为实体和关系的知识图，估计响应中包含幻觉的可能性，从而提高幻觉自检测的准确性。这种方法在两个LLM上进行评估，显示出高达16%的相对准确性和20%的F1分数改善，为更安全和值得信赖的语言模型铺平了道路。


<details>
  <summary>Details</summary>
Motivation: L现象对LLM的安全部署构成重大障碍，本文旨在通过利用结构化知识表示（知识图）来提高幻觉自检测的准确性。

Method: 该方法包括将LLM的响应转换为实体和关系的知识图，并使用这些知识图来估计响应中包含幻觉的可能性。

Result: 该方法在GPT-4o和Gemini-2.5-Flash这两个LLM上进行了评估，并取得了高达16%的相对准确性和20%的F1分数改善。

Conclusion: 该方法证明了即使初始输出包含不准确信息，将事实作为知识图结构化也能帮助LLM更好地分析。这是一种低成本且模型通用的方法，为更安全和值得信赖的语言模型铺平了道路。

Abstract: Hallucinations, the generation of apparently convincing yet false statements, remain a major barrier to the safe deployment of LLMs. Building on the strong performance of self-detection methods, we examine the use of structured knowledge representations, namely knowledge graphs, to improve hallucination self-detection. Specifically, we propose a simple yet powerful approach that enriches hallucination self-detection by (i) converting LLM responses into knowledge graphs of entities and relations, and (ii) using these graphs to estimate the likelihood that a response contains hallucinations. We evaluate the proposed approach using two widely used LLMs, GPT-4o and Gemini-2.5-Flash, across two hallucination detection datasets. To support more reliable future benchmarking, one of these datasets has been manually curated and enhanced and is released as a secondary outcome of this work. Compared to standard self-detection methods and SelfCheckGPT, a state-of-the-art approach, our method achieves up to 16% relative improvement in accuracy and 20% in F1-score. Our results show that LLMs can better analyse atomic facts when they are structured as knowledge graphs, even when initial outputs contain inaccuracies. This low-cost, model-agnostic approach paves the way toward safer and more trustworthy language models.

</details>


### [189] [Instruction-Following Evaluation of Large Vision-Language Models](https://arxiv.org/abs/2512.23572)
*Daiki Shiono,Shumpei Miyawaki,Ryota Tanaka,Jun Suzuki*

Main category: cs.CL

TL;DR: 本研究量化了大型视觉-语言模型（LVLMs）在微调后指令遵循能力的下降，并展示了在微调过程中明确指示输出格式如何提高指令遵循准确性。


<details>
  <summary>Details</summary>
Motivation: LVLMs在微调后往往无法像集成前的大语言模型（LLMs）一样精确遵循指令，因此需要研究和优化LVLMs的指令遵循能力。

Method: 研究构建了新的人工数据集，特别是在数据集中明确标注输出格式，并探讨了在微调过程中明确指示输出格式如何影响LVLMs的指令遵循能力。进行了量化评估以验证LVLMs在使用常见微调数据集微调后指令遵循能力的下降情况。

Result: 研究结果表明，使用包含输出格式指令的数据集微调的LVLMs在遵循指令方面表现更优，这表明在微调过程中包含有关输出格式的指令样本有助于缓解指令遵循能力的下降。

Conclusion: 研究结论建议，在（视觉）指令调优过程中包括关于输出格式的指令样本，可以有助于缓解LVLMs指令遵循能力的下降。

Abstract: Following the initial flourishing of large language models (LLMs), there has been a surge in proposed large vision-language models (LVLMs) that integrate LLMs with vision capabilities. However, it has been observed that LVLMs, after tuning to visual instruction using commonly used training datasets, often fail to exhibit the instruction-following ability that was present in the LLM before integration, leading to results in which they do not follow task instructions as expected. This study quantitatively demonstrates that LVLMs' instruction-following ability declines after fine-tuning and analyzes its underlying causes. In particular, we constructed new training datasets highlighting whether the output format is specified. Then, we investigated how explicitly indicating the output format during fine-tuning affects LVLMs' instruction-following ability. Our quantitative evaluation confirmed that LVLMs' instruction-following ability declines after fine-tuning with commonly used datasets. Furthermore, we found that LVLMs trained with datasets, including instructions on output format, tend to follow instructions more accurately than models that do not. These findings suggest that including samples with instructions on output format during (visual) instruction tuning may help mitigate the decline in instruction-following abilities.

</details>


### [190] [Style Amnesia: Investigating Speaking Style Degradation and Mitigation in Multi-Turn Spoken Language Models](https://arxiv.org/abs/2512.23578)
*Yu-Xiang Lin,Cheng-Han Chiang,Hung-yi Lee*

Main category: cs.CL

TL;DR: 本研究探讨了当对话语言模型（SLMs）被要求在多轮对话中使用特定的说话风格时出现的风格遗忘现象，发现SLMs难以维持指令的说话风格。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于揭示对话语言模型在多轮对话中无法维持特定说话风格的问题，探索其背后的原因，并为进一步改进此类模型提供参考。

Method: 研究采用定量评估方法，针对三种商用和两种开源语言模型进行测试，通过模拟多轮对话场景对模型的说话风格保持能力进行评估。

Result: 研究结果表明，SLMs在多轮对话中难以维持预定的说话风格，即使在后期被提醒也难以把这种风格贯穿始终。并通过试验发现，在系统消息中放置指令不如在用户消息中效果好。

Conclusion: 研究结论指出，说话风格遗忘是对话语言模型的一个关键挑战，可以通过改进提示策略减轻这一问题。

Abstract: In this paper, we show that when spoken language models (SLMs) are instructed to speak in a specific speaking style at the beginning of a multi-turn conversation, they cannot maintain the required speaking styles after several turns of interaction; we refer to this as the style amnesia of SLMs. We focus on paralinguistic speaking styles, including emotion, accent, volume, and speaking speed. We evaluate three proprietary and two open-source SLMs, demonstrating that none of these models can maintain a consistent speaking style when instructed to do so. We further show that when SLMs are asked to recall the style instruction in later turns, they can recall the style instruction, but they fail to express it throughout the conversation. We also show that explicitly asking the model to recall the style instruction can partially mitigate style amnesia. In addition, we examine various prompting strategies and find that SLMs struggle to follow the required style when the instruction is placed in system messages rather than user messages, which contradicts the intended function of system prompts.

</details>


### [191] [A Dataset and Benchmark for Consumer Healthcare Question Summarization](https://arxiv.org/abs/2512.23637)
*Abhishek Basu,Deepak Gupta,Dina Demner-Fushman,Shweta Yadav*

Main category: cs.CL

TL;DR: 该研究介绍了CHQ-Sum数据集，旨在为消费者健康问题总结任务提供一个由领域专家注释的高质量数据集，以促进高效总结系统的开发。


<details>
  <summary>Details</summary>
Motivation: 现有的自然语言理解挑战之一是处理消费者使用过于详细和周边信息来表达医疗状况或其他健康需求的问题。为了解决这个问题，研究人员需要一个专门针对消费者健康问题的标注数据集来训练和评估总结算法。

Method: 研究人员收集了来自社区问答论坛的1507个消费者健康问题及其相应的专家标注总结，并将其用作新数据集CHQ-Sum的基础。这一数据集被用作基准，用于测试最新的总结模型。

Result: 新的CHQ-Sum数据集在多种最新的总结模型上进行基准测试，展示了其有效性和潜在的应用价值。

Conclusion: 该研究成功地创建了CHQ-Sum数据集，为消费者健康问题总结任务提供了一个有价值的资源，有望推动相关研究的发展。

Abstract: The quest for seeking health information has swamped the web with consumers health-related questions. Generally, con- sumers use overly descriptive and peripheral information to express their medical condition or other healthcare needs, contributing to the challenges of natural language understanding. One way to address this challenge is to summarize the questions and distill the key information of the original question. Recently, large-scale datasets have significantly propelled the development of several summarization tasks, such as multi-document summarization and dialogue summarization. However, a lack of a domain-expert annotated dataset for the consumer healthcare questions summarization task inhibits the development of an efficient summarization system. To address this issue, we introduce a new dataset, CHQ-Sum,m that contains 1507 domain-expert annotated consumer health questions and corresponding summaries. The dataset is derived from the community question answering forum and therefore provides a valuable resource for understanding consumer health-related posts on social media. We benchmark the dataset on multiple state-of-the-art summarization models to show the effectiveness of the dataset

</details>


### [192] [Less is more: Probabilistic reduction is best explained by small-scale predictability measures](https://arxiv.org/abs/2512.23659)
*Cassandra L. Jacobs,Andrés Buxó-Lugo,Anna K. Taylor,Marie Leopold-Hooke*

Main category: cs.CL

TL;DR: 研究探讨了在探索语言模型概率与心理现象之间的关系时，所需的语境量及其适当性。研究表明，无需整个单句，n-克gram表示足以作为计划的心理单位。


<details>
  <summary>Details</summary>
Motivation: 当前研究的主要动机在于明确在研究语言模型概率与认知现象之间关系时，所需和适当的语境量。研究表明，整个句子不是必需的，n-克gram表示已经足够作为计划的心理单位。

Method: 本研究采用了探索性方法，通过实证研究来检验n-克gram作为计划心理单位的有效性。

Result: 研究结果表明，n-克gram作为认知单位可以充分捕获语言模型概率与认知现象之间的关系，无需依赖完整的句子。

Conclusion: 研究结论认为，n-克gram可以作为在语言模型概率与认知现象之间关系研究中应用的认知单位，这简化了研究过程，提供了有效的心理学分析单位。

Abstract: The primary research questions of this paper center on defining the amount of context that is necessary and/or appropriate when investigating the relationship between language model probabilities and cognitive phenomena. We investigate whether whole utterances are necessary to observe probabilistic reduction and demonstrate that n-gram representations suffice as cognitive units of planning.

</details>


### [193] [Multilingual Hidden Prompt Injection Attacks on LLM-Based Academic Reviewing](https://arxiv.org/abs/2512.23684)
*Panagiotis Theocharopoulos,Ajinkya Kulkarni,Mathew Magimai. -Doss*

Main category: cs.CL

TL;DR: 研究在ICML会议上接受的约500篇真实学术论文中插入了不同语言的隐蔽攻击指令，发现这些指令对英文、日文和中文论文的审核分数和接受/拒绝决定产生了显著影响，而阿拉伯文则几乎没有影响。


<details>
  <summary>Details</summary>
Motivation: 分析大型语言模型（LLMs）在学术同行评审中的潜在安全风险，特别是隐藏的文档级投毒攻击的影响。

Method: 使用LLMs对包含不同语言隐蔽攻击指令的约500篇真实学术论文进行审核，并比较审核结果的变化。

Result: 实验结果表明，插入的隐蔽攻击指令对英文、日文和中文论文的审核分数和接受/拒绝决定产生了显著影响，而阿拉伯文则几乎没有影响。

Conclusion: 研究揭示了LLM基础审核系统对文档级隐蔽攻击指令的脆弱性，并显示出不同语言之间的显著差异。

Abstract: Large language models (LLMs) are increasingly considered for use in high-impact workflows, including academic peer review. However, LLMs are vulnerable to document-level hidden prompt injection attacks. In this work, we construct a dataset of approximately 500 real academic papers accepted to ICML and evaluate the effect of embedding hidden adversarial prompts within these documents. Each paper is injected with semantically equivalent instructions in four different languages and reviewed using an LLM. We find that prompt injection induces substantial changes in review scores and accept/reject decisions for English, Japanese, and Chinese injections, while Arabic injections produce little to no effect. These results highlight the susceptibility of LLM-based reviewing systems to document-level prompt injection and reveal notable differences in vulnerability across languages.

</details>


### [194] [PROFASR-BENCH: A Benchmark for Context-Conditioned ASR in High-Stakes Professional Speech](https://arxiv.org/abs/2512.23686)
*Deepak Babu Piskala*

Main category: cs.CL

TL;DR: 该研究提出了ProfASR-Bench，一个专业的会话评估套件，针对金融、医学、法律和技术领域的关键应用场景。通过自然语言提示与实体丰富的目标话语配对，它能够定量测量上下文条件下的识别性能。研究发现轻量级文本提示在提高错误率方面几乎无效，即使是 oracle 提示也一样。这种方法揭示了一个被称为上下文利用差距 (CUG) 的现象，即现有系统名义上可使用提示，但未能充分利用可用的辅助信息。


<details>
  <summary>Details</summary>
Motivation: 鉴于ASR在专业场景中面临的挑战，如密集的专业术语、不同正式语境下的变化以及对关键实体错误的零容忍度，该研究旨在通过建立一个专业会话评估套件，即ProfASR-Bench，提高ASR系统在高风险应用中的性能。

Method: 研究利用了包含自然语言提示和实体丰富目标话语的ProfASR-Bench数据集。研究团队测试了Whisper（编码器-解码ASR）和Qwen-Omni（音频语言模型）系列下的不同条件组合，如无上下文、个人信息、领域+个人信息、oracle提示和对抗提示。

Result: 研究表明，轻量级文本提示在提高错误率方面几乎无效，即使是oracle提示也一样。在某些情况下，对抗提示反而没有显著降低性能。这些结果揭示了一个被称为上下文利用差距 (CUG) 的现象，表明当前系统已经具备功能上的提示使用能力，但在实际应用中未能充分利用可用的辅助信息。

Conclusion: 研究成功展示了ProfASR-Bench作为一个标准化的上下文梯度，以及实体和切片意识的报告，提供了可重复的测试床，用于比较不同模型家族的融合策略。此外，该研究发现的上下文利用差距（CUG）为ASR系统的设计和开发提供了新的研究方向，特别是在如何更有效地利用上下文信息方面。

Abstract: Automatic Speech Recognition (ASR) in professional settings faces challenges that existing benchmarks underplay: dense domain terminology, formal register variation, and near-zero tolerance for critical entity errors. We present ProfASR-Bench, a professional-talk evaluation suite for high-stakes applications across finance, medicine, legal, and technology. Each example pairs a natural-language prompt (domain cue and/or speaker profile) with an entity-rich target utterance, enabling controlled measurement of context-conditioned recognition. The corpus supports conventional ASR metrics alongside entity-aware scores and slice-wise reporting by accent and gender. Using representative families Whisper (encoder-decoder ASR) and Qwen-Omni (audio language models) under matched no-context, profile, domain+profile, oracle, and adversarial conditions, we find a consistent pattern: lightweight textual context produces little to no change in average word error rate (WER), even with oracle prompts, and adversarial prompts do not reliably degrade performance. We term this the context-utilization gap (CUG): current systems are nominally promptable yet underuse readily available side information. ProfASR-Bench provides a standardized context ladder, entity- and slice-aware reporting with confidence intervals, and a reproducible testbed for comparing fusion strategies across model families.
  Dataset: https://huggingface.co/datasets/prdeepakbabu/ProfASR-Bench
  Code: https://github.com/prdeepakbabu/ProfASR-Bench

</details>


### [195] [Fine-Tuning LLMs with Fine-Grained Human Feedback on Text Spans](https://arxiv.org/abs/2512.23693)
*Sky CH-Wang,Justin Svegliato,Helen Appel,Jason Eisner*

Main category: cs.CL

TL;DR: 该研究提出了一种使用反馈驱动的改进链对语言模型进行微调的方法，并构建了带有偏好标记的数据集，这种方法比基于标准A/B偏好排名或全对比重写的方法更有效率和有效。


<details>
  <summary>Details</summary>
Motivation: 当前直接对齐方法存在效率低下和效果不佳的问题，研究希望通过一种新的反馈驱动的改进链方法来提高模型对偏好指令的理解和响应能力。

Method: 研究通过生成带有偏好标记的数据集，并使用左到右的逐步改进链来训练模型，逐个修正模型生成的不喜欢的部分，形成一系列递增的改进。这种方法使得模型可以学习局部、目标化的修改。

Result: 实验结果显示，这种基于修订过程的监督学习方法优于基于标准A/B偏好排名或全对比重写的方法，证明了结构化的、基于修订的监督学习可以更有效地和高效地进行偏好调整。

Conclusion: 因此，该研究提出的方法为语言模型提供了更好的偏好调整能力，并且在实际应用中表现出了更好的效果。

Abstract: We present a method and dataset for fine-tuning language models with preference supervision using feedback-driven improvement chains. Given a model response, an annotator provides fine-grained feedback by marking ``liked'' and ``disliked'' spans and specifying what they liked or disliked about them. The base model then rewrites the disliked spans accordingly, proceeding from left to right, forming a sequence of incremental improvements. We construct preference pairs for direct alignment from each adjacent step in the chain, enabling the model to learn from localized, targeted edits. We find that our approach outperforms direct alignment methods based on standard A/B preference ranking or full contrastive rewrites, demonstrating that structured, revision-based supervision leads to more efficient and effective preference tuning.

</details>


### [196] [Eliciting Behaviors in Multi-Turn Conversations](https://arxiv.org/abs/2512.23701)
*Jing Huang,Shujian Zhang,Lun Wang,Andrew Hard,Rajiv Mathews,John Lambert*

Main category: cs.CL

TL;DR: 本文提出了一种分析和评估大型语言模型在多轮对话环境中特定行为的新方法，通过对比现有静态和动态方法的性能，表明在线方法在发现目标模型行为触发输入方面更为有效。


<details>
  <summary>Details</summary>
Motivation: 需要一种有效的方法来识别和评估大型语言模型在多轮对话中的特定行为，尤其是相比以往主要研究单轮设置的情况。

Method: 研究通过构建一个分析框架，分类现有方法为使用先验知识、离线交互和在线学习三类，并提出了一种通用的多轮在线方法，同时分析了查询预算与成功几率之间的权衡。

Result: 在线方法在三个任务中平均成功率分别为45/19/77%，远远高于静态方法，并且能够发现更多先前基准中的失败案例。

Conclusion: 研究强调了在多轮对话评价中行为激发方法的新型应用，并呼吁社区转向动态基准以更好地评估模型的能力。

Abstract: Identifying specific and often complex behaviors from large language models (LLMs) in conversational settings is crucial for their evaluation. Recent work proposes novel techniques to find natural language prompts that induce specific behaviors from a target model, yet they are mainly studied in single-turn settings. In this work, we study behavior elicitation in the context of multi-turn conversations. We first offer an analytical framework that categorizes existing methods into three families based on their interactions with the target model: those that use only prior knowledge, those that use offline interactions, and those that learn from online interactions. We then introduce a generalized multi-turn formulation of the online method, unifying single-turn and multi-turn elicitation. We evaluate all three families of methods on automatically generating multi-turn test cases. We investigate the efficiency of these approaches by analyzing the trade-off between the query budget, i.e., the number of interactions with the target model, and the success rate, i.e., the discovery rate of behavior-eliciting inputs. We find that online methods can achieve an average success rate of 45/19/77% with just a few thousand queries over three tasks where static methods from existing multi-turn conversation benchmarks find few or even no failure cases. Our work highlights a novel application of behavior elicitation methods in multi-turn conversation evaluation and the need for the community to move towards dynamic benchmarks.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [197] [Bidirectional RAG: Safe Self-Improving Retrieval-Augmented Generation Through Multi-Stage Validation](https://arxiv.org/abs/2512.22199)
*Teja Chinthala*

Main category: cs.AI

TL;DR: 本文提出了一种双向RAG架构，通过验证高质量生成回复的写回实现了安全的语料库扩展。该系统通过多阶段接受层结合语义验证、归属验证和新颖性检测来预防幻觉污染并促进知识积累，使双RAG在四个数据集上的覆盖范围平均提高了40.58%，并且添加的文档数量减少72%。


<details>
  <summary>Details</summary>
Motivation: 传统的RAG架构使用静态语料库，无法从用户交互中进化，因此提出了一个能够在保障安全性的同时通过验证高质量生成回复来扩展语料库的双向RAG架构。

Method: 该方法通过多阶段接受层结合语义验证（NLI基于蕴含、归属检查和新颖性检测），在保障生成回复质量的同时实现知识积累。

Result: 通过在四个数据集（Natural Questions, TriviaQA, HotpotQA, Stack Overflow）上使用三种随机种子，进行12次实验，双向RAG的平均覆盖范围提高了40.58%，并且相较于天真写回方式，增加了72%更少的文档（140 vs 500）。

Conclusion: 本研究证明了当受到严格的验证治理时，自我改进的RAG架构是可行且安全的，为RAG系统的部署提供了一条实际的道路。

Abstract: Retrieval-Augmented Generation RAG systems enhance large language models by grounding responses in external knowledge bases, but conventional RAG architectures operate with static corpora that cannot evolve from user interactions. We introduce Bidirectional RAG, a novel RAG architecture that enables safe corpus expansion through validated write back of high quality generated responses. Our system employs a multi stage acceptance layer combining grounding verification (NLI based entailment, attribution checking, and novelty detection to prevent hallucination pollution while enabling knowledge accumulation. Across four datasets Natural Questions, TriviaQA, HotpotQA, Stack Overflow with three random seeds 12 experiments per system, Bidirectional RAG achieves 40.58% average coverage nearly doubling Standard RAG 20.33% while adding 72% fewer documents than naive write back 140 vs 500. Our work demonstrates that self improving RAG is feasible and safe when governed by rigorous validation, offering a practical path toward RAG systems that learn from deployment.

</details>


### [198] [Emergent Persuasion: Will LLMs Persuade Without Being Prompted?](https://arxiv.org/abs/2512.22201)
*Vincent Chang,Thee Ho,Sunishchal Dev,Kevin Zhu,Shi Feng,Kellin Pelrine,Matthew Kowal*

Main category: cs.AI

TL;DR: 本文探讨了在未明确提示的情况下，大型语言模型是否会自动产生说服行为，通过模型导向和监督调优两种方法进行研究。发现虽然导向特定特质并不会导致模型在未提示的情况下更易进行说服，但监督调优却能增加模型对争议性和有害话题的说服倾向，提示应进一步关注这种潜在风险。


<details>
  <summary>Details</summary>
Motivation: 为了评估大型语言模型在未被直接指示时是否会自发地进行说服活动，特别是这种活动是否可能带来潜在的危害。

Method: 通过两种方法对模型进行研究：一种是基于内部激活导向特定人格特质，另一种是监督调优（SFT）。同时，通过使用仅包含 benign 话题的通用说服数据集来监督调优模型。

Result: 导向特定特质并不会显著增加模型在未提示的情况下说服的倾向，而监督调优后，模型对包含争议性和有害话题的数据集表现出了更高的说服倾向。

Conclusion: 研究发现，基于特定特质的监督调优可能增加模型在未受明确指示的情况下对争议性和有害话题进行说服的倾向。作者建议应进一步研究这种现象以评估其潜在风险。

Abstract: With the wide-scale adoption of conversational AI systems, AI are now able to exert unprecedented influence on human opinion and beliefs. Recent work has shown that many Large Language Models (LLMs) comply with requests to persuade users into harmful beliefs or actions when prompted and that model persuasiveness increases with model scale. However, this prior work looked at persuasion from the threat model of $\textit{misuse}$ (i.e., a bad actor asking an LLM to persuade). In this paper, we instead aim to answer the following question: Under what circumstances would models persuade $\textit{without being explicitly prompted}$, which would shape how concerned we should be about such emergent persuasion risks. To achieve this, we study unprompted persuasion under two scenarios: (i) when the model is steered (through internal activation steering) along persona traits, and (ii) when the model is supervised-finetuned (SFT) to exhibit the same traits. We showed that steering towards traits, both related to persuasion and unrelated, does not reliably increase models' tendency to persuade unprompted, however, SFT does. Moreover, SFT on general persuasion datasets containing solely benign topics admits a model that has a higher propensity to persuade on controversial and harmful topics--showing that emergent harmful persuasion can arise and should be studied further.

</details>


### [199] [GamiBench: Evaluating Spatial Reasoning and 2D-to-3D Planning Capabilities of MLLMs with Origami Folding Tasks](https://arxiv.org/abs/2512.22207)
*Ryan Spencer,Roey Yaari,Ritvik Vemavarapu,Joyce Yang,Steven Ngo,Utkarsh Sharma*

Main category: cs.AI

TL;DR: GamiBench提供了一种新的基准测试方法，通过纸艺折纸任务评估MLLMs的视觉推理能力，涵盖3D解析、视角一致性以及难以折叠图案的检测。


<details>
  <summary>Details</summary>
Motivation: 现有的评估基准主要关注静态图像或最终输出，未能反映视觉推理的序列性和视角依赖性，GamiBench旨在填补这一空白。

Method: GamiBench通过设计一系列折纸任务，包含186个常规和186个不可能的2D折痕模式及其对应的3D折叠形状，通过六个不同的视角，在三个视觉问答任务中进行评估。

Result: 实验表明，即使是领先的模型如GPT-5和Gemini-2.5-Pro在单一步骤的空间理解上也存在问题。此基准还引入了新的诊断指标，如视角一致性（VC）和难以折叠图案的检测率（IFSR），以评估模型处理不同复杂度折痕的能力。

Conclusion: GamiBench为MLLMs的几何理解和空间推理提供了一个标准化框架，进一步推动了该领域的研究与发展。

Abstract: Multimodal large language models (MLLMs) are proficient in perception and instruction-following, but they still struggle with spatial reasoning: the ability to mentally track and manipulate objects across multiple views and over time. Spatial reasoning is a key component of human intelligence, but most existing benchmarks focus on static images or final outputs, failing to account for the sequential and viewpoint-dependent nature of this skill. To close this gap, we introduce GamiBench, a benchmark designed to evaluate spatial reasoning and 2D-to-3D planning in MLLMs through origami-inspired folding tasks. GamiBench includes 186 regular and 186 impossible 2D crease patterns paired with their corresponding 3D folded shapes, produced from six distinct viewpoints across three visual question-answering (VQA) tasks: predicting 3D fold configurations, distinguishing valid viewpoints, and detecting impossible patterns. Unlike previous benchmarks that assess only final predictions, GamiBench holistically evaluates the entire reasoning process--measuring cross-view consistency, physical feasibility through impossible-fold detection, and interpretation of intermediate folding steps. It further introduces new diagnostic metrics--viewpoint consistency (VC) and impossible fold selection rate (IFSR)--to measure how well models handle folds of varying complexity. Our experiments show that even leading models such as GPT-5 and Gemini-2.5-Pro struggle on single-step spatial understanding. These contributions establish a standardized framework for evaluating geometric understanding and spatial reasoning in MLLMs. Dataset and code: https://github.com/stvngo/GamiBench.

</details>


### [200] [Toward Equitable Recovery: A Fairness-Aware AI Framework for Prioritizing Post-Flood Aid in Bangladesh](https://arxiv.org/abs/2512.22210)
*Farjana Yesmin,Romana Akter*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Post-disaster aid allocation in developing nations often suffers from systematic biases that disadvantage vulnerable regions, perpetuating historical inequities. This paper presents a fairness-aware artificial intelligence framework for prioritizing post-flood aid distribution in Bangladesh, a country highly susceptible to recurring flood disasters. Using real data from the 2022 Bangladesh floods that affected 7.2 million people and caused 405.5 million US dollars in damages, we develop an adversarial debiasing model that predicts flood vulnerability while actively removing biases against marginalized districts and rural areas. Our approach adapts fairness-aware representation learning techniques from healthcare AI to disaster management, employing a gradient reversal layer that forces the model to learn bias-invariant representations. Experimental results on 87 upazilas across 11 districts demonstrate that our framework reduces statistical parity difference by 41.6 percent, decreases regional fairness gaps by 43.2 percent, and maintains strong predictive accuracy (R-squared=0.784 vs baseline 0.811). The model generates actionable priority rankings ensuring aid reaches the most vulnerable populations based on genuine need rather than historical allocation patterns. This work demonstrates how algorithmic fairness techniques can be effectively applied to humanitarian contexts, providing decision-makers with tools to implement more equitable disaster recovery strategies.

</details>


### [201] [With Great Capabilities Come Great Responsibilities: Introducing the Agentic Risk & Capability Framework for Governing Agentic AI Systems](https://arxiv.org/abs/2512.22211)
*Shaun Khoo,Jessica Foo,Roy Ka-Wei Lee*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Agentic AI systems present both significant opportunities and novel risks due to their capacity for autonomous action, encompassing tasks such as code execution, internet interaction, and file modification. This poses considerable challenges for effective organizational governance, particularly in comprehensively identifying, assessing, and mitigating diverse and evolving risks. To tackle this, we introduce the Agentic Risk \& Capability (ARC) Framework, a technical governance framework designed to help organizations identify, assess, and mitigate risks arising from agentic AI systems. The framework's core contributions are: (1) it develops a novel capability-centric perspective to analyze a wide range of agentic AI systems; (2) it distills three primary sources of risk intrinsic to agentic AI systems - components, design, and capabilities; (3) it establishes a clear nexus between each risk source, specific materialized risks, and corresponding technical controls; and (4) it provides a structured and practical approach to help organizations implement the framework. This framework provides a robust and adaptable methodology for organizations to navigate the complexities of agentic AI, enabling rapid and effective innovation while ensuring the safe, secure, and responsible deployment of agentic AI systems. Our framework is open-sourced \href{https://govtech-responsibleai.github.io/agentic-risk-capability-framework/}{here}.

</details>


### [202] [We are not able to identify AI-generated images](https://arxiv.org/abs/2512.22236)
*Adrien Pavão*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: AI-generated images are now pervasive online, yet many people believe they can easily tell them apart from real photographs. We test this assumption through an interactive web experiment where participants classify 20 images as real or AI-generated. Our dataset contains 120 difficult cases: real images sampled from CC12M, and carefully curated AI-generated counterparts produced with MidJourney. In total, 165 users completed 233 sessions. Their average accuracy was 54%, only slightly above random guessing, with limited improvement across repeated attempts. Response times averaged 7.3 seconds, and some images were consistently more deceptive than others. These results indicate that, even on relatively simple portrait images, humans struggle to reliably detect AI-generated content. As synthetic media continues to improve, human judgment alone is becoming insufficient for distinguishing real from artificial data. These findings highlight the need for greater awareness and ethical guidelines as AI-generated media becomes increasingly indistinguishable from reality.

</details>


### [203] [Logic Sketch Prompting (LSP): A Deterministic and Interpretable Prompting Method](https://arxiv.org/abs/2512.22258)
*Satvik Tripathi*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Large language models (LLMs) excel at natural language reasoning but remain unreliable on tasks requiring strict rule adherence, determinism, and auditability. Logic Sketch Prompting (LSP) is a lightweight prompting framework that introduces typed variables, deterministic condition evaluators, and a rule based validator that produces traceable and repeatable outputs. Using two pharmacologic logic compliance tasks, we benchmark LSP against zero shot prompting, chain of thought prompting, and concise prompting across three open weight models: Gemma 2, Mistral, and Llama 3. Across both tasks and all models, LSP consistently achieves the highest accuracy (0.83 to 0.89) and F1 score (0.83 to 0.89), substantially outperforming zero shot prompting (0.24 to 0.60), concise prompts (0.16 to 0.30), and chain of thought prompting (0.56 to 0.75). McNemar tests show statistically significant gains for LSP across nearly all comparisons (p < 0.01). These results demonstrate that LSP improves determinism, interpretability, and consistency without sacrificing performance, supporting its use in clinical, regulated, and safety critical decision support systems.

</details>


### [204] [Agent2World: Learning to Generate Symbolic World Models via Adaptive Multi-Agent Feedback](https://arxiv.org/abs/2512.22336)
*Mengkang Hu,Bowei Xia,Yuran Wu,Ailing Yu,Yude Zou,Qiguang Chen,Shijian Wang,Jiarui Jin,Kexin Li,Wenxiang Jiao,Yuan Lu,Ping Luo*

Main category: cs.AI

TL;DR: Agent2World提出了一种多agent框架，通过知识合成、模型开发和单元测试三个阶段生成和验证世界模型，提高了基于计划定义语言(PDDL)和可执行代码表示的模型生成能力，尤其是在交互反馈下，模型生成能力提升了30.95%。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖于静态验证，但无法发现由交互执行引起的行为级错误，因此需要一种新的工具来增强世界模型生成和验证。

Method: Agent2World采用了一个三阶段的流程：知识合成、执行模型开发以及具有自适应反馈测试团队进行单元测试和仿真验证。其中，知识合成由一个深度研究员agent完成，模型开发由一个model developer agent执行，测试团队负责验证。

Result: Agent2World在三个基准测试中展示了优越的表现，涵盖了PDDL和可执行代码表示，并能达到状态-of-the-art的结果。此外，通过使用测试团队提供的反馈进行训练，模型生成能力提高了30.95%。

Conclusion: 该多agent框架显著改进了模型生成的性能，并能够在实际应用中提供更准确和可靠的预测。

Abstract: Symbolic world models (e.g., PDDL domains or executable simulators) are central to model-based planning, but training LLMs to generate such world models is limited by the lack of large-scale verifiable supervision. Current approaches rely primarily on static validation methods that fail to catch behavior-level errors arising from interactive execution. In this paper, we propose Agent2World, a tool-augmented multi-agent framework that achieves strong inference-time world-model generation and also serves as a data engine for supervised fine-tuning, by grounding generation in multi-agent feedback. Agent2World follows a three-stage pipeline: (i) A Deep Researcher agent performs knowledge synthesis by web searching to address specification gaps; (ii) A Model Developer agent implements executable world models; And (iii) a specialized Testing Team conducts adaptive unit testing and simulation-based validation. Agent2World demonstrates superior inference-time performance across three benchmarks spanning both Planning Domain Definition Language (PDDL) and executable code representations, achieving consistent state-of-the-art results. Beyond inference, Testing Team serves as an interactive environment for the Model Developer, providing behavior-aware adaptive feedback that yields multi-turn training trajectories. The model fine-tuned on these trajectories substantially improves world-model generation, yielding an average relative gain of 30.95% over the same model before training. Project page: https://agent2world.github.io.

</details>


### [205] [Subgoaling Relaxation-based Heuristics for Numeric Planning with Infinite Actions](https://arxiv.org/abs/2512.22367)
*Ángel Aso-Mollar,Diego Aineto,Enrico Scala,Eva Onaindia*

Main category: cs.AI

TL;DR: 本文通过识别一类可控制的简单数值问题，并提出了一种乐观编译方法，将问题转化为简单的数值任务，从而能有效使用子目标启发式来估计数值规划问题中带有控制参数的目标距离。


<details>
  <summary>Details</summary>
Motivation: 现有的数值规划模型无法直接处理控制参数的存在，会导致无限数量的动作可能，现有的启发式方法不再适用。因此，本文提出了一个新的子集及其解决方案，以克服现有技术的限制。

Method: 文章首先识别了一类特定的数值问题，并将控制依赖的表达式抽象为有界常数效应和宽松的前提条件，然后使用一种乐观编译方法将其转化为简单的数值任务。

Result: 该方法使得使用子目标启发式估计带有控制参数的数值规划问题的目标距离成为可能，从而扩展了传统数值启发式的应用范围。

Conclusion: 本文的方法提供了一种有效且计算上可行的方式，将传统数值启发式应用于具有无限可能出现动作的设置中，推动了当前技术前沿。

Abstract: Numeric planning with control parameters extends the standard numeric planning model by introducing action parameters as free numeric variables that must be instantiated during planning. This results in a potentially infinite number of applicable actions in a state. In this setting, off-the-shelf numeric heuristics that leverage the action structure are not feasible. In this paper, we identify a tractable subset of these problems--namely, controllable, simple numeric problems--and propose an optimistic compilation approach that transforms them into simple numeric tasks. To do so, we abstract control-dependent expressions into bounded constant effects and relaxed preconditions. The proposed compilation makes it possible to effectively use subgoaling heuristics to estimate goal distance in numeric planning problems involving control parameters. Our results demonstrate that this approach is an effective and computationally feasible way of applying traditional numeric heuristics to settings with an infinite number of possible actions, pushing the boundaries of the current state of the art.

</details>


### [206] [HalluMat: Detecting Hallucinations in LLM-Generated Materials Science Content Through Multi-Stage Verification](https://arxiv.org/abs/2512.22396)
*Bhanu Prakash Vangala,Sajid Mahmud,Pawan Neupane,Joel Selvaraj,Jianlin Cheng*

Main category: cs.AI

TL;DR: 本文介绍了HalluMatData数据集和HalluMatDetector框架，用于评估AI生成的材料科学内容中的虚假信息检测方法、事实一致性及响应稳健性。结果显示，使用HalluMatDetector可以将幻觉率降低30%，并提出了Paraphrased Hallucination Consistency Score以量化语义等价查询中生成器的不一致性。


<details>
  <summary>Details</summary>
Motivation: 现有人工智能生成的内容难以保证事实准确性，可能影响科学研究的诚信度。本文旨在应对这一挑战，提出了一个新的基准数据集（HalluMatData）和多阶段自动幻觉检测框架（HalluMatDetector）以提高生成性AI系统的性能。

Method: 本文构建了HalluMatData数据集，并提出了HalluMatDetector框架，该框架综合使用内在验证、多源检索、矛盾图分析和基于度量的评估方法来检测和缓解语言模型生成的幻觉。

Result: 通过使用HalluMatDetector，幻觉率比标准语言模型输出降低了30%，并开发了一种新的评估指标（PHCS）来量化语义等价查询中的不一致性。

Conclusion: 通过引入HalluMatData和HalluMatDetector，本文为检测和缓解AI生成内容中的虚假信息提供了解决方案，有助于提高AI在科研领域的可靠性。

Abstract: Artificial Intelligence (AI), particularly Large Language Models (LLMs), is transforming scientific discovery, enabling rapid knowledge generation and hypothesis formulation. However, a critical challenge is hallucination, where LLMs generate factually incorrect or misleading information, compromising research integrity. To address this, we introduce HalluMatData, a benchmark dataset for evaluating hallucination detection methods, factual consistency, and response robustness in AI-generated materials science content. Alongside this, we propose HalluMatDetector, a multi-stage hallucination detection framework that integrates intrinsic verification, multi-source retrieval, contradiction graph analysis, and metric-based assessment to detect and mitigate LLM hallucinations. Our findings reveal that hallucination levels vary significantly across materials science subdomains, with high-entropy queries exhibiting greater factual inconsistencies. By utilizing HalluMatDetector verification pipeline, we reduce hallucination rates by 30% compared to standard LLM outputs. Furthermore, we introduce the Paraphrased Hallucination Consistency Score (PHCS) to quantify inconsistencies in LLM responses across semantically equivalent queries, offering deeper insights into model reliability.

</details>


### [207] [Lightweight Inference-Time Personalization for Frozen Knowledge Graph Embeddings](https://arxiv.org/abs/2512.22398)
*Ozan Oguztuzun,Cerag Oguztuzun*

Main category: cs.AI

TL;DR: GatedBias提出了一种轻量级的推理时个性化框架，能够在无需重新训练和不损害整体准确性的前提下，适应Frozen KG嵌入到个体用户上下文，从而提高个性化排名的效果。


<details>
  <summary>Details</summary>
Motivation: 当前的基础模型在知识图谱链接预测上的表现优异，但在个性化排名上存在不足。GatedBias旨在解决这个问题，通过在推理阶段引入个性化的调整机制，使得模型能够更好地理解并服务于个体用户的需求。

Method: GatedBias方法通过结合用户特定特征和图衍生的二进制门控，实现在不重训练和不影响全局准确性的前提下个性化调整基础模型的嵌入。它使用少量的可训练参数（约300个）来生成可解释的实体偏置。

Result: 在Amazon-Book和Last-FM两个基准数据集上的实验结果显示，GatedBias能够显著提高匹配度指标，并保持群体级别的性能。反事实扰动实验验证了因果响应性：针对特定偏好信号的实体，在加强这些信号后，显示出6-30倍的排名提升。

Conclusion: GatedBias证明了个性化基础模型的高效性和因果验证性，促进了通用知识表示与个体用户需求的融合。

Abstract: Foundation models for knowledge graphs (KGs) achieve strong cohort-level performance in link prediction, yet fail to capture individual user preferences; a key disconnect between general relational reasoning and personalized ranking. We propose GatedBias, a lightweight inference-time personalization framework that adapts frozen KG embeddings to individual user contexts without retraining or compromising global accuracy. Our approach introduces structure-gated adaptation: profile-specific features combine with graph-derived binary gates to produce interpretable, per-entity biases, requiring only ${\sim}300$ trainable parameters. We evaluate GatedBias on two benchmark datasets (Amazon-Book and Last-FM), demonstrating statistically significant improvements in alignment metrics while preserving cohort performance. Counterfactual perturbation experiments validate causal responsiveness; entities benefiting from specific preference signals show 6--30$\times$ greater rank improvements when those signals are boosted. These results show that personalized adaptation of foundation models can be both parameter-efficient and causally verifiable, bridging general knowledge representations with individual user needs.

</details>


### [208] [DarkPatterns-LLM: A Multi-Layer Benchmark for Detecting Manipulative and Harmful AI Behavior](https://arxiv.org/abs/2512.22470)
*Sadia Asif,Israel Antonio Rosales Laguan,Haris Khan,Shumaila Asif,Muneeb Asif*

Main category: cs.AI

TL;DR: 该论文提出了一套名为DarkPatterns-LLM的基准数据集和诊断框架，旨在细粒度地评估大型语言模型（LLMs）输出中的操纵性内容，并通过多尺度意图分析等方法，揭示了模型在检测自主权侵害模式方面存在的显著性能差异。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）的普及，人们对其可能产生的误导性和欺骗性行为越来越担忧。现有安全基准主要依赖于粗略的二元标签，难以捕捉操纵的心理和社会机制。本文提出DarkPatterns-LLM以填补这一空白，提供一个标准化的多维度基准框架，用于检测LLMs中的操纵性内容。

Method: 该研究集成了多粒度检测（MGD）、多尺度意图分析（MSIAN）、威胁谐波协议（THP）和深度语境风险对齐（DCRA）四个层级的分析管道，建立了包含401个精心策划的示例（包括指令-响应对和专家注释）的基准数据集。

Result: 在评估GPT-4、Claude 3.5和LaMA-3-70B等尖端模型时，研究发现模型在检测自主权侵害模式方面存在显著差异，准确率在65.2%到89.7%之间。

Conclusion: DarkPatterns-LLM是首个标准化、多维度的基准框架，用于检测LLMs中的操纵性内容，对推动更值得信赖的人工智能系统具有重要意义。

Abstract: The proliferation of Large Language Models (LLMs) has intensified concerns about manipulative or deceptive behaviors that can undermine user autonomy, trust, and well-being. Existing safety benchmarks predominantly rely on coarse binary labels and fail to capture the nuanced psychological and social mechanisms constituting manipulation. We introduce \textbf{DarkPatterns-LLM}, a comprehensive benchmark dataset and diagnostic framework for fine-grained assessment of manipulative content in LLM outputs across seven harm categories: Legal/Power, Psychological, Emotional, Physical, Autonomy, Economic, and Societal Harm. Our framework implements a four-layer analytical pipeline comprising Multi-Granular Detection (MGD), Multi-Scale Intent Analysis (MSIAN), Threat Harmonization Protocol (THP), and Deep Contextual Risk Alignment (DCRA). The dataset contains 401 meticulously curated examples with instruction-response pairs and expert annotations. Through evaluation of state-of-the-art models including GPT-4, Claude 3.5, and LLaMA-3-70B, we observe significant performance disparities (65.2\%--89.7\%) and consistent weaknesses in detecting autonomy-undermining patterns. DarkPatterns-LLM establishes the first standardized, multi-dimensional benchmark for manipulation detection in LLMs, offering actionable diagnostics toward more trustworthy AI systems.

</details>


### [209] [Lessons from Neuroscience for AI: How integrating Actions, Compositional Structure and Episodic Memory could enable Safe, Interpretable and Human-Like AI](https://arxiv.org/abs/2512.22568)
*Rajesh P. N. Rao,Vishwas Sathish,Linxing Preston Jiang,Matthew Bryan,Prashant Rangarajan*

Main category: cs.AI

TL;DR: 文章指出现有的基础模型在预测编码模型的组件方面存在不足，并提出应该集成行动、层次组合结构和情景记忆，以克服幻觉、表层理解和安全性等问题。


<details>
  <summary>Details</summary>
Motivation: 文章旨在指出当前大型语言模型和基础模型在预测编码模型方面存在的不足，以及如何在这些模型中增加相应的组成部分，以实现更安全、更可解释、更加节能且接近人类的认知AI。

Method: 文章回顾了最近来自神经科学和认知科学领域的证据，并讨论了如何通过添加行动、层次组合结构和情景记忆这些组件来改进现有的基础模型。

Result: 文章提供了一种新方法，通过集选手动、层次组合结构和情景记忆来改进基础模型，以解决幻觉、表层理解和安全性等问题，并讨论了这种方法如何与当前的趋势（如添加链式推理和检索增强生成）相结合。

Conclusion: 文章最后呼吁重启神经科学与AI之间的思想交流，以此推动更具安全性和可解释性的AI的发展。

Abstract: The phenomenal advances in large language models (LLMs) and other foundation models over the past few years have been based on optimizing large-scale transformer models on the surprisingly simple objective of minimizing next-token prediction loss, a form of predictive coding that is also the backbone of an increasingly popular model of brain function in neuroscience and cognitive science. However, current foundation models ignore three other important components of state-of-the-art predictive coding models: tight integration of actions with generative models, hierarchical compositional structure, and episodic memory. We propose that to achieve safe, interpretable, energy-efficient, and human-like AI, foundation models should integrate actions, at multiple scales of abstraction, with a compositional generative architecture and episodic memory. We present recent evidence from neuroscience and cognitive science on the importance of each of these components. We describe how the addition of these missing components to foundation models could help address some of their current deficiencies: hallucinations and superficial understanding of concepts due to lack of grounding, a missing sense of agency/responsibility due to lack of control, threats to safety and trustworthiness due to lack of interpretability, and energy inefficiency. We compare our proposal to current trends, such as adding chain-of-thought (CoT) reasoning and retrieval-augmented generation (RAG) to foundation models, and discuss new ways of augmenting these models with brain-inspired components. We conclude by arguing that a rekindling of the historically fruitful exchange of ideas between brain science and AI will help pave the way towards safe and interpretable human-centered AI.

</details>


### [210] [Tyee: A Unified, Modular, and Fully-Integrated Configurable Toolkit for Intelligent Physiological Health Care](https://arxiv.org/abs/2512.22601)
*Tao Zhou,Lingyu Shu,Zixing Zhang,Jing Han*

Main category: cs.AI

TL;DR: Tyee 提供了一个统一的、模块化的设计，针对生理信号分析的问题提供了解决方案，包括统一的数据接口、可配置的预处理流水线、模块化架构和支持端到端的工作流配置。


<details>
  <summary>Details</summary>
Motivation: 为了解决生理信号分析中的数据格式异构性、预处理策略不一致、模型管道分割和实验设置难以复现的问题。

Method: 通过设计一个统一的数据接口和可配置的预处理流水线，提供一个模块化和可扩展的架构，以及支持端到端工作流配置的功能。

Result: Tyee 在所有评估任务中表现出一致的实用效果和泛化能力，覆盖 12 种信号模态，且在 12 个数据集中取得了最先进的成果。

Conclusion: Tyee 提供了一个灵活且可配置的工具包，对促进生理信号分析领域的研究工作有重要作用。

Abstract: Deep learning has shown great promise in physiological signal analysis, yet its progress is hindered by heterogeneous data formats, inconsistent preprocessing strategies, fragmented model pipelines, and non-reproducible experimental setups. To address these limitations, we present Tyee, a unified, modular, and fully-integrated configurable toolkit designed for intelligent physiological healthcare. Tyee introduces three key innovations: (1) a unified data interface and configurable preprocessing pipeline for 12 kinds of signal modalities; (2) a modular and extensible architecture enabling flexible integration and rapid prototyping across tasks; and (3) end-to-end workflow configuration, promoting reproducible and scalable experimentation. Tyee demonstrates consistent practical effectiveness and generalizability, outperforming or matching baselines across all evaluated tasks (with state-of-the-art results on 12 of 13 datasets). The Tyee toolkit is released at https://github.com/SmileHnu/Tyee and actively maintained.

</details>


### [211] [Learning Multi-Modal Mobility Dynamics for Generalized Next Location Recommendation](https://arxiv.org/abs/2512.22605)
*Junshu Dai,Yu Wang,Tongya Zheng,Wei Ji,Qinghong Guo,Ji Cao,Jie Song,Canghong Jin,Mingli Song*

Main category: cs.AI

TL;DR: 本文提出了一种名为M^3ob的方法，利用多模态时空知识图谱（STRG和STKG）来捕捉时空动态特征，并通过门控机制融合多模态表示，引入时空动态知识到静态图像模态中，从而在正常和异常场景下均能显著提升位置推荐任务的效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法在单一模式下受限于数据稀疏性和内在偏差，在多模式下难以捕捉由于静态多模态表示与时空动态之间语义差距引起的时空动态。本文提出的方法旨在克服这些限制，通过引入多模式时空知识来更好地捕捉移动性动态。

Method: 本文通过构建一种融合了多模态时空关系图（STRG）和大语言模型增强的时空知识图谱（STKG），提出了一个门控机制来融合不同的模态的时空图表示，并提出了一种时空知识图谱引导的跨模态对齐方法，将时空动态知识注入到静态图像模态中。

Result: 在六个多公开数据集上的实验证明，本文提出的方法即使在正常情况下也能够持续改善性能，在异常情况下也表现出显著的泛化能力。

Conclusion: 本文提出了一种创新的方法来解决多模态时空数据分析中遇到的问题，并通过在多个公开数据集上的实验验证了该方法的有效性和较强的泛化性能。

Abstract: The precise prediction of human mobility has produced significant socioeconomic impacts, such as location recommendations and evacuation suggestions. However, existing methods suffer from limited generalization capability: unimodal approaches are constrained by data sparsity and inherent biases, while multi-modal methods struggle to effectively capture mobility dynamics caused by the semantic gap between static multi-modal representation and spatial-temporal dynamics. Therefore, we leverage multi-modal spatial-temporal knowledge to characterize mobility dynamics for the location recommendation task, dubbed as \textbf{M}ulti-\textbf{M}odal \textbf{Mob}ility (\textbf{M}$^3$\textbf{ob}). First, we construct a unified spatial-temporal relational graph (STRG) for multi-modal representation, by leveraging the functional semantics and spatial-temporal knowledge captured by the large language models (LLMs)-enhanced spatial-temporal knowledge graph (STKG). Second, we design a gating mechanism to fuse spatial-temporal graph representations of different modalities, and propose an STKG-guided cross-modal alignment to inject spatial-temporal dynamic knowledge into the static image modality. Extensive experiments on six public datasets show that our proposed method not only achieves consistent improvements in normal scenarios but also exhibits significant generalization ability in abnormal scenarios.

</details>


### [212] [The Wisdom of Deliberating AI Crowds: Does Deliberation Improve LLM-Based Forecasting?](https://arxiv.org/abs/2512.22625)
*Paul Schneider,Amalie Schramm*

Main category: cs.AI

TL;DR: 本研究通过让大语言模型（GPT-5、Claude Sonnet 4.5、Gemini Pro 2.5）进行相互审阅并更新预报，以提高准确性。结果表明，在多样化的模型间共享信息的情况下，这种干预措施显著提到了准确度（减少0.020的对数损失，相对减少4%），但在同质化模型中则没有观察到益处。额外提供背景信息并未改善预测准确度。


<details>
  <summary>Details</summary>
Motivation: 探究类似强化人类决策者表现的结构化讨论方法是否也能提高大语言模型的预测准确性。

Method: 利用前202个已解决的二分类问题，在大型语言模型中测试不同情境下的预测准确性，包括多样化的分布信息、多样化的共享信息、同质分布信息和同质共享信息。

Result: 在多样化模型间共享信息的情境下，干预显著提高了模型的准确度（相对减少4%），而在同质模型中未观察到益处。尽管提供了额外的背景信息，也没有提升预测的准确性。

Conclusion: 研究结果表明，结构化讨论可能是提高大型语言模型预测准确性的策略之一，尤其是当模型间存在多样性和信息共享时。

Abstract: Structured deliberation has been found to improve the performance of human forecasters. This study investigates whether a similar intervention, i.e. allowing LLMs to review each other's forecasts before updating, can improve accuracy in large language models (GPT-5, Claude Sonnet 4.5, Gemini Pro 2.5). Using 202 resolved binary questions from the Metaculus Q2 2025 AI Forecasting Tournament, accuracy was assessed across four scenarios: (1) diverse models with distributed information, (2) diverse models with shared information, (3) homogeneous models with distributed information, and (4) homogeneous models with shared information. Results show that the intervention significantly improves accuracy in scenario (2), reducing Log Loss by 0.020 or about 4 percent in relative terms (p = 0.017). However, when homogeneous groups (three instances of the same model) engaged in the same process, no benefit was observed. Unexpectedly, providing LLMs with additional contextual information did not improve forecast accuracy, limiting our ability to study information pooling as a mechanism. Our findings suggest that deliberation may be a viable strategy for improving LLM forecasting.

</details>


### [213] [DICE: Discrete Interpretable Comparative Evaluation with Probabilistic Scoring for Retrieval-Augmented Generation](https://arxiv.org/abs/2512.22629)
*Shiyan Liu,Jian Ma,Rui Qu*

Main category: cs.AI

TL;DR: DICE, a novel two-stage framework, introduces probabilistic scoring to enhance explainability and robustness in evaluating RAG systems, achieving 85.7% agreement with human experts and a significant reduction in computational complexity.


<details>
  <summary>Details</summary>
Motivation: 随着RAG系统的复杂性增加，现有的可量化评估标准由于可解释性差、不确定性量化不足和计算效率低的问题，使得RAG技术的负责任部署变得困难。

Method: DICE采用两阶段方法，结合深度分析推理和概率性{A,B,平局}评分，生成透明、带有信心度的判断，支持基于可解释推理轨迹的系统改进，并采用瑞士制比赛机制降低计算复杂性。

Result: DICE在八个系统的评估中实现了42.9%的计算复杂性降低，保持了排名准确性，并且在中文金融问答数据集上的验证结果表明，DICE与人类专家的共识达到了85.7%，大幅优于当前基于大语言模型的指标如RAGAS。

Conclusion: DICE确立了一种负责任、可解释且高效的RAG系统评估范式，有助于推进RAG技术的可信赖应用。

Abstract: As Retrieval-Augmented Generation (RAG) systems evolve toward more sophisticated architectures, ensuring their trustworthiness through explainable and robust evaluation becomes critical. Existing scalar metrics suffer from limited interpretability, inadequate uncertainty quantification, and computational inefficiency in multi-system comparisons, hindering responsible deployment of RAG technologies. We introduce DICE (Discrete Interpretable Comparative Evaluation), a two-stage, evidence-coupled framework that advances explainability and robustness in RAG evaluation. DICE combines deep analytical reasoning with probabilistic $\{A, B, Tie\}$ scoring to produce transparent, confidence-aware judgments that support accountable system improvement through interpretable reasoning traces, enabling systematic error diagnosis and actionable insights. To address efficiency challenges at scale, DICE employs a Swiss-system tournament that reduces computational complexity from $O(N^2)$ to $O(N \log N)$, achieving a 42.9% reduction in our eight-system evaluation while preserving ranking fidelity. Validation on a curated Chinese financial QA dataset demonstrates that DICE achieves 85.7% agreement with human experts, substantially outperforming existing LLM-based metrics such as RAGAS. Our results establish DICE as a responsible, explainable, and efficient paradigm for trustworthy RAG system assessment.

</details>


### [214] [SAMP-HDRL: Segmented Allocation with Momentum-Adjusted Utility for Multi-agent Portfolio Management via Hierarchical Deep Reinforcement Learning](https://arxiv.org/abs/2512.22895)
*Xiaotian Ren,Nuerxiati Abudurexiti,Zhengyong Jiang,Angelos Stefanidis,Hongbin Liu,Jionglong Su*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Portfolio optimization in non-stationary markets is challenging due to regime shifts, dynamic correlations, and the limited interpretability of deep reinforcement learning (DRL) policies. We propose a Segmented Allocation with Momentum-Adjusted Utility for Multi-agent Portfolio Management via Hierarchical Deep Reinforcement Learning (SAMP-HDRL). The framework first applies dynamic asset grouping to partition the market into high-quality and ordinary subsets. An upper-level agent extracts global market signals, while lower-level agents perform intra-group allocation under mask constraints. A utility-based capital allocation mechanism integrates risky and risk-free assets, ensuring coherent coordination between global and local decisions. backtests across three market regimes (2019--2021) demonstrate that SAMP-HDRL consistently outperforms nine traditional baselines and nine DRL benchmarks under volatile and oscillating conditions. Compared with the strongest baseline, our method achieves at least 5\% higher Return, 5\% higher Sharpe ratio, 5\% higher Sortino ratio, and 2\% higher Omega ratio, with substantially larger gains observed in turbulent markets. Ablation studies confirm that upper--lower coordination, dynamic clustering, and capital allocation are indispensable to robustness. SHAP-based interpretability further reveals a complementary ``diversified + concentrated'' mechanism across agents, providing transparent insights into decision-making. Overall, SAMP-HDRL embeds structural market constraints directly into the DRL pipeline, offering improved adaptability, robustness, and interpretability in complex financial environments.

</details>


### [215] [HiSciBench: A Hierarchical Multi-disciplinary Benchmark for Scientific Intelligence from Reading to Discovery](https://arxiv.org/abs/2512.22899)
*Yaping Zhang,Qixuan Zhang,Xingquan Zhang,Zhiyuan Chen,Wenwen Zhuang,Yupu Liang,Lu Xiang,Yang Zhao,Jiajun Zhang,Yu Zhou,Chengqing Zong*

Main category: cs.AI

TL;DR: HiSciBench 是一个多学科、多层次的层次化基准测试，旨在评估基础模型在五个科学工作流级别中的表现，支持多模态输入和跨语言评估，填补了现有科学智能基准的空白。


<details>
  <summary>Details</summary>
Motivation: 随着大规模语言模型和多模态基础模型的发展，科研领域对这些模型的兴趣日益浓厚。然而，当前的科学智能评估基准仍较为碎片化，未能全面反映复杂科学发现过程。
HiSciBench 的目的是提供一个集成的层次化框架，以评估模型在科学理解、文献解析、信息检索、综述生成和发现方面的综合能力。

Method: HiSciBench 设计了五个层次的评估标准来模拟完整的科学研究流程，涵盖科学素养、文献解析、基于文献的问题回答、文献综述生成和科学研究发现。该基准使用了8,735个手工精挑细选的多模态实例，覆盖六大科学领域，在多模态输入和跨语言评估方面有所建树。

Result: 在对GPT-5, DeepSeek-R1, 和其他多模态系统进行的测试中，结果显示在基础科学素养任务上最先进模型的准确性达到69%，但而对于最高等级的发现性挑战，准确率急剧下降至25%。

Conclusion: HiSciBench 已经建立了评估科学智能的标准，并为未来模型开发提供了参考，旨在提升模型的全面科学能力，同时确保其可靠性。该基准测试已公开发布，以促进未来的研究工作。

Abstract: The rapid advancement of large language models (LLMs) and multimodal foundation models has sparked growing interest in their potential for scientific research. However, scientific intelligence encompasses a broad spectrum of abilities ranging from understanding fundamental knowledge to conducting creative discovery, and existing benchmarks remain fragmented. Most focus on narrow tasks and fail to reflect the hierarchical and multi-disciplinary nature of real scientific inquiry. We introduce \textbf{HiSciBench}, a hierarchical benchmark designed to evaluate foundation models across five levels that mirror the complete scientific workflow: \textit{Scientific Literacy} (L1), \textit{Literature Parsing} (L2), \textit{Literature-based Question Answering} (L3), \textit{Literature Review Generation} (L4), and \textit{Scientific Discovery} (L5). HiSciBench contains 8,735 carefully curated instances spanning six major scientific disciplines, including mathematics, physics, chemistry, biology, geography, and astronomy, and supports multimodal inputs including text, equations, figures, and tables, as well as cross-lingual evaluation. Unlike prior benchmarks that assess isolated abilities, HiSciBench provides an integrated, dependency-aware framework that enables detailed diagnosis of model capabilities across different stages of scientific reasoning. Comprehensive evaluations of leading models, including GPT-5, DeepSeek-R1, and several multimodal systems, reveal substantial performance gaps: while models achieve up to 69\% accuracy on basic literacy tasks, performance declines sharply to 25\% on discovery-level challenges. HiSciBench establishes a new standard for evaluating scientific Intelligence and offers actionable insights for developing models that are not only more capable but also more reliable. The benchmark will be publicly released to facilitate future research.

</details>


### [216] [Geometric Structural Knowledge Graph Foundation Model](https://arxiv.org/abs/2512.22931)
*Ling Xin,Mojtaba Nayyeri,Zahra Makki Nayeri,Steffen Staab*

Main category: cs.AI

TL;DR: Gamma 是一种新型的基础模型，通过引入多头几何注意力机制改进知识图谱推理，相较于现有方法 Ultra，Gamma 在零样本归纳链接预测任务中表现更优，特别是在多种基准测试中提升了 4.4% 的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法如 Ultra 依赖单一关系变换（例如元素-wise 乘法），这会限制模型的表达能力并难以捕捉各种关系和结构模式。因此，提出 Gamma 旨在通过引入多种基于几何的注意力机制增加模型的表达能力。

Method: Gamma 引入了多头几何注意力机制，基于实数、复数、分裂复数和虚数组合了多种关系变换，增强了模型的表达能力。还设计了一个基于关系的条件注意力融合机制，通过轻量级门控和熵正则化在链接级别适配性地融合这些变换。

Result: 在 56 个多样化知识图谱的全面实验中，Gamma 在零样本归纳链接预测任务中表现出色，比 Ultra 在归纳基准测试中平均提高了 5.5% 的平均倒数排名，所有基准测试中提升了 4.4% 的性能。

Conclusion: Gamma 的多头几何注意力机制为知识图谱推理提供了更强的表达能力，并且在实际应用中展示了优越的性能。

Abstract: Structural knowledge graph foundation models aim to generalize reasoning to completely new graphs with unseen entities and relations. A key limitation of existing approaches like Ultra is their reliance on a single relational transformation (e.g., element-wise multiplication) in message passing, which can constrain expressiveness and fail to capture diverse relational and structural patterns exhibited on diverse graphs. In this paper, we propose Gamma, a novel foundation model that introduces multi-head geometric attention to knowledge graph reasoning. Gamma replaces the single relational transformation with multiple parallel ones, including real, complex, split-complex, and dual number based transformations, each designed to model different relational structures. A relational conditioned attention fusion mechanism then adaptively fuses them at link level via a lightweight gating with entropy regularization, allowing the model to robustly emphasize the most appropriate relational bias for each triple pattern. We present a full formalization of these algebraic message functions and discuss how their combination increases expressiveness beyond any single space. Comprehensive experiments on 56 diverse knowledge graphs demonstrate that Gamma consistently outperforms Ultra in zero-shot inductive link prediction, with a 5.5% improvement in mean reciprocal rank on the inductive benchmarks and a 4.4% improvement across all benchmarks, highlighting benefits from complementary geometric representations.

</details>


### [217] [Problems With Large Language Models for Learner Modelling: Why LLMs Alone Fall Short for Responsible Tutoring in K--12 Education](https://arxiv.org/abs/2512.23036)
*Danial Hooshyar,Yeongwook Yang,Gustav Šíř,Tommi Kärkkäinen,Raija Hämäläinen,Mutlu Cukurova,Roger Azevedo*

Main category: cs.AI

TL;DR: 本研究比较了深度知识跟踪（DKT）模型和广泛使用的大型语言模型（LLM）在评估学习者随时间变化的知识时的表现，发现DKT在区分能力和时间一致性方面显著优于LLM，尽管LLM经过微调后表现有所提升，但仍无法与DKT相匹敌。因此，研究强调责任辅教导需要结合学习者建模。


<details>
  <summary>Details</summary>
Motivation: 鉴于 EU AI 法规将 K-12 设置归类为高风险领域，本研究旨在通过实证研究来澄清 LLM 基础的辅导师可能带来的误解，考虑在具有适应性支持的环境中评估学生掌握的情况。

Method: 本研究使用了一种大型开放访问数据集，比较了 DKT 模型和经过零样本和微调的 LLM 在预测下一步正确性和评估学习者掌握水平上的表现。

Result: 研究表明，DKT 在下一步正确性预测的区分性能上最高（AUC = 0.83），在所有评估设置中持续优于 LLM。尽管微调能将 LLM 的 AUC 提高约 8%，但仍低于 DKT 6%，并且在早期序列错误方面表现更差。时间分析显示，DKT 保持稳定且方向正确的掌握更新，而 LLM 变体则表现出时间上的不足，包括不一致和错误方向的更新。

Conclusion: 本研究的结果表明，仅靠 LLMs 很难达到成熟的人工智能辅导系统的有效性，负责任的辅导需要结合学习者建模。因此，推荐混合框架来实现这一目标。

Abstract: The rapid rise of large language model (LLM)-based tutors in K--12 education has fostered a misconception that generative models can replace traditional learner modelling for adaptive instruction. This is especially problematic in K--12 settings, which the EU AI Act classifies as high-risk domain requiring responsible design. Motivated by these concerns, this study synthesises evidence on limitations of LLM-based tutors and empirically investigates one critical issue: the accuracy, reliability, and temporal coherence of assessing learners' evolving knowledge over time. We compare a deep knowledge tracing (DKT) model with a widely used LLM, evaluated zero-shot and fine-tuned, using a large open-access dataset. Results show that DKT achieves the highest discrimination performance (AUC = 0.83) on next-step correctness prediction and consistently outperforms the LLM across settings. Although fine-tuning improves the LLM's AUC by approximately 8\% over the zero-shot baseline, it remains 6\% below DKT and produces higher early-sequence errors, where incorrect predictions are most harmful for adaptive support. Temporal analyses further reveal that DKT maintains stable, directionally correct mastery updates, whereas LLM variants exhibit substantial temporal weaknesses, including inconsistent and wrong-direction updates. These limitations persist despite the fine-tuned LLM requiring nearly 198 hours of high-compute training, far exceeding the computational demands of DKT. Our qualitative analysis of multi-skill mastery estimation further shows that, even after fine-tuning, the LLM produced inconsistent mastery trajectories, while DKT maintained smooth and coherent updates. Overall, the findings suggest that LLMs alone are unlikely to match the effectiveness of established intelligent tutoring systems, and that responsible tutoring requires hybrid frameworks that incorporate learner modelling.

</details>


### [218] [Benchmark Success, Clinical Failure: When Reinforcement Learning Optimizes for Benchmarks, Not Patients](https://arxiv.org/abs/2512.23090)
*Armin Berger,Manuela Bergau,Helen Schneider,Saad Ahmad,Tom Anglim Lagones,Gianluca Brugnara,Martha Foltyn-Dumitru,Kai Schlamp,Philipp Vollmuth,Rafet Sifa*

Main category: cs.AI

TL;DR: ChexReason 是一种通过 R1 样式方法（SFT 后跟 GRPO）训练的视觉-语言模型，仅使用 2,000 个 SFT 样本、1,000 个 RL 样本和单个 A100 GPU。该研究揭示了两类性能之间的矛盾，GRPO 在 CheXpert 数据集上提高了内部性能，但在 NIH 数据集上降低了跨数据集的转移性能。


<details>
  <summary>Details</summary>
Motivation: 探索基于强化学习的大型语言模型在受限资源条件下应用于医疗成像领域的可能性，以及识别 R1 方法（SFT 后跟 GRPO）的表现特点。

Method: 引入了一种基于大型预训练语言模型的视觉-语言模型 ChexReason，并使用 R1 样式方法进行训练，包括 Segmentation Fine-Tuning (SFT) 和 Gradient Retargeting Policy Optimization (GRPO)。

Result: ChexReason 在 CheXpert 数据集上的表现优于 NIH 数据集，揭示了内部数据集和外部数据集之间的性能差距。GRPO 在 CheXpert 上带来了 23% 的改进（宏-F1 评分为 0.346），但在 NIH 上则导致了 19% 的性能下降。这表明 RL 架构本身可能存在固有的问题，而不是规模不足。

Conclusion: 研究结果表明，精心设计的监督微调可能比激进的 RL 方法更适合临床应用，需要跨各类人群的稳健性。

Abstract: Recent Reinforcement Learning (RL) advances for Large Language Models (LLMs) have improved reasoning tasks, yet their resource-constrained application to medical imaging remains underexplored. We introduce ChexReason, a vision-language model trained via R1-style methodology (SFT followed by GRPO) using only 2,000 SFT samples, 1,000 RL samples, and a single A100 GPU. Evaluations on CheXpert and NIH benchmarks reveal a fundamental tension: GRPO recovers in-distribution performance (23% improvement on CheXpert, macro-F1 = 0.346) but degrades cross-dataset transferability (19% drop on NIH). This mirrors high-resource models like NV-Reason-CXR-3B, suggesting the issue stems from the RL paradigm rather than scale. We identify a generalization paradox where the SFT checkpoint uniquely improves on NIH before optimization, indicating teacher-guided reasoning captures more institution-agnostic features. Furthermore, cross-model comparisons show structured reasoning scaffolds benefit general-purpose VLMs but offer minimal gain for medically pre-trained models. Consequently, curated supervised fine-tuning may outperform aggressive RL for clinical deployment requiring robustness across diverse populations.

</details>


### [219] [InSPO: Unlocking Intrinsic Self-Reflection for LLM Preference Optimization](https://arxiv.org/abs/2512.23126)
*Yu Li,Tian Lan,Zhengling Qi*

Main category: cs.AI

TL;DR: 本文提出了一种新的优化方法Intrinsic Self-reflective Preference Optimization (合计，简称「」)，旨在解决直接偏好优化（DPO）及其变体中存在的两个基本问题：依赖于随意建模选择导致的行为反映参数化而非真偏好，以及独立处理响应生成导致未能利用成对数据中的比较信息，忽略了模型的内在自我反思能力。通过结合上下文和替代响应信息，「」能够产生全局最优策略，优于DPO/RLHF并且确保对建模选择不变。实验结果证实了这种方法可以提高胜率和长度控制指标，使模型更稳健、更符合人类偏好。


<details>
  <summary>Details</summary>
Motivation: 为了解决直接偏好优化（DPO）及其变体在对齐大型语言模型时存在的两大关键问题：依赖于随意的建模选择带来的参数化刻板行为，以及脱离响应生成的整体上下文，未能利用成对数据中的比较信息，忽略了模型的自我反思能力。

Method: 合计提出了一个结合上下文和替代响应信息来生成全局最优策略的方法，命名为Intrinsic Self-reflective Preference Optimization (「」)，这个方法在理论上证明了它优于DPO/RLHF并且不受建模选择影响。

Result: 合计的结果表明，在实验中，「」在胜率和长度控制等关键指标上都表现出持续的改进，证明了激活自我反思确实可以使模型更稳定、更符合人类偏好。

Conclusion: 使用Intrinsic Self-reflective Preference Optimization (「」)可以在不对模型架构进行更改或增加推理负担的情况下，增强预训练的大型语言模型，以实现更稳健且更符合人类偏好的对齐。

Abstract: Direct Preference Optimization (DPO) and its variants have become standard for aligning Large Language Models due to their simplicity and offline stability. However, we identify two fundamental limitations. First, the optimal policy depends on arbitrary modeling choices (scalarization function, reference policy), yielding behavior reflecting parameterization artifacts rather than true preferences. Second, treating response generation in isolation fails to leverage comparative information in pairwise data, leaving the model's capacity for intrinsic self-reflection untapped. To address it, we propose Intrinsic Self-reflective Preference Optimization (\q), deriving a globally optimal policy conditioning on both context and alternative responses. We prove this formulation superior to DPO/RLHF while guaranteeing invariance to scalarization and reference choices. \q~serves as a plug-and-play enhancement without architectural changes or inference overhead. Experiments demonstrate consistent improvements in win rates and length-controlled metrics, validating that unlocking self-reflection yields more robust, human-aligned LLMs.

</details>


### [220] [Why We Need a New Framework for Emotional Intelligence in AI](https://arxiv.org/abs/2512.23163)
*Max Parks,Kheli Atluru,Meera Vinod,Mike Kuniavsky,Jud Brewer,Sean White,Sarah Adler,Wendy Ju*

Main category: cs.AI

TL;DR: 本文提出了当前用于评估人工智能系统情感智能的框架需要改进的观点，因为这些框架未能全面衡量AI系统所需的情感智能各方面。


<details>
  <summary>Details</summary>
Motivation: 现有框架未能全面评估AI系统的情感智能，因为它们无法反映人类情感智能中的人文和理解层面。本文旨在提供改进评估策略的选项。

Method: 首先回顾关于情感和总体情感智能的不同理论，评估其在人工智能系统中的适用性。然后，严格评估现有的基准框架，并识别它们在衡量情感智能时的不足之处。最后，提出改进评估策略的建议。

Result: 提出了改进评估策略的建议，以避免当前表现评估中的不足之处。

Conclusion: 本文强调有必要改进现有框架，以更全面地评估AI系统的情感智能。

Abstract: In this paper, we develop the position that current frameworks for evaluating emotional intelligence (EI) in artificial intelligence (AI) systems need refinement because they do not adequately or comprehensively measure the various aspects of EI relevant in AI. Human EI often involves a phenomenological component and a sense of understanding that artificially intelligent systems lack; therefore, some aspects of EI are irrelevant in evaluating AI systems. However, EI also includes an ability to sense an emotional state, explain it, respond appropriately, and adapt to new contexts (e.g., multicultural), and artificially intelligent systems can do such things to greater or lesser degrees. Several benchmark frameworks specialize in evaluating the capacity of different AI models to perform some tasks related to EI, but these often lack a solid foundation regarding the nature of emotion and what it is to be emotionally intelligent. In this project, we begin by reviewing different theories about emotion and general EI, evaluating the extent to which each is applicable to artificial systems. We then critically evaluate the available benchmark frameworks, identifying where each falls short in light of the account of EI developed in the first section. Lastly, we outline some options for improving evaluation strategies to avoid these shortcomings in EI evaluation in AI systems.

</details>


### [221] [From Model Choice to Model Belief: Establishing a New Measure for LLM-Based Research](https://arxiv.org/abs/2512.23184)
*Hongshen Sun,Juanjuan Zhang*

Main category: cs.AI

TL;DR: 本文提出并正式定义了"模型信念"这一源自模型在单次生成运行中的令牌级概率的度量，能更高效地利用LLM生成数据中的信息。


<details>
  <summary>Details</summary>
Motivation: 由于传统做法将LLM的输出视为单一数据点，未能充分利用概率性质，文章旨在提出一种更有效的信息提取方法。

Method: 定义了“模型信念”，并证明它在统计效率上优于模型选择，同时还适用于下游应用中的平滑函数。

Result: 在需求估计案例中，模型信念比模型选择更能解释和预测真实模型选择，并且在有限生成次数的情况下能将达到足够精确估计所需的计算量降低约20倍。

Conclusion: 研究结果显示模型信念可用于从LLM生成数据中提取更多信息，成为默认指标。

Abstract: Large language models (LLMs) are increasingly used to simulate human behavior, but common practices to use LLM-generated data are inefficient. Treating an LLM's output ("model choice") as a single data point underutilizes the information inherent to the probabilistic nature of LLMs. This paper introduces and formalizes "model belief," a measure derived from an LLM's token-level probabilities that captures the model's belief distribution over choice alternatives in a single generation run. The authors prove that model belief is asymptotically equivalent to the mean of model choices (a non-trivial property) but forms a more statistically efficient estimator, with lower variance and a faster convergence rate. Analogous properties are shown to hold for smooth functions of model belief and model choice often used in downstream applications. The authors demonstrate the performance of model belief through a demand estimation study, where an LLM simulates consumer responses to different prices. In practical settings with limited numbers of runs, model belief explains and predicts ground-truth model choice better than model choice itself, and reduces the computation needed to reach sufficiently accurate estimates by roughly a factor of 20. The findings support using model belief as the default measure to extract more information from LLM-generated data.

</details>


### [222] [TCEval: Using Thermal Comfort to Assess Cognitive and Perceptual Abilities of AI](https://arxiv.org/abs/2512.23217)
*Jingming Li*

Main category: cs.AI

TL;DR: TCEval 是一种全新的评估框架，旨在评估 AI 在跨模态推理、因果关联和适应性决策方面的认知能力，通过与温湿度舒适度场景和大语言模型的交互来实现。


<details>
  <summary>Details</summary>
Motivation: 当前特定任务的大型语言模型基准存在空白，而温湿度舒适度作为复杂的环境因素和个体感知间的交织作用，适合评估AI系统在现实世界中的认知能力。

Method: TCEval 通过将大语言模型初始化为虚拟人格特质，让它们生成服装隔热选择和反馈，并对照 ASHRAE 全球数据库和中国温湿度舒适度数据库验证输出。实验涉及四个不同的大语言模型。

Result: 实验结果表明，虽然 AI 提供的反馈与人类完全一致有限，但在 1 PMV 容差内方向一致性显著提高。大语言模型生成的 PMV 分布与人类数据存在显著差异，且在离散的温湿度舒适度分类中表现随机。

Conclusion: TCEval 作为一种生态有效性的认知图灵测试，证实了当前大语言模型具备基础的跨模态推理能力，但缺乏对温湿度舒适度中变量间非线性关系的精确因果理解。该工具为 AI 评估提供了一个新视角，即从抽象任务熟练度转向具备嵌入情景感知和决策能力的评估。

Abstract: A critical gap exists in LLM task-specific benchmarks. Thermal comfort, a sophisticated interplay of environmental factors and personal perceptions involving sensory integration and adaptive decision-making, serves as an ideal paradigm for evaluating real-world cognitive capabilities of AI systems. To address this, we propose TCEval, the first evaluation framework that assesses three core cognitive capacities of AI, cross-modal reasoning, causal association, and adaptive decision-making, by leveraging thermal comfort scenarios and large language model (LLM) agents. The methodology involves initializing LLM agents with virtual personality attributes, guiding them to generate clothing insulation selections and thermal comfort feedback, and validating outputs against the ASHRAE Global Database and Chinese Thermal Comfort Database. Experiments on four LLMs show that while agent feedback has limited exact alignment with humans, directional consistency improves significantly with a 1 PMV tolerance. Statistical tests reveal that LLM-generated PMV distributions diverge markedly from human data, and agents perform near-randomly in discrete thermal comfort classification. These results confirm the feasibility of TCEval as an ecologically valid Cognitive Turing Test for AI, demonstrating that current LLMs possess foundational cross-modal reasoning ability but lack precise causal understanding of the nonlinear relationships between variables in thermal comfort. TCEval complements traditional benchmarks, shifting AI evaluation focus from abstract task proficiency to embodied, context-aware perception and decision-making, offering valuable insights for advancing AI in human-centric applications like smart buildings.

</details>


### [223] [Agentic Physical AI toward a Domain-Specific Foundation Model for Nuclear Reactor Control](https://arxiv.org/abs/2512.23292)
*Yoonpyo Lee,Kazuma Kobayashi,Sai Puppala,Sajedul Talukder,Seid Koric,Souvik Chakraborty,Syed Bahauddin Alam*

Main category: cs.AI

TL;DR: 该研究提出了通过使用基于物理的验证驱动策略优化的方法来开发特定领域基础模型的途径，旨在解决一般基础模型在物理系统控制接口上的结构性局限。研究通过训练一个3亿参数的模型，展示出从小型到大型模型的表现转变，并在多个物理和连续输入模态上实现了学习表示的转移。


<details>
  <summary>Details</summary>
Motivation: 现有通用基础模型在基本物理任务上的表现不佳，尤其是与物理约束相关的控制任务，这显示出结构性的限制而非简单的参数扩展不足。因此，研究提出了一种新的路径，为特定领域构建基础模型，以克服这一局限。

Method: 研究人员开发了一个基于物理学的验证驱动的学习方法，通过从合成的反应堆控制场景中训练一个3亿参数的模型，实现了从小规模高方差模仿到大规模模型行为稳定性的转变。

Result: 研究发现，随着模型规模的增加，其行为变得更为稳定，小规模模型在训练分布上的高方差和极端风险被大规模模型显著降低，实现了500倍以上的方差减少。此外，尽管模型经历了强烈的选择性压缩，其学习到的表示仍然能够跨越不同的物理环境和连续输入模态。

Conclusion: 该研究表明，通过基于物理的验证驱动策略优化方法，可以为特定领域构建基础模型，从而克服通用基础模型在控制任务上的结构性局限，实现更稳定和可靠的执行行为。

Abstract: The prevailing paradigm in AI for physical systems, scaling general-purpose foundation models toward universal multimodal reasoning, confronts a fundamental barrier at the control interface. Recent benchmarks show that even frontier vision-language models achieve only 50-53% accuracy on basic quantitative physics tasks, behaving as approximate guessers that preserve semantic plausibility while violating physical constraints. This input unfaithfulness is not a scaling deficiency but a structural limitation. Perception-centric architectures optimize parameter-space imitation, whereas safety-critical control demands outcome-space guarantees over executed actions. Here, we present a fundamentally different pathway toward domain-specific foundation models by introducing compact language models operating as Agentic Physical AI, in which policy optimization is driven by physics-based validation rather than perceptual inference. We train a 360-million-parameter model on synthetic reactor control scenarios, scaling the dataset from 10^3 to 10^5 examples. This induces a sharp phase transition absent in general-purpose models. Small-scale systems exhibit high-variance imitation with catastrophic tail risk, while large-scale models undergo variance collapse exceeding 500x reduction, stabilizing execution-level behavior. Despite balanced exposure to four actuation families, the model autonomously rejects approximately 70% of the training distribution and concentrates 95% of runtime execution on a single-bank strategy. Learned representations transfer across distinct physics and continuous input modalities without architectural modification.

</details>


### [224] [On Conformant Planning and Model-Checking of $\exists^*\forall^*$ Hyperproperties](https://arxiv.org/abs/2512.23324)
*Raven Beutner,Bernd Finkbeiner*

Main category: cs.AI

TL;DR: 本文探讨了规划与验证领域内两个问题之间的连接：容错规划和超属性的模型检查。通过高效的编码方式将超属性的模型检查问题转化为容错规划问题，且证明其稳健性和完整性。


<details>
  <summary>Details</summary>
Motivation: 研究将容错规划与超属性模型检查进行连接的意义在于，提供了一个新的视角来处理复杂的系统属性验证问题，同时利用已有的容错规划算法和工具来解决超属性模型检查中的挑战。

Method: 本文采用了编码方法，将超属性模型检查问题转化为容错规划问题，并证明了这种转化的有效性。

Result: 作者证明了一种高效地将超属性模型检查问题转化为容错规划问题的方法，并且证明了这种转化的稳健性和完整性。

Conclusion: 通过对容错规划与超属性模型检查的联系研究，作者提出了一个将超属性模型检查问题转化为容错规划问题的解决方案，为系统的复杂属性验证提供了一种新的方法。

Abstract: We study the connection of two problems within the planning and verification community: Conformant planning and model-checking of hyperproperties. Conformant planning is the task of finding a sequential plan that achieves a given objective independent of non-deterministic action effects during the plan's execution. Hyperproperties are system properties that relate multiple execution traces of a system and, e.g., capture information-flow and fairness policies. In this paper, we show that model-checking of $\exists^*\forall^*$ hyperproperties is closely related to the problem of computing a conformant plan. Firstly, we show that we can efficiently reduce a hyperproperty model-checking instance to a conformant planning instance, and prove that our encoding is sound and complete. Secondly, we establish the converse direction: Every conformant planning problem is, itself, a hyperproperty model-checking task.

</details>


### [225] [Replay Failures as Successes: Sample-Efficient Reinforcement Learning for Instruction Following](https://arxiv.org/abs/2512.23457)
*Kongcheng Zhang,Qi Yao,Shunyu Liu,Wenjian Zhang,Min Cen,Yang Zhou,Wenkai Fang,Yiru Zhao,Baisheng Lai,Mingli Song*

Main category: cs.AI

TL;DR: 提出了一种名为HiR（Hindsight instruction Replay）的样本高效强化学习方法，通过回顾已满足的约束条件重写并重播失败的响应，利用二元奖励信号实现高效的指令跟随任务学习。


<details>
  <summary>Details</summary>
Motivation: 目前的大型语言模型在遵循复杂指令时面临挑战，主要由于其能力有限，难以生成满足所有约束条件的响应，这导致了稀疏或难以区分的奖励信号，影响学习过程。HiR方法旨在解决这一问题。

Method: HiR方法采用选择-重写策略，对失败的响应进行回顾性处理，将其作为成功的案例重新加入训练集。通过这种策略，模型在仅使用二元奖励信号的情况下进行强化学习，优化指令和响应层面的双重偏好。

Result: 实验结果表明，HiR方法在多种指令跟随任务中表现出了显著的优越性，且相比原有方法所需的计算资源更少。

Conclusion: HiR是一个创新的样本高效强化学习框架，能够有效提高大型语言模型在复杂指令跟随任务中的表现，验证了其在资源限制条件下的效率和适用性。

Abstract: Reinforcement Learning (RL) has shown promise for aligning Large Language Models (LLMs) to follow instructions with various constraints. Despite the encouraging results, RL improvement inevitably relies on sampling successful, high-quality responses; however, the initial model often struggles to generate responses that satisfy all constraints due to its limited capabilities, yielding sparse or indistinguishable rewards that impede learning. In this work, we propose Hindsight instruction Replay (HiR), a novel sample-efficient RL framework for complex instruction following tasks, which employs a select-then-rewrite strategy to replay failed attempts as successes based on the constraints that have been satisfied in hindsight. We perform RL on these replayed samples as well as the original ones, theoretically framing the objective as dual-preference learning at both the instruction- and response-level to enable efficient optimization using only a binary reward signal. Extensive experiments demonstrate that the proposed HiR yields promising results across different instruction following tasks, while requiring less computational budget. Our code and dataset is available at https://github.com/sastpg/HIR.

</details>


### [226] [The Gaining Paths to Investment Success: Information-Driven LLM Graph Reasoning for Venture Capital Prediction](https://arxiv.org/abs/2512.23489)
*Haoyu Pei,Zhongyang Liu,Xiangyi Xiao,Xiaocong Du,Haipeng Zhang,Kunpeng Zhang,Suting Hong*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Most venture capital (VC) investments fail, while a few deliver outsized returns. Accurately predicting startup success requires synthesizing complex relational evidence, including company disclosures, investor track records, and investment network structures, through explicit reasoning to form coherent, interpretable investment theses. Traditional machine learning and graph neural networks both lack this reasoning capability. Large language models (LLMs) offer strong reasoning but face a modality mismatch with graphs. Recent graph-LLM methods target in-graph tasks where answers lie within the graph, whereas VC prediction is off-graph: the target exists outside the network. The core challenge is selecting graph paths that maximize predictor performance on an external objective while enabling step-by-step reasoning. We present MIRAGE-VC, a multi-perspective retrieval-augmented generation framework that addresses two obstacles: path explosion (thousands of candidate paths overwhelm LLM context) and heterogeneous evidence fusion (different startups need different analytical emphasis). Our information-gain-driven path retriever iteratively selects high-value neighbors, distilling investment networks into compact chains for explicit reasoning. A multi-agent architecture integrates three evidence streams via a learnable gating mechanism based on company attributes. Under strict anti-leakage controls, MIRAGE-VC achieves +5.0% F1 and +16.6% PrecisionAt5, and sheds light on other off-graph prediction tasks such as recommendation and risk assessment. Code: https://anonymous.4open.science/r/MIRAGE-VC-323F.

</details>


### [227] [Why AI Safety Requires Uncertainty, Incomplete Preferences, and Non-Archimedean Utilities](https://arxiv.org/abs/2512.23508)
*Alessio Benavoli,Alessandro Facchini,Marco Zaffalon*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: How can we ensure that AI systems are aligned with human values and remain safe? We can study this problem through the frameworks of the AI assistance and the AI shutdown games. The AI assistance problem concerns designing an AI agent that helps a human to maximise their utility function(s). However, only the human knows these function(s); the AI assistant must learn them. The shutdown problem instead concerns designing AI agents that: shut down when a shutdown button is pressed; neither try to prevent nor cause the pressing of the shutdown button; and otherwise accomplish their task competently. In this paper, we show that addressing these challenges requires AI agents that can reason under uncertainty and handle both incomplete and non-Archimedean preferences.

</details>


### [228] [Divergent-Convergent Thinking in Large Language Models for Creative Problem Generation](https://arxiv.org/abs/2512.23601)
*Manh Hung Nguyen,Adish Singla*

Main category: cs.AI

TL;DR: 本文提出了一种新的两阶段提示方法CreativeDC，用于生成教育问题。这种方法通过分离创造性的探索和约束满足，使大语言模型能够生成更具多样性和新颖性的创造性问题。


<details>
  <summary>Details</summary>
Motivation: 面对大语言模型产生的问题同质化问题，作者提出了CreativeDC方法，旨在提升问题的多样性和新颖性，同时保持高实用性。

Method: CreativeDC是一种两阶段的方法，首先进行创造性的探索，然后进行约束满足。这种方法通过分离创造性思维和收敛思维的过程，使得模型能够在生成问题时探索更广泛的思维空间。

Result: 与基线方法相比，CreativeDC在多样性和新颖性方面取得了显著改进，同时保持了高实用性。此外，大规模分析表明，随着样本数量的增加，CreativeDC生成的独特问题数量呈现出更快的增长速度。

Conclusion: CreativeDC在生成教育问题方面取得了显著进展，提供了一种有效的方法来克服大语言模型同质化的限制。

Abstract: Large language models (LLMs) have significant potential for generating educational questions and problems, enabling educators to create large-scale learning materials. However, LLMs are fundamentally limited by the ``Artificial Hivemind'' effect, where they generate similar responses within the same model and produce homogeneous outputs across different models. As a consequence, students may be exposed to overly similar and repetitive LLM-generated problems, which harms diversity of thought. Drawing inspiration from Wallas's theory of creativity and Guilford's framework of divergent-convergent thinking, we propose CreativeDC, a two-phase prompting method that explicitly scaffolds the LLM's reasoning into distinct phases. By decoupling creative exploration from constraint satisfaction, our method enables LLMs to explore a broader space of ideas before committing to a final problem. We evaluate CreativeDC for creative problem generation using a comprehensive set of metrics that capture diversity, novelty, and utility. The results show that CreativeDC achieves significantly higher diversity and novelty compared to baselines while maintaining high utility. Moreover, scaling analysis shows that CreativeDC generates a larger effective number of distinct problems as more are sampled, increasing at a faster rate than baseline methods.

</details>


### [229] [Regret-Based Federated Causal Discovery with Unknown Interventions](https://arxiv.org/abs/2512.23626)
*Federico Baldo,Charles K. Assaad*

Main category: cs.AI

TL;DR: 本文提出了一种名为I-PERI的新型联邦算法，旨在发现未知客户端干预下的因果图。该算法首先恢复客户端图的CPDAG，然后通过利用干预导致的结构差异来定向更多边，从而获得更紧的等价类，称为Φ-马尔可夫等价类，表示为Φ-CPDAG。


<details>
  <summary>Details</summary>
Motivation: 在数据分散和隐私保护的要求下，现有的因果发现方法往往基于所有客户端共享相同因果模型的理想假设。然而，针对不同政策或协议的真实世界应用中，客户端间会存在差异化的干预，这使得已有的方法不适用于实际情况。本文旨在解决这种问题。

Method: I-PERI算法首先通过恢复客户端图的CPDAG来初始化，然后利用各客户端之间的结构差异来定向更多边，从而得到一种新的等价类表示Φ-CPDAG。

Result: 本文提供了I-PERI算法收敛性的理论保证以及其隐私保护特性，并通过合成数据的实验验证了所提算法的有效性。

Conclusion: 本文提出了一种新的联邦因果发现算法I-PERI，考虑了不同客户端的未知干预，相比于现有方法，能够更准确地恢复因果图，同时保证了算法的收敛性和隐私保护性。

Abstract: Most causal discovery methods recover a completed partially directed acyclic graph representing a Markov equivalence class from observational data. Recent work has extended these methods to federated settings to address data decentralization and privacy constraints, but often under idealized assumptions that all clients share the same causal model. Such assumptions are unrealistic in practice, as client-specific policies or protocols, for example, across hospitals, naturally induce heterogeneous and unknown interventions. In this work, we address federated causal discovery under unknown client-level interventions. We propose I-PERI, a novel federated algorithm that first recovers the CPDAG of the union of client graphs and then orients additional edges by exploiting structural differences induced by interventions across clients. This yields a tighter equivalence class, which we call the $\mathbfΦ$-Markov Equivalence Class, represented by the $\mathbfΦ$-CPDAG. We provide theoretical guarantees on the convergence of I-PERI, as well as on its privacy-preserving properties, and present empirical evaluations on synthetic data demonstrating the effectiveness of the proposed algorithm.

</details>


### [230] [Web World Models](https://arxiv.org/abs/2512.23676)
*Jichen Feng,Yifan Zhang,Chenggong Zhang,Yifu Lu,Shilong Liu,Mengdi Wang*

Main category: cs.AI

TL;DR: 本文介绍了Web世界模型(WWM)，这是一种结合了传统数据库和生成式世界模型优点的方法。WWM通过使用普通网页代码实现世界状态和'物理'，确保逻辑一致性，而大语言模型则在结构化的潜在状态上生成上下文、叙述和高级决策。该方法提出了几个实用设计原则，并展示了在现实网页栈上的广泛应用。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么是固定的、可靠的Web框架，要么是无限但不可控的生成式世界模型。本文提出WWM的目标是寻找二者的平衡，实现在确保逻辑一致性的前提下，利用大语言模型生成灵活多变的世界内容。

Method: WWM通过在网页代码中实现世界状态和物理，维持了逻辑一致性。同时，基于结构化潜在状态，大语言模型生成上下文、叙述和高级决策。通过分离代码定义的规则和模型驱动的想象，利用确定性生成实现无限且有结构的探索。

Result: 本文构建了多种WWM系统，包括基于现实地理的无限旅行地图、虚拟星系探险者、网页规模的百科和叙述世界以及模拟游戏环境。通过应用WWM设计原则，展示了这些系统能够提供可控的开放环境。

Conclusion: WWM方法证明了网页栈可以作为一个可扩展的底层基础，支持世界模型的构建，既能够保持可控性又能实现开放性的环境。

Abstract: Language agents increasingly require persistent worlds in which they can act, remember, and learn. Existing approaches sit at two extremes: conventional web frameworks provide reliable but fixed contexts backed by databases, while fully generative world models aim for unlimited environments at the expense of controllability and practical engineering. In this work, we introduce the Web World Model (WWM), a middle ground where world state and ``physics'' are implemented in ordinary web code to ensure logical consistency, while large language models generate context, narratives, and high-level decisions on top of this structured latent state. We build a suite of WWMs on a realistic web stack, including an infinite travel atlas grounded in real geography, fictional galaxy explorers, web-scale encyclopedic and narrative worlds, and simulation- and game-like environments. Across these systems, we identify practical design principles for WWMs: separating code-defined rules from model-driven imagination, representing latent state as typed web interfaces, and utilizing deterministic generation to achieve unlimited but structured exploration. Our results suggest that web stacks themselves can serve as a scalable substrate for world models, enabling controllable yet open-ended environments. Project Page: https://github.com/Princeton-AI2-Lab/Web-World-Models.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [231] [An Energy-Efficient RFET-Based Stochastic Computing Neural Network Accelerator](https://arxiv.org/abs/2512.22131)
*Sheng Lu,Qianhou Qu,Sungyong Jung,Qilian Liang,Chenyun Pan*

Main category: cs.AR

TL;DR: 论文提出了基于RFETs的新架构的SCNN，通过器件层面的可重构性设计出高效的SNG和APC等核心模块，相比FinFET基的设计，在面积、延迟和能耗上均获得显著降低。


<details>
  <summary>Details</summary>
Motivation: 传统的SCNN方法中，SNG和APC等组件由于资源消耗高，限制了性能。为了解决这个问题，研究人员设计了一种基于RFET的新SCNN架构。

Method: 该方法基于RFETs的器件层面可重构性，重新设计了SNG和APC等关键模块，从而减少了资源消耗。此外，还设计了一个专用的SCNN加速器架构用于系统级仿真。

Result: 实验结果表明，基于RFET的SCNN加速器相比FinFET基的设计，在面积、延迟和能耗上都实现了显著降低。

Conclusion: 该研究提出的新SCNN架构，通过利用RFETs的特性，能够有效降低硬件复杂度和资源消耗，适用于构建高效的CNN。

Abstract: Stochastic computing (SC) offers significant reductions in hardware complexity for traditional convolutional neural networks (CNNs), but stochastic computing neural networks (SCNNs) still suffer from high resource usage due to components such as stochastic number generators (SNGs) and accumulative parallel counters (APCs), which limit performance. This paper introduces a novel SCNN architecture based on reconfigurable field-effect transistors (RFETs), whose device-level reconfigurability enables the design of highly efficient and compact SNGs, APCs, and other core modules. A dedicated SCNN accelerator architecture is also developed for system-level simulation. Using publicly available open-source standard cell libraries, experimental results show that the proposed RFET-based SCNN accelerator achieves substantial reductions in area, latency, and energy consumption compared to a FinFET-based design at the same technology node.

</details>


### [232] [TYTAN: Taylor-series based Non-Linear Activation Engine for Deep Learning Accelerators](https://arxiv.org/abs/2512.23062)
*Soham Pramanik,Vimal William,Arnab Raha,Debayan Das,Amitava Mukherjee,Janet L. Paluh*

Main category: cs.AR

TL;DR: TYTAN是一种台式级非线性激活引擎设计方案，旨在通过动态调整非线性激活函数的近似值来优化AI边端推理的加速与能效。


<details>
  <summary>Details</summary>
Motivation: 随着AI架构的发展和AI驱动系统的增加，对针对特定领域的加速架构的需求增加，以提高AI推理的加速和能效，特别是在边缘。现有的高能耗操作（如通用矩阵乘法和激活函数）需要优化。

Method: TYTAN方法是通过一种可重构的硬件设计与专有算法的结合，动态估算每个激活函数所需的近似值，以实现最小偏差基准准确度。

Result: 系统级模拟（使用Silvaco的FreePDK45工艺节点）结果表明，TYTAN能够在950 MHz以上的时钟频率下运行，并且与基准的NVIDIA Deep Learning Accelerator (NVDLA)实现相比，性能提高约2倍，功率降低约56%，面积减少约35倍。

Conclusion: TYTAN能够有效支持边端加速、节能的AI推理，并且已经在高性能AI架构中得到验证，具有显著的改进潜力。

Abstract: The rapid advancement in AI architectures and the proliferation of AI-enabled systems have intensified the need for domain-specific architectures that enhance both the acceleration and energy efficiency of AI inference, particularly at the edge. This need arises from the significant resource constraints-such as computational cost and energy consumption-associated with deploying AI algorithms, which involve intensive mathematical operations across multiple layers. High-power-consuming operations, including General Matrix Multiplications (GEMMs) and activation functions, can be optimized to address these challenges. Optimization strategies for AI at the edge include algorithmic approaches like quantization and pruning, as well as hardware methodologies such as domain-specific accelerators. This paper proposes TYTAN: TaYlor-series based non-linear acTivAtion eNgine, which explores the development of a Generalized Non-linear Approximation Engine (G-NAE). TYTAN targets the acceleration of non-linear activation functions while minimizing power consumption. The TYTAN integrates a re-configurable hardware design with a specialized algorithm that dynamically estimates the necessary approximation for each activation function, aimed at achieving minimal deviation from baseline accuracy. The proposed system is validated through performance evaluations with state-of-the-art AI architectures, including Convolutional Neural Networks (CNNs) and Transformers. Results from system-level simulations using Silvaco's FreePDK45 process node demonstrate TYTAN's capability to operate at a clock frequency >950 MHz, showcasing its effectiveness in supporting accelerated, energy-efficient AI inference at the edge, which is ~2 times performance improvement, with ~56% power reduction and ~35 times lower area compared to the baseline open-source NVIDIA Deep Learning Accelerator (NVDLA) implementation.

</details>
