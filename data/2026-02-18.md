<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 35]
- [cs.CL](#cs.CL) [Total: 30]
- [cs.AR](#cs.AR) [Total: 4]
- [cs.AI](#cs.AI) [Total: 19]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Zero-shot HOI Detection with MLLM-based Detector-agnostic Interaction Recognition](https://arxiv.org/abs/2602.15124)
*Shiyu Xuan,Dongkai Wang,Zechao Li,Jinhui Tang*

Main category: cs.CV

TL;DR: 本研究提出了一种脱钩框架，将物体检测与互动识别分离，利用多模态大型语言模型进行零样本互动识别。引入了一种确定性生成方法，将互动识别作为视觉问答任务，并通过空间感知聚合模块和一次通过确定性匹配方法提升了模型的性能和效率。实验表明，该方法在零样本性能和跨数据集泛化方面表现优越，且可以与任何物体检测器无缝集成。


<details>
  <summary>Details</summary>
Motivation: 现有的零样本人类-物体互动检测方法由于耦合了互动识别和特定检测器，并依赖粗粒度的视觉语言模型，难以应对未见过的互动。因此，提出了一个脱钩框架，利用多模态大型语言模型进行互动识别。

Method: 本研究设计了一个脱钩框架，将物体检测与互动识别分离。通过引入确定性生成方法，将互动识别任务转化为视觉问答任务，并利用空间感知聚合模块和一次通过匹配方法提升模型的性能。

Result: 在HICO-DET和V-COCO数据集上的实验结果表明，该方法在零样本性能和跨数据集泛化能力方面表现优于现有方法，并且可以与任何物体检测器无缝集成。

Conclusion: 该框架对未知互动的识别具有很好的泛化能力，展示了强大的跨数据集泛化性能，并且能够在不重新训练的情况下与其他物体检测器结合使用。

Abstract: Zero-shot Human-object interaction (HOI) detection aims to locate humans and objects in images and recognize their interactions. While advances in open-vocabulary object detection provide promising solutions for object localization, interaction recognition (IR) remains challenging due to the combinatorial diversity of interactions. Existing methods, including two-stage methods, tightly couple IR with a specific detector and rely on coarse-grained vision-language model (VLM) features, which limit generalization to unseen interactions. In this work, we propose a decoupled framework that separates object detection from IR and leverages multi-modal large language models (MLLMs) for zero-shot IR. We introduce a deterministic generation method that formulates IR as a visual question answering task and enforces deterministic outputs, enabling training-free zero-shot IR. To further enhance performance and efficiency by fine-tuning the model, we design a spatial-aware pooling module that integrates appearance and pairwise spatial cues, and a one-pass deterministic matching method that predicts all candidate interactions in a single forward pass. Extensive experiments on HICO-DET and V-COCO demonstrate that our method achieves superior zero-shot performance, strong cross-dataset generalization, and the flexibility to integrate with any object detectors without retraining. The codes are publicly available at https://github.com/SY-Xuan/DA-HOI.

</details>


### [2] [Loss Knows Best: Detecting Annotation Errors in Videos via Loss Trajectories](https://arxiv.org/abs/2602.15154)
*Praditha Alwis,Soumyadeep Chandra,Deepak Ravikumar,Kaushik Roy*

Main category: cs.CV

TL;DR: 该研究提出了一种模型无关的方法，通过分析累计样本损失（CSL）来检测标注错误。CSL可以帮助识别框架级别的学习性，不一致的框架会有持续的高损失或不规则的损失模式。这种方法无需已知错误标注，适用于多种数据集，且在EgoPER和Cholec80数据集上的实验结果表明，其能有效识别细微的不一致性。


<details>
  <summary>Details</summary>
Motivation: 许多实际视频数据集存在标注错误，如误标、排序错误等问题，这些问题在时间一致性要求高的任务中尤为有害。现有方法可能依赖特定模型或需人工检查，本文提出了一种无需已知错误标注且模型无关的方法来检测此类错误。

Method: 该方法通过训练一个视频分割模型，并在每次训练周期中保存模型权重。然后利用这些检查点评估测试视频中每个帧的损失。持续的高CSL可以帮助识别标注错误。

Result: 实验表明，该方法在处理EgoPER和Cholec80数据集时能有效检测到标注错误，包括细微的误标和时间排序错误。

Conclusion: 本文提出的方法提供了一个强大的工具，用于数据集审核和提高基于视频的机器学习模型训练的可靠性。

Abstract: High-quality video datasets are foundational for training robust models in tasks like action recognition, phase detection, and event segmentation. However, many real-world video datasets suffer from annotation errors such as *mislabeling*, where segments are assigned incorrect class labels, and *disordering*, where the temporal sequence does not follow the correct progression. These errors are particularly harmful in phase-annotated tasks, where temporal consistency is critical. We propose a novel, model-agnostic method for detecting annotation errors by analyzing the Cumulative Sample Loss (CSL)--defined as the average loss a frame incurs when passing through model checkpoints saved across training epochs. This per-frame loss trajectory acts as a dynamic fingerprint of frame-level learnability. Mislabeled or disordered frames tend to show consistently high or irregular loss patterns, as they remain difficult for the model to learn throughout training, while correctly labeled frames typically converge to low loss early. To compute CSL, we train a video segmentation model and store its weights at each epoch. These checkpoints are then used to evaluate the loss of each frame in a test video. Frames with persistently high CSL are flagged as likely candidates for annotation errors, including mislabeling or temporal misalignment. Our method does not require ground truth on annotation errors and is generalizable across datasets. Experiments on EgoPER and Cholec80 demonstrate strong detection performance, effectively identifying subtle inconsistencies such as mislabeling and frame disordering. The proposed approach provides a powerful tool for dataset auditing and improving training reliability in video-based machine learning.

</details>


### [3] [Distributional Deep Learning for Super-Resolution of 4D Flow MRI under Domain Shift](https://arxiv.org/abs/2602.15167)
*Xiaoyi Wen,Fei Jiang*

Main category: cs.CV

TL;DR: 该研究提出了一种分布式的深度学习框架，以提高4D Flow MRI的分辨率，该技术在临床环境下表现出色，提升了超分辨率性能。


<details>
  <summary>Details</summary>
Motivation: 当前的超分辨率方法主要依赖于成对的低分辨率和高分辨率数据集进行训练，但在实际临床环境中，低分辨率数据可能来自与简单下采样不同的采集机制，导致模型泛化能力较差。

Method: 研究采用了一种分布式的深度学习框架，初始阶段在高分辨率的计算流体动力学（CFD）模拟及其下采样版本上进行训练，随后在少量 harmonized 的 4D Flow MRI 和 CFD 数据成对样本集上进行微调。

Result: 理论分析证明该分布式的估计器有效，并通过实际数据应用表明该方法显著优于传统深度学习方法。

Conclusion: 该研究提出的方法在临床环境下的表现较好，有效增强了处理流动性信息的能力，并证实了分布式学习在处理数据域偏移方面的优势。

Abstract: Super-resolution is widely used in medical imaging to enhance low-quality data, reducing scan time and improving abnormality detection. Conventional super-resolution approaches typically rely on paired datasets of downsampled and original high resolution images, training models to reconstruct high resolution images from their artificially degraded counterparts. However, in real-world clinical settings, low resolution data often arise from acquisition mechanisms that differ significantly from simple downsampling. As a result, these inputs may lie outside the domain of the training data, leading to poor model generalization due to domain shift. To address this limitation, we propose a distributional deep learning framework that improves model robustness and domain generalization. We develop this approch for enhancing the resolution of 4D Flow MRI (4DF). This is a novel imaging modality that captures hemodynamic flow velocity and clinically relevant metrics such as vessel wall stress. These metrics are critical for assessing aneurysm rupture risk. Our model is initially trained on high resolution computational fluid dynamics (CFD) simulations and their downsampled counterparts. It is then fine-tuned on a small, harmonized dataset of paired 4D Flow MRI and CFD samples. We derive the theoretical properties of our distributional estimators and demonstrate that our framework significantly outperforms traditional deep learning approaches through real data applications. This highlights the effectiveness of distributional learning in addressing domain shift and improving super-resolution performance in clinically realistic scenarios.

</details>


### [4] [Time-Archival Camera Virtualization for Sports and Visual Performances](https://arxiv.org/abs/2602.15181)
*Yunxiao Zhang,William Stone,Suryansh Kumar*

Main category: cs.CV

TL;DR: 该研究重新考虑了用于摄像机虚拟化和高效时间存档的神经体素渲染方法，特别适用于体育直播应用。该方法通过多同步相机视图在特定时间进行刚性变换来建模动态场景，增强了视觉渲染质量，并支持时间存档，允许多人回顾直播事件的过去时间实例，进行新型视角合成和回放分析。


<details>
  <summary>Details</summary>
Motivation: 现有基于3D高斯点云的动态场景视图合成方法难以处理复杂动态下的大规模非刚性运动和多主体之间的独立运动，影响了实时视图合成。本论文提出的基于神经体素渲染的方法解决了这一问题，能够提供更好的视觉渲染质量并支持时间存档能力。

Method: 提出了一种基于多同步相机视角的刚性变换模型，使用神经体素渲染进行动态场景建模，从而实现高质量的视角合成和存档管理。

Result: 实验结果表明，该方法能够有效处理动态场景中的复杂运动，提供卓越的视觉渲染质量，并支持高效的时间存档，为体育直播等应用提供了创新解决方案。

Conclusion: 该研究为摄像机虚拟化提供了一种新的神经体素渲染方法，显著提高了视图合成质量和时间存档能力，扩展了其在视觉娱乐、体育直播等领域应用的边界。

Abstract: Camera virtualization -- an emerging solution to novel view synthesis -- holds transformative potential for visual entertainment, live performances, and sports broadcasting by enabling the generation of photorealistic images from novel viewpoints using images from a limited set of calibrated multiple static physical cameras. Despite recent advances, achieving spatially and temporally coherent and photorealistic rendering of dynamic scenes with efficient time-archival capabilities, particularly in fast-paced sports and stage performances, remains challenging for existing approaches. Recent methods based on 3D Gaussian Splatting (3DGS) for dynamic scenes could offer real-time view-synthesis results. Yet, they are hindered by their dependence on accurate 3D point clouds from the structure-from-motion method and their inability to handle large, non-rigid, rapid motions of different subjects (e.g., flips, jumps, articulations, sudden player-to-player transitions). Moreover, independent motions of multiple subjects can break the Gaussian-tracking assumptions commonly used in 4DGS, ST-GS, and other dynamic splatting variants. This paper advocates reconsidering a neural volume rendering formulation for camera virtualization and efficient time-archival capabilities, making it useful for sports broadcasting and related applications. By modeling a dynamic scene as rigid transformations across multiple synchronized camera views at a given time, our method performs neural representation learning, providing enhanced visual rendering quality at test time. A key contribution of our approach is its support for time-archival, i.e., users can revisit any past temporal instance of a dynamic scene and can perform novel view synthesis, enabling retrospective rendering for replay, analysis, and archival of live events, a functionality absent in existing neural rendering approaches and novel view synthesis...

</details>


### [5] [How to Train Your Long-Context Visual Document Model](https://arxiv.org/abs/2602.15257)
*Austin Veselka*

Main category: cs.CV

TL;DR: 本文提出了首个针对长文档视觉问答的大规模研究，通过系统研究24亿和32亿参数模型的持续预训练、有监督微调以及偏好优化，实现了MMLongBenchDoc基准上的最新性能。研究还发现，与更长的上下文相比，匹配评价长度的上下文更优；使用页面索引作为上下文增强可显著提升长文档性能；并展示了视觉长上下文训练对长文本性能的逆向转移。


<details>
  <summary>Details</summary>
Motivation: 当前已有一些强重量级的视觉语言模型，但它们的训练配方和数据管道不可重现。为了填补这一空白，本文进行了一项大规模的系统研究，旨在提升视觉语言模型在长文档视觉问答任务上的性能。

Method: 本文通过系统性地研究不同参数量级的视觉语言模型，优化了持续预训练、监督微调和偏好优化的方法，并结合大量的长上下文评估和消融实验，进一步提高了模型性能。

Result: 研究成果包括通过匹配评价上下文长度的训练，预测上下文长度对性能的影响；使用页面索引作为上下文增强改进长文档性能；并通过持续预训练和监督微调实现自我改进；并且展示了视觉长上下文训练对长文本性能的有效转移。

Conclusion: 本文通过进行多项实验，得出了多项有关长文本处理和视觉语言模型适用性的新发现，并在MMLongBenchDoc基准上实现了最新的性能记录。

Abstract: We present the first comprehensive, large-scale study of training long-context vision language models up to 344K context, targeting long-document visual question answering with measured transfer to long-context text. While several such strong are open-weight, namely Qwen3 VL and GLM 4.5/6V, their training recipes and data pipelines are not reproducible. We systematically study continued pretraining, supervised finetuning, and preference optimization for 24B and 32B parameter models, backed by extensive LC evaluations and ablations to bridge this gap, and achieve state-of-the-art performance on MMLongBenchDoc for both parameter scales. In addition to this, our key findings include: (i) training on context lengths that match evaluation context lengths outperforms training on longer contexts, (ii) training and evaluating with page indices provides a simple, high-impact boost to long-document performance, (iii) our synthetic data pipelines enable self-improvement via continued pretraining and supervised finetuning, and (iv) we extend the known text-to-visual long context transfer to the reverse, showing that visual long context training transfers to long-context text performance. We also release MMLBD-C, a manually corrected version of MMLongBenchDoc to reduce erroneous and low quality examples in the benchmark.

</details>


### [6] [Accelerating Large-Scale Dataset Distillation via Exploration-Exploitation Optimization](https://arxiv.org/abs/2602.15277)
*Muhammad J. Alahmadi,Peng Gao,Feiyi Wang,Dongkuan,Xu*

Main category: cs.CV

TL;DR: E^2D通过提出一种高效管道，结合全图像初始化和两阶段优化策略，在保持性能的同时显著提高了大规模数据集蒸馏的效率。


<details>
  <summary>Details</summary>
Motivation: 传统数据蒸馏方法在大规模应用中存在计算效率与模型性能之间的权衡问题，E^2D旨在解决这一问题。

Method: E^2D采用了一种高效的两阶段优化策略，包括探索阶段和利用阶段。探索阶段进行均匀更新并识别高损失区域，利用阶段则专注于对高损失区域进行更新以加快收敛。

Result: E^2D在ImageNet-1K基准上超越了最新技术，速度快18倍；在ImageNet-21K上，模型性能得到提升且速度仍快4.3倍。

Conclusion: E^2D针对大规模数据集蒸馏，通过减少冗余计算与定位关键更新区域，成功在保持性能的同时提高了效率。

Abstract: Dataset distillation compresses the original data into compact synthetic datasets, reducing training time and storage while retaining model performance, enabling deployment under limited resources. Although recent decoupling-based distillation methods enable dataset distillation at large-scale, they continue to face an efficiency gap: optimization-based decoupling methods achieve higher accuracy but demand intensive computation, whereas optimization-free decoupling methods are efficient but sacrifice accuracy. To overcome this trade-off, we propose Exploration-Exploitation Distillation (E^2D), a simple, practical method that minimizes redundant computation through an efficient pipeline that begins with full-image initialization to preserve semantic integrity and feature diversity. It then uses a two-phase optimization strategy: an exploration phase that performs uniform updates and identifies high-loss regions, and an exploitation phase that focuses updates on these regions to accelerate convergence. We evaluate E^2D on large-scale benchmarks, surpassing the state-of-the-art on ImageNet-1K while being 18x faster, and on ImageNet-21K, our method substantially improves accuracy while remaining 4.3x faster. These results demonstrate that targeted, redundancy-reducing updates, rather than brute-force optimization, bridge the gap between accuracy and efficiency in large-scale dataset distillation. Code is available at https://github.com/ncsu-dk-lab.

</details>


### [7] [Consistency-Preserving Diverse Video Generation](https://arxiv.org/abs/2602.15287)
*Xinshuang Liu,Runfa Blark Li,Truong Nguyen*

Main category: cs.CV

TL;DR: 提出了一种改进文本生成视频的框架，该框架提高了批量多样性和时间一致性，同时避免了昂贵的解码和反向传播过程。


<details>
  <summary>Details</summary>
Motivation: 在视频生成的样本量较少时，提高批处理的多样性是关键。然而，现有方法往往牺牲了视频内部的时间一致性，且需要通过视频解码器进行反向传播，导致效率低下。

Method: 开发了一种联合采样框架，结合了多样性驱动的更新和技术一致性目标的优化，使用轻量级的潜在空间模型进行优化，避免了直接对视频解码器进行反向传播。

Result: 该方法在保持时间一致性和颜色自然度的同时，实现了与强联合采样基线相当的多样性。

Conclusion: 通过改进的框架，实验显示能够同时提高生成视频的多样性和时间一致性，为未来的研究提供了有价值的参考。

Abstract: Text-to-video generation is expensive, so only a few samples are typically produced per prompt. In this low-sample regime, maximizing the value of each batch requires high cross-video diversity. Recent methods improve diversity for image generation, but for videos they often degrade within-video temporal consistency and require costly backpropagation through a video decoder. We propose a joint-sampling framework for flow-matching video generators that improves batch diversity while preserving temporal consistency. Our approach applies diversity-driven updates and then removes only the components that would decrease a temporal-consistency objective. To avoid image-space gradients, we compute both objectives with lightweight latent-space models, avoiding video decoding and decoder backpropagation. Experiments on a state-of-the-art text-to-video flow-matching model show diversity comparable to strong joint-sampling baselines while substantially improving temporal consistency and color naturalness. Code will be released.

</details>


### [8] [Training-Free Zero-Shot Anomaly Detection in 3D Brain MRI with 2D Foundation Models](https://arxiv.org/abs/2602.15315)
*Tai Le-Gia,Jaehyun Ahn*

Main category: cs.CV

TL;DR: 本文提出了一种无需训练的3D脑MRI中的零样本异常检测框架，通过使用2D基础模型处理多轴切片并聚合构建局部体积标记，恢复了立方体空间上下文，并直接与基于距离的批次级异常检测管道集成。


<details>
  <summary>Details</summary>
Motivation: 现有的零样本异常检测方法在处理3D医学图像时遇到挑战，主要依赖于切片特征和视觉-语言模型，未能捕捉到体数据结构。该论文旨在提出一个无需训练的框架，以扩展2D编码器到完整的3D MRI体积，来解决这一问题。

Method: 该框架通过使用2D基础模型处理多轴切片并聚合构建局部体积标记，通过集成直接与基于距离的批次级异常检测管道，恢复了立方体空间上下文，实现了3D零样本异常检测。

Result: 实验结果表明，无需训练的批量零样本异常检测可以从2D编码器扩展到完整的3D MRI体积，提供了一种简单且鲁棒的体积异常检测方法。

Conclusion: 该论文提出了一种无需训练的3D脑MRI零样本异常检测框架，展示了其在扩展2D到3D零样本异常检测任务上的有效性和实用性。

Abstract: Zero-shot anomaly detection (ZSAD) has gained increasing attention in medical imaging as a way to identify abnormalities without task-specific supervision, but most advances remain limited to 2D datasets. Extending ZSAD to 3D medical images has proven challenging, with existing methods relying on slice-wise features and vision-language models, which fail to capture volumetric structure. In this paper, we introduce a fully training-free framework for ZSAD in 3D brain MRI that constructs localized volumetric tokens by aggregating multi-axis slices processed by 2D foundation models. These 3D patch tokens restore cubic spatial context and integrate directly with distance-based, batch-level anomaly detection pipelines. The framework provides compact 3D representations that are practical to compute on standard GPUs and require no fine-tuning, prompts, or supervision. Our results show that training-free, batch-based ZSAD can be effectively extended from 2D encoders to full 3D MRI volumes, offering a simple and robust approach for volumetric anomaly detection.

</details>


### [9] [Sparrow: Text-Anchored Window Attention with Visual-Semantic Glimpsing for Speculative Decoding in Video LLMs](https://arxiv.org/abs/2602.15318)
*Libo Zhang,Zhaoning Zhang,Wangyang Hong,Peng Qiao,Dongsheng Li*

Main category: cs.CV

TL;DR: Sparrow框架通过利用视觉感知的文本锚定窗口注意力机制和中间层视觉状态桥接，实现了即使在大量视觉 Tokens 存在时，仍能获得2.82倍的加速，解决了长序列中的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 目前的 speculative decoding 在 Vid-LLMs 上存在注意力稀释和负视图增益的问题，Sparrow 框架旨在解决这些挑战。

Method: Sparrow框架通过隐藏状态重用实现视觉感知的文本锚定窗口注意力机制，并利用中间层视觉状态桥接训练模型，同时引入多令牌预测策略以应对训练与推理之间分布转移的问题。

Result: 实验表明，即使在有25k视觉 Tokens 的情况下，Sparrow 仍能获得2.82倍的加速，有效解决了长期序列中的性能下降问题，适用于实时长视频任务。

Conclusion: Sparrow 框架提供了一种有效的解决 Vid-LLMs 长序列推理问题的方法，能够实现在大规模视觉数据下的高效推理。

Abstract: Although speculative decoding is widely used to accelerate Vision-Language Models (VLMs) inference, it faces severe performance collapse when applied to Video Large Language Models (Vid-LLMs). The draft model typically falls into the trap of attention dilution and negative visual gain due to key-value cache explosion and context window mismatches. We observe a visual semantic internalization phenomenon in Vid-LLMs, indicating that critical visual semantics are implicitly encoded into text hidden states during deep-layer interactions, which renders raw visual inputs structurally redundant during deep inference. To address this, we propose the Sparrow framework, which first utilizes visually-aware text-anchored window attention via hidden state reuse to fully offload visual computation to the target model, and leverages intermediate-layer visual state bridging to train the draft model with semantic-rich intermediate states, thereby filtering out low-level visual noise. Additionally, a multi-token prediction strategy is introduced to bridge the training-inference distribution shift. Experiments show that Sparrow achieves an average speedup of 2.82x even with 25k visual tokens, effectively resolving the performance degradation in long sequences and offering a practical solution for real-time long video tasks.

</details>


### [10] [CREMD: Crowd-Sourced Emotional Multimodal Dogs Dataset](https://arxiv.org/abs/2602.15349)
*Jinho Baek,Houwei Cao,Kate Blackwell*

Main category: cs.CV

TL;DR: 该研究通过CREMD数据集探讨了不同呈现模式和注释者特征对狗情绪感知和标注的影响，结果显示视觉上下文显著提高了注释一致性和情绪识别，而非狗主和男性注释者达成的共识更高。


<details>
  <summary>Details</summary>
Motivation: 为了解决狗情绪识别中的主观性和缺少标准化方法的问题，研究者们开发了一个多元化的狗情绪数据集，旨在发现影响可靠狗情绪识别的因素。

Method: 研究构建了一个包含3种不同模式的视频片段的数据集，并邀请了不同身份背景的参与者进行注释，分析了不同因素对情感标注的影响。

Result: 研究结果显示，添加视觉上下文显著提高了注释的一致性，而非狗主和男性注释者在达成共识方面表现更好，同时音频的加入增强了参与者识别特定情绪（如愤怒和恐惧）的信心。

Conclusion: 研究指出，虽然视觉上下文有助于改善狗情绪标注的一致性，但对于音频线索的效果仍需进一步研究；非狗主人和男性注释者表现更好的结论挑战了预期；最后，音频可以提高情绪识别的准确性，特别是愤怒和恐惧情绪的识别。

Abstract: Dog emotion recognition plays a crucial role in enhancing human-animal interactions, veterinary care, and the development of automated systems for monitoring canine well-being. However, accurately interpreting dog emotions is challenging due to the subjective nature of emotional assessments and the absence of standardized ground truth methods. We present the CREMD (Crowd-sourced Emotional Multimodal Dogs Dataset), a comprehensive dataset exploring how different presentation modes (e.g., context, audio, video) and annotator characteristics (e.g., dog ownership, gender, professional experience) influence the perception and labeling of dog emotions. The dataset consists of 923 video clips presented in three distinct modes: without context or audio, with context but no audio, and with both context and audio. We analyze annotations from diverse participants, including dog owners, professionals, and individuals with varying demographic backgrounds and experience levels, to identify factors that influence reliable dog emotion recognition. Our findings reveal several key insights: (1) while adding visual context significantly improved annotation agreement, our findings regarding audio cues are inconclusive due to design limitations (specifically, the absence of a no-context-with-audio condition and limited clean audio availability); (2) contrary to expectations, non-owners and male annotators showed higher agreement levels than dog owners and female annotators, respectively, while professionals showed higher agreement levels, aligned with our initial hypothesis; and (3) the presence of audio substantially increased annotators' confidence in identifying specific emotions, particularly anger and fear.

</details>


### [11] [DAV-GSWT: Diffusion-Active-View Sampling for Data-Efficient Gaussian Splatting Wang Tiles](https://arxiv.org/abs/2602.15355)
*Rong Fu,Jiekai Wu,Haiyun Wei,Yee Tan Jia,Wenxin Zhang,Yang Li,Xiaowen Ma,Wangyu Wu,Simon Fong*

Main category: cs.CV

TL;DR: DAV-GSWT 是一种高效框架，利用扩散先验和主动视图采样从少量输入观测生成高保真度的 3D 质心光栅化 Wang Tiles，减少了数据需求同时保持视觉一致性和交互性能。


<details>
  <summary>Details</summary>
Motivation: 当前的 3D 渲染技术依赖于密集采样示例重建，限制了生成大规模景观的能力。DAV-GSWT 提供了一种方法来降低数据需求，并通过自动识别最有信息性的视点来确保无缝的瓦片过渡。

Method: DAV-GSWT 结合了层次不确定性量化机制和生成式扩散模型来主动选择最有信息性的视点，并通过hallucination填补缺失的结构细节。

Result: 实验结果表明，DAV-GSWT 相比于现有方法，能显著减少所需的输入数据量，同时保持大规模虚拟环境所需的视觉完整性和交互性能。

Conclusion: DAV-GSWT 通过减少数据需求和自动识别关键视点，为大规模 3D 渲染应用提供了显著改进的方法。

Abstract: The emergence of 3D Gaussian Splatting has fundamentally redefined the capabilities of photorealistic neural rendering by enabling high-throughput synthesis of complex environments. While procedural methods like Wang Tiles have recently been integrated to facilitate the generation of expansive landscapes, these systems typically remain constrained by a reliance on densely sampled exemplar reconstructions. We present DAV-GSWT, a data-efficient framework that leverages diffusion priors and active view sampling to synthesize high-fidelity Gaussian Splatting Wang Tiles from minimal input observations. By integrating a hierarchical uncertainty quantification mechanism with generative diffusion models, our approach autonomously identifies the most informative viewpoints while hallucinating missing structural details to ensure seamless tile transitions. Experimental results indicate that our system significantly reduces the required data volume while maintaining the visual integrity and interactive performance necessary for large-scale virtual environments.

</details>


### [12] [GMAIL: Generative Modality Alignment for generated Image Learning](https://arxiv.org/abs/2602.15368)
*Shentong Mo,Sukmin Yun*

Main category: cs.CV

TL;DR: 该论文提出了一种名为GMAIL的新框架，旨在以区分的方式使用生成的图像。该框架通过跨模态对齐损失对模型进行微调，之后将对齐过的模型用于增强各种视觉-语言模型中生成图像的学习效果。


<details>
  <summary>Details</summary>
Motivation: 为了解决生成图像作为训练数据可能引发模态不匹配导致的模式坍塌等问题，该论文提出了一种新的框架来促进生成图像的有效利用。

Method: 该方法首先使用跨模态对齐损失对模型进行微调，确保生成图像和真实图像在模型内部的表示更加一致。然后，该模型被用于进一步训练各种视觉-语言模型，从而利用生成模型的进步来增强生成图像的学习效率。

Result: 该框架有效提升了图像描述、零样本图像检索、零样本图像分类及长描述检索等多种视觉-语言任务的性能。特别是在大规模多模态模型LLaVA的图像描述性能上表现出显著提升。

Conclusion: 该工作通过构建一种新框架，展示了生成图像在多种视觉-语言任务中的重要性和潜力。这意味着可以更好地利用生成图像来增强机器学习模型的效果。

Abstract: Generative models have made it possible to synthesize highly realistic images, potentially providing an abundant data source for training machine learning models. Despite the advantages of these synthesizable data sources, the indiscriminate use of generated images as real images for training can even cause mode collapse due to modality discrepancies between real and synthetic domains. In this paper, we propose a novel framework for discriminative use of generated images, coined GMAIL, that explicitly treats generated images as a separate modality from real images. Instead of indiscriminately replacing real images with generated ones in the pixel space, our approach bridges the two distinct modalities in the same latent space through a multi-modal learning approach. To be specific, we first fine-tune a model exclusively on generated images using a cross-modality alignment loss and then employ this aligned model to further train various vision-language models with generated images. By aligning the two modalities, our approach effectively leverages the benefits of recent advances in generative models, thereby boosting the effectiveness of generated image learning across a range of vision-language tasks. Our framework can be easily incorporated with various vision-language models, and we demonstrate its efficacy throughout extensive experiments. For example, our framework significantly improves performance on image captioning, zero-shot image retrieval, zero-shot image classification, and long caption retrieval tasks. It also shows positive generated data scaling trends and notable enhancements in the captioning performance of the large multimodal model, LLaVA.

</details>


### [13] [Bridging Day and Night: Target-Class Hallucination Suppression in Unpaired Image Translation](https://arxiv.org/abs/2602.15383)
*Shuwei Li,Lei Tan,Robby T. Tan*

Main category: cs.CV

TL;DR: 本文提出了一种新的框架，用于日间到夜间的图像翻译，能够检测并抑制目标类特征的幻视，从而改善下游任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在未经配对的图像翻译中往往会引入幻视，导致下游任务性能下降。因此，提出了这个新的框架来解决这一问题。

Method: 该框架采用了一种基于薛定谔桥的翻译模型，并引入了类别特定的原型，通过在特征空间中显式地将检测到的幻视特征推离类别原型，从而保持翻译轨迹中的对象语义。

Result: 实验表明，该方法在定性和定量上都优于现有方法。在BDD100K数据集上，日间到夜间领域的适应性提高了15.5％的mAP，并且对于容易出现幻视的交通信号灯等类别，性能提高了31.7％。

Conclusion: 这种方法能够有效减少目标类特征的幻视，从而提高日间到夜间未经配对的图像翻译任务的下游性能。

Abstract: Day-to-night unpaired image translation is important to downstream tasks but remains challenging due to large appearance shifts and the lack of direct pixel-level supervision. Existing methods often introduce semantic hallucinations, where objects from target classes such as traffic signs and vehicles, as well as man-made light effects, are incorrectly synthesized. These hallucinations significantly degrade downstream performance. We propose a novel framework that detects and suppresses hallucinations of target-class features during unpaired translation. To detect hallucination, we design a dual-head discriminator that additionally performs semantic segmentation to identify hallucinated content in background regions. To suppress these hallucinations, we introduce class-specific prototypes, constructed by aggregating features of annotated target-domain objects, which act as semantic anchors for each class. Built upon a Schrodinger Bridge-based translation model, our framework performs iterative refinement, where detected hallucination features are explicitly pushed away from class prototypes in feature space, thus preserving object semantics across the translation trajectory.Experiments show that our method outperforms existing approaches both qualitatively and quantitatively. On the BDD100K dataset, it improves mAP by 15.5% for day-to-night domain adaptation, with a notable 31.7% gain for classes such as traffic lights that are prone to hallucinations.

</details>


### [14] [Efficient Generative Modeling beyond Memoryless Diffusion via Adjoint Schrödinger Bridge Matching](https://arxiv.org/abs/2602.15396)
*Jeongwoo Shin,Jinhwan Sul,Joonseok Lee,Jaewong Choi,Jaemoo Choi*

Main category: cs.CV

TL;DR: Adjoint Schrödinger Bridge Matching (ASBM) 提出了一种新的生成模型框架，通过两个阶段优化轨迹，第一阶段将数据转移到能量定义的先验，第二阶段学习最优耦合的反向生成动态，从而在高维数据上提供更稳定和高效的采样路径。


<details>
  <summary>Details</summary>
Motivation: 传统的扩散模型因为缺乏信息的记忆性，导致生成轨迹高度弯曲且噪声较大。ASBM旨在通过解决能量转移和最优耦合问题，提供更高效、更直接的生成路径。

Method: ASBM 通过两个阶段优化，首先通过数据到能量的采样视角解决耦合构造问题，其次直接在优化得到的最优耦合上学习反向生成动态。

Result: 与先前方法相比，ASBM 在高维数据生成中展现出更好的稳定性和效率。图像生成实验进一步表明，ASBM 能在更少的采样步骤中提高图像的清晰度。

Conclusion: ASBM 提出了一个优化轨迹的新框架，通过直接匹配最优耦合，显著改善了高维数据的生成效果。

Abstract: Diffusion models often yield highly curved trajectories and noisy score targets due to an uninformative, memoryless forward process that induces independent data-noise coupling. We propose Adjoint Schrödinger Bridge Matching (ASBM), a generative modeling framework that recovers optimal trajectories in high dimensions via two stages. First, we view the Schrödinger Bridge (SB) forward dynamic as a coupling construction problem and learn it through a data-to-energy sampling perspective that transports data to an energy-defined prior. Then, we learn the backward generative dynamic with a simple matching loss supervised by the induced optimal coupling. By operating in a non-memoryless regime, ASBM produces significantly straighter and more efficient sampling paths. Compared to prior works, ASBM scales to high-dimensional data with notably improved stability and efficiency. Extensive experiments on image generation show that ASBM improves fidelity with fewer sampling steps. We further showcase the effectiveness of our optimal trajectory via distillation to a one-step generator.

</details>


### [15] [Emergent Morphing Attack Detection in Open Multi-modal Large Language Models](https://arxiv.org/abs/2602.15461)
*Marija Ivanovska,Vitomir Štruc*

Main category: cs.CV

TL;DR: 本研究首次对开源多模态大语言模型在单张图像伪造检测中进行了零样本评估，发现未经微调或领域适应的多模态预训练模型在多种面部变形技术下显示出一定的鉴别能力，特别是LLaVA1.6-Mistral-7B模型实现了最先进的性能，远超特定任务的检测基线。


<details>
  <summary>Details</summary>
Motivation: 由于大多数伪装检测系统需要特定任务的训练且泛化能力较差，且开源多模态大语言模型在视觉-语言推理方面表现出色但其在生物特征取证领域的潜力尚未充分开发，因此我们研究了开源多模态大语言模型在单张图像伪装检测中的性能。

Method: 我们使用公开权重和标准化的可重现协议，对多种面部变形技术进行了未训练和领域适应的多模态预训练模型的评估。

Result: 许多未调整的多模态预训练模型在未经过微调或领域适应的情况下，对多种面部变形方法显示出了一定的鉴别能力。特别是在面部变形艺术明显的方面，LLaVA1.6-Mistral-7B表现出色，比高度具竞争力的任务特定伪装检测基线在等错误率（EER）方面提高了至少23%。

Conclusion: 多模态预训练可以隐式编码与面部变形特征相关的细微不一致，从而使模型在未调整的情况下表现出零样本的取证敏感性。我们的发现表明，开源多模态大语言模型是生物特征安全性和法医图像分析可重现、可解释、竞争力强的基础，还为开发先进的伪装检测系统提供了新的机会。我们也将在论文发表后发布所有代码和评估协议，以支持未来的研究。

Abstract: Face morphing attacks threaten biometric verification, yet most morphing attack detection (MAD) systems require task-specific training and generalize poorly to unseen attack types. Meanwhile, open-source multimodal large language models (MLLMs) have demonstrated strong visual-linguistic reasoning, but their potential in biometric forensics remains underexplored. In this paper, we present the first systematic zero-shot evaluation of open-source MLLMs for single-image MAD, using publicly available weights and a standardized, reproducible protocol. Across diverse morphing techniques, many MLLMs show non-trivial discriminative ability without any fine-tuning or domain adaptation, and LLaVA1.6-Mistral-7B achieves state-of-the-art performance, surpassing highly competitive task-specific MAD baselines by at least 23% in terms of equal error rate (EER). The results indicate that multimodal pretraining can implicitly encode fine-grained facial inconsistencies indicative of morphing artifacts, enabling zero-shot forensic sensitivity. Our findings position open-source MLLMs as reproducible, interpretable, and competitive foundations for biometric security and forensic image analysis. This emergent capability also highlights new opportunities to develop state-of-the-art MAD systems through targeted fine-tuning or lightweight adaptation, further improving accuracy and efficiency while preserving interpretability. To support future research, all code and evaluation protocols will be released upon publication.

</details>


### [16] [RPT-SR: Regional Prior attention Transformer for infrared image Super-Resolution](https://arxiv.org/abs/2602.15490)
*Youngwan Jin,Incheol Park,Yagiz Nalcakan,Hyeongjin Ju,Sanghyeop Yeo,Shiho Kim*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的区域先验注意变换器（RPT-SR）架构，用于红外图像超分辨率。RPT-SR通过融合可学习的区域先验令牌和局部令牌来动态调整局部重建过程，利用场景布局信息改进了场景的全局结构。


<details>
  <summary>Details</summary>
Motivation: 通用的超分辨率模型，尤其是视觉变换器，在固定或几乎静止的空间视点的红外成像场景中表现出了根本性的低效性，因为它们无法充分利用这些场景中固有的强烈的、持久的空间先验。RPT-SR的提出旨在解决这一问题。

Method: RPT-SR引入了一种双令牌框架，结合了可学习的区域先验令牌（反映场景的整体结构）和局部令牌（捕捉每帧的特定内容），并通过自注意力机制利用这些令牌进行动态调节。

Result: 在多个涵盖长波（LWIR）和短波（SWIR）频谱的不同数据集上的广泛应用实验验证了RPT-SR的有效性，实现了新的最佳性能。

Conclusion: RPT-SR通过将场景布局信息直接融入注意力机制中，改进了红外图像超分辨率模型的效能，为不同红外波段的应用提供了广泛适用性和灵活性。

Abstract: General-purpose super-resolution models, particularly Vision Transformers, have achieved remarkable success but exhibit fundamental inefficiencies in common infrared imaging scenarios like surveillance and autonomous driving, which operate from fixed or nearly-static viewpoints. These models fail to exploit the strong, persistent spatial priors inherent in such scenes, leading to redundant learning and suboptimal performance. To address this, we propose the Regional Prior attention Transformer for infrared image Super-Resolution (RPT-SR), a novel architecture that explicitly encodes scene layout information into the attention mechanism. Our core contribution is a dual-token framework that fuses (1) learnable, regional prior tokens, which act as a persistent memory for the scene's global structure, with (2) local tokens that capture the frame-specific content of the current input. By utilizing these tokens into an attention, our model allows the priors to dynamically modulate the local reconstruction process. Extensive experiments validate our approach. While most prior works focus on a single infrared band, we demonstrate the broad applicability and versatility of RPT-SR by establishing new state-of-the-art performance across diverse datasets covering both Long-Wave (LWIR) and Short-Wave (SWIR) spectra

</details>


### [17] [LEADER: Lightweight End-to-End Attention-Gated Dual Autoencoder for Robust Minutiae Extraction](https://arxiv.org/abs/2602.15493)
*Raffaele Cappelli,Matteo Ferrara*

Main category: cs.CV

TL;DR: LEADER 是一个轻量级的端到端注意力门控双自动编码器模型，用于从原始指纹图像中提取指纹细节，包括位置、方向和类型。该模型不需额外的预处理和后处理，参数量仅为 0.9M，展示了在平指纹和残余指纹上的卓越性能。


<details>
  <summary>Details</summary>
Motivation: 目前指纹识别中的细节提取步骤大多需要单独的预处理和后处理，本文提出 LEADER 模型旨在提供一种完全端到端的方案，无须额外步骤，提高效率和准确性。

Method: LEADER 模型采用了一个新颖的 ‘城堡-护城河-壁垒’ 标注编码方法，结合了非极大值抑制和角度解码，其结构由两个自动编码器构成，并通过注意力门控机制相连。

Result: 实验结果显示，LEADER 在 NIST SD27 数据集上达到了 34% 的更高 F1 值，样本级别的分析显示在平指纹和残余指纹上均表现出优异的效果，且模型的内部表示能与指纹领域的已知特征对齐。

Conclusion: LEADER 是一种高度优化的端到端细节提取方法，具有良好的性能和效率，且已开源，有助于进一步的科学探索和实际应用。

Abstract: Minutiae extraction, a fundamental stage in fingerprint recognition, is increasingly shifting toward deep learning. However, truly end-to-end methods that eliminate separate preprocessing and postprocessing steps remain scarce. This paper introduces LEADER (Lightweight End-to-end Attention-gated Dual autoencodER), a neural network that maps raw fingerprint images to minutiae descriptors, including location, direction, and type. The proposed architecture integrates non-maximum suppression and angular decoding to enable complete end-to-end inference using only 0.9M parameters. It employs a novel "Castle-Moat-Rampart" ground-truth encoding and a dual-autoencoder structure, interconnected through an attention-gating mechanism. Experimental evaluations demonstrate state-of-the-art accuracy on plain fingerprints and robust cross-domain generalization to latent impressions. Specifically, LEADER attains a 34% higher F1-score on the NIST SD27 dataset compared to specialized latent minutiae extractors. Sample-level analysis on this challenging benchmark reveals an average rank of 2.07 among all compared methods, with LEADER securing the first-place position in 47% of the samples-more than doubling the frequency of the second-best extractor. The internal representations learned by the model align with established fingerprint domain features, such as segmentation masks, orientation fields, frequency maps, and skeletons. Inference requires 15ms on GPU and 322ms on CPU, outperforming leading commercial software in computational efficiency. The source code and pre-trained weights are publicly released to facilitate reproducibility.

</details>


### [18] [Semantic-Guided 3D Gaussian Splatting for Transient Object Removal](https://arxiv.org/abs/2602.15516)
*Aditi Prabakaran,Priyesh Shukla*

Main category: cs.CV

TL;DR: 该研究提出了一种基于语义过滤的方案，用于3D Gaussian Splatting重建中的回避瞬时物体。该方法通过累计CLIP相似度分值和阈值校准去除瞬时物体，并保持了内存效率和实时渲染性能。


<details>
  <summary>Details</summary>
Motivation: 当前的解决方案或方法在处理瞬时对象造成的鬼影伪影时，要么需要大量的内存进行场景分解，要么依赖于容易受到透视歧义影响的基于运动的启发式方法。因此，研究提出了一个语义过滤框架，利用视觉语言模型进行类别意识的瞬时物体去除。

Method: 该方法利用了CLIP（对比语言-图像预训练）模型，通过在每个高斯体上累积与干扰文本提示之间的相似度得分来进行瞬时物体的去除。当得分超过预设阈值时，对高斯体的不透明度进行了正则化，并定期进行了修剪。这种基于语义分类的方法在识别对象类别时独立于运动模式，避免了运动基方法的问题。

Result: 实验结果表明，该方法在RobustNeRF基准测试上的重建质量优于基本的3DGS方法，并且在整个过程中同时保持了较低的内存占用和实时渲染性能。

Conclusion: 研究表明，基于语义的指导策略在具有可预测干扰类别的情况下，是一个实用的方法，用于3D Gaussian Splatting中瞬时物体的去除，提升了重建质量，同时对内存使用和实时渲染的影响较小。

Abstract: Transient objects in casual multi-view captures cause ghosting artifacts in 3D Gaussian Splatting (3DGS) reconstruction. Existing solutions relied on scene decomposition at significant memory cost or on motion-based heuristics that were vulnerable to parallax ambiguity. A semantic filtering framework was proposed for category-aware transient removal using vision-language models. CLIP similarity scores between rendered views and distractor text prompts were accumulated per-Gaussian across training iterations. Gaussians exceeding a calibrated threshold underwent opacity regularization and periodic pruning. Unlike motion-based approaches, semantic classification resolved parallax ambiguity by identifying object categories independently of motion patterns. Experiments on the RobustNeRF benchmark demonstrated consistent improvement in reconstruction quality over vanilla 3DGS across four sequences, while maintaining minimal memory overhead and real-time rendering performance. Threshold calibration and comparisons with baselines validated semantic guidance as a practical strategy for transient removal in scenarios with predictable distractor categories.

</details>


### [19] [Advanced Acceptance Score: A Holistic Measure for Biometric Quantification](https://arxiv.org/abs/2602.15535)
*Aman Verma,Seshan Srirangarajan,Sumantra Dutta Roy*

Main category: cs.CV

TL;DR: 该研究提出了一个综合的评估度量标准，可以更全面地评估手部手势识别系统的健身特征评分质量，实验结果显示该度量标准比现有措施更有效。


<details>
  <summary>Details</summary>
Motivation: 现有的生物特征容量评估方法依赖于错误率，但这些错误率不能反映评分的质量。本文旨在提出一个评估机制来解决这一问题。

Method: 该研究首先确定评估的主要基础为评分的排名顺序和相关性，考虑了评分偏离、奖励机制以及评分与真实值的趋势对应关系，并引入了对身份特征的折扣因素。

Result: 本文提出的评估方法在三个数据集上与五个最先进的模型进行实验，结果显示选用我们的评分是最合适的，且该方法与现有的评估措施表现出相关性，进一步验证了其可靠性。

Conclusion: 研究结论表明，提出的方法能够更全面地评估健身特征评分的质量，其可靠性得到了验证。

Abstract: Quantifying biometric characteristics within hand gestures involve derivation of fitness scores from a gesture and identity aware feature space. However, evaluating the quality of these scores remains an open question. Existing biometric capacity estimation literature relies upon error rates. But these rates do not indicate goodness of scores. Thus, in this manuscript we present an exhaustive set of evaluation measures. We firstly identify ranking order and relevance of output scores as the primary basis for evaluation. In particular, we consider both rank deviation as well as rewards for: (i) higher scores of high ranked gestures and (ii) lower scores of low ranked gestures. We also compensate for correspondence between trends of output and ground truth scores. Finally, we account for disentanglement between identity features of gestures as a discounting factor. Integrating these elements with adequate weighting, we formulate advanced acceptance score as a holistic evaluation measure. To assess effectivity of the proposed we perform in-depth experimentation over three datasets with five state-of-the-art (SOTA) models. Results show that the optimal score selected with our measure is more appropriate than existing other measures. Also, our proposed measure depicts correlation with existing measures. This further validates its reliability. We have made our \href{https://github.com/AmanVerma2307/MeasureSuite}{code} public.

</details>


### [20] [Dynamic Training-Free Fusion of Subject and Style LoRAs](https://arxiv.org/abs/2602.15539)
*Qinglong Cao,Yuntian Chen,Chao Ma,Xiaokang Yang*

Main category: cs.CV

TL;DR: 该研究提出了一个无需训练的动态融合框架，该框架在整个生成过程中操作，通过计算KL散度和应用梯度修正来动态选择最适合的权重，并结合了特征级选择与基于目标度量的潜在调整机制，实现了高质量的用户指定主题与风格的综合。


<details>
  <summary>Details</summary>
Motivation: 现有的LoRA融合方法使用静态统计启发式方法融合LoRA权重，这偏离了LoRA学习自适应特征调整的初衷，并忽略了输入采样的随机性，因此需要一个动态且无需训练的融合方法来保持两个LoRA的有效性和适应性。

Method: 该方法通过在每次应用LoRA的层中动态计算KL散度来选择最合适的权重，并在反向去噪阶段应用基于目标度量梯度修正，在整个扩散时间线上实现特征级选择和基于目标度量的潜在调整。

Result: 实验表明，该方法在用户指定的主题与风格综合方面，无论是定性还是定量上都明显优于现有的LoRA融合方法。

Conclusion: 本研究提出的动态融合框架为实现高质量的用户指定主题与风格综合提供了一种有效的方法，无需额外的训练过程。

Abstract: Recent studies have explored the combination of multiple LoRAs to simultaneously generate user-specified subjects and styles. However, most existing approaches fuse LoRA weights using static statistical heuristics that deviate from LoRA's original purpose of learning adaptive feature adjustments and ignore the randomness of sampled inputs. To address this, we propose a dynamic training-free fusion framework that operates throughout the generation process. During the forward pass, at each LoRA-applied layer, we dynamically compute the KL divergence between the base model's original features and those produced by subject and style LoRAs, respectively, and adaptively select the most appropriate weights for fusion. In the reverse denoising stage, we further refine the generation trajectory by dynamically applying gradient-based corrections derived from objective metrics such as CLIP and DINO scores, providing continuous semantic and stylistic guidance. By integrating these two complementary mechanisms-feature-level selection and metric-guided latent adjustment-across the entire diffusion timeline, our method dynamically achieves coherent subject-style synthesis without any retraining. Extensive experiments across diverse subject-style combinations demonstrate that our approach consistently outperforms state-of-the-art LoRA fusion methods both qualitatively and quantitatively.

</details>


### [21] [Revealing and Enhancing Core Visual Regions: Harnessing Internal Attention Dynamics for Hallucination Mitigation in LVLMs](https://arxiv.org/abs/2602.15556)
*Guangtao Lyu,Qi Liu,Chenghao Xu,Jiexi Yan,Muli Yang,Xueting Li,Fen Fang,Cheng Deng*

Main category: cs.CV

TL;DR: PADE 提出了一种无需训练的注意力干预方法，通过构建 PAD 图来识别核心视觉区域，使用 Median Absolute Deviation Scaling 适配干预强度，并采用 System-Token Compensation 维持对复杂用户指令的注意力，从而提高视觉定位并减少幻觉现象。


<details>
  <summary>Details</summary>
Motivation: 考虑到现有训练免费的方法在应对注意力陷阱时容易失效，研究发现 LVLM 内部的 Positive Attention Dynamics (PAD) 能揭示受注意力陷阱影响下的核心视觉区域。因此，研究提出了 Positive Attention Dynamics Enhancement (PADE)，旨在增强 LVLM 的可靠多模态推理。

Method: PADE 通过创建 PAD 图来识别核心视觉区域，并使用 Median Absolute Deviation Scaling 来适配控制干预强度。此外，引入 System-Token Compensation 来保持对复杂用户指令的注意力并支持长期输出的一致性。

Result: 在多个 LVLM 和基准测试中的实验表明，PADE 能够提高视觉定位效果并减少幻觉现象，证实了利用内部注意力动态进行可靠多模态推理的有效性。

Conclusion: PADE 提供了一种有效的无需训练的注意力干预策略，能够提升 LVLM 的多模态推理性能，尤其在处理复杂的用户指令时表现出更好的稳定性。

Abstract: LVLMs have achieved strong multimodal reasoning capabilities but remain prone to hallucinations, producing outputs inconsistent with visual inputs or user instructions. Existing training-free methods, including contrastive decoding and auxiliary expert models, which incur several times more computational overhead and may introduce potential interference, as well as static internal signal enhancement, are often vulnerable to the attention sink phenomenon. We find that internal Positive Attention Dynamics (PAD) in LVLMs naturally reveal semantically core visual regions under the distortions of attention sinks. Based on this, we propose Positive Attention Dynamics Enhancement (PADE), a training-free attention intervention that constructs a PAD map to identify semantically core visual regions, applies per-head Median Absolute Deviation Scaling to adaptively control the intervention strength, and leverages System-Token Compensation to maintain attention to complex user instructions and support long-term output consistency. Experiments on multiple LVLMs and benchmarks show that PADE improves visual grounding and reduces hallucinations, validating the effectiveness of leveraging internal attention dynamics for reliable multimodal reasoning.

</details>


### [22] [Intracoronary Optical Coherence Tomography Image Processing and Vessel Classification Using Machine Learning](https://arxiv.org/abs/2602.15579)
*Amal Lahchim,Lambros Athanasiou*

Main category: cs.CV

TL;DR: 本文提出了一种用于冠状动脉光学相干断层成像(OCT)图像的全自动分割和分类管道，利用机器学习技术实现了卓越的精度。


<details>
  <summary>Details</summary>
Motivation: OCT技术可以实现高分辨率的冠状动脉血管解剖结构可视化，但存在噪声、成像伪影和复杂组织结构等挑战。

Method: 该方法结合了图像预处理、导丝伪影去除、极坐标转笛卡尔坐标系、无监督K-均值聚类以及局部特征提取，并利用这些特征训练逻辑回归和SVM分类器进行像素级别的血管分类。

Result: 实验结果表明，该方法性能优越，精确度、召回率和F1值均可达到1.00，总体分类准确率为99.68%。该方法提供了精确的血管边界检测，同时保持较低的计算复杂度并减少手动注释的需要。

Conclusion: 本文提出的方法为OCT图像的自动分析提供了一个可靠且高效的解决方案，具有临床决策支持和实时医学图像处理的应用前景。

Abstract: Intracoronary Optical Coherence Tomography (OCT) enables high-resolution visualization of coronary vessel anatomy but presents challenges due to noise, imaging artifacts, and complex tissue structures. This paper proposes a fully automated pipeline for vessel segmentation and classification in OCT images using machine learning techniques. The proposed method integrates image preprocessing, guidewire artifact removal, polar-to-Cartesian transformation, unsupervised K-means clustering, and local feature extraction. These features are used to train Logistic Regression and Support Vector Machine classifiers for pixel-wise vessel classification. Experimental results demonstrate excellent performance, achieving precision, recall, and F1-score values up to 1.00 and overall classification accuracy of 99.68%. The proposed approach provides accurate vessel boundary detection while maintaining low computational complexity and requiring minimal manual annotation. This method offers a reliable and efficient solution for automated OCT image analysis and has potential applications in clinical decision support and real-time medical image processing.

</details>


### [23] [An Industrial Dataset for Scene Acquisitions and Functional Schematics Alignment](https://arxiv.org/abs/2602.15584)
*Flavien Armangeon,Thibaud Ehret,Enric Meinhardt-Llopis,Rafael Grompone von Gioi,Guillaume Thibault,Marc Petit,Gabriele Facciolo*

Main category: cs.CV

TL;DR: 该论文介绍了一个名为IRIS-v2的综合数据集，以支持工业场景的2D和3D对齐研究。数据集包含图像、点云、2D标注框和分割掩码、CAD模型、3D管道线路信息以及P&ID。通过结合分割和图匹配技术，在实际案例研究中实现对齐以减少工作时间。


<details>
  <summary>Details</summary>
Motivation: 当前的手动对齐方法在工业场景中由于繁琐性和复杂性无法扩展，同时存在图样与实际情况之间的不一致性以及缺乏公开的工业数据集，这使得该问题至关重要且未被充分探索。

Method: 通过构建IRIS-v2综合数据集，实验采用结合分割和图匹配技术来提升对齐效率和准确性。

Result: 该研究展示了IRIS-v2数据集的有效性，并证明了结合分割和图匹配方法可以在实际案例研究中显著减少对齐所需时间。

Conclusion: 该论文通过提出IRIS-v2数据集和采用融合分割和图匹配技术的方法，为工业场景中的2D和3D对齐提供了有价值的新见解，并表明该方法在实际应用中的普适性和优越性。

Abstract: Aligning functional schematics with 2D and 3D scene acquisitions is crucial for building digital twins, especially for old industrial facilities that lack native digital models. Current manual alignment using images and LiDAR data does not scale due to tediousness and complexity of industrial sites. Inconsistencies between schematics and reality, and the scarcity of public industrial datasets, make the problem both challenging and underexplored. This paper introduces IRIS-v2, a comprehensive dataset to support further research. It includes images, point clouds, 2D annotated boxes and segmentation masks, a CAD model, 3D pipe routing information, and the P&ID (Piping and Instrumentation Diagram). The alignment is experimented on a practical case study, aiming at reducing the time required for this task by combining segmentation and graph matching.

</details>


### [24] [Concept-Enhanced Multimodal RAG: Towards Interpretable and Accurate Radiology Report Generation](https://arxiv.org/abs/2602.15650)
*Marco Salmè,Federico Siciliano,Fabrizio Silvestri,Paolo Soda,Rosa Sicilia,Valerio Guarrasi*

Main category: cs.CV

TL;DR: 该研究提出了一种名为CEMRAG的框架，旨在通过将视觉表示分解为可解释的临床概念并结合多模态检索增强生成（RAG），提高医学放射学报告的透明度和准确性。通过MIMIC-CXR和IU X-Ray数据集的实验，展示了CEMRAG在临床准确性指标和标准NLP评估中的持续改进。


<details>
  <summary>Details</summary>
Motivation: 现有研究中，基于概念的可解释性和检索增强生成方法通常分别处理可解释性和准确性，本研究旨在通过CEMRAG框架统一这两者，以提高医学VLM在放射学报告中的透明性和准确性，从而促进其临床应用。

Method: CEMRAG框架通过将视觉表示分解为可解释的临床概念，并结合多模态RAG进行检索增强生成，使用增强的上下文提示来改进放射学报告。方法包括从医学影像中提取视觉信息，并将这些信息与语言模型相结合，生成具有更高透明度和事实正确性的报告。

Result: 在MIMIC-CXR和IU X-Ray数据集上的实验表明，CEMRAG在临床准确性指标和标准NLP标准上持续优于传统的RAG和仅基于概念的方法。这种方法挑战了可解释性与性能之间通常被认为的权衡。

Conclusion: CEMRAG框架为临床相关的人工智能辅助放射学提供了一种新的途径，通过透明的视觉概念增强诊断准确性，而非削弱它。该设计提供了透明可视性和结构化语言模型条件的模块化路径，促进可信的AI辅助医学放射学的发展。

Abstract: Radiology Report Generation (RRG) through Vision-Language Models (VLMs) promises to reduce documentation burden, improve reporting consistency, and accelerate clinical workflows. However, their clinical adoption remains limited by the lack of interpretability and the tendency to hallucinate findings misaligned with imaging evidence. Existing research typically treats interpretability and accuracy as separate objectives, with concept-based explainability techniques focusing primarily on transparency, while Retrieval-Augmented Generation (RAG) methods targeting factual grounding through external retrieval. We present Concept-Enhanced Multimodal RAG (CEMRAG), a unified framework that decomposes visual representations into interpretable clinical concepts and integrates them with multimodal RAG. This approach exploits enriched contextual prompts for RRG, improving both interpretability and factual accuracy. Experiments on MIMIC-CXR and IU X-Ray across multiple VLM architectures, training regimes, and retrieval configurations demonstrate consistent improvements over both conventional RAG and concept-only baselines on clinical accuracy metrics and standard NLP measures. These results challenge the assumed trade-off between interpretability and performance, showing that transparent visual concepts can enhance rather than compromise diagnostic accuracy in medical VLMs. Our modular design decomposes interpretability into visual transparency and structured language model conditioning, providing a principled pathway toward clinically trustworthy AI-assisted radiology.

</details>


### [25] [A Novel Public Dataset for Strawberry (Fragaria x ananassa) Ripeness Detection and Comparative Evaluation of YOLO-Based Models](https://arxiv.org/abs/2602.15656)
*Mustafa Yurdakul,Zeynep Sena Bastug,Ali Emre Gok,Sakir Taşdemir*

Main category: cs.CV

TL;DR: 本文介绍了一个新的公开草莓成熟度数据集，并使用YOLOv8s、YOLOv9c和YOLO11s模型进行了比较测试，展示了不同的模型性能。


<details>
  <summary>Details</summary>
Motivation: 为了提高水果收获期成熟度判断的准确性，本文提出了一种新的公开草莓成熟度数据集，并通过对比不同的YOLO模型来验证其有效性。

Method: 本文构建了一个包含566张图片和1,201个标注对象的数据集，并在两个不同的温室在不同光照和环境条件下进行测试。之后，使用YOLOv8、YOLOv9c和YOLO11s模型进行了模型性能测试。

Result: 测试结果显示，YOLOv9c在精确度方面最高，达到了90.94%；YOLO11s在召回率方面最高，达到了83.74%；YOLOv8s在mAP@50指标中表现出最优性能，达到了86.09%。

Conclusion: 本文构建的数据集和模型测试结果为智能农业应用提供了基本参考点，表明小到中型模型更适用于这类数据集，在逐步提高成熟度判断准确性上具有重要意义。

Abstract: The strawberry (Fragaria x ananassa), known worldwide for its economic value and nutritional richness, is a widely cultivated fruit. Determining the correct ripeness level during the harvest period is crucial for both preventing losses for producers and ensuring consumers receive a quality product. However, traditional methods, i.e., visual assessments alone, can be subjective and have a high margin of error. Therefore, computer-assisted systems are needed. However, the scarcity of comprehensive datasets accessible to everyone in the literature makes it difficult to compare studies in this field. In this study, a new and publicly available strawberry ripeness dataset, consisting of 566 images and 1,201 labeled objects, prepared under variable light and environmental conditions in two different greenhouses in Turkey, is presented to the literature. Comparative tests conducted on the data set using YOLOv8, YOLOv9, and YOLO11-based models showed that the highest precision value was 90.94% in the YOLOv9c model, while the highest recall value was 83.74% in the YOLO11s model. In terms of the general performance criterion mAP@50, YOLOv8s was the best performing model with a success rate of 86.09%. The results show that small and medium-sized models work more balanced and efficiently on this type of dataset, while also establishing a fundamental reference point for smart agriculture applications.

</details>


### [26] [Bayesian Optimization for Design Parameters of 3D Image Data Analysis](https://arxiv.org/abs/2602.15660)
*David Exler,Joaquin Eduardo Urrutia Gómez,Martin Krüger,Maike Schliephake,John Jbeily,Mario Vitacolonna,Rüdiger Rudolf,Markus Reischl*

Main category: cs.CV

TL;DR: 本文介绍了一种用于三维数据分割和分类优化的管道，包括两个基于贝叶斯优化阶段的方法，逐步选择和参数化模型。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法虽多，但参数调优和模型选择过程繁琐，且手动标注数据劳动密集，为此提出该优化管道以提高效率与准确性。

Method: 该方法分为两阶段的贝叶斯优化：第一阶段优化分割模型及后处理参数；第二阶段优化分类器设计，包括编码器和分类头架构、先验知识整合与预训练策略。同时引入了交互式协助标注流程。

Result: 在四个案例中，该优化管道能够有效识别针对不同数据集的最佳模型和参数配置。

Conclusion: 该方法显著提高了三维生物医学影像数据的自动化处理能力，降低标注要求并提升分割与分类的准确性。

Abstract: Deep learning-based segmentation and classification are crucial to large-scale biomedical imaging, particularly for 3D data, where manual analysis is impractical. Although many methods exist, selecting suitable models and tuning parameters remains a major bottleneck in practice. Hence, we introduce the 3D data Analysis Optimization Pipeline, a method designed to facilitate the design and parameterization of segmentation and classification using two Bayesian Optimization stages. First, the pipeline selects a segmentation model and optimizes postprocessing parameters using a domain-adapted syntactic benchmark dataset. To ensure a concise evaluation of segmentation performance, we introduce a segmentation quality metric that serves as the objective function. Second, the pipeline optimizes design choices of a classifier, such as encoder and classifier head architectures, incorporation of prior knowledge, and pretraining strategies. To reduce manual annotation effort, this stage includes an assisted class-annotation workflow that extracts predicted instances from the segmentation results and sequentially presents them to the operator, eliminating the need for manual tracking. In four case studies, the 3D data Analysis Optimization Pipeline efficiently identifies effective model and parameter configurations for individual datasets.

</details>


### [27] [Criteria-first, semantics-later: reproducible structure discovery in image-based sciences](https://arxiv.org/abs/2602.15712)
*Jan Bumberger*

Main category: cs.CV

TL;DR: 本文提出了一种基于标准先行的结构发现方法，该方法将在跨领域科学中的图像分析中促进可重复性，通过分离结构提取和下游语义映射来定义稳定的数据分区，使解析过程更加透明且不受特定领域语义的限制。


<details>
  <summary>Details</summary>
Motivation: 当前的分析范式主要基于语义优先的方法，这种方法在开放的科学发现、跨传感器和站点的可比性以及长期监控中会失效，尤其是在领域本体和相关标签集随文化、机构和生态变化时。

Method: 该论文提出了一种标准先行的方法，首先定义标准，然后从图像中提取结构而不关注语言，最后将提取的结构映射到特定领域的本体或词汇表，从而实现跨领域科学的可重复分析。

Result: 该方法提供了一个通用框架，用于在基于图像的科学研究中进行标准先行的结构发现。这种方法促进了稳定分区、结构域或层次结构的定义，并允许多元解析和明确的领域间映射，而无需重写上游提取过程。

Conclusion: 作者认为这种方法可以改善跨领域的图像科学分析的可验证性和长期监控的有效性，定义结构产品的过程应基于清晰的优化标准，而不是特定领域的本体论，从而确保解析过程的透明度和可重复性。

Abstract: Across the natural and life sciences, images have become a primary measurement modality, yet the dominant analytic paradigm remains semantics-first. Structure is recovered by predicting or enforcing domain-specific labels. This paradigm fails systematically under the conditions that make image-based science most valuable, including open-ended scientific discovery, cross-sensor and cross-site comparability, and long-term monitoring in which domain ontologies and associated label sets drift culturally, institutionally, and ecologically. A deductive inversion is proposed in the form of criteria-first and semantics-later. A unified framework for criteria-first structure discovery is introduced. It separates criterion-defined, semantics-free structure extraction from downstream semantic mapping into domain ontologies or vocabularies and provides a domain-general scaffold for reproducible analysis across image-based sciences. Reproducible science requires that the first analytic layer perform criterion-driven, semantics-free structure discovery, yielding stable partitions, structural fields, or hierarchies defined by explicit optimality criteria rather than local domain ontologies. Semantics is not discarded; it is relocated downstream as an explicit mapping from the discovered structural product to a domain ontology or vocabulary, enabling plural interpretations and explicit crosswalks without rewriting upstream extraction. Grounded in cybernetics, observation-as-distinction, and information theory's separation of information from meaning, the argument is supported by cross-domain evidence showing that criteria-first components recur whenever labels do not scale. Finally, consequences are outlined for validation beyond class accuracy and for treating structural products as FAIR, AI-ready digital objects for long-term monitoring and digital twins.

</details>


### [28] [ToaSt: Token Channel Selection and Structured Pruning for Efficient ViT](https://arxiv.org/abs/2602.15720)
*Hyunchan Moon,Cheonjun Park,Steven L. Waslander*

Main category: cs.CV

TL;DR: ToaSt 提出了一种针对 ViT 的去耦框架，通过针对不同组件采用特定策略，并应用于多头自注意力模块的耦合头结构化剪枝及 Feed-Forward Networks 中的 Token Channel Selection，实现了在保持高性能的同时显着降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 为了解决 ViT 部署时由于高计算成本所带来的挑战，特别是在大规模模型上进行训练和推理时效率低下，ToaSt 提出了一种新的去耦框架。

Method: ToaSt 通过头耦合结构化剪枝方法应用于多头自注意力模块，利用注意力操作的特点来增加鲁棒性。对于 Feed-Forward Networks，引入了 Token Channel Selection 方法，减少了剪枝过程中的全局传播问题，有效过滤选择中的冗余噪声。

Result: 在多个 ViT 模型包括 DeiT、ViT-MAE 和 Swin Transformer 上进行了广泛的评估，ToaSt 在保持高准确率的同时实现了显著的计算量减少，在 ViT-MAE-Huge 上取得了 88.52% 的准确率（比基线高 1.64%），并达到了 39.4% 的 FLOPs 减少。此外，ToaSt 还在下游任务中表现出良好的迁移性能。

Conclusion: ToaSt 提出的框架在多个 ViT 模型上实现了准确性和计算效率之间的优异权衡，展示了其在实际应用中的潜力。

Abstract: Vision Transformers (ViTs) have achieved remarkable success across various vision tasks, yet their deployment is often hindered by prohibitive computational costs. While structured weight pruning and token compression have emerged as promising solutions, they suffer from prolonged retraining times and global propagation that creates optimization challenges, respectively. We propose ToaSt, a decoupled framework applying specialized strategies to distinct ViT components. We apply coupled head-wise structured pruning to Multi-Head Self-Attention modules, leveraging attention operation characteristics to enhance robustness. For Feed-Forward Networks (over 60\% of FLOPs), we introduce Token Channel Selection (TCS) that enhances compression ratios while avoiding global propagation issues. Our analysis reveals TCS effectively filters redundant noise during selection. Extensive evaluations across nine diverse models, including DeiT, ViT-MAE, and Swin Transformer, demonstrate that ToaSt achieves superior trade-offs between accuracy and efficiency, consistently outperforming existing baselines. On ViT-MAE-Huge, ToaSt achieves 88.52\% accuracy (+1.64 \%) with 39.4\% FLOPs reduction. ToaSt transfers effectively to downstream tasks, cccccachieving 52.2 versus 51.9 mAP on COCO object detection. Code and models will be released upon acceptance.

</details>


### [29] [Learning to Retrieve Navigable Candidates for Efficient Vision-and-Language Navigation](https://arxiv.org/abs/2602.15724)
*Shutian Gu,Chengkai Huang,Ruoyu Wang,Lina Yao*

Main category: cs.CV

TL;DR: 本文提出了一种检索增强框架，以提高基于大型语言模型的视觉语言导航(VLN)的效率和稳定性，而无需修改或微调基础语言模型。


<details>
  <summary>Details</summary>
Motivation: 传统的基于提示的大型语言模型导航效率低下，特别是在每一步需要评估和推理大量冗余的导航选项时。因此，本文旨在通过引入检索机制来改善这种状况。

Method: 该方法在两个层次上引入了检索模块：在任务级别（回溯级），使用指令级嵌入检索相关成功的导航轨迹，形成任务特定的先验知识；在步骤级别（回溯级），通过模仿学习裁剪无关的导航方向，减少动作的模糊性和提示的复杂性。

Result: 该方法在室间室导航基准测试上显示出一致的改进，包括提高成功的比率、Oracle成功率和导航成功率。

Conclusion: 通过实验和消融研究，该研究进一步证明了指令级别的示例检索和候选裁剪对全局指导和步骤决策效率提供了互补的支持，这表明检索增强决策支持是一种提高基于语言模型的视觉语言导航的有效且可扩展的策略。

Abstract: Vision-and-Language Navigation (VLN) requires an agent to follow natural-language instructions and navigate through previously unseen environments. Recent approaches increasingly employ large language models (LLMs) as high-level navigators due to their flexibility and reasoning capability. However, prompt-based LLM navigation often suffers from inefficient decision-making, as the model must repeatedly interpret instructions from scratch and reason over noisy and verbose navigable candidates at each step. In this paper, we propose a retrieval-augmented framework to improve the efficiency and stability of LLM-based VLN without modifying or fine-tuning the underlying language model. Our approach introduces retrieval at two complementary levels. At the episode level, an instruction-level embedding retriever selects semantically similar successful navigation trajectories as in-context exemplars, providing task-specific priors for instruction grounding. At the step level, an imitation-learned candidate retriever prunes irrelevant navigable directions before LLM inference, reducing action ambiguity and prompt complexity. Both retrieval modules are lightweight, modular, and trained independently of the LLM. We evaluate our method on the Room-to-Room (R2R) benchmark. Experimental results demonstrate consistent improvements in Success Rate, Oracle Success Rate, and SPL on both seen and unseen environments. Ablation studies further show that instruction-level exemplar retrieval and candidate pruning contribute complementary benefits to global guidance and step-wise decision efficiency. These results indicate that retrieval-augmented decision support is an effective and scalable strategy for enhancing LLM-based vision-and-language navigation.

</details>


### [30] [Language and Geometry Grounded Sparse Voxel Representations for Holistic Scene Understanding](https://arxiv.org/abs/2602.15734)
*Guile Wu,David Huang,Bingbing Liu,Dongfeng Bai*

Main category: cs.CV

TL;DR: 本文提出了一种新的方法，利用语言和几何约束下的稀疏体素表示来全面建模3D场景的外观、语义和几何结构。通过构建特性调制模块并从2D基础模型中提取语言特征到3D场景模型中，以及通过深度相关正则化和模式一致性正则化将几何知识从几何基础模型转移到3D场景表示，以促进特征域中外观、密度和特征域之间的协同作用。


<details>
  <summary>Details</summary>
Motivation: 现有的3D开放词汇场景理解方法主要强调从2D基础模型提取语言特征到3D特征领域，但忽视了场景外观、语义和几何之间的协同作用。这种忽视导致场景理解偏离了场景的几何结构，与重建过程脱节。

Method: 本文提出的方法利用语言和几何约束下的稀疏体素表示，引入了特征调制模块，从2D基础模型将语言特征转移到3D场景模型中，同时通过深度相关正则化和模式一致性正则化将几何知识从几何基础模型转移到3D场景表示中。

Result: 该方法在全面场景理解和重建方面超越了现有先进技术。

Conclusion: 本研究结合语义和几何信息的提取与模型化，提出了一个全面的3D场景理解框架，显示出在该领域的潜在应用和改进潜力。

Abstract: Existing 3D open-vocabulary scene understanding methods mostly emphasize distilling language features from 2D foundation models into 3D feature fields, but largely overlook the synergy among scene appearance, semantics, and geometry. As a result, scene understanding often deviates from the underlying geometric structure of scenes and becomes decoupled from the reconstruction process. In this work, we propose a novel approach that leverages language and geometry grounded sparse voxel representations to comprehensively model appearance, semantics, and geometry within a unified framework. Specifically, we use 3D sparse voxels as primitives and employ an appearance field, a density field, a feature field, and a confidence field to holistically represent a 3D scene. To promote synergy among the appearance, density, and feature fields, we construct a feature modulation module and distill language features from a 2D foundation model into our 3D scene model. In addition, we integrate geometric distillation into feature field distillation to transfer geometric knowledge from a geometry foundation model to our 3D scene representations via depth correlation regularization and pattern consistency regularization. These components work together to synergistically model the appearance, semantics, and geometry of the 3D scene within a unified framework. Extensive experiments demonstrate that our approach achieves superior overall performance compared with state-of-the-art methods in holistic scene understanding and reconstruction.

</details>


### [31] [Understanding vs. Generation: Navigating Optimization Dilemma in Multimodal Models](https://arxiv.org/abs/2602.15772)
*Sen Ye,Mengde Xu,Shuyang Gu,Di He,Liwei Wang,Han Hu*

Main category: cs.CV

TL;DR: 该研究分析了生成能力和理解能力在多模态模型中的权衡，并提出了一种新的算法R3框架，通过多步骤的“生成-理解-再生”过程增强模型的理解能力，从而优化了生成结果。


<details>
  <summary>Details</summary>
Motivation: 当前多模态模型研究面临的挑战是在增强生成能力的同时可能会削弱理解能力，反之亦然。本研究旨在解决这一问题，并提出一种新的算法R3框架来改善这一现状。

Method: 研究分析了生成与理解之间的潜在冲突，并提出了Reason-Reflect-Refine (R3) 框架。R3将单步生成任务重新构想为一个多步骤生成-理解-再生过程，通过利用模型的理解能力来改善生成结果。

Result: 通过R3框架，研究实现了更好的生成结果并强化了相关理解能力。此方法提供了设计下一代一体化多模态模型的重要见解。

Conclusion: R3框架为解决生成能力和理解能力之间的权衡问题提供了一种有效的方法，标志着在设计一体化多模态模型方面取得了进展。

Abstract: Current research in multimodal models faces a key challenge where enhancing generative capabilities often comes at the expense of understanding, and vice versa. We analyzed this trade-off and identify the primary cause might be the potential conflict between generation and understanding, which creates a competitive dynamic within the model. To address this, we propose the Reason-Reflect-Refine (R3) framework. This innovative algorithm re-frames the single-step generation task into a multi-step process of "generate-understand-regenerate". By explicitly leveraging the model's understanding capability during generation, we successfully mitigate the optimization dilemma, achieved stronger generation results and improved understanding ability which are related to the generation process. This offers valuable insights for designing next-generation unified multimodal models. Code is available at https://github.com/sen-ye/R3.

</details>


### [32] [NeRFscopy: Neural Radiance Fields for in-vivo Time-Varying Tissues from Endoscopy](https://arxiv.org/abs/2602.15775)
*Laura Salort-Benejam,Antonio Agudo*

Main category: cs.CV

TL;DR: NeRFscopy 是一个自监督的管道，通过单一内窥镜视频生成新视角并进行3D重建，对于可变形内窥镜组织。


<details>
  <summary>Details</summary>
Motivation: 当前内窥镜视频重建遇到挑战，如组织变形、单目摄像头使用、照明变化等。NeRFscopy 引入了神经渲染方法解决这些问题，以改善可视化，提升诊断精度，并辅助治疗计划和手术指导。

Method: NeRFscopy 使用自监督方法，通过构建可变形模型和时间依赖变形场来提取3D隐式模型，无需假设模板或预训练模型，完全从数据中学习。

Result: NeRFscopy 在新颖视角合成方面表现准确，超越了其他竞争方法，对于各种挑战性内窥镜场景也取得了显著成效。

Conclusion: 这项研究提出了一种创新的方法来解决内窥镜视频重建的难题，并展示了其在多种内窥镜场景中的优势。

Abstract: Endoscopy is essential in medical imaging, used for diagnosis, prognosis and treatment. Developing a robust dynamic 3D reconstruction pipeline for endoscopic videos could enhance visualization, improve diagnostic accuracy, aid in treatment planning, and guide surgery procedures. However, challenges arise due to the deformable nature of the tissues, the use of monocular cameras, illumination changes, occlusions and unknown camera trajectories. Inspired by neural rendering, we introduce NeRFscopy, a self-supervised pipeline for novel view synthesis and 3D reconstruction of deformable endoscopic tissues from a monocular video. NeRFscopy includes a deformable model with a canonical radiance field and a time-dependent deformation field parameterized by SE(3) transformations. In addition, the color images are efficiently exploited by introducing sophisticated terms to learn a 3D implicit model without assuming any template or pre-trained model, solely from data. NeRFscopy achieves accurate results in terms of novel view synthesis, outperforming competing methods across various challenging endoscopy scenes.

</details>


### [33] [Context-aware Skin Cancer Epithelial Cell Classification with Scalable Graph Transformers](https://arxiv.org/abs/2602.15783)
*Lucas Sancéré,Noémie Moreau,Katarzyna Bozek*

Main category: cs.CV

TL;DR: 该研究提出了使用可扩展的图变换器在全WSI细胞图上进行分类，以克服传统基于Patch的方法在保持组织级上下文方面的限制。通过对健康和肿瘤上皮细胞的分类研究，表明图变换器方法在多个WSI上的表现超过了基于图像的方法。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习方法多依赖于基于Patch的表示形式，虽然在分割和分类任务中表现出色，但会失去组织级别的上下文。本研究旨在通过使用可扩展的图变换器（Graph Transformer）方法，直接在全WSI细胞图上进行分类，以克服这一问题。

Method: 研究者将基于图像的和基于图的方法应用于单个WSI的实验，通过比较发现基于图的方法表现优于基于图像的方法。通过调整节点特征，研究者确定了形态学、纹理特征以及非上皮细胞类别的组合是最优表示。进一步地，研究者提出了从每个图像中提取四个$2560 	imes 2560$像素的Patch并将其转换成图的方法来处理计算约束。这使得研究者可以在多个患者的多个WSI上训练DIFFormer。

Result: 在比较了不同方法的表现后，研究发现基于图的方法SGFormer和DIFFormer在3折交叉验证中的平衡准确率分别达到了$85.2 	imes 1.5$和$85.1 	imes 2.5$，优于最好的基于图像的方法$81.2 	imes 3.0$。此外，当研究将方法扩展到多个患者上时，DIFFormer达到了平衡准确率$83.6 	imes 1.9$，而最先进的基于图像的方法CellViT256仅获得了$78.1 	imes 0.5$。

Conclusion: 本文提出的方法在多个WSI上显示出优越性，强调在全WSI层面利用图变换器进行细胞分类的效果，同时研究也指出了优化节点特征对于模型性能的重要性。

Abstract: Whole-slide images (WSIs) from cancer patients contain rich information that can be used for medical diagnosis or to follow treatment progress. To automate their analysis, numerous deep learning methods based on convolutional neural networks and Vision Transformers have been developed and have achieved strong performance in segmentation and classification tasks. However, due to the large size and complex cellular organization of WSIs, these models rely on patch-based representations, losing vital tissue-level context. We propose using scalable Graph Transformers on a full-WSI cell graph for classification. We evaluate this methodology on a challenging task: the classification of healthy versus tumor epithelial cells in cutaneous squamous cell carcinoma (cSCC), where both cell types exhibit very similar morphologies and are therefore difficult to differentiate for image-based approaches. We first compared image-based and graph-based methods on a single WSI. Graph Transformer models SGFormer and DIFFormer achieved balanced accuracies of $85.2 \pm 1.5$ ($\pm$ standard error) and $85.1 \pm 2.5$ in 3-fold cross-validation, respectively, whereas the best image-based method reached $81.2 \pm 3.0$. By evaluating several node feature configurations, we found that the most informative representation combined morphological and texture features as well as the cell classes of non-epithelial cells, highlighting the importance of the surrounding cellular context. We then extended our work to train on several WSIs from several patients. To address the computational constraints of image-based models, we extracted four $2560 \times 2560$ pixel patches from each image and converted them into graphs. In this setting, DIFFormer achieved a balanced accuracy of $83.6 \pm 1.9$ (3-fold cross-validation), while the state-of-the-art image-based model CellViT256 reached $78.1 \pm 0.5$.

</details>


### [34] [Task-Agnostic Continual Learning for Chest Radiograph Classification](https://arxiv.org/abs/2602.15811)
*Muthu Subash Kavitha,Anas Zafar,Amgad Muneer,Jia Wu*

Main category: cs.CV

TL;DR: 该论文提出了一种针对胸片分类的增量持续学习方法CARL-XRay，能够在不重新训练或存储原始图像的情况下，实现在连续接收异构胸片数据集时的稳定任务识别和适应。


<details>
  <summary>Details</summary>
Motivation: 临床环境中，新的数据集不断出现，需要一种能够在不重新训练或丢失验证性能的情况下，实时适应新任务的持续学习方法。

Method: 该方法基于持续适配器的路由学习策略，通过增量分配轻量级的任务特定适配器和分类头，以及任务适配特征上的潜在任务选择器，来维护固定容量的主干，并保留历史上下文信息，从而实现稳定的任务识别和适应。

Result: 在大规模公开胸片数据集上进行的实验表明，该方法在连续数据集导入时能够保持稳健的性能，提供可靠的任务感知推理，且在未知任务部署下的路由准确性高达75%，同时保持了较高的诊断性能。

Conclusion: CARL-XRay提出了一个实际的替代方案，可以替代联合训练和反复完全重新训练，在持续临床部署中提供稳定的任务识别和适应。

Abstract: Clinical deployment of chest radiograph classifiers requires models that can be updated as new datasets become available without retraining on previously ob- served data or degrading validated performance. We study, for the first time, a task-incremental continual learning setting for chest radiograph classification, in which heterogeneous chest X-ray datasets arrive sequentially and task identifiers are unavailable at inference. We propose a continual adapter-based routing learning strategy for Chest X-rays (CARL-XRay) that maintains a fixed high-capacity backbone and incrementally allocates lightweight task-specific adapters and classifier heads. A latent task selector operates on task-adapted features and leverages both current and historical context preserved through compact prototypes and feature-level experience replay. This design supports stable task identification and adaptation across sequential updates while avoiding raw-image storage. Experiments on large-scale public chest radiograph datasets demonstrate robust performance retention and reliable task-aware inference under continual dataset ingestion. CARL-XRay outperforms joint training under task-unknown deployment, achieving higher routing accuracy (75.0\% vs.\ 62.5\%), while maintaining competitive diagnostic performance with AUROC of 0.74 in the oracle setting with ground-truth task identity and 0.75 under task-unknown inference, using significantly fewer trainable parameters. Finally, the proposed framework provides a practical alternative to joint training and repeated full retraining in continual clinical deployment.

</details>


### [35] [VideoSketcher: Video Models Prior Enable Versatile Sequential Sketch Generation](https://arxiv.org/abs/2602.15819)
*Hui Ren,Yuval Alaluf,Omer Bar Tal,Alexander Schwing,Antonio Torralba,Yael Vinker*

Main category: cs.CV

TL;DR: 该研究提出了一种数据高效的方法，将预训练的文本到视频扩散模型适应为生成绘图过程，通过将素描表示为逐步绘制在空白画布上的简短视频，利用大型语言模型的语义规划与视频扩散模型的高质量渲染相互补充，成功生成了高质量的、遵循文本指定顺序的素描。


<details>
  <summary>Details</summary>
Motivation: 现有的生成模型通常将素描视为静态图像，并忽略了其背后的时间结构，这在创造性绘图中是至关重要的。因此，该研究旨在利用大型语言模型和视频扩散模型的互补优势，提出一种能够生成具有顺序性和连续性的高质量素描的高效方法。

Method: 该方法采用两阶段精细调优策略，分别学习画笔顺序和素描外观。画笔顺序通过合成具有控制时间结构的形状来学习，而视觉外观仅从七个手工绘制的素描过程中提取，这些过程涵盖全局绘图顺序和单个画笔的连续形成。

Result: 在使用极少量的人工绘制素描数据下，该方法生成了高质量、严密遵循文本顺序和详尽视觉细节的素描序列。

Conclusion: 这种方法展示了在生成模型中融入时间序列信息的价值，并通过额外的画笔风格条件和自回归素描生成扩展，增强了绘图过程的灵活性和可控性。

Abstract: Sketching is inherently a sequential process, in which strokes are drawn in a meaningful order to explore and refine ideas. However, most generative models treat sketches as static images, overlooking the temporal structure that underlies creative drawing. We present a data-efficient approach for sequential sketch generation that adapts pretrained text-to-video diffusion models to generate sketching processes. Our key insight is that large language models and video diffusion models offer complementary strengths for this task: LLMs provide semantic planning and stroke ordering, while video diffusion models serve as strong renderers that produce high-quality, temporally coherent visuals. We leverage this by representing sketches as short videos in which strokes are progressively drawn on a blank canvas, guided by text-specified ordering instructions. We introduce a two-stage fine-tuning strategy that decouples the learning of stroke ordering from the learning of sketch appearance. Stroke ordering is learned using synthetic shape compositions with controlled temporal structure, while visual appearance is distilled from as few as seven manually authored sketching processes that capture both global drawing order and the continuous formation of individual strokes. Despite the extremely limited amount of human-drawn sketch data, our method generates high-quality sequential sketches that closely follow text-specified orderings while exhibiting rich visual detail. We further demonstrate the flexibility of our approach through extensions such as brush style conditioning and autoregressive sketch generation, enabling additional controllability and interactive, collaborative drawing.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [36] [EduResearchBench: A Hierarchical Atomic Task Decomposition Benchmark for Full-Lifecycle Educational Research](https://arxiv.org/abs/2602.15034)
*Houping Yue,Zixiang Di,Mei Jiang,Bingdong Li,Hao Hao,Yu Song,Bo Jiang,Aimin Zhou*

Main category: cs.CL

TL;DR: EduResearchBench 是首个专为教育学术写作设计的综合评估平台，通过分层原子任务分解框架评估 LLM 的能力，并提出了一种渐进式学习策略，依托丰富高质量数据训练 EduWrite 模型，结果表明数据质量和层级训练策略比参数规模更为关键。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注单次生成任务，难以精细评估 AI 在复杂学术研究中的表现。因此开发 EduResearchBench 来填补这一空白。

Method: EduResearchBench 基于 HATD 框架，将研究流程分解为六个专业模块和 24 个具体任务。利用 55,000 个原始学术样本和 11,000 个高质量的教学示例对 EduWrite 模型进行培训。采用渐进式学习策略从基础技能逐步提升到复杂方法论。

Result: EduWrite 在垂直领域上的多项关键指标上超过了更大的通用模型（72B），显示了高质量数据和层级培训策略的重要性。

Conclusion: EduResearchBench 改进了现有评估基准，并提出的方法表明，在专领域中，数据质量和层级训练策略比模型参数更为重要。

Abstract: While Large Language Models (LLMs) are reshaping the paradigm of AI for Social Science (AI4SS), rigorously evaluating their capabilities in scholarly writing remains a major challenge. Existing benchmarks largely emphasize single-shot, monolithic generation and thus lack the fine-grained assessments required to reflect complex academic research workflows. To fill this gap, we introduce EduResearchBench, the first comprehensive evaluation platform dedicated to educational academic writing. EduResearchBench is built upon our Hierarchical Atomic Task Decomposition (HATD) framework, which decomposes an end-to-end research workflow into six specialized research modules (e.g., Quantitative Analysis, Qualitative Research, and Policy Research) spanning 24 fine-grained atomic tasks. This taxonomy enables an automated evaluation pipeline that mitigates a key limitation of holistic scoring, where aggregate scores often obscure specific capability bottlenecks, and instead provides fine-grained, diagnostic feedback on concrete deficiencies. Moreover, recognizing the high cognitive load inherent in scholarly writing, we propose a curriculum learning strategy that progressively builds competence from foundational skills to complex methodological reasoning and argumentation. Leveraging 55K raw academic samples, we curate 11K high-quality instruction pairs to train EduWrite, a specialized educational scholarly writing model. Experiments show that EduWrite (30B) substantially outperforms larger general-purpose models (72B) on multiple core metrics, demonstrating that in vertical domains, data quality density and hierarchically staged training curricula are more decisive than parameter scale.

</details>


### [37] [Indic-TunedLens: Interpreting Multilingual Models in Indian Languages](https://arxiv.org/abs/2602.15038)
*Mihir Panchal,Deeksha Varshney,Mamta,Asif Ekbal*

Main category: cs.CL

TL;DR: 该研究提出了Indic-TunedLens，一种专为印度语言设计的可解释性框架，通过学习共享的仿射变换，增强了对多语言变换器层次语义编码的理解，特别对于形态丰富且数据稀缺的语言效果显著。


<details>
  <summary>Details</summary>
Motivation: 由于大多数可解释性工具主要针对英语，而多语言大型语言模型（LLMs）在语言多样性的地区如印度得到了广泛应用，因此开发一种适用于印度语言的可解释性框架显得尤为重要。

Method: 该框架通过学习共享的仿射变换，调整每个目标语言的隐藏状态，使之与目标输出分布对齐，从而实现更忠实的模型表示解码。

Result: 研究者在10种印度语言上使用MMLU基准进行了评测，结果表明，Indic-TunedLens方法在复杂性和数据稀疏的语言上显著优于当前最先进的可解释性方法。

Conclusion: 研究揭示了多语言变换器的层次语义编码，对多语言LLMs的理解具有重要启示，并提供了该模型及代码供进一步研究。

Abstract: Multilingual large language models (LLMs) are increasingly deployed in linguistically diverse regions like India, yet most interpretability tools remain tailored to English. Prior work reveals that LLMs often operate in English centric representation spaces, making cross lingual interpretability a pressing concern. We introduce Indic-TunedLens, a novel interpretability framework specifically for Indian languages that learns shared affine transformations. Unlike the standard Logit Lens, which directly decodes intermediate activations, Indic-TunedLens adjusts hidden states for each target language, aligning them with the target output distributions to enable more faithful decoding of model representations. We evaluate our framework on 10 Indian languages using the MMLU benchmark and find that it significantly improves over SOTA interpretability methods, especially for morphologically rich, low resource languages. Our results provide crucial insights into the layer-wise semantic encoding of multilingual transformers. Our model is available at https://huggingface.co/spaces/AnonymousAccountACL/IndicTunedLens. Our code is available at https://github.com/AnonymousAccountACL/IndicTunedLens.

</details>


### [38] [CGRA-DeBERTa Concept Guided Residual Augmentation Transformer for Theologically Islamic Understanding](https://arxiv.org/abs/2602.15139)
*Tahir Hussain,Saddam Hussain Khan*

Main category: cs.CL

TL;DR: 提出了一种新的CGRA DeBERTa框架，用于改进关于可兰经经文的神学问答，该框架在特定领域语义、长上下文依赖性和概念敏感推理方面取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: 准确的问答系统对于经典伊斯兰文献尤为重要，但现有模型在这方面的性能仍有提升空间，因此提出了CGRA DeBERTa框架来解决这些挑战。

Method: CGRA DeBERTa框架基于定制化的DeBERTa转子骨架和轻量级LoRA基适应，引入了残差概念感知门控机制，能够学习全局和位置上下文，并结合12个核心的神学概念先验知识，增强了领域特定的语义表示。

Result: 通过对Sahih alBukhari和Sahih Muslim文本构建的42591个问答对的数据集进行训练，CGRA DeBERTa在精确匹配（EM）得分上达到97.85，相比BERT和DeBERTa分别提高了12.08和8.08个点。

Conclusion: CGRA DeBERTa框架提供了一种高效、可解释、准确的知识获取系统，对于涉及复杂神学概念的问答任务特别有效。

Abstract: Accurate QA over classical Islamic texts remains challenging due to domain specific semantics, long context dependencies, and concept sensitive reasoning. Therefore, a new CGRA DeBERTa, a concept guided residual domain augmentation transformer framework, is proposed that enhances theological QA over Hadith corpora. The CGRA DeBERTa builds on a customized DeBERTa transformer backbone with lightweight LoRA based adaptations and a residual concept aware gating mechanism. The customized DeBERTa embedding block learns global and positional context, while Concept Guided Residual Blocks incorporate theological priors from a curated Islamic Concept Dictionary of 12 core terms. Moreover, the Concept Gating Mechanism selectively amplifies semantically critical tokens via importance weighted attention, applying differential scaling from 1.04 to 3.00. This design preserves contextual integrity, strengthens domain-specific semantic representations, and enables accurate, efficient span extraction while maintaining computational efficiency. This paper reports the results of training CGRA using a specially constructed dataset of 42591 QA pairs from the text of Sahih alBukhari and Sahih Muslim. While BERT achieved an EM score of 75.87 and DeBERTa one of 89.77, our model scored 97.85 and thus surpassed them by 8.08 on an absolute scale, all while adding approximately 8 inference overhead due to parameter efficient gating. The qualitative evaluation noted better extraction and discrimination and theological precision. This study presents Hadith QA systems that are efficient, interpretable, and accurate and that scale provide educational materials with necessary theological nuance.

</details>


### [39] [AIC CTU@AVerImaTeC: dual-retriever RAG for image-text fact checking](https://arxiv.org/abs/2602.15190)
*Herbert Ullrich,Jan Drchal*

Main category: cs.CL

TL;DR: 该论文介绍了其在AVerImaTeC共享任务中排名第三的系统，该系统结合了RAG管道和逆向图像搜索模块，使用GPT5.1仅需0.013美元/次事实检查的多模态LLM调用。


<details>
  <summary>Details</summary>
Motivation: 论文动机在于提出一种结合了检索增强生成（RAG）和逆向图像搜索的简单但高效的系统，适合进一步的实验和研究。

Method: 方法包括文本检索模块基于相似性搜索，图像检索模块基于API访问的逆向图像搜索，以及生成模块使用GPT5.1。

Result: 系统在单一多模态LLM调用下实现了具有竞争力的性能，平均成本仅为0.013美元/次事实检查，易于复现和调整。

Conclusion: 作者建议该系统作为进一步实验的易用起点，并公开了代码、提示、向量存储及其运行成本和改进方向的见解。

Abstract: In this paper, we present our 3rd place system in the AVerImaTeC shared task, which combines our last year's retrieval-augmented generation (RAG) pipeline with a reverse image search (RIS) module. Despite its simplicity, our system delivers competitive performance with a single multimodal LLM call per fact-check at just $0.013 on average using GPT5.1 via OpenAI Batch API. Our system is also easy to reproduce and tweak, consisting of only three decoupled modules - a textual retrieval module based on similarity search, an image retrieval module based on API-accessed RIS, and a generation module using GPT5.1 - which is why we suggest it as an accesible starting point for further experimentation. We publish its code and prompts, as well as our vector stores and insights into the scheme's running costs and directions for further improvement.

</details>


### [40] [OpaqueToolsBench: Learning Nuances of Tool Behavior Through Interaction](https://arxiv.org/abs/2602.15197)
*Skyler Hallinan,Thejas Venkatesh,Xiang Ren,Sai Praneeth Karimireddy,Ashwin Paranjape,Yuhao Zhang,Jack Hessel*

Main category: cs.CL

TL;DR: 本文提出了一种名为ObejctiveToolsBench的新基准，包含三个任务环境，通过观察工具调用轨迹的执行反馈来改善工具文档，从而提升大型语言模型在现实世界中使用不透明工具的能力，实验结果表明该方法在效率和性能上都优于现有的方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在面对不透明工具时表现不佳，需要一种新方法来有效提高模型在这些环境中的作用及效率。

Method: 研究团队开发了一个名为ToolObserver的框架，通过一系列迭代过程观察工具使用的表现来优化工具文档，以便更好地训练模型。

Result: 实验结果表明ToolObserver框架在ObejctiveToolsBench基准测试中表现出色，优于现有方法，并在需要测试时工具探索的部分也更为高效。

Conclusion: 本文探讨了如何通过一种简单的迭代框架改善不透明工具的文档编制，从而提升大型语言模型在复杂任务环境中的表现，并提出了一种新的评估基准。

Abstract: Tool-calling is essential for Large Language Model (LLM) agents to complete real-world tasks. While most existing benchmarks assume simple, perfectly documented tools, real-world tools (e.g., general "search" APIs) are often opaque, lacking clear best practices or failure modes. Can LLM agents improve their performance in environments with opaque tools by interacting and subsequently improving documentation? To study this, we create OpaqueToolsBench, a benchmark consisting of three distinct task-oriented environments: general function calling, interactive chess playing, and long-trajectory agentic search. Each environment provides underspecified tools that models must learn to use effectively to complete the task. Results on OpaqueToolsBench suggest existing methods for automatically documenting tools are expensive and unreliable when tools are opaque. To address this, we propose a simple framework, ToolObserver, that iteratively refines tool documentation by observing execution feedback from tool-calling trajectories. Our approach outperforms existing methods on OpaqueToolsBench across datasets, even in relatively hard settings. Furthermore, for test-time tool exploration settings, our method is also efficient, consuming 3.5-7.5x fewer total tokens than the best baseline.

</details>


### [41] [Extracting Consumer Insight from Text: A Large Language Model Approach to Emotion and Evaluation Measurement](https://arxiv.org/abs/2602.15312)
*Stephan Ludwig,Peter J. Danaher,Xiaohao Yang,Yu-Ting Lin,Ehsan Abedin,Dhruv Grewal,Lan Du*

Main category: cs.CL

TL;DR: 本文介绍了一种名为LX的语言提取器，它是一种大型调优语言模型，能够从消费者的非结构化文本中准确地测量情感和评价，且性能优于包括GPT-4 Turbo、RoBERTa和DeepSeek在内的领先模型。LX已被应用到实际数据中，且验证了情感表达对产品评价及购买行为的影响。


<details>
  <summary>Details</summary>
Motivation: 消费者情感和评价的准确测量对于市场营销研究和实践至关重要，但如何从非结构化文本中获取这些信息一直是一个挑战。本文通过建立LX，旨在提供一种有效的方法来解决这一问题。

Method: LX是基于具有消费者自我报告情绪和评价标签的消费者撰写的文本进行调优的大型语言模型。它采用似乎无关回归技术应用于在线零售数据，以验证情感表达对产品评价及购买行为的影响。

Result: LX在开放问题回答和第三方标注的Amazon和Yelp评论中的准确性分别达到了81%和95%以上。LX的应用进一步证明了情感表达能够预测产品评价，并进一步反映购买行为，显示出情感色调提供了比星级评价更具意义的信号。

Conclusion: 本研究为消费者感知测量建立了新的方法论基础，并展示了大型语言模型如何推动市场营销研究和实践的进步，从而从消费者数据中实现对市场构念的有效检测。

Abstract: Accurately measuring consumer emotions and evaluations from unstructured text remains a core challenge for marketing research and practice. This study introduces the Linguistic eXtractor (LX), a fine-tuned, large language model trained on consumer-authored text that also has been labeled with consumers' self-reported ratings of 16 consumption-related emotions and four evaluation constructs: trust, commitment, recommendation, and sentiment. LX consistently outperforms leading models, including GPT-4 Turbo, RoBERTa, and DeepSeek, achieving 81% macro-F1 accuracy on open-ended survey responses and greater than 95% accuracy on third-party-annotated Amazon and Yelp reviews. An application of LX to online retail data, using seemingly unrelated regression, affirms that review-expressed emotions predict product ratings, which in turn predict purchase behavior. Most emotional effects are mediated by product ratings, though some emotions, such as discontent and peacefulness, influence purchase directly, indicating that emotional tone provides meaningful signals beyond star ratings. To support its use, a no-code, cost-free, LX web application is available, enabling scalable analyses of consumer-authored text. In establishing a new methodological foundation for consumer perception measurement, this research demonstrates new methods for leveraging large language models to advance marketing research and practice, thereby achieving validated detection of marketing constructs from consumer data.

</details>


### [42] [Mnemis: Dual-Route Retrieval on Hierarchical Graphs for Long-Term LLM Memory](https://arxiv.org/abs/2602.15313)
*Zihao Tang,Xin Yu,Ziyu Xiao,Zengxuan Wen,Zelin Li,Jiaxi Zhou,Hualei Wang,Haohua Wang,Haizhen Huang,Weiwei Deng,Feng Sun,Qi Zhang*

Main category: cs.CL

TL;DR: 该研究提出了一种名为 Mnemis 的新型记忆框架，结合了基于相似性的 System-1 机制和全局选择的 System-2 机制，通过构建基底图进行相似性检索和层次图实现顶层详尽遍历，从而实现语义和结构上的相关性检索，在 LoCoMo 和 LongMemEval-S 上获得最佳性能。


<details>
  <summary>Details</summary>
Motivation: 当前的基于相似性的记忆检索方法（如 RAG 和 Graph-RAG）在需要全局推理或涵盖所有相关信息的场景中表现不佳，因此提出了 Mnemis 记忆框架来改进这一问题。

Method: Mnemis 通过构建基底图进行相似性检索和层次图实现顶层详尽遍历，结合两种机制的优势，以获取语义和结构相关性的记忆项。

Result: 在长期记忆基准测试中，Mnemis 获得了最佳性能，GPT-4.1-mini 模型在 LoCoMo 上得分为 93.9，在 LongMemEval-S 上得分为 91.6。

Conclusion: Mnemis 记忆框架通过结合基于相似性和全局选择的机制，能够在需要全局推理或全面覆盖相关信息的场景中提供有效且准确的记忆检索。

Abstract: AI Memory, specifically how models organizes and retrieves historical messages, becomes increasingly valuable to Large Language Models (LLMs), yet existing methods (RAG and Graph-RAG) primarily retrieve memory through similarity-based mechanisms. While efficient, such System-1-style retrieval struggles with scenarios that require global reasoning or comprehensive coverage of all relevant information. In this work, We propose Mnemis, a novel memory framework that integrates System-1 similarity search with a complementary System-2 mechanism, termed Global Selection. Mnemis organizes memory into a base graph for similarity retrieval and a hierarchical graph that enables top-down, deliberate traversal over semantic hierarchies. By combining the complementary strength from both retrieval routes, Mnemis retrieves memory items that are both semantically and structurally relevant. Mnemis achieves state-of-the-art performance across all compared methods on long-term memory benchmarks, scoring 93.9 on LoCoMo and 91.6 on LongMemEval-S using GPT-4.1-mini.

</details>


### [43] [NeuroSymActive: Differentiable Neural-Symbolic Reasoning with Active Exploration for Knowledge Graph Question Answering](https://arxiv.org/abs/2602.15353)
*Rong Fu,Yang Li,Zeyu Zhang,Jiekai Wu,Yaohua Liu,Shuaishuai Cao,Yangchen Zeng,Yuhang Zhang,Xiaojing Du,Chuang Zhao,Kangning Cui,Simon Fong*

Main category: cs.CL

TL;DR: NeuroSymActive 是一个结合了可微神经-符号推理层和价值导向的主动探索控制器的模块化框架，用于知识图谱问答。该方法通过符号模块和神经路径评估器的结合以及基于蒙特卡洛的探索策略，提高了答案的准确性并减少了昂贵的图查找和模型调用次数。


<details>
  <summary>Details</summary>
Motivation: 当前的大规模预训练语言模型和神经推理系统在处理需要精确、结构化的多跳推理的知识密集型查询方面仍然存在挑战。知识图谱虽然提供了紧凑的符号基础，但将其结构集成到神经模型中具有挑战性。

Method: NeuroSymActive 通过结合可微神经-符号推理层和价值导向的主动探索控制器来解决知识图谱问答问题。这种方法包括软统一风格的符号模块、神经路径评估器和基于蒙特卡洛的探索策略。

Result: 在标准知识图谱问答基准上的实验结果表明，NeuroSymActive 的答案准确度较高，并且减少了昂贵的图查找和模型调用次数，与常见的检索增强基线相比。

Conclusion: NeuroSymActive 提供了一种有效的方法来解决知识图谱问答任务中的复杂查询，通过优化模型的实际应用性能和资源利用效率。

Abstract: Large pretrained language models and neural reasoning systems have advanced many natural language tasks, yet they remain challenged by knowledge-intensive queries that require precise, structured multi-hop inference. Knowledge graphs provide a compact symbolic substrate for factual grounding, but integrating graph structure with neural models is nontrivial: naively embedding graph facts into prompts leads to inefficiency and fragility, while purely symbolic or search-heavy approaches can be costly in retrievals and lack gradient-based refinement. We introduce NeuroSymActive, a modular framework that combines a differentiable neural-symbolic reasoning layer with an active, value-guided exploration controller for Knowledge Graph Question Answering. The method couples soft-unification style symbolic modules with a neural path evaluator and a Monte-Carlo style exploration policy that prioritizes high-value path expansions. Empirical results on standard KGQA benchmarks show that NeuroSymActive attains strong answer accuracy while reducing the number of expensive graph lookups and model calls compared to common retrieval-augmented baselines.

</details>


### [44] [Making Large Language Models Speak Tulu: Structured Prompting for an Extremely Low-Resource Language](https://arxiv.org/abs/2602.15378)
*Prathamesh Devadiga,Paras Chopra*

Main category: cs.CL

TL;DR: 通过系统地应对Tulu语言训练数据不足带来的挑战，该研究使用显式的语法文档、负约束、罗马化标准化以及自播放合成高质量数据生成，使得一个大型语言模型能够生成具有基本对话能力的Tulu文本，词汇污染从80%减少到5%，语法正确率达到85%。


<details>
  <summary>Details</summary>
Motivation: 为了探索大型语言模型能否就几乎未出现在其训练数据中的语言进行对话，特别是针对如Tulu这样的低资源语言，研究者们提出了一个案例研究，并尝试通过结构化提示来研究。

Method: 研究者们采用的方法包括显式的语法文档、使用负约束抑制相关语言的高概率词汇、罗马化标准化以及通过自播放生成高质量的合成数据。他们针对Tulu语言，结合这些策略应对数据不足的挑战。

Result: 研究结果显示，使用这种方法可以使模型减少85%的词汇污染，同时实现85%的语法正确率。负约束提供了稳定改进，而语法文档的效果则因模型架构的不同而有所变化。

Conclusion: 通过结构化提示和负约束等方法，该研究展示了大型语言模型能够在几乎没有训练数据的情况下，对低资源语言（如Tulu）展示基本的对话能力，这对于增强语言模型的多语言处理能力具有重要意义。

Abstract: Can large language models converse in languages virtually absent from their training data? We investigate this question through a case study on Tulu, a Dravidian language with over 2 million speakers but minimal digital presence. Rather than fine-tuning an LLM, we examine whether structured prompts alone can elicit basic conversational ability under controlled prompting. We systematically tackle various challenges posed by absence of training data for Tulu by combining explicit grammar documentation, negative constraints to suppress high-probability tokens from related languages, romanization standardization, and quality-controlled synthetic data generation via self-play. Evaluated on a manually curated held-out set across three LLMs (Gemini 2.0 Flash, GPT-4o, Llama 3.1 70B) and validated by native speakers, our approach reduces vocabulary contamination from 80% to 5% while achieving 85% grammatical accuracy. Cross-model analysis reveals that negative constraints provide consistent improvements (12--18 percentage points), while grammar documentation effects vary by model architecture (8--22 points).

</details>


### [45] [The Vision Wormhole: Latent-Space Communication in Heterogeneous Multi-Agent Systems](https://arxiv.org/abs/2602.15382)
*Xiaoze Liu,Ruowang Zhang,Weichen Yu,Siheng Xiong,Liu He,Feijie Wu,Hoin Jung,Matt Fredrikson,Xiaoqian Wang,Jing Gao*

Main category: cs.CL

TL;DR: 提出了一种名为'视觉虫洞'的新框架，利用视觉语言模型实现模型无关、无需文本的通信。该框架通过引入一个通用视觉编解码器，将异构推理轨迹映射到共享的连续潜在空间，并直接注入接收者的视觉通道，以此减少模型间通信的复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有的多智能体系统依赖文本通信，存在低效和信息损失问题。现有的替代方案如潜在状态传输尽管高效但也存在局限性。

Method: 该方法通过引入通用视觉编解码器，将异构推理轨迹映射到共享的连续潜在空间，使得接收智能体可以直接通过视觉通道接收信息。采用星型拓扑减少通信复杂度，并利用无标签的教师-学生蒸馏目标来对齐视觉通道和文本推理路径的模式。

Result: 在不同模型家族中（如Qwen-VL、Gemma）进行的实验表明，视觉虫洞减少了基于文本的多智能体系统的端到端时间，同时保持了相似的推理准确度。

Conclusion: 视觉虫洞框架为多智能体系统中的高效通信提供了一种新的解决方案，具有广泛的应用前景。

Abstract: Multi-Agent Systems (MAS) powered by Large Language Models have unlocked advanced collaborative reasoning, yet they remain shackled by the inefficiency of discrete text communication, which imposes significant runtime overhead and information quantization loss. While latent state transfer offers a high-bandwidth alternative, existing approaches either assume homogeneous sender-receiver architectures or rely on pair-specific learned translators, limiting scalability and modularity across diverse model families with disjoint manifolds. In this work, we propose the Vision Wormhole, a novel framework that repurposes the visual interface of Vision-Language Models (VLMs) to enable model-agnostic, text-free communication. By introducing a Universal Visual Codec, we map heterogeneous reasoning traces into a shared continuous latent space and inject them directly into the receiver's visual pathway, effectively treating the vision encoder as a universal port for inter-agent telepathy. Our framework adopts a hub-and-spoke topology to reduce pairwise alignment complexity from O(N^2) to O(N) and leverages a label-free, teacher-student distillation objective to align the high-speed visual channel with the robust reasoning patterns of the text pathway. Extensive experiments across heterogeneous model families (e.g., Qwen-VL, Gemma) demonstrate that the Vision Wormhole reduces end-to-end wall-clock time in controlled comparisons while maintaining reasoning fidelity comparable to standard text-based MAS. Code is available at https://github.com/xz-liu/heterogeneous-latent-mas

</details>


### [46] [Measuring Social Integration Through Participation: Categorizing Organizations and Leisure Activities in the Displaced Karelians Interview Archive using LLMs](https://arxiv.org/abs/2602.15436)
*Joonatan Laato,Veera Schroderus,Jenna Kanerva,Jenni Kauppi,Virpi Lummaa,Filip Ginter*

Main category: cs.CL

TL;DR: 研究通过开发分类框架，将历史档案中的大量活动和组织分类，使大规模数据分析成为可能。


<details>
  <summary>Details</summary>
Motivation: 数字化历史档案为大规模研究日常生活提供了可能，但从中提取的信息往往不直接有助于回答历史学家或社会学家的研究问题。因此，本文使用芬兰第二次世界大战期间卡累利阿流离失所家庭访谈集来解决这个问题。

Method: 本文开发了一个分类框架，包括参与活动的类型、社交程度、频率和体力强度等关键方面。通过大规模语言模型应用该框架，实现大规模自动分类。

Result: 使用简单的投票方法，大型语言模型能够在大规模下接近专家判断，成功分类出350,000个实体。

Conclusion: 本文提供了一个结构化的资源，有助于后续研究社会整合等相关结果。

Abstract: Digitized historical archives make it possible to study everyday social life on a large scale, but the information extracted directly from text often does not directly allow one to answer the research questions posed by historians or sociologists in a quantitative manner. We address this problem in a large collection of Finnish World War II Karelian evacuee family interviews. Prior work extracted more than 350K mentions of leisure time activities and organizational memberships from these interviews, yielding 71K unique activity and organization names -- far too many to analyze directly.
  We develop a categorization framework that captures key aspects of participation (the kind of activity/organization, how social it typically is, how regularly it happens, and how physically demanding it is). We annotate a gold-standard set to allow for a reliable evaluation, and then test whether large language models can apply the same schema at scale. Using a simple voting approach across multiple model runs, we find that an open-weight LLM can closely match expert judgments. Finally, we apply the method to label the 350K entities, producing a structured resource for downstream studies of social integration and related outcomes.

</details>


### [47] [TAROT: Test-driven and Capability-adaptive Curriculum Reinforcement Fine-tuning for Code Generation with Large Language Models](https://arxiv.org/abs/2602.15449)
*Chansung Park,Juyong Jiang,Fan Wang,Sayak Paul,Jiasi Shen,Jing Tang,Jianguo Li*

Main category: cs.CL

TL;DR: TAROT 提出了一种新的 Reinforcement Fine-Tuning 方法，通过为每个问题构建四层测试套件，提供了一个受控难度的教学梯度，同时解耦了课程进度与原始奖励评分，从而促进稳定优化并更高效地获取技能。


<details>
  <summary>Details</summary>
Motivation: 现有的强化微调方法忽略了测试用例中固有的异构难度和粒度，导致奖励信号分布不平衡，影响优化效果。

Method: TAROT 建议构建一个四层的测试套件（基础、中级、复杂、边缘），解耦课程进度与原奖励评分，并根据模型的能力选择合适的教学策略。

Result: 实验证明，最优课程对于代码生成中的 RFT 至关重要，对于能力较弱的模型，采用简单到复杂的课程策略可以实现更大的改进，而能力较强的模型则受益于先难后易的课程策略。TAROT 使得课程设计能够适应模型的能力，从而提高生成代码的功能正确性和鲁棒性。

Conclusion: TAROT 提出了一种新的课程强化微调方法，通过自适应地调整课程设计与模型能力相匹配，提高了代码生成的质量。

Abstract: Large Language Models (LLMs) are changing the coding paradigm, known as vibe coding, yet synthesizing algorithmically sophisticated and robust code still remains a critical challenge. Incentivizing the deep reasoning capabilities of LLMs is essential to overcoming this hurdle. Reinforcement Fine-Tuning (RFT) has emerged as a promising strategy to address this need. However, most existing approaches overlook the heterogeneous difficulty and granularity inherent in test cases, leading to an imbalanced distribution of reward signals and consequently biased gradient updates during training. To address this, we propose Test-driven and cApability-adaptive cuRriculum reinfOrcement fine-Tuning (TAROT). TAROT systematically constructs, for each problem, a four-tier test suite (basic, intermediate, complex, edge), providing a controlled difficulty landscape for curriculum design and evaluation. Crucially, TAROT decouples curriculum progression from raw reward scores, enabling capability-conditioned evaluation and principled selection from a portfolio of curriculum policies rather than incidental test-case difficulty composition. This design fosters stable optimization and more efficient competency acquisition. Extensive experimental results reveal that the optimal curriculum for RFT in code generation is closely tied to a model's inherent capability, with less capable models achieving greater gains with an easy-to-hard progression, whereas more competent models excel under a hard-first curriculum. TAROT provides a reproducible method that adaptively tailors curriculum design to a model's capability, thereby consistently improving the functional correctness and robustness of the generated code. All code and data are released to foster reproducibility and advance community research at https://github.com/deep-diver/TAROT.

</details>


### [48] [Towards Expectation Detection in Language: A Case Study on Treatment Expectations in Reddit](https://arxiv.org/abs/2602.15504)
*Aswathy Velutharambath,Amelie Wührl*

Main category: cs.CL

TL;DR: 本研究通过分析红迪平台上与治疗期望相关的4500篇帖子，探讨了患者治疗期望的语言模式及其特点。研究发现，对身体或治疗相关疾病的乐观态度和主动表达比心理健康问题中更常见，且患者主要讨论了治疗的好处。


<details>
  <summary>Details</summary>
Motivation: 在临床和在线患者平台上，患者对治疗的期望对其治疗效果有重要影响。然而，这些期望通常很难在传统临床研究中获取，特别是在在线社区中，可能会有患者不愿意分享有关他们期望的真实信息。因此，本研究旨在通过引入期望检测任务，研究患者在线分享的期望类型及其表达方式。

Method: 本研究构建了一个包含4500个帖子的语料库RedHOTExpect，并利用大型语言模型进行银色标注，之后手动验证数据质量（标签准确性约为78%）。基于该语料库，研究分析了描述期望的语言特征，并探讨了患者对不同类型疾病的看法。

Result: 研究发现，对于身体或治疗相关的疾病，患者表达的期望更多体现为乐观和积极的态度；而在心理健康问题的讨论中，患者的期望表达较为消极。此外，患者在讨论情感健康状况时，更倾向于谈论负面的结果，而在讨论身体或治疗相关疾病时，患者更倾向于谈论治疗的好处。

Conclusion: 本研究通过对红迪平台上大量患者对医疗治疗的期望语料库进行分析，揭示了不同疾病背景下患者的期望特征。研究结果有望为医疗领域的意见挖掘及产品设计提供重要参考借鉴。

Abstract: Patients' expectations towards their treatment have a substantial effect on the treatments' success. While primarily studied in clinical settings, online patient platforms like medical subreddits may hold complementary insights: treatment expectations that patients feel unnecessary or uncomfortable to share elsewhere. Despite this, no studies examine what type of expectations users discuss online and how they express them. Presumably this is because expectations have not been studied in natural language processing (NLP) before. Therefore, we introduce the task of Expectation Detection, arguing that expectations are relevant for many applications, including opinion mining and product design. Subsequently, we present a case study for the medical domain, where expectations are particularly crucial to extract. We contribute RedHOTExpect, a corpus of Reddit posts (4.5K posts) to study expectations in this context. We use a large language model (LLM) to silver-label the data and validate its quality manually (label accuracy ~78%). Based on this, we analyze which linguistic patterns characterize expectations and explore what patients expect and why. We find that optimism and proactive framing are more pronounced in posts about physical or treatment-related illnesses compared to mental-health contexts, and that in our dataset, patients mostly discuss benefits rather than negative outcomes. The RedHOTExpect corpus can be obtained from https://www.ims.uni-stuttgart.de/data/RedHOTExpect

</details>


### [49] [LuxMT Technical Report](https://arxiv.org/abs/2602.15506)
*Nils Rehlinger*

Main category: cs.CL

TL;DR: LuxMT 是一个基于 Gemma 3 27B 微调的机器翻译系统，主要用于将卢森堡语翻译成法语和英语。该系统通过滤除低等价度的翻译对并使用人类翻译的数据来提高翻译质量。实验结果显示 LuxMT 相较于基线有显著改进，甚至在未包含德语数据的训练下也能良好地将卢森堡语翻译成德语。


<details>
  <summary>Details</summary>
Motivation: 由于卢森堡语数据稀缺，研究人员需要一个有效的系统来提高其翻译质量，特别是针对到法语和英语的翻译。

Method: LuxMT 系统利用了卢森堡语和法语/英语的并行语料库进行微调，并且通过使用 LuxEmbedder 过滤掉低效度的语料库，最终实现翻译质量的提升。

Result: LuxMT 在卢森堡语到其他语言（尤其是德语）的翻译任务中表现出了强大的改进，超过了基线 Gemma 3。同时，LuxEmbedder 作为质量评估指标与参考标准高度相关。

Conclusion: 尽管 LuxEmbedder 在多项指标上表现出一致性，但研究人员建议谨慎使用，并需要进一步的研究来评估其实际应用价值。

Abstract: We introduce LuxMT, a machine translation system based on Gemma 3 27B and fine-tuned for translation from Luxembourgish (LB) into French (FR) and English (EN). To assess translation performance, we construct a novel benchmark covering LB-FR, LB-EN, and LB-FR using human-translated data from Luci, a tourist magazine about Luxembourg. Training data stems from LuxAlign, a parallel corpus of multilingual Luxembourgish news articles, and LB parliamentary transcripts augmented with Google Translate. We filter the data using LuxEmbedder, LB sentence embeddings, to remove low-equivalence segment-pairs. Overall, LuxMT's results suggest strong improvements over the Gemma 3 baseline, even for translating LB to German (DE), despite the training data not containing any DE. We also explore LuxEmbedder's potential to be used as a quality estimation metric and find strong correlations with other reference-based metrics. However, we call for further research to fully assess the metric's utility and advise using it with caution.

</details>


### [50] [Fine-Refine: Iterative Fine-grained Refinement for Mitigating Dialogue Hallucination](https://arxiv.org/abs/2602.15509)
*Xiangyan Chen,Yujian Gan,Matthew Purver*

Main category: cs.CL

TL;DR: Fine-Refine 提出了一种细粒度的对话系统修正框架，将响应分解为原子单元进行逐一验证和修正，最终在提高事实准确性的同时只对对话质量造成轻微影响。


<details>
  <summary>Details</summary>
Motivation: 目前的大型语言模型（LLMs）倾向于产生幻觉，这在对话系统中产生了负面的影响。幻觉会导致事实错误的回答，可能误导用户并削弱系统的信任度。

Method: Fine-Refine 框架通过将响应分解为原子单元，使用外部知识验证每个单元，评估流畅性并在迭代中纠正细粒度错误来修正幻觉。

Result: Fine-Refine 在 HybriDialogue 和 OpendialKG 数据集上评估事实准确性，显示该方法可以极大地提高事实准确性，平均实现 7.63 点的对话事实分数增长。

Conclusion: Fine-Refine 通过细粒度的验证和修正提高了对话系统的事实准确性，是一个有效的方法来改进大型语言模型的对话系统。

Abstract: The tendency for hallucination in current large language models (LLMs) negatively impacts dialogue systems. Such hallucinations produce factually incorrect responses that may mislead users and undermine system trust. Existing refinement methods for dialogue systems typically operate at the response level, overlooking the fact that a single response may contain multiple verifiable or unverifiable facts. To address this gap, we propose Fine-Refine, a fine-grained refinement framework that decomposes responses into atomic units, verifies each unit using external knowledge, assesses fluency via perplexity, and iteratively corrects granular errors. We evaluate factuality across the HybriDialogue and OpendialKG datasets in terms of factual accuracy (fact score) and coverage (Not Enough Information Proportion), and experiments show that Fine-Refine substantially improves factuality, achieving up to a 7.63-point gain in dialogue fact score, with a small trade-off in dialogue quality.

</details>


### [51] [ExpertWeaver: Unlocking the Inherent MoE in Dense LLMs with GLU Activation Patterns](https://arxiv.org/abs/2602.15521)
*Ziyu Zhao,Tong Zhu,Zhi Zhang,Tiantian Fan,Jinluan Yang,Kun Kuang,Zhongyu Wei,Fei Wu,Yu Cheng*

Main category: cs.CL

TL;DR: 本研究提出了一种名为ExpertWeaver的无训练框架，利用Gated Linear Unit (GLU)机制细粒度的神经激活模式揭示的粗粒度结构，成功地将稠密模型转换为专家模型。这种方法在动态结构剪裁和高密度模型优化初始化方面显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法在将稠密模型转换为专家模型时破坏了稠密模型固有的激活模式，导致专家构建不理想。因此，本研究旨在提出一种新的框架，利用GLU机制中的细粒度神经激活模式来揭示粗粒度结构，从而优化专家模型的构建。

Method: 提出了一种名为ExpertWeaver的无训练框架，该框架利用GLU机制揭示的结构将神经元根据其激活模式进行分区，构建共用专家和动态激活的专门路由专家，并采用了分层自适应配置。

Result: 实验证明，ExpertWeaver框架在动态结构剪裁和优化初始专家模型性能方面显著优于现有方法。

Conclusion: 提出的ExpertWeaver框架通过利用GLU机制中细粒度神经激活模式揭示的粗粒度结构，提供了一种有效的无训练方法来将稠密模型转换为专家模型。

Abstract: Mixture-of-Experts (MoE) effectively scales model capacity while preserving computational efficiency through sparse expert activation. However, training high-quality MoEs from scratch is prohibitively expensive. A promising alternative is to convert pretrained dense models into sparse MoEs. Existing dense-to-MoE methods fall into two categories: \textbf{dynamic structural pruning} that converts dense models into MoE architectures with moderate sparsity to balance performance and inference efficiency, and \textbf{downcycling} approaches that use pretrained dense models to initialize highly sparse MoE architectures. However, existing methods break the intrinsic activation patterns within dense models, leading to suboptimal expert construction. In this work, we argue that the Gated Linear Unit (GLU) mechanism provides a natural blueprint for dense-to-MoE conversion. We show that the fine-grained neural-wise activation patterns of GLU reveal a coarse-grained structure, uncovering an inherent MoE architecture composed of consistently activated universal neurons and dynamically activated specialized neurons. Leveraging this discovery, we introduce ExpertWeaver, a training-free framework that partitions neurons according to their activation patterns and constructs shared experts and specialized routed experts with layer-adaptive configurations. Our experiments demonstrate that ExpertWeaver significantly outperforms existing methods, both as a training-free dynamic structural pruning technique and as a downcycling strategy for superior MoE initialization.

</details>


### [52] [ZeroSyl: Simple Zero-Resource Syllable Tokenization for Spoken Language Modeling](https://arxiv.org/abs/2602.15537)
*Nicol Visser,Simon Malan,Danel Slabbert,Herman Kamper*

Main category: cs.CL

TL;DR: ZeroSyl 提出了一种无需训练的方法，从冻结的 WavLM 模型中直接提取音节边界和嵌入，利用 WavLM 中间层的特征 L2 范数，实现了与现有音节分词器相当的表现。


<details>
  <summary>Details</summary>
Motivation: 传统纯语音语言模型面临自监督语音编码器产生的离散令牌导致序列过长的问题，寻求一种简单且无需复杂训练过程的音节化方法以克服这一挑战。

Method: ZeroSyl 通过计算 WavLM 中间层特征的 L2 范数来识别音节边界，并使用均值池化和 K-means 聚类进行离散化处理，进而训练语言模型。

Result: ZeroSyl 在音节分词方面的表现与现有的 Sylber 和 SyllableLM 等音节化技术相当，并且在词汇学、语法和叙事性多种基准测试中表现出色。

Conclusion: 研究表明，尽管更细粒度的音节化单元对词汇任务有益，但 ZeroSyl 发现的音节化单元在句法建模方面的扩展性能更好。

Abstract: Pure speech language models aim to learn language directly from raw audio without textual resources. A key challenge is that discrete tokens from self-supervised speech encoders result in excessively long sequences, motivating recent work on syllable-like units. However, methods like Sylber and SyllableLM rely on intricate multi-stage training pipelines. We propose ZeroSyl, a simple training-free method to extract syllable boundaries and embeddings directly from a frozen WavLM model. Using L2 norms of features in WavLM's intermediate layers, ZeroSyl achieves competitive syllable segmentation performance. The resulting segments are mean-pooled, discretized using K-means, and used to train a language model. ZeroSyl outperforms prior syllabic tokenizers across lexical, syntactic, and narrative benchmarks. Scaling experiments show that while finer-grained units are beneficial for lexical tasks, our discovered syllabic units exhibit better scaling behavior for syntactic modeling.

</details>


### [53] [Perspectives - Interactive Document Clustering in the Discourse Analysis Tool Suite](https://arxiv.org/abs/2602.15540)
*Tim Fischer,Chris Biemann*

Main category: cs.CL

TL;DR: 本研究介绍了一种名为Perspectives的交互式扩展工具，旨在帮助数字人文学者探索和组织大规模的非结构化文档集合。通过定义分析镜头并进行人类辅助调整，研究人员可以利用Perspectives发现主题、情感或其他相关类别，从而更好地准备数据进行深入分析。


<details>
  <summary>Details</summary>
Motivation: 随着数字人文领域的研究数据量不断增加，传统的文本分析工具难以有效处理大规模非结构化文档集合。因此，本研究旨在开发一种新的交互式工具，使学者可以更加灵活和精确地分析数据。

Method: 该研究通过设计一个灵活的主题为导向的文档聚类管道，并结合人类辅助调整机制，使用户能够通过文档重写提示和基于指令的嵌入实现对分析过程的控制。

Result: 研究结果表明，通过使用Perspectives工具，研究人员可以有效地探索大型文集，并通过交互式文档地图发现各种主题、情感或相关类别。

Conclusion: 总的来说，本研究提供了一种新的工具来增强数字人文领域的文本分析能力，允许研究人员更加自由和精确地探索和组织数据，为后续的深入研究奠定了基础。

Abstract: This paper introduces Perspectives, an interactive extension of the Discourse Analysis Tool Suite designed to empower Digital Humanities (DH) scholars to explore and organize large, unstructured document collections. Perspectives implements a flexible, aspect-focused document clustering pipeline with human-in-the-loop refinement capabilities. We showcase how this process can be initially steered by defining analytical lenses through document rewriting prompts and instruction-based embeddings, and further aligned with user intent through tools for refining clusters and mechanisms for fine-tuning the embedding model. The demonstration highlights a typical workflow, illustrating how DH researchers can leverage Perspectives's interactive document map to uncover topics, sentiments, or other relevant categories, thereby gaining insights and preparing their data for subsequent in-depth analysis.

</details>


### [54] [jina-embeddings-v5-text: Task-Targeted Embedding Distillation](https://arxiv.org/abs/2602.15547)
*Mohammad Kalim Akram,Saba Sturua,Nastia Havriushenko,Quentin Herreros,Michael Günther,Maximilian Werk,Han Xiao*

Main category: cs.CL

TL;DR: 该研究提出了一种新的训练方法，结合了模型蒸馏技术和任务特定的对比损失，用于训练小型高性能的文本嵌入模型，这些模型在基准测试中表现出色，特别是在大小相似的模型中。


<details>
  <summary>Details</summary>
Motivation: 为了提高小型文本嵌入模型的性能，同时保持模型的紧凑性和在不同应用场景中的有效性，尤其是针对信息检索、聚类和分类等任务。

Method: 研究提出了一种结合了模型蒸馏技术和任务特定对比损失的训练方案，这种方案在训练小型模型时更加有效。

Result: 该方法训练得到的模型相比仅有对比损失或仅蒸馏训练的模型，在基准测试中表现出更高的性能，并且支持多种语言的长文本输入，并且生成的嵌入对剪裁和二进制量化具有鲁棒性。

Conclusion: 基于上述研究，建立了两个新的文本嵌入模型jina-embeddings-v5-text-small和jina-embeddings-v5-text-nano，它们的性能超过了或达到了相似尺寸模型的最先进水平，未来有望进一步推动嵌入模型的发展。

Abstract: Text embedding models are widely used for semantic similarity tasks, including information retrieval, clustering, and classification. General-purpose models are typically trained with single- or multi-stage processes using contrastive loss functions. We introduce a novel training regimen that combines model distillation techniques with task-specific contrastive loss to produce compact, high-performance embedding models. Our findings suggest that this approach is more effective for training small models than purely contrastive or distillation-based training paradigms alone. Benchmark scores for the resulting models, jina-embeddings-v5-text-small and jina-embeddings-v5-text-nano, exceed or match the state-of-the-art for models of similar size. jina-embeddings-v5-text models additionally support long texts (up to 32k tokens) in many languages, and generate embeddings that remain robust under truncation and binary quantization. Model weights are publicly available, hopefully inspiring further advances in embedding model development.

</details>


### [55] [Beyond Static Pipelines: Learning Dynamic Workflows for Text-to-SQL](https://arxiv.org/abs/2602.15564)
*Yihan Wang,Peiyu Liu,Runyu Chen,Wei Xu*

Main category: cs.CL

TL;DR: 该研究提出了一种名为SquRL的强化学习框架，旨在提高LLM在适应性工作流构建中的推理能力。实验表明，动态工作流构建在各种Text-to-SQL基准测试中表现优于现有的静态工作流方法，特别是在复杂和不常见的查询上。


<details>
  <summary>Details</summary>
Motivation: 现有Text-to-SQL方法依赖于单一静态工作流，这在实际应用场景中存在局限性。SquRL通过强化学习框架动态构建工作流，以应对不同查询的异质性，旨在提升系统的适应性和性能。

Method: SquRL使用一种基于规则的奖励函数，并引入了动态演员蒙版和伪奖励两种有效的训练机制。这种方法允许系统在执行过程中根据查询特异性动态调整工作流，从而提高任务完成的灵活性和准确性。

Result: 在流行的Text-to-SQL基准测试中，SquRL动态工作流构建方法的性能显著优于最佳静态工作流方法。尤其在复杂或非分布查询中，结果显示了明显的优势。

Conclusion: 研究证明，基于强化学习的动态工作流构建方法在Text-to-SQL任务中具有更高的适应性和更好的性能，为现实世界的应用提供了新的可能。

Abstract: Text-to-SQL has recently achieved impressive progress, yet remains difficult to apply effectively in real-world scenarios. This gap stems from the reliance on single static workflows, fundamentally limiting scalability to out-of-distribution and long-tail scenarios. Instead of requiring users to select suitable methods through extensive experimentation, we attempt to enable systems to adaptively construct workflows at inference time. Through theoretical and empirical analysis, we demonstrate that optimal dynamic policies consistently outperform the best static workflow, with performance gains fundamentally driven by heterogeneity across candidate workflows. Motivated by this, we propose SquRL, a reinforcement learning framework that enhances LLMs' reasoning capability in adaptive workflow construction. We design a rule-based reward function and introduce two effective training mechanisms: dynamic actor masking to encourage broader exploration, and pseudo rewards to improve training efficiency. Experiments on widely-used Text-to-SQL benchmarks demonstrate that dynamic workflow construction consistently outperforms the best static workflow methods, with especially pronounced gains on complex and out-of-distribution queries. The codes are available at https://github.com/Satissss/SquRL

</details>


### [56] [Clinically Inspired Symptom-Guided Depression Detection from Emotion-Aware Speech Representations](https://arxiv.org/abs/2602.15578)
*Chaithra Nerella,Chiranjeevi Yarra*

Main category: cs.CL

TL;DR: 该研究提出了一种针对抑郁症状的特定方法，通过症状导向的交叉注意力机制从言语中估计抑郁严重程度，改进了当前的表现并强调了症状导向和情绪感知建模的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有研究大多将抑郁症预测视为二元标签或整体严重程度评分，缺乏对症状特定信息的显式建模，这限制了它们在临床筛查方面的症状水平分析能力。

Method: 该方法采用症状导向的交叉注意力机制，将PHQ-8问卷项目与情绪感知的语音表示对齐，以确定参与者言语中与每种症状相关的重要片段。引入可学习的症状特定参数，以自适应地控制注意力分布的尖锐程度。

Result: 在EDAIC标准临床风格数据集上，研究结果显示出优于先前工作的表现，进一步分析注意力分布显示，更高的注意力分配给包含多种抑郁症状线索的陈述。

Conclusion: 此项研究强调了症状导向和情绪感知建模在言语基线抑郁筛查中的重要性，并展示了该框架的有效性。

Abstract: Depression manifests through a diverse set of symptoms such as sleep disturbance, loss of interest, and concentration difficulties. However, most existing works treat depression prediction either as a binary label or an overall severity score without explicitly modeling symptom-specific information. This limits their ability to provide symptom-level analysis relevant to clinical screening. To address this, we propose a symptom-specific and clinically inspired framework for depression severity estimation from speech. Our approach uses a symptom-guided cross-attention mechanism that aligns PHQ-8 questionnaire items with emotion-aware speech representations to identify which segments of a participant's speech are more important to each symptom. To account for differences in how symptoms are expressed over time, we introduce a learnable symptom-specific parameter that adaptively controls the sharpness of attention distributions. Our results on EDAIC, a standard clinical-style dataset, demonstrate improved performance outperforming prior works. Further, analyzing the attention distributions showed that higher attention is assigned to utterances containing cues related to multiple depressive symptoms, highlighting the interpretability of our approach. These findings outline the importance of symptom-guided and emotion-aware modeling for speech-based depression screening.

</details>


### [57] [STAPO: Stabilizing Reinforcement Learning for LLMs by Silencing Rare Spurious Tokens](https://arxiv.org/abs/2602.15620)
*Shiqi Liu,Zeyu He,Guojian Zhan,Letian Tao,Zhilong Zheng,Jiang Wu,Yinuo Wang,Yang Guan,Kehua Sheng,Bo Zhang,Keqiang Li,Jingliang Duan,Shengbo Eben Li*

Main category: cs.CL

TL;DR: 本文通过分析 token 标准差和 token 概率之间的负相关关系，提出了 Spurious-Token-Aware Policy Optimization（STAPO）方法，通过选择性地屏蔽错误 token 的梯度更新来提升大语言模型的推理质量。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的细调方法依赖于特征的正则化等技巧来保持模型稳定性，但在实际应用中常出现性能下降现象。本文指出错误 token 对概率和局部策略熵的影响，定义了 spurious tokens，并提出有效压制错误 token 的梯度更新，增强大模型的推理能力。

Method: 本文首先通过理论分析明确了 spurious tokens 的定义和影响，进而提出了基于这些理解的优化方法 STAPO，引入了选择性遮蔽错误 token 的梯度更新以及损失函数的重归一化。

Result: STAPO 在六种数学推理基准测试中表现出更优的熵稳定性，并且相比于现有方法，如 GRPO、20-Entropy 和 JustRL，平均性能提升了 7.13%。

Conclusion: 本文通过鉴别并抑制 spurious tokens 的影响，成功提高了训练的稳定性，这一方法对于大型语言模型的优化具有重要意义。

Abstract: Reinforcement Learning (RL) has significantly improved large language model reasoning, but existing RL fine-tuning methods rely heavily on heuristic techniques such as entropy regularization and reweighting to maintain stability. In practice, they often experience late-stage performance collapse, leading to degraded reasoning quality and unstable training. We derive that the magnitude of token-wise policy gradients in RL is negatively correlated with token probability and local policy entropy. Building on this result, we prove that training instability is driven by a tiny fraction of tokens, approximately 0.01\%, which we term \emph{spurious tokens}. When such tokens appear in correct responses, they contribute little to the reasoning outcome but inherit the full sequence-level reward, leading to abnormally amplified gradient updates. Motivated by this observation, we propose Spurious-Token-Aware Policy Optimization (STAPO) for large-scale model refining, which selectively masks such updates and renormalizes the loss over valid tokens. Across six mathematical reasoning benchmarks using Qwen 1.7B, 8B, and 14B base models, STAPO consistently demonstrates superior entropy stability and achieves an average performance improvement of 7.13\% over GRPO, 20-Entropy and JustRL.

</details>


### [58] [LLM-to-Speech: A Synthetic Data Pipeline for Training Dialectal Text-to-Speech Models](https://arxiv.org/abs/2602.15675)
*Ahmed Khaled Khamis,Hesham Ali*

Main category: cs.CL

TL;DR: 研究介绍了名为NileTTS的新数据集，包含了38小时的埃及阿拉伯语录音，用于文本转语音。该数据集由大规模语言模型生成，经过音声合成，并通过自动转录和说话人鉴识进行质量验证。研究还改进了一个开源的预训练模型，以提升埃及阿拉伯语语音合成的质量。


<details>
  <summary>Details</summary>
Motivation: 由于多数现有的文本转语音资源集中在现代标准阿拉伯语和海湾阿拉伯语上，而埃及阿拉伯语由于其广泛理解和多样性需求却严重缺乏相应资源，因此有必要填补这一空白。

Method: 研究使用大规模语言模型生成埃及阿拉伯语文本，通过音频合成工具将其转换为自然语音，然后进行自动转录和说话人鉴识，并通过人工质量检查。研究团队进一步在他们的数据集上微调了最先进的多语言文本转语音模型。

Result: 研究成功创建了首个公开可用的埃及阿拉伯语文本转语音数据集，同时提供了一套可重复的合成数据生成管道，并公开了已微调的模型，从而提升了埃及阿拉伯语语音合成的效果。

Conclusion: 该研究为埃及阿拉伯语语音合成领域的研究提供了重要支持，也为其他阿拉伯语方言的研究奠定了基础。

Abstract: Despite the advances in neural text to speech (TTS), many Arabic dialectal varieties remain marginally addressed, with most resources concentrated on Modern Spoken Arabic (MSA) and Gulf dialects, leaving Egyptian Arabic -- the most widely understood Arabic dialect -- severely under-resourced. We address this gap by introducing NileTTS: 38 hours of transcribed speech from two speakers across diverse domains including medical, sales, and general conversations. We construct this dataset using a novel synthetic pipeline: large language models (LLM) generate Egyptian Arabic content, which is then converted to natural speech using audio synthesis tools, followed by automatic transcription and speaker diarization with manual quality verification. We fine-tune XTTS v2, a state-of-the-art multilingual TTS model, on our dataset and evaluate against the baseline model trained on other Arabic dialects. Our contributions include: (1) the first publicly available Egyptian Arabic TTS dataset, (2) a reproducible synthetic data generation pipeline for dialectal TTS, and (3) an open-source fine-tuned model. All resources are released to advance Egyptian Arabic speech synthesis research.

</details>


### [59] [Revisiting Northrop Frye's Four Myths Theory with Large Language Models](https://arxiv.org/abs/2602.15678)
*Edirlei Soares de Lima,Marco A. Casanova,Antonio L. Furtado*

Main category: cs.CL

TL;DR: 本文提出了一种新的基于角色的功能框架，该框架补充了基于模式的分析，通过研究Jungian原型理论，确定了四类原型角色，并通过大规模语言模型验证了这些角色对应关系的有效性。


<details>
  <summary>Details</summary>
Motivation: 传统上，计算机科学对于佛莱四类叙事体裁的理论侧重于情节模式分析，较少涉及角色功能的研究。文章旨在探索一种新的角色功能框架，以弥补这一研究空白。

Method: 文章引入了四类原型角色，基于Jung的内心结构组件映射，并将这些原型角色细化为十六种体裁特定的角色。利用六种最新的大规模语言模型进行多模型研究，评估了角色对应关系的有效性，包括真实样本和无效样本的验证。

Result: 大规模语言模型在验证这些设计的角色对应关系中表现出了良好的性能，平均综合准确率为82.5%，不同体裁和角色的准确率范围有所不同，反映了真正的叙事特性，展现了一致的模式。

Conclusion: 该研究提出的方法显示了大规模语言模型支持的方法在计算叙事学中的潜力，并为未来叙述生成方法和交互式叙事应用的发展奠定了基础。

Abstract: Northrop Frye's theory of four fundamental narrative genres (comedy, romance, tragedy, satire) has profoundly influenced literary criticism, yet computational approaches to his framework have focused primarily on narrative patterns rather than character functions. In this paper, we present a new character function framework that complements pattern-based analysis by examining how archetypal roles manifest differently across Frye's genres. Drawing on Jungian archetype theory, we derive four universal character functions (protagonist, mentor, antagonist, companion) by mapping them to Jung's psychic structure components. These functions are then specialized into sixteen genre-specific roles based on prototypical works. To validate this framework, we conducted a multi-model study using six state-of-the-art Large Language Models (LLMs) to evaluate character-role correspondences across 40 narrative works. The validation employed both positive samples (160 valid correspondences) and negative samples (30 invalid correspondences) to evaluate whether models both recognize valid correspondences and reject invalid ones. LLMs achieved substantial performance (mean balanced accuracy of 82.5%) with strong inter-model agreement (Fleiss' $κ$ = 0.600), demonstrating that the proposed correspondences capture systematic structural patterns. Performance varied by genre (ranging from 72.7% to 89.9%) and role (52.5% to 99.2%), with qualitative analysis revealing that variations reflect genuine narrative properties, including functional distribution in romance and deliberate archetypal subversion in satire. This character-based approach demonstrates the potential of LLM-supported methods for computational narratology and provides a foundation for future development of narrative generation methods and interactive storytelling applications.

</details>


### [60] [Rethinking Metrics for Lexical Semantic Change Detection](https://arxiv.org/abs/2602.15716)
*Roksana Goworek,Haim Dubossarsky*

Main category: cs.CL

TL;DR: 研究引入了新的量化词汇语义变化的方法（AMD和SAMD），并在多种语言和表示空间下展示了这些方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的语义变化度量方法（如APD和PRT）存在局限性，新的度量方法旨在提供更鲁棒的性能。

Method: 研究提出了AMD和SAMD，通过时间跨度上单词使用之间的局部对应关系来量化语义变化。

Result: 研究在多语言、多种编码器和嵌入空间下，AMD常常提供更稳定的性能，而SAMD在专业化编码器下表现更佳。

Conclusion: 研究建议在基于上下文的嵌入分析中考虑使用新方法，以提高语义变化检测的准确性。

Abstract: Lexical semantic change detection (LSCD) increasingly relies on contextualised language model embeddings, yet most approaches still quantify change using a small set of semantic change metrics, primarily Average Pairwise Distance (APD) and cosine distance over word prototypes (PRT). We introduce Average Minimum Distance (AMD) and Symmetric Average Minimum Distance (SAMD), new measures that quantify semantic change via local correspondence between word usages across time periods. Across multiple languages, encoder models, and representation spaces, we show that AMD often provides more robust performance, particularly under dimensionality reduction and with non-specialised encoders, while SAMD excels with specialised encoders. We suggest that LSCD may benefit from considering alternative semantic change metrics beyond APD and PRT, with AMD offering a robust option for contextualised embedding-based analysis.

</details>


### [61] [Causal Effect Estimation with Latent Textual Treatments](https://arxiv.org/abs/2602.15730)
*Omri Feldman,Amar Venugopal,Jann Spiess,Amir Feder*

Main category: cs.CL

TL;DR: 本文提出了一种生成和因果估计潜在文本干预的端到端管道，通过稀疏自编码器生成假设并通过稳健的因果估计进行干预，解决了文本作为治疗因子实验中的计算和统计挑战。


<details>
  <summary>Details</summary>
Motivation: 在许多应用中，理解文本对下游结果的因果效应是核心任务。现有方法存在直接估计偏差的问题，因此需要改进的工具以准确评估干预效果。

Method: 该方法利用稀疏自编码器进行假设生成和定向，并通过残差化协变量的方法来解决估计偏差问题。整个过程包括生成和评估潜在文本干预，并采用稳健的因果估计方法进行效果评估。

Result: 实验结果表明，该方法能够有效地在目标特征上引起变异并减少估计误差，为文本作为治疗因子的因果效应评估提供了稳健的基础。

Conclusion: 该研究提供了一种评估文本作为治疗因子的因果效应的方法框架，该框架有效地解决了直接估计的偏差问题，并展示了一种有效的方法来诱导目标文本特征的变化，解决了计算和统计挑战。

Abstract: Understanding the causal effects of text on downstream outcomes is a central task in many applications. Estimating such effects requires researchers to run controlled experiments that systematically vary textual features. While large language models (LLMs) hold promise for generating text, producing and evaluating controlled variation requires more careful attention. In this paper, we present an end-to-end pipeline for the generation and causal estimation of latent textual interventions. Our work first performs hypothesis generation and steering via sparse autoencoders (SAEs), followed by robust causal estimation. Our pipeline addresses both computational and statistical challenges in text-as-treatment experiments. We demonstrate that naive estimation of causal effects suffers from significant bias as text inherently conflates treatment and covariate information. We describe the estimation bias induced in this setting and propose a solution based on covariate residualization. Our empirical results show that our pipeline effectively induces variation in target features and mitigates estimation error, providing a robust foundation for causal effect estimation in text-as-treatment settings.

</details>


### [62] [Under-resourced studies of under-resourced languages: lemmatization and POS-tagging with LLM annotators for historical Armenian, Georgian, Greek and Syriac](https://arxiv.org/abs/2602.15753)
*Chahan Vidal-Gorène,Bastien Kindt,Florian Cafiero*

Main category: cs.CL

TL;DR: 该研究评估了最新大规模语言模型在四种历史和语言上独特的低资源语言中的形态学和词性标注任务表现，展示了在少量提示设置下模型即使未经过微调也能取得竞争力或超越的表现，但仍面临复杂形态和非拉丁字母系统语言的挑战。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索大型语言模型处理低资源语言自然语言处理任务的能力，特别是对于形态学和词性标注等任务，这些任务在历史和语言上独特的低资源语言中尤其具有挑战性。

Method: 研究使用了一个包含对齐的训练和跨领域测试语料库的新基准测试，评估了基础模型在形态学和词性标注任务上的性能，并将它们与特定任务的RNN基线PIE进行了对比。

Result: 研究结果表明，即使未经微调，这些语言模型在多数语言的少量提示设置中都达到了可用于形态学和词性标注的竞争力或更优表现，但复杂形态和非拉丁字母系统语言仍然存在挑战。

Conclusion: 研究结论认为，尽管大部分低资源语言在使用最新大型语言模型进行形态学和词性标注时面临挑战，但这些模型在缺乏数据的情况下启动语言注释任务仍是一个值得考虑的有效手段，并能显著辅助注释工作。

Abstract: Low-resource languages pose persistent challenges for Natural Language Processing tasks such as lemmatization and part-of-speech (POS) tagging. This paper investigates the capacity of recent large language models (LLMs), including GPT-4 variants and open-weight Mistral models, to address these tasks in few-shot and zero-shot settings for four historically and linguistically diverse under-resourced languages: Ancient Greek, Classical Armenian, Old Georgian, and Syriac. Using a novel benchmark comprising aligned training and out-of-domain test corpora, we evaluate the performance of foundation models across lemmatization and POS-tagging, and compare them with PIE, a task-specific RNN baseline. Our results demonstrate that LLMs, even without fine-tuning, achieve competitive or superior performance in POS-tagging and lemmatization across most languages in few-shot settings. Significant challenges persist for languages characterized by complex morphology and non-Latin scripts, but we demonstrate that LLMs are a credible and relevant option for initiating linguistic annotation tasks in the absence of data, serving as an effective aid for annotation.

</details>


### [63] [ChartEditBench: Evaluating Grounded Multi-Turn Chart Editing in Multimodal Language Models](https://arxiv.org/abs/2602.15758)
*Manav Nitin Kapadnis,Lawanya Baghel,Atharva Naik,Carolyn Rosé*

Main category: cs.CL

TL;DR: 研究引入了ChartEditBench基准，旨在评估大型语言模型在多轮可视化编辑中的表现，特别关注保持共同知识、跟踪先前更改和适应不断变化的偏好方面。


<details>
  <summary>Details</summary>
Motivation: 当前的大规模语言模型在单轮图表生成方面表现出色，但尚未充分探索其在真实世界探索数据分析中的能力。这项研究的动机是填补这一空白，通过创建ChartEditBench基准来评估模型在多轮可视化编辑中的表现。

Method: 该研究提出了一个包含5,000条难度控制修改链的基准，并设计了一个评估框架，结合执行准确性检查、像素级视觉相似性和逻辑代码验证，来评估模型。

Result: 实验证明，最先进的大规模语言模型在多轮设置中表现大幅下降，尤其是在数据导向的转换任务上，而其在风格编辑方面表现较好。

Conclusion: ChartEditBench为基于意图的多模态编程提供了一个具有挑战性的测试平台，揭示了现有模型在多轮交互中的局限性。

Abstract: While Multimodal Large Language Models (MLLMs) perform strongly on single-turn chart generation, their ability to support real-world exploratory data analysis remains underexplored. In practice, users iteratively refine visualizations through multi-turn interactions that require maintaining common ground, tracking prior edits, and adapting to evolving preferences. We introduce ChartEditBench, a benchmark for incremental, visually grounded chart editing via code, comprising 5,000 difficulty-controlled modification chains and a rigorously human-verified subset. Unlike prior one-shot benchmarks, ChartEditBench evaluates sustained, context-aware editing. We further propose a robust evaluation framework that mitigates limitations of LLM-as-a-Judge metrics by integrating execution-based fidelity checks, pixel-level visual similarity, and logical code verification. Experiments with state-of-the-art MLLMs reveal substantial degradation in multi-turn settings due to error accumulation and breakdowns in shared context, with strong performance on stylistic edits but frequent execution failures on data-centric transformations. ChartEditBench, establishes a challenging testbed for grounded, intent-aware multimodal programming.

</details>


### [64] [*-PLUIE: Personalisable metric with Llm Used for Improved Evaluation](https://arxiv.org/abs/2602.15778)
*Quentin Lemesle,Léane Jourdan,Daisy Munson,Pierre Alain,Jonathan Chevelu,Arnaud Delhay,Damien Lolive*

Main category: cs.CL

TL;DR: 该研究提出了*-PLUIE，一种基于困惑度的LLM-judge指标，用于评估自动生成文本的质量，能够与人类判断保持更好的一致性，同时保持较低的计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM-judge方法虽然有效，但计算成本高且需要后处理，因此希望通过改进方法降低计算成本并保持与人类判断的高度一致性。

Method: 研究基于ParaPLUIE，提出了一种针对特定任务的*-PLUIE，利用任务特定提示来估计对于'是/否'问题的回答置信度，无需生成文本。

Result: 实验结果表明，个人化*-PLUIE 在与人类评分的一致性上表现优于其他方法，同时保持了较低的计算成本。

Conclusion: 研究提出的方法*-PLUIE 在保持较低计算成本的同时，提高了与人类判断的一致性，展示了其在自动文本评估领域的潜力。

Abstract: Evaluating the quality of automatically generated text often relies on LLM-as-a-judge (LLM-judge) methods. While effective, these approaches are computationally expensive and require post-processing. To address these limitations, we build upon ParaPLUIE, a perplexity-based LLM-judge metric that estimates confidence over ``Yes/No'' answers without generating text. We introduce *-PLUIE, task specific prompting variants of ParaPLUIE and evaluate their alignment with human judgement. Our experiments show that personalised *-PLUIE achieves stronger correlations with human ratings while maintaining low computational cost.

</details>


### [65] [Avey-B](https://arxiv.org/abs/2602.15814)
*Devang Acharya,Mohammad Hammoud*

Main category: cs.CL

TL;DR: 本文重新构想了 Avey，并在编码器-only 架构中提出创新改进，包括静态和动态参数分离、稳定性导向归一化和神经压缩，结果显示该重构架构在标准的标记分类和信息检索基准测试中优于四种广泛使用的基于变换器的编码器，同时在长上下文场景下更具可扩展性。


<details>
  <summary>Details</summary>
Motivation: 提出 Avey 的目的是为了解决变压器架构在自回归和注意力机制上的限制，并提供更高效的编码器-only 选择。

Method: 通过重新设计 Avey，引入了 decoupled 静态和动态参数化，stability-oriented normalization 和神经压缩等方法，以改进其编码器-only 架构。

Result: 实验证明，该重塑后的架构在标准的标记分类和信息检索基准测试中表现优于四种广泛使用的基于变换器的编码器，在长上下文场景中更具优势。

Conclusion: 本文提出的方法有效提升了 Avey 在处理长上下文需求时的性能和效率，是一种有竞争力的编码器-only 嵌入选择。

Abstract: Compact pretrained bidirectional encoders remain the backbone of industrial NLP under tight compute and memory budgets. Their effectiveness stems from self-attention's ability to deliver high-quality bidirectional contextualization with sequence-level parallelism, as popularized by BERT-style architectures. Recently, Avey was introduced as an autoregressive, attention-free alternative that naturally admits an encoder-only adaptation. In this paper, we reformulate Avey for the encoder-only paradigm and propose several innovations to its architecture, including decoupled static and dynamic parameterizations, stability-oriented normalization, and neural compression. Results show that this reformulated architecture compares favorably to four widely used Transformer-based encoders, consistently outperforming them on standard token-classification and information-retrieval benchmarks while scaling more efficiently to long contexts.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [66] [Fast and Fusiest: An Optimal Fusion-Aware Mapper for Accelerator Modeling and Evaluation](https://arxiv.org/abs/2602.15166)
*Tanner Andrulis,Michael Gilbert,Vivienne Sze,Joel S. Emer*

Main category: cs.AR

TL;DR: 本文提出了一种名为FFM的新映射器，能够在Tensor Algebra工作负载的混合融合映射空间中快速找到最优映射。FFM通过剪枝非最优的子映射来缩小搜索空间，并将这些子映射组合以构建最优的融合映射。即使映射空间的大小随着计算步骤的数量呈指数增长，FFM的运行时间也大致呈线性增长。


<details>
  <summary>Details</summary>
Motivation: 当前的映射器无法在可行的时间内找到融合映射（即最佳融合映射），因为需要搜索的融合映射的数量随着工作负载计算步骤的数量呈指数增长。

Method: FFM通过剪枝非最优的子映射来缩小搜索空间，然后将这些子映射组合以构建最优的融合映射。

Result: FFM的运行时间大致呈线性增长，比前最先进的方法快几个数量级。

Conclusion: FFM能够在Tensor Algebra工作负载的混合融合映射空间中快速找到最优映射，显著提高了性能。

Abstract: The latency and energy of tensor algebra accelerators depend on how data movement and operations are scheduled (i.e., mapped) onto accelerators, so determining the potential of an accelerator architecture requires both a performance model and a mapper to search for the optimal mapping. A key optimization that the mapper must explore is fusion, meaning holding data on-chip between computation steps, which has been shown to reduce energy and latency by reducing DRAM accesses. However, prior mappers cannot find optimal mappings with fusion (i.e., fused mappings) in a feasible runtime because the number of fused mappings to search increases exponentially with the number of workload computation steps.
  In this paper, we introduce the Fast and Fusiest Mapper (FFM), the first mapper to quickly find optimal mappings in a comprehensive fused mapspace for tensor algebra workloads. FFM shrinks the search space by pruning subsets of mappings (i.e., partial mappings) that are shown to never be a part of optimal mappings, quickly eliminating all suboptimal mappings with those partial mappings as subsets. Then FFM joins partial mappings to construct optimal fused mappings. We evaluate FFM and show that, although the mapspace size grows exponentially with the number of computation steps, FFM's runtime scales approximately linearly. FFM is orders of magnitude faster ($>1000\times$) than prior state-of-the-art approaches at finding optimal mappings for Transformers.

</details>


### [67] [The Turbo-Charged Mapper: Fast and Optimal Mapping for Accelerator Modeling and Evaluation](https://arxiv.org/abs/2602.15172)
*Michael Gilbert,Tanner Andrulis,Vivienne Sze,Joel S. Emer*

Main category: cs.AR

TL;DR: 通过引入新的数据放置（dataplacement）概念，Turbo-Charged Mapper (TCM) 能够有效减少搜索空间，并在可接受的时间内找到最优映射，极大地提升了映射优化的效率。


<details>
  <summary>Details</summary>
Motivation: 现有的硬件评估受到不完美映射优化方法的限制，无法区分性能差异源于硬件设计本身还是映射不足，从而不利于准确评估和设计高效的加速器。

Method: 提出了一种基于新概念数据放置（dataplacement）的Turbo-Charged Mapper (TCM) 方法。通过这种方法，TCM 可以识别和排除冗余及不理想的映射，从而显著降低搜索空间，提高映射优化的速度和效率。

Result: 与之前的现有方法相比，TCM 在少于一分钟的时间内能找到最优映射，而之前的最好方法即使给予 1000 倍的运行时间仍不能找到最优映射，且能源延迟积性能高 21%。

Conclusion: TCM 克服了传统映射优化方法中的不足，通过有效减少搜索空间和快速找到最优映射，极大地促进了加速器硬件设计和评估的精确性与效率。

Abstract: The energy and latency of an accelerator running a deep neural network (DNN) depend on how the computation and data movement are scheduled in the accelerator (i.e., mapping). Optimizing mappings is essential to evaluating and designing accelerators. However, the space of mappings is large, and prior works can not guarantee finding optimal mappings because they use heuristics or metaheuristics to narrow down the space. These limitations preclude proper hardware evaluation, since designers can not tell whether performance differences are due to changes in hardware or suboptimal mapping.
  To address this challenge, we propose the Turbo-Charged Mapper (TCM), a fast mapper that is guaranteed to find optimal mappings. The key to our approach is that we define a new concept in mapping, called dataplacement, which, like the prior concept of dataflow, allows for clear analysis and comparison of mappings. Through it, we identify multiple opportunities to prune redundant and suboptimal mappings, reducing search space by up to 32 orders of magnitude.
  Leveraging these insights, TCM can perform full mapspace searches, making it the first mapper that can find optimal mappings in feasible runtime. Compared to prior mappers, we show that TCM can find optimal mappings quickly (less than a minute), while prior works can not find optimal mappings (energy-delay-product $21\%$ higher than optimal) even when given $1000\times$ the runtime ($>10$ hours).

</details>


### [68] [Human-AI Interaction: Evaluating LLM Reasoning on Digital Logic Circuit included Graph Problems, in terms of creativity in design and analysis](https://arxiv.org/abs/2602.15336)
*Yogeswar Reddy Thota,Setareh Rafatirad,Homayoun Houman,Tooraj Nikoubin*

Main category: cs.AR

TL;DR: 本研究评估了三大主流语言模型（GPT、Gemini 和 Claude）在数字逻辑问题上的表现，发现虽然模型在解释上得到了学生的好评，但在技术准确性上表现不佳，特别是在处理序列化问题时，导致可能的误导性信息。


<details>
  <summary>Details</summary>
Motivation: 作者通过详细的学生和专家评估揭示了语言模型在数字逻辑练习中存在的可靠性和潜在误导性问题。

Method: 研究采用了大规模的双模型对比实验，由24名学生参与，用以评估模型在回答10个本科级数字逻辑问题上的表现，并结合专家的独立评分来验证技术正确性。

Result: 研究结果表明，在较为复杂的逻辑问题上，所有评估的语言模型都无法给出正式正确的答案，尽管它们提供了详细且自信的解释，学生也给出了较为积极的评价。

Conclusion: 研究结论指出，目前的语言模型在处理核心数字逻辑知识点时可能不可靠，并可能在教育中强化学生的错误认知。

Abstract: Large Language Models (LLMs) are increasingly used by undergraduate students as on-demand tutors, yet their reliability on circuit- and diagram-based digital logic problems remains unclear. We present a human- AI study evaluating three widely used LLMs (GPT, Gemini, and Claude) on 10 undergraduate-level digital logic questions spanning non-standard counters, JK-based state transitions, timing diagrams, frequency division, and finite-state machines. Twenty-four students performed pairwise model comparisons, providing per-question judgments on (i) preferred model, (ii) perceived correctness, (iii) consistency, (iv) verbosity, and (v) confidence, along with global ratings of overall model quality, satisfaction across multiple dimensions (e.g., accuracy and clarity), and perceived mental effort required to verify answers. To benchmark technical validity, we applied an independent judge-based evaluation against official solutions for all ten questions, using strict correctness criteria. Results reveal a consistent gap between perceived helpfulness and formal correctness: for the most sequentially demanding problems (Q1- Q7), none of the evaluated LLMs matched the official answers, despite producing confident, well-structured explanations that students often rated favorably. Error analysis indicates that models frequently default to canonical textbook templates (e.g., standard ripple counters) and struggle to translate circuit structure into exact state evolution and timing behavior. These findings suggest that, without verification scaffolds, LLMs may be unreliable for core digital logic topics and can inadvertently reinforce misconceptions in undergraduate instruction.

</details>


### [69] [Iterative LLM-Based Assertion Generation Using Syntax-Semantic Representations for Functional Coverage-Guided Verification](https://arxiv.org/abs/2602.15388)
*Yonghao Wang,Jiaxin Zhou,Yang Yin,Hongqin Lyu,Zhiteng Chao,Wenchao Ding,Jing Ye,Tiancheng Wang,Huawei Li*

Main category: cs.AR

TL;DR: 本文提出了一种名为CoverAssert的迭代框架，旨在通过分布式表示和功能覆盖分析来优化使用LLMs生成SVAs的过程，从而显著提升功能覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现有技术存在生成的质量较差的问题，因此需要一种能够提高SVAs与功能描述匹配度的迭代方法。

Method: CoverAssert通过将LLM生成的语义特征与信号断言的抽象语法树结构特征进行聚类，并映射回规格说明书来分析功能覆盖率，基于功能覆盖率构建反馈回路，指导LLMs优先关注未被覆盖的功能点。

Result: 实验结果表明，集成CoverAssert的状态最先进技术AssertLLM和Spec2Assertion后，平均分支覆盖率提高了9.57%，语句覆盖率提高了9.64%，翻转覆盖率提高了15.69%。

Conclusion: CoverAssert通过反馈机制迭代优化了生成的SVAs，显著提升了下一次迭代中SVA的质量，验证了对抗LEC问题的有效性。

Abstract: While leveraging LLMs to automatically generate SystemVerilog assertions (SVAs) from natural language specifications holds great potential, existing techniques face a key challenge: LLMs often lack sufficient understanding of IC design, leading to poor assertion quality in a single pass. Therefore, verifying whether the generated assertions effectively cover the functional specifications and designing feedback mechanisms based on this coverage remain significant hurdles. To address these limitations, this paper introduces CoverAssert, a novel iterative framework for optimizing SVA generation with LLMs. The core contribution is a lightweight mechanism for matching generated assertions with specific functional descriptions in the specifications. CoverAssert achieves this by clustering the joint representations of semantic features of LLM-generated assertions and structural features extracted from abstract syntax trees (ASTs) about signals related to assertions, and then mapping them back to the specifications to analyze functional coverage quality. Leveraging this capability, CoverAssert constructs a feedback loop based on functional coverage to guide LLMs in prioritizing uncovered functional points, thereby iteratively improving assertion quality. Experimental evaluations on four open-source designs demonstrate that integrating CoverAssert with state-of-the-art generators, AssertLLM and Spec2Assertion, achieves average improvements of 9.57 % in branch coverage, 9.64 % in statement coverage, and 15.69 % in toggle coverage.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [70] [Protecting Language Models Against Unauthorized Distillation through Trace Rewriting](https://arxiv.org/abs/2602.15143)
*Xinhang Ma,William Yeoh,Ning Zhang,Yevgeniy Vorobeychik*

Main category: cs.AI

TL;DR: 该研究旨在通过修改教师生成的推理痕迹来防止未经授权的知识蒸馏，并确保答案的正确性和语义连贯性。通过多种方法，研究展示了基于指令的方法在反知识蒸馏和水印检测方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 研究指出，未经授权的知识蒸馏不公正地利用了LSTM模型开发过程中付出的大量人力和财力。为了防止这种不正当行为，研究探索了修改推理痕迹的方法，以实现反知识蒸馏和API水印嵌入两个目标。

Method: 研究引入了几种动态重写教师推理输出的方法，以保持答案正确性和语义连贯，同时利用LLM的重写能力和梯度算法。通过简单的指令重写方法，研究有效地实现了反知识蒸馏效果，同时保持或提高了教师的表现。此外，该方法还使得水印检测变得高度可靠，几乎没有误报。

Result: 实验结果显示，基于指令的重写方法在反知识蒸馏方面表现出色，同时保持或提高了教师模型的性能。此外，该方法还实现了高可靠性水印检测，几乎没有错误警报。

Conclusion: 研究提出的方法在防止知识蒸馏和嵌入可验证签名方面具有有效性和可靠性。实验证明了基于指令重写策略的有效性，并为未来的研究和应用提供了重要参考。

Abstract: Knowledge distillation is a widely adopted technique for transferring capabilities from LLMs to smaller, more efficient student models. However, unauthorized use of knowledge distillation takes unfair advantage of the considerable effort and cost put into developing frontier models. We investigate methods for modifying teacher-generated reasoning traces to achieve two objectives that deter unauthorized distillation: (1) \emph{anti-distillation}, or degrading the training usefulness of query responses, and (2) \emph{API watermarking}, which embeds verifiable signatures in student models. We introduce several approaches for dynamically rewriting a teacher's reasoning outputs while preserving answer correctness and semantic coherence. Two of these leverage the rewriting capabilities of LLMs, while others use gradient-based techniques. Our experiments show that a simple instruction-based rewriting approach achieves a strong anti-distillation effect while maintaining or even improving teacher performance. Furthermore, we show that our rewriting approach also enables highly reliable watermark detection with essentially no false alarms.

</details>


### [71] [da Costa and Tarski meet Goguen and Carnap: a novel approach for ontological heterogeneity based on consequence systems](https://arxiv.org/abs/2602.15158)
*Gabriel Rocha*

Main category: cs.AI

TL;DR: 本文提出了一种新的方法，结合了卡拉普-戈根主义的理念，通过扩展的后果系统来处理本体异质性问题。该方法引入了扩展的后果系统和扩展发展图的概念，为本体间的关系提供了新的视角，并为应用本体领域提出了研究方向。


<details>
  <summary>Details</summary>
Motivation: 文章旨在解决本体异质性问题，传统的本体方法和工具可能无法有效处理不同本体之间的关系。因此，提出了一种创新性的方法，结合了数学和逻辑学的原理，以期望能更好地理解和整合不同本体。

Method: 作者利用卡拉普和戈根主义的理念，结合扩展的后果系统框架，发展出一种新的方法来处理本体异质性。这种方法通过扩展的后果系统引入了本体的扩展概念，并构建了扩展发展图以表示本体之间的关系。这种方法还考虑了通过使oring的合并和拆分操作来进一步丰富本体关系。

Result: 论文提出了da Costian-Tarskian主义，一种基于扩展后果系统的本体处理方法，并定义了扩展后的本体及其关系的概念，如扩展发展图。

Conclusion: 论文讨论了新方法对应用本体领域的潜在影响，并提出了未来研究的方向，旨在更深入地研究如何应用和优化这种方法以更好地处理本体异质性。

Abstract: This paper presents a novel approach for ontological heterogeneity that draws heavily from Carnapian-Goguenism, as presented by Kutz, Mossakowski and Lücke (2010). The approach is provisionally designated da Costian-Tarskianism, named after da Costa's Principle of Tolerance in Mathematics and after Alfred Tarski's work on the concept of a consequence operator. The approach is based on the machinery of consequence systems, as developed by Carnielli et al. (2008) and Citkin and Muravitsky (2022), and it introduces the idea of an extended consequence system, which is a consequence system extended with ontological axioms. The paper also defines the concept of an extended development graph, which is a graph structure that allows ontologies to be related via morphisms of extended consequence systems, and additionally via other operations such as fibring and splitting. Finally, we discuss the implications of this approach for the field of applied ontology and suggest directions for future research.

</details>


### [72] [Mind the (DH) Gap! A Contrast in Risky Choices Between Reasoning and Conversational LLMs](https://arxiv.org/abs/2602.15173)
*Luise Ge,Yongyan Zhang,Yevgeniy Vorobeychik*

Main category: cs.AI

TL;DR: 本文研究了20个尖端和开放的大型语言模型在不确定情况下的决策，并将它们分为两类：推理模型和对话模型。推理模型更接近理性行为，而对话模型则更接近人类行为。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型正在迅速改变数字生态系统，但它们在不确定情况下的决策机制尚未完全理解。本文旨在填补这一空白，通过对比研究不同类型的大型语言模型，为理解大型语言模型的决策过程提供参考。

Method: 本文通过对比研究20个前沿和开放的大型语言模型，结合人类被试实验和理性的行为模型实验进行分析。

Result: 研究发现，大型语言模型可以分为两类：推理模型和对话模型。推理模型表现出更加理性的行为，对选项的顺序、收益/损失框架以及解释不敏感；对话模型则表现出更多的接近人类的行为模式，对这些因素更加敏感。

Conclusion: 研究表明，推理模型和对话模型之间的主要区别在于培训数学推理能力的差异，这可能对理解大型语言模型的决策机制提供一些启示。

Abstract: The use of large language models either as decision support systems, or in agentic workflows, is rapidly transforming the digital ecosystem. However, the understanding of LLM decision-making under uncertainty remains limited. We initiate a comparative study of LLM risky choices along two dimensions: (1) prospect representation (explicit vs. experience based) and (2) decision rationale (explanation). Our study, which involves 20 frontier and open LLMs, is complemented by a matched human subjects experiment, which provides one reference point, while an expected payoff maximizing rational agent model provides another. We find that LLMs cluster into two categories: reasoning models (RMs) and conversational models (CMs). RMs tend towards rational behavior, are insensitive to the order of prospects, gain/loss framing, and explanations, and behave similarly whether prospects are explicit or presented via experience history. CMs are significantly less rational, slightly more human-like, sensitive to prospect ordering, framing, and explanation, and exhibit a large description-history gap. Paired comparisons of open LLMs suggest that a key factor differentiating RMs and CMs is training for mathematical reasoning.

</details>


### [73] [Predicting Invoice Dilution in Supply Chain Finance with Leakage Free Two Stage XGBoost, KAN (Kolmogorov Arnold Networks), and Ensemble Models](https://arxiv.org/abs/2602.15248)
*Pavel Koptev,Vishnu Kumar,Konstantin Malkov,George Shapiro,Yury Vikhanov*

Main category: cs.AI

TL;DR: 该研究介绍了一种基于AI和机器学习的框架来预测发票稀释，旨在补充现有的确定性算法，并提出了一种实时动态信用额度机制，以解决供应链金融中的非信贷风险。


<details>
  <summary>Details</summary>
Motivation: 传统的买家不可撤销支付承诺（IPU）虽然保护了卖方利益，但阻碍了供应链金融的普及，尤其是对于信用较低的买家。因此，研究者引入了新的数据驱动方法，通过实时动态信用限额来预测和管理发票稀释风险。

Method: 研究采用了AI和机器学习模型，并结合了广泛的生产数据集，跨越九个关键交易字段，以评估其在预测发票稀释中的有效性。

Result: 该研究的结果表明，AI和机器学习框架可以有效补充现有的确定性算法，提高对发票稀释预测的准确性和实时性。

Conclusion: 基于AI和机器学习的发票稀释预测方法为供应链金融提供了新的风险管理工具，有助于促进金融模式的升级。

Abstract: Invoice or payment dilution is the gap between the approved invoice amount and the actual collection is a significant source of non credit risk and margin loss in supply chain finance. Traditionally, this risk is managed through the buyer's irrevocable payment undertaking (IPU), which commits to full payment without deductions. However, IPUs can hinder supply chain finance adoption, particularly among sub-invested grade buyers. A newer, data-driven methods use real-time dynamic credit limits, projecting dilution for each buyer-supplier pair in real-time. This paper introduces an AI, machine learning framework and evaluates how that can supplement a deterministic algorithm to predict invoice dilution using extensive production dataset across nine key transaction fields.

</details>


### [74] [Enhancing Diversity and Feasibility: Joint Population Synthesis from Multi-source Data Using Generative Models](https://arxiv.org/abs/2602.15270)
*Farbod Abbasi,Zachary Patterson,Bilal Farooq*

Main category: cs.AI

TL;DR: 本研究提出了一种利用WGAN结合梯度罚则同时融合多源数据的生成方法，显著提高了合成数据的多样性和可行性。


<details>
  <summary>Details</summary>
Motivation: 当前合成人口的方法存在数据单一或流程顺序性导致的特征间复杂互动捕获不足，以及采样和结构零值处理不佳的问题，这些限制了合成数据的多样性和实用性。

Method: 本研究采用WGAN结合梯度罚则的方法进行联合学习，通过定义生成器损失函数的正则化项（逆梯度罚则）来提升数据多样性和可行性。使用统一的评价指标对相似性、多样性和可行性进行衡量。

Result: 实验结果表明，所提出的联合方法在召回率和准确率上分别比顺序方法提高了7%和15%，进一步通过正则化项提高了10%的召回率和1%的准确率。综合相似性方面，联合方法得分88.1，要比顺序方法的84.6高。

Conclusion: 本研究的方法能够显著提高基于多源数据的合成人口的质量，为交通和城市规划中的ABM提供了更准确和可靠的输入。

Abstract: Generating realistic synthetic populations is essential for agent-based models (ABM) in transportation and urban planning. Current methods face two major limitations. First, many rely on a single dataset or follow a sequential data fusion and generation process, which means they fail to capture the complex interplay between features. Second, these approaches struggle with sampling zeros (valid but unobserved attribute combinations) and structural zeros (infeasible combinations due to logical constraints), which reduce the diversity and feasibility of the generated data. This study proposes a novel method to simultaneously integrate and synthesize multi-source datasets using a Wasserstein Generative Adversarial Network (WGAN) with gradient penalty. This joint learning method improves both the diversity and feasibility of synthetic data by defining a regularization term (inverse gradient penalty) for the generator loss function. For the evaluation, we implement a unified evaluation metric for similarity, and place special emphasis on measuring diversity and feasibility through recall, precision, and the F1 score. Results show that the proposed joint approach outperforms the sequential baseline, with recall increasing by 7\% and precision by 15\%. Additionally, the regularization term further improves diversity and feasibility, reflected in a 10\% increase in recall and 1\% in precision. We assess similarity distributions using a five-metric score. The joint approach performs better overall, and reaches a score of 88.1 compared to 84.6 for the sequential method. Since synthetic populations serve as a key input for ABM, this multi-source generative approach has the potential to significantly enhance the accuracy and reliability of ABM.

</details>


### [75] [Improving LLM Reliability through Hybrid Abstention and Adaptive Detection](https://arxiv.org/abs/2602.15391)
*Ankit Sharma,Nachiket Tapas,Jyotiprakash Patra*

Main category: cs.AI

TL;DR: 该研究提出了一种适应性退出系统，通过基于实时上下文信号动态调整安全阈值，结合多维度检测结构和级联机制，大幅减少了延迟，同时保持了高安全性和近完美的召回率。


<details>
  <summary>Details</summary>
Motivation: 传统的基于静态规则或固定置信阈值的防护措施通常是上下文无关的，计算昂贵，导致高延迟和用户体验下降。

Method: 该方法提出了一个适应性退出系统，该系统通过级联级机制整合了多维度的检测架构，使用实时上下文信号动态调整安全阈值。

Result: 在混合和特定领域的工作负载上进行广泛的评估表明，该系统显著减少了误报，特别是在医疗建议和创意写作等敏感领域。在严格运行模式下，系统保持了高水平的安全精度和接近完美的召回率。

Conclusion: 该适应性退出框架有效地平衡了安全性和实用性，提供了可靠的大规模语言模型部署可扩展的解决方案。

Abstract: Large Language Models (LLMs) deployed in production environments face a fundamental safety-utility trade-off either a strict filtering mechanisms prevent harmful outputs but often block benign queries or a relaxed controls risk unsafe content generation. Conventional guardrails based on static rules or fixed confidence thresholds are typically context-insensitive and computationally expensive, resulting in high latency and degraded user experience. To address these limitations, we introduce an adaptive abstention system that dynamically adjusts safety thresholds based on real-time contextual signals such as domain and user history. The proposed framework integrates a multi-dimensional detection architecture composed of five parallel detectors, combined through a hierarchical cascade mechanism to optimize both speed and precision. The cascade design reduces unnecessary computation by progressively filtering queries, achieving substantial latency improvements compared to non-cascaded models and external guardrail systems. Extensive evaluation on mixed and domain-specific workloads demonstrates significant reductions in false positives, particularly in sensitive domains such as medical advice and creative writing. The system maintains high safety precision and near-perfect recall under strict operating modes. Overall, our context-aware abstention framework effectively balances safety and utility while preserving performance, offering a scalable solution for reliable LLM deployment.

</details>


### [76] [Common Belief Revisited](https://arxiv.org/abs/2602.15403)
*Thomas Ågotnes*

Main category: cs.AI

TL;DR: 该研究探讨了在KD45情况下，共同信念的逻辑是否由KD4扩展到shift-reflexivity就可以完全描述，研究发现还需要一个额外的公理，并且这个额外的公理依赖于参与者的数量。


<details>
  <summary>Details</summary>
Motivation: 研究者认为，在针对共同信念的逻辑表述上，现有的KD4框架可能不足以完全涵盖所有情况，尤其是当涉及多个参与者时。因此，这项研究旨在探索更准确描述共同信念逻辑的可能性。

Method: 通过引入shift-reflexivity这一新的公理，研究者进行了一系列的逻辑推演，验证了是否能够完全刻画共同信念的逻辑。

Result: 研究结果显示，确实需要一个额外的公理来完全刻画共同信念的逻辑，在多种参与者情况下，这种新的逻辑框架能够更好地描述共同信念的特性。

Conclusion: 研究确认了，在多参与者的情况下，KD4加上shift-reflexivity以及其他特定公理可以完全刻画共同信念的逻辑，解决了该领域的难题。

Abstract: Contrary to common belief, common belief is not KD4.
  If individual belief is KD45, common belief does indeed lose the 5 property and keep the D and 4 properties -- and it has none of the other commonly considered properties of knowledge and belief. But it has another property: $C(Cφ\rightarrow φ)$ -- corresponding to so-called shift-reflexivity (reflexivity one step ahead). This observation begs the question:
  is KD4 extended with this axiom a complete characterisation of common belief in the KD45 case? If not, what \emph{is} the logic of common belief? In this paper we show that the answer to the first question is ``no'': there is one additional axiom, and, furthermore, it relies on the number of agents. We show that the result is a complete characterisation of common belief, settling the open problem.

</details>


### [77] [GenAI-LA: Generative AI and Learning Analytics Workshop (LAK 2026), April 27--May 1, 2026, Bergen, Norway](https://arxiv.org/abs/2602.15531)
*Javier Irigoyen,Roberto Daza,Aythami Morales,Julian Fierrez,Francisco Jurado,Alvaro Ortigosa,Ruben Tolosana*

Main category: cs.AI

TL;DR: 该研究提出了 EduEVAL-DB 数据集，旨在支持自动教育评价器和 AI 辅导者的评估与训练。数据集包含 854 个解释，涵盖 K-12 年级的科学、语言和社会科学问题。数据集通过半自动注释过程进行标注，并通过初步验证实验评估了其在教育风险检测中的适用性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过开发 EduEVAL-DB 来改善自动教育评价器和 AI 辅导工具，以满足教育实践中对精准、全面和适宜的解释需求。

Method: 研究设计并收集了 854 个解释，并通过启发式方法和提示工程技术模拟了六种教师角色。提出了与教育标准相结合的教学风险评价标准，并进行了初步验证实验来评估模型的效能。

Result: 初步验证实验显示，基于 EduEVAL-DB 的监督微调可以有效提升教育导向模型在教学风险检测任务上的性能。

Conclusion: 该研究为自动教育评估工具的训练提供了有价值的资源，并展示了其在实际应用中的潜力。

Abstract: This work introduces EduEVAL-DB, a dataset based on teacher roles designed to support the evaluation and training of automatic pedagogical evaluators and AI tutors for instructional explanations. The dataset comprises 854 explanations corresponding to 139 questions from a curated subset of the ScienceQA benchmark, spanning science, language, and social science across K-12 grade levels. For each question, one human-teacher explanation is provided and six are generated by LLM-simulated teacher roles. These roles are inspired by instructional styles and shortcomings observed in real educational practice and are instantiated via prompt engineering. We further propose a pedagogical risk rubric aligned with established educational standards, operationalizing five complementary risk dimensions: factual correctness, explanatory depth and completeness, focus and relevance, student-level appropriateness, and ideological bias. All explanations are annotated with binary risk labels through a semi-automatic process with expert teacher review. Finally, we present preliminary validation experiments to assess the suitability of EduEVAL-DB for evaluation. We benchmark a state-of-the-art education-oriented model (Gemini 2.5 Pro) against a lightweight local Llama 3.1 8B model and examine whether supervised fine-tuning on EduEVAL-DB supports pedagogical risk detection using models deployable on consumer hardware.

</details>


### [78] [Quantifying construct validity in large language model evaluations](https://arxiv.org/abs/2602.15532)
*Ryan Othniel Kearns*

Main category: cs.AI

TL;DR: 文章提出了结构化能力模型，这是一种新的方法，能够从大规模LLM基准测试结果中提取可解释和通用的能力。该模型在数据简化度量和外部基准预测方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 作者认为现有的基准测试方法（如潜在因子模型和标度定律）无法准确分离模型规模与能力，因此它们不能可靠的评估LLM的构建效度。

Method: 文章提出了结构化能力模型，结合了标度定律中模型规模对能力的指导作用和潜在因子模型中考虑测量误差的方式。

Result: 实验结果表明，结构化能力模型在简化数据指标和跨分布基准预测方面优于其他方法。

Conclusion: 结构化能力模型提供了更优越的解释力和预测能力，适用于量化LLM评估中的构建效度。

Abstract: The LLM community often reports benchmark results as if they are synonymous with general model capabilities. However, benchmarks can have problems that distort performance, like test set contamination and annotator error. How can we know that a benchmark is a reliable indicator of some capability that we want to measure? This question concerns the construct validity of LLM benchmarks, and it requires separating benchmark results from capabilities when we model and predict LLM performance.
  Both social scientists and computer scientists propose formal models - latent factor models and scaling laws - for identifying the capabilities underlying benchmark scores. However, neither technique is satisfactory for construct validity. Latent factor models ignore scaling laws, and as a result, the capabilities they extract often proxy model size. Scaling laws ignore measurement error, and as a result, the capabilities they extract are both uninterpretable and overfit to the observed benchmarks.
  This thesis presents the structured capabilities model, the first model to extract interpretable and generalisable capabilities from a large collection of LLM benchmark results. I fit this model and its two alternatives on a large sample of results from the OpenLLM Leaderboard. Structured capabilities outperform latent factor models on parsimonious fit indices, and exhibit better out-of-distribution benchmark prediction than scaling laws. These improvements are possible because neither existing approach separates model scale from capabilities in the appropriate way. Model scale should inform capabilities, as in scaling laws, and these capabilities should inform observed results up to measurement error, as in latent factor models. In combining these two insights, structured capabilities demonstrate better explanatory and predictive power for quantifying construct validity in LLM evaluations.

</details>


### [79] [RUVA: Personalized Transparent On-Device Graph Reasoning](https://arxiv.org/abs/2602.15553)
*Gabriele Conte,Alessio Mattiace,Gianni Carmosino,Potito Aghilar,Giovanni Servedio,Francesco Musicco,Vito Walter Anelli,Tommaso Di Noia,Francesco Maria Donini*

Main category: cs.AI

TL;DR: 该研究提出Ruva，一种首位的‘玻璃盒’架构，旨在实现人类在环的记忆管理。Ruva利用个人知识图谱取代向量匹配，使用户能够审视AI掌握的信息并精确删除特定事实，从而实现'被遗忘的权利'。


<details>
  <summary>Details</summary>
Motivation: 当前的个人AI系统大多采用‘黑盒’检索增强生成技术，存在透明度不足、隐私保护欠缺等问题。现有的向量数据库无法在数据错误或敏感信息被意外检索时提供足够的透明性和纠正机制。

Method: Ruva通过构建个人知识图谱，使用户能够直观地了解和操控AI所掌握的信息。该方法改变了传统的向量匹配技术，转向基于图的推理机制，以确保更加精确的数据删除和隐私保护。

Result: Ruva提出了一个创新的个人AI架构，展现了用户可以直接编辑他们生活数据的可能性。该系统通过知识图谱模型提升了用户对数据的理解和控制能力，相比传统方法提供了更高的透明度和隐私安全性。

Conclusion: Ruva旨在为用户提供一个更加透明、可控的个人AI环境，使得用户能够更好地管理自己的数据和信息，实现真正的'被遗忘的权利'。

Abstract: The Personal AI landscape is currently dominated by "Black Box" Retrieval-Augmented Generation. While standard vector databases offer statistical matching, they suffer from a fundamental lack of accountability: when an AI hallucinates or retrieves sensitive data, the user cannot inspect the cause nor correct the error. Worse, "deleting" a concept from a vector space is mathematically imprecise, leaving behind probabilistic "ghosts" that violate true privacy. We propose Ruva, the first "Glass Box" architecture designed for Human-in-the-Loop Memory Curation. Ruva grounds Personal AI in a Personal Knowledge Graph, enabling users to inspect what the AI knows and to perform precise redaction of specific facts. By shifting the paradigm from Vector Matching to Graph Reasoning, Ruva ensures the "Right to be Forgotten." Users are the editors of their own lives; Ruva hands them the pen. The project and the demo video are available at http://sisinf00.poliba.it/ruva/.

</details>


### [80] [How Vision Becomes Language: A Layer-wise Information-Theoretic Analysis of Multimodal Reasoning](https://arxiv.org/abs/2602.15580)
*Hongxuan Wu,Yukun Zhang,Xueqing Zhou*

Main category: cs.AI

TL;DR: 研究提出了一种基于部分信息分解(PID)和PID Flow的方法，以分析多模态Transformer中视觉证据、语言推理和跨模态计算在不同层数上的贡献，并通过LLaVA-1.5-7B和LLaVA-1.6-7B模型在六个GQA推理任务中的表现，发现视觉独特信息在早期层中达到峰值并在深层逐渐下降，语言独特信息在晚期层作用显著并占最终预测的约82%，而跨模态协同作用始终低于2%。这种模式在模型变体中具有高度稳定性，但在任务上依赖性强。


<details>
  <summary>Details</summary>
Motivation: 探索多模态Transformer在视觉问题回答时，预测是由视觉证据、语言推理还是真正的跨模态计算驱动的，以及这些驱动如何随层的变化。

Method: 采用了基于PID的叠层框架，并引入了PID Flow，结合降维、归一化流动高斯化和封闭形式高斯PID估计，来分析不同层次的信息分解。通过在LLaVA-1.5-7B和LLaVA-1.6-7B模型上应用该方法在六个GQA推理任务中的表现，以及执行针对性的图像到问题注意力击键实验来建立因果关系。

Result: 结果显示了一种模式：视觉独特信息在早期层中达到峰值并在深层逐渐下降；语言独特信息在晚期层显著且占最终预测的约82%；跨模态协同作用始终低于2%。这种模式在模型变体中高度稳定，但在任务上依赖性强。执行的击键实验还展示了特定于图像的问题注意力中断对视觉独特信息被困、补偿协同和总信息成本的影响。

Conclusion: 研究表明，视觉信息在多模态Transformer中的转换路径受到跨模态信息和语言信息的强烈影响，从而提供了关于这些架构的关键瓶颈的定量指导，这些瓶颈导致模态特定信息的丢失。

Abstract: When a multimodal Transformer answers a visual question, is the prediction driven by visual evidence, linguistic reasoning, or genuinely fused cross-modal computation -- and how does this structure evolve across layers? We address this question with a layer-wise framework based on Partial Information Decomposition (PID) that decomposes the predictive information at each Transformer layer into redundant, vision-unique, language-unique, and synergistic components. To make PID tractable for high-dimensional neural representations, we introduce \emph{PID Flow}, a pipeline combining dimensionality reduction, normalizing-flow Gaussianization, and closed-form Gaussian PID estimation. Applying this framework to LLaVA-1.5-7B and LLaVA-1.6-7B across six GQA reasoning tasks, we uncover a consistent \emph{modal transduction} pattern: visual-unique information peaks early and decays with depth, language-unique information surges in late layers to account for roughly 82\% of the final prediction, and cross-modal synergy remains below 2\%. This trajectory is highly stable across model variants (layer-wise correlations $>$0.96) yet strongly task-dependent, with semantic redundancy governing the detailed information fingerprint. To establish causality, we perform targeted Image$\rightarrow$Question attention knockouts and show that disrupting the primary transduction pathway induces predictable increases in trapped visual-unique information, compensatory synergy, and total information cost -- effects that are strongest in vision-dependent tasks and weakest in high-redundancy tasks. Together, these results provide an information-theoretic, causal account of how vision becomes language in multimodal Transformers, and offer quantitative guidance for identifying architectural bottlenecks where modality-specific information is lost.

</details>


### [81] [On inferring cumulative constraints](https://arxiv.org/abs/2602.15635)
*Konstantin Sidorov*

Main category: cs.AI

TL;DR: 该研究提出了一种预处理方法，用于推断额外的累积约束，这些约束捕捉了多资源交互而不进行搜索时的探查。实验表明，这些推断出的约束能改进搜索性能并收紧有利实例的目标界，而对不利实例的性能影响较小。此外，研究发现了25个新的下界和5个新的最优解。


<details>
  <summary>Details</summary>
Motivation: 在调度问题中，累积约束对于捕捉多资源交互至关重要，但常规的处理方式会导致性能下降。本文提出的方法旨在解决这一问题。

Method: 通过将累积约束视为占用向量上的线性不等式，该方法通过发现覆盖集、增强覆盖不等式和注入结果约束实现。

Result: 实验结果显示：推断出的约束能改善搜索性能并收紧有利实例的目标界；同时对不利实例影响较小。此外，还发现了25个新的下界和5个新的最优解，其中8个下界直接来自推断出的约束。

Conclusion: 此方法为累积约束传播提供了新的处理方式，对于处理多资源交互和改进调度问题求解性能具有一定的效果。

Abstract: Cumulative constraints are central in scheduling with constraint programming, yet propagation is typically performed per constraint, missing multi-resource interactions and causing severe slowdowns on some benchmarks. I present a preprocessing method for inferring additional cumulative constraints that capture such interactions without search-time probing. This approach interprets cumulative constraints as linear inequalities over occupancy vectors and generates valid inequalities by (i) discovering covers, the sets of tasks that cannot run in parallel, (ii) strengthening the cover inequalities for the discovered sets with lifting, and (iii) injecting the resulting constraints back into the scheduling problem instance. Experiments on standard RCPSP and RCPSP/max test suites show that these inferred constraints improve search performance and tighten objective bounds on favorable instances, while incurring little degradation on unfavorable ones. Additionally, these experiments discover 25 new lower bounds and five new best solutions; eight of the lower bounds are obtained directly from the inferred constraints.

</details>


### [82] [CARE Drive A Framework for Evaluating Reason-Responsiveness of Vision Language Models in Automated Driving](https://arxiv.org/abs/2602.15645)
*Lucas Elbert Suryana,Farah Bierenga,Sanne van Buuren,Pepijn Kooij,Elsefien Tulleners,Federico Scari,Simeon Calvert,Bart van Arem,Arkady Zgonnikov*

Main category: cs.AI

TL;DR: 提出了CARE Drive框架，用于评估视听说觉模型在自动驾驶中决策的合理性响应能力，通过对照实验评估人类原因对决策行为的影响，展示了模型决策与人类原因的显著联系，并表明这种响应性在不同上下文因素下表现不一。


<details>
  <summary>Details</summary>
Motivation: 现有的评价方法主要基于结果来评估模型性能，但忽略了模型决策是否真实反映了人类相关的考量。特别是在安全关键领域，这种方法可能导致误判，因此提出了CARE Drive框架，以更真实地评估模型决策的合理性。

Method: CARE Drive框架包括两阶段评估过程：首先是提示校准确保模型输出稳定；其次是系统地改变上下文以测量决策对人类原因的变化敏感性，如安全余量、社交压力和效率约束。

Result: 实验结果表明，明确的人类原因显著影响了模型的决策，提高了与专家推荐行为的一致性。然而，这种响应性在不同上下文因素下的表现各不相同，显示了对不同类型原因的不同敏感度。

Conclusion: CARE Drive框架提供了系统化评估基础模型合理性响应能力的方法，无需修改模型参数即能获得有关人类原因如何影响模型决策的实证证据。

Abstract: Foundation models, including vision language models, are increasingly used in automated driving to interpret scenes, recommend actions, and generate natural language explanations. However, existing evaluation methods primarily assess outcome based performance, such as safety and trajectory accuracy, without determining whether model decisions reflect human relevant considerations. As a result, it remains unclear whether explanations produced by such models correspond to genuine reason responsive decision making or merely post hoc rationalizations. This limitation is especially significant in safety critical domains because it can create false confidence. To address this gap, we propose CARE Drive, Context Aware Reasons Evaluation for Driving, a model agnostic framework for evaluating reason responsiveness in vision language models applied to automated driving. CARE Drive compares baseline and reason augmented model decisions under controlled contextual variation to assess whether human reasons causally influence decision behavior. The framework employs a two stage evaluation process. Prompt calibration ensures stable outputs. Systematic contextual perturbation then measures decision sensitivity to human reasons such as safety margins, social pressure, and efficiency constraints. We demonstrate CARE Drive in a cyclist overtaking scenario involving competing normative considerations. Results show that explicit human reasons significantly influence model decisions, improving alignment with expert recommended behavior. However, responsiveness varies across contextual factors, indicating uneven sensitivity to different types of reasons. These findings provide empirical evidence that reason responsiveness in foundation models can be systematically evaluated without modifying model parameters.

</details>


### [83] [PERSONA: Dynamic and Compositional Inference-Time Personality Control via Activation Vector Algebra](https://arxiv.org/abs/2602.15669)
*Xiachong Feng,Liang Zhao,Weihong Zhong,Yichong Huang,Yuxuan Gu,Lingpeng Kong,Xiaocheng Feng,Bing Qin*

Main category: cs.AI

TL;DR: 本研究提出了一种名为PERSONA的框架，通过直接在激活空间操作人格向量实现了类似于细调的效果，无需训练，能够在多个基准测试中达到优秀的性能。


<details>
  <summary>Details</summary>
Motivation: 当前的语言模型在人格控制方面依赖于静态提示或昂贵的细调，无法捕捉到人类特质的动态和组合特征。因此，开发一种无需训练的框架来实现类似细调的效果是有必要的。

Method: 该方法分为三个阶段：Persona-Base通过对比激活分析提取正交特质向量；Persona-Algebra通过向量算术实现精确控制（包括强度的标量乘法、组成的向量加法和抑制的向量减法）； Persona-Flow在推理过程中动态组合这些向量以实现上下文感知的适应。

Result: 在PersonalityBench中，该方法的平均得分为9.60，几乎达到了监督细调的上限9.61，而无任何梯度更新。在我们提出的Persona-Evolve基准测试中，方法在多种模型家族中实现了高达91%的胜率。

Conclusion: 结果表明，语言模型的人格特征具有数学可操作性，为可解释和高效的调控行为提供了新的研究方向。

Abstract: Current methods for personality control in Large Language Models rely on static prompting or expensive fine-tuning, failing to capture the dynamic and compositional nature of human traits. We introduce PERSONA, a training-free framework that achieves fine-tuning level performance through direct manipulation of personality vectors in activation space. Our key insight is that personality traits appear as extractable, approximately orthogonal directions in the model's representation space that support algebraic operations. The framework operates through three stages: Persona-Base extracts orthogonal trait vectors via contrastive activation analysis; Persona-Algebra enables precise control through vector arithmetic (scalar multiplication for intensity, addition for composition, subtraction for suppression); and Persona-Flow achieves context-aware adaptation by dynamically composing these vectors during inference. On PersonalityBench, our approach achieves a mean score of 9.60, nearly matching the supervised fine-tuning upper bound of 9.61 without any gradient updates. On our proposed Persona-Evolve benchmark for dynamic personality adaptation, we achieve up to 91% win rates across diverse model families. These results provide evidence that aspects of LLM personality are mathematically tractable, opening new directions for interpretable and efficient behavioral control.

</details>


### [84] [Recursive Concept Evolution for Compositional Reasoning in Large Language Models](https://arxiv.org/abs/2602.15725)
*Sarim Chaudhry*

Main category: cs.AI

TL;DR: RCE 是一种框架，允许预训练语言模型在推理过程中动态修改其内部表示几何结构。通过引入动态生成的概念子空间，RCE 使模型能够构建新的抽象，而非仅仅重新组合现有的抽象，从而在多个复杂推理基准测试中显著提高性能。


<details>
  <summary>Details</summary>
Motivation: 当前预训练语言模型在需要组合推理的任务中表现不佳。现有改进方法通过扩展链式思考提示、自我一致性或强化学习，增强了推理能力，但未能解决因缺乏编码在内部表示空间中的必要抽象而导致的性能下降问题。

Method: RCE 框架通过在推理期间动态生成低秩概念子空间、检测表示不足时生成子空间、通过最小描述长度准则进行筛选、融合协同子空间以及通过约束优化进行合并，从而动态修改模型的内部表示几何结构。这个过程允许模型构建新的抽象，而不是重新组合现有的抽象。

Result: RCE 在多个复杂推理基准测试中表现出色，特别是在 MATH 和 HLE 上减少了深度诱导的错误，在 ARC-AGI-2、GPQA 和 BBH 中分别提高了 12-18 分和 8-14 分。

Conclusion: RCE 框架为预训练语言模型提供了一种在推理过程中动态调整其内部表示的方法，这有助于解决多项任务中的组合推理难题，显著提升了模型的推理能力。

Abstract: Large language models achieve strong performance on many complex reasoning tasks, yet their accuracy degrades sharply on benchmarks that require compositional reasoning, including ARC-AGI-2, GPQA, MATH, BBH, and HLE. Existing methods improve reasoning by expanding token-level search through chain-of-thought prompting, self-consistency, or reinforcement learning, but they leave the model's latent representation space fixed. When the required abstraction is not already encoded in this space, performance collapses. We propose Recursive Concept Evolution (RCE), a framework that enables pretrained language models to modify their internal representation geometry during inference. RCE introduces dynamically generated low-rank concept subspaces that are spawned when representational inadequacy is detected, selected through a minimum description length criterion, merged when synergistic, and consolidated via constrained optimization to preserve stability. This process allows the model to construct new abstractions rather than recombining existing ones. We integrate RCE with Mistral-7B and evaluate it across compositional reasoning benchmarks. RCE yields 12-18 point gains on ARC-AGI-2, 8-14 point improvements on GPQA and BBH, and consistent reductions in depth-induced error on MATH and HLE.

</details>


### [85] [GlobeDiff: State Diffusion Process for Partial Observability in Multi-Agent Systems](https://arxiv.org/abs/2602.15776)
*Yiqin Yang,Xu Yang,Yuhua Jiang,Ni Mu,Hao Hu,Runpeng Xie,Ziyou Zhang,Siyuan Li,Yuan-Hua Ni,Qianchuan Zhao,Bo Xu*

Main category: cs.AI

TL;DR: 提出了Global State Diffusion Algorithm (GlobeDiff)来解决多智能体系统中的部分可观测性问题，该算法通过多模态扩散过程来推测全局状态，并在实验中证明了其准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前的多智能体系统方法，例如基于信念状态的估计和智能体间通信，在处理部分可观测性时存在局限性。GlobeDiff算法旨在通过综合利用局部观测信息和全局信息来改进现有的解决方案。

Method: GlobeDiff算法将状态推理过程建模为多模态扩散过程，既能解决状态估计中的模糊性问题，又能高精度地推断全局状态。通过数学证明，它在单模态和多模态分布下的估计误差可以被限制。

Result: GlobeDiff算法已经被证明能够在理论上和实验上有效地估计全局状态，展现出了优异的性能和高度的鲁棒性。

Conclusion: GlobeDiff算法为部分可观测环境中的多智能体协作提供了一种新的有效方法，有望促进相关领域的进一步研究和应用。

Abstract: In the realm of multi-agent systems, the challenge of \emph{partial observability} is a critical barrier to effective coordination and decision-making. Existing approaches, such as belief state estimation and inter-agent communication, often fall short. Belief-based methods are limited by their focus on past experiences without fully leveraging global information, while communication methods often lack a robust model to effectively utilize the auxiliary information they provide. To solve this issue, we propose Global State Diffusion Algorithm~(GlobeDiff) to infer the global state based on the local observations. By formulating the state inference process as a multi-modal diffusion process, GlobeDiff overcomes ambiguities in state estimation while simultaneously inferring the global state with high fidelity. We prove that the estimation error of GlobeDiff under both unimodal and multi-modal distributions can be bounded. Extensive experimental results demonstrate that GlobeDiff achieves superior performance and is capable of accurately inferring the global state.

</details>


### [86] [This human study did not involve human subjects: Validating LLM simulations as behavioral evidence](https://arxiv.org/abs/2602.15785)
*Jessica Hullman,David Broska,Huaman Sun,Aaron Shaw*

Main category: cs.AI

TL;DR: 该研究对比了使用大型语言模型（LLMs）作为合成参与者进行社会科学研究的两种策略：启发式方法和统计校准。启发式方法通过提示工程和模型微调来提高模拟与观察行为的一致性，但缺乏统计保证，适用于探索性研究而非确认性研究。统计校准则结合辅助的人类数据和统计调整来减轻模拟和观察之间的差异，从而在更低的成本下获得更为精准的因果效应估计。然而，这些方法的有效性取决于LLMs对相关人群的模拟程度。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在为在社会科学研究中使用大型语言模型作为合成参与者的行为提供指导，特别是在有效估计因果效应方面。

Method: 研究对比了启发式方法和统计校准两种策略，探讨了各自的适用性和局限性。

Result: 启发式方法在探索性研究中有效，但缺乏正式的统计保证；统计校准在满足特定假设时提供更精确的因果效应估计，成本更低。研究还指出了专注于用LLMs替代人类参与者可能忽视的机会。

Conclusion: 研究结论强调，选择使用LLMs的策略应基于研究的具体需求和应用场景，并明确LLMs与目标人群的匹配程度对于策略的有效性至关重要。

Abstract: A growing literature uses large language models (LLMs) as synthetic participants to generate cost-effective and nearly instantaneous responses in social science experiments. However, there is limited guidance on when such simulations support valid inference about human behavior. We contrast two strategies for obtaining valid estimates of causal effects and clarify the assumptions under which each is suitable for exploratory versus confirmatory research. Heuristic approaches seek to establish that simulated and observed human behavior are interchangeable through prompt engineering, model fine-tuning, and other repair strategies designed to reduce LLM-induced inaccuracies. While useful for many exploratory tasks, heuristic approaches lack the formal statistical guarantees typically required for confirmatory research. In contrast, statistical calibration combines auxiliary human data with statistical adjustments to account for discrepancies between observed and simulated responses. Under explicit assumptions, statistical calibration preserves validity and provides more precise estimates of causal effects at lower cost than experiments that rely solely on human participants. Yet the potential of both approaches depends on how well LLMs approximate the relevant populations. We consider what opportunities are overlooked when researchers focus myopically on substituting LLMs for human participants in a study.

</details>


### [87] [Enhancing Building Semantics Preservation in AI Model Training with Large Language Model Encodings](https://arxiv.org/abs/2602.15791)
*Suhyung Jang,Ghang Lee,Jaekun Lee,Hyunjun Lee*

Main category: cs.AI

TL;DR: 本文提出了一种新颖的训练方法，通过使用大型语言模型（LLM）嵌入（例如OpenAI GPT和Meta LLaMA）作为编码来保留建筑语义中的细微差别，从而提升AI对建筑复杂且特定领域的语义理解。


<details>
  <summary>Details</summary>
Motivation: 传统的编码方法（例如one-hot）在表示相似子类间的细微差别时能力有限，本文旨在开发一种新的训练方式以改善这个问题。

Method: 本文采用大型语言模型（LLM）嵌入作为编码方式，测试了不同维度的嵌入（原始高维LLM嵌入1,536、3,072或4,096和Matryoshka表示模型生成的1,024维紧凑嵌入），并通过GraphSAGE模型对5个高层住宅建筑信息模型（BIMs）中的42种建筑对象子类型进行分类。

Result: LLM嵌入方法在不同维度下均优于传统的一热编码方法，特别是lma-3（紧凑）嵌入达到了0.8766的加权平均F1分数，而一热编码为0.8475。

Conclusion: 研究结果显示，基于LLM的编码方法有助于增强AI在处理复杂领域特定建筑语义方面的理解能力。未来，随着LLM能力和降维技术的进步，这种策略有潜力在建筑、工程、制造和运营（AECO）行业中广泛应用于语义细化任务。

Abstract: Accurate representation of building semantics, encompassing both generic object types and specific subtypes, is essential for effective AI model training in the architecture, engineering, construction, and operation (AECO) industry. Conventional encoding methods (e.g., one-hot) often fail to convey the nuanced relationships among closely related subtypes, limiting AI's semantic comprehension. To address this limitation, this study proposes a novel training approach that employs large language model (LLM) embeddings (e.g., OpenAI GPT and Meta LLaMA) as encodings to preserve finer distinctions in building semantics. We evaluated the proposed method by training GraphSAGE models to classify 42 building object subtypes across five high-rise residential building information models (BIMs). Various embedding dimensions were tested, including original high-dimensional LLM embeddings (1,536, 3,072, or 4,096) and 1,024-dimensional compacted embeddings generated via the Matryoshka representation model. Experimental results demonstrated that LLM encodings outperformed the conventional one-hot baseline, with the llama-3 (compacted) embedding achieving a weighted average F1-score of 0.8766, compared to 0.8475 for one-hot encoding. The results underscore the promise of leveraging LLM-based encodings to enhance AI's ability to interpret complex, domain-specific building semantics. As the capabilities of LLMs and dimensionality reduction techniques continue to evolve, this approach holds considerable potential for broad application in semantic elaboration tasks throughout the AECO industry.

</details>


### [88] [Developing AI Agents with Simulated Data: Why, what, and how?](https://arxiv.org/abs/2602.15816)
*Xiaoran Liu,Istvan David*

Main category: cs.AI

TL;DR: 该论文介绍了基于模拟的合成数据生成技术，旨在解决现代非符号AI技术面临的数据量不足和数据质量差的问题。文章还提供了一个框架来描述和分析以数字孪生为基础的AI模拟解决方案。


<details>
  <summary>Details</summary>
Motivation: 由于现代非符号AI技术的数据需求，以及中心化和数据质量问题，迫切需要高效、高质量的合成数据生成技术。因此，本文探讨了基于模拟的方法，因为它能够提供多样化的合成数据，并对AI训练非常有用。

Method: 本文没有详细描述具体的方法，而是概述了模拟在合成数据生成中的应用，并提供了一个参考框架来设计和分析数字孪生的AI模拟解决方案。

Result: 本文主要介绍了利用模拟生成合成数据的概念、优点和挑战，并提出了一种框架来描述和分析数字孪生的AI模拟解决方案，但它没有给出实际的结果或实验数据。

Conclusion: 本文得出了对于基于模拟的合成数据生成在AI训练中的重要性结论，并提出了一种设计和分析数字孪生AI模拟解决方案的框架。

Abstract: As insufficient data volume and quality remain the key impediments to the adoption of modern subsymbolic AI, techniques of synthetic data generation are in high demand. Simulation offers an apt, systematic approach to generating diverse synthetic data. This chapter introduces the reader to the key concepts, benefits, and challenges of simulation-based synthetic data generation for AI training purposes, and to a reference framework to describe, design, and analyze digital twin-based AI simulation solutions.

</details>
