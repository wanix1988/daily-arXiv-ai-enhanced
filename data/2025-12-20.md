<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 23]
- [cs.CL](#cs.CL) [Total: 5]
- [cs.AI](#cs.AI) [Total: 3]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [GeoPredict: Leveraging Predictive Kinematics and 3D Gaussian Geometry for Precise VLA Manipulation](https://arxiv.org/abs/2512.16811)
*Jingjing Qian,Boyao Han,Chen Shi,Lei Xiao,Long Yang,Shaoshuai Shi,Li Jiang*

Main category: cs.CV

TL;DR: GeoPredict 提出了一种几何感知的 VLA 框架，通过引入轨迹模块和预测的3D几何模块，增强连续动作策略，以实现更精准的3D推理，从而改善机器人操作任务表现。


<details>
  <summary>Details</summary>
Motivation: 现有的 VLA 模型在机器人操作任务中表现良好，但主要作为反应式的2D处理方式，缺乏对精确的3D推理的支持，尤其是在需要精确的3D推理的复杂任务中表现不佳。

Method: GeoPredict 使用了一个轨迹模块和一个预测的3D几何模块。轨迹模块编码运动历史并预测多步机器人手臂的3D关键点轨迹。3D几何模块则使用轨迹指引进行未来关键点轨迹的预测强化，提供未来工作空间几何的预测。这些预测模块在训练阶段用于监督，但在推理阶段无需复杂的3D解码，仅需轻量的查询令牌。

Result: 实验证实在 RoboCasa Human-50，LIBERO 和真实世界的操作任务中，GeoPredict 比强大的 VLA 基线模型更优，特别是在涉及几何的任务和需要大量空间推理的情况下。

Conclusion: GeoPredict 框架通过结合几何感知和预测机制显著提升了 VLA 模型在3D操作任务中的性能。

Abstract: Vision-Language-Action (VLA) models achieve strong generalization in robotic manipulation but remain largely reactive and 2D-centric, making them unreliable in tasks that require precise 3D reasoning. We propose GeoPredict, a geometry-aware VLA framework that augments a continuous-action policy with predictive kinematic and geometric priors. GeoPredict introduces a trajectory-level module that encodes motion history and predicts multi-step 3D keypoint trajectories of robot arms, and a predictive 3D Gaussian geometry module that forecasts workspace geometry with track-guided refinement along future keypoint trajectories. These predictive modules serve exclusively as training-time supervision through depth-based rendering, while inference requires only lightweight additional query tokens without invoking any 3D decoding. Experiments on RoboCasa Human-50, LIBERO, and real-world manipulation tasks show that GeoPredict consistently outperforms strong VLA baselines, especially in geometry-intensive and spatially demanding scenarios.

</details>


### [2] [DenseBEV: Transforming BEV Grid Cells into 3D Objects](https://arxiv.org/abs/2512.16818)
*Marius Dähling,Sebastian Krebs,J. Marius Zöllner*

Main category: cs.CV

TL;DR: 本文提出了一种直接利用BEV特征单元作为锚点的端到端方法，在不进行后处理的情况下通过在大量查询中应用BEV非极大值抑制确保高效训练，并且通过引入混合时序建模方法进一步提高了检测性能。


<details>
  <summary>Details</summary>
Motivation: 当前的多摄像机3D目标检测研究中，BEV基变体经常被用来替代传统模型中随机查询作为锚点的方法。不过，传统的方法是对这些随机查询进行优化，而本文试图通过使用直接作为锚点的BEV特征单元来提供一种更加直观且有效的方案。

Method: 本文提出了一种名为DenseBEV的新方法。该方法直接使用BEV特征单元作为锚点，提出了一种基于BEV非极大值抑制的两阶段锚点生成方法，以避免大规模查询带来的注意机制缩放问题，同时通过在检测查询中嵌入时序BEV信息以增强检测性能，引入了混合时序建模方法。

Result: 在nuScenes数据集下，DenseBEV方法在NDS和mAP上表现出一致且显著的改进，尤其在小目标如行人检测方面效果显著，mAP提高了3.8%，LET-mAP提高了8%。并在Waymo Open数据集上达到了60.7%的LET-mAP，超越了之前的最佳性能5.4%。

Conclusion: DenseBEV方法通过上述方法在多摄像机3D目标检测方面取得了较大进展，尤其是在对小目标的检测上表现突出，并在公开数据集Waymo Open上达到了最新的性能水平。

Abstract: In current research, Bird's-Eye-View (BEV)-based transformers are increasingly utilized for multi-camera 3D object detection. Traditional models often employ random queries as anchors, optimizing them successively. Recent advancements complement or replace these random queries with detections from auxiliary networks. We propose a more intuitive and efficient approach by using BEV feature cells directly as anchors. This end-to-end approach leverages the dense grid of BEV queries, considering each cell as a potential object for the final detection task. As a result, we introduce a novel two-stage anchor generation method specifically designed for multi-camera 3D object detection. To address the scaling issues of attention with a large number of queries, we apply BEV-based Non-Maximum Suppression, allowing gradients to flow only through non-suppressed objects. This ensures efficient training without the need for post-processing. By using BEV features from encoders such as BEVFormer directly as object queries, temporal BEV information is inherently embedded. Building on the temporal BEV information already embedded in our object queries, we introduce a hybrid temporal modeling approach by integrating prior detections to further enhance detection performance. Evaluating our method on the nuScenes dataset shows consistent and significant improvements in NDS and mAP over the baseline, even with sparser BEV grids and therefore fewer initial anchors. It is particularly effective for small objects, enhancing pedestrian detection with a 3.8% mAP increase on nuScenes and an 8% increase in LET-mAP on Waymo. Applying our method, named DenseBEV, to the challenging Waymo Open dataset yields state-of-the-art performance, achieving a LET-mAP of 60.7%, surpassing the previous best by 5.4%. Code is available at https://github.com/mdaehl/DenseBEV.

</details>


### [3] [Next-Generation License Plate Detection and Recognition System using YOLOv8](https://arxiv.org/abs/2512.16826)
*Arslan Amin,Rafia Mumtaz,Muhammad Jawad Bashir,Syed Mohammad Hassan Zaidi*

Main category: cs.CV

TL;DR: 本研究评估了YOLOv8在车牌识别和字符识别任务中的性能，通过两个不同的数据集进行了训练和测试，结果显示YOLOv8 Nano在车牌识别任务中表现突出，YOLOv8 Small在字符识别任务中表现优秀。研究还提出了一种基于x轴位置排序的字符排序方法，构建了YOLOv8 Nano用于车牌识别和YOLOv8 Small用于字符识别的优化管道，平衡了计算效率和高准确性，适用于边缘设备上的智能交通系统。


<details>
  <summary>Details</summary>
Motivation: 随着交通管理和车辆监控的需求日益增长，实时的车牌检测和识别技术显得尤为重要。尽管已有许多方法解决这一问题，但仍然缺乏在各种环境下保持一致高精度的解决方案。

Method: 本研究采用了YOLOv8系列的Nano和Small两种变体，分别针对车牌识别和字符识别任务进行测试。

Result: 实验结果显示，YOLOv8 Nano在车牌识别任务中展现出精度为0.964，mAP50为0.918；YOLOv8 Small在字符识别任务中的精度为0.92，mAP50为0.91。此外，还提出了一种基于字符在x轴位置排序的方法，优化了识别流程，提高了字符识别的准确性。

Conclusion: 该研究提出的方法不仅在计算效率和准确性上表现出色，还为未来的智能交通系统提供了坚实的框架，特别是在边缘计算设备的应用上。

Abstract: In the evolving landscape of traffic management and vehicle surveillance, efficient license plate detection and recognition are indispensable. Historically, many methodologies have tackled this challenge, but consistent real-time accuracy, especially in diverse environments, remains elusive. This study examines the performance of YOLOv8 variants on License Plate Recognition (LPR) and Character Recognition tasks, crucial for advancing Intelligent Transportation Systems. Two distinct datasets were employed for training and evaluation, yielding notable findings. The YOLOv8 Nano variant demonstrated a precision of 0.964 and mAP50 of 0.918 on the LPR task, while the YOLOv8 Small variant exhibited a precision of 0.92 and mAP50 of 0.91 on the Character Recognition task. A custom method for character sequencing was introduced, effectively sequencing the detected characters based on their x-axis positions. An optimized pipeline, utilizing YOLOv8 Nano for LPR and YOLOv8 Small for Character Recognition, is proposed. This configuration not only maintains computational efficiency but also ensures high accuracy, establishing a robust foundation for future real-world deployments on edge devices within Intelligent Transportation Systems. This effort marks a significant stride towards the development of smarter and more efficient urban infrastructures.

</details>


### [4] [Radiology Report Generation with Layer-Wise Anatomical Attention](https://arxiv.org/abs/2512.16841)
*Emmanuel D. Muñiz-De-León,Jorge A. Rosales-de-Golferichs,Ana S. Muñoz-Rodríguez,Alejandro I. Trejo-Castro,Eduardo de Avila-Armenta,Antonio Martínez-Torteya*

Main category: cs.CV

TL;DR: 该研究提出了一种紧凑的图像到文本架构，能够在单张胸片上生成胸部X光报告的‘发现’部分，通过融合自蒸馏与无标签v3视觉变换器编码器和增强解码器实现了明显性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的多模态系统依赖于大规模多模态训练、临床元数据和多种影像视图，资源密集且难以在大多数环境中应用。因此，该研究旨在提出一种更加紧凑且依赖单一影像输入的解决方案。

Method: 该方法结合了冻结的DINOv3视觉变换器编码器和通过层次高斯平滑集成肺和心部分割掩模的增强GPT-2解码器。解码器通过层级的解剖学注意力机制增强了对相关区域的关注。

Result: 在MIMIC-CXR数据集上的实验结果显示，该方法在CheXpert宏F1分数和Micro-F1分数上分别提高了168%和146%，在更广泛的14次观察结果中也提高了86%。此外，结构一致性通过RadGraph F1分数提高了9.7%。

Conclusion: 研究表明，解码器级别的解剖学引导能够提高空间定位能力并增强相关临床区域的连贯性。尽管模型较小且仅基于单张影像，但其性能表现表明多模态多层解剖注意力原则的有效性。

Abstract: Automatic radiology report generation is a promising application of multimodal deep learning, aiming to reduce reporting workload and improve consistency. However, current state-of-the-art (SOTA) systems - such as Multimodal AI for Radiology Applications (MAIRA-2) and Medical Pathways Language Model-Multimodal (MedPaLM-M) - depend on large-scale multimodal training, clinical metadata, and multiple imaging views, making them resource-intensive and inaccessible for most settings. We introduce a compact image-to-text architecture that generates the Findings section of chest X-ray reports from a single frontal image. The model combines a frozen Self-Distillation with No Labels v3 (DINOv3) Vision Transformer (ViT) encoder with a Generative Pre-trained Transformer 2 (GPT-2) decoder enhanced by layer-wise anatomical attention. This mechanism integrates lung and heart segmentation masks through hierarchical Gaussian smoothing, biasing attention toward clinically relevant regions without adding trainable parameters. Evaluated on the official Medical Information Mart for Intensive Care-Chest X-ray (MIMIC-CXR) dataset using Chest Radiograph Expert (CheXpert) and Radiology Graph (RadGraph) metrics, our approach achieved substantial gains: CheXpert Macro-F1 for five key pathologies increased by 168% (0.083 -> 0.238) and Micro-F1 by 146% (0.137 -> 0.337), while broader performance across 14 observations improved by 86% (0.170 -> 0.316). Structural coherence also improved, with RadGraph F1 rising by 9.7%. Despite its small size and purely image-conditioned design, the model demonstrates that decoder-level anatomical guidance improves spatial grounding and enhances coherence in clinically relevant regions. The source code is publicly available at: https://github.com/devMuniz02/UDEM-CXR-Reporting-Thesis-2025.

</details>


### [5] [OPENTOUCH: Bringing Full-Hand Touch to Real-World Interaction](https://arxiv.org/abs/2512.16842)
*Yuxin Ray Song,Jinzhou Li,Rao Fu,Devin Murphy,Kaichen Zhou,Rishi Shiv,Yaqi Li,Haoyu Xiong,Crystal Elaine Owens,Yilun Du,Yiyue Luo,Xianyi Cheng,Antonio Torralba,Wojciech Matusik,Paul Pu Liang*

Main category: cs.CV

TL;DR: 本研究介绍了首个野生环境下第一人称整手触觉数据集OpenTouch，包含5.1小时的同步视频-触觉-姿态数据以及详细的文本注释。利用此数据集，提出触觉检索和分类基准，展示了触觉信号在抓取理解、跨模态对齐和从野生视频查询中可靠检索方面的价值。通过发布该标注数据集和基准测试，旨在推动多模态第一人称感知、内置学习和富含接触的机器人操纵。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏可靠的穿戴式触觉传感器和已对齐第一人称视频与全手触觉的实际数据集。因此，我们旨在填补视觉感知与物理交互之间的空白，通过提出首个野生环境下第一人称整手触觉数据集OpenTouch来推进多模态第一人称感知、内置学习和富含接触的机器人操纵。

Method: 我们将5.1小时的同步视频-触觉-姿态数据和2,900个精心策划的剪辑结合并进行了详细的文字标注，构建了OpenTouch数据集。通过这一数据集，我们提出了触觉检索和分类基准，利用触觉信号来增强跨模态对齐。

Result: 研究表明，触觉信号在抓取理解、跨模态对齐和从野生视频查询中可靠检索等方面具备高效作用。这一成果展示了通过OpenTouch数据集，能够提升机器人对环境的理解和互动能力。

Conclusion: 通过发布OpenTouch数据集和基准测试，本研究有效促进了多模态第一人称感知、内置学习和富含接触的机器人操纵的发展。

Abstract: The human hand is our primary interface to the physical world, yet egocentric perception rarely knows when, where, or how forcefully it makes contact. Robust wearable tactile sensors are scarce, and no existing in-the-wild datasets align first-person video with full-hand touch. To bridge the gap between visual perception and physical interaction, we present OpenTouch, the first in-the-wild egocentric full-hand tactile dataset, containing 5.1 hours of synchronized video-touch-pose data and 2,900 curated clips with detailed text annotations. Using OpenTouch, we introduce retrieval and classification benchmarks that probe how touch grounds perception and action. We show that tactile signals provide a compact yet powerful cue for grasp understanding, strengthen cross-modal alignment, and can be reliably retrieved from in-the-wild video queries. By releasing this annotated vision-touch-pose dataset and benchmark, we aim to advance multimodal egocentric perception, embodied learning, and contact-rich robotic manipulation.

</details>


### [6] [GenEval 2: Addressing Benchmark Drift in Text-to-Image Evaluation](https://arxiv.org/abs/2512.16853)
*Amita Kamath,Kai-Wei Chang,Ranjay Krishna,Luke Zettlemoyer,Yushi Hu,Marjan Ghazvininejad*

Main category: cs.CV

TL;DR: 本文提出了一种新的评估方法Soft-TIFA，并且引入了改进的GenEval 2基准，以减少模型评估中的基准漂移问题。


<details>
  <summary>Details</summary>
Motivation: 当前Text-to-Image模型评估中的静态基准判断方法无法跟上新模型的能力，导致了基准漂移和不准确的模型评估。

Method: 本文通过创建一个新基准GenEval 2和软TIFA评估方法，改进了视觉概念的覆盖范围和复合性，并结合视觉原始概念来评估模型。

Result: GenEval 2和Soft-TIFA能够更好地与人类判断对齐，且具有更高的挑战性，减少模型评估中的基准漂移。

Conclusion: 文章强调了持续审计和改进对Text-to-Image和相关自动模型评估基准的重要性，指出了避免基准漂移的挑战性。

Abstract: Automating Text-to-Image (T2I) model evaluation is challenging; a judge model must be used to score correctness, and test prompts must be selected to be challenging for current T2I models but not the judge. We argue that satisfying these constraints can lead to benchmark drift over time, where the static benchmark judges fail to keep up with newer model capabilities. We show that benchmark drift is a significant problem for GenEval, one of the most popular T2I benchmarks. Although GenEval was well-aligned with human judgment at the time of its release, it has drifted far from human judgment over time -- resulting in an absolute error of as much as 17.7% for current models. This level of drift strongly suggests that GenEval has been saturated for some time, as we verify via a large-scale human study. To help fill this benchmarking gap, we introduce a new benchmark, GenEval 2, with improved coverage of primitive visual concepts and higher degrees of compositionality, which we show is more challenging for current models. We also introduce Soft-TIFA, an evaluation method for GenEval 2 that combines judgments for visual primitives, which we show is more well-aligned with human judgment and argue is less likely to drift from human-alignment over time (as compared to more holistic judges such as VQAScore). Although we hope GenEval 2 will provide a strong benchmark for many years, avoiding benchmark drift is far from guaranteed and our work, more generally, highlights the importance of continual audits and improvement for T2I and related automated model evaluation benchmarks.

</details>


### [7] [RePlan: Reasoning-guided Region Planning for Complex Instruction-based Image Editing](https://arxiv.org/abs/2512.16864)
*Tianyuan Qu,Lei Ke,Xiaohang Zhan,Longxiang Tang,Yuqi Liu,Bohao Peng,Bei Yu,Dong Yu,Jiaya Jia*

Main category: cs.CV

TL;DR: RePlan 是一种计划然后执行的框架，结合了视觉语言计划器和扩散编辑器，通过分步骤推理和明确地将指令关联到目标区域，实现了对复杂指令和模糊场景的高效处理，特别是在精细标注和知识密集型编辑方面表现突出。


<details>
  <summary>Details</summary>
Motivation: 现有模型在处理复杂指令和模糊场景时表现不佳，因此需要一种新的框架来提高指令执行的精度和可靠性。

Method: RePlan 使用视觉语言计划器分解复杂指令并通过训练-free 的注意力区域注入机制对目标区域进行编辑，同时结合 GRPO 基准化的强化学习提高模型的推理准确性和格式可靠性。

Result: RePlan 在复杂场景下表现出色，特别是在小规模数据集训练时，其性能优于强基线模型，在区域精度和整体逼真度上都有显著提升。

Conclusion: RePlan 提供了一种有效的解决方案，以处理复杂指令和模糊场景中的图像编辑问题，证明了该框架的有效性，并提出了 IV-Edit 作为专门针对这些任务的新基准。

Abstract: Instruction-based image editing enables natural-language control over visual modifications, yet existing models falter under Instruction-Visual Complexity (IV-Complexity), where intricate instructions meet cluttered or ambiguous scenes. We introduce RePlan (Region-aligned Planning), a plan-then-execute framework that couples a vision-language planner with a diffusion editor. The planner decomposes instructions via step-by-step reasoning and explicitly grounds them to target regions; the editor then applies changes using a training-free attention-region injection mechanism, enabling precise, parallel multi-region edits without iterative inpainting. To strengthen planning, we apply GRPO-based reinforcement learning using 1K instruction-only examples, yielding substantial gains in reasoning fidelity and format reliability. We further present IV-Edit, a benchmark focused on fine-grained grounding and knowledge-intensive edits. Across IV-Complex settings, RePlan consistently outperforms strong baselines trained on far larger datasets, improving regional precision and overall fidelity. Our project page: https://replan-iv-edit.github.io

</details>


### [8] [Pixel Seal: Adversarial-only training for invisible image and video watermarking](https://arxiv.org/abs/2512.16874)
*Tomáš Souček,Pierre Fernandez,Hady Elsahar,Sylvestre-Alvise Rebuffi,Valeriu Lacatusu,Tuan Tran,Tom Sander,Alexandre Mourachko*

Main category: cs.CV

TL;DR: 本文提出了一种名为Pixel Seal的新颖图像和视频水印方法，解决了现有方法存在的显著问题：感知损失、优化不稳定以及高分辨率下的鲁棒性和不可感知性降低。Pixel Seal通过对抗训练、分阶段训练和高分辨率适应技术提升了水印质量和适应性，极大地增强了水印系统的可靠性和实用性。


<details>
  <summary>Details</summary>
Motivation: 针对当前图像和视频水印技术在鲁棒性和不可感知性之间难以平衡的问题，以及训练模型遇到的优化不稳定和高分辨率下鲁棒性降低的挑战，本文提出了一种新技术Pixel Seal，以提升水印的质量和实用性。

Method: Pixel Seal采用了对抗训练、分阶段训练和高分辨率适应技术。对抗训练消除不稳定的像素级不可感知损失，分阶段训练通过分离鲁棒性和不可感知性来稳定收敛，而高分辨率适应则通过基于JND的衰减和训练时的上采样模拟来去除放大带来的问题。

Result: 实验结果显示，Pixel Seal在不同图像类型和多种变换下具有更好的鲁棒性和不可感知性，相比现有的最优方法有明显改进。此外，该模型还能够高效地应用于视频中，展示了强大的实用性和可扩展性。

Conclusion: Pixel Seal为图像和视频的鲁棒水印提供了一个强有力的新方案，能够在实际应用中确保数字内容的可信来源。

Abstract: Invisible watermarking is essential for tracing the provenance of digital content. However, training state-of-the-art models remains notoriously difficult, with current approaches often struggling to balance robustness against true imperceptibility. This work introduces Pixel Seal, which sets a new state-of-the-art for image and video watermarking. We first identify three fundamental issues of existing methods: (i) the reliance on proxy perceptual losses such as MSE and LPIPS that fail to mimic human perception and result in visible watermark artifacts; (ii) the optimization instability caused by conflicting objectives, which necessitates exhaustive hyperparameter tuning; and (iii) reduced robustness and imperceptibility of watermarks when scaling models to high-resolution images and videos. To overcome these issues, we first propose an adversarial-only training paradigm that eliminates unreliable pixel-wise imperceptibility losses. Second, we introduce a three-stage training schedule that stabilizes convergence by decoupling robustness and imperceptibility. Third, we address the resolution gap via high-resolution adaptation, employing JND-based attenuation and training-time inference simulation to eliminate upscaling artifacts. We thoroughly evaluate the robustness and imperceptibility of Pixel Seal on different image types and across a wide range of transformations, and show clear improvements over the state-of-the-art. We finally demonstrate that the model efficiently adapts to video via temporal watermark pooling, positioning Pixel Seal as a practical and scalable solution for reliable provenance in real-world image and video settings.

</details>


### [9] [Memory-Enhanced SAM3 for Occlusion-Robust Surgical Instrument Segmentation](https://arxiv.org/abs/2512.16880)
*Valay Bundele,Mehran Hosseinzadeh,Hendrik P. A. Lensch*

Main category: cs.CV

TL;DR: ReMeDI-SAM3 提出了一种无需训练的、基于记忆增强的手术器械分割方法，通过引入可选择记忆库、分段内插方案以及基于特征的重新识别模块，在 EndoVis17 和 EndoVis18 数据集上实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 针对传统方法 SAM3 在处理手术视频时遇到的混淆、长时间遮挡、强反射等因素的挑战，提出了 ReMeDI-SAM3 来改进其表现。

Method: ReMeDI-SAM3 通过多种机制增强 SAM3 的性能：一是引入了相关性感知记忆过滤，为每个遮挡区域维护一个特定的记忆区域；二是使用分段内插方案扩展了有效记忆容量；三是加入了基于特征的重新识别模块，结合时序投票，提高了遮挡后的身份识别准确性。

Result: 在 EndoVis17 和 EndoVis18 数据集上，ReMeDI-SAM3 实现了约 7% 和 16% 的绝对 mcIoU 提升，超过了包括训练依赖方法在内的现有方法。

Conclusion: ReMeDI-SAM3 成功解决了 SAM3 中的内存更新不分选择性、固定容量以及遮挡恢复问题，并通过广泛的实验证明了其有效性和效率。

Abstract: Accurate surgical instrument segmentation in endoscopic videos is crucial for computer-assisted interventions, yet remains challenging due to frequent occlusions, rapid motion, specular artefacts, and long-term instrument re-entry. While SAM3 provides a powerful spatio-temporal framework for video object segmentation, its performance in surgical scenes is limited by indiscriminate memory updates, fixed memory capacity, and weak identity recovery after occlusions. We propose ReMeDI-SAM3, a training-free memory-enhanced extension of SAM3, that addresses these limitations through three components: (i) relevance-aware memory filtering with a dedicated occlusion-aware memory for storing pre-occlusion frames, (ii) a piecewise interpolation scheme that expands the effective memory capacity, and (iii) a feature-based re-identification module with temporal voting for reliable post-occlusion identity disambiguation. Together, these components mitigate error accumulation and enable reliable recovery after occlusions. Evaluations on EndoVis17 and EndoVis18 under a zero-shot setting show absolute mcIoU improvements of around 7% and 16%, respectively, over vanilla SAM3, outperforming even prior training-based approaches. Project page: https://valaybundele.github.io/remedi-sam3/.

</details>


### [10] [M-PhyGs: Multi-Material Object Dynamics from Video](https://arxiv.org/abs/2512.16885)
*Norika Wada,Kohei Yamashita,Ryo Kawahara,Ko Nishino*

Main category: cs.CV

TL;DR: M-PhyGs 利用视频数据高效估计多材料物体的物理参数，特别关注花朵，引入了多材料物理高斯模型，同时实现了材料分割和参数恢复。


<details>
  <summary>Details</summary>
Motivation: 现有方法在估计现实世界物体的物理材料参数时受到均匀单一材料、预学习动力学或简化拓扑结构的限制。M-PhyGs 旨在解决现实世界中多材料物体的复杂性问题。

Method: M-PhyGs 通过一个从自然场景捕获的短视频，联合分割物体并将相似材料合并，同时考虑重力情境下的连续力学参数恢复。引入了3D和2D损失的级联机制及利用时间 minibatching 来提高效率。

Result: 实验结果表明，M-PhyGs 在 Phlowers 数据集上的表现准确且有效，其各组件均显示出良好的性能。

Conclusion: M-PhyGs 为多材料物体的物理参数估计提供了一种新的高效方法。'

Abstract: Knowledge of the physical material properties governing the dynamics of a real-world object becomes necessary to accurately anticipate its response to unseen interactions. Existing methods for estimating such physical material parameters from visual data assume homogeneous single-material objects, pre-learned dynamics, or simplistic topologies. Real-world objects, however, are often complex in material composition and geometry lying outside the realm of these assumptions. In this paper, we particularly focus on flowers as a representative common object. We introduce Multi-material Physical Gaussians (M-PhyGs) to estimate the material composition and parameters of such multi-material complex natural objects from video. From a short video captured in a natural setting, M-PhyGs jointly segments the object into similar materials and recovers their continuum mechanical parameters while accounting for gravity. M-PhyGs achieves this efficiently with newly introduced cascaded 3D and 2D losses, and by leveraging temporal mini-batching. We introduce a dataset, Phlowers, of people interacting with flowers as a novel platform to evaluate the accuracy of this challenging task of multi-material physical parameter estimation. Experimental results on Phlowers dataset demonstrate the accuracy and effectiveness of M-PhyGs and its components.

</details>


### [11] [LinkedOut: Linking World Knowledge Representation Out of Video LLM for Next-Generation Video Recommendation](https://arxiv.org/abs/2512.16891)
*Haichao Zhang,Yao Lu,Lichen Wang,Yunzhe Li,Daiwei Chen,Yunpeng Xu,Yun Fu*

Main category: cs.CV

TL;DR: LinkedOut 提出了一个能够直接从视频中提取 VLLM 世界知识的新表示方法，该方法能够在不使用手工标签的情况下处理原始帧，并且实现了视觉推荐任务中的最新成果。


<details>
  <summary>Details</summary>
Motivation: 随着视频理解任务如视频推荐的需求增长，当前的 VLLM 存在高延迟、不支持多视频输入以及丢弃视觉细节等问题。

Method: LinkedOut 使用 VLLM 从原始帧中提取语义支持、知识感知的令牌，并通过可提示的查询和可选的辅助模态进行指导。此外，它引入了一种跨层知识融合 MoE，从丰富的 VLLM 特征中选择合适的抽象层次，实现个性化、可解释和低延迟的推荐。

Result: LinkedOut 是首个能够在没有手工标签的情况下处理原始帧的基于 VLLM 的视频推荐方法，已在标准基准测试中达到最新水平。

Conclusion: LinkedOut 的研究为充分利用 VLLM 的世界知识先验和视觉推理来促进下游视觉任务提供了实用路径，并通过解释性研究和消除测试确认了层次多样性和按层融合的益处。

Abstract: Video Large Language Models (VLLMs) unlock world-knowledge-aware video understanding through pretraining on internet-scale data and have already shown promise on tasks such as movie analysis and video question answering. However, deploying VLLMs for downstream tasks such as video recommendation remains challenging, since real systems require multi-video inputs, lightweight backbones, low-latency sequential inference, and rapid response. In practice, (1) decode-only generation yields high latency for sequential inference, (2) typical interfaces do not support multi-video inputs, and (3) constraining outputs to language discards fine-grained visual details that matter for downstream vision tasks. We argue that these limitations stem from the absence of a representation that preserves pixel-level detail while leveraging world knowledge. We present LinkedOut, a representation that extracts VLLM world knowledge directly from video to enable fast inference, supports multi-video histories, and removes the language bottleneck. LinkedOut extracts semantically grounded, knowledge-aware tokens from raw frames using VLLMs, guided by promptable queries and optional auxiliary modalities. We introduce a cross-layer knowledge fusion MoE that selects the appropriate level of abstraction from the rich VLLM features, enabling personalized, interpretable, and low-latency recommendation. To our knowledge, LinkedOut is the first VLLM-based video recommendation method that operates on raw frames without handcrafted labels, achieving state-of-the-art results on standard benchmarks. Interpretability studies and ablations confirm the benefits of layer diversity and layer-wise fusion, pointing to a practical path that fully leverages VLLM world-knowledge priors and visual reasoning for downstream vision tasks such as recommendation.

</details>


### [12] [FlashPortrait: 6x Faster Infinite Portrait Animation with Adaptive Latent Prediction](https://arxiv.org/abs/2512.16900)
*Shuyuan Tu,Yueming Pan,Yinming Huang,Xintong Han,Zhen Xing,Qi Dai,Kai Qiu,Chong Luo,Zuxuan Wu*

Main category: cs.CV

TL;DR: FlashPortrait 是一种端到端的视频扩散转换器，能够生成保持身份一致性、无限长的视频，同时将推理速度提高至原来的6倍。


<details>
  <summary>Details</summary>
Motivation: 现有的基于扩散加速长画幅动画的方法难以保证身份一致性，FlashPortrait 提出了一种新的方法以解决这一问题。

Method: FlashPortrait 采用了一种基于扩散的模型，首先使用现成的提取器计算身份无关的面部表情特征，然后通过标准化面部特征与扩散潜在特征来提高身份稳定性。在推理阶段，通过动态滑动窗口方案和加权混合确保长动画中的平滑过渡和身份一致性。模型利用高阶潜在特征直接预测未来的潜在特征，从而跳过许多去噪步骤，实现6倍的加速。

Result: FlashPortrait 在基准测试中展现了良好的效果，无论是定性和定量上都证明了其有效性和优越性。

Conclusion: FlashPortrait 的提出为长绘画动画的高效生成和身份保持提供了一个新的解决方案。

Abstract: Current diffusion-based acceleration methods for long-portrait animation struggle to ensure identity (ID) consistency. This paper presents FlashPortrait, an end-to-end video diffusion transformer capable of synthesizing ID-preserving, infinite-length videos while achieving up to 6x acceleration in inference speed. In particular, FlashPortrait begins by computing the identity-agnostic facial expression features with an off-the-shelf extractor. It then introduces a Normalized Facial Expression Block to align facial features with diffusion latents by normalizing them with their respective means and variances, thereby improving identity stability in facial modeling. During inference, FlashPortrait adopts a dynamic sliding-window scheme with weighted blending in overlapping areas, ensuring smooth transitions and ID consistency in long animations. In each context window, based on the latent variation rate at particular timesteps and the derivative magnitude ratio among diffusion layers, FlashPortrait utilizes higher-order latent derivatives at the current timestep to directly predict latents at future timesteps, thereby skipping several denoising steps and achieving 6x speed acceleration. Experiments on benchmarks show the effectiveness of FlashPortrait both qualitatively and quantitatively.

</details>


### [13] [Alchemist: Unlocking Efficiency in Text-to-Image Model Training via Meta-Gradient Data Selection](https://arxiv.org/abs/2512.16905)
*Kaixin Ding,Yang Zhou,Xi Chen,Miao Yang,Jiarong Ou,Rui Chen,Xin Tao,Hengshuang Zhao*

Main category: cs.CV

TL;DR: Alchemist 是一种元梯度驱动的数据选择框架，旨在优化大规模文本-图像数据对，通过自动评估每个样本的影响并选择有信息性的数据子集，改善视觉质量和下游性能。


<details>
  <summary>Details</summary>
Motivation: 现有 T2I 生成模型依赖于人工筛选或单一维度特征评分，难以处理大规模低质量或冗余数据，Alchemist 通过元梯度优化实现自动化数据筛选，提高数据效率。

Method: Alchemist 包含两个关键阶段：数据评级和数据剪枝。首先训练一个轻量级评分器，基于梯度信息评估样本影响，然后采用 Shift-Gsampling 策略选择有效数据集用于模型训练。

Result: 实验表明，Alchemist 在合成和网页抓取的数据集上均能提高视觉质量和下游性能，使用其选择的数据训练可超越使用完整数据集的效果。

Conclusion: Alchemist 是首个基于元梯度的 T2I 模型训练数据选择框架，实现了自动化的高效数据筛选，显著提升了模型性能。

Abstract: Recent advances in Text-to-Image (T2I) generative models, such as Imagen, Stable Diffusion, and FLUX, have led to remarkable improvements in visual quality. However, their performance is fundamentally limited by the quality of training data. Web-crawled and synthetic image datasets often contain low-quality or redundant samples, which lead to degraded visual fidelity, unstable training, and inefficient computation. Hence, effective data selection is crucial for improving data efficiency. Existing approaches rely on costly manual curation or heuristic scoring based on single-dimensional features in Text-to-Image data filtering. Although meta-learning based method has been explored in LLM, there is no adaptation for image modalities. To this end, we propose **Alchemist**, a meta-gradient-based framework to select a suitable subset from large-scale text-image data pairs. Our approach automatically learns to assess the influence of each sample by iteratively optimizing the model from a data-centric perspective. Alchemist consists of two key stages: data rating and data pruning. We train a lightweight rater to estimate each sample's influence based on gradient information, enhanced with multi-granularity perception. We then use the Shift-Gsampling strategy to select informative subsets for efficient model training. Alchemist is the first automatic, scalable, meta-gradient-based data selection framework for Text-to-Image model training. Experiments on both synthetic and web-crawled datasets demonstrate that Alchemist consistently improves visual quality and downstream performance. Training on an Alchemist-selected 50% of the data can outperform training on the full dataset.

</details>


### [14] [VIVA: VLM-Guided Instruction-Based Video Editing with Reward Optimization](https://arxiv.org/abs/2512.16906)
*Xiaoyan Cong,Haotian Yang,Angtian Wang,Yizhi Wang,Yiding Yang,Canyu Zhang,Chongyang Ma*

Main category: cs.CV

TL;DR: VIVA提出了一种新的框架，通过VLM指导编码和奖励优化，旨在提高指令驱动的视频编辑性能。该框架包括一个文本指令编码器、后训练阶段的编辑策略优化以及用于生成合成视频指令数据的数据构造管道。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于简单的配对数据训练，难以应对多样性和复杂性的现实指令。VIVA旨在通过VLM指导的编码增强理解和处理自然语言指令，优化奖励机制以适应视频编辑场景，从而实现更好的一致性、内容保真度和美学效果。

Method: VIVA使用VLM指导的指令编码器将文本、视频起始帧和可选参考图像转换为视觉指导的指令表示，以便为扩散变换器提供精细的空间和语义上下文。通过Edit-GRPO后训练阶段直接优化模型，以相对奖励实现指令忠实、内容保真和审美愉悦的编辑。

Result: 实验表明，VIVA在指令跟随、泛化能力和编辑质量方面优于现有最先进的方法。

Conclusion: VIVA提出了一种创新的方法来解决现有方法在处理复杂指令时的泛化问题，通过VLM指导编码和奖励优化策略，显著提高了指令驱动视频编辑的效果和质量。

Abstract: Instruction-based video editing aims to modify an input video according to a natural-language instruction while preserving content fidelity and temporal coherence. However, existing diffusion-based approaches are often trained on paired data of simple editing operations, which fundamentally limits their ability to generalize to diverse and complex, real-world instructions. To address this generalization gap, we propose VIVA, a scalable framework for instruction-based video editing that leverages VLM-guided encoding and reward optimization. First, we introduce a VLM-based instructor that encodes the textual instruction, the first frame of the source video, and an optional reference image into visually-grounded instruction representations, providing fine-grained spatial and semantic context for the diffusion transformer backbone. Second, we propose a post-training stage, Edit-GRPO, which adapts Group Relative Policy Optimization to the domain of video editing, directly optimizing the model for instruction-faithful, content-preserving, and aesthetically pleasing edits using relative rewards. Furthermore, we propose a data construction pipeline designed to synthetically generate diverse, high-fidelity paired video-instruction data of basic editing operations. Extensive experiments show that VIVA achieves superior instruction following, generalization, and editing quality over state-of-the-art methods. Website: https://viva-paper.github.io

</details>


### [15] [Flowing from Reasoning to Motion: Learning 3D Hand Trajectory Prediction from Egocentric Human Interaction Videos](https://arxiv.org/abs/2512.16907)
*Mingfei Chen,Yifan Wang,Zhengqin Li,Homanga Bharadhwaj,Yujin Chen,Chuan Qin,Ziyi Kou,Yuan Tian,Eric Whitmire,Rajinder Sodhi,Hrvoje Benko,Eli Shlizerman,Yue Liu*

Main category: cs.CV

TL;DR: 本研究提出了EgoMAN数据集和模型，旨在解决现有3D手轨迹预测中的数据和建模问题，实现细粒度的动作和时空推理，并能泛化到真实场景。


<details>
  <summary>Details</summary>
Motivation: 现有3D手轨迹预测工作受限于数据集的运动与语义监督脱钩以及模型构建的弱关联推理与行动，因此需要一个更贴近真实场景并能进行细粒度时空推理的数据集及模型。

Method: 该研究分别建立了EgoMAN数据集和相应EgoMAN模型。EgoMAN数据集包含大量第一人称视角的交互阶段感知三维手轨迹，并提供了语义、空间及运动推理所需的结构化问题答案对。EgoMAN模型通过轨迹-令牌接口连接视觉语言推理与运动生成，逐步训练以确保推理与运动动态的对齐。

Result: 该方法生成的轨迹与真实场景具有高度一致性和阶段意识，能够在不同真实世界场景中实现良好的泛化。

Conclusion: EgoMAN数据集和模型的提出极大地推动了3D手轨迹预测的发展，实现了精细的时空推理和更好的泛化性能。

Abstract: Prior works on 3D hand trajectory prediction are constrained by datasets that decouple motion from semantic supervision and by models that weakly link reasoning and action. To address these, we first present the EgoMAN dataset, a large-scale egocentric dataset for interaction stage-aware 3D hand trajectory prediction with 219K 6DoF trajectories and 3M structured QA pairs for semantic, spatial, and motion reasoning. We then introduce the EgoMAN model, a reasoning-to-motion framework that links vision-language reasoning and motion generation via a trajectory-token interface. Trained progressively to align reasoning with motion dynamics, our approach yields accurate and stage-aware trajectories with generalization across real-world scenes.

</details>


### [16] [SceneDiff: A Benchmark and Method for Multiview Object Change Detection](https://arxiv.org/abs/2512.16908)
*Yuqun Wu,Chih-hao Lin,Henry Che,Aditi Tiwari,Chuhang Zou,Shenlong Wang,Derek Hoiem*

Main category: cs.CV

TL;DR: 探讨了图像或视频中同一场景不同时刻之间的对象添加、删除或移动识别问题。提出的SceneDiff基准数据集首次提供了多视角变更检测，并包含350个视频对与数千个变更对象。同时介绍了SceneDiff方法，这是一种无需训练的新方法，利用预训练的3D、分割和图像编码模型，在多视角对象变更检测方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 识别场景中物体的变化对于许多应用至关重要，例如机器人整理、建筑进度监测和消防安全。尽管多视角视角能导致物体看似发生变化，但研究人员仍需通过新基准数据集和方法来克服这一挑战。

Method: 引入了无需训练的SceneDiff方法，该方法利用预训练的3D、分割和图像编码模型进行多视角物体变更检测。通过3D对齐场景，提取对象区域，并比较空间和语义特征以检测变化。

Result: 在多视角和两视角基准上的实验表明，SceneDiff方法在所有基准数据集上均表现出显著性能优越（多基准相对AP提升高达94%）。

Conclusion: 本文提出了首个包含多视角对象变更检测的SceneDiff基准数据集和无需训练的SceneDiff方法，该方法在多视角物体变更检测任务中超越了现有方法。

Abstract: We investigate the problem of identifying objects that have been added, removed, or moved between a pair of captures (images or videos) of the same scene at different times. Detecting such changes is important for many applications, such as robotic tidying or construction progress and safety monitoring. A major challenge is that varying viewpoints can cause objects to falsely appear changed. We introduce SceneDiff Benchmark, the first multiview change detection benchmark with object instance annotations, comprising 350 diverse video pairs with thousands of changed objects. We also introduce the SceneDiff method, a new training-free approach for multiview object change detection that leverages pretrained 3D, segmentation, and image encoding models to robustly predict across multiple benchmarks. Our method aligns the captures in 3D, extracts object regions, and compares spatial and semantic region features to detect changes. Experiments on multi-view and two-view benchmarks demonstrate that our method outperforms existing approaches by large margins (94% and 37.4% relative AP improvements). The benchmark and code will be publicly released.

</details>


### [17] [EasyV2V: A High-quality Instruction-based Video Editing Framework](https://arxiv.org/abs/2512.16920)
*Jinjie Mai,Chaoyang Wang,Guocheng Gordon Qian,Willi Menapace,Sergey Tulyakov,Bernard Ghanem,Peter Wonka,Ashkan Mirzaei*

Main category: cs.CV

TL;DR: 引入了一个名为EasyV2V的框架，用于基于指令的视频编辑。该框架通过多种数据增强技术、简洁的模型设计和统一的空间-时间控制机制，实现了视频编辑性能的新突破。


<details>
  <summary>Details</summary>
Motivation: 尽管图像编辑已经取得了显著进步，但视频编辑依然是一个较为未被探索的领域，面临着保持连贯性、实现精确控制以及拓展应用范围的挑战。因此，研究者旨在设计一个简单、有效并且能够解决上述问题的框架。

Method: 该方法从数据、模型和控制三个方面着手：1) 利用快速逆操作组成现有专家以构建多样的视频对，并通过单帧监督、共享仿射运动的伪配对和密集字幕片段来提升数据质量；2) 观察预训练的文本到视频模型具备编辑能力，从而简化了设计，仅需轻量化LoRA微调和序列简单拼接即可训练出强大的模型；3) 在控制方面，引入单一蒙版机制统一空间-时间控制，并支持参考图像的可选项。

Result: EasyV2V框架展示了多变的输入方式，包括视频+文本、视频+掩码+文本、视频+掩码+参考+文本，并且在视频编辑任务上达到了前所未有的性能，超越了竞争对手和商业系统。

Conclusion: 研究者提出了一种名为EasyV2V的简单而有效的框架，旨在通过数据增强、简化模型设计和统一的控制机制解决视频编辑中的挑战，并且所提出的系统展示了卓越的任务性能。

Abstract: While image editing has advanced rapidly, video editing remains less explored, facing challenges in consistency, control, and generalization. We study the design space of data, architecture, and control, and introduce \emph{EasyV2V}, a simple and effective framework for instruction-based video editing. On the data side, we compose existing experts with fast inverses to build diverse video pairs, lift image edit pairs into videos via single-frame supervision and pseudo pairs with shared affine motion, mine dense-captioned clips for video pairs, and add transition supervision to teach how edits unfold. On the model side, we observe that pretrained text-to-video models possess editing capability, motivating a simplified design. Simple sequence concatenation for conditioning with light LoRA fine-tuning suffices to train a strong model. For control, we unify spatiotemporal control via a single mask mechanism and support optional reference images. Overall, EasyV2V works with flexible inputs, e.g., video+text, video+mask+text, video+mask+reference+text, and achieves state-of-the-art video editing results, surpassing concurrent and commercial systems. Project page: https://snap-research.github.io/easyv2v/

</details>


### [18] [MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning](https://arxiv.org/abs/2512.16909)
*Yuanchen Ju,Yongyuan Liang,Yen-Jen Wang,Nandiraju Gireesh,Yuanliang Ju,Seungjae Lee,Qiao Gu,Elvis Hsieh,Furong Huang,Koushil Sreenath*

Main category: cs.CV

TL;DR: 本文介绍了MomaGraph，一种统一的场景表示方法，结合了空间功能关系和部件级互动元素。为此，作者贡献了MomaGraph-Scenes，首个大型富标注的任务导向场景图数据集，及MomaGraph-Bench，涵盖六个推理能力的系统评估套件。基于此，开发了MomaGraph-R1，一种7B视觉语言模型，使用 reinforcement learning 训练在MomaGraph-Scenes上。实验表明，该模型在基准测试上达到71.6%的准确率，超越了最佳基线模型11.4%，并在公开基准测试和真实机器人实验中表现出良好的泛化和迁移。


<details>
  <summary>Details</summary>
Motivation: 为了实现家庭环境中移动机械人的导航和操作，需要一个紧凑且丰富的场景表示，能够捕捉物体的位置、功能及其可行动的部分。现有的方法通常忽视了当前任务最相关的信息，因此作者提出了MomaGraph，解决了这些限制。

Method: 通过引入MomaGraph，该方法整合了空间功能关系和部件级互动元素。同时，作者贡献了MomaGraph-Scenes，一个包含富有标注的任务导向场景图数据集，以及MomaGraph-Bench，一个覆盖六个推理能力的系统评估套件。基于此基础，开发了一种使用reinforcement learning训练的7B视觉语言模型MomaGraph-R1。该模型能够预测任务导向的场景图，并采用Graph-then-Plan框架作为零样本任务规划器。

Result: MomaGraph在基准测试上实现了71.6%的准确率，超越了最佳基线模型11.4%，且在各种公开基准测试和真实机器人实验中表现出良好的泛化和迁移。

Conclusion: 该工作通过MomaGraph提出了一个新的场景表示方法，并结合了大量的数据和系统评估，展示了其在家庭环境中的有效性和潜力。

Abstract: Mobile manipulators in households must both navigate and manipulate. This requires a compact, semantically rich scene representation that captures where objects are, how they function, and which parts are actionable. Scene graphs are a natural choice, yet prior work often separates spatial and functional relations, treats scenes as static snapshots without object states or temporal updates, and overlooks information most relevant for accomplishing the current task. To address these limitations, we introduce MomaGraph, a unified scene representation for embodied agents that integrates spatial-functional relationships and part-level interactive elements. However, advancing such a representation requires both suitable data and rigorous evaluation, which have been largely missing. We thus contribute MomaGraph-Scenes, the first large-scale dataset of richly annotated, task-driven scene graphs in household environments, along with MomaGraph-Bench, a systematic evaluation suite spanning six reasoning capabilities from high-level planning to fine-grained scene understanding. Built upon this foundation, we further develop MomaGraph-R1, a 7B vision-language model trained with reinforcement learning on MomaGraph-Scenes. MomaGraph-R1 predicts task-oriented scene graphs and serves as a zero-shot task planner under a Graph-then-Plan framework. Extensive experiments demonstrate that our model achieves state-of-the-art results among open-source models, reaching 71.6% accuracy on the benchmark (+11.4% over the best baseline), while generalizing across public benchmarks and transferring effectively to real-robot experiments.

</details>


### [19] [Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification](https://arxiv.org/abs/2512.16921)
*Qihao Liu,Chengzhi Mao,Yaojie Liu,Alan Yuille,Wen-Sheng Chu*

Main category: cs.CV

TL;DR: 引入了AuditDM框架，通过自动化发现和纠正大模型（MLLMs）的缺陷模式，该框架通过强化学习训练模拟器生成具有最大分歧的目标模型的挑战性问题和反事实图像，从而发现多种模型弱点并优化模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统的评估方法对于多模态大模型的可解释性不足，无法充分揭示模型之间的能力差距，因此需要一种新的框架来提高模型的可解释性和性能。

Method: 通过强化学习训练一个审计模型，生成能够最大化目标模型间分歧的挑战性问题和反事实图像，该审计模型能够主动发现和纠正大模型的错误模式。

Result: 审计模型在Gemma-3和PaliGemma-2等先进模型的应用中发现了20多种不同的错误类型，并通过调整这些发现，所有模型在16个基准测试中都得到了改进，甚至使3B参数模型超越了28B参数模型。

Conclusion: 该工作表明，当数据扩展效应减弱时，有针对性的模型审计可以提高模型诊断和改进的有效性。

Abstract: Conventional evaluation methods for multimodal LLMs (MLLMs) lack interpretability and are often insufficient to fully disclose significant capability gaps across models. To address this, we introduce AuditDM, an automated framework that actively discovers and rectifies MLLM failure modes by auditing their divergence. AuditDM fine-tunes an MLLM as an auditor via reinforcement learning to generate challenging questions and counterfactual images that maximize disagreement among target models. Once trained, the auditor uncovers diverse, interpretable exemplars that reveal model weaknesses and serve as annotation-free data for rectification. When applied to SoTA models like Gemma-3 and PaliGemma-2, AuditDM discovers more than 20 distinct failure types. Fine-tuning on these discoveries consistently improves all models across 16 benchmarks, and enables a 3B model to surpass its 28B counterpart. Our results suggest that as data scaling hits diminishing returns, targeted model auditing offers an effective path to model diagnosis and improvement.

</details>


### [20] [SFTok: Bridging the Performance Gap in Discrete Tokenizers](https://arxiv.org/abs/2512.16910)
*Qihang Rao,Borui Zhang,Wenzhao Zheng,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: 该研究通过提出SFTok，改进了图像去量化过程，使其在多步过程中解决了训练与推理之间的不一致性问题，达到了高解析度图像重建的最佳效果。


<details>
  <summary>Details</summary>
Motivation: 当前的图像令牌化方法在多模态系统中的应用受到限制，主要因为离散令牌与自回归范式自然契合，但效果仍不如连续型令牌。

Method: SFTok通过引入多步迭代机制、自我激励指导视觉重建和去偏与调整训练策略来实现精准重建。

Result: SFTok在高压缩率下（每张图像64个令牌）实现了在ImageNet上的最佳重建质量（rFID = 1.21），并在类别到图像生成任务中展现出色表现（gFID = 2.29）。

Conclusion: SFTok为高分辨率图像生成提供了改进的解决方案，并且在多项测试中达到了先进的重建结果。

Abstract: Recent advances in multimodal models highlight the pivotal role of image tokenization in high-resolution image generation. By compressing images into compact latent representations, tokenizers enable generative models to operate in lower-dimensional spaces, thereby improving computational efficiency and reducing complexity. Discrete tokenizers naturally align with the autoregressive paradigm but still lag behind continuous ones, limiting their adoption in multimodal systems. To address this, we propose \textbf{SFTok}, a discrete tokenizer that incorporates a multi-step iterative mechanism for precise reconstruction. By integrating \textbf{self-forcing guided visual reconstruction} and \textbf{debias-and-fitting training strategy}, SFTok resolves the training-inference inconsistency in multi-step process, significantly enhancing image reconstruction quality. At a high compression rate of only 64 tokens per image, SFTok achieves state-of-the-art reconstruction quality on ImageNet (rFID = 1.21) and demonstrates exceptional performance in class-to-image generation tasks (gFID = 2.29).

</details>


### [21] [StereoPilot: Learning Unified and Efficient Stereo Conversion via Generative Priors](https://arxiv.org/abs/2512.16915)
*Guibao Shen,Yihua Du,Wenhang Ge,Jing He,Chirui Chang,Donghao Zhou,Zhen Yang,Luozhou Wang,Xin Tao,Ying-Cong Chen*

Main category: cs.CV

TL;DR: 该研究提出了一种名为UniStereo的大型统一数据集，旨在解决单目到立体视图转换中的误差传播、深度歧义和格式不一致问题。同时，研究开发了一种高效的前馈模型StereoPilot，能够在不依赖显式深度图或迭代扩散采样的情况下直接生成目标视角，实验表明StereoPilot在视觉保真度和计算效率方面均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着立体显示器如VR头盔和3D电影院的迅速发展，对于高质量立体视频内容的需求不断增加。然而，目前单目到立体视图的自动转换受到多阶段深度扭曲修补（DWI）管道限制的影响，该方法存在错误传播、深度歧义以及平行与汇聚立体配置格式不一致的问题。

Method: 研究首先构建了一个涵盖立体格式转换的新型大规模统一数据集UniStereo。基于该数据集，提出了StereoPilot高效前馈模型，该模型具备可学习域转换器及循环一致性损失，能够直接生成目标视图，适应不同的立体格式。

Result: 实验证明，StereoPilot在视觉保真度和计算效率方面显著优于现有最先进的方法。

Conclusion: 该研究通过提出StereoPilot解决了一维至三维视频转化的多个挑战，为研究领域提供了重要的工具和数据支持，具有广泛的应用前景。

Abstract: The rapid growth of stereoscopic displays, including VR headsets and 3D cinemas, has led to increasing demand for high-quality stereo video content. However, producing 3D videos remains costly and complex, while automatic Monocular-to-Stereo conversion is hindered by the limitations of the multi-stage ``Depth-Warp-Inpaint'' (DWI) pipeline. This paradigm suffers from error propagation, depth ambiguity, and format inconsistency between parallel and converged stereo configurations. To address these challenges, we introduce UniStereo, the first large-scale unified dataset for stereo video conversion, covering both stereo formats to enable fair benchmarking and robust model training. Building upon this dataset, we propose StereoPilot, an efficient feed-forward model that directly synthesizes the target view without relying on explicit depth maps or iterative diffusion sampling. Equipped with a learnable domain switcher and a cycle consistency loss, StereoPilot adapts seamlessly to different stereo formats and achieves improved consistency. Extensive experiments demonstrate that StereoPilot significantly outperforms state-of-the-art methods in both visual fidelity and computational efficiency. Project page: https://hit-perfect.github.io/StereoPilot/.

</details>


### [22] [AdaTooler-V: Adaptive Tool-Use for Images and Videos](https://arxiv.org/abs/2512.16918)
*Chaoyang Wang,Kaituo Feng,Dongyang Chen,Zhongyu Wang,Zhixun Li,Sicheng Gao,Meng Meng,Xu Zhou,Manyuan Zhang,Yuzhang Shang,Xiangyu Yue*

Main category: cs.CV

TL;DR: 本文提出了一种适应性工具使用方法AdaTooler-V，通过自适应调整奖励尺度来避免不必要的工具使用，显著提高了推理效率和模型性能。同时，该研究构建了两个数据集支持训练，并在多项基准测试中表现出色，特别是在高分辨率基准测试中超越了商业模型。


<details>
  <summary>Details</summary>
Motivation: 现有开源模型在视觉推理任务中存在盲目使用工具的问题，这导致了推理效率的降低和性能的下降。为了解决这一问题，研究提出了AdaTooler-V，通过引入AT-GRPO算法来适应性地调整奖励尺度，鼓励模型仅在工具提供真正改进时才调用工具。

Method: 研究采用了强化学习算法AT-GRPO来调整奖励尺度，从而引导模型在真正需要使用工具时才调用工具。同时，还构建了两个可用于训练的自定义数据集，以支持模型适应性工具使用的研究。

Result: 实验结果显示，AdaTooler-V在多个基准测试中表现出色，特别是在高分辨率基准测试中达到了89.8%的准确率，超过了商业模型GPT-4o和Gemini 1.5 Pro。

Conclusion: 研究提出了AdaTooler-V，一种适应性工具使用方法，并通过其优越的性能验证了该方法的有效性，同时提供了相关的代码、模型和数据以供进一步研究。

Abstract: Recent advances have shown that multimodal large language models (MLLMs) benefit from multimodal interleaved chain-of-thought (CoT) with vision tool interactions. However, existing open-source models often exhibit blind tool-use reasoning patterns, invoking vision tools even when they are unnecessary, which significantly increases inference overhead and degrades model performance. To this end, we propose AdaTooler-V, an MLLM that performs adaptive tool-use by determining whether a visual problem truly requires tools. First, we introduce AT-GRPO, a reinforcement learning algorithm that adaptively adjusts reward scales based on the Tool Benefit Score of each sample, encouraging the model to invoke tools only when they provide genuine improvements. Moreover, we construct two datasets to support training: AdaTooler-V-CoT-100k for SFT cold start and AdaTooler-V-300k for RL with verifiable rewards across single-image, multi-image, and video data. Experiments across twelve benchmarks demonstrate the strong reasoning capability of AdaTooler-V, outperforming existing methods in diverse visual reasoning tasks. Notably, AdaTooler-V-7B achieves an accuracy of 89.8\% on the high-resolution benchmark V*, surpassing the commercial proprietary model GPT-4o and Gemini 1.5 Pro. All code, models, and data are released.

</details>


### [23] [The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text](https://arxiv.org/abs/2512.16924)
*Hanlin Wang,Hao Ouyang,Qiuyu Wang,Yue Yu,Yihao Meng,Wen Wang,Ka Leong Cheng,Shuailei Ma,Qingyan Bai,Yixuan Li,Cheng Chen,Yanhong Zeng,Xing Zhu,Yujun Shen,Qifeng Chen*

Main category: cs.CV

TL;DR: WorldCanvas 是一个框架，通过结合文本、轨迹和参考图像，生成具有丰富用户导向的模拟事件。它超越了仅基于文本的方法和现有的轨迹控制图像到视频的方法，支持多智能体交互、对象进出、参考引导的外观变化和反直观事件，从而产生不仅具有时间连贯性而且具有涌现一致性的视频。


<details>
  <summary>Details</summary>
Motivation: 现有方法如仅基于文本的生成和轨迹控制的图像到视频的方法存在局限性，无法生成具有多智能体交互和参考引导属性的连贯事件。WorldCanvas旨在克服这些限制，实现更高级的模拟器，使其成为用户形状的互动模拟器，而不仅仅是被动的预测器。

Method: WorldCanvas通过结合轨迹（包括运动、时间和可见性信息）和自然语言（赋予事件语义意图）以及参考图像（为对象身份提供视觉锚定），创建一个强大的多模态框架。这种方法能够生成包括复杂的多智能体交互、对象的进入和退出、基于参考的外观变化以及反直观事件的连贯视频。

Result: WorldCanvas生成的视频不仅展示了时间连贯性，而且还展示了涌现的一致性，即即使在临时消失的情况下，对象身份和场景也得以保持。

Conclusion: WorldCanvas通过生成具有丰富连贯性的事件和增强的互动性，成功地将 мир 模型从被动预测器转变为互动的、用户形状的模拟器。

Abstract: We present WorldCanvas, a framework for promptable world events that enables rich, user-directed simulation by combining text, trajectories, and reference images. Unlike text-only approaches and existing trajectory-controlled image-to-video methods, our multimodal approach combines trajectories -- encoding motion, timing, and visibility -- with natural language for semantic intent and reference images for visual grounding of object identity, enabling the generation of coherent, controllable events that include multi-agent interactions, object entry/exit, reference-guided appearance and counterintuitive events. The resulting videos demonstrate not only temporal coherence but also emergent consistency, preserving object identity and scene despite temporary disappearance. By supporting expressive world events generation, WorldCanvas advances world models from passive predictors to interactive, user-shaped simulators. Our project page is available at: https://worldcanvas.github.io/.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [24] [Grammar-Forced Translation of Natural Language to Temporal Logic using LLMs](https://arxiv.org/abs/2512.16814)
*William English,Dominic Simon,Sumit Kumar Jha,Rickard Ewetz*

Main category: cs.CL

TL;DR: GraFT 通过限制每一步的有效输出 token 集合来简化自然语言到时间逻辑的翻译任务，显著提高了翻译准确度。


<details>
  <summary>Details</summary>
Motivation: 当前方法在精确的原子命题提升、处理共指和从少量数据中学习方面存在困难。

Method: GraFT 通过在每一步限制有效输出 token 数量，减少解空间并加速学习过程。

Result: 在 CW、GLTL 和 Navi 基准测试中，GraFT 的端到端翻译准确度提高了 5.49%，跨域翻译准确度平均提高了 14.06%。

Conclusion: GraFT 提供了一种有效解决 NL 到 TL 翻译问题的方法，通过减少解空间实现更高的学习效率。

Abstract: Translating natural language (NL) into a formal language such as temporal logic (TL) is integral for human communication with robots and autonomous systems. State-of-the-art approaches decompose the task into a lifting of atomic propositions (APs) phase and a translation phase. However, existing methods struggle with accurate lifting, the existence of co-references, and learning from limited data. In this paper, we propose a framework for NL to TL translation called Grammar Forced Translation (GraFT). The framework is based on the observation that previous work solves both the lifting and translation steps by letting a language model iteratively predict tokens from its full vocabulary. In contrast, GraFT reduces the complexity of both tasks by restricting the set of valid output tokens from the full vocabulary to only a handful in each step. The solution space reduction is obtained by exploiting the unique properties of each problem. We also provide a theoretical justification for why the solution space reduction leads to more efficient learning. We evaluate the effectiveness of GraFT using the CW, GLTL, and Navi benchmarks. Compared with state-of-the-art translation approaches, it can be observed that GraFT the end-to-end translation accuracy by 5.49% and out-of-domain translation accuracy by 14.06% on average.

</details>


### [25] [What Do Prosody and Text Convey? Characterizing How Meaningful Information is Distributed Across Multiple Channels](https://arxiv.org/abs/2512.16832)
*Aditya Yadavalli,Tiago Pimentel,Tamar I Regev,Ethan Wilcox,Alex Warstadt*

Main category: cs.CL

TL;DR: 本文提出了一种信息论方法，量化了非言语信息（例如语音中的情感和语气）与文本信息之间的差异，并在电视和播客的语音数据上测试了这种方法，发现语音渠道比文本渠道传输了关于讽刺和情绪的信息多约一个数量级，而在关于问题性方面，语音提供的额外信息较小。


<details>
  <summary>Details</summary>
Motivation: 本文旨在揭示语音（Prosody）在沟通中的重要作用，尤其是它所传达的信息是否能够通过文本信息捕捉到。通过量化分析，文章揭示了语音信息对于情感和讽刺表达的必要性。

Method: 作者利用大型语音和语言模型，估计了特定意义维度（如情感）与任何通信渠道（如音频或文本）之间的互信息。这种方法被应用于电视和播客的语音数据中，以量化有关讽刺、情感和疑问性信息的传播。

Result: 研究成果表明，对于讽刺和情感，语音渠道——通过隐含的对音调、语气的关注——相比文本渠道能够提供多约一个数量级的信息。而对于疑问性，语音提供的额外信息较小。

Conclusion: 基于这些结果，文章提出了进一步应用该方法到更多含义维度、通信渠道和语言上的计划。

Abstract: Prosody -- the melody of speech -- conveys critical information often not captured by the words or text of a message. In this paper, we propose an information-theoretic approach to quantify how much information is expressed by prosody alone and not by text, and crucially, what that information is about. Our approach applies large speech and language models to estimate the mutual information between a particular dimension of an utterance's meaning (e.g., its emotion) and any of its communication channels (e.g., audio or text). We then use this approach to quantify how much information is conveyed by audio and text about sarcasm, emotion, and questionhood, using speech from television and podcasts. We find that for sarcasm and emotion the audio channel -- and by implication the prosodic channel -- transmits over an order of magnitude more information about these features than the text channel alone, at least when long-term context beyond the current sentence is unavailable. For questionhood, prosody provides comparatively less additional information. We conclude by outlining a program applying our approach to more dimensions of meaning, communication channels, and languages.

</details>


### [26] [LLMCache: Layer-Wise Caching Strategies for Accelerated Reuse in Transformer Inference](https://arxiv.org/abs/2512.16843)
*Harsh Vardhan Bansal*

Main category: cs.CL

TL;DR: LLMCache通过基于输入序列语义相似性的中间激活重用，为Transformer推理加速，实现了高达3.1倍的加速，同时保持接近99.5%的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有缓存机制在自回归解码中有加速效果，但范围和适用性有限。LLMCache通过引入一种轻量级的指纹匹配机制和可调的淘汰策略，实现了可应用于任意Transformer层、跨编码器和解码器架构的模型无关缓存框架。

Method: LLMCache采用了一种基于语义相似性的指纹化方法来匹配输入，并使用灵活的淘汰策略以管理缓存老化。

Result: 在BERT和GPT-2上的实验结果展示了在SQuAD、WikiText-103和OpenBookQA上的最高3.1倍的推理时间加速，同时准确率下降少于0.5%。

Conclusion: LLMCache作为一种实际且通用的优化Transformer推理的缓存解决方案，在实际应用中展现出了潜力。

Abstract: Transformer-based language models have achieved remarkable performance across a wide range of tasks, yet their high inference latency poses a significant challenge for real-timeand large-scale deployment. While existing caching mechanisms,such as token-level key-value caches, offer speedups in autore-gressive decoding, they are limited in scope and applicability. In this paper, we present LLMCache, a novel layer-wise caching framework that accelerates transformer inference by reusing intermediate activations based on semantic similarity of input sequences. Unlike prior work, LLMCache is model-agnostic,operates across both encoder and decoder architectures, and supports caching at arbitrary transformer layers. We introduce a lightweight fingerprinting mechanism for matching seman-tically similar inputs and propose adaptive eviction strategies to manage cache staleness. Experiments on BERT and GPT-2 across SQuAD, WikiText-103, and OpenBookQA show up to 3.1 X speedup in inference time with <0.5% accuracy degradation. Our results highlight LLMCache as a practical and general-purpose solution for optimizing transformer inference in real-world applications

</details>


### [27] [In-Context Algebra](https://arxiv.org/abs/2512.16902)
*Eric Todd,Jannik Brinkmann,Rohit Gandikota,David Bau*

Main category: cs.CL

TL;DR: 本文研究了当训练变压器解决具有可变符号意义的序列上的算术任务时，它们所学习的机制。尽管任务具有挑战性，变压器仍然实现了接近完美的准确率，并且能够泛化到未见过的代数群。研究还发现了三种模型学习的机制。


<details>
  <summary>Details</summary>
Motivation: 先前的研究发现，在固定符号意义的设置下，变压器能够发展出类似于代数结构的几何嵌入。本文旨在进一步探索当符号意义不是固定的时，变压器如何进行推理并学习符号推理机制。

Method: 本文通过设计一个新任务，其中代数群元素的符号分配在一个序列中从一个到另一个变化来探索这个问题。通过使用有针对性的数据分布，设计因果测试来测试一组假设机制。实验中观察到了三种一致学习的机制：可交换复制、恒等元素识别和闭合基于的消除。

Result: 变压器在该新任务中实现了接近完美的准确率，并且在未见过的代数群中泛化良好。研究还验证了三种学习的机制：可交换复制、恒等元素识别和闭合基于的消除。

Conclusion: 本文的研究表明，在动态上下文中训练变压器进行变量推理，能够学习到符号推理机制，这些机制与固定符号设置下的几何表示不同。这种研究有助于理解变压器在复杂任务中的推理能力。

Abstract: We investigate the mechanisms that arise when transformers are trained to solve arithmetic on sequences where tokens are variables whose meaning is determined only through their interactions. While prior work has found that transformers develop geometric embeddings that mirror algebraic structure, those previous findings emerge from settings where arithmetic-valued tokens have fixed meanings. We devise a new task in which the assignment of symbols to specific algebraic group elements varies from one sequence to another. Despite this challenging setup, transformers achieve near-perfect accuracy on the task and even generalize to unseen algebraic groups. We develop targeted data distributions to create causal tests of a set of hypothesized mechanisms, and we isolate three mechanisms models consistently learn: commutative copying where a dedicated head copies answers, identity element recognition that distinguishes identity-containing facts, and closure-based cancellation that tracks group membership to constrain valid answers. Complementary to the geometric representations found in fixed-symbol settings, our findings show that models develop symbolic reasoning mechanisms when trained to reason in-context with variables whose meanings are not fixed.

</details>


### [28] [Constructive Circuit Amplification: Improving Math Reasoning in LLMs via Targeted Sub-Network Updates](https://arxiv.org/abs/2512.16914)
*Nikhil Prakash,Donghao Ren,Dominik Moritz,Yannick Assogba*

Main category: cs.CL

TL;DR: 该研究提出了一种名为Constructive Circuit Amplification的新方法，通过精确更新模型中的关键组件来提升特定任务的性能，同时不影响其他能力。在数学推理任务上，该方法实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 受到先前研究表明的存在的负责特定任务的电路可以通过增强这些电路来提高模型性能的启发，研究团队提出了Constructive Circuit Amplification 方法。

Method: 该方法首先从模型推理过程中识别出对执行任务至关重要的标记和组件，然后只更新这些选定的关键组件，而不会改装大量其他组件。

Result: 在多个模型上进行数学推理任务时，这种方法提高了高达 11.4% 的准确性，只修改了模型组件的 1.59%，并且对其他能力几乎没有影响。

Conclusion: 研究结果表明，通过选择性地更新稀疏的模型组件，可以可靠地增强特定功能而不影响其他能力。

Abstract: Prior studies investigating the internal workings of LLMs have uncovered sparse subnetworks, often referred to as circuits, that are responsible for performing specific tasks. Additionally, it has been shown that model performance improvement through fine-tuning often results from the strengthening of existing circuits in the model. Taken together, these findings suggest the possibility of intervening directly on such circuits to make precise, task-targeted updates. Motivated by these findings, we propose a novel method called Constructive Circuit Amplification which identifies pivotal tokens from model reasoning traces as well as model components responsible for the desired task, and updates only those components. Applied to mathematical reasoning, it improves accuracy by up to +11.4% across multiple models while modifying as little as 1.59% of model components, with minimal impact on other abilities as measured by MMLU, TriviaQA, and TruthfulQA. These results demonstrate that targeted capabilities can be reliably enhanced by selectively updating a sparse set of model components.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [29] [Generative Adversarial Reasoner: Enhancing LLM Reasoning with Adversarial Reinforcement Learning](https://arxiv.org/abs/2512.16917)
*Qihao Liu,Luoxin Ye,Wufei Ma,Yu-Cheng Chou,Alan Yuille*

Main category: cs.AI

TL;DR: 引入了一种称为生成对抗推理器（GAR）的单策略联合训练框架，该框架通过对抗强化学习协同进化较大的语言模型（LLM）推理器和基于LLM的鉴别器，以提高数学推理能力。该方法在数学基准测试中取得了显著的改进，尤其是在流程完整分割策略、逻辑一致性奖励和样本效率方面。


<details>
  <summary>Details</summary>
Motivation: LLMs虽然在数学推理方面表现出色，但仍然存在过程错误，如计算错误、脆弱逻辑和表面上合理但无效的步骤。GAR通过对推理过程各个片段进行逻辑完整分割并用简洁结构化的理由进行评估，旨在解决这些问题。

Method: GAR框架采用对抗强化学习机制进行联合训练。其中，LLM推理器和基于LLM的鉴别器互相影响，共同进化，以提升推理的逻辑一致性和准确性。通过分段推理链并进行完整逻辑分割，鉴别器对每个片段进行准确性和合理性的评估。这种机制提供了密集且可靠的操作级奖励，补充了稀疏的精确匹配信号，实现了对性能的精确激励。

Result: 在数学基准测试中，GAR方法显著提升了多种模型的表现。具体而言，对于AIME24基准，GAR分别提高了DeepSeek-R1-Distill-Qwen-7B 和 DeepSeek-R1-Distill-Llama-8B的表现。同时，模块化的鉴别器也能够适应多种奖励塑形目标，如教师的蒸馏、偏好的对齐及基于数学证明的推理。

Conclusion: 研究指出，GAR框架通过对抗强化学习与逻辑完整分割的策略，不仅提高了LLM在数学推理中的整体质量，还增强了样本效率和步骤级的奖励信号清晰度。这种方法为未来的LLM研究和应用提供了新的思路。

Abstract: Large language models (LLMs) with explicit reasoning capabilities excel at mathematical reasoning yet still commit process errors, such as incorrect calculations, brittle logic, and superficially plausible but invalid steps. In this paper, we introduce Generative Adversarial Reasoner, an on-policy joint training framework designed to enhance reasoning by co-evolving an LLM reasoner and an LLM-based discriminator through adversarial reinforcement learning. A compute-efficient review schedule partitions each reasoning chain into logically complete slices of comparable length, and the discriminator evaluates each slice's soundness with concise, structured justifications. Learning couples complementary signals: the LLM reasoner is rewarded for logically consistent steps that yield correct answers, while the discriminator earns rewards for correctly detecting errors or distinguishing traces in the reasoning process. This produces dense, well-calibrated, on-policy step-level rewards that supplement sparse exact-match signals, improving credit assignment, increasing sample efficiency, and enhancing overall reasoning quality of LLMs. Across various mathematical benchmarks, the method delivers consistent gains over strong baselines with standard RL post-training. Specifically, on AIME24, we improve DeepSeek-R1-Distill-Qwen-7B from 54.0 to 61.3 (+7.3) and DeepSeek-R1-Distill-Llama-8B from 43.7 to 53.7 (+10.0). The modular discriminator also enables flexible reward shaping for objectives such as teacher distillation, preference alignment, and mathematical proof-based reasoning.

</details>


### [30] [TOGGLE: Temporal Logic-Guided Large Language Model Compression for Edge](https://arxiv.org/abs/2512.16855)
*Khurram Khalil,Khaza Anuarul Hoque*

Main category: cs.AI

TL;DR: TOGGLE通过利用信号时序逻辑（STL）正式指定和执行语言属性，实现了LLM的压缩，同时减少了计算资源消耗和模型大小，且不损失关键语言特性。


<details>
  <summary>Details</summary>
Motivation: 现有压缩技术可能会损害关键的语言属性，并缺乏正式保证保留模型的行为，这限制了LLMs在资源受限的边缘设备上的部署。

Method: TOGGLE提出了一种新颖的框架，利用STL来正式指定和确保压缩过程中的语言属性。通过STL稳健性引导的贝叶斯优化，系统地探索逐层量化和剪枝配置。

Result: TOGGLE在四种LLM架构中实现了最大的计算成本减少3.3倍（FLOPs），模型大小减少68.8%，且满足所有语言属性。

Conclusion: TOGGLE首次将形式方法整合到LLM压缩中，使LLM能够在边缘硬件上有效且可验证地部署。

Abstract: Large Language Models (LLMs) deliver exceptional performance across natural language tasks but demand substantial computational resources, limiting their deployment on resource-constrained edge devices. Existing compression techniques, such as quantization and pruning, often degrade critical linguistic properties and lack formal guarantees for preserving model behavior. We propose Temporal Logic-Guided Large Language Model Compression (TOGGLE), a novel framework that leverages Signal Temporal Logic (STL) to formally specify and enforce linguistic properties during compression. TOGGLE employs an STL robustness-guided Bayesian optimization to systematically explore layer-wise quantization and pruning configurations, generating compressed models that formally satisfy specified linguistic constraints without retraining or fine-tuning. Evaluating TOGGLE on four LLM architectures (GPT-2, DeepSeek-V2 7B, LLaMA 3 8B, and Mistral 7B), we achieve up to 3.3x reduction in computational costs (FLOPs) and up to a 68.8% reduction in model size while satisfying all linguistic properties. TOGGLE represents the first integration of formal methods into LLM compression, enabling efficient, verifiable deployment of LLMs on edge hardware.

</details>


### [31] [The Social Responsibility Stack: A Control-Theoretic Architecture for Governing Socio-Technical AI](https://arxiv.org/abs/2512.16873)
*Otman A. Basir*

Main category: cs.AI

TL;DR: 本文提出了一种六层的Social Responsibility Stack (SRS)框架，旨在嵌入社会价值观作为AI系统中的显式约束，确保AI系统的社会责任。SRS整合了设计时的防护措施与运行时的监控以及机构监督，用闭环的监督控制框架来建模责任。


<details>
  <summary>Details</summary>
Motivation: 现有的负责任的AI和治理努力提供了重要的规范性原则，但缺乏在整个系统生命周期中运行的可执行工程机制。研究强调了需要构建一个有效的框架来嵌入社会责任，这与现有的理论框架形成补充。

Method: 本文通过构建一个六层的架构框架Social Responsibility Stack (SRS)，确保将社会责任作为显式的约束嵌入AI系统。该框架涵盖了设计时的防护措施、运行时的监控以及机构监督。这种架构利用闭环的监督控制框架来建模责任。

Result: 研究开发了一种统一的基于约束的建模方式，引入了安全边界和反馈解释，并展示了如何持续监测和执行公平性、自主性、认知负担和解释质量。

Conclusion: 通过在临床决策支持、合作自动驾驶和公共部门系统等案例中应用，SRS将规范性目标转化为可操作的工程和运营控制措施，提供了伦理、控制理论与AI治理之间的桥梁，为可问责性、适应性和可审计性的人工智能系统提供了实际基础。

Abstract: Artificial intelligence systems are increasingly deployed in domains that shape human behaviour, institutional decision-making, and societal outcomes. Existing responsible AI and governance efforts provide important normative principles but often lack enforceable engineering mechanisms that operate throughout the system lifecycle. This paper introduces the Social Responsibility Stack (SRS), a six-layer architectural framework that embeds societal values into AI systems as explicit constraints, safeguards, behavioural interfaces, auditing mechanisms, and governance processes. SRS models responsibility as a closed-loop supervisory control problem over socio-technical systems, integrating design-time safeguards with runtime monitoring and institutional oversight. We develop a unified constraint-based formulation, introduce safety-envelope and feedback interpretations, and show how fairness, autonomy, cognitive burden, and explanation quality can be continuously monitored and enforced. Case studies in clinical decision support, cooperative autonomous vehicles, and public-sector systems illustrate how SRS translates normative objectives into actionable engineering and operational controls. The framework bridges ethics, control theory, and AI governance, providing a practical foundation for accountable, adaptive, and auditable socio-technical AI systems.

</details>
