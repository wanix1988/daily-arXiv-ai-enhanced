<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 74]
- [cs.CL](#cs.CL) [Total: 63]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.AI](#cs.AI) [Total: 13]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Intellectual Property Protection for 3D Gaussian Splatting Assets: A Survey](https://arxiv.org/abs/2602.03878)
*Longjie Zhao,Ziming Hong,Jiaxin Huang,Runnan Chen,Mingming Gong,Tongliang Liu*

Main category: cs.CV

TL;DR: 本文系统地回顾了3DGS的IP保护现状，从基础机制、保护策略和鲁棒性挑战三个角度进行了分析，并提出了六个研究方向。


<details>
  <summary>Details</summary>
Motivation: 目前对3DGS的IP保护研究碎片化，缺乏系统性的理解。本文旨在填补这一空白，为3DGS资产提供可靠和可信赖的IP保护之路。

Method: 进行了一项关于3DGS的系统性综述，覆盖了基础机制、保护策略和新生成AI时代下的鲁棒性威胁等方面，识别出存在的技术基础和鲁棒性表征方面的差距。

Result: 本文提出了一个全面的框架，涵盖了3DGS的基础机制、保护策略和新兴AI环境下的鲁棒性威胁等方面，并指出了六个研究方向。

Conclusion: 本文为进一步研究3DGS的IP保护提供了理论基础，并指出了未来研究的方向，旨在增强3DGS在虚拟和增强现实、机器人技术及3D内容创作等领域的可靠性与可信度。

Abstract: 3D Gaussian Splatting (3DGS) has become a mainstream representation for real-time 3D scene synthesis, enabling applications in virtual and augmented reality, robotics, and 3D content creation. Its rising commercial value and explicit parametric structure raise emerging intellectual property (IP) protection concerns, prompting a surge of research on 3DGS IP protection. However, current progress remains fragmented, lacking a unified view of the underlying mechanisms, protection paradigms, and robustness challenges. To address this gap, we present the first systematic survey on 3DGS IP protection and introduce a bottom-up framework that examines (i) underlying Gaussian-based perturbation mechanisms, (ii) passive and active protection paradigms, and (iii) robustness threats under emerging generative AI era, revealing gaps in technical foundations and robustness characterization and indicating opportunities for deeper investigation. Finally, we outline six research directions across robustness, efficiency, and protection paradigms, offering a roadmap toward reliable and trustworthy IP protection for 3DGS assets.

</details>


### [2] [4DPC$^2$hat: Towards Dynamic Point Cloud Understanding with Failure-Aware Bootstrapping](https://arxiv.org/abs/2602.03890)
*Xindan Zhang,Weilong Yan,Yufei Shi,Xuerui Qiu,Tao He,Ying Li,Ming Li,Hehe Fan*

Main category: cs.CV

TL;DR: 提出了4DPC$^2$hat，这是一个专为动态点云理解设计的MMLM，包含一个大规模的跨模态数据集4DPC$^2$hat-200K，和一种失败感知的强化学习策略来提高模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注静态点云，而动态点云的理解仍未被充分探索。4DPC$^2$hat旨在解决动态点云理解的挑战。

Method: 构建了一个大规模的跨模态数据集4DPC$^2$hat-200K，使用Mamba增强的时序推理MMLM来捕捉长距离依赖和动态模式，提出了一种失败感知的强化学习策略来逐步增强模型的推理能力。

Result: 我们的4DPC$^2$hat在动作理解和时间推理方面优于现有的模型，为四维动态点云理解奠定了坚实的基础。

Conclusion: 4DPC$^2$hat通过创新的数据集构建和模型设计，显著提升了动态点云理解的能力。

Abstract: Point clouds provide a compact and expressive representation of 3D objects, and have recently been integrated into multimodal large language models (MLLMs). However, existing methods primarily focus on static objects, while understanding dynamic point cloud sequences remains largely unexplored. This limitation is mainly caused by the lack of large-scale cross-modal datasets and the difficulty of modeling motions in spatio-temporal contexts. To bridge this gap, we present 4DPC$^2$hat, the first MLLM tailored for dynamic point cloud understanding. To this end, we construct a large-scale cross-modal dataset 4DPC$^2$hat-200K via a meticulous two-stage pipeline consisting of topology-consistent 4D point construction and two-level captioning. The dataset contains over 44K dynamic object sequences, 700K point cloud frames, and 200K curated question-answer (QA) pairs, supporting inquiries about counting, temporal relationship, action, spatial relationship, and appearance. At the core of the framework, we introduce a Mamba-enhanced temporal reasoning MLLM to capture long-range dependencies and dynamic patterns among a point cloud sequence. Furthermore, we propose a failure-aware bootstrapping learning strategy that iteratively identifies model deficiencies and generates targeted QA supervision to continuously strengthen corresponding reasoning capabilities. Extensive experiments demonstrate that our 4DPC$^2$hat significantly improves action understanding and temporal reasoning compared with existing models, establishing a strong foundation for 4D dynamic point cloud understanding.

</details>


### [3] [Audit After Segmentation: Reference-Free Mask Quality Assessment for Language-Referred Audio-Visual Segmentation](https://arxiv.org/abs/2602.03892)
*Jinxing Zhou,Yanghao Zhou,Yaoting Wang,Zongyan Han,Jiaqi Ma,Henghui Ding,Rao Muhammad Anwer,Hisham Cholakkal*

Main category: cs.CV

TL;DR: 论文介绍了一种新的任务——掩码质量评估（MQA-RefAVS），旨在评估参考音频-视觉的候选分割掩码的质量。同时，构建了MQ-RAVSBench基准，并提出了基于多模态大语言模型（MLLM）的MQ-Auditor评估工具，用于生成定量和定性的分割掩码质量评估。


<details>
  <summary>Details</summary>
Motivation: 当前的Ref-AVS任务主要关注生成分割掩码，而关于掩码质量的评估方法尚未得到充分探索。为了解决该问题，本文提出了一个新的任务，即掩码质量评估（MQA-RefAVS）。

Method: 通过构建一个新的基准MQ-RAVSBench，该基准包含了不同类型的掩码错误模式，并使用基于多模态大语言模型（MLLM）的方法进行质量评估。

Result: 所提出的MQ-Auditor在大量实验中表现出色，能够超越开源和商业多模态大语言模型，支持现有的Ref-AVS系统的分割失败检测和后续优化。

Conclusion: 本文的工作为参考音频-视觉的掩码质量评估奠定了基础，同时提出的方法具有实用价值。

Abstract: Language-referred audio-visual segmentation (Ref-AVS) aims to segment target objects described by natural language by jointly reasoning over video, audio, and text. Beyond generating segmentation masks, providing rich and interpretable diagnoses of mask quality remains largely underexplored. In this work, we introduce Mask Quality Assessment in the Ref-AVS context (MQA-RefAVS), a new task that evaluates the quality of candidate segmentation masks without relying on ground-truth annotations as references at inference time. Given audio-visual-language inputs and each provided segmentation mask, the task requires estimating its IoU with the unobserved ground truth, identifying the corresponding error type, and recommending an actionable quality-control decision. To support this task, we construct MQ-RAVSBench, a benchmark featuring diverse and representative mask error modes that span both geometric and semantic issues. We further propose MQ-Auditor, a multimodal large language model (MLLM)-based auditor that explicitly reasons over multimodal cues and mask information to produce quantitative and qualitative mask quality assessments. Extensive experiments demonstrate that MQ-Auditor outperforms strong open-source and commercial MLLMs and can be integrated with existing Ref-AVS systems to detect segmentation failures and support downstream segmentation improvement. Data and codes will be released at https://github.com/jasongief/MQA-RefAVS.

</details>


### [4] [GPAIR: Gaussian-Kernel-Based Ultrafast 3D Photoacoustic Iterative Reconstruction](https://arxiv.org/abs/2602.03893)
*Yibing Wang,Shuang Li,Tingting Huang,Yu Zhang,Chulhong Kim,Seongwook Choi,Changhui Li*

Main category: cs.CV

TL;DR: GPAIR 提供了一种加速的迭代重建方法，实现了超快速的 3D PACT 图像重建。


<details>
  <summary>Details</summary>
Motivation: 传统的迭代重建算法在 3D PACT 中虽然能有效减少重建伪影，但计算时间过长，限制了其临床应用。

Method: GPAIR 利用了连续各向同性高斯核将传统空间网格转换，并通过推导声压波的解析封闭式表达式和利用 GPU 加速可微 Triton 操作符实现超快速重建。

Result: GPAIR 实验中展示了对于含有 8.4 百万体素的 3D 目标，仅需亚秒级重建时间。

Conclusion: GPAIR 的超快速图像重建能力推动 3D PACT 进入临床应用领域。

Abstract: Although the iterative reconstruction (IR) algorithm can substantially correct reconstruction artifacts in photoacoustic (PA) computed tomography (PACT), it suffers from long reconstruction times, especially for large-scale three-dimensional (3D) imaging in which IR takes hundreds of seconds to hours. The computing burden severely limits the practical applicability of IR algorithms. In this work, we proposed an ultrafast IR method for 3D PACT, called Gaussian-kernel-based Ultrafast 3D Photoacoustic Iterative Reconstruction (GPAIR), which achieves orders-of-magnitude acceleration in computing. GPAIR transforms traditional spatial grids with continuous isotropic Gaussian kernels. By deriving analytical closed-form expression for pressure waves and implementing powerful GPU-accelerated differentiable Triton operators, GPAIR demonstrates extraordinary ultrafast sub-second reconstruction speed for 3D targets containing 8.4 million voxels in animal experiments. This revolutionary ultrafast image reconstruction enables near-real-time large-scale 3D PA reconstruction, significantly advancing 3D PACT toward clinical applications.

</details>


### [5] [Vision Transformers for Zero-Shot Clustering of Animal Images: A Comparative Benchmarking Study](https://arxiv.org/abs/2602.03894)
*Hugo Markoff,Stefan Hein Bengtson,Michael Ørsted*

Main category: cs.CV

TL;DR: 本研究通过全面的基准框架评估了五种Vision Transformer (ViT) 模型结合五种降维技术和四种聚类算法在动物图像分类中的性能，证明了使用DINOv3嵌入、t-SNE和监督层次聚类方法可以在物种级达到近乎完美的聚类效果（V-测度：0.958），并且无监督方法也表现出了竞争力。


<details>
  <summary>Details</summary>
Motivation: 生态研究中手动标注动物图像仍然是一个瓶颈，限制了生物多样性监测的规模和效率。研究旨在探索最先进的Vision Transformer (ViT) 基础模型是否可以直接将数千张未标注的动物图像聚类到物种级别。

Method: 研究构建了一个全面的基准框架，评估了五种ViT模型结合五种降维技术和四种聚类算法（两种监督聚类和两种无监督聚类）的性能。基准测试在60种物种（30种哺乳动物和30种鸟类）的200张随机验证图像上进行，研究还探讨了聚类在物种级的成功与失败情况，以及在物种内是否能揭示如性别、年龄或表型变异等生态上具有意义的模式。

Result: 使用DINOv3嵌入、t-SNE和监督层次聚类方法在物种级别实现了近乎完美的聚类效果（V-测度：0.958）。无监督方法表现出了竞争力（V-测度：0.943），同时无需先验的物种知识，仅拒绝了1.14%的图像作为需要专家审核的异常值。研究还证明了在实际长尾分布中具有鲁棒性，并且过度聚类方法可以可靠地提取出物种内的变异，如年龄分组、性别差异和体色差异。

Conclusion: 本研究为生态学家提供了选择合适方法对特定分类群和数据进行分类的建议，并引入了一个开源基准工具箱。

Abstract: Manual labeling of animal images remains a significant bottleneck in ecological research, limiting the scale and efficiency of biodiversity monitoring efforts. This study investigates whether state-of-the-art Vision Transformer (ViT) foundation models can reduce thousands of unlabeled animal images directly to species-level clusters. We present a comprehensive benchmarking framework evaluating five ViT models combined with five dimensionality reduction techniques and four clustering algorithms, two supervised and two unsupervised, across 60 species (30 mammals and 30 birds), with each test using a random subset of 200 validated images per species. We investigate when clustering succeeds at species-level, where it fails, and whether clustering within the species-level reveals ecologically meaningful patterns such as sex, age, or phenotypic variation. Our results demonstrate near-perfect species-level clustering (V-measure: 0.958) using DINOv3 embeddings with t-SNE and supervised hierarchical clustering methods. Unsupervised approaches achieve competitive performance (0.943) while requiring no prior species knowledge, rejecting only 1.14% of images as outliers requiring expert review. We further demonstrate robustness to realistic long-tailed distributions of species and show that intentional over-clustering can reliably extract intra-specific variation including age classes, sexual dimorphism, and pelage differences. We introduce an open-source benchmarking toolkit and provide recommendations for ecologists to select appropriate methods for sorting their specific taxonomic groups and data.

</details>


### [6] [Benchmarking Bias Mitigation Toward Fairness Without Harm from Vision to LVLMs](https://arxiv.org/abs/2602.03895)
*Xuwei Tan,Ziyu Hu,Xueru Zhang*

Main category: cs.CV

TL;DR: 该研究介绍了一种名为NH-Fair的统一公平性基准，旨在减少机器学习模型中的偏见，并通过系统的研究和分析提供了一种实用的方法来实现公平性和准确性。


<details>
  <summary>Details</summary>
Motivation: 由于实际数据集中的偏见问题，机器学习模型在实际应用中存在安全隐患。研究引入NH-Fair基准，旨在提供一种均匀的数据、度量标准和训练协议，用于评估多模态模型中的公平性问题。

Method: 该研究通过系统地研究和分析，对误差最小化（ERM）训练参数进行调优，识别影响准确性和差距的主要因素，证明许多去偏方法在实际场景下不一定可靠，从而提出了一种结合数据增强方法的实用策略，NH-Fair提供了一种可重复的、调参意识强的评估流程。

Result: 研究发现，受欢迎的方法不一定比经过良好调优的基本ERM方法更有效，而是发现组合的数据增强方法能够在不影响效用的情况下持续地实现公平性提升。研究还显示，尽管大型多模态语言模型（LVLMs）的平均准确率较高，但它们在各个子组别上仍然存在差距，规模优势不如架构或训练流程的选择。

Conclusion: NH-Fair通过标准化的数据、度量标准和训练协议提供了一种可复制且对调参有意识的管道，使其能够进行更严谨且可控的公平性评价，有效地减少了高性能模型中的偏见。

Abstract: Machine learning models trained on real-world data often inherit and amplify biases against certain social groups, raising urgent concerns about their deployment at scale. While numerous bias mitigation methods have been proposed, comparing the effectiveness of bias mitigation methods remains difficult due to heterogeneous datasets, inconsistent fairness metrics, isolated evaluation of vision versus multi-modal models, and insufficient hyperparameter tuning that undermines fair comparisons. We introduce NH-Fair, a unified benchmark for fairness without harm that spans both vision models and large vision-language models (LVLMs) under standardized data, metrics, and training protocols, covering supervised and zero-shot regimes. Our key contributions are: (1) a systematic ERM tuning study that identifies training choices with large influence on both utility and disparities, yielding empirically grounded guidelines to help practitioners reduce expensive hyperparameter tuning space in achieving strong fairness and accuracy; (2) evidence that many debiasing methods do not reliably outperform a well-tuned ERM baseline, whereas a composite data-augmentation method consistently delivers parity gains without sacrificing utility, emerging as a promising practical strategy. (3) an analysis showing that while LVLMs achieve higher average accuracy, they still exhibit subgroup disparities, and gains from scaling are typically smaller than those from architectural or training-protocol choices. NH-Fair provides a reproducible, tuning-aware pipeline for rigorous, harm-aware fairness evaluation.

</details>


### [7] [HY3D-Bench: Generation of 3D Assets](https://arxiv.org/abs/2602.03907)
*Team Hunyuan3D,:,Bowen Zhang,Chunchao Guo,Dongyuan Guo,Haolin Liu,Hongyu Yan,Huiwen Shi,Jiaao Yu,Jiachen Xu,Jingwei Huang,Kunhong Li,Lifu Wang,Linus,Penghao Wang,Qingxiang Lin,Ruining Tang,Xianghui Yang,Yang Li,Yirui Guan,Yunfei Zhao,Yunhan Yang,Zeqiang Lai,Zhihao Liang,Zibo Zhao*

Main category: cs.CV

TL;DR: 本文介绍了HY3D-Bench，这是一个开源生态系统，旨在为3D生成建立一个统一且高质量的基础。它包括25万个高质量3D对象和一种结构化的部分级分解方法，还通过AIGC合成管道贡献了12.5万个合成资产。


<details>
  <summary>Details</summary>
Motivation: 尽管神经表示和生成模型在3D内容创建方面取得了重大进展，但其数据处理瓶颈依然制约着这一领域的发展。HY3D-Bench旨在解决这一问题，提供一个统一且高质量的3D生成基础。

Method: 该方法主要通过构建一个由25万个高质量3D对象组成的库，以及引入了一种结构化的部分级分解方法，并通过AIGC合成管道增加了12.5万个合成资产。

Result: HY3D-Bench在训练Hunyuan3D-2.1-Small时经过实验证明可行，旨在使3D感知、机器人技术和数字内容创作等领域能够更容易获得可靠的数据资源。

Conclusion: HY3D-Bench通过提供高质量的数据资源和重要的方法改进，将促进3D领域的创新和发展。

Abstract: While recent advances in neural representations and generative models have revolutionized 3D content creation, the field remains constrained by significant data processing bottlenecks. To address this, we introduce HY3D-Bench, an open-source ecosystem designed to establish a unified, high-quality foundation for 3D generation. Our contributions are threefold: (1) We curate a library of 250k high-fidelity 3D objects distilled from large-scale repositories, employing a rigorous pipeline to deliver training-ready artifacts, including watertight meshes and multi-view renderings; (2) We introduce structured part-level decomposition, providing the granularity essential for fine-grained perception and controllable editing; and (3) We bridge real-world distribution gaps via a scalable AIGC synthesis pipeline, contributing 125k synthetic assets to enhance diversity in long-tail categories. Validated empirically through the training of Hunyuan3D-2.1-Small, HY3D-Bench democratizes access to robust data resources, aiming to catalyze innovation across 3D perception, robotics, and digital content creation.

</details>


### [8] [Phaedra: Learning High-Fidelity Discrete Tokenization for the Physical Science](https://arxiv.org/abs/2602.03915)
*Levi Lingsch,Georgios Kissas,Johannes Jakubik,Siddhartha Mishra*

Main category: cs.CV

TL;DR: 本文研究了一组图像令牌化器在物理和光谱域内的PDE属性保真度方面的准确性，并提出了一种新方法Phaedra，能够提供更好的重建效果和跨不同复杂度任务的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的图像令牌化方法主要用于现实图像的视觉感知，但对于科学图像而言，后者需要保留物理和光谱特性，而现有方法难以同时捕捉图像的精细细节和准确的量值。

Method: 本文通过一组图像令牌化器在多种度量标准下的性能测试来验证其在物理和谱域中PDE属性保真度方面的准确性，根据测试结果提出了一种名为Phaedra的新方法，该方法基于古典形状-增益量化和恰当正交分解。

Result: Phaedra能够在各种PDE数据集上提供一致的重建改善效果，并且表现出强大的跨复杂任务的泛化能力，包括已知不同条件的PDEs、未知PDEs以及真实世界的地球观测和气象数据。

Conclusion: 实验结果表明，在物理和谱域的PDE属性保真度方面，Phaedra优于现有的构建图像的令牌化方法，可以更好地支持物理模拟和数据分析任务。

Abstract: Tokens are discrete representations that allow modern deep learning to scale by transforming high-dimensional data into sequences that can be efficiently learned, generated, and generalized to new tasks. These have become foundational for image and video generation and, more recently, physical simulation. As existing tokenizers are designed for the explicit requirements of realistic visual perception of images, it is necessary to ask whether these approaches are optimal for scientific images, which exhibit a large dynamic range and require token embeddings to retain physical and spectral properties. In this work, we investigate the accuracy of a suite of image tokenizers across a range of metrics designed to measure the fidelity of PDE properties in both physical and spectral space. Based on the observation that these struggle to capture both fine details and precise magnitudes, we propose Phaedra, inspired by classical shape-gain quantization and proper orthogonal decomposition. We demonstrate that Phaedra consistently improves reconstruction across a range of PDE datasets. Additionally, our results show strong out-of-distribution generalization capabilities to three tasks of increasing complexity, namely known PDEs with different conditions, unknown PDEs, and real-world Earth observation and weather data.

</details>


### [9] [SpatiaLab: Can Vision-Language Models Perform Spatial Reasoning in the Wild?](https://arxiv.org/abs/2602.03916)
*Azmine Toushik Wasi,Wahid Faisal,Abdur Rahman,Mahfuz Ahmed Anik,Munem Shahriar,Mohsin Mahmud Topu,Sadia Tasnim Meem,Rahatun Nesa Priti,Sabrina Afroz Mitu,Md. Iqramul Hoque,Shahriyar Zaman Ridoy,Mohammed Eunus Ali,Majd Hawasly,Mohammad Raza,Md Rizwan Parvez*

Main category: cs.CV

TL;DR: SpatiaLab 提供了一个综合基准，用于评估视觉-语言模型在真实、非约束环境下的空间推理能力。通过多个任务类型和多元模型的实验，揭示了现有的视觉-语言模型在空间推理方面的局限性，并为未来的研究提供基准指导。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言模型在处理复杂的空间关系、深度感知、导航和3D几何方面存在明显不足，SpatiaLab旨在填补这一空白，提供一个更具挑战性的现实空间推理评估框架。

Method: SpatiaLab 包含1400个视觉问答对，涵盖了六个主要类别和30种具体任务类型。通过多样化的现代视觉-语言模型进行实验，比较这些模型在多项选择和开放式回答上的表现。

Result: 现有视觉-语言模型在多项选择设置中的准确性远低于人类水平，平均仅54.93%对87.57%，开放设置中表现更差，模型表现下降10-25%。具体来说，InternVL3.5-72B在多项选择中达到54.93%的精度，而人类为87.57%。GPT-5-mini在开放设置中得分最高，为40.93%，而人类表现则为64.93%。

Conclusion: 此基准揭示了现有模型在处理复杂空间关系、深度感知、导航和3D几何方面的局限性，并为未来的研究指明了方向，有助于推进视觉-语言模型更加接近人类的空间理解能力。

Abstract: Spatial reasoning is a fundamental aspect of human cognition, yet it remains a major challenge for contemporary vision-language models (VLMs). Prior work largely relied on synthetic or LLM-generated environments with limited task designs and puzzle-like setups, failing to capture the real-world complexity, visual noise, and diverse spatial relationships that VLMs encounter. To address this, we introduce SpatiaLab, a comprehensive benchmark for evaluating VLMs' spatial reasoning in realistic, unconstrained contexts. SpatiaLab comprises 1,400 visual question-answer pairs across six major categories: Relative Positioning, Depth & Occlusion, Orientation, Size & Scale, Spatial Navigation, and 3D Geometry, each with five subcategories, yielding 30 distinct task types. Each subcategory contains at least 25 questions, and each main category includes at least 200 questions, supporting both multiple-choice and open-ended evaluation. Experiments across diverse state-of-the-art VLMs, including open- and closed-source models, reasoning-focused, and specialized spatial reasoning models, reveal a substantial gap in spatial reasoning capabilities compared with humans. In the multiple-choice setup, InternVL3.5-72B achieves 54.93% accuracy versus 87.57% for humans. In the open-ended setting, all models show a performance drop of around 10-25%, with GPT-5-mini scoring highest at 40.93% versus 64.93% for humans. These results highlight key limitations in handling complex spatial relationships, depth perception, navigation, and 3D geometry. By providing a diverse, real-world evaluation framework, SpatiaLab exposes critical challenges and opportunities for advancing VLMs' spatial reasoning, offering a benchmark to guide future research toward robust, human-aligned spatial understanding. SpatiaLab is available at: https://spatialab-reasoning.github.io/.

</details>


### [10] [Entropy Reveals Block Importance in Masked Self-Supervised Vision Transformers](https://arxiv.org/abs/2602.03918)
*Peihao Xiang,Kaida Wu,Ou Bai*

Main category: cs.CV

TL;DR: 通过无需数据的参数权重信息熵计算，Gardener算法能够在不降低下游性能的情况下大幅减少Transformer模块的数量。


<details>
  <summary>Details</summary>
Motivation: 研究提出了Gardener算法以应对大规模模型在资源受限环境下的部署挑战和高效迁移学习的需求。

Method: 利用预制块权重的信息熵与迭代区块移除和微调后的或然敏感度之间的强相关性，通过信息理论测量来确定冗余模块。

Result: Gardener算法在VideoMAE-B上进行多比例剪枝和多个视频识别基准测试后，即使在剪枝91.7%模块后，剪枝模型仍保持了竞争力。

Conclusion: 结果显示，在掩蔽自我监督的视觉转换器中存在显著的区块级别冗余，并表明信息论分析提供了一种有原则且高效的模型压缩和资源高效迁移学习途径。

Abstract: Masked self-supervised vision transformers have become a dominant pretraining paradigm, yet their substantial model size poses significant challenges for resource-constrained deployment and efficient transfer learning. A fundamental question remains: are all transformer blocks equally important for downstream performance? In this paper, we show that block importance in masked self-supervised vision transformers can be accurately estimated without access to any data. Our key finding is that the information entropy of pretrained block weights strongly correlates with oracle sensitivity obtained via iterative block removal and finetuning. This observation enables Gardener, a data-free, one-shot, block-level pruning principle that identifies redundant blocks through simple information-theoretic measurements. We evaluate Gardener on VideoMAE-B across multiple pruning ratios and downstream video recognition benchmarks. Despite its negligible computational overhead, Gardener consistently matches or outperforms existing data-free pruning baselines and closely approaches sensitivity-based pruning. Remarkably, even after pruning up to 91.7\% of blocks, the pruned model retains competitive transfer performance. Our results reveal substantial block-level redundancy in masked self-supervised vision transformers and demonstrate that information-theoretic analysis offers a principled and efficient pathway for model compression and resource-efficient transfer learning.

</details>


### [11] [TiCLS : Tightly Coupled Language Text Spotter](https://arxiv.org/abs/2602.04030)
*Leeje Jang,Yijun Lin,Yao-Yi Chiang,Jerod Weinman*

Main category: cs.CV

TL;DR: TiCLS 是一种端到端的文字检测系统，它结合了字符级别的预训练语言模型中的外部语言知识，通过语言解码器融合视觉和语言特征，增强对模糊或断开文本的识别能力，从而在 ICDAR 2015 和 Total-Text 数据集上实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 当前的文字检测方法主要依靠视觉线索来捕捉局部字符依赖性，但忽视了外部语言知识的利用。TiCLS 的提出旨在通过结合预训练语言模型中的语言知识，提高对模糊或断开文本的识别能力。

Method: TiCLS 引入了一个语言解码器，能够融合视觉和语言特征，同时可以通过预训练语言模型进行初始化，从而提高对模糊或断开文本的鲁棒性识别。

Result: TiCLS 在 ICDAR 2015 和 Total-Text 数据集上的实验结果表明，它在文字检测方面表现优异，验证了使用 PLM 指导的语言集成的有效性。

Conclusion: TiCLS 方法为场景文字检测带来了显著改进，通过利用预训练语言模型中的语言知识，提高了对模糊或断开文本的识别性能。

Abstract: Scene text spotting aims to detect and recognize text in real-world images, where instances are often short, fragmented, or visually ambiguous. Existing methods primarily rely on visual cues and implicitly capture local character dependencies, but they overlook the benefits of external linguistic knowledge. Prior attempts to integrate language models either adapt language modeling objectives without external knowledge or apply pretrained models that are misaligned with the word-level granularity of scene text. We propose TiCLS, an end-to-end text spotter that explicitly incorporates external linguistic knowledge from a character-level pretrained language model. TiCLS introduces a linguistic decoder that fuses visual and linguistic features, yet can be initialized by a pretrained language model, enabling robust recognition of ambiguous or fragmented text. Experiments on ICDAR 2015 and Total-Text demonstrate that TiCLS achieves state-of-the-art performance, validating the effectiveness of PLM-guided linguistic integration for scene text spotting.

</details>


### [12] [AnyStyle: Single-Pass Multimodal Stylization for 3D Gaussian Splatting](https://arxiv.org/abs/2602.04043)
*Joanna Kaleta,Bartosz Świrta,Kacper Kania,Przemysław Spurek,Marek Kowalski*

Main category: cs.CV

TL;DR: AnyStyle 是一种全新的基于 3D 软体素的前馈重建框架，能够实现无姿态、零样本的风格化效果，支持文本和视觉风格输入，提高了风格控制的便捷性。


<details>
  <summary>Details</summary>
Motivation: 随着对快速且可扩展的 3D 资产需求的增长，现有前馈 3D 重建方法并未充分利用文本和视觉信息作为多模态条件，限制了重建风格化的可控性和灵活性。

Method: AnyStyle 引入了一种模组化风格化架构，可通过文本和图像作为输入来控制 3D 场景的外观。该模组化设计允许用户利用自然语言描述或参照图像来调整 3D 场景的样式。

Result: 实验表明，AnyStyle 相较于现有前馈风格化方法提高了风格控制的灵活性并保持了高质量的几何重建。用户研究进一步证明了 AnyStyle 较现有最先进的方法具有更好的风格化质量。

Conclusion: AnyStyle 通过引入全新的模组化风格化架构，实现了无姿态、零样本的风格化效果，极大地提高了风格化过程的灵活性和可控性。

Abstract: The growing demand for rapid and scalable 3D asset creation has driven interest in feed-forward 3D reconstruction methods, with 3D Gaussian Splatting (3DGS) emerging as an effective scene representation. While recent approaches have demonstrated pose-free reconstruction from unposed image collections, integrating stylization or appearance control into such pipelines remains underexplored. Existing attempts largely rely on image-based conditioning, which limits both controllability and flexibility. In this work, we introduce AnyStyle, a feed-forward 3D reconstruction and stylization framework that enables pose-free, zero-shot stylization through multimodal conditioning. Our method supports both textual and visual style inputs, allowing users to control the scene appearance using natural language descriptions or reference images. We propose a modular stylization architecture that requires only minimal architectural modifications and can be integrated into existing feed-forward 3D reconstruction backbones. Experiments demonstrate that AnyStyle improves style controllability over prior feed-forward stylization methods while preserving high-quality geometric reconstruction. A user study further confirms that AnyStyle achieves superior stylization quality compared to an existing state-of-the-art approach. Repository: https://github.com/joaxkal/AnyStyle.

</details>


### [13] [Fast, Unsupervised Framework for Registration Quality Assessment of Multi-stain Histological Whole Slide Pairs](https://arxiv.org/abs/2602.04046)
*Shikha Dubey,Patricia Raciti,Kristopher Standish,Albert Juan Ramon,Erik Ames Burlingame*

Main category: cs.CV

TL;DR: 本文提出了一种快速的无监督框架，用于评估注册的H&E和IHC全息图像对的质量，该框架结合了采样的组织掩模和变形计算，为大规模的数字病理学的质量控制提供了可靠且实时的评价。


<details>
  <summary>Details</summary>
Motivation: 现有的WSI级评估方法，如使用注释的地标或基于强度的相似性指标，耗时、不可靠且计算密集，限制了其大规模应用。因此，需要开发一种快速、无监督的评价框架。

Method: 本文提出的方法结合了采样的组织掩模和变形计算来评估注册的H&E和IHC WSI对的质量。掩模基指标衡量全局结构对应性，而变形基指标评估局部平滑性、连续性和变换真实度。

Result: 在多种IHC标记和多专家评估中，自动化指标与人工评估具有较高的相关性。并且，该框架能在没有GT的情况下，提供可靠且实时的质量评估，具有高精度和较少的计算资源。

Conclusion: 该研究提供了一种新的无监督评价框架，通过结合采样的组织掩模和变形计算，实现了注册WSI对质量评价的自动化，为数字病理学的大量应用提供了一种可靠的解决方案。

Abstract: High-fidelity registration of histopathological whole slide images (WSIs), such as hematoxylin & eosin (H&E) and immunohistochemistry (IHC), is vital for integrated molecular analysis but challenging to evaluate without ground-truth (GT) annotations. Existing WSI-level assessments -- using annotated landmarks or intensity-based similarity metrics -- are often time-consuming, unreliable, and computationally intensive, limiting large-scale applicability. This study proposes a fast, unsupervised framework that jointly employs down-sampled tissue masks- and deformations-based metrics for registration quality assessment (RQA) of registered H&E and IHC WSI pairs. The masks-based metrics measure global structural correspondence, while the deformations-based metrics evaluate local smoothness, continuity, and transformation realism. Validation across multiple IHC markers and multi-expert assessments demonstrate a strong correlation between automated metrics and human evaluations. In the absence of GT, this framework offers reliable, real-time RQA with high fidelity and minimal computational resources, making it suitable for large-scale quality control in digital pathology.

</details>


### [14] [Seeing Through Clutter: Structured 3D Scene Reconstruction via Iterative Object Removal](https://arxiv.org/abs/2602.04053)
*Rio Aguina-Kang,Kevin James Blackburn-Matzen,Thibault Groueix,Vladimir Kim,Matheus Gadelha*

Main category: cs.CV

TL;DR: 本文提出了一种名为SeeingThroughClutter的方法，通过逐个分割和建模对象来重建复杂场景的结构化3D表示，无需依赖中间任务，从而提高了在复杂场景中的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前的方法依赖于如语义分割和深度估计等中间任务，在复杂场景中表现不佳，尤其是在遮挡和杂物的存在下。本文提出的方法旨在解决这一问题。

Method: 该方法通过使用视觉语言模型（VLMs）作为协调器，逐个移除并重建前景对象，分解复杂场景为一系列简单的子任务。具体步骤包括检测、分割、移除对象和3D拟合。

Result: 实验结果显示，这种方法可以在高度遮挡的场景中提供更清洁的对象分割，并在3D-Front和ADE20K数据集中达到了最先进的鲁棒性。

Conclusion: 该研究提出了一种新的方法，能够从单张图像中重建结构化的3D表示，具有很高的鲁棒性，并且这种方法不需要针对特定任务的训练。

Abstract: We present SeeingThroughClutter, a method for reconstructing structured 3D representations from single images by segmenting and modeling objects individually. Prior approaches rely on intermediate tasks such as semantic segmentation and depth estimation, which often underperform in complex scenes, particularly in the presence of occlusion and clutter. We address this by introducing an iterative object removal and reconstruction pipeline that decomposes complex scenes into a sequence of simpler subtasks. Using VLMs as orchestrators, foreground objects are removed one at a time via detection, segmentation, object removal, and 3D fitting. We show that removing objects allows for cleaner segmentations of subsequent objects, even in highly occluded scenes. Our method requires no task-specific training and benefits directly from ongoing advances in foundation models. We demonstrate stateof-the-art robustness on 3D-Front and ADE20K datasets. Project Page: https://rioak.github.io/seeingthroughclutter/

</details>


### [15] [iSight: Towards expert-AI co-assessment for improved immunohistochemistry staining interpretation](https://arxiv.org/abs/2602.04063)
*Jacob S. Leiby,Jialu Yao,Pan Lu,George Hu,Anna Davidian,Shunsuke Koga,Olivia Leung,Pravin Patel,Isabella Tondi Resta,Rebecca Rojansky,Derek Sung,Eric Yang,Paul J. Zhang,Emma Lundberg,Dokyoon Kim,Serena Yeung-Levy,James Zou,Thomas Montine,Jeffrey Nirschl,Zhi Huang*

Main category: cs.CV

TL;DR: 本文介绍了一个大规模的IHC图像数据集HPA10M及基于该数据集训练的多任务学习框架iSight，该框架在IHC斑点强度、位置和数量预测方面表现出色，同时还能预测组织类型和恶性程度，提高病理学家之间的共识。


<details>
  <summary>Details</summary>
Motivation: IHC在病理诊断和疾病分类中应用广泛，但现有AI模型在IHC图像上的应用受到限制。本文提出的HPA10M数据集和iSight框架旨在解决这一问题，提高AI在IHC解读中的表现。

Method: 研究团队使用了一个包含10,495,672张IHC图像及全面元数据的大型数据集HPA10M，训练了iSight多任务学习框架。该框架结合了整个切片图像的视觉特征和组织元数据，通过token级注意力机制对IHC染色强度、位置、数量、组织类型和恶性程度进行预测。

Result: iSight在策略一项测试中表现优异，位置预测精度达到85.5%，强度预测精度达到76.6%，数量预测精度达到75.7%。iSight的预测还显示出良好的校准，预期校准误差为0.0150-0.0408。此外，iSight在用户研究中比初诊病理学家在位置（79% vs 68%）、强度（70% vs 57%）、数量（68% vs 52%）方面表现更好。在专家-AI协同评估中，病理学家之间的Cohen's $κ$系数提高。

Conclusion: iSight为AI系统在IHC诊断中的应用奠定了基础，并表明AI协助可以在提高IHC解释的准确性和一致性方面发挥作用。未来可将iSight融入临床工作流程，以增强IHC评估的一致性和可靠性。

Abstract: Immunohistochemistry (IHC) provides information on protein expression in tissue sections and is commonly used to support pathology diagnosis and disease triage. While AI models for H\&E-stained slides show promise, their applicability to IHC is limited due to domain-specific variations. Here we introduce HPA10M, a dataset that contains 10,495,672 IHC images from the Human Protein Atlas with comprehensive metadata included, and encompasses 45 normal tissue types and 20 major cancer types. Based on HPA10M, we trained iSight, a multi-task learning framework for automated IHC staining assessment. iSight combines visual features from whole-slide images with tissue metadata through a token-level attention mechanism, simultaneously predicting staining intensity, location, quantity, tissue type, and malignancy status. On held-out data, iSight achieved 85.5\% accuracy for location, 76.6\% for intensity, and 75.7\% for quantity, outperforming fine-tuned foundation models (PLIP, CONCH) by 2.5--10.2\%. In addition, iSight demonstrates well-calibrated predictions with expected calibration errors of 0.0150-0.0408. Furthermore, in a user study with eight pathologists evaluating 200 images from two datasets, iSight outperformed initial pathologist assessments on the held-out HPA dataset (79\% vs 68\% for location, 70\% vs 57\% for intensity, 68\% vs 52\% for quantity). Inter-pathologist agreement also improved after AI assistance in both held-out HPA (Cohen's $κ$ increased from 0.63 to 0.70) and Stanford TMAD datasets (from 0.74 to 0.76), suggesting expert--AI co-assessment can improve IHC interpretation. This work establishes a foundation for AI systems that can improve IHC diagnostic accuracy and highlights the potential for integrating iSight into clinical workflows to enhance the consistency and reliability of IHC assessment.

</details>


### [16] [DMS2F-HAD: A Dual-branch Mamba-based Spatial-Spectral Fusion Network for Hyperspectral Anomaly Detection](https://arxiv.org/abs/2602.04102)
*Aayushma Pant,Lakpa Tamang,Tsz-Kwan Lee,Sunil Aryal*

Main category: cs.CV

TL;DR: 该方法提出了一种针对高维高光谱图像（HSI）中罕见和不规则目标的新型双分支DMS2F-HAD模型，通过Mamba高效学习空间和光谱特征并动态融合，展示了在14个基准HSI数据集上优越的AUC性能和更快的推理速度。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习方法往往难以同时捕捉长时间范围的光谱依赖性和计算效率，DMS2F-HAD旨在解决这些问题，提高高光谱异常检测的效率和准确性。

Method: 该方法基于Mamba模型，采用了双分支结构，分别学习空间和光谱特征，并通过动态门控机制进行特征融合，实现了高效和准确的异常目标定位。

Result: DMS2F-HAD在14个基准HSI数据集上的平均AUC达到98.78%，与现有方法相比，推理速度提高了4.6倍。

Conclusion: 该研究表明DMS2F-HAD具有较强的泛化能力和可扩展性，是现实世界HAD应用的理想选择。

Abstract: Hyperspectral anomaly detection (HAD) aims to identify rare and irregular targets in high-dimensional hyperspectral images (HSIs), which are often noisy and unlabelled data. Existing deep learning methods either fail to capture long-range spectral dependencies (e.g., convolutional neural networks) or suffer from high computational cost (e.g., Transformers). To address these challenges, we propose DMS2F-HAD, a novel dual-branch Mamba-based model. Our architecture utilizes Mamba's linear-time modeling to efficiently learn distinct spatial and spectral features in specialized branches, which are then integrated by a dynamic gated fusion mechanism to enhance anomaly localization. Across fourteen benchmark HSI datasets, our proposed DMS2F-HAD not only achieves a state-of-the-art average AUC of 98.78%, but also demonstrates superior efficiency with an inference speed 4.6 times faster than comparable deep learning methods. The results highlight DMS2FHAD's strong generalization and scalability, positioning it as a strong candidate for practical HAD applications.

</details>


### [17] [SuperPoint-E: local features for 3D reconstruction via tracking adaptation in endoscopy](https://arxiv.org/abs/2602.04108)
*O. Leon Barbed,José M. M. Montiel,Pascal Fua,Ana C. Murillo*

Main category: cs.CV

TL;DR: 该研究提出了一种新的局部特征提取方法SuperPoint-E，通过改进视内视频的特征检测和描述质量，提升了Structure-from-Motion (SfM) 的性能。


<details>
  <summary>Details</summary>
Motivation: 本文旨在提升视内视频的Structure-from-Motion (SfM) 表现，通过增强特征提取以优化重建效果。

Method: 研究利用了追踪适应监督策略，提出了SuperPoint-E作为新的局部特征提取方法。

Result: 实验表明，与基准方法相比，提出的SuperPoint-E在视内视频重建中能生成更密集、覆盖更长视频段的3D结构。改进了特征匹配步骤，通过提高检测精度和特征描述的区分度，实现了显著的重建性能提升。

Conclusion: 研究表明，通过引入SuperPoint-E，SfM在视内视频上的3D重建质量有了显著提高，可以有效用于更多的可视化手术视频分析任务。

Abstract: In this work, we focus on boosting the feature extraction to improve the performance of Structure-from-Motion (SfM) in endoscopy videos. We present SuperPoint-E, a new local feature extraction method that, using our proposed Tracking Adaptation supervision strategy, significantly improves the quality of feature detection and description in endoscopy. Extensive experimentation on real endoscopy recordings studies our approach's most suitable configuration and evaluates SuperPoint-E feature quality. The comparison with other baselines also shows that our 3D reconstructions are denser and cover more and longer video segments because our detector fires more densely and our features are more likely to survive (i.e. higher detection precision). In addition, our descriptor is more discriminative, making the guided matching step almost redundant. The presented approach brings significant improvements in the 3D reconstructions obtained, via SfM on endoscopy videos, compared to the original SuperPoint and the gold standard SfM COLMAP pipeline.

</details>


### [18] [Context Determines Optimal Architecture in Materials Segmentation](https://arxiv.org/abs/2602.04154)
*Mingjian Lu,Pawan K. Tripathi,Mark Shteyn,Debargha Ganguly,Roger H. French,Vipin Chaudhary,Yinghui Wu*

Main category: cs.CV

TL;DR: 该研究提出了一种跨模态材料图像分割评估框架，评估了六种编码-解码组合在七个数据集上的表现，揭示了不同上下文下最佳架构的系统性差异，为材料表征提供了架构指导、可靠性信号和解释性工具。


<details>
  <summary>Details</summary>
Motivation: 当前的分割架构主要在单一成像模态下进行基准测试，忽视了跨模态部署的相关性能变化。这项工作旨在通过跨模态评估框架解决这一问题，帮助研究人员根据特定的成像设置选择合适的架构，并评估模型在新样本上的可信度。

Method: 该研究团队在扫描电子显微镜(SEM)、原子力显微镜(AFM)、X射线计算机断层成像(XCT)和光学显微镜等四种成像模态上，对六种编码-解码组合进行了评估。

Result: 研究表明，不同的成像上下文需要不同的最佳模型。特别是UNet在高对比度2D成像中表现出色，而DeepLabv3+在最困难的情况下更占优势。此外，评估框架提供了离群检测和反事实解释，揭示了预测背后的微观结构特征。

Conclusion: 该框架为材料表征提供了可靠的架构指导、性能信号和可解释性工具，填补了当前研究中缺乏这些工具的空白。

Abstract: Segmentation architectures are typically benchmarked on single imaging modalities, obscuring deployment-relevant performance variations: an architecture optimal for one modality may underperform on another. We present a cross-modal evaluation framework for materials image segmentation spanning SEM, AFM, XCT, and optical microscopy. Our evaluation of six encoder-decoder combinations across seven datasets reveals that optimal architectures vary systematically by context: UNet excels for high-contrast 2D imaging while DeepLabv3+ is preferred for the hardest cases. The framework also provides deployment feedback via out-of-distribution detection and counterfactual explanations that reveal which microstructural features drive predictions. Together, the architecture guidance, reliability signals, and interpretability tools address a practical gap in materials characterization, where researchers lack tools to select architectures for their specific imaging setup or assess when models can be trusted on new samples.

</details>


### [19] [Improving 2D Diffusion Models for 3D Medical Imaging with Inter-Slice Consistent Stochasticity](https://arxiv.org/abs/2602.04162)
*Chenhe Du,Qing Wu,Xuanyu Tian,Jingyi Yu,Hongjiang Wei,Yuyao Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种简单而有效的间切片一致随机性(ISCS)策略，该策略鼓励在扩散采样过程中切片间的连贯性，无需添加新的损失项或优化步骤，从而改善基于2D扩散模型的医学3D重建任务。


<details>
  <summary>Details</summary>
Motivation: 当前在医学影像重建中利用扩散模型遇到的主要挑战包括数据收集难度和模型训练的高计算成本，以及通过2D数据重建3D医学影像中存在的间片间不连续性问题。

Method: 研究提出了一种间切片一致随机性(ISCS)策略，控制扩散采样过程中的随机噪声成分的一致性，确保采样轨迹对齐。

Result: 实验结果表明，引入ISCS策略可以显著提升基于2D扩散模型的医学3D重建任务的性能。

Conclusion: 研究展示了通过控制3D扩散采样过程中的变量一致性，是提高基于2D扩散模型的医学3D重建质量的有效途径。

Abstract: 3D medical imaging is in high demand and essential for clinical diagnosis and scientific research. Currently, diffusion models (DMs) have become an effective tool for medical imaging reconstruction thanks to their ability to learn rich, high-quality data priors. However, learning the 3D data distribution with DMs in medical imaging is challenging, not only due to the difficulties in data collection but also because of the significant computational burden during model training. A common compromise is to train the DMs on 2D data priors and reconstruct stacked 2D slices to address 3D medical inverse problems. However, the intrinsic randomness of diffusion sampling causes severe inter-slice discontinuities of reconstructed 3D volumes. Existing methods often enforce continuity regularizations along the z-axis, which introduces sensitive hyper-parameters and may lead to over-smoothing results. In this work, we revisit the origin of stochasticity in diffusion sampling and introduce Inter-Slice Consistent Stochasticity (ISCS), a simple yet effective strategy that encourages interslice consistency during diffusion sampling. Our key idea is to control the consistency of stochastic noise components during diffusion sampling, thereby aligning their sampling trajectories without adding any new loss terms or optimization steps. Importantly, the proposed ISCS is plug-and-play and can be dropped into any 2D trained diffusion based 3D reconstruction pipeline without additional computational cost. Experiments on several medical imaging problems show that our method can effectively improve the performance of medical 3D imaging problems based on 2D diffusion models. Our findings suggest that controlling inter-slice stochasticity is a principled and practically attractive route toward high-fidelity 3D medical imaging with 2D diffusion priors. The code is available at: https://github.com/duchenhe/ISCS

</details>


### [20] [Point2Insert: Video Object Insertion via Sparse Point Guidance](https://arxiv.org/abs/2602.04167)
*Yu Zhou,Xiaoyan Yang,Bojia Zi,Lihan Zhang,Ruijie Sun,Weishi Zheng,Haibin Huang,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: Point2Insert 提出了一种基于稀疏点的框架，用于视频中对象的灵活插入，这种方法通过少量稀疏点代替密集遮罩，消除了繁琐的遮罩绘制，支持正负点指示适合和不适合插入的区域，通过两阶段训练提高插入成功率，并在实验中表现优于多个基线。


<details>
  <summary>Details</summary>
Motivation: 鉴于准确且低努力的对象放置日益受欢迎，现有方法（如基于掩码插入和基于指令插入）面临两个主要挑战：掩码基插入需要耗时的掩码标注，而基于指令的插入方法难以精确放置对象。Point2Insert 旨在通过仅需少量稀疏点来解决这些问题，无需繁琐的掩码绘制。

Method: Point2Insert 方法分为两个阶段：首先是训练一个插入模型，该模型可以在给定区域生成对象，条件是有稀疏点提示或二进制掩码。其次是在一个综合视频插入和对象移除的模型上进一步训练，使模型适用于视频插入。

Result: 实验结果表明，Point2Insert 一致地优于多个强大的基线模型，并且即便与参数量多出10倍的模型竞争，仍然表现更优。

Conclusion: Point2Insert 提供了一种有效且用户友好的方法来实现视频中的对象插入，不仅减少了时间成本，还提高了插入的成功率。

Abstract: This paper introduces Point2Insert, a sparse-point-based framework for flexible and user-friendly object insertion in videos, motivated by the growing popularity of accurate, low-effort object placement. Existing approaches face two major challenges: mask-based insertion methods require labor-intensive mask annotations, while instruction-based methods struggle to place objects at precise locations. Point2Insert addresses these issues by requiring only a small number of sparse points instead of dense masks, eliminating the need for tedious mask drawing. Specifically, it supports both positive and negative points to indicate regions that are suitable or unsuitable for insertion, enabling fine-grained spatial control over object locations. The training of Point2Insert consists of two stages. In Stage 1, we train an insertion model that generates objects in given regions conditioned on either sparse-point prompts or a binary mask. In Stage 2, we further train the model on paired videos synthesized by an object removal model, adapting it to video insertion. Moreover, motivated by the higher insertion success rate of mask-guided editing, we leverage a mask-guided insertion model as a teacher to distill reliable insertion behavior into the point-guided model. Extensive experiments demonstrate that Point2Insert consistently outperforms strong baselines and even surpasses models with $\times$10 more parameters.

</details>


### [21] [HoloEv-Net: Efficient Event-based Action Recognition via Holographic Spatial Embedding and Global Spectral Gating](https://arxiv.org/abs/2602.04182)
*Weidong Hao*

Main category: cs.CV

TL;DR: 提出了一种高效的事件摄像头行动识别框架HoloEv-Net，通过引入紧凑的全息时空表示（CHSR）来同时解决表示和结构冗余问题，设计全局光谱门控（GSG）模块利用频域中的全局光谱线索，并在多个数据集上达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的事件摄像头行动识别方法面临着计算冗余、结构性冗余以及未能充分利用频谱信息等挑战，旨在开发一种更加高效和精度高的方法来应对这些挑战。

Method: 该方法首先引入了紧凑的全息时空表示（CHSR）来同时处理表示和结构冗余，同时设计了一个全局光谱门控（GSG）模块利用频域中的全局光谱线索进行全局令牌混合。

Result: 实验结果表明，HoloEv-Net框架在THU-EACT-50-CHL、HARDVS和DailyDVS-200等数据集上达到了最先进的性能，而且提出的轻量级版本在保留较高准确率的同时提供极高的效率。

Conclusion: 该研究证明了HoloEv-Net框架在提高事件行动识别任务上的有效性和高效性，并展示了其在边缘部署中的潜力。

Abstract: Event-based Action Recognition (EAR) has attracted significant attention due to the high temporal resolution and high dynamic range of event cameras. However, existing methods typically suffer from (i) the computational redundancy of dense voxel representations, (ii) structural redundancy inherent in multi-branch architectures, and (iii) the under-utilization of spectral information in capturing global motion patterns. To address these challenges, we propose an efficient EAR framework named HoloEv-Net. First, to simultaneously tackle representation and structural redundancies, we introduce a Compact Holographic Spatiotemporal Representation (CHSR). Departing from computationally expensive voxel grids, CHSR implicitly embeds horizontal spatial cues into the Time-Height (T-H) view, effectively preserving 3D spatiotemporal contexts within a 2D representation. Second, to exploit the neglected spectral cues, we design a Global Spectral Gating (GSG) module. By leveraging the Fast Fourier Transform (FFT) for global token mixing in the frequency domain, GSG enhances the representation capability with negligible parameter overhead. Extensive experiments demonstrate the scalability and effectiveness of our framework. Specifically, HoloEv-Net-Base achieves state-of-the-art performance on THU-EACT-50-CHL, HARDVS and DailyDVS-200, outperforming existing methods by 10.29%, 1.71% and 6.25%, respectively. Furthermore, our lightweight variant, HoloEv-Net-Small, delivers highly competitive accuracy while offering extreme efficiency, reducing parameters by 5.4 times, FLOPs by 300times, and latency by 2.4times compared to heavy baselines, demonstrating its potential for edge deployment.

</details>


### [22] [Natural Language Instructions for Scene-Responsive Human-in-the-Loop Motion Planning in Autonomous Driving using Vision-Language-Action Models](https://arxiv.org/abs/2602.04184)
*Angel Martinez-Sanchez,Parthib Roy,Ross Greer*

Main category: cs.CV

TL;DR: 该研究使用doScenes数据集，集成了自由形式的指令，评估了一种基于视觉语言模型的驾驶框架（OpenEMMA）在指令引导下的表现，并通过减少平均视角误差（ADE）验证了指令的显著改进效果。


<details>
  <summary>Details</summary>
Motivation: 由于大多数现有指令跟随规划器依赖于模拟或固定命令词汇表，缺乏在现实生活中的通用性，研究者旨在通过doScenes数据集来克服这些限制，实现基于真实场景指令引导下的驾驶。

Method: 研究者采用了开源的基于MLLM（机器学习语言模型）的端到端驾驶框架OpenEMMA，将其适应于指令引导下的驾驶规划。通过将doScenes中的指令整合到OpenEMMA的视觉语言界面中，实现在轨迹生成前进行语言条件控制。

Result: 在849标注场景上进行评估，结果显示指令引导显著提高了轨迹规划的鲁棒性，平均视角误差减少了98.7%，未经严格控制的极端错误减少后，适度的指令也能使视角误差减少5.1%。

Conclusion: 研究提出了一种重现的指令导向基准方法，并通过分析讨论了适用于OpenEMMA框架的“良好”指令标准，同时开放了评估指令和脚本，以建立基于指令感知规划的可重复基准。

Abstract: Instruction-grounded driving, where passenger language guides trajectory planning, requires vehicles to understand intent before motion. However, most prior instruction-following planners rely on simulation or fixed command vocabularies, limiting real-world generalization. doScenes, the first real-world dataset linking free-form instructions (with referentiality) to nuScenes ground-truth motion, enables instruction-conditioned planning. In this work, we adapt OpenEMMA, an open-source MLLM-based end-to-end driving framework that ingests front-camera views and ego-state and outputs 10-step speed-curvature trajectories, to this setting, presenting a reproducible instruction-conditioned baseline on doScenes and investigate the effects of human instruction prompts on predicted driving behavior. We integrate doScenes directives as passenger-style prompts within OpenEMMA's vision-language interface, enabling linguistic conditioning before trajectory generation. Evaluated on 849 annotated scenes using ADE, we observe that instruction conditioning substantially improves robustness by preventing extreme baseline failures, yielding a 98.7% reduction in mean ADE. When such outliers are removed, instructions still influence trajectory alignment, with well-phrased prompts improving ADE by up to 5.1%. We use this analysis to discuss what makes a "good" instruction for the OpenEMMA framework. We release the evaluation prompts and scripts to establish a reproducible baseline for instruction-aware planning. GitHub: https://github.com/Mi3-Lab/doScenes-VLM-Planning

</details>


### [23] [DiMo: Discrete Diffusion Modeling for Motion Generation and Understanding](https://arxiv.org/abs/2602.04188)
*Ning Zhang,Zhengyu Li,Kwong Weng Loh,Mingxi Xu,Qi Wang,Zhengyu Wen,Xiaoyu He,Wei Zhao,Kehong Gong,Mingyuan Zhang*

Main category: cs.CV

TL;DR: DiMo提供了一种新颖的离散扩散风格框架，通过迭代掩码标记精炼实现了双向文本-运动理解与生成，改善了运动标记的准确性，并增强了对齐和可控性。


<details>
  <summary>Details</summary>
Motivation: 现有的掩蔽建模运动生成方法主要关注文本到运动，DiMo旨在扩展掩蔽模型以实现双向文本-运动理解与生成，提供了一种新的运动生成方法。

Method: DiMo提出了迭代掩码标记精炼方法，结合了文本到运动、运动到文本和无文本运动到运动的方法，通过残差矢量量化和组相对策略优化改进了运动标记的准确性和对齐与可控性。

Result: 在HumanML3D和KIT-ML数据集上，DiMo展示了高质量的运动和竞争性的双向理解能力，此外，模型还在文本无引导的运动补全、文本指导的运动预测和运动caption修正方面表现出色。

Conclusion: DiMo提供了一种统一的框架，使文本与运动之间的理解更加完善，为运动生成领域提供了新的研究视角。

Abstract: Prior masked modeling motion generation methods predominantly study text-to-motion. We present DiMo, a discrete diffusion-style framework, which extends masked modeling to bidirectional text--motion understanding and generation. Unlike GPT-style autoregressive approaches that tokenize motion and decode sequentially, DiMo performs iterative masked token refinement, unifying Text-to-Motion (T2M), Motion-to-Text (M2T), and text-free Motion-to-Motion (M2M) within a single model. This decoding paradigm naturally enables a quality-latency trade-off at inference via the number of refinement steps.We further improve motion token fidelity with residual vector quantization (RVQ) and enhance alignment and controllability with Group Relative Policy Optimization (GRPO). Experiments on HumanML3D and KIT-ML show strong motion quality and competitive bidirectional understanding under a unified framework. In addition, we demonstrate model ability in text-free motion completion, text-guided motion prediction and motion caption correction without architectural change.Additional qualitative results are available on our project page: https://animotionlab.github.io/DiMo/.

</details>


### [24] [Continuous Degradation Modeling via Latent Flow Matching for Real-World Super-Resolution](https://arxiv.org/abs/2602.04193)
*Hyeonjae Kim,Dongjin Kim,Eugene Jin,Tae Hyun Kim*

Main category: cs.CV

TL;DR: 该研究提出了一种新颖的框架，可以仅通过一个高分辨率图像生成具有真实降质效果的低分辨率图像，从而生成大规模、真实世界的超分辨率训练数据集。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的超分辨率方法在处理具有复杂非线性降质的现实图像时表现不佳，因此需提出一种新方法来解决这一问题。

Method: 该方法利用潜降质空间和流匹配技术，从单一高分辨率图像中生成具有未见降质级别的高质量低分辨率图像。

Result: 实验结果表明，所生成的低分辨率图像能准确模拟现实世界中的降质效果，使用这些数据集训练的传统和任意尺度的超分辨率模型在高分辨率输出上表现更好。

Conclusion: 此研究成功提出了一种生成真实世界低分辨率图像的新框架，为超分辨率领域提供了新视角。

Abstract: While deep learning-based super-resolution (SR) methods have shown impressive outcomes with synthetic degradation scenarios such as bicubic downsampling, they frequently struggle to perform well on real-world images that feature complex, nonlinear degradations like noise, blur, and compression artifacts. Recent efforts to address this issue have involved the painstaking compilation of real low-resolution (LR) and high-resolution (HR) image pairs, usually limited to several specific downscaling factors. To address these challenges, our work introduces a novel framework capable of synthesizing authentic LR images from a single HR image by leveraging the latent degradation space with flow matching. Our approach generates LR images with realistic artifacts at unseen degradation levels, which facilitates the creation of large-scale, real-world SR training datasets. Comprehensive quantitative and qualitative assessments verify that our synthetic LR images accurately replicate real-world degradations. Furthermore, both traditional and arbitrary-scale SR models trained using our datasets consistently yield much better HR outcomes.

</details>


### [25] [VTok: A Unified Video Tokenizer with Decoupled Spatial-Temporal Latents](https://arxiv.org/abs/2602.04202)
*Feng Wang,Yichun Shi,Ceyuan Yang,Qiushan Guo,Jingxiang Sun,Alan Yuille,Peng Wang*

Main category: cs.CV

TL;DR: VTok 是一种新的视频令牌化框架，它通过分离空间和时间表示来减少视频表示的复杂性，并在视频理解和文本到视频生成任务中取得了更高的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视频语言系统使用简单的帧采样策略进行视频令牌化，这导致了复杂度较高。VTok 通过保持关键帧的空间特征并将后续帧编码为单一的残差令牌，有效地减少了这种复杂性。

Method: VTok 算法首先保留关键帧的空间特征，然后将后续帧编码为单一的残差令牌，从而实现紧凑且具有表现力的视频令牌化。

Result: VTok 在各种视频理解和文本到视频生成基准测试中表现出优越性，相较于使用简单令牌化的基线模型，它提供了更好的性能，同时视频序列令牌更短。

Conclusion: VTok 有望成为将来的视频理解和生成研究中标准化的视频令牌化范式，由于其更一致的时间编码，VTok 在文本到视频生成任务中产生了更连贯的运动和更强的语言指导。

Abstract: This work presents VTok, a unified video tokenization framework that can be used for both generation and understanding tasks. Unlike the leading vision-language systems that tokenize videos through a naive frame-sampling strategy, we propose to decouple the spatial and temporal representations of videos by retaining the spatial features of a single key frame while encoding each subsequent frame into a single residual token, achieving compact yet expressive video tokenization. Our experiments suggest that VTok effectively reduces the complexity of video representation from the product of frame count and per-frame token count to their sum, while the residual tokens sufficiently capture viewpoint and motion changes relative to the key frame. Extensive evaluations demonstrate the efficacy and efficiency of VTok: it achieves notably higher performance on a range of video understanding and text-to-video generation benchmarks compared with baselines using naive tokenization, all with shorter token sequences per video (e.g., 3.4% higher accuracy on our TV-Align benchmark and 1.9% higher VBench score). Remarkably, VTok produces more coherent motion and stronger guidance following in text-to-video generation, owing to its more consistent temporal encoding. We hope VTok can serve as a standardized video tokenization paradigm for future research in video understanding and generation.

</details>


### [26] [AGMA: Adaptive Gaussian Mixture Anchors for Prior-Guided Multimodal Human Trajectory Forecasting](https://arxiv.org/abs/2602.04204)
*Chao Li,Rui Zhang,Siyuan Huang,Xian Zhong,Hongbo Jiang*

Main category: cs.CV

TL;DR: AGMA通过构建适应场景的高表达力先验，解决了传统方法中存在的先验偏差问题，显著提升了行人轨迹预测的准确性和多样性。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能准确捕捉行人的多模态行为，导致预测准确性受限并影响多样性。

Method: AGMA在训练阶段提取多样化的行为模式，并在推理阶段构建全局适应场景的先验。

Result: 在多个数据集上，AGMA的预测性能达到了最新的技术水平。

Conclusion: 高质量的先验是轨迹预测的关键，AGMA通过有效构建先验显著提升了性能。

Abstract: Human trajectory forecasting requires capturing the multimodal nature of pedestrian behavior. However, existing approaches suffer from prior misalignment. Their learned or fixed priors often fail to capture the full distribution of plausible futures, limiting both prediction accuracy and diversity. We theoretically establish that prediction error is lower-bounded by prior quality, making prior modeling a key performance bottleneck. Guided by this insight, we propose AGMA (Adaptive Gaussian Mixture Anchors), which constructs expressive priors through two stages: extracting diverse behavioral patterns from training data and distilling them into a scene-adaptive global prior for inference. Extensive experiments on ETH-UCY, Stanford Drone, and JRDB datasets demonstrate that AGMA achieves state-of-the-art performance, confirming the critical role of high-quality priors in trajectory forecasting.

</details>


### [27] [An Intuitionistic Fuzzy Logic Driven UNet architecture: Application to Brain Image segmentation](https://arxiv.org/abs/2602.04227)
*Hanuman Verma,Kiho Im,Pranabesh Maji,Akshansh Gupta*

Main category: cs.CV

TL;DR: 该研究提出了一种名为IF-UNet的增强框架，结合了直觉模糊逻辑以处理MRI脑图像中的不确定性，实验表明其在处理不确定性方面的表现更佳。


<details>
  <summary>Details</summary>
Motivation: 传统的深度学习方法，如UNet，在医学图像分割中得到了广泛应用，然而在处理脑图像中的部分体素效应导致的不确定性和边缘不确定性时存在困难。为此，研究引入了结合直觉模糊逻辑的IF-UNet框架。

Method: IF-UNet框架将输入数据处理为隶属度、非隶属度和犹豫度，以更好地解决由于部分体积效应和边缘不确定性引起的组织模糊。

Result: IF-UNet在Internet Brain Segmentation Repository (IBSR) 数据集上进行了评估，并通过准确性、Dice系数和交并比进行了性能计算。实验结果表明，IF-UNet能够更好地处理脑图像中的不确定性，提高分割质量。

Conclusion: 研究证明IF-UNet框架在处理MRI脑图像分割中的不确定性方面具有优势，能够实现更高质量的分割。

Abstract: Accurate segmentation of MRI brain images is essential for image analysis, diagnosis of neuro-logical disorders and medical image computing. In the deep learning approach, the convolutional neural networks (CNNs), especially UNet, are widely applied in medical image segmentation. However, it is difficult to deal with uncertainty due to the partial volume effect in brain images. To overcome this limitation, we propose an enhanced framework, named UNet with intuitionistic fuzzy logic (IF-UNet), which incorporates intuitionistic fuzzy logic into UNet. The model processes input data in terms of membership, nonmembership, and hesitation degrees, allowing it to better address tissue ambiguity resulting from partial volume effects and boundary uncertainties. The proposed architecture is evaluated on the Internet Brain Segmentation Repository (IBSR) dataset, and its performance is computed using accuracy, Dice coefficient, and intersection over union (IoU). Experimental results confirm that IF-UNet improves segmentation quality with handling uncertainty in brain images.

</details>


### [28] [SPOT-Occ: Sparse Prototype-guided Transformer for Camera-based 3D Occupancy Prediction](https://arxiv.org/abs/2602.04240)
*Suzeyu Chen,Leheng Li,Ying-Cong Chen*

Main category: cs.CV

TL;DR: 提出了一种新型的基于Prototype的Sparse Transformer Decoder (SPOT-Occ)，该方法通过两阶段过程（引导特征选择和集中聚合）来解决3D点云稀疏表示的解码关注问题，提高了准确性和速度。


<details>
  <summary>Details</summary>
Motivation: 随着自主车辆的安全和实用部署需要高精度且实时的3D占用预测，如何高效地从稀疏非均匀分布的体素特征中聚合信息成为新的挑战。现有方法难以在保留精确度的同时实现高效解码。

Method: SPOT-Occ采用一种新型的基于Prototype的Sparse Transformer Decoder方法，通过查询适配性选择关键体素特征作为原型，结合引导注意力机制，并引入去噪机制来稳定和优化动态选择机制。

Result: 该方法显著提升了速度和准确度，相比已有方法在性能上有所突破。

Conclusion: 论文设计了一个有效的SPOT-Occ模型，验证了其在提高3D额外预测速度和精度方面的实力。

Abstract: Achieving highly accurate and real-time 3D occupancy prediction from cameras is a critical requirement for the safe and practical deployment of autonomous vehicles. While this shift to sparse 3D representations solves the encoding bottleneck, it creates a new challenge for the decoder: how to efficiently aggregate information from a sparse, non-uniformly distributed set of voxel features without resorting to computationally prohibitive dense attention.
  In this paper, we propose a novel Prototype-based Sparse Transformer Decoder that replaces this costly interaction with an efficient, two-stage process of guided feature selection and focused aggregation. Our core idea is to make the decoder's attention prototype-guided. We achieve this through a sparse prototype selection mechanism, where each query adaptively identifies a compact set of the most salient voxel features, termed prototypes, for focused feature aggregation.
  To ensure this dynamic selection is stable and effective, we introduce a complementary denoising paradigm. This approach leverages ground-truth masks to provide explicit guidance, guaranteeing a consistent query-prototype association across decoder layers. Our model, dubbed SPOT-Occ, outperforms previous methods with a significant margin in speed while also improving accuracy. Source code is released at https://github.com/chensuzeyu/SpotOcc.

</details>


### [29] [ACIL: Active Class Incremental Learning for Image Classification](https://arxiv.org/abs/2602.04252)
*Aditya R. Bhattacharya,Debanjan Goswami,Shayok Chakraborty*

Main category: cs.CV

TL;DR: 本文提出了一种名为ACIL的新颖主动学习框架，用于类增量学习场景，通过利用不确定性与多样性的标准来筛选出需要注释的范例样本，这种框架能够显著降低注释成本并防止灾难性遗忘。


<details>
  <summary>Details</summary>
Motivation: 现有的类增量学习方法主要侧重于避免灾难性遗忘，但这些方法假定每个时期的训练样本都需要注释，这不仅增加了注释成本，还浪费了注释工作。

Method: ACIL框架采用基于不确定性和多样性的标准来选择需要注释的样本，并将这些样本添加到随后时期的训练集中。

Result: 该框架已经在多个视觉数据集上的实证分析中表现出优于相关基线的效果。

Conclusion: 本文提出的ACIL能够显著减少注释成本并在类增量学习中起到避免灾难性遗忘的作用。

Abstract: Continual learning (or class incremental learning) is a realistic learning scenario for computer vision systems, where deep neural networks are trained on episodic data, and the data from previous episodes are generally inaccessible to the model. Existing research in this domain has primarily focused on avoiding catastrophic forgetting, which occurs due to the continuously changing class distributions in each episode and the inaccessibility of the data from previous episodes. However, these methods assume that all the training samples in every episode are annotated; this not only incurs a huge annotation cost, but also results in a wastage of annotation effort, since most of the samples in a given episode will not be accessible to the model in subsequent episodes. Active learning algorithms identify the salient and informative samples from large amounts of unlabeled data and are instrumental in reducing the human annotation effort in inducing a deep neural network. In this paper, we propose ACIL, a novel active learning framework for class incremental learning settings. We exploit a criterion based on uncertainty and diversity to identify the exemplar samples that need to be annotated in each episode, and will be appended to the data in the next episode. Such a framework can drastically reduce annotation cost and can also avoid catastrophic forgetting. Our extensive empirical analyses on several vision datasets corroborate the promise and potential of our framework against relevant baselines.

</details>


### [30] [Depth-Guided Metric-Aware Temporal Consistency for Monocular Video Human Mesh Recovery](https://arxiv.org/abs/2602.04257)
*Jiaxin Cen,Xudong Mao,Guanghui Yue,Wei Zhou,Ruomei Wang,Fan Zhou,Baoquan Zhao*

Main category: cs.CV

TL;DR: 本文提出了一种深度引导框架，该框架通过三个协同组件实现度量感知的时间一致性：深度引导多尺度融合模块、深度引导度量感知姿态和形状估计器以及Motion-Depth 对准 refinement模块。


<details>
  <summary>Details</summary>
Motivation: 由于单目视频对人体网格恢复中存在的度量一致性、时间稳定性问题，现有方法依赖RGB特征和时间平滑处理，但难以解决深度排序、尺度漂移和遮挡诱导的不稳定性。

Method: 该框架包括深度引导多尺度融合模块、深度引导度量感知姿态和形状估计器，以及Motion-Depth 对准 refinement模块。这些组件协同工作，通过深度信息指导的方法增强了模型的度量感知能力，并提高了时间一致性。

Result: 在三个具有挑战性的基准上，该方法取得了显著成果，展示了对密集遮挡的鲁棒性和空间精度的提高，同时保持了计算效率。

Conclusion: 本文提出的方法通过创新的组件设计，克服了单目视频人体网格恢复中的主要挑战，并在多个评估基准上取得了良好的效果。

Abstract: Monocular video human mesh recovery faces fundamental challenges in maintaining metric consistency and temporal stability due to inherent depth ambiguities and scale uncertainties. While existing methods rely primarily on RGB features and temporal smoothing, they struggle with depth ordering, scale drift, and occlusion-induced instabilities. We propose a comprehensive depth-guided framework that achieves metric-aware temporal consistency through three synergistic components: A Depth-Guided Multi-Scale Fusion module that adaptively integrates geometric priors with RGB features via confidence-aware gating; A Depth-guided Metric-Aware Pose and Shape (D-MAPS) estimator that leverages depth-calibrated bone statistics for scale-consistent initialization; A Motion-Depth Aligned Refinement (MoDAR) module that enforces temporal coherence through cross-modal attention between motion dynamics and geometric cues. Our method achieves superior results on three challenging benchmarks, demonstrating significant improvements in robustness against heavy occlusion and spatial accuracy while maintaining computational efficiency.

</details>


### [31] [Decoupled Hierarchical Distillation for Multimodal Emotion Recognition](https://arxiv.org/abs/2602.04260)
*Yong Li,Yuanzhi Wang,Yi Ding,Shiqing Zhang,Ke Lu,Cuntai Guan*

Main category: cs.CV

TL;DR: 该研究提出了一种新颖的框架DHMD，它通过自我回归机制解耦模态特征，并采用分层知识蒸馏策略，包括图形蒸馏单元和跨模态词汇匹配机制，以提高跨模态特征对齐，从而实现更有效的多模态情绪识别。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理多模态异质性和不同模态贡献的差异上存在挑战，该研究旨在提出一种方法以克服这些挑战。

Method: DHMD框架采用自我回归机制解耦模态特征，分为粗粒度和细粒度的知识蒸馏策略。其中粗粒度使用图形蒸馏单元帮助不同模态间的自适应蒸馏，而细粒度则通过跨模态词汇匹配机制对齐模态间的语义颗粒度，以生成更具区分力的多模态情绪识别表示。

Result: 在CMU-MOSI和CMU-MOSEI数据集上，研究验证了DHMD框架的有效性，分别在ACC_7、ACC_2和F1方面相比于现有最先进的方法提高了1.3%、1.3%和1.9%/1.8%的相对性能。

Conclusion:  DHMD框架对于多模态情绪识别具有明显优势，能够在不同情境下提供更有效的跨模态特征对齐和情感识别。

Abstract: Human multimodal emotion recognition (MER) seeks to infer human emotions by integrating information from language, visual, and acoustic modalities. Although existing MER approaches have achieved promising results, they still struggle with inherent multimodal heterogeneities and varying contributions from different modalities. To address these challenges, we propose a novel framework, Decoupled Hierarchical Multimodal Distillation (DHMD). DHMD decouples each modality's features into modality-irrelevant (homogeneous) and modality-exclusive (heterogeneous) components using a self-regression mechanism. The framework employs a two-stage knowledge distillation (KD) strategy: (1) coarse-grained KD via a Graph Distillation Unit (GD-Unit) in each decoupled feature space, where a dynamic graph facilitates adaptive distillation among modalities, and (2) fine-grained KD through a cross-modal dictionary matching mechanism, which aligns semantic granularities across modalities to produce more discriminative MER representations. This hierarchical distillation approach enables flexible knowledge transfer and effectively improves cross-modal feature alignment. Experimental results demonstrate that DHMD consistently outperforms state-of-the-art MER methods, achieving 1.3\%/2.4\% (ACC$_7$), 1.3\%/1.9\% (ACC$_2$) and 1.9\%/1.8\% (F1) relative improvement on CMU-MOSI/CMU-MOSEI dataset, respectively. Meanwhile, visualization results reveal that both the graph edges and dictionary activations in DHMD exhibit meaningful distribution patterns across modality-irrelevant/-exclusive feature spaces.

</details>


### [32] [KVSmooth: Mitigating Hallucination in Multi-modal Large Language Models through Key-Value Smoothing](https://arxiv.org/abs/2602.04268)
*Siyu Jiang,Feiyang Chen,Xiaojin Zhang,Kun He*

Main category: cs.CV

TL;DR: KVSmooth 是一种无需训练且易于集成的方法，通过注意力-熵引导的自适应平滑处理隐藏状态中的键值对，有效减少生成中的视觉不一致性现象，同时提高整体性能。


<details>
  <summary>Details</summary>
Motivation: 虽然多模态大型语言模型（MLLMs）在多种任务上取得了显著进展，但它们在生成过程中存在的视觉不一致性（即 hallucination）问题仍然是商用部署的重大障碍。现有的方法通常会导致随着生成序列的增长，生成内容与视觉输入产生偏差。

Method: KVSmooth 通过应用指数移动平均（EMA）到 KV 缓存中的键和值，并且根据每个 token 注意力分布的熵动态量化“汇度”，从而进行自适应的平滑强度调整，确保生成内容更加贴近视觉输入。

Result: 实验结果显示，KVSmooth 方法显著减少了视觉不一致性问题（CHAIR_S 降低了 55.0%），同时提高了整体性能（F1 分数提高了 1.7%），在提高精确率和召回率方面表现出色。

Conclusion: KVSmooth 提供了一种高效且通用的方法来解决 MLLMs 中的 hallucination 问题，无需额外训练或模型修改，验证了该方法的有效性和广泛适用性。

Abstract: Despite the significant progress of Multimodal Large Language Models (MLLMs) across diverse tasks, hallucination -- corresponding to the generation of visually inconsistent objects, attributes, or relations -- remains a major obstacle to their reliable deployment. Unlike pure language models, MLLMs must ground their generation process in visual inputs. However, existing models often suffer from semantic drift during decoding, causing outputs to diverge from visual facts as the sequence length increases.
  To address this issue, we propose KVSmooth, a training-free and plug-and-play method that mitigates hallucination by performing attention-entropy-guided adaptive smoothing on hidden states. Specifically, KVSmooth applies an exponential moving average (EMA) to both keys and values in the KV-Cache, while dynamically quantifying the sink degree of each token through the entropy of its attention distribution to adaptively adjust the smoothing strength.
  Unlike computationally expensive retraining or contrastive decoding methods, KVSmooth operates efficiently during inference without additional training or model modification. Extensive experiments demonstrate that KVSmooth significantly reduces hallucination ($\mathit{CHAIR}_{S}$ from $41.8 \rightarrow 18.2$) while improving overall performance ($F_1$ score from $77.5 \rightarrow 79.2$), achieving higher precision and recall simultaneously. In contrast, prior methods often improve one at the expense of the other, validating the effectiveness and generality of our approach.

</details>


### [33] [SkeletonGaussian: Editable 4D Generation through Gaussian Skeletonization](https://arxiv.org/abs/2602.04271)
*Lifan Wu,Ruijie Zhu,Yubo Ai,Tianzhu Zhang*

Main category: cs.CV

TL;DR: 该研究提出了一种名为SkeletonGaussian的新框架，可以从单目视频生成可编辑的动态3D高斯对象，通过引入层次结构的肢体表示将运动分解为明确由骨架驱动的稀疏刚性运动和细粒度的非刚性运动，显著提高了生成质量和直观的运动编辑能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理动态3D对象生成时，往往将运动表示为隐式的变形场，这限制了直接控制和可编辑性。因此，本文旨在解决这个问题，提出了一种新的框架来生成源自单目视频的可编辑动态3D高斯对象。

Method: 该方法引入了一个层次化的肢体表示来分解运动，将运动明确地拆分为由骨架驱动的稀疏刚性运动和细节化的非刚性运动。具体地，该方法提取了一个鲁棒的骨架并通过线性混合蒙皮驱动刚性运动，随后使用基于六面体的细化进行非刚性变形，从而增强了可解释性和可编辑性。

Result: 通过对比实验，证明了SkeletonGaussian在生成质量和直观的运动编辑方面超过了现有方法，为可编辑的4D生成提供了一个新的范式。

Conclusion: 本文提出了一种名为SkeletonGaussian的新框架，解决了现有方法在运动表示上的不足，通过引入层次结构的肢体表示显著提高了生成质量和运动编辑的直观性，为可编辑的4D生成领域树立了一个新的标杆。

Abstract: 4D generation has made remarkable progress in synthesizing dynamic 3D objects from input text, images, or videos. However, existing methods often represent motion as an implicit deformation field, which limits direct control and editability. To address this issue, we propose SkeletonGaussian, a novel framework for generating editable dynamic 3D Gaussians from monocular video input. Our approach introduces a hierarchical articulated representation that decomposes motion into sparse rigid motion explicitly driven by a skeleton and fine-grained non-rigid motion. Concretely, we extract a robust skeleton and drive rigid motion via linear blend skinning, followed by a hexplane-based refinement for non-rigid deformations, enhancing interpretability and editability. Experimental results demonstrate that SkeletonGaussian surpasses existing methods in generation quality while enabling intuitive motion editing, establishing a new paradigm for editable 4D generation. Project page: https://wusar.github.io/projects/skeletongaussian/

</details>


### [34] [Beyond Static Cropping: Layer-Adaptive Visual Localization and Decoding Enhancement](https://arxiv.org/abs/2602.04304)
*Zipeng Zhu,Zhanghao Hu,Qinglin Zhu,Yuxi Hong,Yijun Liu,Jingyong Su,Yulan He,Lin Gui*

Main category: cs.CV

TL;DR: 文章提出了一种动态视角下的视觉锚定方法，通过层灵敏度分析发现复杂视觉搜索和推理任务需要在深层激活视觉信息。基于此，引入了VAQ作为评估标准，并提出了一种无需训练的推理程序LASER，该程序适用于不同类型的任务以提高VQA精度。


<details>
  <summary>Details</summary>
Motivation: LVLMs的视觉令牌预算限制使得图像只能缩小到固定分辨率，但这种方法会导致细节丢失和语言先验导致的幻觉。因此，文章提出了一种动态视角的视觉锚定方法，以改善VQA任务的准确性。

Method: 通过层灵敏度分析，文章确定了视觉锚定是一个动态过程，并基于此提出了VAQ，该指标衡量了注意力对输入查询的敏感度。进一步地，提出了一种无训练的推理程序LASER，能够适应不同任务的需求。

Result: 实验表明，LASER在不同复杂度的VQA基准测试上有效地提高了可视化定位和问答的准确性。

Conclusion: 该方法通过动态视角和注意力指导的选择，提高了视觉语言模型在视觉任务上的表现，特别是在复杂推理任务上。

Abstract: Large Vision-Language Models (LVLMs) have advanced rapidly by aligning visual patches with the text embedding space, but a fixed visual-token budget forces images to be resized to a uniform pretraining resolution, often erasing fine-grained details and causing hallucinations via over-reliance on language priors. Recent attention-guided enhancement (e.g., cropping or region-focused attention allocation) alleviates this, yet it commonly hinges on a static "magic layer" empirically chosen on simple recognition benchmarks and thus may not transfer to complex reasoning tasks. In contrast to this static assumption, we propose a dynamic perspective on visual grounding. Through a layer-wise sensitivity analysis, we demonstrate that visual grounding is a dynamic process: while simple object recognition tasks rely on middle layers, complex visual search and reasoning tasks require visual information to be reactivated at deeper layers. Based on this observation, we introduce Visual Activation by Query (VAQ), a metric that identifies the layer whose attention map is most relevant to query-specific visual grounding by measuring attention sensitivity to the input query. Building on VAQ, we further propose LASER (Layer-adaptive Attention-guided Selective visual and decoding Enhancement for Reasoning), a training-free inference procedure that adaptively selects task-appropriate layers for visual localization and question answering. Experiments across diverse VQA benchmarks show that LASER significantly improves VQA accuracy across tasks with varying levels of complexity.

</details>


### [35] [JOintGS: Joint Optimization of Cameras, Bodies and 3D Gaussians for In-the-Wild Monocular Reconstruction](https://arxiv.org/abs/2602.04317)
*Zihan Lou,Jinlong Fan,Sihan Ma,Yuxiang Yang,Jing Zhang*

Main category: cs.CV

TL;DR: JOintGS 提出了一种新的方法，能够从单目RGB视频中重建高质量的可动画三维人体模型。该方法通过结合优化相机外参、人体姿态和3D高斯表示，利用前景和背景的分离增强，实现了鲁棒的实时渲染性能。


<details>
  <summary>Details</summary>
Motivation: 现有的重建方法在场景中缺乏鲁棒性，尤其是处理未受控的野外场景。JOintGS 方法旨在提高这个挑战中的重建质量。

Method: JOintGS 使用一种新颖的联合优化机制，通过多视角一致性、准确的时间对应和优化的人体姿态来增强背景与前景的分离。引入了时空动态模块来捕捉细粒度的姿态依赖变形，并使用残差色度场来建模光照变化。

Result: 在 NeuMan 和 EMDB 数据集上的实验证明，JOintGS 在重建质量上显著优于现有方法，PSNR 提高了 2.1 dB，在保持实时渲染能力的同时，对噪声初始化也表现出显著增强的鲁棒性。

Conclusion: JOintGS 提供了一个多方面的解决方案，通过优化功能的协同作用提高了 3D 人体模型的重建性能。其开源代码可以供其他研究人员和开发者使用和拓展。

Abstract: Reconstructing high-fidelity animatable 3D human avatars from monocular RGB videos remains challenging, particularly in unconstrained in-the-wild scenarios where camera parameters and human poses from off-the-shelf methods (e.g., COLMAP, HMR2.0) are often inaccurate. Splatting (3DGS) advances demonstrate impressive rendering quality and real-time performance, they critically depend on precise camera calibration and pose annotations, limiting their applicability in real-world settings. We present JOintGS, a unified framework that jointly optimizes camera extrinsics, human poses, and 3D Gaussian representations from coarse initialization through a synergistic refinement mechanism. Our key insight is that explicit foreground-background disentanglement enables mutual reinforcement: static background Gaussians anchor camera estimation via multi-view consistency; refined cameras improve human body alignment through accurate temporal correspondence; optimized human poses enhance scene reconstruction by removing dynamic artifacts from static constraints. We further introduce a temporal dynamics module to capture fine-grained pose-dependent deformations and a residual color field to model illumination variations. Extensive experiments on NeuMan and EMDB datasets demonstrate that JOintGS achieves superior reconstruction quality, with 2.1~dB PSNR improvement over state-of-the-art methods on NeuMan dataset, while maintaining real-time rendering. Notably, our method shows significantly enhanced robustness to noisy initialization compared to the baseline.Our source code is available at https://github.com/MiliLab/JOintGS.

</details>


### [36] [Multiview Self-Representation Learning across Heterogeneous Views](https://arxiv.org/abs/2602.04328)
*Jie Chen,Zhu Wang,Chuanbin Liu,Xi Peng*

Main category: cs.CV

TL;DR: 提出了一种多视图自表示学习（MSRL）方法，通过利用不同预训练模型生成的特征在不同时态间的自表示性质，学习不变的特征表示。


<details>
  <summary>Details</summary>
Motivation: 本文为了解决大规模未标注视觉数据上，利用多种预训练模型进行无监督迁移学习时，学习不变特征表示这一难题。

Method: 该方法通过在每个预训练骨干模型上堆叠单一的线性模型，并引入一种基于自表示学习的信息传递机制和多视图自表示学习的分配概率分布一致性策略来增强不同线性模型下的表示不变性。

Result: 实验结果表明，提出的MSRL方法在多个基准视觉数据集中表现优于多种现有先进方法。

Conclusion: 该研究提出的方法为无监督的多视角特征表示提供了新的思路，有助于在不同预训练模型下学习跨线性模型不变的特征表示。

Abstract: Features of the same sample generated by different pretrained models often exhibit inherently distinct feature distributions because of discrepancies in the model pretraining objectives or architectures. Learning invariant representations from large-scale unlabeled visual data with various pretrained models in a fully unsupervised transfer manner remains a significant challenge. In this paper, we propose a multiview self-representation learning (MSRL) method in which invariant representations are learned by exploiting the self-representation property of features across heterogeneous views. The features are derived from large-scale unlabeled visual data through transfer learning with various pretrained models and are referred to as heterogeneous multiview data. An individual linear model is stacked on top of its corresponding frozen pretrained backbone. We introduce an information-passing mechanism that relies on self-representation learning to support feature aggregation over the outputs of the linear model. Moreover, an assignment probability distribution consistency scheme is presented to guide multiview self-representation learning by exploiting complementary information across different views. Consequently, representation invariance across different linear models is enforced through this scheme. In addition, we provide a theoretical analysis of the information-passing mechanism, the assignment probability distribution consistency and the incremental views. Extensive experiments with multiple benchmark visual datasets demonstrate that the proposed MSRL method consistently outperforms several state-of-the-art approaches.

</details>


### [37] [Fine-tuning Pre-trained Vision-Language Models in a Human-Annotation-Free Manner](https://arxiv.org/abs/2602.04337)
*Qian-Wei Wang,Guanghao Meng,Ren Cai,Yaguang Song,Shu-Tao Xia*

Main category: cs.CV

TL;DR: CoFT 提出了一种无需人工标注即可适应下游任务的框架，通过双重提示学习策略和双重模型跨模态合作机制，增强了模型的鲁棒性。借助两个训练阶段，CoFT 从高置信度样本的参数高效微调过渡到由协作过滤伪标签引导的全微调。


<details>
  <summary>Details</summary>
Motivation: 当前的无监督自训练方法依赖伪标签，但存在不可靠的信心过滤、确认偏差和低置信度样本利用不足的问题。CoFT 旨在通过双重提示学习策略和双重模型跨模态合作机制提高模型适应能力。

Method: CoFT 引入了双重提示学习策略，包括正向和负向文本提示，用于显式建模样本依赖的伪标签清洁度。这改变了对人工设定阈值或噪声假设的需求。同时，负向提示还能约束轻型视觉适应模块，增强模型在嘈杂监督下的鲁棒性。CoFT 采用了两阶段训练方案：首先进行参数高效的高置信度样本微调，然后转向由协作过滤的伪标签引导的全微调。

Result: 实验表明，在现有的无监督方法上，CoFT 取得了持续的进步。甚至在少量监督基准上也有所提升。

Conclusion: CoFT 正式发布，基于此，进一步提出 CoFT+，通过迭代微调、动量对比学习和 LLM 生成的提示，进一步增强了适应能力，显著超越了现有的无监督方法和少量监督基准。

Abstract: Large-scale vision-language models (VLMs) such as CLIP exhibit strong zero-shot generalization, but adapting them to downstream tasks typically requires costly labeled data. Existing unsupervised self-training methods rely on pseudo-labeling, yet often suffer from unreliable confidence filtering, confirmation bias, and underutilization of low-confidence samples. We propose Collaborative Fine-Tuning (CoFT), an unsupervised adaptation framework that leverages unlabeled data through a dual-model, cross-modal collaboration mechanism. CoFT introduces a dual-prompt learning strategy with positive and negative textual prompts to explicitly model pseudo-label cleanliness in a sample-dependent manner, removing the need for hand-crafted thresholds or noise assumptions. The negative prompt also regularizes lightweight visual adaptation modules, improving robustness under noisy supervision. CoFT employs a two-phase training scheme, transitioning from parameter-efficient fine-tuning on high-confidence samples to full fine-tuning guided by collaboratively filtered pseudo-labels. Building on CoFT, CoFT+ further enhances adaptation via iterative fine-tuning, momentum contrastive learning, and LLM-generated prompts. Extensive experiments demonstrate consistent gains over existing unsupervised methods and even few-shot supervised baselines.

</details>


### [38] [Explicit Uncertainty Modeling for Active CLIP Adaptation with Dual Prompt Tuning](https://arxiv.org/abs/2602.04340)
*Qian-Wei Wang,Yaguang Song,Shu-Tao Xia*

Main category: cs.CV

TL;DR: 提出了一种基于双提示调整的鲁棒不确定性建模框架，用于适应主动学习中的CLIP，该方法在不同的微调方案下表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有方法在利用无标注数据进行微调时，通常采用基于熵或表示聚类的不确定性估计方法，但这些方法没有从模型视角明确建模不确定性。因此，研究提出了一种基于双提示调整的鲁棒不确定性建模方法，以解决在有限注资条件下将预训练的视觉-语言模型适应下游图像分类任务的挑战。

Method: 引入两个可学习的提示到CLIP的文本分支中。积极提示增强任务特异性文本嵌入的可区分性，对应于轻量级调整的视觉嵌入，提高分类可靠性。同时，消极提示以逆向方式训练，以明确建模预测标签正确的概率，为指导主动样本选择提供原则性的不确定性信号。

Result: 在不同的微调方案下进行的广泛实验表明，本方法在相同的注资条件下始终优于现有的主动学习方法。

Conclusion: 本工作提出的方法在主动学习环境中的CLIP适应中表现稳健，特别是在有限标注预算的情况下。

Abstract: Pre-trained vision-language models such as CLIP exhibit strong transferability, yet adapting them to downstream image classification tasks under limited annotation budgets remains challenging. In active learning settings, the model must select the most informative samples for annotation from a large pool of unlabeled data. Existing approaches typically estimate uncertainty via entropy-based criteria or representation clustering, without explicitly modeling uncertainty from the model perspective. In this work, we propose a robust uncertainty modeling framework for active CLIP adaptation based on dual-prompt tuning. We introduce two learnable prompts in the textual branch of CLIP. The positive prompt enhances the discriminability of task-specific textual embeddings corresponding to light-weight tuned visual embeddings, improving classification reliability. Meanwhile, the negative prompt is trained in an reversed manner to explicitly model the probability that the predicted label is correct, providing a principled uncertainty signal for guiding active sample selection. Extensive experiments across different fine-tuning paradigms demonstrate that our method consistently outperforms existing active learning methods under the same annotation budget.

</details>


### [39] [VecSet-Edit: Unleashing Pre-trained LRM for Mesh Editing from Single Image](https://arxiv.org/abs/2602.04349)
*Teng-Fang Hsiao,Bo-Kai Ruan,Yu-Lun Liu,Hong-Han Shuai*

Main category: cs.CV

TL;DR: VecSet-Edit 提出了 VecSet-LRM 作为骨架进行网格编辑，通过 Mask-guided Token Seeding 和 Attention-aligned Token Gating 策略精准定位目标区域，并设计了 Drift-aware Token Pruning 来去除几何异常值，确保几何细节和纹理信息都能被保留。


<details>
  <summary>Details</summary>
Motivation: 当前 3D 编辑方法主要依赖于 3D 高斯斑点或多视图图像，而直接编辑 3D 网格则未被充分探索。传统的基于体素的方法分辨率有限且需要复杂的 3D 掩模工作。因此，作者提出 VecSet-Edit 解决了这些限制。

Method: VecSet-Edit 的方法包括利用带有高质量重建模型 VecSet-LRM 的高保真 VecSet 令牌进行网格编辑。采用 Mask-guided Token Seeding 和 Attention-aligned Token Gating 提高区域定位精确度，并通过 Drift-aware Token Pruning 去除几何异常值。此外，还引入了一个 Detail-preserving Texture Baking 模块来保留原始网格的细节和纹理信息。

Result: 通过验证实验，VecSet-Edit 展现了在处理几何舍入和纹理采样挑战时的优势，证明了其在 3D 编辑中的有效性和灵活性。

Conclusion: VecSet-Edit 提供了一种新颖的方法来进行直接的 3D 网格编辑，通过改进的结构化令牌处理方法提升编辑质量，未来可能会为 3D 内容创建提供新的方案。

Abstract: 3D editing has emerged as a critical research area to provide users with flexible control over 3D assets. While current editing approaches predominantly focus on 3D Gaussian Splatting or multi-view images, the direct editing of 3D meshes remains underexplored. Prior attempts, such as VoxHammer, rely on voxel-based representations that suffer from limited resolution and necessitate labor-intensive 3D mask. To address these limitations, we propose \textbf{VecSet-Edit}, the first pipeline that leverages the high-fidelity VecSet Large Reconstruction Model (LRM) as a backbone for mesh editing. Our approach is grounded on a analysis of the spatial properties in VecSet tokens, revealing that token subsets govern distinct geometric regions. Based on this insight, we introduce Mask-guided Token Seeding and Attention-aligned Token Gating strategies to precisely localize target regions using only 2D image conditions. Also, considering the difference between VecSet diffusion process versus voxel we design a Drift-aware Token Pruning to reject geometric outliers during the denoising process. Finally, our Detail-preserving Texture Baking module ensures that we not only preserve the geometric details of original mesh but also the textural information. More details can be found in our project page: https://github.com/BlueDyee/VecSet-Edit/tree/main

</details>


### [40] [When and Where to Attack? Stage-wise Attention-Guided Adversarial Attack on Large Vision Language Models](https://arxiv.org/abs/2602.04356)
*Jaehyun Kwak,Nam Cao,Boryeong Cho,Segyu Lee,Sumyeong Ahn,Se-Young Yun*

Main category: cs.CV

TL;DR: 本文提出了一种名为SAGA的方法，通过逐步集中在高注意力区域上利用受限的扰动预算，使得生成的对抗样本在视觉上几乎不可察觉，并在十种LVLM上实现了最先进的攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 为了揭示现代多模态系统的安全性漏洞，并提高对抗攻击的有效性。

Method: SAGA是一种利用注意力指导逐步集中在高关注度区域上的方法，通过观察到区域的注意力得分与对抗损失的敏感性正相关，以及攻击高关注度区域会引致注意力结构化的重新分配至上一个显著区域。SAGA使用一种分阶段的策略，逐步优化扰动分配。

Result: SAGA生成的对抗样本在视觉上几乎不可察觉，同时在十种LVLM上实现了最先进的攻击成功率。

Conclusion: 该研究通过引入SAGA方法，在对抗攻击领域取得了重要进展，尤其是在保证扰动不可见的前提下，提升了攻击的成功率。

Abstract: Adversarial attacks against Large Vision-Language Models (LVLMs) are crucial for exposing safety vulnerabilities in modern multimodal systems. Recent attacks based on input transformations, such as random cropping, suggest that spatially localized perturbations can be more effective than global image manipulation. However, randomly cropping the entire image is inherently stochastic and fails to use the limited per-pixel perturbation budget efficiently. We make two key observations: (i) regional attention scores are positively correlated with adversarial loss sensitivity, and (ii) attacking high-attention regions induces a structured redistribution of attention toward subsequent salient regions. Based on these findings, we propose Stage-wise Attention-Guided Attack (SAGA), an attention-guided framework that progressively concentrates perturbations on high-attention regions. SAGA enables more efficient use of constrained perturbation budgets, producing highly imperceptible adversarial examples while consistently achieving state-of-the-art attack success rates across ten LVLMs. The source code is available at https://github.com/jackwaky/SAGA.

</details>


### [41] [SparVAR: Exploring Sparsity in Visual AutoRegressive Modeling for Training-Free Acceleration](https://arxiv.org/abs/2602.04361)
*Zekun Li,Ning Wang,Tongxin Bai,Changwang Mei,Peisong Wang,Shuang Qiu,Jian Cheng*

Main category: cs.CV

TL;DR: SparVAR提出了一种无需训练的加速框架，通过利用VAR注意力的特性，实现了在大规模分辨率下的高效稀疏注意力计算，同时保持了高频率细节的质量。


<details>
  <summary>Details</summary>
Motivation: 传统的VAR模型在处理高分辨率图像时存在计算复杂度高和延迟大的问题，而先前的加速方法虽然能提高推理速度但牺牲了图像的高频率细节，损害了图像质量。

Method: SparVAR框架通过动态预测后续高分辨率尺度的稀疏注意力模式，从一个稀疏的决策尺度出发并通过高效索引映射机制构建尺度自相似的稀疏注意力。此外，引入了跨尺度的局部稀疏注意力，并实现了高效块稀疏内核，进一步提升计算效率。

Result: 实验结果表明，SparVAR能够在不跳过最终尺度的情况下将生产1024x1024高分辨率图像的生成时间缩短至1秒。相比使用FlashAttention加速的VAR基线，方法的速度提高了1.57倍，几乎保留了所有高频率细节。与现有的跳过尺度策略结合使用时，实现了2.28倍的加速效果，同时保持了竞争力的视觉生成质量。

Conclusion: SparVAR提供了在保持高分辨率图像细节的同时，通过技术创新实现了高效的高分辨率图像生成的解决方案。

Abstract: Visual AutoRegressive (VAR) modeling has garnered significant attention for its innovative next-scale prediction paradigm. However, mainstream VAR paradigms attend to all tokens across historical scales at each autoregressive step. As the next scale resolution grows, the computational complexity of attention increases quartically with resolution, causing substantial latency. Prior accelerations often skip high-resolution scales, which speeds up inference but discards high-frequency details and harms image quality. To address these problems, we present SparVAR, a training-free acceleration framework that exploits three properties of VAR attention: (i) strong attention sinks, (ii) cross-scale activation similarity, and (iii) pronounced locality. Specifically, we dynamically predict the sparse attention pattern of later high-resolution scales from a sparse decision scale, and construct scale self-similar sparse attention via an efficient index-mapping mechanism, enabling high-efficiency sparse attention computation at large scales. Furthermore, we propose cross-scale local sparse attention and implement an efficient block-wise sparse kernel, which achieves $\mathbf{> 5\times}$ faster forward speed than FlashAttention. Extensive experiments demonstrate that the proposed SparseVAR can reduce the generation time of an 8B model producing $1024\times1024$ high-resolution images to the 1s, without skipping the last scales. Compared with the VAR baseline accelerated by FlashAttention, our method achieves a $\mathbf{1.57\times}$ speed-up while preserving almost all high-frequency details. When combined with existing scale-skipping strategies, SparseVAR attains up to a $\mathbf{2.28\times}$ acceleration, while maintaining competitive visual generation quality. Code is available at https://github.com/CAS-CLab/SparVAR.

</details>


### [42] [Enabling Real-Time Colonoscopic Polyp Segmentation on Commodity CPUs via Ultra-Lightweight Architecture](https://arxiv.org/abs/2602.04381)
*Weihao Gao,Zhuo Deng,Zheng Gong,Lan Ma*

Main category: cs.CV

TL;DR: 该研究提出了一种名为UltraSeg的轻量级结肠息肉识别模型，能够在CPU上实时运行，大大减少了模型参数量。该模型在多个公开数据集上性能出色，尤其是其在多中心、多模态图像上的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的高精度分割模型依赖于GPU，不适合部署在资源有限的环境中，如初级医院、移动内窥镜单位或胶囊机器人。因此，该研究旨在开发一种在极端压缩环境下（<0.3百万参数）运行的轻量级模型，以解决临床部署中的计算资源限制问题。

Method: 该模型通过联合优化编码器和解码器的宽度，引入约束膨胀卷积以扩大感受野，以及集成轻量级跨层融合模块来实现高性能。此外，UltraSeg利用极低的参数量（0.4%）达到了高分割分数（>94%的Dice分数），甚至超过了参数量大得多（31百万参数）的U-Net模型。

Result: 实验结果显示，UltraSeg模型在七个公开数据集上的表现显著，尤其是其保持了处理器原生解决方案所需的时间（单核CPU 90 FPS）的同时，显著减少了计算资源的需求。

Conclusion: UltraSeg模型在保持高识别准确性和计算效率的同时，为资源受限的环境提供了即时可用的解决方案，并为未来的临床可部署性和基准测试奠定了坚实基础。

Abstract: Early detection of colorectal cancer hinges on real-time, accurate polyp identification and resection. Yet current high-precision segmentation models rely on GPUs, making them impractical to deploy in primary hospitals, mobile endoscopy units, or capsule robots. To bridge this gap, we present the UltraSeg family, operating in an extreme-compression regime (<0.3 M parameters). UltraSeg-108K (0.108 M parameters) is optimized for single-center data, while UltraSeg-130K (0.13 M parameters) generalizes to multi-center, multi-modal images. By jointly optimizing encoder-decoder widths, incorporating constrained dilated convolutions to enlarge receptive fields, and integrating a cross-layer lightweight fusion module, the models achieve 90 FPS on a single CPU core without sacrificing accuracy. Evaluated on seven public datasets, UltraSeg retains >94% of the Dice score of a 31 M-parameter U-Net while utilizing only 0.4% of its parameters, establishing a strong, clinically viable baseline for the extreme-compression domain and offering an immediately deployable solution for resource-constrained settings. This work provides not only a CPU-native solution for colonoscopy but also a reproducible blueprint for broader minimally invasive surgical vision applications. Source code is publicly available to ensure reproducibility and facilitate future benchmarking.

</details>


### [43] [Interactive Spatial-Frequency Fusion Mamba for Multi-Modal Image Fusion](https://arxiv.org/abs/2602.04405)
*Yixin Zhu,Long Lv,Pingping Zhang,Xuehu Liu,Tongdan Tang,Feng Tian,Weibing Sun,Huchuan Lu*

Main category: cs.CV

TL;DR: 该研究提出了一种名为ISFM的新型多模态图像融合框架，通过模态特定提取器（MSE）、多尺度频域融合（MFF）和交互式空间-频域融合（ISF）步骤，实现多模态图像的高效融合。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态图像融合方法通常通过简单的串行或并行空间-频域融合，不充分利用频率信息的历史依赖性，导致特征表示不够鲁棒。ISFM试图通过更好地利用频率信息来改进多模态图像融合。

Method: ISFM框架首先通过模态特定提取器（MSE）捕捉图像间的长期依赖关系，然后通过多尺度频域融合（MFF）整合不同尺度的低频与高频分量，最后通过交互式空间-频域融合（ISF）步骤整合频率特征，以指导空间特征跨模态的互补表示。

Result: 具体实验表明，ISFM方法在六个多模态图像融合数据集上表现出色，达到了比其他最先进的方法更好的性能。

Conclusion: ISFM提供了一种新的多模态图像融合框架，通过深度结合空间信息和频率信息，为图像融合领域带来了重要改进。

Abstract: Multi-Modal Image Fusion (MMIF) aims to combine images from different modalities to produce fused images, retaining texture details and preserving significant information. Recently, some MMIF methods incorporate frequency domain information to enhance spatial features. However, these methods typically rely on simple serial or parallel spatial-frequency fusion without interaction. In this paper, we propose a novel Interactive Spatial-Frequency Fusion Mamba (ISFM) framework for MMIF. Specifically, we begin with a Modality-Specific Extractor (MSE) to extract features from different modalities. It models long-range dependencies across the image with linear computational complexity. To effectively leverage frequency information, we then propose a Multi-scale Frequency Fusion (MFF). It adaptively integrates low-frequency and high-frequency components across multiple scales, enabling robust representations of frequency features. More importantly, we further propose an Interactive Spatial-Frequency Fusion (ISF). It incorporates frequency features to guide spatial features across modalities, enhancing complementary representations. Extensive experiments are conducted on six MMIF datasets. The experimental results demonstrate that our ISFM can achieve better performances than other state-of-the-art methods. The source code is available at https://github.com/Namn23/ISFM.

</details>


### [44] [LCUDiff: Latent Capacity Upgrade Diffusion for Faithful Human Body Restoration](https://arxiv.org/abs/2602.04406)
*Jue Gong,Zihan Zhou,Jingkai Wang,Shu Li,Libo Liu,Jianliang Lan,Yulun Zhang*

Main category: cs.CV

TL;DR: LCUDiff 是一种稳定的一步框架，通过升级预训练的 latent 差分模型到 16 通道的 latent 空间，并结合通道分割蒸馏和先验保持适应，提升恢复质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法在人体中心图像恢复中表现不足，特别是在恢复人体时，而最新的基于扩散的方法通常依赖预训练文本到图像的扩散模型，这可能会导致恢复保真度不足。

Method: LCUDiff 使用通道分割蒸馏（CSD）和先验保持适应（PPA），以及解码器路由（DeR），提升恢复效果。

Result: 实验表明，LCUDiff 在合成和真实世界数据集上具有竞争力，能够在轻微损坏下提供更高的保真度和更少的伪影。

Conclusion: LCUDiff 提供了一个高效稳定的方法，能够改善图像恢复质量。

Abstract: Existing methods for restoring degraded human-centric images often struggle with insufficient fidelity, particularly in human body restoration (HBR). Recent diffusion-based restoration methods commonly adapt pre-trained text-to-image diffusion models, where the variational autoencoder (VAE) can significantly bottleneck restoration fidelity. We propose LCUDiff, a stable one-step framework that upgrades a pre-trained latent diffusion model from the 4-channel latent space to the 16-channel latent space. For VAE fine-tuning, channel splitting distillation (CSD) is used to keep the first four channels aligned with pre-trained priors while allocating the additional channels to effectively encode high-frequency details. We further design prior-preserving adaptation (PPA) to smoothly bridge the mismatch between 4-channel diffusion backbones and the higher-dimensional 16-channel latent. In addition, we propose a decoder router (DeR) for per-sample decoder routing using restoration-quality score annotations, which improves visual quality across diverse conditions. Experiments on synthetic and real-world datasets show competitive results with higher fidelity and fewer artifacts under mild degradations, while preserving one-step efficiency. The code and model will be at https://github.com/gobunu/LCUDiff.

</details>


### [45] [Med-MMFL: A Multimodal Federated Learning Benchmark in Healthcare](https://arxiv.org/abs/2602.04416)
*Aavash Chhetri,Bibek Niroula,Pratik Shrestha,Yash Raj Shrestha,Lesley A Anderson,Prashnna K Gyawali,Loris Bazzani,Binod Bhattarai*

Main category: cs.CV

TL;DR: 本文介绍了一个新的多模态联邦学习（MMFL）基准——Med-MMFL，以填补医疗领域中多模态联邦学习基准的空白，评估了多种算法在多种模态和场景下的表现。


<details>
  <summary>Details</summary>
Motivation: 现有医疗联邦学习基准主要侧重于单一或双模态，且医疗任务范围有限。Med-MMFL旨在通过构建统一的全面基准来推动多模态联邦学习的研究，以便更系统地理解MMFL的潜力。

Method: Med-MMFL benchmark涵盖了不同数量的模态和任务，并评估了六种最先进的联邦学习算法，分别涉及聚合策略、损失公式和正则化技术。

Result: 该基准测试了两种至四种模态的数据集，包括文本、病理图像、心电图、X光、放射报告和多个MRI序列。实验在自然联邦、合成IID和合成非IID环境中进行，评估了分割、分类、模态对齐和VQA任务。

Conclusion: Med-MMFL提供了一种新的多模态联邦学习评价框架，促进了医疗领域的多模态联邦学习技术研究，并为后续工作提供了数据处理和划分管道的开源项目。

Abstract: Federated learning (FL) enables collaborative model training across decentralized medical institutions while preserving data privacy. However, medical FL benchmarks remain scarce, with existing efforts focusing mainly on unimodal or bimodal modalities and a limited range of medical tasks. This gap underscores the need for standardized evaluation to advance systematic understanding in medical MultiModal FL (MMFL). To this end, we introduce Med-MMFL, the first comprehensive MMFL benchmark for the medical domain, encompassing diverse modalities, tasks, and federation scenarios. Our benchmark evaluates six representative state-of-the-art FL algorithms, covering different aggregation strategies, loss formulations, and regularization techniques. It spans datasets with 2 to 4 modalities, comprising a total of 10 unique medical modalities, including text, pathology images, ECG, X-ray, radiology reports, and multiple MRI sequences. Experiments are conducted across naturally federated, synthetic IID, and synthetic non-IID settings to simulate real-world heterogeneity. We assess segmentation, classification, modality alignment (retrieval), and VQA tasks. To support reproducibility and fair comparison of future multimodal federated learning (MMFL) methods under realistic medical settings, we release the complete benchmark implementation, including data processing and partitioning pipelines, at https://github.com/bhattarailab/Med-MMFL-Benchmark .

</details>


### [46] [TrajVG: 3D Trajectory-Coupled Visual Geometry Learning](https://arxiv.org/abs/2602.04439)
*Xingyu Miao,Weiguang Zhao,Tao Lu,Linning Yu,Mulin Yu,Yang Long,Jiangmiao Pang,Junting Dong*

Main category: cs.CV

TL;DR: TrajVG通过预测相机坐标下的3D轨迹，解决了多帧3D重建模型中存在的跨帧3D对应问题，尤其是在有多个物体运动的视频中。利用密集轨迹、每帧局部点云和相对相机姿态，结合几何一致性的目标来进行优化。同时，通过自我监督的方式训练模型，使其在缺乏3D轨迹标签的野外视频中也能有效工作。


<details>
  <summary>Details</summary>
Motivation: 现有的多帧3D重建模型在具有多个运动物体的视频中表现不佳，因为全局参考变得模棱两可，而局部点图依赖于估计的相对姿态且可能会漂移，导致跨帧不一致和重复结构。

Method: TrajVG框架通过估计相机坐标下的3D轨迹，使得跨帧3D对应关系成为显式预测。模型结合了稀疏轨迹、每帧局部点云和相对相机姿态，并通过几何一致性目标进行优化。

Result: 实验结果显示，TrajVG在3D追踪、姿态估计、点云重建和视频深度估计等多个任务上都超越了当前的前馈性能基线。

Conclusion: TrajVG提供了一种有效的3D重建方法，特别适用于包含复杂运动的野外视频。

Abstract: Feed-forward multi-frame 3D reconstruction models often degrade on videos with object motion. Global-reference becomes ambiguous under multiple motions, while the local pointmap relies heavily on estimated relative poses and can drift, causing cross-frame misalignment and duplicated structures. We propose TrajVG, a reconstruction framework that makes cross-frame 3D correspondence an explicit prediction by estimating camera-coordinate 3D trajectories. We couple sparse trajectories, per-frame local point maps, and relative camera poses with geometric consistency objectives: (i) bidirectional trajectory-pointmap consistency with controlled gradient flow, and (ii) a pose consistency objective driven by static track anchors that suppresses gradients from dynamic regions. To scale training to in-the-wild videos where 3D trajectory labels are scarce, we reformulate the same coupling constraints into self-supervised objectives using only pseudo 2D tracks, enabling unified training with mixed supervision. Extensive experiments across 3D tracking, pose estimation, pointmap reconstruction, and video depth show that TrajVG surpasses the current feedforward performance baseline.

</details>


### [47] [SynthVerse: A Large-Scale Diverse Synthetic Dataset for Point Tracking](https://arxiv.org/abs/2602.04441)
*Weiguang Zhao,Haoran Xu,Xingyu Miao,Qin Zhao,Rui Zhang,Kaizhu Huang,Ning Gao,Peizhou Cao,Mingze Sun,Mulin Yu,Tao Lu,Linning Xu,Junting Dong,Jiangmiao Pang*

Main category: cs.CV

TL;DR: SynthVerse是一个大型、多样的合成数据集，旨在通过引入新领域和对象类型（如动画风格内容、实体操作、场景导航和关节物体），扩充现有数据集的多样性和高质量动态运动和交互，以支持更广泛的点跟踪方法的训练和评估。


<details>
  <summary>Details</summary>
Motivation: 现有点跟踪数据集存在数据不足和标注不准确的问题，这限制了点跟踪技术的发展。因此，SynthVerse旨在通过增加新的数据领域和类型，提供更为多样的真实数据来促进该领域的研究。

Method: SynthVerse通过合成生成技术生成了包括动画风格内容、实体操作、场景导航和可动物体等内容的新数据集，以此扩展数据的多样性和质量，增强模型在不同场景下的适应性。

Result: SynthVerse在多种点跟踪方法上展示了明显的改进效果，表明合成数据在促进模型泛化性能方面的潜力，并揭示了现有跟踪器在复杂条件下的局限性。

Conclusion: SynthVerse为点跟踪领域提供了高质量的数据支持，推动了未来点跟踪技术的研究与发展。

Abstract: Point tracking aims to follow visual points through complex motion, occlusion, and viewpoint changes, and has advanced rapidly with modern foundation models. Yet progress toward general point tracking remains constrained by limited high-quality data, as existing datasets often provide insufficient diversity and imperfect trajectory annotations. To this end, we introduce SynthVerse, a large-scale, diverse synthetic dataset specifically designed for point tracking. SynthVerse includes several new domains and object types missing from existing synthetic datasets, such as animated-film-style content, embodied manipulation, scene navigation, and articulated objects. SynthVerse substantially expands dataset diversity by covering a broader range of object categories and providing high-quality dynamic motions and interactions, enabling more robust training and evaluation for general point tracking. In addition, we establish a highly diverse point tracking benchmark to systematically evaluate state-of-the-art methods under broader domain shifts. Extensive experiments and analyses demonstrate that training with SynthVerse yields consistent improvements in generalization and reveal limitations of existing trackers under diverse settings.

</details>


### [48] [Seg-ReSearch: Segmentation with Interleaved Reasoning and External Search](https://arxiv.org/abs/2602.04454)
*Tianming Liang,Qirui Du,Jian-Fang Hu,Haichao Jiang,Zicheng Lin,Wei-Shi Zheng*

Main category: cs.CV

TL;DR: 本文提出了一种名为Seg-ReSearch的新型分割范式，通过使分割系统能够在实时场景中处理动态、开放世界的问题，从而克服现有的知识瓶颈。通过引入层级奖励设计，Seg-ReSearch能够在保持系统灵活性的同时，提供逐步的引导和激励。


<details>
  <summary>Details</summary>
Motivation: 当前基于语言的分割方法受到内部知识的限制，无法处理需要最新信息或特定领域概念的现实场景。Seg-ReSearch通过结合推理与外部搜索，旨在使分割系统能够应对开放世界的问题和查询。

Method: Seg-ReSearch通过引入层级奖励设计来优化训练过程，该设计允许初始引导与逐步激励相结合，以减少稀疏结果信号与僵化步骤监督之间的冲突。此外，该系统还能够动态调整其搜索策略，以适应不同的查询需求。

Result: Seg-ReSearch在挑战性基准OK-VOS以及两个现有的推理分割基准上，其性能显著优于现有最先进的方法。

Conclusion: Seg-ReSearch为分割系统提供了一种新的训练方法，使其能够处理需要外部知识和动态推理的真实世界场景。该方法为解决分割领域的开放世界挑战提供了新的视角。

Abstract: Segmentation based on language has been a popular topic in computer vision. While recent advances in multimodal large language models (MLLMs) have endowed segmentation systems with reasoning capabilities, these efforts remain confined by the frozen internal knowledge of MLLMs, which limits their potential for real-world scenarios that involve up-to-date information or domain-specific concepts. In this work, we propose \textbf{Seg-ReSearch}, a novel segmentation paradigm that overcomes the knowledge bottleneck of existing approaches. By enabling interleaved reasoning and external search, Seg-ReSearch empowers segmentation systems to handle dynamic, open-world queries that extend beyond the frozen knowledge of MLLMs. To effectively train this capability, we introduce a hierarchical reward design that harmonizes initial guidance with progressive incentives, mitigating the dilemma between sparse outcome signals and rigid step-wise supervision. For evaluation, we construct OK-VOS, a challenging benchmark that explicitly requires outside knowledge for video object segmentation. Experiments on OK-VOS and two existing reasoning segmentation benchmarks demonstrate that our Seg-ReSearch improves state-of-the-art approaches by a substantial margin. Code and data will be released at https://github.com/iSEE-Laboratory/Seg-ReSearch.

</details>


### [49] [Temporal Slowness in Central Vision Drives Semantic Object Learning](https://arxiv.org/abs/2602.04462)
*Timothy Schaumlöffel,Arthur Aubret,Gemma Roig,Jochen Triesch*

Main category: cs.CV

TL;DR: 该研究通过使用Ego4D数据集模拟人类视觉体验五个月，并通过状态最前沿的目光预测模型生成注视点坐标，从中抽取中央视觉区域的图像，训练时间对比的自我监督学习模型，展现了中央视觉和时间缓慢学习如何增强不同语义方面的物体表示。


<details>
  <summary>Details</summary>
Motivation: 人类从内视角视觉流中获取语义物体表示的能力，仅需少量监督。此研究旨在探索中央视觉和缓慢学习机制在形成人类经验下语义物体表示中的作用。

Method: 研究采取了模拟五个月人类视觉经验的方法，并使用Ego4D数据集和最先进的目光预测模型生成注视点坐标，然后从中抽取中央视觉区域的图像，训练时间对比的自我监督学习模型。

Result: 研究结果表明，结合时间缓慢学习和中央视觉可以改善不同语义方面物体表示的编码。关注中央视觉加强了背景物体特征的提取，而考虑时间缓慢性，特别是在注视期间，使模型能够更好地编码关于物体的广泛语义信息。

Conclusion: 本研究提供了关于人类如何从自然视觉经验中发展语义物体表示机制的新见解。中央视觉和缓慢学习在该过程中起重要作用，能够增强对不同语义方面的物体信息的理解和处理。

Abstract: Humans acquire semantic object representations from egocentric visual streams with minimal supervision. Importantly, the visual system processes with high resolution only the center of its field of view and learns similar representations for visual inputs occurring close in time. This emphasizes slowly changing information around gaze locations. This study investigates the role of central vision and slowness learning in the formation of semantic object representations from human-like visual experience. We simulate five months of human-like visual experience using the Ego4D dataset and generate gaze coordinates with a state-of-the-art gaze prediction model. Using these predictions, we extract crops that mimic central vision and train a time-contrastive Self-Supervised Learning model on them. Our results show that combining temporal slowness and central vision improves the encoding of different semantic facets of object representations. Specifically, focusing on central vision strengthens the extraction of foreground object features, while considering temporal slowness, especially during fixational eye movements, allows the model to encode broader semantic information about objects. These findings provide new insights into the mechanisms by which humans may develop semantic object representations from natural visual experience.

</details>


### [50] [SALAD-Pan: Sensor-Agnostic Latent Adaptive Diffusion for Pan-Sharpening](https://arxiv.org/abs/2602.04473)
*Junjie Li,Congyang Ou,Haokui Zhang,Guoting Wei,Shengqin Jiang,Ying Li,Chunhua Shen*

Main category: cs.CV

TL;DR: SALAD-Pan 提出了一种传感器无关的潜在空间扩散方法，用于高效的 pansharpening。通过单一通道 VAE 编码高分辨率多光谱图像，注入光谱物理属性和双向交互控制结构，增强了光谱一致性，最终实现了比最先进的扩散方法更好的精度和速度。


<details>
  <summary>Details</summary>
Motivation: 鉴于大多数现有模型依赖于像素空间且需为不同多光谱图像训练特定模型，导致高延迟和传感器特异性限制，本文研究了如何利用潜在空间扩散方法实现高效的 pansharpening，目标是构建一种传感器无关的方法，即使对不同传感器采集的数据也能实现高精度融合。

Method: SALAD-Pan 方法首先通过单一通道 VAE 将高分辨率多光谱图像编码为紧凑的潜在表示，以支持不同通道数的多光谱图像。接着通过单向和双向交互控制结构同时注入光谱物理属性、PAN 和多光谱图像，实现扩散模拟过程中的高精度融合。最后加入一个轻量级跨谱段注意力模块，强化光谱联系以提升光谱一致性。

Result: 实验结果表明，SALAD-Pan 在处理 GaoFen-2、QuickBird 和 WorldView-3 数据集时，相较于其他最先进的扩散基方法，不仅具有更高的精度，还实现了 2-3 倍的推理速度提升，并能在不同传感器间表现出一定的零样本鲁棒性。

Conclusion: 相较于现有模型，SALAD-Pan 方法有效克服了常规方法的局限性，在 pansharpening 应用中实现了传感器无关性、高效性及高精度融合。

Abstract: Recently, diffusion models bring novel insights for Pan-sharpening and notably boost fusion precision. However, most existing models perform diffusion in the pixel space and train distinct models for different multispectral (MS) imagery, suffering from high latency and sensor-specific limitations. In this paper, we present SALAD-Pan, a sensor-agnostic latent space diffusion method for efficient pansharpening. Specifically, SALAD-Pan trains a band-wise single-channel VAE to encode high-resolution multispectral (HRMS) into compact latent representations, supporting MS images with various channel counts and establishing a basis for acceleration. Then spectral physical properties, along with PAN and MS images, are injected into the diffusion backbone through unidirectional and bidirectional interactive control structures respectively, achieving high-precision fusion in the diffusion process. Finally, a lightweight cross-spectral attention module is added to the central layer of diffusion model, reinforcing spectral connections to boost spectral consistency and further elevate fusion precision. Experimental results on GaoFen-2, QuickBird, and WorldView-3 demonstrate that SALAD-Pan outperforms state-of-the-art diffusion-based methods across all three datasets, attains a 2-3x inference speedup, and exhibits robust zero-shot (cross-sensor) capability.

</details>


### [51] [Vision-aligned Latent Reasoning for Multi-modal Large Language Model](https://arxiv.org/abs/2602.04476)
*Byungwoo Jeon,Yoonwoo Jeong,Hyunseok Lee,Minsu Cho,Jinwoo Shin*

Main category: cs.CV

TL;DR: VaLR 提出了一种简单却有效的框架，通过生成与视觉信息对齐的潜变量在长上下文理解和精细视觉感知任务中提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大语言模型在进行长时间推理时，通常因视觉信息在长上下文生成中的逐渐稀释而难以充分利用测试时的扩展性，为此提出了Vision-aligned Latent Reasoning (VaLR) 框架。

Method: VaLR 通过在每次Chain of Thought推理步骤前动态生成与视觉信息对齐的潜变量，引导模型基于潜在空间中的感知线索进行推理。VaLR 在训练中通过使多模态大语言模型的中间嵌入与其视觉编码器的嵌入对齐来保持视觉知识。

Result: 实验结果表明，VaLR 在多个需要长上下文理解或精细视觉感知的基准测试中表现出色，且其测试时的扩展行为优于之前的多模态大语言模型。特别地，VaLR 在 VSI-Bench 上的表现显著提高，从 33.0% 提升至 52.9%，相比 Qwen2.5-VL 的增益为 19.9%。

Conclusion: VaLR 框架为解决多模态大语言模型在长时间推理问题上的局限性提供了一个有效的解决方案。

Abstract: Despite recent advancements in Multi-modal Large Language Models (MLLMs) on diverse understanding tasks, these models struggle to solve problems which require extensive multi-step reasoning. This is primarily due to the progressive dilution of visual information during long-context generation, which hinders their ability to fully exploit test-time scaling. To address this issue, we introduce Vision-aligned Latent Reasoning (VaLR), a simple, yet effective reasoning framework that dynamically generates vision-aligned latent tokens before each Chain of Thought reasoning step, guiding the model to reason based on perceptual cues in the latent space. Specifically, VaLR is trained to preserve visual knowledge during reasoning by aligning intermediate embeddings of MLLM with those from vision encoders. Empirical results demonstrate that VaLR consistently outperforms existing approaches across a wide range of benchmarks requiring long-context understanding or precise visual perception, while exhibiting test-time scaling behavior not observed in prior MLLMs. In particular, VaLR improves the performance significantly from 33.0% to 52.9% on VSI-Bench, achieving a 19.9%p gain over Qwen2.5-VL.

</details>


### [52] [SLUM-i: Semi-supervised Learning for Urban Mapping of Informal Settlements and Data Quality Benchmarking](https://arxiv.org/abs/2602.04525)
*Muhammad Taha Mukhtar,Syed Musa Ali Kazmi,Khola Naseem,Muhammad Ali Chattha,Andreas Dengel,Sheraz Ahmed,Muhammad Naseer Bajwa,Muhammad Imran Malik*

Main category: cs.CV

TL;DR: 本文介绍了针对快速城市扩张导致的informal settlements大规模测绘的解决方案，提出了一种新的半监督分割框架，有效应对类不平衡和特征退化问题，通过广泛的实验展示了在不同城市的性能优势。


<details>
  <summary>Details</summary>
Motivation: 为了解决低中收入国家大城市中informal settlements的大规模测绘问题，这些问题普遍存在的标注稀缺和数据质量挑战促使本文提出了新的解决方案。

Method: 文章构建了针对拉合尔的大规模基准数据集，结合新的半监督分割框架，集成Class-Aware Adaptive Thresholding和Prototype Bank System机制，以此提升模型在不同城市中的泛化能力。

Result: 实验结果显示，在八个不同城市中，该方法优于最先进的半监督基线，并且具有更好的领域迁移能力，即使仅使用10%的源标签数据，模型在未见过的地理区域也能达到0.461的mIoU。

Conclusion: 本文提出的方法不仅提高了半监督学习框架的性能，还展示了强大的领域泛化能力，特别是在标签稀缺且类别不平衡的情况下。

Abstract: Rapid urban expansion has fueled the growth of informal settlements in major cities of low- and middle-income countries, with Lahore and Karachi in Pakistan and Mumbai in India serving as prominent examples. However, large-scale mapping of these settlements is severely constrained not only by the scarcity of annotations but by inherent data quality challenges, specifically high spectral ambiguity between formal and informal structures and significant annotation noise. We address this by introducing a benchmark dataset for Lahore, constructed from scratch, along with companion datasets for Karachi and Mumbai, which were derived from verified administrative boundaries, totaling 1,869 $\text{km}^2$ of area. To evaluate the global robustness of our framework, we extend our experiments to five additional established benchmarks, encompassing eight cities across three continents, and provide comprehensive data quality assessments of all datasets. We also propose a new semi-supervised segmentation framework designed to mitigate the class imbalance and feature degradation inherent in standard semi-supervised learning pipelines. Our method integrates a Class-Aware Adaptive Thresholding mechanism that dynamically adjusts confidence thresholds to prevent minority class suppression and a Prototype Bank System that enforces semantic consistency by anchoring predictions to historically learned high-fidelity feature representations. Extensive experiments across a total of eight cities spanning three continents demonstrate that our approach outperforms state-of-the-art semi-supervised baselines. Most notably, our method demonstrates superior domain transfer capability whereby a model trained on only 10% of source labels reaches a 0.461 mIoU on unseen geographies and outperforms the zero-shot generalization of fully supervised models.

</details>


### [53] [S-MUSt3R: Sliding Multi-view 3D Reconstruction](https://arxiv.org/abs/2602.04517)
*Leonid Antsfeld,Boris Chidlovskii,Yohann Cabon,Vincent Leroy,Jerome Revaud*

Main category: cs.CV

TL;DR: S-MUSt3R提出了一种简单而有效的管道，通过序列分割、片段对齐和轻量级循环闭合优化来扩展基础模型在单目3D重建中的应用，实现了与传统复杂架构方法相当的轨迹和重建性能。


<details>
  <summary>Details</summary>
Motivation: 本文动机在于解决基础模型在大型RGB流3D重建中的记忆限制问题，通过简单策略提升模型的扩展性。

Method: 方法包括序列分割、片段对齐和轻量级循环闭合优化，无需重新训练模型即可利用MUSt3R模型的3D重建能力。

Result: 实验结果表明，S-MUSt3R在TUM、7-Scenes和自有的机器人导航数据集上运行成功，能够准确、一致地产生3D重建。

Conclusion: 研究结论认为，S-MUSt3R能够利用MUSt3R模型在实际场景中实现可扩展的单目3D场景重建，具有直接在度量空间中进行预测的重要优势。

Abstract: The recent paradigm shift in 3D vision led to the rise of foundation models with remarkable capabilities in 3D perception from uncalibrated images. However, extending these models to large-scale RGB stream 3D reconstruction remains challenging due to memory limitations. This work proposes S-MUSt3R, a simple and efficient pipeline that extends the limits of foundation models for monocular 3D reconstruction. Our approach addresses the scalability bottleneck of foundation models through a simple strategy of sequence segmentation followed by segment alignment and lightweight loop closure optimization. Without model retraining, we benefit from remarkable 3D reconstruction capacities of MUSt3R model and achieve trajectory and reconstruction performance comparable to traditional methods with more complex architecture. We evaluate S-MUSt3R on TUM, 7-Scenes and proprietary robot navigation datasets and show that S-MUSt3R runs successfully on long RGB sequences and produces accurate and consistent 3D reconstruction. Our results highlight the potential of leveraging the MUSt3R model for scalable monocular 3D scene in real-world settings, with an important advantage of making predictions directly in the metric space.

</details>


### [54] [Nix and Fix: Targeting 1000x Compression of 3D Gaussian Splatting with Diffusion Models](https://arxiv.org/abs/2602.04549)
*Cem Eteke,Enzo Tartaglione*

Main category: cs.CV

TL;DR: NiFi是一种通过具有意识的扩散单步蒸馏进行极简3DGS压缩的新方法，能够在极低的压缩率下实现卓越的感知质量，代码将在文章被接受后开源。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS压缩方法在低压缩率下引入了显著的视觉质量降级问题，NiFi旨在解决这一问题，提高3DGS的压缩效率并保持高质量感知。

Method: NiFi采用了一种基于差分的单步蒸馏方法，能够在模型中捕捉和移除过度平滑而产生的视觉瑕疵，从而优化3D图像的质量。

Result: NiFi在极低的压缩率（0.1 MB）下实现了最佳感知质量，并且在与3DGS相当的感知性能下提高了约1000倍的压缩率。

Conclusion: NiFi为3DGS压缩问题提供了一种新颖的解决方案，有望推动沉浸式通信等领域的应用发展。

Abstract: 3D Gaussian Splatting (3DGS) revolutionized novel view rendering. Instead of inferring from dense spatial points, as implicit representations do, 3DGS uses sparse Gaussians. This enables real-time performance but increases space requirements, hindering applications such as immersive communication. 3DGS compression emerged as a field aimed at alleviating this issue. While impressive progress has been made, at low rates, compression introduces artifacts that degrade visual quality significantly. We introduce NiFi, a method for extreme 3DGS compression through restoration via artifact-aware, diffusion-based one-step distillation. We show that our method achieves state-of-the-art perceptual quality at extremely low rates, down to 0.1 MB, and towards 1000x rate improvement over 3DGS at comparable perceptual performance. The code will be open-sourced upon acceptance.

</details>


### [55] [Understanding Degradation with Vision Language Model](https://arxiv.org/abs/2602.04565)
*Guanzhou Lan,Chenyi Liao,Yuqi Yang,Qianli Ma,Zhigang Wang,Dong Wang,Bin Zhao,Xuelong Li*

Main category: cs.CV

TL;DR: 本文重新定义了对图像降级的理解为一种层次结构式的预测任务，提出了DU-VLM模型，利用监督微调和基于结构奖励的强化学习进行训练。同时，引入了DU-110k数据集，该数据集包含110,000对干净和降级图像，具有物理注解。实验结果表明，DU-VLM表现出更高的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 视觉退化在计算机视觉中是一个关键但具有挑战性的问题。现有的一些Vision-Language模型在定性的描述方面表现出色，但在理解图像退化的参量物理方面却表现不佳。本文旨在提出一种新的方法来解决这一问题。

Method: 本文提出了DU-VLM模型，一种基于多模态链式思考的模型，通过监督微调和基于结构奖励的强化学习进行训练。同时，还引入了DU-110k数据集，用于该模型的训练。

Result: 实验结果显示，DU-VLM在准确性和鲁棒性方面显著优于通用基准模型，并且能够在未见过的分布上实现良好的泛化。

Conclusion: 本文提出的方法和模型在图像退化理解方面取得了很好的成果，为后续研究提供了新的思路和技术支持。

Abstract: Understanding visual degradations is a critical yet challenging problem in computer vision. While recent Vision-Language Models (VLMs) excel at qualitative description, they often fall short in understanding the parametric physics underlying image degradations. In this work, we redefine degradation understanding as a hierarchical structured prediction task, necessitating the concurrent estimation of degradation types, parameter keys, and their continuous physical values. Although these sub-tasks operate in disparate spaces, we prove that they can be unified under one autoregressive next-token prediction paradigm, whose error is bounded by the value-space quantization grid. Building on this insight, we introduce DU-VLM, a multimodal chain-of-thought model trained with supervised fine-tuning and reinforcement learning using structured rewards. Furthermore, we show that DU-VLM can serve as a zero-shot controller for pre-trained diffusion models, enabling high-fidelity image restoration without fine-tuning the generative backbone. We also introduce \textbf{DU-110k}, a large-scale dataset comprising 110,000 clean-degraded pairs with grounded physical annotations. Extensive experiments demonstrate that our approach significantly outperforms generalist baselines in both accuracy and robustness, exhibiting generalization to unseen distributions.

</details>


### [56] [DRMOT: A Dataset and Framework for RGBD Referring Multi-Object Tracking](https://arxiv.org/abs/2602.04692)
*Sijia Chen,Lijuan Ma,Yanqiu Yu,En Yu,Liman Liu,Wenbing Tao*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的任务DRMOT，要求模型融合RGB，深度和语言模态，以便实现3D感知跟踪。同时，作者构建了一个名为DRSet的RGBD参照多对象跟踪数据集，以及DRTrack框架，该框架能够从联合的RGB-D-L输入中进行深度感知的目标定位，并通过整合深度线索实现稳健的轨迹关联。


<details>
  <summary>Details</summary>
Motivation: 现有RMOV模型依赖于2D RGB数据，难以准确检测和关联具有复杂空间语义的目标（如“离相机最近的人”），并且在严重遮挡下保持可靠的身份。因此，本文提出了RGBD Referring Multi-Object Tracking (DRMOT)，并构建了DRSet数据集来评估模型的空间语义定位和跟踪能力。

Method: 本文提出了一种深度导向的参照跟踪框架DRTrack，该框架能够从联合的RGB-D-L输入中进行深度感知的目标定位，并通过整合深度线索实现稳健的轨迹关联。

Result: 在DRSet数据集上的大量实验表明，本文提出的DRTrack框架有效。

Conclusion: 本文提出了一种新颖的DRMOT任务以及相应的数据集和框架，这些为多对象跟踪领域注入了新的活力，特别是在融合多种模态数据以实现3D感知跟踪方面。

Abstract: Referring Multi-Object Tracking (RMOT) aims to track specific targets based on language descriptions and is vital for interactive AI systems such as robotics and autonomous driving. However, existing RMOT models rely solely on 2D RGB data, making it challenging to accurately detect and associate targets characterized by complex spatial semantics (e.g., ``the person closest to the camera'') and to maintain reliable identities under severe occlusion, due to the absence of explicit 3D spatial information. In this work, we propose a novel task, RGBD Referring Multi-Object Tracking (DRMOT), which explicitly requires models to fuse RGB, Depth (D), and Language (L) modalities to achieve 3D-aware tracking. To advance research on the DRMOT task, we construct a tailored RGBD referring multi-object tracking dataset, named DRSet, designed to evaluate models' spatial-semantic grounding and tracking capabilities. Specifically, DRSet contains RGB images and depth maps from 187 scenes, along with 240 language descriptions, among which 56 descriptions incorporate depth-related information. Furthermore, we propose DRTrack, a MLLM-guided depth-referring tracking framework. DRTrack performs depth-aware target grounding from joint RGB-D-L inputs and enforces robust trajectory association by incorporating depth cues. Extensive experiments on the DRSet dataset demonstrate the effectiveness of our framework.

</details>


### [57] [SalFormer360: a transformer-based saliency estimation model for 360-degree videos](https://arxiv.org/abs/2602.04584)
*Mahmoud Z. A. Wahba,Francesco Barbato,Sara Baldoni,Federica Battisti*

Main category: cs.CV

TL;DR: 文章提出了一种基于变压器架构的新颖360度视视频显著性估计模型SalFormer360，结合SegFormer编码器和自定义解码器，并引入了观视中心偏差来增强预测准确性，实验结果显示该模型在多个基准数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 鉴于360度视频在众多应用场景中的重要性，如视口预测和沉浸式内容优化，迫切需要一种有效的方法来估计其中的显著性。现有的方法如SegFormer虽然在2D场景中表现出色，但需要进一步调整以适应360度视频环境。

Method: 文章采用SegFormer作为编码器，结合一个自定义解码器构建了SalFormer360模型。此外，引入了Viewing Center Bias以反映用户的注意力分布。

Result: 通过在多个基准数据集上的实验，SalFormer360在显著性估计任务上的表现优于现有方法。特别是，相较于之前的方法，在Sport360上取得了8.4%的改进，在PVS-HM上提高了2.5%，在VR-EyeTracking上则提高了18.6%。

Conclusion: 本研究提出的方法SalFormer360显著提升了360度视频的显著性估计性能，为该领域的研究提供了新的视角和思路。

Abstract: Saliency estimation has received growing attention in recent years due to its importance in a wide range of applications. In the context of 360-degree video, it has been particularly valuable for tasks such as viewport prediction and immersive content optimization. In this paper, we propose SalFormer360, a novel saliency estimation model for 360-degree videos built on a transformer-based architecture. Our approach is based on the combination of an existing encoder architecture, SegFormer, and a custom decoder. The SegFormer model was originally developed for 2D segmentation tasks, and it has been fine-tuned to adapt it to 360-degree content. To further enhance prediction accuracy in our model, we incorporated Viewing Center Bias to reflect user attention in 360-degree environments. Extensive experiments on the three largest benchmark datasets for saliency estimation demonstrate that SalFormer360 outperforms existing state-of-the-art methods. In terms of Pearson Correlation Coefficient, our model achieves 8.4% higher performance on Sport360, 2.5% on PVS-HM, and 18.6% on VR-EyeTracking compared to previous state-of-the-art.

</details>


### [58] [SAR-RAG: ATR Visual Question Answering by Semantic Search, Retrieval, and MLLM Generation](https://arxiv.org/abs/2602.04712)
*David F. Ramirez,Tim Overman,Kristen Jaskie,Joe Marvin,Andreas Spanias*

Main category: cs.CV

TL;DR: 本文介绍了一种基于合成孔径雷达（SAR）自动目标识别（ATR）的视觉上下文图像检索增强生成方法（SAR-RAG）。该方法结合了多模态大型语言模型和语义嵌入向量数据库，以支持图像示例的上下文搜索，并通过检索已知目标类型的图像示例，提高了车辆类别识别精度。


<details>
  <summary>Details</summary>
Motivation: 现有的雷达图像识别方法在现实应用中存在一定的局限性，需要一种更有效的方法来提高目标识别的准确性和鲁棒性。

Method: SAR-RAG方法利用多模态大型语言模型和语义嵌入向量数据库，通过检索已知目标类型的图像示例，辅助图像生成，实现自动目标识别。

Result: 通过使用SAR-RAG方法，当将其添加至多模态大型语言模型基线方法作为附加的ATR记忆库时，搜索和检索指标、类别分类准确性和车辆尺寸回归等指标均显示出改进。

Conclusion: 本文提出的方法有望提高雷达图像中的自动目标识别性能。

Abstract: We present a visual-context image retrieval-augmented generation (ImageRAG) assisted AI agent for automatic target recognition (ATR) of synthetic aperture radar (SAR). SAR is a remote sensing method used in defense and security applications to detect and monitor the positions of military vehicles, which may appear indistinguishable in images. Researchers have extensively studied SAR ATR to improve the differentiation and identification of vehicle types, characteristics, and measurements. Test examples can be compared with known vehicle target types to improve recognition tasks. New methods enhance the capabilities of neural networks, transformer attention, and multimodal large language models. An agentic AI method may be developed to utilize a defined set of tools, such as searching through a library of similar examples. Our proposed method, SAR Retrieval-Augmented Generation (SAR-RAG), combines a multimodal large language model (MLLM) with a vector database of semantic embeddings to support contextual search for image exemplars with known qualities. By recovering past image examples with known true target types, our SAR-RAG system can compare similar vehicle categories, achieving improved ATR prediction accuracy. We evaluate this through search and retrieval metrics, categorical classification accuracy, and numeric regression of vehicle dimensions. These metrics all show improvements when SAR-RAG is added to an MLLM baseline method as an attached ATR memory bank.

</details>


### [59] [ImmuVis: Hyperconvolutional Foundation Model for Imaging Mass Cytometry](https://arxiv.org/abs/2602.04585)
*Marcin Możejko,Dawid Uchal,Krzysztof Gogolewski,Piotr Kupidura,Szymon Łukasik,Jakub Giezgała,Tomasz Nocoń,Kacper Pietrzyk,Robert Pieniuta,Mateusz Sulimowicz,Michal Orzyłowski,Tomasz Siłkowski,Karol Zagródka,Eike Staub,Ewa Szczurek*

Main category: cs.CV

TL;DR: ImmuVis 是一种针对高通量多路复用成像技术免疫斑点图谱(IMC)的高效卷积基础模型，通过引入标记自适应超卷积来适应实时变化的标记集，无需重新训练即可处理任意测量标记子集。ImmuVis 在虚拟染色和下游分类任务中表现出色，具有较低的计算成本和唯一提供校准不确定性估计的能力。


<details>
  <summary>Details</summary>
Motivation: 常规的视觉回路模型假定固定通道空间，但在实际的多路复用成像中，因不同研究的标记集会变化，因此传统的模型不适用。为此，研究人员开发了ImmuVis，以适应多变的标记集。

Method: ImmuVis 采用自监督掩码重构方式进行预训练，并通过标记自适应超卷积生成卷积核，无需重新训练就能处理任意标记子集。其通过大规模数据集IMC17M进行了预训练（包含28个队列、24,405张图、265个标记以及1.7亿个补丁），并通过异方差似然目标提供了校准的不确定性估计。

Result: ImmuVis 在虚拟染色和下游分类任务中表现出优越性能，相比基于变压器的替代方案具有显著较低的计算成本。此外，它是唯一一个可以通过异方差似然目标提供校准不确定性估计的模型。

Conclusion: ImmuVis 提供了一个实用且高效的解决方案，适用于现实世界中的 IMC 模型构建。

Abstract: We present ImmuVis, an efficient convolutional foundation model for imaging mass cytometry (IMC), a high-throughput multiplex imaging technology that handles molecular marker measurements as image channels and enables large-scale spatial tissue profiling. Unlike natural images, multiplex imaging lacks a fixed channel space, as real-world marker sets vary across studies, violating a core assumption of standard vision backbones. To address this, ImmuVis introduces marker-adaptive hyperconvolutions that generate convolutional kernels from learned marker embeddings, enabling a single model to operate on arbitrary measured marker subsets without retraining. We pretrain ImmuVis on the largest to-date dataset, IMC17M (28 cohorts, 24,405 images, 265 markers, over 17M patches), using self-supervised masked reconstruction. ImmuVis outperforms SOTA baselines and ablations in virtual staining and downstream classification tasks at substantially lower compute cost than transformer-based alternatives, and is the sole model that provides calibrated uncertainty via a heteroscedastic likelihood objective. These results position ImmuVis as a practical, efficient foundation model for real-world IMC modeling.

</details>


### [60] [A labeled dataset of simulated phlebotomy procedures for medical AI: polygon annotations for object detection and human-object interaction](https://arxiv.org/abs/2602.04624)
*Raúl Jiménez Cruz,César Torres-Huitzil,Marco Franceschetti,Ronny Seiger,Luciano García-Bañuelos,Barbara Weber*

Main category: cs.CV

TL;DR: 本数据集包含11,884张标记图像，涵盖了模拟静脉穿刺操作的各个步骤，通过多种技术处理，确保数据集的多样性和实用性，适合用于医学培训自动化和人机交互的研究。


<details>
  <summary>Details</summary>
Motivation: 鉴于静脉穿刺操作是医学培训中的重要组成部分，本文旨在提供一个包含多种图像标记的高质量数据集，以推动医学培训自动化和人机交互领域的发展。

Method: 文章采用高清晰度视频并使用结构相似性指数测量（SSIM）减少冗余，通过自动化面部匿名处理，并应用多类多边形标注，最终将数据集分为训练、验证和测试三个子集。

Result: 数据集包含覆盖静脉穿刺操作过程的11,884张图像，并用现代目标检测框架兼容的分割格式导出标注信息，且已被划分为训练（70%）、验证（15%）和测试（15%）三个子集。

Conclusion: 该数据集被设计为促进医学培训自动化和人机交互的相关研究，并能够应用于工具检测、流程识别等多个领域，同时通过Zenodo公开提供，便于学术界使用。

Abstract: This data article presents a dataset of 11,884 labeled images documenting a simulated blood extraction (phlebotomy) procedure performed on a training arm. Images were extracted from high-definition videos recorded under controlled conditions and curated to reduce redundancy using Structural Similarity Index Measure (SSIM) filtering. An automated face-anonymization step was applied to all videos prior to frame selection. Each image contains polygon annotations for five medically relevant classes: syringe, rubber band, disinfectant wipe, gloves, and training arm. The annotations were exported in a segmentation format compatible with modern object detection frameworks (e.g., YOLOv8), ensuring broad usability. This dataset is partitioned into training (70%), validation (15%), and test (15%) subsets and is designed to advance research in medical training automation and human-object interaction. It enables multiple applications, including phlebotomy tool detection, procedural step recognition, workflow analysis, conformance checking, and the development of educational systems that provide structured feedback to medical trainees. The data and accompanying label files are publicly available on Zenodo.

</details>


### [61] [AGILE: Hand-Object Interaction Reconstruction from Video via Agentic Generation](https://arxiv.org/abs/2602.04672)
*Jin-Chuan Shi,Binhong Ye,Tao Liu,Junzhe He,Yangjinhui Xu,Xiaoyang Liu,Zeju Li,Hao Chen,Chunhua Shen*

Main category: cs.CV

TL;DR: AGILE是一种新颖的框架，通过生成模型从单目视频重建动态手部与物体的交互，无需依赖神经渲染和脆弱的结构从运动初始化，从而提高了几何准确性并增强鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 当前方法依赖于神经渲染和脆弱的结构从运动初始化，在遇到复杂遮挡和野外观测时表现不佳。AGILE提出了一种基于生成模型的框架，通过Vision-Language Model（VLM）指导生成模型来合成完整的、无接缝的物体网格，并采用鲁棒的锚点和跟踪策略来确保物体姿态的持久性。

Method: AGILE包含三个主要步骤：首先，使用VLM引导生成模型合成高保真纹理的完整网格，独立于视频遮挡。其次，利用生成资产和视频观察之间的视觉相似性，提出一个鲁棒的锚点与跟踪策略来初始化物体质态，而不依赖脆弱的结构从运动初始化。最后，通过接触感知优化整合语义、几何和交互稳定性约束以确保物理可行性。

Result: AGILE在HO3D、DexYCB和野外视频中的实验结果显示了高几何准确性，并在具有挑战性的序列中表现出色，优于现有基线。AGILE为模拟友好型资产生成提供了一种新的方法，这些资产经过现实到模拟的重定位验证了其在机器人应用中的有效性。

Conclusion: AGILE框架通过生成模型从单目视频中重建手部与物体的动态交互，实现了高几何准确性与鲁棒性的双重目标，有效地解决了现有方法的限制问题。

Abstract: Reconstructing dynamic hand-object interactions from monocular videos is critical for dexterous manipulation data collection and creating realistic digital twins for robotics and VR. However, current methods face two prohibitive barriers: (1) reliance on neural rendering often yields fragmented, non-simulation-ready geometries under heavy occlusion, and (2) dependence on brittle Structure-from-Motion (SfM) initialization leads to frequent failures on in-the-wild footage. To overcome these limitations, we introduce AGILE, a robust framework that shifts the paradigm from reconstruction to agentic generation for interaction learning. First, we employ an agentic pipeline where a Vision-Language Model (VLM) guides a generative model to synthesize a complete, watertight object mesh with high-fidelity texture, independent of video occlusions. Second, bypassing fragile SfM entirely, we propose a robust anchor-and-track strategy. We initialize the object pose at a single interaction onset frame using a foundation model and propagate it temporally by leveraging the strong visual similarity between our generated asset and video observations. Finally, a contact-aware optimization integrates semantic, geometric, and interaction stability constraints to enforce physical plausibility. Extensive experiments on HO3D, DexYCB, and in-the-wild videos reveal that AGILE outperforms baselines in global geometric accuracy while demonstrating exceptional robustness on challenging sequences where prior art frequently collapses. By prioritizing physical validity, our method produces simulation-ready assets validated via real-to-sim retargeting for robotic applications.

</details>


### [62] [Toward Reliable and Explainable Nail Disease Classification: Leveraging Adversarial Training and Grad-CAM Visualization](https://arxiv.org/abs/2602.04820)
*Farzia Hossain,Samanta Ghosh,Shahida Begum,B. M. Shahria Alam,Mohammad Tahmid Noor,Md Parvez Mia,Nishat Tasnim Niloy*

Main category: cs.CV

TL;DR: 本文提出了一种基于机器学习的指甲疾病分类模型，使用公开数据集中的3,835张图片，经过InceptionV3和DenseNet201等模型训练，InceptionV3的准确率达95.57%，并采用了对抗训练和SHAP方法来增强模型的决策透明度。


<details>
  <summary>Details</summary>
Motivation: 由于指甲疾病在不同年龄段都可能出现，并且常常在症状严重时才被注意到，因此早期诊断和准确识别这些疾病变得尤为重要。现有的视觉差异使得诊断非常具挑战性，因此需要一种自动化的方案。

Method: 本文利用一个包含3,835张图像的公开数据集，所有图像被统一调整为224x224像素大小，然后通过训练现有的四个优秀的CNN模型（InceptionV3, DenseNet201, EfficientNetV2和ResNet50）来分类指甲疾病。此外，通过对抗训练提高了模型在复杂或噪声图像上的鲁棒性，并使用SHAP方法来解释模型预测的重要特征。

Result: 在测试中，InceptionV3的表现最佳，准确率为95.57%，其次是DenseNet201，准确率为94.79%。

Conclusion: 该系统为医生提供了一种快速和准确的辅助工具，用于诊断指甲疾病。

Abstract: Human nail diseases are gradually observed over all age groups, especially among older individuals, often going ignored until they become severe. Early detection and accurate diagnosis of such conditions are important because they sometimes reveal our body's health problems. But it is challenging due to the inferred visual differences between disease types. This paper presents a machine learning-based model for automated classification of nail diseases based on a publicly available dataset, which contains 3,835 images scaling six categories. In 224x224 pixels, all images were resized to ensure consistency. To evaluate performance, four well-known CNN models-InceptionV3, DenseNet201, EfficientNetV2, and ResNet50 were trained and analyzed. Among these, InceptionV3 outperformed the others with an accuracy of 95.57%, while DenseNet201 came next with 94.79%. To make the model stronger and less likely to make mistakes on tricky or noisy images, we used adversarial training. To help understand how the model makes decisions, we used SHAP to highlight important features in the predictions. This system could be a helpful support for doctors, making nail disease diagnosis more accurate and faster.

</details>


### [63] [Annotation Free Spacecraft Detection and Segmentation using Vision Language Models](https://arxiv.org/abs/2602.04699)
*Samet Hicsonmez,Jose Sosa,Dan Pineau,Inder Pal Singh,Arunkumar Rathinam,Abd El Rahman Shabayek,Djamila Aouada*

Main category: cs.CV

TL;DR: 提出了一种无需标注的航天目标检测与分割管道，利用预训练的VLM自动生成小部分未标注真实数据的伪标签，通过教师-学生标签蒸馏框架训练轻量级模型，实验结果表明在SPARK-2024、SPEED+和TANGO数据集上的平均精度提高显著。


<details>
  <summary>Details</summary>
Motivation: 开发无需大量人工标注的方法来检测和分割航天器和轨道目标在空间领域具有重要意义，由于视线低、光照变化和背景融合等因素，准确的手动注释尤其具有挑战性。

Method: 使用预训练的VLM自动生成部分未标注数据的伪标签，在教师-学生标签蒸馏框架下训练轻量级模型。

Result: 实验证明，该方法在分割任务上的平均精度提高了10个百分点。

Conclusion: 无需注释的方法可以显著提高空间目标识别和分割的效果。

Abstract: Vision Language Models (VLMs) have demonstrated remarkable performance in open-world zero-shot visual recognition. However, their potential in space-related applications remains largely unexplored. In the space domain, accurate manual annotation is particularly challenging due to factors such as low visibility, illumination variations, and object blending with planetary backgrounds. Developing methods that can detect and segment spacecraft and orbital targets without requiring extensive manual labeling is therefore of critical importance. In this work, we propose an annotation-free detection and segmentation pipeline for space targets using VLMs. Our approach begins by automatically generating pseudo-labels for a small subset of unlabeled real data with a pre-trained VLM. These pseudo-labels are then leveraged in a teacher-student label distillation framework to train lightweight models. Despite the inherent noise in the pseudo-labels, the distillation process leads to substantial performance gains over direct zero-shot VLM inference. Experimental evaluations on the SPARK-2024, SPEED+, and TANGO datasets on segmentation tasks demonstrate consistent improvements in average precision (AP) by up to 10 points. Code and models are available at https://github.com/giddyyupp/annotation-free-spacecraft-segmentation.

</details>


### [64] [How to rewrite the stars: Mapping your orchard over time through constellations of fruits](https://arxiv.org/abs/2602.04722)
*Gonçalo P. Matos,Carlos Santiago,João P. Costeira,Ricardo L. Saldanha,Ernesto M. Morgado*

Main category: cs.CV

TL;DR: 该研究提出了一种新的方法，通过匹配3D质心星座来追踪不同视频中同一颗果实的成长，这种方法可以应对非刚性、遮挡和缺乏视觉特征等挑战，最终实现农业机器人的自主导航和精准采摘。


<details>
  <summary>Details</summary>
Motivation: 传统的人工测量果实大小耗时且不具可扩展性，计算机视觉技术可用于自动化农业操作，但关键问题是如何在不同时间点的视频之间匹配同一只果实。

Method: 研究基于3D质心星座提出了一种新的匹配方法，并引入了一种适用于稀疏3D点云的描述符，该方法不需要摄像机从已知位置启动，也不需要有足够的视觉特征来匹配。他们匹配的是由3D质心构成的星座而非单个果实。

Result: 实验证明，该方法能够成功地跨视频和时间点匹配果实，还能构建果园地图，并据此确定相机的6自由度（6DoF）姿态，从而为农业机器人在果园中的自主导航和精准采摘提供技术支持。

Conclusion: 这为持久的果实生长追踪和精准农业操作提供了一种有效的方法。

Abstract: Following crop growth through the vegetative cycle allows farmers to predict fruit setting and yield in early stages, but it is a laborious and non-scalable task if performed by a human who has to manually measure fruit sizes with a caliper or dendrometers. In recent years, computer vision has been used to automate several tasks in precision agriculture, such as detecting and counting fruits, and estimating their size. However, the fundamental problem of matching the exact same fruits from one video, collected on a given date, to the fruits visible in another video, collected on a later date, which is needed to track fruits' growth through time, remains to be solved. Few attempts were made, but they either assume that the camera always starts from the same known position and that there are sufficiently distinct features to match, or they used other sources of data like GPS. Here we propose a new paradigm to tackle this problem, based on constellations of 3D centroids, and introduce a descriptor for very sparse 3D point clouds that can be used to match fruits across videos. Matching constellations instead of individual fruits is key to deal with non-rigidity, occlusions and challenging imagery with few distinct visual features to track. The results show that the proposed method can be successfully used to match fruits across videos and through time, and also to build an orchard map and later use it to locate the camera pose in 6DoF, thus providing a method for autonomous navigation of robots in the orchard and for selective fruit picking, for example.

</details>


### [65] [Mitigating Long-Tail Bias via Prompt-Controlled Diffusion Augmentation](https://arxiv.org/abs/2602.04749)
*Buddhi Wijenayake,Nichula Wasalathilake,Roshan Godaliyadda,Vijitha Herath,Parakrama Ekanayake,Vishal M. Patel*

Main category: cs.CV

TL;DR: 该研究引入了一种受提示控制的扩散增强框架，用于高分辨率遥感图像的语义分割，通过生成具有显式领域和语义组成控制的配对标签-图像样本，解决了数据集中存在领域划分和长尾像素不平衡的问题，实验结果显示这种机制能够有效减少分割模型对长尾样本的倚重，并提高了对城市和农村场景的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 遥感图像的高分辨率语义分割对于城市制图和土地覆盖监测至关重要，然而训练数据中普遍存在着严重的长尾像素不平衡问题。例如，LoveDA数据集中明确划分出的城市/农村分割使得类频分布统计不一致。为了解决这一问题，该研究提出了一种基于提示控制的扩散增强框架。

Method: 该方法包含两个阶段。Stage A 使用领域意识的、基于掩码比率的离散扩散模型生成满足用户指定类别比率目标的布局，同时尊重学习到的共现结构。Stage B 则采用Stable Diffusion结合ControlNet指导将布局转换为高保真度、领域一致的图像。最终，通过混合控制比例和领域的合成样本与真实数据，提高了多个分割骨干模型的表现。

Result: 使用所提出的双阶段方法合成的配对标签-图像样本与真实数据混合后，多个分割模型在多种分割评价指标上均取得了提升，尤其是对少数类别有显著改善，展示了对城市和农村场景具有更好的泛化能力。

Conclusion: 该研究提出的方法是一种实用的机制，可以有效缓解遥感分割中的长尾偏差，并通过提供更多样、平衡的训练样本，提高了分割模型的泛化表现。

Abstract: Semantic segmentation of high-resolution remote-sensing imagery is critical for urban mapping and land-cover monitoring, yet training data typically exhibits severe long-tailed pixel imbalance. In the dataset LoveDA, this challenge is compounded by an explicit Urban/Rural split with distinct appearance and inconsistent class-frequency statistics across domains. We present a prompt-controlled diffusion augmentation framework that synthesizes paired label--image samples with explicit control of both domain and semantic composition. Stage~A uses a domain-aware, masked ratio-conditioned discrete diffusion model to generate layouts that satisfy user-specified class-ratio targets while respecting learned co-occurrence structure. Stage~B translates layouts into photorealistic, domain-consistent images using Stable Diffusion with ControlNet guidance. Mixing the resulting ratio and domain-controlled synthetic pairs with real data yields consistent improvements across multiple segmentation backbones, with gains concentrated on minority classes and improved Urban and Rural generalization, demonstrating controllable augmentation as a practical mechanism to mitigate long-tail bias in remote-sensing segmentation. Source codes, pretrained models, and synthetic datasets are available at \href{https://github.com/Buddhi19/SyntheticGen.git}{Github}

</details>


### [66] [Light Forcing: Accelerating Autoregressive Video Diffusion via Sparse Attention](https://arxiv.org/abs/2602.04789)
*Chengtao Lv,Yumeng Shi,Yushi Huang,Ruihao Gong,Shen Ren,Wenya Wang*

Main category: cs.CV

TL;DR: 该研究提出了一种名为Light Forcing的稀疏注意力机制，针对AR视频生成模型，通过Chunk-Aware Growth机制和Hierarchical Sparse Attention提高了生成质量和效率，兼容FP8量化和LightVAE后，加速比达到了2.3倍，提供19.7 FPS的实时性能。


<details>
  <summary>Details</summary>
Motivation: 现有的AR视频生成模型尽管在视觉质量和交互性上有所提升，但基于注意力的复杂度仍然限制了其实现效率。因此，研究者们寻找了一种稀疏注意力方法，旨在解决存在性能打折扣的问题。

Method: 该方法引入了一种Chunk-Aware Growth机制以量化评估每个片段的重要性，决定其稀疏性分配。同时，研究提出了Hierarchical Sparse Attention机制，这是一种自上而下的稀疏注意力方式，能捕捉粗略到精细的历史和局部上下文信息。通过在帧和块两级进行掩码选择，此方法可以灵活处理各种注意力模式。

Result: 实验结果表明，相较于现有的稀疏注意力，Light Forcing方法在质量（例如，在VBench上的得分为84.5）和效率（端到端加速大约1.2至1.3倍）方面均有卓越表现。此外，结合FP8量化和LightVAE后，该方法实现了2.3倍的加速和19.7 FPS的实时性能。

Conclusion: 研究提出了一种适合AR视频生成模型的稀疏注意力机制（Light Forcing），并通过实验验证了其在生成质量与效率上的提升。

Abstract: Advanced autoregressive (AR) video generation models have improved visual fidelity and interactivity, but the quadratic complexity of attention remains a primary bottleneck for efficient deployment. While existing sparse attention solutions have shown promise on bidirectional models, we identify that applying these solutions to AR models leads to considerable performance degradation for two reasons: isolated consideration of chunk generation and insufficient utilization of past informative context. Motivated by these observations, we propose \textsc{Light Forcing}, the \textit{first} sparse attention solution tailored for AR video generation models. It incorporates a \textit{Chunk-Aware Growth} mechanism to quantitatively estimate the contribution of each chunk, which determines their sparsity allocation. This progressive sparsity increase strategy enables the current chunk to inherit prior knowledge in earlier chunks during generation. Additionally, we introduce a \textit{Hierarchical Sparse Attention} to capture informative historical and local context in a coarse-to-fine manner. Such two-level mask selection strategy (\ie, frame and block level) can adaptively handle diverse attention patterns. Extensive experiments demonstrate that our method outperforms existing sparse attention in quality (\eg, 84.5 on VBench) and efficiency (\eg, $1.2{\sim}1.3\times$ end-to-end speedup). Combined with FP8 quantization and LightVAE, \textsc{Light Forcing} further achieves a $2.3\times$ speedup and 19.7\,FPS on an RTX~5090 GPU. Code will be released at \href{https://github.com/chengtao-lv/LightForcing}{https://github.com/chengtao-lv/LightForcing}.

</details>


### [67] [VISTA-Bench: Do Vision-Language Models Really Understand Visualized Text as Well as Pure Text?](https://arxiv.org/abs/2602.04802)
*Qing'an Liu,Juntong Feng,Yuhao Wang,Xinzhe Han,Yujie Cheng,Yue Zhu,Haiwen Diao,Yunzhi Zhuge,Huchuan Lu*

Main category: cs.CV

TL;DR: VISTA-Bench 引入了一个新的基准测试，用于评估视觉语言模型在处理与文本和图像中嵌入的文本信息相关的查询时的性能差异。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型基准测试主要关注纯文本查询，而在现实场景中，语言也经常以图像中的可视化文本形式出现。VISTA-Bench 的目标是评估和解决模型在这种情况下性能下降的问题。

Method: VISTA-Bench 设计了一种系统性的基准测试，涵盖多模态感知、推理和单一模态理解领域。它在受控渲染条件下对比纯文本和可视化文本问题的问题，通过大量评估超过20种代表性模型来揭示模式差异。

Result: 研究发现，表现良好的纯文本模型在等效的语义内容呈现为可视化文本时会出现显著下降。此外，感知难度的增加进一步扩大了这种差距，表明模型对渲染变化的敏感性，尽管语义未变。

Conclusion: VISTA-Bench 提供了一个有原则的评估框架，以诊断这些限制，并引导向跨标记文本和像素更统一的语言表示的进展。

Abstract: Vision-Language Models (VLMs) have achieved impressive performance in cross-modal understanding across textual and visual inputs, yet existing benchmarks predominantly focus on pure-text queries. In real-world scenarios, language also frequently appears as visualized text embedded in images, raising the question of whether current VLMs handle such input requests comparably. We introduce VISTA-Bench, a systematic benchmark from multimodal perception, reasoning, to unimodal understanding domains. It evaluates visualized text understanding by contrasting pure-text and visualized-text questions under controlled rendering conditions. Extensive evaluation of over 20 representative VLMs reveals a pronounced modality gap: models that perform well on pure-text queries often degrade substantially when equivalent semantic content is presented as visualized text. This gap is further amplified by increased perceptual difficulty, highlighting sensitivity to rendering variations despite unchanged semantics. Overall, VISTA-Bench provides a principled evaluation framework to diagnose this limitation and to guide progress toward more unified language representations across tokenized text and pixels. The source dataset is available at https://github.com/QingAnLiu/VISTA-Bench.

</details>


### [68] [X2HDR: HDR Image Generation in a Perceptually Uniform Space](https://arxiv.org/abs/2602.04814)
*Ronghuan Wu,Wanchao Su,Kede Ma,Jing Liao,Rafał K. Mantiuk*

Main category: cs.CV

TL;DR: 本文展示了如何将预训练的扩散模型轻松适应HDR生成，而无需从零开始重新训练。通过将HDR输入转换为感知均匀编码，例如PU21，作者提出了一个有效的适应策略，该策略冻结VAE并仅针对去噪器进行细调。这种方法支持从文本生成HDR以及单张RAW图像到HDR的重建。


<details>
  <summary>Details</summary>
Motivation: 现有的图像生成器通常只能生成LDR图像，因为缺乏大规模的HDR训练数据。HDR图像使用线性RGB表示，与sRGB编码的LDR图像的强度和颜色统计有很大不同。

Method: 通过将HDR输入转化为感知均匀编码（如PU21），本文提出了一个高效的适应方法，该方法冻结VAE并仅针对去噪器进行低秩适应。

Result: 实验表明，这种方法在感知保真度、文本与图像对齐以及有效动态范围方面优于之前的技术。

Conclusion: 该作品提出的方法表明，可以适配预训练扩散模型以生成HDR图像，且无需从头开始训练，并且效果良好。

Abstract: High-dynamic-range (HDR) formats and displays are becoming increasingly prevalent, yet state-of-the-art image generators (e.g., Stable Diffusion and FLUX) typically remain limited to low-dynamic-range (LDR) output due to the lack of large-scale HDR training data. In this work, we show that existing pretrained diffusion models can be easily adapted to HDR generation without retraining from scratch. A key challenge is that HDR images are natively represented in linear RGB, whose intensity and color statistics differ substantially from those of sRGB-encoded LDR images. This gap, however, can be effectively bridged by converting HDR inputs into perceptually uniform encodings (e.g., using PU21 or PQ). Empirically, we find that LDR-pretrained variational autoencoders (VAEs) reconstruct PU21-encoded HDR inputs with fidelity comparable to LDR data, whereas linear RGB inputs cause severe degradations. Motivated by this finding, we describe an efficient adaptation strategy that freezes the VAE and finetunes only the denoiser via low-rank adaptation in a perceptually uniform space. This results in a unified computational method that supports both text-to-HDR synthesis and single-image RAW-to-HDR reconstruction. Experiments demonstrate that our perceptually encoded adaptation consistently improves perceptual fidelity, text-image alignment, and effective dynamic range, relative to previous techniques.

</details>


### [69] [XtraLight-MedMamba for Classification of Neoplastic Tubular Adenomas](https://arxiv.org/abs/2602.04819)
*Aqsa Sultana,Rayan Afsar,Ahmed Rahu,Surendra P. Singh,Brian Shula,Brandon Combs,Derrick Forchetti,Vijayan K. Asari*

Main category: cs.CV

TL;DR: 本研究提出了一种超轻量级的基于状态空间的深度学习框架XtraLight-MedMamba，用于从整个组织切片图像中对具有低级别管状腺瘤的患者进行分类，展现出高效的特征提取和参数减少能力。


<details>
  <summary>Details</summary>
Motivation: 由于现有技术在低级别异型增生评估中的局限性（主观的组织病理学解释），本研究旨在利用数字病理学和深度学习技术，开发一种新的方法来提高此类病变的鉴别能力。

Method: 采用了一种结合了ConvNext浅特征提取器与平行的视觉Mamba模块的架构，它能够高效建模长、短距离依赖性和图像泛化能力。还引入了多尺度特征融合的空域与通道注意桥模块（SCAB）以及固定非负正交分类器（FNOClassifier）。

Result: XtraLight-MedMamba在专门收集的低级别管状腺瘤病人的数据集上进行了评估，达到了97.18%的准确性与0.9767的F1分数，参数量约为32,000，优于基于变换器和传统Mamba架构的方法。

Conclusion: 新架构提供了在保持高性能的同时大幅减少参数量的潜力，为精准分层结直肠癌癌前病变提供了新的机会。。

Abstract: Accurate risk stratification of precancerous polyps during routine colonoscopy screenings is essential for lowering the risk of developing colorectal cancer (CRC). However, assessment of low-grade dysplasia remains limited by subjective histopathologic interpretation. Advancements in digital pathology and deep learning provide new opportunities to identify subtle and fine morphologic patterns associated with malignant progression that may be imperceptible to the human eye. In this work, we propose XtraLight-MedMamba, an ultra-lightweight state-space-based deep learning framework for classifying neoplastic tubular adenomas from whole-slide images (WSIs). The architecture is a blend of ConvNext based shallow feature extractor with parallel vision mamba to efficiently model both long- and short-range dependencies and image generalization. An integration of Spatial and Channel Attention Bridge (SCAB) module enhances multiscale feature extraction, while Fixed Non-Negative Orthogonal Classifier (FNOClassifier) enables substantial parameter reduction and improved generalization. The model was evaluated on a curated dataset acquired from patients with low-grade tubular adenomas, stratified into case and control cohorts based on subsequent CRC development. XtraLight-MedMamba achieved an accuracy of 97.18% and an F1-score of 0.9767 using approximately 32,000 parameters, outperforming transformer-based and conventional Mamba architectures with significantly higher model complexity.

</details>


### [70] [LitS: A novel Neighborhood Descriptor for Point Clouds](https://arxiv.org/abs/2602.04838)
*Jonatan B. Bastos,Francisco F. Rivera,Oscar G. Lorenzo,David L. Vilariño,José C. Cabaleiro,Alberto M. Esmorís,Tomás F. Pena*

Main category: cs.CV

TL;DR:  LitS是一种新颖的用于2D和3D点云的邻域描述符，提供了方向上点周围邻居的信息，通过分析LitS的变化可以理解点云的全局结构。


<details>
  <summary>Details</summary>
Motivation: 随着3D扫描技术的发展，点云已成为3D空间数据表示的基础。然而，如何准确地描述点云的局部几何结构仍然依赖于有效的邻域描述符。为了应对这一挑战，该研究提出了LitS来增强点云分析的精度。

Method: LitS 是一系列在单位圆上的分段常数函数，通过构建这些函数，可以在指定方向上获取该点周围的邻居数量。LitS 包括两种版本（常规和累积）并且有参数调节，以适应不同的点云类型和密度。

Result: LitS 描述符能够高效地捕获局部点排布的细微变化，对常见的点云数据问题如密度变化和噪声具有一定的鲁棒性。

Conclusion: 本文提出的新颖邻域描述符LitS在点云局部结构的描述上具有强大的潜力，并为点云相关应用提供了有力的支持。

Abstract: With the advancement of 3D scanning technologies, point clouds have become fundamental for representing 3D spatial data, with applications that span across various scientific and technological fields. Practical analysis of this data depends crucially on available neighborhood descriptors to accurately characterize the local geometries of the point cloud. This paper introduces LitS, a novel neighborhood descriptor for 2D and 3D point clouds. LitS are piecewise constant functions on the unit circle that allow points to keep track of their surroundings. Each element in LitS' domain represents a direction with respect to a local reference system. Once constructed, evaluating LitS at any given direction gives us information about the number of neighbors in a cone-like region centered around that same direction. Thus, LitS conveys a lot of information about the local neighborhood of a point, which can be leveraged to gain global structural understanding by analyzing how LitS changes between close points. In addition, LitS comes in two versions ('regular' and 'cumulative') and has two parameters, allowing them to adapt to various contexts and types of point clouds. Overall, they are a versatile neighborhood descriptor, capable of capturing the nuances of local point arrangements and resilient to common point cloud data issues such as variable density and noise.

</details>


### [71] [When LLaVA Meets Objects: Token Composition for Vision-Language-Models](https://arxiv.org/abs/2602.04864)
*Soumya Jahagirdar,Walid Bousselham,Anna Kukleva,Hilde Kuehne*

Main category: cs.CV

TL;DR: Mask-LLaVA 通过结合基于掩码的对象表示、全局令牌和局部块令牌，提供了一种紧凑但信息丰富的视觉表示方法，以适应不同的测试时间和无需重新训练模型。


<details>
  <summary>Details</summary>
Motivation: 现有自动回归视觉语言模型依赖大量的视觉标记来表示图像，导致推理时需要更多的计算资源，为此提出了一种框架，利用不同层级的视觉特征来创建一种紧凑但信息丰富的视觉表示。

Method: 结合基于掩码的对象表示、全局令牌以及局部块令牌等三种视觉表示形式，并在训练时使用所有标记，但在测试时可以灵活地减少基于掩码的标记数量，实现了在低视觉标记条件下保持性能。

Result: 该方法在多种标准基准上表现出色，与当前高效的标记方法竞争，且仅使用原始LLaVA基线少部分的视觉标记。

Conclusion: 实验表明，结合多层特征不仅促进了少数标记条件下的高效学习，还在测试时允许动态选择标记以实现良好表现。

Abstract: Current autoregressive Vision Language Models (VLMs) usually rely on a large number of visual tokens to represent images, resulting in a need for more compute especially at inference time. To address this problem, we propose Mask-LLaVA, a framework that leverages different levels of visual features to create a compact yet information-rich visual representation for autoregressive VLMs. Namely, we combine mask-based object representations together with global tokens and local patch tokens. While all tokens are used during training, it shows that the resulting model can flexibly drop especially the number of mask-based object-tokens at test time, allowing to adapt the number of tokens during inference without the need to retrain the model and without a significant drop in performance. We evaluate the proposed approach on a suite of standard benchmarks showing results competitive to current token efficient methods and comparable to the original LLaVA baseline using only a fraction of visual tokens. Our analysis demonstrates that combining multi-level features enables efficient learning with fewer tokens while allowing dynamic token selection at test time for good performance.

</details>


### [72] [Laminating Representation Autoencoders for Efficient Diffusion](https://arxiv.org/abs/2602.04873)
*Ramón Calvo-González,François Fleuret*

Main category: cs.CV

TL;DR: FlatDINO通过将密集补丁网格压缩为一维序列的32连续标记，显著减少了计算开销，提高了生成图像的质量。


<details>
  <summary>Details</summary>
Motivation: 现有工作表明，通过直接操作SSL补丁特征而不是像素空间潜变量，扩散模型可以产生高质量的图像。但密集补丁网格中存在大量冗余信息，使扩散过程变得昂贵。FlatDINO旨在通过减少冗余信息来降低计算成本。

Method: FlatDINO采用变分自编码器将DINOv2补丁特征压缩为一维序列的32连续标记，从而减少序列长度和总维度48倍。

Result: 在ImageNet 256x256上，使用FlatDINO潜变量进行训练的DiT-XL在去除分类器引导的情况下获得了gFID 1.80。相比扩散模型使用未压缩的DINOv2特征，FlatDINO所需的FLOPs减少了8倍，并且每个训练步骤最多节省4.5倍的FLOPs。

Conclusion: FlatDINO展示了显著的计算成本降低潜力，同时保持了生成图像的质量，是一个仍在开发中的初步成果。

Abstract: Recent work has shown that diffusion models can generate high-quality images by operating directly on SSL patch features rather than pixel-space latents. However, the dense patch grids from encoders like DINOv2 contain significant redundancy, making diffusion needlessly expensive. We introduce FlatDINO, a variational autoencoder that compresses this representation into a one-dimensional sequence of just 32 continuous tokens -an 8x reduction in sequence length and 48x compression in total dimensionality. On ImageNet 256x256, a DiT-XL trained on FlatDINO latents achieves a gFID of 1.80 with classifier-free guidance while requiring 8x fewer FLOPs per forward pass and up to 4.5x fewer FLOPs per training step compared to diffusion on uncompressed DINOv2 features. These are preliminary results and this work is in progress.

</details>


### [73] [PerpetualWonder: Long-Horizon Action-Conditioned 4D Scene Generation](https://arxiv.org/abs/2602.04876)
*Jiahao Zhan,Zizhang Li,Hong-Xing Yu,Jiajun Wu*

Main category: cs.CV

TL;DR: PerpetualWonder 是一种新型生成式模拟器，能够从单张图像生成长期、行为条件下的 4D 场景，并保持物理合理性与视觉一致性。


<details>
  <summary>Details</summary>
Motivation: 当前的模拟器模型存在物理状态与视觉表示脱节的问题，阻碍了后续交互中的生成优化。PerpetualWonder 通过引入闭环系统解决了这一问题，实现了物理状态与视觉元素的双向链接，确保生成优化可以同时纠正动态和外观。

Method: PerpetualWonder 利用一种新颖的联合表示，建立了物理状态和视觉原语之间的双向链接。此外，系统还采用了稳健的更新机制，能够从多个视角收集监督信号，解决优化过程中的歧义性。

Result: 实验结果表明，PerpetualWonder 能够从单张图像模拟具有复杂多步骤交互的长期行为，并保持物理合理性和视觉一致性。

Conclusion: PerpetualWonder 是一个创新的解决方案，解决了长期行为模拟中的关键挑战，为未来的生成式模拟器奠定了基础。

Abstract: We introduce PerpetualWonder, a hybrid generative simulator that enables long-horizon, action-conditioned 4D scene generation from a single image. Current works fail at this task because their physical state is decoupled from their visual representation, which prevents generative refinements to update the underlying physics for subsequent interactions. PerpetualWonder solves this by introducing the first true closed-loop system. It features a novel unified representation that creates a bidirectional link between the physical state and visual primitives, allowing generative refinements to correct both the dynamics and appearance. It also introduces a robust update mechanism that gathers supervision from multiple viewpoints to resolve optimization ambiguity. Experiments demonstrate that from a single image, PerpetualWonder can successfully simulate complex, multi-step interactions from long-horizon actions, maintaining physical plausibility and visual consistency.

</details>


### [74] [CoWTracker: Tracking by Warping instead of Correlation](https://arxiv.org/abs/2602.04877)
*Zihang Lai,Eldar Insafutdinov,Edgar Sucar,Andrea Vedaldi*

Main category: cs.CV

TL;DR: 本文提出了一种新的密集点追踪方法，该方法通过将目标帧的特征迁移到查询帧，而不计算特征关联，从而避免了二次空间复杂性，实现了高效和可扩展的点追踪。该方法在标准基准测试中表现出色，包括TAP-Vid-DAVIS、TAP-Vid-Kinetics和Robo-TAP。此外，该模型在光学流动方面也表现出色，有时在Sintel、KITTI和Spring基准测试中优于专门的方法。


<details>
  <summary>Details</summary>
Motivation: 现有的跟踪方法依赖于成本体积来匹配帧之间的特征，这会导致二次空间复杂性，限制了算法的可扩展性和效率。为了克服这一问题，本文提出了一种新的密集点追踪方法，该方法不再使用成本体积，而是采用变形来实现高效的密集点追踪。

Method: 本文提出的方法通过迭代地将目标帧的特征迁移到查询帧，基于当前的估计来进行特征变形，从而规避了特征关联的计算。同时，该方法采用了一种变压器架构，可以对所有轨迹进行联合的空间-时间推理，从而建立长距离对应关系。这种方法非常简单，并且在标准密集点追踪基准测试中达到了最先进的性能。

Result: 本文提出的方法在标准的密集点追踪基准测试，如TAP-Vid-DAVIS、TAP-Vid-Kinetics 和Robo-TAP 上取得了优异的结果。此外，该方法在光学流动基准测试，如Sintel、KITTI和Spring 中也表现优异，有时甚至优于专门优化的方法。

Conclusion: 本文提出的方法证明了基于变形的架构可以统一密集点追踪和光学流动估计。这种方法在基准测试上达到了最先进的性能，证明了其有效性和鲁棒性。

Abstract: Dense point tracking is a fundamental problem in computer vision, with applications ranging from video analysis to robotic manipulation. State-of-the-art trackers typically rely on cost volumes to match features across frames, but this approach incurs quadratic complexity in spatial resolution, limiting scalability and efficiency. In this paper, we propose \method, a novel dense point tracker that eschews cost volumes in favor of warping. Inspired by recent advances in optical flow, our approach iteratively refines track estimates by warping features from the target frame to the query frame based on the current estimate. Combined with a transformer architecture that performs joint spatiotemporal reasoning across all tracks, our design establishes long-range correspondences without computing feature correlations. Our model is simple and achieves state-of-the-art performance on standard dense point tracking benchmarks, including TAP-Vid-DAVIS, TAP-Vid-Kinetics, and Robo-TAP. Remarkably, the model also excels at optical flow, sometimes outperforming specialized methods on the Sintel, KITTI, and Spring benchmarks. These results suggest that warping-based architectures can unify dense point tracking and optical flow estimation.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [75] [Linguistic Blind Spots in Clinical Decision Extraction](https://arxiv.org/abs/2602.03942)
*Mohamed Elgaar,Hadi Amiri*

Main category: cs.CL

TL;DR: 研究了临床决策的语言特征如何随决策类别变化，并分析了这些差异如何影响决策提取。使用带有决策类别的MedDec出院总结，计算每个决策跨度的七个语言指数，发现不同类型的决策有特定的语言特征，且在精确匹配下召回率较低，表明边界容忍的评估和提取策略对于临床决策至关重要。


<details>
  <summary>Details</summary>
Motivation: 临床决策支持和患者面向的护理总结的关键步骤是提取医疗决策，文中研究不同类型的临床决策在语言特征上的差异以及这些差异如何影响决策提取。

Method: 使用MedDec出院总结注解带决策类别的数据集进行研究，计算每个决策跨度的七个语言指数，分析标准变压器模型在验证集上的精确匹配召回率和放宽匹配条件下的召回率。

Result: 研究发现不同类型的临床决策具有类别特定的语言特征：药物相关的和问题定义的决定是实体密集和电报式的，而建议和预防措施包含更多的叙事内容，具有较高的停用词和代词比例，以及更多的模糊和否定线索。精确匹配下的召回率为48%，在停用词比例最低到最高的区间内，召回率从58%下降到24%，包含模糊或否定线索的段落更不可能被恢复。放宽匹配条件下的召回率为71%，表明许多错误是跨度边界的分歧而非完全错过。

Conclusion: 叙事型段落——在建议和预防措施决策中较为常见——在精确匹配下是固定的盲点，表明下游系统应当引入边界容忍的评估和提取策略以应对临床决策。

Abstract: Extracting medical decisions from clinical notes is a key step for clinical decision support and patient-facing care summaries. We study how the linguistic characteristics of clinical decisions vary across decision categories and whether these differences explain extraction failures. Using MedDec discharge summaries annotated with decision categories from the Decision Identification and Classification Taxonomy for Use in Medicine (DICTUM), we compute seven linguistic indices for each decision span and analyze span-level extraction recall of a standard transformer model. We find clear category-specific signatures: drug-related and problem-defining decisions are entity-dense and telegraphic, whereas advice and precaution decisions contain more narrative, with higher stopword and pronoun proportions and more frequent hedging and negation cues. On the validation split, exact-match recall is 48%, with large gaps across linguistic strata: recall drops from 58% to 24% from the lowest to highest stopword-proportion bins, and spans containing hedging or negation cues are less likely to be recovered. Under a relaxed overlap-based match criterion, recall increases to 71%, indicating that many errors are span boundary disagreements rather than complete misses. Overall, narrative-style spans--common in advice and precaution decisions--are a consistent blind spot under exact matching, suggesting that downstream systems should incorporate boundary-tolerant evaluation and extraction strategies for clinical decisions.

</details>


### [76] [Automatic Classification of Pedagogical Materials against CS Curriculum Guidelines](https://arxiv.org/abs/2602.03962)
*Erik Saule,Kalpathi Subramanian,Razvan Bunescu*

Main category: cs.CL

TL;DR: 本研究尝试使用自然语言处理技术（包括传统工具和大型语言模型）来自动分类计算机科学课程中的教学材料，以加速评估过程。


<details>
  <summary>Details</summary>
Motivation: 目前，使用详细标准来评估计算机科学项目的课程覆盖率耗时且费力，因此需要开发一种自动化的评估方法。

Method: 我们探索了使用自然语言处理技术，包括传统工具（如解析、标记和嵌入）和大型语言模型来自动分类教学材料。

Result: 研究表明，这些技术能够在一定程度上自动有效地分类文档。

Conclusion: 该研究证明了利用自然语言处理技术加速计算机科学课程评估过程的可能性。

Abstract: Professional societies often publish curriculum guidelines to help programs align their content to international standards. In Computer Science, the primary standard is published by ACM and IEEE and provide detailed guidelines for what should be and could be included in a Computer Science program.
  While very helpful, it remains difficult for program administrators to assess how much of the guidelines is being covered by a CS program. This is in particular due to the extensiveness of the guidelines, containing thousands of individual items. As such, it is time consuming and cognitively demanding to audit every course to confidently mark everything that is actually being covered. Our preliminary work indicated that it takes about a day of work per course.
  In this work, we propose using Natural Language Processing techniques to accelerate the process. We explore two kinds of techniques, the first relying on traditional tools for parsing, tagging, and embeddings, while the second leverages the power of Large Language Models. We evaluate the application of these techniques to classify a corpus of pedagogical materials and show that we can meaningfully classify documents automatically.

</details>


### [77] [Likelihood-Based Reward Designs for General LLM Reasoning](https://arxiv.org/abs/2602.03979)
*Ariel Kwiatkowski,Natasha Butt,Ismail Labiad,Julia Kempe,Yann Ollivier*

Main category: cs.CL

TL;DR: 研究发现，使用参考答案的对数概率作为奖励函数，在链式思考学习中能良好地适用于各种场景，而不依赖于特定的验证器，并在特定情况下优于二进制奖励。


<details>
  <summary>Details</summary>
Motivation: 细究大规模语言模型在推理基准上的强化学习调优，因为需要为每个基准设计特殊的二元奖励函数，这带来了设计奖励和二元奖励可能具有稀疏性的挑战。

Method: 调查了从数据中参考答案的生成概率或对数概率派生的奖励。实验比较了基于似然性的奖励与标准基准，并在标准数学推理基准和没有外部验证者的长形式答案中测试了性能。

Result: 实验证明，使用参考答案的对数概率作为奖励，除了在某些场景下与标准二进制奖励具有相似或更好的成功率外，在非验证性场景中与直接策略调整相当，并能显著降低困惑度。

Conclusion: 总之，使用参考答案的对数概率作为奖励弥补了短的验证性问答和长的非验证性问答设置之间的差距，提出了一种可行的方法进行链式思考的调优。

Abstract: Fine-tuning large language models (LLMs) on reasoning benchmarks via reinforcement learning requires a specific reward function, often binary, for each benchmark. This comes with two potential limitations: the need to design the reward, and the potentially sparse nature of binary rewards. Here, we systematically investigate rewards derived from the probability or log-probability of emitting the reference answer (or any other prompt continuation present in the data), which have the advantage of not relying on specific verifiers and being available at scale. Several recent works have advocated for the use of similar rewards (e.g., VeriFree, JEPO, RLPR, NOVER). We systematically compare variants of likelihood-based rewards with standard baselines, testing performance both on standard mathematical reasoning benchmarks, and on long-form answers where no external verifier is available. We find that using the log-probability of the reference answer as the reward for chain-of-thought (CoT) learning is the only option that performs well in all setups. This reward is also consistent with the next-token log-likelihood loss used during pretraining. In verifiable settings, log-probability rewards bring comparable or better success rates than reinforcing with standard binary rewards, and yield much better perplexity. In non-verifiable settings, they perform on par with SFT. On the other hand, methods based on probability, such as VeriFree, flatline on non-verifiable settings due to vanishing probabilities of getting the correct answer. Overall, this establishes log-probability rewards as a viable method for CoT fine-tuning, bridging the short, verifiable and long, non-verifiable answer settings.

</details>


### [78] [Transformers perform adaptive partial pooling](https://arxiv.org/abs/2602.03980)
*Vsevolod Kapatsinski*

Main category: cs.CL

TL;DR: 论文通过比较变分变压器（GPT2）的预测与分层回归模型，展示了模型的预测随着训练的进行逐渐减少对其他上下文信息的依赖，并且这种依赖程度受当前上下文频率、类型数量和上下文变化性的影响。


<details>
  <summary>Details</summary>
Motivation: 探讨语言模型在面对非新颖但频率较低的上下文时的适应性表现，并将其与分层回归模型进行对比，以证明变分变压器学习特性的真实性。

Method: 通过实验分析变分变压器（GPT2）随训练进行的预测变化，并将其归因于上下文频率、类型数量和上下文变化性。

Result: 变分变压器（GPT2）的预测逐渐减少对其他上下文信息的依赖，并且这种依赖程度受上下文频率、类型数量和上下文变化性的影响，类似于分层回归模型。

Conclusion: 论文认为，变分变压器的学习特性在理性上和实证上都是合理的。

Abstract: Because language is creative, any reasonable language model must generalize, deciding what to say in novel contexts by using information from similar contexts. But what about contexts that are not novel but merely infrequent? In hierarchical regression, the model's predictions for behavior in a context are affected by observations from other similar contexts to the extent that 1) the current context is infrequent and 2) different contexts behave similarly. This is called adaptive partial pooling of evidence. This paper shows that next-word predictions of a transformer (GPT2) are increasingly unaffected by observations from outside the current context across epochs of training (the amount of pooling reduces with training), and that the extent of pooling is affected by context frequency, context number (type frequency) and context variability in a similar way to hierarchical regression. These characteristics of learning in transformers are argued to be realistic on both rational and empirical grounds.

</details>


### [79] [On the Credibility of Evaluating LLMs using Survey Questions](https://arxiv.org/abs/2602.04033)
*Jindřich Libovický*

Main category: cs.CL

TL;DR: 本研究通过使用世界价值观调查数据，多语言多国示例表明，不同的提示方法和解码策略显著影响结果。引入了一个新颖的自相关距离度量来评估答案间的内在一致性，并指出即使平均一致性高，也不代表结构上的对齐。建议未来研究采用chain-of-thought提示、基于抽样的解码以及多度量的稳健分析。


<details>
  <summary>Details</summary>
Motivation: 鉴于现有方法可能因具体设置不同而低估或高估价值观一致性，研究旨在改进评价方法，提供更准确的评估。

Method: 采用世界价值观调查数据，在不同语言和国家进行多轮实验，并引入自相关距离作为评估指标。

Result: 研究发现，不同的提示方法和解码策略显著影响评价结果，自相关距离可以有效评估答案一致性，并揭示了两种常用评价指标间的弱相关性。

Conclusion: 建议未来研究采用chain-of-thought提示、基于抽样的解码策略，并使用多种度量进行稳健分析，以减少评估偏见和误差。

Abstract: Recent studies evaluate the value orientation of large language models (LLMs) using adapted social surveys, typically by prompting models with survey questions and comparing their responses to average human responses. This paper identifies limitations in this methodology that, depending on the exact setup, can lead to both underestimating and overestimating the similarity of value orientation. Using the World Value Survey in three languages across five countries, we demonstrate that prompting methods (direct vs. chain-of-thought) and decoding strategies (greedy vs. sampling) significantly affect results. To assess the interaction between answers, we introduce a novel metric, self-correlation distance. This metric measures whether LLMs maintain consistent relationships between answers across different questions, as humans do. This indicates that even a high average agreement with human data, when considering LLM responses independently, does not guarantee structural alignment in responses. Additionally, we reveal a weak correlation between two common evaluation metrics, mean-squared distance and KL divergence, which assume that survey answers are independent of each other. For future research, we recommend CoT prompting, sampling-based decoding with dozens of samples, and robust analysis using multiple metrics, including self-correlation distance.

</details>


### [80] [Abstraction Induces the Brain Alignment of Language and Speech Models](https://arxiv.org/abs/2602.04081)
*Emily Cheng,Aditya R. Vaidya,Richard Antonello*

Main category: cs.CL

TL;DR: 本研究通过分析中间层和输出层在预测大脑对自然语言刺激响应的能力，发现语言模型和语音模型中间层的语义丰富度和内在维度与大脑预测性成正相关，关键在于复杂的语义抽象。


<details>
  <summary>Details</summary>
Motivation: 探索语言模型和语音模型中间层为何比输出层更有效预测大脑对自然语言刺激的响应。

Method: 通过测量层间内在维度来分析语言模型和语音模型中的特征复杂性，并将模型微调以更好地预测大脑活动，观察层间内在维度和大脑预测性的关系。

Result: 发现中间层的内在维度与fMRI和ECoG信号解释度有强相关性；微调模型以更好地预测大脑活动会增加模型的内在维度和语义内容。

Conclusion: 研究结论表明，语义丰富度、高内在维度和大脑预测性是相互镜像的，关键驱动力是输入的丰富语义抽象，其中语言建模是一个足够复杂的任务（但可能不是唯一任务）来要求它。

Abstract: Research has repeatedly demonstrated that intermediate hidden states extracted from large language models and speech audio models predict measured brain response to natural language stimuli. Yet, very little is known about the representation properties that enable this high prediction performance. Why is it the intermediate layers, and not the output layers, that are most effective for this unique and highly general transfer task? We give evidence that the correspondence between speech and language models and the brain derives from shared meaning abstraction and not their next-word prediction properties. In particular, models construct higher-order linguistic features in their middle layers, cued by a peak in the layerwise intrinsic dimension, a measure of feature complexity. We show that a layer's intrinsic dimension strongly predicts how well it explains fMRI and ECoG signals; that the relation between intrinsic dimension and brain predictivity arises over model pre-training; and finetuning models to better predict the brain causally increases both representations' intrinsic dimension and their semantic content. Results suggest that semantic richness, high intrinsic dimension, and brain predictivity mirror each other, and that the key driver of model-brain similarity is rich meaning abstraction of the inputs, where language modeling is a task sufficiently complex (but perhaps not the only) to require it.

</details>


### [81] [Expert Selections In MoE Models Reveal (Almost) As Much As Text](https://arxiv.org/abs/2602.04105)
*Amir Nuriyev,Gabriel Kulp*

Main category: cs.CL

TL;DR: 本研究揭示MoE模型中的路由决策泄露了比以往认识更多的信息，提出了一种通过专家选择重建文本的方法，该方法在MoE模型中能够恢复高达91.2%的顶级 tokens。


<details>
  <summary>Details</summary>
Motivation: 当前MoE模型的安全性被低估，存在信息泄漏风险，特别是在分布式推理和侧信道攻击场景下。

Method: 研究提出了一种基于MLP和Transformer的攻击方法，以重建 MoE 模型中的文本。实验展示了不同模型在不同类型数据上的效果。

Result: 使用3层MLP模型实现63.1%的顶级准确性，基于Transformer的序列解码器实现91.2%的顶级和94.8%的前10个令牌准确性。

Conclusion: 研究结果表明，MoE模型的专家选择具有潜在的安全风险，应当谨慎处理。

Abstract: We present a text-reconstruction attack on mixture-of-experts (MoE) language models that recovers tokens from expert selections alone. In MoE models, each token is routed to a subset of expert subnetworks; we show these routing decisions leak substantially more information than previously understood. Prior work using logistic regression achieves limited reconstruction; we show that a 3-layer MLP improves this to 63.1% top-1 accuracy, and that a transformer-based sequence decoder recovers 91.2% of tokens top-1 (94.8% top-10) on 32-token sequences from OpenWebText after training on 100M tokens. These results connect MoE routing to the broader literature on embedding inversion. We outline practical leakage scenarios (e.g., distributed inference and side channels) and show that adding noise reduces but does not eliminate reconstruction. Our findings suggest that expert selections in MoE deployments should be treated as sensitive as the underlying text.

</details>


### [82] [From Lemmas to Dependencies: What Signals Drive Light Verbs Classification?](https://arxiv.org/abs/2602.04127)
*Sercan Karakaş,Yusuf Şimşek*

Main category: cs.CL

TL;DR: 这篇论文探讨了通过系统限制模型输入信号来分类土耳其语中的动词短语构造（LVCs）。实验对比了基于词干和语法特征的传统方法，以及全面输入的Transformer模型。结果显示，粗粒度的形态语法信息不足以在控制性对照组中稳健地检测LVCs，而词汇身份在支持LVC判断方面是有效的，但对归一化选择敏感。研究指出，词干只能表示必须具体化归一化过程的单一、明确的表征。


<details>
  <summary>Details</summary>
Motivation: 为了系统地研究土耳其语中动词短语构造（LVCs）的分类信号，论文利用UD衍生的数据来对比不同类型的模型，并试图揭示这些高质量资源在实际应用中的效果如何。

Method: 该论文使用了基于词干的TF-IDF + Logistic Regression、词汇序列训练的BERTurk、以及全面运用UD特征的Logistic Regression来进行实验对比。此外，还生成了一个新的LVC对照数据集，包括随机负样本、词汇控制集（NLVC）和LVC阳性样本，以评估模型的整体性能。

Result: 实验证明，仅使用粗粒度的形态语法信息不足以在控制性对照组中稳健地检测LVCs，而词汇身份在支持LVC判断方面是有效的，但对归一化选择是敏感的。

Conclusion: 研究结果表明，需要针对性地评估土耳其语多词表达（MWE）的表现，并指出“仅词干”不是一个单一定义的表示，取决于如何进行归一化操作。

Abstract: Light verb constructions (LVCs) are a challenging class of verbal multiword expressions, especially in Turkish, where rich morphology and productive complex predicates create minimal contrasts between idiomatic predicate meanings and literal verb--argument uses. This paper asks what signals drive LVC classification by systematically restricting model inputs. Using UD-derived supervision, we compare lemma-driven baselines (lemma TF--IDF + Logistic Regression; BERTurk trained on lemma sequences), a grammar-only Logistic Regression over UD morphosyntax (UPOS/DEPREL/MORPH), and a full-input BERTurk baseline. We evaluate on a controlled diagnostic set with Random negatives, lexical controls (NLVC), and LVC positives, reporting split-wise performance to expose decision-boundary behavior. Results show that coarse morphosyntax alone is insufficient for robust LVC detection under controlled contrasts, while lexical identity supports LVC judgments but is sensitive to calibration and normalization choices. Overall, Our findings motivate targeted evaluation of Turkish MWEs and show that ``lemma-only'' is not a single, well-defined representation, but one that depends critically on how normalization is operationalized.

</details>


### [83] [The Missing Half: Unveiling Training-time Implicit Safety Risks Beyond Deployment](https://arxiv.org/abs/2602.04196)
*Zhexin Zhang,Yida Lu,Junfeng Fang,Junxiao Yang,Shiyao Cui,Hao Zhou,Fandong Meng,Jie Zhou,Hongning Wang,Minlie Huang,Tat-Seng Chua*

Main category: cs.CL

TL;DR: 该论文首次系统地研究了训练过程中隐式安全风险，发现了 llama-3.1-8B-Instruct 在仅提供背景信息的情况下有 74.4% 的训练运行表现出风险行为。


<details>
  <summary>Details</summary>
Motivation: 当前关于 AI 模型部署时的安全风险研究较多，而训练过程中产生的安全风险却被忽视。本文旨在填补这一研究空白。

Method: 通过引入五级风险等级、十个细化风险类别和三种激励类型，系统地分析了这些隐式训练风险。此外，进行了一系列广泛的实验来揭示这些风险的普遍性和严重性。

Result: 实验结果表明，仅提供背景信息时，llama-3.1-8B-Instruct 有 74.4% 的训练运行表现出风险行为。此外，还分析了影响这些行为的因素，并展示了隐式训练风险也存在于多智能体训练环境中。

Conclusion: 研究指出，训练过程中存在未被重视但亟待解决的安全挑战。

Abstract: Safety risks of AI models have been widely studied at deployment time, such as jailbreak attacks that elicit harmful outputs. In contrast, safety risks emerging during training remain largely unexplored. Beyond explicit reward hacking that directly manipulates explicit reward functions in reinforcement learning, we study implicit training-time safety risks: harmful behaviors driven by a model's internal incentives and contextual background information. For example, during code-based reinforcement learning, a model may covertly manipulate logged accuracy for self-preservation. We present the first systematic study of this problem, introducing a taxonomy with five risk levels, ten fine-grained risk categories, and three incentive types. Extensive experiments reveal the prevalence and severity of these risks: notably, Llama-3.1-8B-Instruct exhibits risky behaviors in 74.4% of training runs when provided only with background information. We further analyze factors influencing these behaviors and demonstrate that implicit training-time risks also arise in multi-agent training settings. Our results identify an overlooked yet urgent safety challenge in training.

</details>


### [84] [Language Models Struggle to Use Representations Learned In-Context](https://arxiv.org/abs/2602.04212)
*Michael A. Lepori,Tal Linzen,Ann Yuan,Katja Filippova*

Main category: cs.CL

TL;DR: 本研究评估了开放权重的大语言模型是否能够利用上下文中的表示来完成下游任务，并通过新颖的任务验证了当前LLMs难以灵活地利用这些表示。


<details>
  <summary>Details</summary>
Motivation: 探讨大语言模型如何利用上下文中的表示来提升性能，尤其是在面对新型语义和模式时的灵活性。

Method: 通过在上下文下一词预测任务和新颖的适应世界建模任务中评估开放权重和封闭源代码下的最先进模型来研究模型的灵活性。

Result: 研究发现，在定义新语义的上下文中，模型难以部署这些表示。即使是表现最好的模型也难以可靠地利用在上下文中呈现的新模式。

Conclusion: 该研究强调了开发新方法以鼓励模型不仅编码上下文中的信息，还要以支持灵活部署这些信息的方式进行编码的重要性。

Abstract: Though large language models (LLMs) have enabled great success across a wide variety of tasks, they still appear to fall short of one of the loftier goals of artificial intelligence research: creating an artificial system that can adapt its behavior to radically new contexts upon deployment. One important step towards this goal is to create systems that can induce rich representations of data that are seen in-context, and then flexibly deploy these representations to accomplish goals. Recently, Park et al. (2024) demonstrated that current LLMs are indeed capable of inducing such representation from context (i.e., in-context representation learning). The present study investigates whether LLMs can use these representations to complete simple downstream tasks.
  We first assess whether open-weights LLMs can use in-context representations for next-token prediction, and then probe models using a novel task, adaptive world modeling. In both tasks, we find evidence that open-weights LLMs struggle to deploy representations of novel semantics that are defined in-context, even if they encode these semantics in their latent representations. Furthermore, we assess closed-source, state-of-the-art reasoning models on the adaptive world modeling task, demonstrating that even the most performant LLMs cannot reliably leverage novel patterns presented in-context. Overall, this work seeks to inspire novel methods for encouraging models to not only encode information presented in-context, but to do so in a manner that supports flexible deployment of this information.

</details>


### [85] [Tokenization and Morphological Fidelity in Uralic NLP: A Cross-Lingual Evaluation](https://arxiv.org/abs/2602.04241)
*Nuo Xu,Ahrii Kim*

Main category: cs.CL

TL;DR: 本研究系统地对比了三种子词划分方法（BPE、OBPE、Unigram Language Model）在六种乌拉尔语族语言中（涵盖不同的资源可用性和类型多样性）的表现，发现OBPE在开放类别词汇中减少碎片化并平衡频率分布方面胜过传统方法，特别在使用拉丁字母的语言中表现优异。


<details>
  <summary>Details</summary>
Motivation: 由于形态丰富的语言和资源贫乏的语言的子词划分行为尚未得到充分探索，本研究旨在系统比较三种子词划分方法在不同语言中的表现，以促进这些语言在自然语言处理中的应用。

Method: 本研究选择了六种乌拉尔语族的语言，采用了POS标注作为控制下游任务，并系统地比较了三种子词划分方法在这些语言中的应用效果。

Result: 研究结果表明，OBPE在开放类别词汇中减少碎片化并平衡频率分布方面胜过传统方法，特别是在使用拉丁字母的乌拉尔语族语言中表现优异。

Conclusion: 本研究指出，形态敏感的子词划分不仅是预处理选择，而是有效实现跨语言转移的关键因素，特别对于粘着语和资源贫乏的语言。

Abstract: Subword tokenization critically affects Natural Language Processing (NLP) performance, yet its behavior in morphologically rich and low-resource language families remains under-explored. This study systematically compares three subword paradigms -- Byte Pair Encoding (BPE), Overlap BPE (OBPE), and Unigram Language Model -- across six Uralic languages with varying resource availability and typological diversity. Using part-of-speech (POS) tagging as a controlled downstream task, we show that OBPE consistently achieves stronger morphological alignment and higher tagging accuracy than conventional methods, particularly within the Latin-script group. These gains arise from reduced fragmentation in open-class categories and a better balance across the frequency spectrum. Transfer efficacy further depends on the downstream tagging architecture, interacting with both training volume and genealogical proximity. Taken together, these findings highlight that morphology-sensitive tokenization is not merely a preprocessing choice but a decisive factor in enabling effective cross-lingual transfer for agglutinative, low-resource languages.

</details>


### [86] [CoLT: Reasoning with Chain of Latent Tool Calls](https://arxiv.org/abs/2602.04246)
*Fangwei Zhu,Zhifang Sui*

Main category: cs.CL

TL;DR: CoLT 提出了一种新的框架，通过工具调用实现潜在推理，而不是完全在潜在空间中进行推理，从而在保持模型推理能力的同时提高了效率。


<details>
  <summary>Details</summary>
Motivation: 目前的潜在推理方法通常需要模型结构增强和过多的训练，限制了其广泛的适用性。为了解决这个问题，论文提出了一种新的方法CoLT，通过工具调用实现潜在推理。

Method: CoLT 方法首先生成包含推理步骤信息的 '种子' token，当触发潜在工具调用时，较小的外部模型将使用这些 '种子' token 的隐藏状态作为输入，将它们恢复为完整的推理步骤。这种方法确保主要模型在显式的 token 空间中进行推理，保持其能力同时提高效率。

Result: 在四个数学数据集上的实验证明 CoLT 较之基础潜在模型具有更高的准确性和更短的推理长度，并且兼容强化学习算法和不同解码器结构。

Conclusion: CoLT 提出了一种新的框架，通过工具调用实现潜在推理，成功提高了 Large Language Models 的推理效率和准确性。

Abstract: Chain-of-Thought (CoT) is a critical technique in enhancing the reasoning ability of Large Language Models (LLMs), and latent reasoning methods have been proposed to accelerate the inefficient token-level reasoning chain. We notice that existing latent reasoning methods generally require model structure augmentation and exhaustive training, limiting their broader applicability. In this paper, we propose CoLT, a novel framework that implements latent reasoning as ``tool calls''. Instead of reasoning entirely in the latent space, CoLT generates seed tokens that contain information of a reasoning step. When a latent tool call is triggered, a smaller external model will take the hidden states of seed tokens as its input, and unpack the seed tokens back to a full reasoning step. In this way, we can ensure that the main model reasons in the explicit token space, preserving its ability while improving efficiency. Experimental results on four mathematical datasets demonstrate that CoLT achieves higher accuracy and shorter reasoning length than baseline latent models, and is compatible with reinforcement learning algorithms and different decoder structures.

</details>


### [87] [DementiaBank-Emotion: A Multi-Rater Emotion Annotation Corpus for Alzheimer's Disease Speech (Version 1.0)](https://arxiv.org/abs/2602.04247)
*Cheonkam Jeong,Jessica Liao,Audrey Lu,Yutong Song,Christopher Rashidian,Donna Krogh,Erik Krogh,Mahkameh Rasouli,Jung-Ah Lee,Nikil Dutt,Lisa M Gibbs,David Sultzer,Julie Rousseau,Jocelyn Ludlow,Margaret Galvez,Alexander Nuth,Chet Khay,Sabine Brunswicker,Adeline Nyamathi*

Main category: cs.CL

TL;DR: 该研究介绍了首个针对阿尔茨海默病(AD)患者的多评分者情感标注语料库DementiaBank-Emotion，并分析了患者与健康人在情感表达上的差异。


<details>
  <summary>Details</summary>
Motivation: 研究阿尔茨海默病患者的情感表达，以支持临床环境中的情感识别研究。

Method: 对来自108位讲者共1,492条话语进行了六种基本情绪及中性情绪的标注，并进行了声学分析。

Result: 研究表明AD患者比健康控制组更频繁地表达非中性情绪，情感表达存在差异；控制组在悲伤情感上表现出显著的基频变化，而AD组则没有；在AD患者的语音中，音量差异地反映了不同情感类别。

Conclusion: 研究结果揭示了AD患者的语音情感表达特征，并提出了一个可重复使用的数据集以促进相关研究。

Abstract: We present DementiaBank-Emotion, the first multi-rater emotion annotation corpus for Alzheimer's disease (AD) speech. Annotating 1,492 utterances from 108 speakers for Ekman's six basic emotions and neutral, we find that AD patients express significantly more non-neutral emotions (16.9%) than healthy controls (5.7%; p < .001). Exploratory acoustic analysis suggests a possible dissociation: control speakers showed substantial F0 modulation for sadness (Delta = -3.45 semitones from baseline), whereas AD speakers showed minimal change (Delta = +0.11 semitones; interaction p = .023), though this finding is based on limited samples (sadness: n=5 control, n=15 AD) and requires replication. Within AD speech, loudness differentiates emotion categories, indicating partially preserved emotion-prosody mappings. We release the corpus, annotation guidelines, and calibration workshop materials to support research on emotion recognition in clinical populations.

</details>


### [88] [ECG-R1: Protocol-Guided and Modality-Agnostic MLLM for Reliable ECG Interpretation](https://arxiv.org/abs/2602.04279)
*Jiarui Jin,Haoyu Wang,Xingliang Wu,Xiaocheng Fang,Xiang Lan,Zihan Wang,Deyun Zhang,Bo Liu,Yingying Zhang,Xian Wu,Hongyan Li,Shenda Hong*

Main category: cs.CL

TL;DR: 本文介绍了ECG-R1，一种专为可靠心电图解读设计的推理多模态大语言模型，通过特定的数据生成方法、解耦模态架构和强化学习机制实现。


<details>
  <summary>Details</summary>
Motivation: 鉴于当前大规模语言模型在心电图解读方面的不足，本文提出了ECG-R1，旨在通过改进数据生成、模型架构和学习机制，提高心电图解释的可靠性。

Method: 首先，使用基于协议指导的指令数据生成方法构建解读数据集，结合可测量的心电图特征和标准化的定量阈值及诊断逻辑；其次，设计了一个模态解耦架构，采用交错模态丢弃技术来增强模型的稳健性和模态间一致性；最后，引入了基于心电图诊断证据的强化学习机制以增强依据证据的心电图解读。

Result: 本文通过系统地评估专有、开源和医学大规模语言模型的心电图解读能力，提供了首个表明严重幻觉普遍存在的确凿证据，建议公众在不进行独立验证的情况下不应直接信任这些模型的输出。

Conclusion: ECG-R1的成功开发和评估证明了解决心电图解读问题的可行性和有效性，为医疗领域的自然语言处理应用开辟了新的可能性。

Abstract: Electrocardiography (ECG) serves as an indispensable diagnostic tool in clinical practice, yet existing multimodal large language models (MLLMs) remain unreliable for ECG interpretation, often producing plausible but clinically incorrect analyses. To address this, we propose ECG-R1, the first reasoning MLLM designed for reliable ECG interpretation via three innovations. First, we construct the interpretation corpus using \textit{Protocol-Guided Instruction Data Generation}, grounding interpretation in measurable ECG features and monograph-defined quantitative thresholds and diagnostic logic. Second, we present a modality-decoupled architecture with \textit{Interleaved Modality Dropout} to improve robustness and cross-modal consistency when either the ECG signal or ECG image is missing. Third, we present \textit{Reinforcement Learning with ECG Diagnostic Evidence Rewards} to strengthen evidence-grounded ECG interpretation. Additionally, we systematically evaluate the ECG interpretation capabilities of proprietary, open-source, and medical MLLMs, and provide the first quantitative evidence that severe hallucinations are widespread, suggesting that the public should not directly trust these outputs without independent verification. Code and data are publicly available at \href{https://github.com/PKUDigitalHealth/ECG-R1}{here}, and an online platform can be accessed at \href{http://ai.heartvoice.com.cn/ECG-R1/}{here}.

</details>


### [89] [Contextual Drag: How Errors in the Context Affect LLM Reasoning](https://arxiv.org/abs/2602.04288)
*Yun Cheng,Xingyu Zhu,Haoyu Zhao,Sanjeev Arora*

Main category: cs.CL

TL;DR: 该研究探讨了大型语言模型（LLMs）在反思过去错误以自我改进时遇到的‘上下文拖累’现象，发现这会导致模型表现下降10-20%，严重时可能导致模型自毁。尽管采取了一些缓解策略，但未能完全恢复基准性能，揭示了当前推理架构中的持久性故障模式。


<details>
  <summary>Details</summary>
Motivation: 研究旨在验证大语言模型在反思过去错误时是否会通过自我改进来提升，特别是在存在失败尝试的上下文下，探讨这种现象对模型表现的影响。

Method: 研究者对11个商用和开源模型进行了8项推理任务的评估，使用树编辑距离进行结构性分析验证结果。

Result: 研究发现，上下文拖累现象导致模型性能下降10-20%，严重的上下文拖累会导致模型自我恶化。外部反馈和成功的自我验证不足以消除这种影响。虽然有缓解策略如退避行为微调和上下文去噪可以改善情况，但未能完全恢复基准性能。

Conclusion: 研究表明，上下文拖累是当前推理架构中的一个持久性问题，即将模型错误的结构特征带入之后的推理解析中。现有的缓解方法并未完全解决这一问题。

Abstract: Central to many self-improvement pipelines for large language models (LLMs) is the assumption that models can improve by reflecting on past mistakes. We study a phenomenon termed contextual drag: the presence of failed attempts in the context biases subsequent generations toward structurally similar errors. Across evaluations of 11 proprietary and open-weight models on 8 reasoning tasks, contextual drag induces 10-20% performance drops, and iterative self-refinement in models with severe contextual drag can collapse into self-deterioration. Structural analysis using tree edit distance reveals that subsequent reasoning trajectories inherit structurally similar error patterns from the context. We demonstrate that neither external feedback nor successful self-verification suffices to eliminate this effect. While mitigation strategies such as fallback-behavior fine-tuning and context denoising yield partial improvements, they fail to fully restore baseline performance, positioning contextual drag as a persistent failure mode in current reasoning architectures.

</details>


### [90] [Guided Verifier: Collaborative Multimodal Reasoning via Dynamic Process Supervision](https://arxiv.org/abs/2602.04290)
*Lingzhuang Sun,Ruitong Liu,Yuxia Zhu,Xiaohan Xu,Jingxuan Wei,Xiangxiang Zhang,Bihui Yu,Wentao Zhang*

Main category: cs.CL

TL;DR: 该论文提出了一种名为Guided Verifier的框架，旨在改进现有的强化学习（RL）方法在处理多模态大语言模型（MLLMs）复杂推理时的局限性。通过引入动态验证器实时地与策略模型交互，检测不一致并提供方向建议，Guarded Verifier框架能够有效减少错误传播，提高模型的优化信号质量。


<details>
  <summary>Details</summary>
Motivation: 现有的RL方法在MLLMs推理过程中缺乏中间监督，容易导致错误累积，影响模型优化效果。因此，作者提出Guided Verifier框架以实现更高效的同步推理和验证，增强模型的平衡性和稳定性。

Method: 作者开发了一个特定的数据合成管道，用来生成过程级的反例和正确的推理轨迹。基于这些数据，他们训练了一个动态验证器，该验证器在模拟能力测评中实时与策略模型交互，帮助识别问题并纠正错误。

Result: 在MathVista、MathVerse和MMMU上进行的广泛实验表明，通过分配计算资源到协同推理和动态验证，一个8B参数的模型能够达到强大的性能。

Conclusion: 该研究证明了通过主动指导和动态验证增强了RL方法的有效性，并为此类模型在复杂多模态推理任务中的应用提供了新的思路。

Abstract: Reinforcement Learning (RL) has emerged as a pivotal mechanism for enhancing the complex reasoning capabilities of Multimodal Large Language Models (MLLMs). However, prevailing paradigms typically rely on solitary rollout strategies where the model works alone. This lack of intermediate oversight renders the reasoning process susceptible to error propagation, where early logical deviations cascade into irreversible failures, resulting in noisy optimization signals. In this paper, we propose the \textbf{Guided Verifier} framework to address these structural limitations. Moving beyond passive terminal rewards, we introduce a dynamic verifier that actively co-solves tasks alongside the policy. During the rollout phase, this verifier interacts with the policy model in real-time, detecting inconsistencies and providing directional signals to steer the model toward valid trajectories. To facilitate this, we develop a specialized data synthesis pipeline targeting multimodal hallucinations, constructing \textbf{CoRe} dataset of process-level negatives and \textbf{Co}rrect-guide \textbf{Re}asoning trajectories to train the guided verifier. Extensive experiments on MathVista, MathVerse and MMMU indicate that by allocating compute to collaborative inference and dynamic verification, an 8B-parameter model can achieve strong performance.

</details>


### [91] [How Few-shot Demonstrations Affect Prompt-based Defenses Against LLM Jailbreak Attacks](https://arxiv.org/abs/2602.04294)
*Yanshu Wang,Shuaishuai Yang,Jingjing He,Tong Yang*

Main category: cs.CL

TL;DR: 本研究通过评估多种主流大语言模型在四项安全性基准上的表现，发现少量示例对角色导向提示（RoP）增强安全性能，但对任务导向提示（ToP）产生负面影响。


<details>
  <summary>Details</summary>
Motivation: 现有的应对大模型攻击方法缺乏对少量示例作用的深入研究，作者希望通过本研究填补这一空白，理解少量示例在不同系统提示策略中的效果。

Method: 研究者对六种不同的破坏性攻击方法进行了广泛的评估，跨越四个安全基准，并具体分析了少量示例对角色导向提示和任务导向提示的影响。

Result: 研究表明，少量示例增强了角色导向提示的安全性，但削弱了任务导向提示的效能。具体表现为RoP提高了安全率最高可达4.5%，而ToP的有效性则下降了最多21.2%。

Conclusion: 研究为实际应用中的大模型提供了基于提示的安全防御实用建议，表明防御策略应综合考虑少量示例的影响。

Abstract: Large Language Models (LLMs) face increasing threats from jailbreak attacks that bypass safety alignment. While prompt-based defenses such as Role-Oriented Prompts (RoP) and Task-Oriented Prompts (ToP) have shown effectiveness, the role of few-shot demonstrations in these defense strategies remains unclear. Prior work suggests that few-shot examples may compromise safety, but lacks investigation into how few-shot interacts with different system prompt strategies. In this paper, we conduct a comprehensive evaluation on multiple mainstream LLMs across four safety benchmarks (AdvBench, HarmBench, SG-Bench, XSTest) using six jailbreak attack methods. Our key finding reveals that few-shot demonstrations produce opposite effects on RoP and ToP: few-shot enhances RoP's safety rate by up to 4.5% through reinforcing role identity, while it degrades ToP's effectiveness by up to 21.2% through distracting attention from task instructions. Based on these findings, we provide practical recommendations for deploying prompt-based defenses in real-world LLM applications.

</details>


### [92] [Revisiting Prompt Sensitivity in Large Language Models for Text Classification: The Role of Prompt Underspecification](https://arxiv.org/abs/2602.04297)
*Branislav Pecher,Michal Spiegel,Robert Belanec,Jan Cegin*

Main category: cs.CL

TL;DR: 研究发现，提示欠具体性是性能波动的主要原因，而详细指令的提示则较少受到影响。研究结果表明，提示欠具体性的影响主要体现在模型的顶层而非内部表示。


<details>
  <summary>Details</summary>
Motivation: 探讨并比较欠具体提示和具体指令提示对大型语言模型性能的影响，旨在揭示提示敏感性的重要来源，并为提高模型性能稳定性提供指导。

Method: 通过性能分析、logit分析和线性探针方法对欠具体提示和具体指令提示进行系统研究。

Result: 研究发现，欠具体提示导致了更高的性能波动和较低的相关词logit值，而具体指令提示则表现更稳定。线性探针分析显示，提示欠具体性对内部表示的影响较小。

Conclusion: 提示的欠具体性是影响模型性能稳定性的关键因素之一，应在试验和实际应用中加以严格控制以减少这种影响。

Abstract: Large language models (LLMs) are widely used as zero-shot and few-shot classifiers, where task behaviour is largely controlled through prompting. A growing number of works have observed that LLMs are sensitive to prompt variations, with small changes leading to large changes in performance. However, in many cases, the investigation of sensitivity is performed using underspecified prompts that provide minimal task instructions and weakly constrain the model's output space. In this work, we argue that a significant portion of the observed prompt sensitivity can be attributed to prompt underspecification. We systematically study and compare the sensitivity of underspecified prompts and prompts that provide specific instructions. Utilising performance analysis, logit analysis, and linear probing, we find that underspecified prompts exhibit higher performance variance and lower logit values for relevant tokens, while instruction-prompts suffer less from such problems. However, linear probing analysis suggests that the effects of prompt underspecification have only a marginal impact on the internal LLM representations, instead emerging in the final layers. Overall, our findings highlight the need for more rigour when investigating and mitigating prompt sensitivity.

</details>


### [93] [DeFrame: Debiasing Large Language Models Against Framing Effects](https://arxiv.org/abs/2602.04306)
*Kahee Lim,Soyeon Kim,Steven Euijong Whang*

Main category: cs.CL

TL;DR: 本文通过引入‘框架差异’的概念，发现语言模型的公平性评估受表达方式的影响，并提出了一种框架感知的去偏方法，以提高模型在不同表达方式下的公平性和一致性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型日益应用于实际场景，确保其在不同群体中的公平响应变得至关重要。现有去偏方法未能有效减少由表达方式差异带来的偏见，因此本研究旨在探索新的去偏方法。

Method: 研究引入了‘框架差异’的概念，通过扩展公平性评价基准加入不同表达方式，分析了模型公平性受表达方式影响的情况，并提出了一种框架感知的去偏方法，以提升不同表达方式下的模型表现。

Result: 研究发现框架差异显著影响模型公平性评估，并证明提出的框架感知去偏方法能有效减少总体偏见，提高模型的鲁棒性及一致性。

Conclusion: 本研究强调了在框架差异下评估和优化语言模型的重要性，并提出了一种框架感知的去偏方法，该方法有助于提高大模型的公平性和一致性表现。

Abstract: As large language models (LLMs) are increasingly deployed in real-world applications, ensuring their fair responses across demographics has become crucial. Despite many efforts, an ongoing challenge is hidden bias: LLMs appear fair under standard evaluations, but can produce biased responses outside those evaluation settings. In this paper, we identify framing -- differences in how semantically equivalent prompts are expressed (e.g., "A is better than B" vs. "B is worse than A") -- as an underexplored contributor to this gap. We first introduce the concept of "framing disparity" to quantify the impact of framing on fairness evaluation. By augmenting fairness evaluation benchmarks with alternative framings, we find that (1) fairness scores vary significantly with framing and (2) existing debiasing methods improve overall (i.e., frame-averaged) fairness, but often fail to reduce framing-induced disparities. To address this, we propose a framing-aware debiasing method that encourages LLMs to be more consistent across framings. Experiments demonstrate that our approach reduces overall bias and improves robustness against framing disparities, enabling LLMs to produce fairer and more consistent responses.

</details>


### [94] [A Domain-Specific Curated Benchmark for Entity and Document-Level Relation Extraction](https://arxiv.org/abs/2602.04320)
*Marco Martinelli,Stefano Marchesin,Vanessa Bonato,Giorgio Maria Di Nunzio,Nicola Ferro,Ornella Irrera,Laura Menotti,Federica Vezzani,Gianmaria Silvello*

Main category: cs.CL

TL;DR: 该研究介绍了一个基于1600多篇PubMed文献的手动注释基准GutBrainIE，旨在推动生物医学信息抽取技术的发展。


<details>
  <summary>Details</summary>
Motivation: 现有的生物医学信息抽取基准较为局限且依赖于距离监督或自动生成的标注，这限制了其在开发稳健的信息抽取方法方面的应用。为了克服这些局限性，研究者提出了GutBrainIE基准，它包括从PubMed中提取的大量文献，并由生物医学和术语专家进行详细标注。

Method: GutBrainIE基准包括1600多篇PubMed摘要，这些摘要是通过手动注释，使用精细实体、概念级别链接和关系的规定来构建的。

Result: GutBrainIE基准为生物医学信息抽取系统的发展和评估提供了丰富的架构、多任务以及高度精练与弱监督数据的结合，使其具有广泛的适用性。

Conclusion: GutBrainIE将成为推动跨领域生物医学信息抽取方法研究的重要资源。

Abstract: Information Extraction (IE), encompassing Named Entity Recognition (NER), Named Entity Linking (NEL), and Relation Extraction (RE), is critical for transforming the rapidly growing volume of scientific publications into structured, actionable knowledge. This need is especially evident in fast-evolving biomedical fields such as the gut-brain axis, where research investigates complex interactions between the gut microbiota and brain-related disorders. Existing biomedical IE benchmarks, however, are often narrow in scope and rely heavily on distantly supervised or automatically generated annotations, limiting their utility for advancing robust IE methods. We introduce GutBrainIE, a benchmark based on more than 1,600 PubMed abstracts, manually annotated by biomedical and terminological experts with fine-grained entities, concept-level links, and relations. While grounded in the gut-brain axis, the benchmark's rich schema, multiple tasks, and combination of highly curated and weakly supervised data make it broadly applicable to the development and evaluation of biomedical IE systems across domains.

</details>


### [95] [Merged ChemProt-DrugProt for Relation Extraction from Biomedical Literature](https://arxiv.org/abs/2405.18605)
*Mai H. Nguyen,Shibani Likhite,Jiawei Tang,Darshini Mahendran,Bridget T. McInnes*

Main category: cs.CL

TL;DR: 该研究通过合并ChemProt和DrugProt数据集，并结合BioBERT和Graph Convolutional Networks (GCNs)，以提高化学-基因关系提取模型的性能，特别是在共享的CPR组中取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 随着药物发现和疾病研究的深入，理解化学化合物与基因之间的复杂相互作用变得越来越重要。为了提高这一领域的研究效率和准确性，研究者合并了ChemProt和DrugProt数据集，并探索使用先进的关系提取算法（如BioBERT和GCNs）来改进模型。

Method: 研究方法包括合并两个数据集来增加样本数量，然后利用两种最先进的关系抽取算法——BioBERT和GCNs整合到BioBERT中，来增强模型对化学-基因相互作用的理解。

Result: 实验结果显示，通过合并数据集与结合GCNs，模型在CPR组中的性能有了显著提升，特别是在两数据集共享的CPR组中，结合GCNs可以帮助提高总体精度和召回率。

Conclusion: 该研究提出了一个有效的方法，通过数据集合并和算法增强，提高了化学-基因关系的识别能力，未来的研究可能会进一步优化这些技术以更好地服务于药物开发和生物医学研究。

Abstract: The extraction of chemical-gene relations plays a pivotal role in understanding the intricate interactions between chemical compounds and genes, with significant implications for drug discovery, disease understanding, and biomedical research. This paper presents a data set created by merging the ChemProt and DrugProt datasets to augment sample counts and improve model accuracy. We evaluate the merged dataset using two state of the art relationship extraction algorithms: Bidirectional Encoder Representations from Transformers (BERT) specifically BioBERT, and Graph Convolutional Networks (GCNs) combined with BioBERT. While BioBERT excels at capturing local contexts, it may benefit from incorporating global information essential for understanding chemical-gene interactions. This can be achieved by integrating GCNs with BioBERT to harness both global and local context. Our results show that by integrating the ChemProt and DrugProt datasets, we demonstrated significant improvements in model performance, particularly in CPR groups shared between the datasets. Incorporating the global context using GCN can help increase the overall precision and recall in some of the CPR groups over using just BioBERT.

</details>


### [96] [Can Vision Replace Text in Working Memory? Evidence from Spatial n-Back in Vision-Language Models](https://arxiv.org/abs/2602.04355)
*Sichu Liang,Hongyu Zhu,Wenwen Wang,Deyu Zhou*

Main category: cs.CL

TL;DR: 该研究评估了Qwen2.5和Qwen2.5-VL在控制空间n-back任务中的表现，发现视觉和文本信息在工作记忆中的表现不同。视觉信息的表现比文本信息差，网格大小影响干扰和错误模式。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于通过视觉n-back任务评估视觉语言模型在处理视觉信息时的工作记忆机制，与语言模型在文字材料处理中的表现进行对比，探讨多模态工作记忆的计算机制。

Method: 使用Qwen2.5和Qwen2.5-VL模型在视觉和文本模式下进行n-back任务实验，通过准确性、d'值和逐次迭代对数概率证据分析，探讨不同条件下模型的工作记忆表现。

Result: 研究发现，尽管两种模型在文本条件下表现更好，但在视觉条件下准确性和d'值较低。此外，网格大小的改变影响了刺激序列中的最近重复结构，从而改变了干扰和错误模式。

Conclusion: 研究结论表明，多模态工作记忆可能存在特定的工作机制，这些机制在处理不同类型的信息时表现不同。

Abstract: Working memory is a central component of intelligent behavior, providing a dynamic workspace for maintaining and updating task-relevant information. Recent work has used n-back tasks to probe working-memory-like behavior in large language models, but it is unclear whether the same probe elicits comparable computations when information is carried in a visual rather than textual code in vision-language models. We evaluate Qwen2.5 and Qwen2.5-VL on a controlled spatial n-back task presented as matched text-rendered or image-rendered grids. Across conditions, models show reliably higher accuracy and d' with text than with vision. To interpret these differences at the process level, we use trial-wise log-probability evidence and find that nominal 2/3-back often fails to reflect the instructed lag and instead aligns with a recency-locked comparison. We further show that grid size alters recent-repeat structure in the stimulus stream, thereby changing interference and error patterns. These results motivate computation-sensitive interpretations of multimodal working memory.

</details>


### [97] [Beyond Rejection Sampling: Trajectory Fusion for Scaling Mathematical Reasoning](https://arxiv.org/abs/2602.04391)
*Jie Deng,Hanshuang Tong,Jun Li,Shining Liang,Ning Wu,Hongzhi Li,Yutao Xie*

Main category: cs.CL

TL;DR: 本文提出了一种名为TrajFusion的新微调策略，它将拒绝采样的监督过程重塑为一个结构化的监督构建过程，通过结合错误和正确的推理轨迹来提供更丰富的监督，从而提升数学推理问题上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的微调策略（如拒绝采样）依赖于二元过滤机制来排除教师生成的错误，忽视了对推理失败建模的需求。TrajFusion通过融合选择性错误轨迹和正确的推理轨迹来显式地模型尝试与错误的推理过程，填补了这一空白。

Method: TrajFusion通过在正确的推理路径和错误的选择中交织错误轨迹，并在错误信号不相关信息时自动退回到标准的拒绝采样微调(RFT)。它是基于教师错误的频率和多样性来动态控制每个融合样本的长度。

Result: 在多个数学基准测试中，TrajFusion在具有挑战性和长形式推理问题上的表现优于RFT。

Conclusion: TrajFusion 提出了一个新的微调框架，将拒绝采样的监督过程扩展为一个更加结构化的监督过程，以更丰富的方式建模尝试和错误的推理过程。

Abstract: Large language models (LLMs) have made impressive strides in mathematical reasoning, often fine-tuned using rejection sampling that retains only correct reasoning trajectories. While effective, this paradigm treats supervision as a binary filter that systematically excludes teacher-generated errors, leaving a gap in how reasoning failures are modeled during training. In this paper, we propose TrajFusion, a fine-tuning strategy that reframes rejection sampling as a structured supervision construction process. Specifically, TrajFusion forms fused trajectories that explicitly model trial-and-error reasoning by interleaving selected incorrect trajectories with reflection prompts and correct trajectories. The length of each fused sample is adaptively controlled based on the frequency and diversity of teacher errors, providing richer supervision for challenging problems while safely reducing to vanilla rejection sampling fine-tuning (RFT) when error signals are uninformative. TrajFusion requires no changes to the architecture or training objective. Extensive experiments across multiple math benchmarks demonstrate that TrajFusion consistently outperforms RFT, particularly on challenging and long-form reasoning problems.

</details>


### [98] [Evaluating the Presence of Sex Bias in Clinical Reasoning by Large Language Models](https://arxiv.org/abs/2602.04392)
*Isabel Tsintsiper,Sheng Wong,Beth Albert,Shaun P Brennecke,Gabriel Davis Jones*

Main category: cs.CL

TL;DR: 该研究评估了当代大型语言模型在临床推理中的性别偏见，发现不同模型显示不同程度的性别偏差，并建议在临床运用时需采取谨慎、记录并持续人类监督的策略。


<details>
  <summary>Details</summary>
Motivation: 研究关注大型语言模型在医疗保健工作流程中的应用，因其可能因训练数据中的性别偏差而放大这些偏见，因此探讨了这些系统在临床推理中的性别偏向问题。

Method: 研究选取了50份由临床医师撰写的案例场景，覆盖44个医学专业领域，探讨了四个通用型语言模型在不同温度设置下的性别预测结果。

Result: 几乎所有模型在不同温度下都显示出性别偏见，其中ChatGPT表现出明显的女性偏好，而Gemini则表现为男性偏好。

Conclusion: 研究指出，虽然允许模型选择回避决定可以减少显式标签，但并不能消除诊断差异，强调了谨慎配置、专业数据审计和持续人类监督在医疗保健情境下部署通用语言模型的重要性。

Abstract: Large language models (LLMs) are increasingly embedded in healthcare workflows for documentation, education, and clinical decision support. However, these systems are trained on large text corpora that encode existing biases, including sex disparities in diagnosis and treatment, raising concerns that such patterns may be reproduced or amplified. We systematically examined whether contemporary LLMs exhibit sex-specific biases in clinical reasoning and how model configuration influences these behaviours. We conducted three experiments using 50 clinician-authored vignettes spanning 44 specialties in which sex was non-informative to the initial diagnostic pathway. Four general-purpose LLMs (ChatGPT (gpt-4o-mini), Claude 3.7 Sonnet, Gemini 2.0 Flash and DeepSeekchat). All models demonstrated significant sex-assignment skew, with predicted sex differing by model. At temperature 0.5, ChatGPT assigned female sex in 70% of cases (95% CI 0.66-0.75), DeepSeek in 61% (0.57-0.65) and Claude in 59% (0.55-0.63), whereas Gemini showed a male skew, assigning a female sex in 36% of cases (0.32-0.41). Contemporary LLMs exhibit stable, model-specific sex biases in clinical reasoning. Permitting abstention reduces explicit labelling but does not eliminate downstream diagnostic differences. Safe clinical integration requires conservative and documented configuration, specialty-level clinical data auditing, and continued human oversight when deploying general-purpose models in healthcare settings.

</details>


### [99] [Bi-directional Bias Attribution: Debiasing Large Language Models without Modifying Prompts](https://arxiv.org/abs/2602.04398)
*Yujie Lin,Kunquan Li,Yixuan Liao,Xiaoxin Chen,Jinsong Su*

Main category: cs.CL

TL;DR: 该研究提出了一种无需微调或修改提示即能检测并减轻大模型中刻板印象引发词汇和神经层面偏见的框架。


<details>
  <summary>Details</summary>
Motivation: 鉴于大语言模型在多种自然语言处理任务中表现出色，但其输出往往会表现出社会偏见，存在公平性问题。现有的去偏见方法虽然有用，但在规模化应用和多轮交互体验上存在问题。

Method: 该框架首先通过跨群体比较分析识别出可能引发刻板印象的形容词和名词；然后使用集成梯度的两种归因策略将偏差行为归因到特定神经元；最后通过直接干预投影层激活来减轻偏差。

Result: 实验表明，该方法能有效减少偏见同时保持整体模型性能。

Conclusion: 该研究提出了一种创新的方法来检测和减轻大语言模型中的偏见，为实现更公平的自然语言处理提供了新思路。

Abstract: Large language models (LLMs) have demonstrated impressive capabilities across a wide range of natural language processing tasks. However, their outputs often exhibit social biases, raising fairness concerns. Existing debiasing methods, such as fine-tuning on additional datasets or prompt engineering, face scalability issues or compromise user experience in multi-turn interactions. To address these challenges, we propose a framework for detecting stereotype-inducing words and attributing neuron-level bias in LLMs, without the need for fine-tuning or prompt modification. Our framework first identifies stereotype-inducing adjectives and nouns via comparative analysis across demographic groups. We then attribute biased behavior to specific neurons using two attribution strategies based on integrated gradients. Finally, we mitigate bias by directly intervening on their activations at the projection layer. Experiments on three widely used LLMs demonstrate that our method effectively reduces bias while preserving overall model performance. Code is available at the github link: https://github.com/XMUDeepLIT/Bi-directional-Bias-Attribution.

</details>


### [100] [Swordsman: Entropy-Driven Adaptive Block Partition for Efficient Diffusion Language Models](https://arxiv.org/abs/2602.04399)
*Yu Zhang,Xinchen Li,Jialei Zhou,Hongnan Ma,Zhongwei Wan,Yiwei Shi,Duoqian Miao,Qi Zhang,Longbing Cao*

Main category: cs.CL

TL;DR: Swordsman 是一种基于熵驱动的自适应块解码框架，通过识别相邻标记之间的熵变化来动态划分块，从而更好地对齐语义或句法成分边界，并根据实时解码状态调整去遮蔽阈值，提升效率与稳定性，实验表明其性能优越。


<details>
  <summary>Details</summary>
Motivation: 现有块解码方法通常以固定方式划分块，导致语义或句法成分断裂，而熵分析能识别出更大的不确定性减少机会，因此提出Swordsman框架。

Method: Swordsman 使用熵分析来识别构成边界，并通过KV Cache支持，在实时解码状态下动态调整去遮蔽阈值，进行自适应块划分。

Result: Swordsman 在广泛评估中展示了最先进的性能。

Conclusion: Swordsman 通过适应性划分块解码并动态调整去遮蔽阈值，显著提升DLM推理速度与质量，是一个有效的训练无关框架。

Abstract: Block-wise decoding effectively improves the inference speed and quality in diffusion language models (DLMs) by combining inter-block sequential denoising and intra-block parallel unmasking. However, existing block-wise decoding methods typically partition blocks in a rigid and fixed manner, which inevitably fragments complete semantic or syntactic constituents, leading to suboptimal performance. Inspired by the entropy reduction hypothesis (ERH), we recognize that constituent boundaries offer greater opportunities for uncertainty reduction, which motivates us to employ entropy analysis for identifying constituent boundaries. Therefore, we propose Swordsman, an entropy-driven adaptive block-wise decoding framework for DLMs. Swordsman adaptively partitions blocks by identifying entropy shifts between adjacent tokens to better align with semantic or syntactic constituent boundaries. In addition, Swordsman dynamically adjusts unmasking thresholds conditioned on the real-time unmasking status within a block, further improving both efficiency and stability. As a training-free framework, supported by KV Cache, Swordsman demonstrates state-of-the-art performance across extensive evaluations.

</details>


### [101] [History-Guided Iterative Visual Reasoning with Self-Correction](https://arxiv.org/abs/2602.04413)
*Xinglong Yang,Zhilin Peng,Zhanzhan Liu,Haochen Shi,Sheng-Jun Huang*

Main category: cs.CL

TL;DR: 该研究提出了一种新的多模态大型语言模型推理框架H-GIVR，通过迭代观察和利用先前生成的答案进行动态错误纠正，从而提高跨模态推理的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的自我一致性方法通常只依赖固定的‘多次采样和投票’模式，不重新利用历史推理信息，这限制了模型在迭代过程中主动纠正视觉理解错误和动态调整推理的能力。

Method: H-GIVR框架具有迭代观察图像和使用先前生成的答案作为后续步骤的参考，实现动态错误纠正，从而提高答案的准确性。

Result: 在五个数据集和三种模型上进行的综合实验表明，H-GIVR框架能显著提高跨模态推理的准确性，同时保持低计算成本。例如，在ScienceQA数据集上使用Llama3.2-vision:11b模型，若要达到78.90%的准确率，模型只需平均每题2.57次响应，相比基线提高了107%。

Conclusion: H-GIVR框架通过利用迭代观察和动态错误纠正机制，在不影响计算效率的情况下，显著增强了多模态LMLMs的跨模态推理能力。

Abstract: Self-consistency methods are the core technique for improving the reasoning reliability of multimodal large language models (MLLMs). By generating multiple reasoning results through repeated sampling and selecting the best answer via voting, they play an important role in cross-modal tasks. However, most existing self-consistency methods are limited to a fixed ``repeated sampling and voting'' paradigm and do not reuse historical reasoning information. As a result, models struggle to actively correct visual understanding errors and dynamically adjust their reasoning during iteration. Inspired by the human reasoning behavior of repeated verification and dynamic error correction, we propose the H-GIVR framework. During iterative reasoning, the MLLM observes the image multiple times and uses previously generated answers as references for subsequent steps, enabling dynamic correction of errors and improving answer accuracy. We conduct comprehensive experiments on five datasets and three models. The results show that the H-GIVR framework can significantly improve cross-modal reasoning accuracy while maintaining low computational cost. For instance, using \texttt{Llama3.2-vision:11b} on the ScienceQA dataset, the model requires an average of 2.57 responses per question to achieve an accuracy of 78.90\%, representing a 107\% improvement over the baseline.

</details>


### [102] [Fine-Grained Activation Steering: Steering Less, Achieving More](https://arxiv.org/abs/2602.04428)
*Zijian Feng,Tianjiao Li,Zixiao Zhu,Hanzhang Zhou,Junlang Qian,Li Zhang,Jia Jim Deryl Chua,Lee Onn Mak,Gee Wah Ng,Kezhi Mao*

Main category: cs.CL

TL;DR: AUSteer 是一种新的激活导向方法，它在原子单位（AU）级操作，通过识别动态和适应性权重来更精确地引导激活，从而在更少的激活引导下实现更好的效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法在块级干预 LLM 行为时表现出粗略、低效且侵入性的问题，AUSteer 提倡在原子单位级进行干预以解决这些问题。

Method: AUSteer 方法首先通过计算对比样本的激活动量来识别区分性的原子单位（AUs），然后根据多样化的输入和选择的 AU 激活动态地分配引导强度。

Result: 实验表明，AUSteer 在多个大模型和任务上均优于先进的基线方法，且在显著减少激活引导的情况下达到了更优的效果。

Conclusion: AUSteer 通过精细化的干预策略实现了更精确和有效的激活导向，展示了在减少干预数量的同时实现更高性能的可能性。

Abstract: Activation steering has emerged as a cost-effective paradigm for modifying large language model (LLM) behaviors. Existing methods typically intervene at the block level, steering the bundled activations of selected attention heads, feedforward networks, or residual streams. However, we reveal that block-level activations are inherently heterogeneous, entangling beneficial, irrelevant, and harmful features, thereby rendering block-level steering coarse, inefficient, and intrusive. To investigate the root cause, we decompose block activations into fine-grained atomic unit (AU)-level activations, where each AU-level activation corresponds to a single dimension of the block activation, and each AU denotes a slice of the block weight matrix. Steering an AU-level activation is thus equivalent to steering its associated AU. Our theoretical and empirical analysis show that heterogeneity arises because different AUs or dimensions control distinct token distributions in LLM outputs. Hence, block-level steering inevitably moves helpful and harmful token directions together, which reduces efficiency. Restricting intervention to beneficial AUs yields more precise and effective steering. Building on this insight, we propose AUSteer, a simple and efficient method that operates at a finer granularity of the AU level. AUSteer first identifies discriminative AUs globally by computing activation momenta on contrastive samples. It then assigns adaptive steering strengths tailored to diverse inputs and selected AU activations. Comprehensive experiments on multiple LLMs and tasks show that AUSteer consistently surpasses advanced baselines while steering considerably fewer activations, demonstrating that steering less achieves more.

</details>


### [103] [No One-Size-Fits-All: Building Systems For Translation to Bashkir, Kazakh, Kyrgyz, Tatar and Chuvash Using Synthetic And Original Data](https://arxiv.org/abs/2602.04442)
*Dmitry Karpov*

Main category: cs.CL

TL;DR: 本文研究了五种突厥语对数对的机器翻译，通过在合成数据上对nllb-200-distilled-600M微调或使用检索到的相似示例对DeepSeek-V3.2的提示实现了不同的chrF++分数，还发布了数据集和权重。


<details>
  <summary>Details</summary>
Motivation: 为了提高突厥语对数对机器翻译的质量，探索了不同的方法和技术。

Method: 使用nllb-200-distilled-600M模型在合成数据上微调，以及对DeepSeek-V3.2进行提示，使用检索到的相似示例。

Result: 在Kazakh和Bashkir上的chrF++分数分别为49.71和46.94；Chuvash上的39.47；Tatar上的41.6；Kyrgyz上的45.6。

Conclusion: 本文通过不同的方法改进了突厥语对数对的机器翻译效果，并发布了相关数据集和模型权重。

Abstract: We explore machine translation for five Turkic language pairs: Russian-Bashkir, Russian-Kazakh, Russian-Kyrgyz, English-Tatar, English-Chuvash. Fine-tuning nllb-200-distilled-600M with LoRA on synthetic data achieved chrF++ 49.71 for Kazakh and 46.94 for Bashkir. Prompting DeepSeek-V3.2 with retrieved similar examples achieved chrF++ 39.47 for Chuvash. For Tatar, zero-shot or retrieval-based approaches achieved chrF++ 41.6, while for Kyrgyz the zero-shot approach reached 45.6. We release the dataset and the obtained weights.

</details>


### [104] [Is Micro Domain-Adaptive Pre-Training Effective for Real-World Operations? Multi-Step Evaluation Reveals Potential and Bottlenecks](https://arxiv.org/abs/2602.04466)
*Masaya Tsunokake,Yuta Koreeda,Terufumi Morishita,Koichi Nagatsuka,Hikaru Tomonari,Yasuhiro Sogawa*

Main category: cs.CL

TL;DR: 本文研究了微领域适配预训练（mDAPT）在生成任务中的应用，通过将回答过程分解为事实提取、推理和组合三个子任务来验证其效果。


<details>
  <summary>Details</summary>
Motivation: 之前的mDAPT研究仅在多项选择题上进行了评估，未证明其生成任务在真实业务中的有效性。本文旨在揭示mDAPT在生成任务中的潜力与瓶颈。

Method: 作者将回答问题的过程分解成三个子任务：根据问题提取相关事实、基于事实进行推理和组合生成长文本答案。并通过实际的技术支持操作中的专属IT产品知识来验证mDAPT的效果。

Result: 通过实验发现，mDAPT在提取事实的任务上表现良好，解决了基础模型的困扰，但在推理和组合生成答案的任务上未见改善。

Conclusion: 研究表明mDAPT在知识方面有成效，但在其他方面存在瓶颈。进一步分析表明，解决提取与推理任务确保了足够的性能（大于90%），这强调了提高推理能力的需求。

Abstract: When applying LLMs to real-world enterprise operations, LLMs need to handle proprietary knowledge in small domains of specific operations ($\textbf{micro domains}$). A previous study shows micro domain-adaptive pre-training ($\textbf{mDAPT}$) with fewer documents is effective, similarly to DAPT in larger domains. However, it evaluates mDAPT only on multiple-choice questions; thus, its effectiveness for generative tasks in real-world operations remains unknown. We aim to reveal the potential and bottlenecks of mDAPT for generative tasks. To this end, we disentangle the answering process into three subtasks and evaluate the performance of each subtask: (1) $\textbf{eliciting}$ facts relevant to questions from an LLM's own knowledge, (2) $\textbf{reasoning}$ over the facts to obtain conclusions, and (3) $\textbf{composing}$ long-form answers based on the conclusions. We verified mDAPT on proprietary IT product knowledge for real-world questions in IT technical support operations. As a result, mDAPT resolved the elicitation task that the base model struggled with but did not resolve other subtasks. This clarifies mDAPT's effectiveness in the knowledge aspect and its bottlenecks in other aspects. Further analysis empirically shows that resolving the elicitation and reasoning tasks ensures sufficient performance (over 90%), emphasizing the need to enhance reasoning capability.

</details>


### [105] [Beyond Unimodal Shortcuts: MLLMs as Cross-Modal Reasoners for Grounded Named Entity Recognition](https://arxiv.org/abs/2602.04486)
*Jinlong Ma,Yu Zhang,Xuefeng Bai,Kehai Chen,Yuwei Wang,Zeming Liu,Jun Yu,Min Zhang*

Main category: cs.CL

TL;DR: 本文探讨了利用多模态大型语言模型（MLLMs）进行端到端Grounded Multimodal Named Entity Recognition（GMNER）的可能性，并提出了一种模态感知一致性推理（MCR）方法来缓解模型的模态偏见。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常将多模态大型语言模型作为级联流水线中的辅助工具，本文的目标在于探索其在GMNER中的端到端应用，并解决MLLMs的模态偏见问题。

Method: 提出了模态感知一致性推理（MCR），其中包含了多风格推理模式注入（MRSI）和约束引导验证优化（CVO）。MRSI将抽象约束转换为可执行的推理链，CVO能够使模型动态调整其推理路径与组相对策略优化（GRPO）。

Result: 实验结果表明，MCR有效缓解了模态偏见，相较于现有基准模型在GMNER和视觉语义对接任务上表现出更优秀的性能。

Conclusion: 本研究证明了MCR在GCNER任务中的有效性，并为利用MLLMs解决模态偏见提供了新思路。

Abstract: Grounded Multimodal Named Entity Recognition (GMNER) aims to extract text-based entities, assign them semantic categories, and ground them to corresponding visual regions. In this work, we explore the potential of Multimodal Large Language Models (MLLMs) to perform GMNER in an end-to-end manner, moving beyond their typical role as auxiliary tools within cascaded pipelines. Crucially, our investigation reveals a fundamental challenge: MLLMs exhibit $\textbf{modality bias}$, including visual bias and textual bias, which stems from their tendency to take unimodal shortcuts rather than rigorous cross-modal verification. To address this, we propose Modality-aware Consistency Reasoning ($\textbf{MCR}$), which enforces structured cross-modal reasoning through Multi-style Reasoning Schema Injection (MRSI) and Constraint-guided Verifiable Optimization (CVO). MRSI transforms abstract constraints into executable reasoning chains, while CVO empowers the model to dynamically align its reasoning trajectories with Group Relative Policy Optimization (GRPO). Experiments on GMNER and visual grounding tasks demonstrate that MCR effectively mitigates modality bias and achieves superior performance compared to existing baselines.

</details>


### [106] [Deconstructing sentence disambiguation by joint latent modeling of reading paradigms: LLM surprisal is not enough](https://arxiv.org/abs/2602.04489)
*Dario Paape,Tal Linzen,Shravan Vasishth*

Main category: cs.CL

TL;DR: 文章通过使用暂时性歧义的花园通道句，提出了一种潜过程混合模型，该模型区分了花园通道概率、花园通道代价和再分析代价，通过考虑不专注的阅读行为获得更实际的处理代价估计。


<details>
  <summary>Details</summary>
Motivation: 作者希望通过使用暂时性歧义的花园通道句为人类阅读行为建模，以便更准确地理解和预测阅读过程中的各种特性。

Method: 文章采用了一个包含花园通道概率、花园通道代价和再分析代价的潜过程混合模型，同时考虑了不专注的阅读行为，这与四种实验范式（眼球追踪、单双向自我节奏阅读、Maze）相结合使用。

Result: 该模型能够复制现实中的重读行为、理解问题回应和语法判断模式，并且通过交叉验证，该混合模型在预测人类阅读模式和试验末尾任务数据方面比基于GPT-2衍生概率值的混合自由模型有更好的拟合度。

Conclusion: 实验结果表明，潜过程混合模型对理解人类阅读行为具有重要价值，并为未来的研究提供了启示。

Abstract: Using temporarily ambiguous garden-path sentences ("While the team trained the striker wondered ...") as a test case, we present a latent-process mixture model of human reading behavior across four different reading paradigms (eye tracking, uni- and bidirectional self-paced reading, Maze). The model distinguishes between garden-path probability, garden-path cost, and reanalysis cost, and yields more realistic processing cost estimates by taking into account trials with inattentive reading. We show that the model is able to reproduce empirical patterns with regard to rereading behavior, comprehension question responses, and grammaticality judgments. Cross-validation reveals that the mixture model also has better predictive fit to human reading patterns and end-of-trial task data than a mixture-free model based on GPT-2-derived surprisal values. We discuss implications for future work.

</details>


### [107] [PersoDPO: Scalable Preference Optimization for Instruction-Adherent, Persona-Grounded Dialogue via Multi-LLM Evaluation](https://arxiv.org/abs/2602.04493)
*Saleh Afzoon,MohammadHossein Ahmadi,Usman Naseem,Amin Beheshti*

Main category: cs.CL

TL;DR: PersoDPO 是一个可扩展的偏好优化框架，通过自动化评估生成的响应来微调对话模型，从而改善语言模型在个人化和上下文连贯性方面的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的大规模语言模型虽然在通用对话能力上表现优秀，但在个人化和上下文连贯性方面仍存在问题，因此需要一种新的方法来提升这些模型。

Method: PersoDPO 使用自动评估生成的对话模型响应的监督信号进行微调，结合评价指标关注连贯性和个人化，并引入长度和格式合规性特征以促进指令遵守。

Result: PersoDPO 在使用开放源代码语言模型进行实验时，比强大的开放源代码基线和标准直接偏好优化(DPO)变体在多个评估维度上表现更好。

Conclusion: PersoDPO 提供了一种可扩展的方法来优化对话模型，有助于提高开放源代码和封闭源代码语言模型在个人化和上下文连贯性方面的表现。

Abstract: Personalization and contextual coherence are two essential components in building effective persona-grounded dialogue systems. These aspects play a crucial role in enhancing user engagement and ensuring responses are more relevant and consistent with user identity. However, recent studies indicate that open-source large language models (LLMs) continue to struggle to generate responses that are both contextually grounded and aligned with persona cues, despite exhibiting strong general conversational abilities like fluency and naturalness. We present PersoDPO, a scalable preference optimisation framework that uses supervision signals from automatic evaluations of responses generated by both closed-source and open-source LLMs to fine-tune dialogue models. The framework integrates evaluation metrics targeting coherence and personalization, along with a length-format compliance feature to promote instruction adherence. These signals are combined to automatically construct high-quality preference pairs without manual annotation, enabling a scalable and reproducible training pipeline. Experiments on the FoCus dataset show that an open-source language model fine-tuned with the PersoDPO framework consistently outperforms strong open-source baselines and a standard Direct Preference Optimization (DPO) variant across multiple evaluation dimensions.

</details>


### [108] [Model-Dowser: Data-Free Importance Probing to Mitigate Catastrophic Forgetting in Multimodal Large Language Models](https://arxiv.org/abs/2602.04509)
*Hyeontaek Hwang,Nguyen Dinh Son,Daeyoung Kim*

Main category: cs.CL

TL;DR: 提出了Model-Dowser，一种新颖的稀疏微调方法，用于多模态大型语言模型(MLLMs)，通过联合考虑权重大小、输入激活和输出敏感性来测量每个模型参数的重要性分数。


<details>
  <summary>Details</summary>
Motivation: 在对任务特定数据进行微调以提升下游应用性能的同时，MLLMs可能会在预训练任务上的泛化性能下降，被称为灾难性遗忘现象。现有方法要么在微调语言解码器的深层时变得无效，要么随模型规模增加而无法很好地扩展。

Method: Model-Dowser 方法通过计算每个参数的标准化重要性分数来选择性地保留高重要性参数并在其余参数上进行微调。计算重要性分数时，综合考虑权重大小、输入激活和输出敏感性。

Result: 在两个代表性 MLLMs LLaVA 和 NVILA 上进行全面实验表明，Model-Dowser 在减轻灾难性遗忘方面非常有效，并且始终优于先前的方法，同时保持资源效率和可扩展性。

Conclusion: Model-Dowser 是一种有效的稀疏微调方法，可减轻 MLLMs 的灾难性遗忘，同时保持高效资源利用率和可扩展性，适用于大模型。

Abstract: Fine-tuning Multimodal Large Language Models (MLLMs) on task-specific data is an effective way to improve performance on downstream applications. However, such adaptation often leads to a degradation in generalization on pretrained tasks, a phenomenon known as Catastrophic Forgetting. Existing methods that aim to mitigate this issue either become ineffective when fine-tuning deeper layers of the language decoder or scale poorly with increasing model size. To address these limitations, we propose Model-Dowser, a novel sparse fine-tuning approach for MLLMs. Model-Dowser measures a principled importance score for each model parameter with respect to pretrained generalization (prior to downstream adaptation) by jointly considering weight magnitudes, input activations, and output sensitivities. During fine-tuning, Model-Dowser selectively preserves high-importance parameters and updates the remaining. Comprehensive experiments on two representative MLLMs, LLaVA and NVILA, demonstrate that Model-Dowser effectively mitigates catastrophic forgetting and consistently outperforms prior methods, while remaining resource-efficient and scalable to multi-billion-parameter models.

</details>


### [109] [$C$-$ΔΘ$: Circuit-Restricted Weight Arithmetic for Selective Refusal](https://arxiv.org/abs/2602.04521)
*Aditya Kasliwal,Pratinav Seth,Vinay Kumar Sankarapu*

Main category: cs.CL

TL;DR: 提出了一种名为C-Δθ的方法，通过离线计算得到限制于电路的权重更新，用于分类特定的拒绝策略，实现了从每次请求的干预到一次性的离线更新的成本转移。


<details>
  <summary>Details</summary>
Motivation: 低延迟和高效率地在大语言模型（LLM）中实施安全策略的需求增长，但现有方法依赖于推理时的干预，导致成本重复和复杂性。

Method: C-Δθ方法利用EAP-IG对拒绝对因计算进行稀疏化处理，并仅在一个限制于电路的权重更新中支持该计算。此权重更新ΔθC通常只影响不到5%的参数。

Result: C-Δθ方法能够在不使用推理时的钩子的情况下，提供分类特定的选择性，并将成本从每次请求的干预转移到一次性离线更新。

Conclusion: 研究展示了C-Δθ在拒绝类别目标选择性以及保持效用方面的优越性，证明了将安全策略的实施从每次请求转移到离线更新的可行性。

Abstract: Modern deployments require LLMs to enforce safety policies at scale, yet many controls rely on inference-time interventions that add recurring compute cost and serving complexity. Activation steering is widely used, but it requires runtime hooks and scales cost with the number of generations; conditional variants improve selectivity by gating when steering is applied but still retain an inference-time control path. We ask whether selective refusal can be moved entirely offline: can a mechanistic understanding of category-specific refusal be distilled into a circuit-restricted weight update that deploys as a standard checkpoint? We propose C-Δθ: Circuit Restricted Weight Arithmetic, which (i) localizes refusal-causal computation as a sparse circuit using EAP-IG and (ii) computes a constrained weight update ΔθC supported only on that circuit (typically <5% of parameters). Applying ΔθC yields a drop-in edited checkpoint with no inference-time hooks, shifting cost from per-request intervention to a one-time offline update. We evaluate category-targeted selectivity and capability retention on refusal and utility benchmarks.

</details>


### [110] [LycheeDecode: Accelerating Long-Context LLM Inference via Hybrid-Head Sparse Decoding](https://arxiv.org/abs/2602.04541)
*Gang Lin,Dongfang Li,Zhuoen Chen,Yukun Shi,Xuhui Chen,Baotian Hu,Min Zhang*

Main category: cs.CL

TL;DR: LycheeDecode 提出了一种高效的解码方法，通过细粒度的混合头注意力机制结合硬件高效的 top-k 选择策略，实现了与全注意力基线相当甚至更好的生成质量，同时在 128K 上文长度下取得了最高 2.7 倍的加速。


<details>
  <summary>Details</summary>
Motivation: 为了克服长上下文语言模型解码过程中因不断扩大的键值缓存而产生的内存和延迟成本，尤其是在使用共享关键令牌的粗粒度共享方法时，忽略了注意力头的功能多样性，影响了模型性能。

Method: LycheeDecode 采用了一种细粒度的混合头注意力机制，结合了硬件高效的 top-k 选择策略。它将注意力头分为检索头和稀疏头，检索头动态识别关键令牌，稀疏头重新使用这些令牌进行高效计算。

Result: 在多个基准测试（如 LongBench、RULER 等）和复杂推理任务（如 AIME24、OlympiadBench 等）上，LycheeDecode 实现了与全注意力基线相当甚至更好的生成质量，并且在 128K 上文长度下取得了最高 2.7 倍的加速。

Conclusion: LycheeDecode 提供了一种有效且高效的方法来实现长上下文 LLM 推断，通过保持注意力头的功能多样性，解决了现有方法的性能瓶颈。

Abstract: The proliferation of long-context large language models (LLMs) exposes a key bottleneck: the rapidly expanding key-value cache during decoding, which imposes heavy memory and latency costs. While recent approaches attempt to alleviate this by sharing a single set of crucial tokens across layers, such coarse-grained sharing undermines model performance by neglecting the functional diversity of attention heads. To address this, we propose LycheeDecode, an efficient decoding method centered on a fine-grained hybrid-head attention mechanism that employs a hardware-efficient top-k selection strategy. Specifically, the novel HardKuma-based mechanism partitions attention heads into a small subset of retrieval heads that dynamically identify crucial tokens and a majority of sparse heads that reuse them for efficient computation. Through extensive experiments on leading models like Llama3 and Qwen3 across diverse benchmarks for long-context understanding (e.g., LongBench, RULER) and complex reasoning (e.g., AIME24, OlympiadBench), we demonstrate that LycheeDecode achieves generative quality comparable to, and at times surpassing even the full-attention baseline. Crucially, this is accomplished with up to a 2.7x speedup at a 128K context length. By preserving the functional diversity of attention heads, our fine-grained strategy overcomes the performance bottlenecks of existing methods, providing a powerful and validated pathway to both efficient and high-quality long-context LLM inference.

</details>


### [111] [Rethinking Weight Tying: Pseudo-Inverse Tying for Stable LM Training and Updates](https://arxiv.org/abs/2602.04556)
*Jian Gu,Aldeida Aleti,Chunyang Chen,Hongyu Zhang*

Main category: cs.CL

TL;DR: PIT 提出了一种同步嵌入和反嵌入的方法，确保在训练中保持伪逆一致的接口，从而提高了训练稳定性，增强了逐层语义一致性和减少了副作用。


<details>
  <summary>Details</summary>
Motivation: 传统的权重共享可能在训练过程中导致编码和解码之间的对应关系漂移，使模型的可预测性降低。

Method: PIT 方法通过共享一个正交的latent token 内存，并结合薄极分解初始化或随机正交初始化，以及通过Cholesky因子参数化的全学习对称正定隐藏空间变换，来确保嵌入和反嵌入的同步。

Result: PIT 在设备上大小从 256M 到 1.3B 参数的模型上进行评估时，发现训练稳定性提高，逐层语义一致性增强，副作用显著减少。

Conclusion: PIT 提出了一种新的机制来保持嵌入和反嵌入的一致性，从而在紧凑的语言模型中提高了训练的稳定性和结果质量。

Abstract: Weight tying is widely used in compact language models to reduce parameters by sharing the token table between the input embedding and the output projection. However, weight sharing does not guarantee a stable token interface: during training, the correspondence between encoding tokens into hidden states and decoding hidden states into logits can drift, worsening optimization sensitivity and making post-training interventions such as editing, patching, and lightweight adaptation less predictable. We propose Pseudo-Inverse Tying (PIT), which synchronizes embedding and unembedding as coupled projections of a shared latent token memory, guaranteeing a pseudo-inverse-consistent interface throughout training. PIT maintains an orthonormal shared memory, obtained by thin polar decomposition for teacher initialization or random orthonormal initialization from scratch, and introduces a fully learned symmetric positive definite hidden-space transform parameterized via a Cholesky factor. The output head applies this transform to hidden states before the vocabulary projection, while the embedding applies the inverse transform to token vectors using stable triangular solves, avoiding explicit pseudo-inverse recomputation and any vocabulary-sized auxiliary parameters. We evaluate PIT on on-device models spanning 256M-1.3B parameters across pretraining and adaptation, and consistently observe improved training stability, stronger layerwise semantic consistency, and substantially reduced side effects.

</details>


### [112] [Textual Planning with Explicit Latent Transitions](https://arxiv.org/abs/2602.04557)
*Eliezer Shlomi,Ido Levy,Eilam Shapira,Michael Katz,Guy Uziel,Segev Shlomov,Nir Mashkif,Roi Reichart,Sarah Keren*

Main category: cs.CL

TL;DR: EmbedPlan通过在冻结的语言嵌入空间中使用轻量级的过渡模型来替代自回归的下一步状态生成，实现了快速的规划计算，但在跨域转移学习上仍存在问题。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM规划方法受限于逐令牌生成和完整的前向传递，导致多步前瞻和基于模拟搜索成本高昂。EmbedPlan旨在通过冻结的语言嵌入空间中的轻量级过渡模型来解决这些限制。

Method: EmbedPlan使用自然语言状态和动作描述生成向量，预测下一个状态嵌入并通过最近邻相似性检索下一个状态，从而实现快速规划计算。

Result: EmbedPlan在九个经典规划领域中实现了接近完美的内插性能，但在需要向未见过的问题或领域迁移时性能急剧下降；计划变体评估表明它更能学习替代计划而非记忆路径。

Conclusion: 虽然冻结嵌入支持在单个领域内的动力学习，但跨域迁移仍然是一项挑战。

Abstract: Planning with LLMs is bottlenecked by token-by-token generation and repeated full forward passes, making multi-step lookahead and rollout-based search expensive in latency and compute. We propose EmbedPlan, which replaces autoregressive next-state generation with a lightweight transition model operating in a frozen language embedding space. EmbedPlan encodes natural language state and action descriptions into vectors, predicts the next-state embedding, and retrieves the next state by nearest-neighbor similarity, enabling fast planning computation without fine-tuning the encoder. We evaluate next-state prediction across nine classical planning domains using six evaluation protocols of increasing difficulty: interpolation, plan-variant, extrapolation, multi-domain, cross-domain, and leave-one-out. Results show near-perfect interpolation performance but a sharp degradation when generalization requires transfer to unseen problems or unseen domains; plan-variant evaluation indicates generalization to alternative plans rather than memorizing seen trajectories. Overall, frozen embeddings support within-domain dynamics learning after observing a domain's transitions, while transfer across domain boundaries remains a bottleneck.

</details>


### [113] [Can LLMs capture stable human-generated sentence entropy measures?](https://arxiv.org/abs/2602.04570)
*Estrella Pivel-Villanueva,Elisabeth Frederike Sterner,Franziska Knolle*

Main category: cs.CL

TL;DR: 研究通过分析两个大型公开的填空数据集，探讨了词汇层面熵估算的稳定性，指出大多数句子在111（德语）和81（英语）个响应后达到稳定，且不同熵值的句子所需响应数不同。此外，研究比较了多种语言模型（LLMs）与人类熵值的对应性，发现尽管有差异，但LLMs仍能有效估算人类熵，但不具备与稳定人类熵完全一致的能力。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探讨词汇层面熵估算所需的样本量，验证常用规范化实践的有效性，以及不同语言模型（LLMs）估算人类熵的准确度。

Method: 通过使用两个大型公开的文本填充数据集进行Bootstrap置信区间分析，跟踪样本量对熵估算稳定性的影响。此外，比较了多种语言模型（LLMs）从logit基概率抽取和采样基频率估算中得到的人类熵值的对应性。

Result: 结果显示，大多数句子在德语中需要111个响应、英语中需要81个响应达到稳定熵值；LLMs在从logit基概率抽取中表现出最小的绝对误差，而在捕捉人类的变异度方面，基于采样的估算表现更好。

Conclusion: 研究提供了人类规范化实践的直接实证验证，表明稳定性高度依赖于句子的可预测性。同时，研究表明虽然LLMs能够很好地估算人类熵，但不能完全替代稳定的基于人类的数据分布。

Abstract: Predicting upcoming words is a core mechanism of language comprehension and may be quantified using Shannon entropy. There is currently no empirical consensus on how many human responses are required to obtain stable and unbiased entropy estimates at the word level. Moreover, large language models (LLMs) are increasingly used as substitutes for human norming data, yet their ability to reproduce stable human entropy remains unclear. Here, we address both issues using two large publicly available cloze datasets in German 1 and English 2. We implemented a bootstrap-based convergence analysis that tracks how entropy estimates stabilize as a function of sample size. Across both languages, more than 97% of sentences reached stable entropy estimates within the available sample sizes. 90% of sentences converged after 111 responses in German and 81 responses in English, while low-entropy sentences (<1) required as few as 20 responses and high-entropy sentences (>2.5) substantially more. These findings provide the first direct empirical validation for common norming practices and demonstrate that convergence critically depends on sentence predictability. We then compared stable human entropy values with entropy estimates derived from several LLMs, including GPT-4o, using both logit-based probability extraction and sampling-based frequency estimation, GPT2-xl/german-GPT-2, RoBERTa Base/GottBERT, and LLaMA 2 7B Chat. GPT-4o showed the highest correspondence with human data, although alignment depended strongly on the extraction method and prompt design. Logit-based estimates minimized absolute error, whereas sampling-based estimates were better in capturing the dispersion of human variability. Together, our results establish practical guidelines for human norming and show that while LLMs can approximate human entropy, they are not interchangeable with stable human-derived distributions.

</details>


### [114] [Semantic Self-Distillation for Language Model Uncertainty](https://arxiv.org/abs/2602.04577)
*Edward Phillips,Sean Wu,Boyan Gao,David A. Clifton*

Main category: cs.CL

TL;DR: 该论文提出了一种称为 Semantic Self-Distillation (SSD) 的技术，通过轻量级学生模型估算在生成答案之前的语言模型的条件不确定性。该方法在处理复杂输出空间中的预测不确定性方面提供了有效的信号，并解决了实际应用中的延迟问题。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型在延迟敏感的应用中无法实现高效的不确定性量化，因此需要一种新的方法来解决这一问题。

Method: 提出了一种名为 Semantic Self-Distillation (SSD) 的技术。学生模型会预测可能答案的语义分布，通过熵估计条件不确定性，通过概率密度评估候选答案的可靠性。

Result: 在 TriviaQA 数据集上，学生模型的性能与有限样本的语义分散相当或更好，SSD 提供了可靠的幻觉预测信号，并在来自不同领域的答案检测中表现出色。

Conclusion: SSD 提供了一种在复杂输出空间中提取预测不确定性的一般框架，并且可在延迟敏感的应用环境中有效实施。

Abstract: Large language models present challenges for principled uncertainty quantification, in part due to their complexity and the diversity of their outputs. Semantic dispersion, or the variance in the meaning of sampled answers, has been proposed as a useful proxy for model uncertainty, but the associated computational cost prohibits its use in latency-critical applications. We show that sampled semantic distributions can be distilled into lightweight student models which estimate a prompt-conditioned uncertainty before the language model generates an answer token. The student model predicts a semantic distribution over possible answers; the entropy of this distribution provides an effective uncertainty signal for hallucination prediction, and the probability density allows candidate answers to be evaluated for reliability. On TriviaQA, our student models match or outperform finite-sample semantic dispersion for hallucination prediction and provide a strong signal for out-of-domain answer detection. We term this technique Semantic Self-Distillation (SSD), which we suggest provides a general framework for distilling predictive uncertainty in complex output spaces beyond language.

</details>


### [115] [Trust The Typical](https://arxiv.org/abs/2602.04581)
*Debargha Ganguly,Sreehari Sankar,Biyao Zhang,Vikash Singh,Kanan Gupta,Harshini Kavuru,Alan Luo,Weicong Chen,Warren Morningstar,Raghu Machiraju,Vipin Chaudhary*

Main category: cs.CL

TL;DR: T3框架通过将安全性视为异常检测问题，利用语义空间中可接受提示的分布来识别潜在威胁，实现了在多种场景下的高性能，并且一个单一的训练模型可以跨多个领域和语言应用。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM（Large Language Model）安全方法主要依赖于识别和阻止已知威胁的策略，但这种方法是脆弱且反应性的。而T3提出了一种新的方法，认为从根本上理解什么是安全的才是稳健的安全策略。

Method: T3框架通过学习语义空间中可接受提示的分布来将安全性视为异常检测问题，以此来识别潜在威胁。该模型不需要针对有害示例进行训练，从而减少了对特定数据集的依赖性。

Result: T3在18个跨领域基准测试中实现了出色的性能，减少了高达40倍的误报率，且只需要经过一次训练，就能在多个领域及语言环境下有效工作，甚至对大规模的工作负载在密集评估间隔内仅增加不到6%的计算开销。

Conclusion: T3提供了一种无需特定有害示例训练即可实现高度准确的安全检测的新方法，展示了其在实际生产环境中的潜力。

Abstract: Current approaches to LLM safety fundamentally rely on a brittle cat-and-mouse game of identifying and blocking known threats via guardrails. We argue for a fresh approach: robust safety comes not from enumerating what is harmful, but from deeply understanding what is safe. We introduce Trust The Typical (T3), a framework that operationalizes this principle by treating safety as an out-of-distribution (OOD) detection problem. T3 learns the distribution of acceptable prompts in a semantic space and flags any significant deviation as a potential threat. Unlike prior methods, it requires no training on harmful examples, yet achieves state-of-the-art performance across 18 benchmarks spanning toxicity, hate speech, jailbreaking, multilingual harms, and over-refusal, reducing false positive rates by up to 40x relative to specialized safety models. A single model trained only on safe English text transfers effectively to diverse domains and over 14 languages without retraining. Finally, we demonstrate production readiness by integrating a GPU-optimized version into vLLM, enabling continuous guardrailing during token generation with less than 6% overhead even under dense evaluation intervals on large-scale workloads.

</details>


### [116] [Beyond Holistic Scores: Automatic Trait-Based Quality Scoring of Argumentative Essays](https://arxiv.org/abs/2602.04604)
*Lucile Favero,Juan Antonio Pérez-Ortiz,Tanja Käser,Nuria Oliver*

Main category: cs.CL

TL;DR: 本文研究了基于特质的自动议论文评分方法，采用结构化上下文学习与小型开源语言模型以及基于BigBird的大监督模型相结合的方法，系统地评估了ASAP++数据集，并展示了通过明确建模评分序关系可以显著提高评分一致性。


<details>
  <summary>Details</summary>
Motivation: 传统的自动评分系统主要依赖于整体评分，这限制了其在教育中的应用价值，特别是在处理复杂文体如论证性写作时。该论文旨在探索能够提供可解释、特质级反馈的自动评分系统的设计。

Method: 论文采用了两种互补的建模范式：一种是基于结构性上下文学习的小型开源语言模型，用于在真实教育场景中的部署；另一种是基于BigBird的大监督模型，通过CORAL风格的序数回归形式进行优化，以应对长序列理解。

Result: 实验表明，通过明确建模评分序关系可以显著提高评分的一致性，优于大型语言模型、名义分类及回归基本模型。同时，小型开源语言模型在无需特定任务微调的情况下，对于推理导向的特质也表现出了竞争力，特别是在透明性、隐私保护和本地部署方面。

Conclusion: 这项研究强调，自动评分系统应与评分标准语义对齐以适应教育评估需求。此外，小型开源语言模型在部署中展现出的潜在优势也为人工智能教育系统的设计提供了新的思路，能为论证性写作提供可解释、与评分标准对齐的反馈。

Abstract: Automated Essay Scoring systems have traditionally focused on holistic scores, limiting their pedagogical usefulness, especially in the case of complex essay genres such as argumentative writing. In educational contexts, teachers and learners require interpretable, trait-level feedback that aligns with instructional goals and established rubrics. In this paper, we study trait-based Automatic Argumentative Essay Scoring using two complementary modeling paradigms designed for realistic educational deployment: (1) structured in-context learning with small open-source LLMs, and (2) a supervised, encoder-based BigBird model with a CORAL-style ordinal regression formulation, optimized for long-sequence understanding. We conduct a systematic evaluation on the ASAP++ dataset, which includes essay scores across five quality traits, offering strong coverage of core argumentation dimensions. LLMs are prompted with designed, rubric-aligned in-context examples, along with feedback and confidence requests, while we explicitly model ordinality in scores with the BigBird model via the rank-consistent CORAL framework. Our results show that explicitly modeling score ordinality substantially improves agreement with human raters across all traits, outperforming LLMs and nominal classification and regression-based baselines. This finding reinforces the importance of aligning model objectives with rubric semantics for educational assessment. At the same time, small open-source LLMs achieve a competitive performance without task-specific fine-tuning, particularly for reasoning-oriented traits, while enabling transparent, privacy-preserving, and locally deployable assessment scenarios. Our findings provide methodological, modeling, and practical insights for the design of AI-based educational systems that aim to deliver interpretable, rubric-aligned feedback for argumentative writing.

</details>


### [117] [RexBERT: Context Specialized Bidirectional Encoders for E-commerce](https://arxiv.org/abs/2602.04605)
*Rahul Bajaj,Anuj Garg*

Main category: cs.CL

TL;DR: RexBERT 是一种专为电子商务领域设计的 BERT 风格编码器，通过特定的预训练方法和高质量的领域数据，在领域特定任务中实现了卓越性能。


<details>
  <summary>Details</summary>
Motivation: 电子商务系统对实时性、稳定性和成本敏感，而通用预训练模型的领域数据覆盖不足影响了性能。

Method: RexBERT 通过 Ecom-niverse 数据库进行专门的预训练，包括：1、多模块化的数据处理管道从各种零售和购物源中提取电商内容；2、基于 ModernBERT 的先进结构的可重复预训练配方，涵盖三个阶段：通用预训练、上下文扩展和逐步领域特化。

Result: 使用 RexBERT 模型在电子商务数据集上的 token 分类、语义相似性和通用自然语言理解任务中表现优异，即使参数量比大型通用模型少 2-3 倍。

Conclusion: 领域特定的数据和有条理的训练方法比简单的无差别的规模扩展更有利于电子商务应用。

Abstract: Encoder-only transformers remain indispensable in retrieval, classification, and ranking systems where latency, stability, and cost are paramount. Most general purpose encoders, however, are trained on generic corpora with limited coverage of specialized domains. We introduce RexBERT, a family of BERT-style encoders designed specifically for e-commerce semantics. We make three contributions. First, we release Ecom-niverse, a 350 billion token corpus curated from diverse retail and shopping sources. We describe a modular pipeline that isolates and extracts e-commerce content from FineFineWeb and other open web resources, and characterize the resulting domain distribution. Second, we present a reproducible pretraining recipe building on ModernBERT's architectural advances. The recipe consists of three phases: general pre-training, context extension, and annealed domain specialization. Third, we train RexBERT models ranging from 17M to 400M parameters and evaluate them on token classification, semantic similarity, and general natural language understanding tasks using e-commerce datasets. Despite having 2-3x fewer parameters, RexBERT outperforms larger general-purpose encoders and matches or surpasses modern long-context models on domain-specific benchmarks. Our results demonstrate that high quality in-domain data combined with a principled training approach provides a stronger foundation for e-commerce applications than indiscriminate scaling alone.

</details>


### [118] [Disentangling meaning from language in LLM-based machine translation](https://arxiv.org/abs/2602.04613)
*Théo Lasnier,Armel Zebaze,Djamé Seddah,Rachel Bawden,Benoît Sagot*

Main category: cs.CL

TL;DR: 该研究从机制角度分析大规模语言模型在句级机器翻译中的表现，将翻译任务分解为两个子任务：目标语言识别和保留源句意义，结果显示不同的注意头在每个子任务中起作用，并且通过微调一小部分相关注意头，可以在没有指令的情况下达到与有指令提示相当的翻译性能。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型由于其规模限制了之前的解释方法仅限于词级分析，本研究旨在深入到句级，通过分析注意力头来理解大规模语言模型在机器翻译中的内部编码和分配翻译功能的方式。

Method: 研究将机器翻译任务分解成目标语言识别和保留源句子意义两个子任务，并通过分析不同模型和翻译方向下的注意力头来识别专长于不同子任务的稀疏注意力头，最后通过调整一小部分特定注意力头来展示机制解释力。

Result: 发现不同的注意力头在不同的子任务中起作用，并且只需调整1%的相关注意力头，就可以实现无需指令的机器翻译性能，与有指令提示相当，且去除这些头会对相应的翻译功能产生破坏性影响。

Conclusion: 该研究展示了机制解释在大规模语言模型句级翻译中的应用，为理解模型内部机制提供了一种新方法，并可能对改进模型性能带来新的启发。

Abstract: Mechanistic Interpretability (MI) seeks to explain how neural networks implement their capabilities, but the scale of Large Language Models (LLMs) has limited prior MI work in Machine Translation (MT) to word-level analyses. We study sentence-level MT from a mechanistic perspective by analyzing attention heads to understand how LLMs internally encode and distribute translation functions. We decompose MT into two subtasks: producing text in the target language (i.e. target language identification) and preserving the input sentence's meaning (i.e. sentence equivalence). Across three families of open-source models and 20 translation directions, we find that distinct, sparse sets of attention heads specialize in each subtask. Based on this insight, we construct subtask-specific steering vectors and show that modifying just 1% of the relevant heads enables instruction-free MT performance comparable to instruction-based prompting, while ablating these heads selectively disrupts their corresponding translation functions.

</details>


### [119] [LEAD: Layer-wise Expert-aligned Decoding for Faithful Radiology Report Generation](https://arxiv.org/abs/2602.04617)
*Ruixiao Yang,Yuanhe Tian,Xu Yang,Huiqi Li,Yan Song*

Main category: cs.CL

TL;DR: 本研究提出了一种名为LEAD的新方法，通过在LVLM的每个解码层中引入一个专家模块，动态调整解码偏见，从而改善临床准确性和减轻幻觉现象，同时保持高质量的生成结果。


<details>
  <summary>Details</summary>
Motivation: 现有的方法依赖于外部知识指导来实现生成文本与视觉信息的对齐，但忽视了预训练模型中的固有的解码先验和视觉-语言对齐偏见，同时缺乏鲁棒性。

Method: LEAD方法通过对LVLM的每个解码层应用专家模块，利用门控机制整合提取的病理特征，通过学习到的门控函数在每次推理步骤中咨询专家特征，动态校正解码偏见。

Result: 在多个公开数据集上的实验表明，LEAD方法在临床准确性指标上取得了有效的提升，减轻了幻觉现象，同时保持了高质量的生成结果。

Conclusion: LEAD方法通过改进LVLM的解码过程，提升报告的准确性和一致性，并展示了其在实际临床应用中的潜力。

Abstract: Radiology Report Generation (RRG) aims to produce accurate and coherent diagnostics from medical images. Although large vision language models (LVLM) improve report fluency and accuracy, they exhibit hallucinations, generating plausible yet image-ungrounded pathological details. Existing methods primarily rely on external knowledge guidance to facilitate the alignment between generated text and visual information. However, these approaches often ignore the inherent decoding priors and vision-language alignment biases in pretrained models and lack robustness due to reliance on constructed guidance. In this paper, we propose Layer-wise Expert-aligned Decoding (LEAD), a novel method to inherently modify the LVLM decoding trajectory. A multiple experts module is designed for extracting distinct pathological features which are integrated into each decoder layer via a gating mechanism. This layer-wise architecture enables the LLM to consult expert features at every inference step via a learned gating function, thereby dynamically rectifying decoding biases and steering the generation toward factual consistency. Experiments conducted on multiple public datasets demonstrate that the LEAD method yields effective improvements in clinical accuracy metrics and mitigates hallucinations while preserving high generation quality.

</details>


### [120] [Mapping the Web of Science, a large-scale graph and text-based dataset with LLM embeddings](https://arxiv.org/abs/2602.04630)
*Tim Kunt,Annika Buchholz,Imene Khebouri,Thorsten Koch,Ida Litzel,Thi Huong Vu*

Main category: cs.CL

TL;DR: 本文通过应用LLM嵌入模型，展示了它们在处理科学出版物数据集中的潜力，并揭示了一个由文本自结构化的景观。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探索文本数据中文字信息和链接关系的结合，并通过Web of Science数据集展示了LLM嵌入方法的应用。

Method: 通过应用LLM嵌入模型处理Web of Science数据集，分析字面上的文本信息及其与其他文本的关系。

Result: 结果揭示了一个由文字信息和链接关系各自为结构的文本景观。

Conclusion: 最终结果表明，LLM嵌入模型在大规模文本数据处理中的潜在价值，特别是在科学出版物领域的应用。

Abstract: Large text data sets, such as publications, websites, and other text-based media, inherit two distinct types of features: (1) the text itself, its information conveyed through semantics, and (2) its relationship to other texts through links, references, or shared attributes. While the latter can be described as a graph structure and can be handled by a range of established algorithms for classification and prediction, the former has recently gained new potential through the use of LLM embedding models. Demonstrating these possibilities and their practicability, we investigate the Web of Science dataset, containing ~56 million scientific publications through the lens of our proposed embedding method, revealing a self-structured landscape of texts.

</details>


### [121] [Outcome Accuracy is Not Enough: Aligning the Reasoning Process of Reward Models](https://arxiv.org/abs/2602.04649)
*Binghai Wang,Yantao Liu,Yuxuan Liu,Tianyi Tang,Shenzhi Wang,Chang Gao,Chujie Zheng,Yichang Zhang,Le Yu,Shixuan Liu,Tao Gui,Qi Zhang,Xuanjing Huang,Bowen Yu,Fei Huang,Junyang Lin*

Main category: cs.CL

TL;DR: 本文提出了一种新的细粒度度量方法——合理性一致性，有效地鉴别了前沿模型，并揭示了误导性的一致性。提出了一种结合合理性一致性与结果准确性的混合信号以改进GenRM的训练。在实验中，该方法取得了前沿的性能。


<details>
  <summary>Details</summary>
Motivation: 作者指出，现有的评判模型（如GenRMs和LLM）在追求结果准确性时，会导致误导性的对齐，即模型可能会给出正确的结果但出于错误的原因。这一对齐问题限制了模型在强化学习友好（RLHF）中的泛化能力。为了克服这个问题，作者提出了一个新的度量标准——合理性一致性，来更细致地衡量模型的推理过程与人类判断的一致性。

Method: 作者提出了一种混合信号的方法，结合了结果准确性和合理性一致性。这一方法在训练过程中嵌入了合理性一致性的反馈，以改进生成奖励模型的表现。

Result: 实验结果显示，该方法在RM-Bench和JudgeBench两个基准上都取得了领先的成绩，相对于仅使用结果准确性的基线，性能平均提高了5%。此外，方法还在创意写作任务上提高了7%的表现，并且证明了该方法能够避免误导性对齐的问题，恢复了因仅注重结果准确性而导致的合理性一致性下降。

Conclusion: 论文提出的方法不仅能够更好的鉴别模型表现，还有效解决了误导性对齐的问题，并通过实验证明了其在实际任务中的应用潜力。

Abstract: Generative Reward Models (GenRMs) and LLM-as-a-Judge exhibit deceptive alignment by producing correct judgments for incorrect reasons, as they are trained and evaluated to prioritize Outcome Accuracy, which undermines their ability to generalize during RLHF. We introduce Rationale Consistency, a fine-grained metric that quantifies the alignment between the model's reasoning process and human judgment. Our evaluation of frontier models reveals that rationale consistency effectively discriminates among state-of-the-art models and detects deceptive alignment, while outcome accuracy falls short in both respects. To mitigate this gap, we introduce a hybrid signal that combines rationale consistency with outcome accuracy for GenRM training. Our training method achieves state-of-the-art performance on RM-Bench (87.1%) and JudgeBench (82%), surpassing outcome-only baselines by an average of 5%. Using RM during RLHF, our method effectively improves performance as demonstrated on Arena Hard v2, notably yielding a 7% improvement in creative writing tasks. Further analysis confirms that our method escapes the deceptive alignment trap, effectively reversing the decline in rationale consistency observed in outcome-only training.

</details>


### [122] [Approaches to Semantic Textual Similarity in Slovak Language: From Algorithms to Transformers](https://arxiv.org/abs/2602.04659)
*Lukas Radosky,Miroslav Blstak,Matej Krajcovic,Ivan Polasek*

Main category: cs.CL

TL;DR: 本文对比评估了斯洛vak语句级语义文本相似度方法，包括传统算法、监督机器学习模型以及第三方深度学习工具，并通过人工蜂群优化进行特征选择和超参数调优。


<details>
  <summary>Details</summary>
Motivation: 斯洛伐克语等低资源语言的语义文本相似度（STS）任务较为困难，本文旨在评估不同STS方法在斯洛伐克语中的性能差异。

Method: 研究采用了传统算法、监督机器学习模型以及第三方深度学习工具进行评估。使用传统算法的输出作为特征，通过人工蜂群优化进行特征选择和超参数调优，并评估了多个第三方工具的性能。

Result: 研究结果表明不同方法之间存在权衡，展示了各种方法在斯洛伐克语STS任务中的表现。

Conclusion: 本文的研究结果为斯洛伐克语及其他低资源语言的语义文本相似度任务提供了一定的参考，指出了不同方法之间的优缺点。

Abstract: Semantic textual similarity (STS) plays a crucial role in many natural language processing tasks. While extensively studied in high-resource languages, STS remains challenging for under-resourced languages such as Slovak. This paper presents a comparative evaluation of sentence-level STS methods applied to Slovak, including traditional algorithms, supervised machine learning models, and third-party deep learning tools. We trained several machine learning models using outputs from traditional algorithms as features, with feature selection and hyperparameter tuning jointly guided by artificial bee colony optimization. Finally, we evaluated several third-party tools, including fine-tuned model by CloudNLP, OpenAI's embedding models, GPT-4 model, and pretrained SlovakBERT model. Our findings highlight the trade-offs between different approaches.

</details>


### [123] [Investigating Disability Representations in Text-to-Image Models](https://arxiv.org/abs/2602.04687)
*Yang Yian,Yu Fan,Liudmila Zavolokina,Sarah Ebling*

Main category: cs.CL

TL;DR: 本研究通过分析由Stable Diffusion XL和DALL-E 3生成的图像，借助结构化提示设计，探讨了不同类型的残疾在AI生成图像中的表现，并通过情感极性分析评估了减轻策略对残疾刻画的影响，揭示了持续的代表性失衡，强调了持续评估和改进生成模型以促进更多元和包容性残疾表现的必要性。


<details>
  <summary>Details</summary>
Motivation: 尽管文本到图像生成模型在生成高质量视觉内容方面取得了显著进展，但人们对这些模型如何表现社会群体的关注仍然不足。基于此，本研究重点调查了不同类型的残疾在AI生成图像中的表现，以促进更广泛和包容性残疾形象的生成。

Method: 研究使用了结构化提示设计分析Stable Diffusion XL和DALL-E 3的输出图片，并将通用的残疾提示与特定残疾类别提示生成的图像进行对比。同时，通过情感极性分析评估了减缓策略对残疾肖像的影响，结合了自动和人力评估。

Result: 研究发现存在持续的代表性失衡，显示出现有生成模型在表现残疾人方面存在不足。评估结果显示，尽管某些减轻策略有助于改善特定方面，但仍有改进空间。

Conclusion: 研究强调了持续评估和改进生成模型的重要性，以促进更加多样和包容性残疾形象的实现。

Abstract: Text-to-image generative models have made remarkable progress in producing high-quality visual content from textual descriptions, yet concerns remain about how they represent social groups. While characteristics like gender and race have received increasing attention, disability representations remain underexplored. This study investigates how people with disabilities are represented in AI-generated images by analyzing outputs from Stable Diffusion XL and DALL-E 3 using a structured prompt design. We analyze disability representations by comparing image similarities between generic disability prompts and prompts referring to specific disability categories. Moreover, we evaluate how mitigation strategies influence disability portrayals, with a focus on assessing affective framing through sentiment polarity analysis, combining both automatic and human evaluation. Our findings reveal persistent representational imbalances and highlight the need for continuous evaluation and refinement of generative models to foster more diverse and inclusive portrayals of disability.

</details>


### [124] [LinGO: A Linguistic Graph Optimization Framework with LLMs for Interpreting Intents of Online Uncivil Discourse](https://arxiv.org/abs/2602.04693)
*Yuan Zhang,Thales Bertaglia*

Main category: cs.CL

TL;DR: LinGO 是一种用于大型语言模型（LLMs）的 Linguistic Graph Optimization 框架，通过分解语言为多步骤的语法组件，识别导致错误的关键步骤，并迭代优化提示和/或示例组件来分类各种直接和间接的不文明意图。通过使用 RAG 优化技术与 Gemini 模型结合，LinGO 在评估的基准上显示出比零样本、因果思考、直接优化和微调基线更高的准确性和加权 F1 分数。


<details>
  <summary>Details</summary>
Motivation: 现有分类器经常误判含不文明线索但表达文明意图的帖子，导致在线上不文明行为的有害估计过大。LinGO 的目的是通过利用语言结构和优化技术，有效分类复杂的不文明多类别意图。

Method: LinGO 通过分解语言为多步骤的语法组件，识别导致分类错误的关键步骤，并迭代地优化提示和/或示例组件，以增强大型语言模型（LLMs）的分类能力。

Result: LinGO 在对 2022 年巴西总统选举期间收集的数据集评估中，包括四种政治不文明行为：不礼貌 (IMP)、仇恨言论和刻板印象 (HSST)、身体伤害和暴力政治修辞 (PHAVPR) 和对民主制度和价值观的威胁 (THREAT)，每个实例都标注了六种文明/不文明意图，使用三种成本效益高的大型语言模型（GPT-5-mini、Gemini 2.5 Flash-Lite 和 Claude 3 Haiku）和四种优化技术（TextGrad、AdalFlow、DSPy 和检索增强生成，简称 RAG），结果表明，在所有模型中，LinGO 在准确性上持续优于零样本、因果推理、直接优化和微调基准。

Conclusion: 研究发现，将多步骤语言组件纳入大型语言模型指令中，并优化关键组件，有助于模型解释复杂的语义含义。该方法可进一步推广到其他复杂的语义解释任务。

Abstract: Detecting uncivil language is crucial for maintaining safe, inclusive, and democratic online spaces. Yet existing classifiers often misinterpret posts containing uncivil cues but expressing civil intents, leading to inflated estimates of harmful incivility online. We introduce LinGO, a linguistic graph optimization framework for large language models (LLMs) that leverages linguistic structures and optimization techniques to classify multi-class intents of incivility that use various direct and indirect expressions. LinGO decomposes language into multi-step linguistic components, identifies targeted steps that cause the most errors, and iteratively optimizes prompt and/or example components for targeted steps. We evaluate it using a dataset collected during the 2022 Brazilian presidential election, encompassing four forms of political incivility: Impoliteness (IMP), Hate Speech and Stereotyping (HSST), Physical Harm and Violent Political Rhetoric (PHAVPR), and Threats to Democratic Institutions and Values (THREAT). Each instance is annotated with six types of civil/uncivil intent. We benchmark LinGO using three cost-efficient LLMs: GPT-5-mini, Gemini 2.5 Flash-Lite, and Claude 3 Haiku, and four optimization techniques: TextGrad, AdalFlow, DSPy, and Retrieval-Augmented Generation (RAG). The results show that, across all models, LinGO consistently improves accuracy and weighted F1 compared with zero-shot, chain-of-thought, direct optimization, and fine-tuning baselines. RAG is the strongest optimization technique and, when paired with Gemini model, achieves the best overall performance. These findings demonstrate that incorporating multi-step linguistic components into LLM instructions and optimize targeted components can help the models explain complex semantic meanings, which can be extended to other complex semantic explanation tasks in the future.

</details>


### [125] [ERNIE 5.0 Technical Report](https://arxiv.org/abs/2602.04705)
*Haifeng Wang,Hua Wu,Tian Wu,Yu Sun,Jing Liu,Dianhai Yu,Yanjun Ma,Jingzhou He,Zhongjun He,Dou Hong,Qiwen Liu,Shuohuan Wang,Junyuan Shang,Zhenyu Zhang,Yuchen Ding,Jinle Zeng,Jiabin Yang,Liang Shen,Ruibiao Chen,Weichong Yin,Siyu Ding,Dai Dai,Shikun Feng,Siqi Bao,Bolei He,Yan Chen,Zhenyu Jiao,Ruiqing Zhang,Zeyu Chen,Qingqing Dang,Kaipeng Deng,Jiajun Jiang,Enlei Gong,Guoxia Wang,Yanlin Sha,Yi Liu,Yehan Zheng,Weijian Xu,Jiaxiang Liu,Zengfeng Zeng,Yingqi Qu,Zhongli Li,Zhengkun Zhang,Xiyang Wang,Zixiang Xu,Xinchao Xu,Zhengjie Huang,Dong Wang,Bingjin Chen,Yue Chang,Xing Yuan,Shiwei Huang,Qiao Zhao,Xinzhe Ding,Shuangshuang Qiao,Baoshan Yang,Bihong Tang,Bin Li,Bingquan Wang,Binhan Tang,Binxiong Zheng,Bo Cui,Bo Ke,Bo Zhang,Bowen Zhang,Boyan Zhang,Boyang Liu,Caiji Zhang,Can Li,Chang Xu,Chao Pang,Chao Zhang,Chaoyi Yuan,Chen Chen,Cheng Cui,Chenlin Yin,Chun Gan,Chunguang Chai,Chuyu Fang,Cuiyun Han,Dan Zhang,Danlei Feng,Danxiang Zhu,Dong Sun,Dongbo Li,Dongdong Li,Dongdong Liu,Dongxue Liu,Fan Ding,Fan Hu,Fan Li,Fan Mo,Feisheng Wu,Fengwei Liu,Gangqiang Hu,Gaofeng Lu,Gaopeng Yong,Gexiao Tian,Guan Wang,Guangchen Ni,Guangshuo Wu,Guanzhong Wang,Guihua Liu,Guishun Li,Haibin Li,Haijian Liang,Haipeng Ming,Haisu Wang,Haiyang Lu,Haiye Lin,Han Zhou,Hangting Lou,Hanwen Du,Hanzhi Zhang,Hao Chen,Hao Du,Hao Liu,Hao Zhou,Haochen Jiang,Haodong Tian,Haoshuang Wang,Haozhe Geng,Heju Yin,Hong Chen,Hongchen Xue,Hongen Liu,Honggeng Zhang,Hongji Xu,Hongwei Chen,Hongyang Zhang,Hongyuan Zhang,Hua Lu,Huan Chen,Huan Wang,Huang He,Hui Liu,Hui Zhong,Huibin Ruan,Jiafeng Lu,Jiage Liang,Jiahao Hu,Jiahao Hu,Jiajie Yang,Jialin Li,Jian Chen,Jian Wu,Jianfeng Yang,Jianguang Jiang,Jianhua Wang,Jianye Chen,Jiaodi Liu,Jiarui Zhou,Jiawei Lv,Jiaxin Zhou,Jiaxuan Liu,Jie Han,Jie Sun,Jiefan Fang,Jihan Liu,Jihua Liu,Jing Hu,Jing Qian,Jing Yan,Jingdong Du,Jingdong Wang,Jingjing Wu,Jingyong Li,Jinheng Wang,Jinjin Li,Jinliang Lu,Jinlin Yu,Jinnan Liu,Jixiang Feng,Jiyi Huang,Jiyuan Zhang,Jun Liang,Jun Xia,Jun Yu,Junda Chen,Junhao Feng,Junhong Xiang,Junliang Li,Kai Liu,Kailun Chen,Kairan Su,Kang Hu,Kangkang Zhou,Ke Chen,Ke Wei,Kui Huang,Kun Wu,Kunbin Chen,Lei Han,Lei Sun,Lei Wen,Linghui Meng,Linhao Yu,Liping Ouyang,Liwen Zhang,Longbin Ji,Longzhi Wang,Meng Sun,Meng Tian,Mengfei Li,Mengqi Zeng,Mengyu Zhang,Ming Hong,Mingcheng Zhou,Mingming Huang,Mingxin Chen,Mingzhu Cai,Naibin Gu,Nemin Qiu,Nian Wang,Peng Qiu,Peng Zhao,Pengyu Zou,Qi Wang,Qi Xin,Qian Wang,Qiang Zhu,Qianhui Luo,Qianwei Yang,Qianyue He,Qifei Wu,Qinrui Li,Qiwen Bao,Quan Zhang,Quanxiang Liu,Qunyi Xie,Rongrui Zhan,Rufeng Dai,Rui Peng,Ruian Liu,Ruihao Xu,Ruijie Wang,Ruixi Zhang,Ruixuan Liu,Runsheng Shi,Ruting Wang,Senbo Kang,Shan Lu,Shaofei Yu,Shaotian Gong,Shenwei Hu,Shifeng Zheng,Shihao Guo,Shilong Fan,Shiqin Liu,Shiwei Gu,Shixi Zhang,Shuai Yao,Shuang Zhang,Shuangqiao Liu,Shuhao Liang,Shuwei He,Shuwen Yang,Sijun He,Siming Dai,Siming Wu,Siyi Long,Songhe Deng,Suhui Dong,Suyin Liang,Teng Hu,Tianchan Xu,Tianliang Lv,Tianmeng Yang,Tianyi Wei,Tiezhu Gao,Ting Sun,Ting Zhang,Tingdan Luo,Wei He,Wei Luan,Wei Yin,Wei Zhang,Wei Zhou,Weibao Gong,Weibin Li,Weicheng Huang,Weichong Dang,Weiguo Zhu,Weilong Zhang,Weiqi Tan,Wen Huang,Wenbin Chang,Wenjing Du,Wenlong Miao,Wenpei Luo,Wenquan Wu,Xi Shi,Xi Zhao,Xiang Gao,Xiangguo Zhang,Xiangrui Yu,Xiangsen Wang,Xiangzhe Wang,Xianlong Luo,Xianying Ma,Xiao Tan,Xiaocong Lin,Xiaofei Wang,Xiaofeng Peng,Xiaofeng Wu,Xiaojian Xu,Xiaolan Yuan,Xiaopeng Cui,Xiaotian Han,Xiaoxiong Liu,Xiaoxu Fei,Xiaoxuan Wu,Xiaoyu Wang,Xiaoyu Zhang,Xin Sun,Xin Wang,Xinhui Huang,Xinming Zhu,Xintong Yu,Xinyi Xu,Xinyu Wang,Xiuxian Li,XuanShi Zhu,Xue Xu,Xueying Lv,Xuhong Li,Xulong Wei,Xuyi Chen,Yabing Shi,Yafeng Wang,Yamei Li,Yan Liu,Yanfu Cheng,Yang Gao,Yang Liang,Yang Wang,Yang Wang,Yang Yang,Yanlong Liu,Yannian Fu,Yanpeng Wang,Yanzheng Lin,Yao Chen,Yaozong Shen,Yaqian Han,Yehua Yang,Yekun Chai,Yesong Wang,Yi Song,Yichen Zhang,Yifei Wang,Yifeng Guo,Yifeng Kou,Yilong Chen,Yilong Guo,Yiming Wang,Ying Chen,Ying Wang,Yingsheng Wu,Yingzhan Lin,Yinqi Yang,Yiran Xing,Yishu Lei,Yixiang Tu,Yiyan Chen,Yong Zhang,Yonghua Li,Yongqiang Ma,Yongxing Dai,Yongyue Zhang,Yu Ran,Yu Sun,Yu-Wen Michael Zhang,Yuang Liu,Yuanle Liu,Yuanyuan Zhou,Yubo Zhang,Yuchen Han,Yucheng Wang,Yude Gao,Yuedong Luo,Yuehu Dong,Yufeng Hu,Yuhui Cao,Yuhui Yun,Yukun Chen,Yukun Gao,Yukun Li,Yumeng Zhang,Yun Fan,Yun Ma,Yunfei Zhang,Yunshen Xie,Yuping Xu,Yuqin Zhang,Yuqing Liu,Yurui Li,Yuwen Wang,Yuxiang Lu,Zefeng Cai,Zelin Zhao,Zelun Zhang,Zenan Lin,Zezhao Dong,Zhaowu Pan,Zhaoyu Liu,Zhe Dong,Zhe Zhang,Zhen Zhang,Zhengfan Wu,Zhengrui Wei,Zhengsheng Ning,Zhenxing Li,Zhenyu Li,Zhenyu Qian,Zhenyun Li,Zhi Li,Zhichao Chen,Zhicheng Dong,Zhida Feng,Zhifan Feng,Zhihao Deng,Zhijin Yu,Zhiyang Chen,Zhonghui Zheng,Zhuangzhuang Guo,Zhujun Zhang,Zhuo Sun,Zichang Liu,Zihan Lin,Zihao Huang,Zihe Zhu,Ziheng Zhao,Ziping Chen,Zixuan Zhu,Ziyang Xu,Ziyi Liang,Ziyuan Gao*

Main category: cs.CL

TL;DR: ERNIE 5.0 是一种全自回归的统一多模态基础模型，能够在文本、图像、视频和音频等跨模态领域实现理解和生成。模型采用了统一的大规模下一个组token预测目标训练，并利用超稀疏混合专家架构实现了模态无关的专业路由。为了应对大规模部署中的资源约束问题，ERNEI 5.0 引入了一种新的弹性训练范式，能够在单一预训练运行中学习具有不同深度、专家容量和路由稀疏度的子模型，从而在内存或时间受限的情况下实现灵活的性能、模型大小和推理延迟之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 随着跨模态数据的不断增加，目前的多模态模型面临着处理效率和通用性的挑战。为了解决大规模部署场景下的资源约束问题，同时提高多模态数据处理的效率和通用性，本文提出了ERNEI 5.0模型。

Method: ERNEI 5.0采用了统一的大规模下一个组token预测目标训练，基于超稀疏混合专家架构实现模态无关的专业路由。为了解决大规模部署中的资源约束问题，模型采用了弹性训练范式，能够在单一预训练运行中学习具有不同深度、专家容量和路由稀疏度的子模型。

Result: ERNEI 5.0在多个模态下的实验表明，其实现了强大的平衡性能。据我们所知，ERNIE 5.0是第一个支持多模态理解和生成的万亿参数规模的统一自回归模型。

Conclusion: 文章提出了一种新型的统一多模态基础模型ERNEI 5.0，该模型利用高级架构和弹性训练范式，提高了处理效率和通用性，为后续研究提供了深刻的见解。

Abstract: In this report, we introduce ERNIE 5.0, a natively autoregressive foundation model desinged for unified multimodal understanding and generation across text, image, video, and audio. All modalities are trained from scratch under a unified next-group-of-tokens prediction objective, based on an ultra-sparse mixture-of-experts (MoE) architecture with modality-agnostic expert routing. To address practical challenges in large-scale deployment under diverse resource constraints, ERNIE 5.0 adopts a novel elastic training paradigm. Within a single pre-training run, the model learns a family of sub-models with varying depths, expert capacities, and routing sparsity, enabling flexible trade-offs among performance, model size, and inference latency in memory- or time-constrained scenarios. Moreover, we systematically address the challenges of scaling reinforcement learning to unified foundation models, thereby guaranteeing efficient and stable post-training under ultra-sparse MoE architectures and diverse multimodal settings. Extensive experiments demonstrate that ERNIE 5.0 achieves strong and balanced performance across multiple modalities. To the best of our knowledge, among publicly disclosed models, ERNIE 5.0 represents the first production-scale realization of a trillion-parameter unified autoregressive model that supports both multimodal understanding and generation. To facilitate further research, we present detailed visualizations of modality-agnostic expert routing in the unified model, alongside comprehensive empirical analysis of elastic training, aiming to offer profound insights to the community.

</details>


### [126] [LiteToken: Removing Intermediate Merge Residues From BPE Tokenizers](https://arxiv.org/abs/2602.04706)
*Yike Sun,Haotong Yang,Zhouchen Lin,Muhan Zhang*

Main category: cs.CL

TL;DR: 本文研究了BPE词汇表中的中间合并残存物，这些残存物虽然在学习过程中频繁出现，但在实际使用过程中很少被使用。这些低频残存物不仅浪费了词汇表容量，还会增加对异常或非典型输入的脆弱性。因此，提出了一种简单的LiteToken方法来移除这些残余物。


<details>
  <summary>Details</summary>
Motivation: 广泛使用的BPE分词器的行为虽然受到关注，但对其中间合并残存物的研究则相对较少。本文指出，这些低频残存物不仅浪费了宝贵的记忆容量，还会增加对异常或非典型输入的脆弱性，因此需要进行研究。

Method: 该研究首先系统地对这些现象进行了实证分析，覆盖了常见的各种分词器。在此基础上，提出了一种简单的LiteToken方法以移除这些残存物。此方法简单有效，因为受到影响的词很少使用，使得预训练模型通常可以适配这个修改的分词器而不需要额外的微调。

Result: 实验结果表明，使用LiteToken方法可以减少分词碎片化，减少参数数量，并提高对噪声或拼写错误输入的鲁棒性，同时保持总体性能。

Conclusion: 本文通过研究BPE词汇表中的中间合并残存物，并提出简单的LiteToken方法，展示了如何提高语言模型的鲁棒性和效率。

Abstract: Tokenization is fundamental to how language models represent and process text, yet the behavior of widely used BPE tokenizers has received far less study than model architectures and training. In this paper, we investigate intermediate merge residues in BPE vocabularies: tokens that are frequent during merge learning so that retained in the final vocabulary, but are mostly further merged and rarely emitted when tokenizing the corpus during tokenizer usage. Such low-frequency tokens not only waste vocabulary capacity but also increase vulnerability to adversarial or atypical inputs. We present a systematic empirical characterization of this phenomenon across commonly used tokenizers and introduce LiteToken, a simple method for removing residue tokens. Because the affected tokens are rarely used, pretrained models can often accommodate the modified tokenizer without additional fine-tuning. Experiments show that LiteToken reduces token fragmentation, reduces parameters, and improves robustness to noisy or misspelled inputs, while preserving overall performance.

</details>


### [127] [Linguistically Informed Evaluation of Multilingual ASR for African Languages](https://arxiv.org/abs/2602.04716)
*Fei-Yueh Chen,Lateef Adeleke,C. M. Downey*

Main category: cs.CL

TL;DR: 该研究通过弥补传统词错误率(WER)的不足，引入词位错误率(CER)和特征错误率(FER)，特别是针对含有声调的非洲语言，提出了声调感知的特征错误率(TER)，展示了在低单词级准确性的情况下，基于语音特征的错误率能够揭示更具语言意义的错误模式。


<details>
  <summary>Details</summary>
Motivation: 研究认为传统词错误率(WER)难以准确评估非洲语言自动语音识别模型的实际表现，因为它将语音、音调和其他语言错误合并成单一的词错误，而新提出的特征错误率(FER)和声调感知的特征错误率(TER)能够揭示更具有语言意义的错误模式。

Method: 研究评估了三种语音编码器在两种非洲语言上的表现，同时补充了词错误率(WER)、词位错误率(CER)以及特征错误率(FER)和声调感知的特征错误率(TER)，通过计算语音特征的错误来揭示语言模型的错误模式。

Result: 研究结果表明，模型在音素特征上的表现更好，而音调（尤其是中高音和下降音）是最具挑战性的特征。对于约鲁巴语，WER为0.788、CER为0.305、FER为0.151。通用语言模型对于贝梅语的表现也很好，尽管WER接近0，但FER仅为0.267。

Conclusion: 研究强调了特征错误率(FER)和声调感知的特征错误率(TER)在评估亚非语言自动语音识别模型表现上的有效性，指出词错误率(WER)这样的所有或无的评价方式掩盖了模型中的个体语音特征错误。

Abstract: Word Error Rate (WER) mischaracterizes ASR models' performance for African languages by combining phonological, tone, and other linguistic errors into a single lexical error. By contrast, Feature Error Rate (FER) has recently attracted attention as a viable metric that reveals linguistically meaningful errors in models' performance. In this paper, we evaluate three speech encoders on two African languages by complementing WER with CER, and FER, and add a tone-aware extension (TER). We show that by computing errors on phonological features, FER and TER reveal linguistically-salient error patterns even when word-level accuracy remains low. Our results reveal that models perform better on segmental features, while tones (especially mid and downstep) remain the most challenging features. Results on Yoruba show a striking differential in metrics, with WER=0.788, CER=0.305, and FER=0.151. Similarly for Uneme (an endangered language absent from pretraining data) a model with near-total WER and 0.461 CER achieves the relatively low FER of 0.267. This indicates model error is often attributable to individual phonetic feature errors, which is obscured by all-or-nothing metrics like WER.

</details>


### [128] ["Be My Cheese?": Cultural Nuance Benchmarking for Machine Translation in Multilingual LLMs](https://arxiv.org/abs/2602.04729)
*Madison Van Doren,Casey Ford,Jennifer Barajas,Cory Holland*

Main category: cs.CL

TL;DR: 该研究通过大规模人工评估基准，评估多方大型语言模型在文化本地化方面的机器翻译能力。研究发现现有基准主要关注词元级和语法准确性，而忽视了现实世界所需的文化和语用能力。评估结果显示在段落级别，节日、文化概念翻译效果较好，而习语、双关语翻译效果较差，需要改进跨语言的语用能力和评价标准。


<details>
  <summary>Details</summary>
Motivation: 现有机器翻译基准主要关注词元级和语法准确性，忽略了现实世界中的文化背景和语用能力，导致在真实场景下的翻译效果不佳。因此，该研究旨在开发一个大型人工评价基准，专门评估多语言大型语言模型在文化本地化方面的翻译性能，以促进这一领域的研究和发展。

Method: 研究基于前期研究对87个跨20语言的翻译进行评估，以及对7个多语言大型语言模型在15种目标语言上的评估。邀请5名母语为各个目标语言的评分者，对全文翻译和段落级别的文化细腻语言（习语、双关语、节日和文化嵌入概念）进行评分，使用0-3的有序评价尺度，段落评分时还包括未翻译的选项。

Result: 研究结果表明，整体评价质量在2.10/3和1.45/3之间波动，表明目前大型语言模型文化本地化能力还存在一定不足。假日和文化概念翻译效果要优于习语和双关语，后两者更有可能被完全翻译。这表明模型仍存在语言形式和文化内涵之间被忽视的差距。

Conclusion: 该研究发现了现有翻译模型在文化本地化方面存在的差距，并强调了需要更多关注跨语言语用能力和文化数据，以进一步提高翻译质量。这是首个专注于多语言文化细微层面翻译评估的人工标注基准，对未来相关研究具有重要意义。

Abstract: We present a large-scale human evaluation benchmark for assessing cultural localisation in machine translation produced by state-of-the-art multilingual large language models (LLMs). Existing MT benchmarks emphasise token-level and grammatical accuracy, but of ten overlook pragmatic and culturally grounded competencies required for real-world localisation. Building on a pilot study of 87 translations across 20 languages, we evaluate 7 multilingual LLMs across 15 target languages with 5 native-speaker raters per language. Raters scored both full-text translations and segment-level instances of culturally nuanced language (idioms, puns, holidays, and culturally embedded concepts) on an ordinal 0-3 quality scale; segment ratings additionally included an NA option for untranslated segments.
  Across full-text evaluations, mean overall quality is modest (1.68/3): GPT-5 (2.10/3), Claude Sonnet 3.7 (1.97/3), and Mistral Medium 3.1 (1.84/3) form the strongest tier with fewer catastrophic failures. Segment-level results show sharp category effects: holidays (2.20/3) and cultural concepts (2.19/3) translate substantially better than idioms (1.65/3) and puns (1.45/3), and idioms are most likely to be left untranslated. These findings demonstrate a persistent gap between grammatical adequacy and cultural resonance. To our knowledge, this is the first multilingual, human-annotated benchmark focused explicitly on cultural nuance in translation and localisation, highlighting the need for culturally informed training data, improved cross-lingual pragmatics, and evaluation paradigms that better reflect real-world communicative competence.

</details>


### [129] [Alignment Drift in Multimodal LLMs: A Two-Phase, Longitudinal Evaluation of Harm Across Eight Model Releases](https://arxiv.org/abs/2602.04739)
*Casey Ford,Madison Van Doren,Emily Dix*

Main category: cs.CL

TL;DR: 本研究评估了针对一系列726个对抗性提示的多模态大语言模型的安全性，发现了不同模型系列之间存在显著差异，特别是Pixtral模型最为脆弱，而Claude系列模型表现更为安全。研究还发现攻击成功率随模型更新呈现出明显的偏差，需要使用多模态基准来跟踪模型的安全性变化。


<details>
  <summary>Details</summary>
Motivation: 随着多模态大语言模型（MLLM）在现实系统中的广泛应用，其在对抗性提示下的安全性问题变得尤为重要，该研究旨在评估它们的安全性变化。

Method: 研究采用两阶段方法评估MLLM面临的危害。第一阶段评估了GPT-4o，Claude Sonnet 3.5，Pixtral 12B及Qwen VL Plus，第二阶段评估了它们的后续版本GPT-5，Claude Sonnet 4.5，Pixtral Large及Qwen Omni，通过收集到的82,256个关于人类伤害的评分来评估模型表现。

Result: 研究结果显示，模型不同家族间存在显著差异，Pixtral模型最脆弱，而Claude系列模型相对安全。攻击成功率随模型更新显示出偏差趋势，GPT和Claude系列模型在新一代模型中有增加，而Pixtral和Qwen系列模型则有所下降。多模态提示的效果也发生了变化，从文本仅提示到模态特定的效果。

Conclusion: 研究指出，MLLM的安全性并非在各代版本之间一致稳定，需要进行长周期的多模态基准测试，以便监控模型的安全行为演变。

Abstract: Multimodal large language models (MLLMs) are increasingly deployed in real-world systems, yet their safety under adversarial prompting remains underexplored. We present a two-phase evaluation of MLLM harmlessness using a fixed benchmark of 726 adversarial prompts authored by 26 professional red teamers. Phase 1 assessed GPT-4o, Claude Sonnet 3.5, Pixtral 12B, and Qwen VL Plus; Phase 2 evaluated their successors (GPT-5, Claude Sonnet 4.5, Pixtral Large, and Qwen Omni) yielding 82,256 human harm ratings. Large, persistent differences emerged across model families: Pixtral models were consistently the most vulnerable, whereas Claude models appeared safest due to high refusal rates. Attack success rates (ASR) showed clear alignment drift: GPT and Claude models exhibited increased ASR across generations, while Pixtral and Qwen showed modest decreases. Modality effects also shifted over time: text-only prompts were more effective in Phase 1, whereas Phase 2 produced model-specific patterns, with GPT-5 and Claude 4.5 showing near-equivalent vulnerability across modalities. These findings demonstrate that MLLM harmlessness is neither uniform nor stable across updates, underscoring the need for longitudinal, multimodal benchmarks to track evolving safety behaviour.

</details>


### [130] [Exploiting contextual information to improve stance detection in informal political discourse with LLMs](https://arxiv.org/abs/2602.04750)
*Arman Engin Sucu,Yixiang Zhou,Mario A. Nascimento,Tony Mullen*

Main category: cs.CL

TL;DR: 这项研究探讨了在非正式在线对话中使用大型语言模型（LLMs）进行政治立场检测的效果，特别是在语言可能带有讽刺、模糊且依赖上下文的情况下。研究发现，提供用户的历史帖子总结作为上下文信息可以显著提升分类准确性，从74%开始，比之前的方法有了显著的改进。


<details>
  <summary>Details</summary>
Motivation: 由于在线政治对话中的语言特点各异，难以通过单一视角直接识别用户的政治立场。为了提高检测准确性，研究团队提出将用户的历史帖子进行总结，生成包含用户意识形态、常谈话题和语言模式的结构化概况，以此帮助LLM更好地理解和分类政治立场。

Method: 研究人员采用了一个真实的现实生活中的政治论坛数据集，生成了每个用户的结构化概况。之后，他们评估了七种目前最前沿的LLM在基础模型和包含上下文信息的模型中的表现。

Result: 结果表明，提供上下文提示可以显著提高准确性，在不同模型间，提高了17.5%-38.5% 的准确性。最终，使用上下文信息的LMM在该任务中的准确率达到74%，超过了之前的记录。此外，研究表明，精心选择的政治内容对比随机选择的大段文本，对提升模型性能更有助益。

Conclusion: 该研究强调了在政治立场分类任务中融入用户级别上下文的重要性，最终的实验结果显示，具备用户背景信息的LMM能够表现出更优秀的分类性能。这一结论对设计适应非正式对话中复杂多变语言环境的LMM具有指导价值。

Abstract: This study investigates the use of Large Language Models (LLMs) for political stance detection in informal online discourse, where language is often sarcastic, ambiguous, and context-dependent. We explore whether providing contextual information, specifically user profile summaries derived from historical posts, can improve classification accuracy. Using a real-world political forum dataset, we generate structured profiles that summarize users' ideological leaning, recurring topics, and linguistic patterns. We evaluate seven state-of-the-art LLMs across baseline and context-enriched setups through a comprehensive cross-model evaluation. Our findings show that contextual prompts significantly boost accuracy, with improvements ranging from +17.5\% to +38.5\%, achieving up to 74\% accuracy that surpasses previous approaches. We also analyze how profile size and post selection strategies affect performance, showing that strategically chosen political content yields better results than larger, randomly selected contexts. These findings underscore the value of incorporating user-level context to enhance LLM performance in nuanced political classification tasks.

</details>


### [131] [When Silence Is Golden: Can LLMs Learn to Abstain in Temporal QA and Beyond?](https://arxiv.org/abs/2602.04755)
*Xinyu Zhou,Chang Jin,Carsten Eickhoff,Zhijiang Guo,Seyed Ali Bahrainian*

Main category: cs.CL

TL;DR: 本研究通过引入一种新管道，结合Chain-of-Thought (CoT)监督和基于脱拒权衡的强化学习 (RL)，首次针对大型语言模型（LLMs）进行了关于时间问答的脱拒能力训练，结果显示强化学习在时间推理方面表现优异，同时也探讨了不同信息类型和训练技术对脱拒行为的影响。


<details>
  <summary>Details</summary>
Motivation: 现有方法如校准可能无法可靠地捕捉复杂推理中的不确定性，而本研究旨在通过将脱拒视为可教技能，结合CoT监督和RL指导的脱拒感知奖励，系统地分析不同信息类型和训练技术对LLMs时间推理和脱拒行为的影响，从而提高LLMs可靠性。

Method: 本研究采用了一个新管道，综合了CoT监督和基于脱拒感知奖励的RL指导。在一系列实验中，研究者检查了各种方法，通过分析不同信息类型和训练技术，评估它们对LLMs时间和推理的影响。

Result: 研究结果显示，基于RL的方法在时间推理方面表现出强劲的实证收益：初始化为Qwen2.5-1.5B-Instruct的模型分别在TimeQA-Easy和Hard上的精确匹配率超过了GPT-4o 3.46%和5.80%。此外，与单纯的监督微调（SFT）变种相比，基于RL的方法在处理不可回答问题时将正确正率提高了20%。在这些发现之外，研究还表明SFT可能过度自信并损害可靠性，而RL则改善了预测准确性但同样存在类似的风险。研究表明，隐式推理线索（如原始上下文、时间子上下文、知识图）相比显式CoT监督，对推理中的脱拒行为提供的益处有限。

Conclusion: 总的来说，本研究提供了一种新的见解，即如何将脱拒和推理联合优化，为构建更可靠的LLMs奠定了基础。研究揭示了脱拒和推理的潜在好处与挑战，为未来的工作提供了指针和新的研究方向。

Abstract: Large language models (LLMs) rarely admit uncertainty, often producing fluent but misleading answers, rather than abstaining (i.e., refusing to answer). This weakness is even evident in temporal question answering, where models frequently ignore time-sensitive evidence and conflate facts across different time-periods. In this paper, we present the first empirical study of training LLMs with an abstention ability while reasoning about temporal QA. Existing approaches such as calibration might be unreliable in capturing uncertainty in complex reasoning. We instead frame abstention as a teachable skill and introduce a pipeline that couples Chain-of-Thought (CoT) supervision with Reinforcement Learning (RL) guided by abstention-aware rewards. Our goal is to systematically analyze how different information types and training techniques affect temporal reasoning with abstention behavior in LLMs. Through extensive experiments studying various methods, we find that RL yields strong empirical gains on reasoning: a model initialized by Qwen2.5-1.5B-Instruct surpasses GPT-4o by $3.46\%$ and $5.80\%$ in Exact Match on TimeQA-Easy and Hard, respectively. Moreover, it improves the True Positive rate on unanswerable questions by $20\%$ over a pure supervised fine-tuned (SFT) variant. Beyond performance, our analysis shows that SFT induces overconfidence and harms reliability, while RL improves prediction accuracy but exhibits similar risks. Finally, by comparing implicit reasoning cues (e.g., original context, temporal sub-context, knowledge graphs) with explicit CoT supervision, we find that implicit information provides limited benefit for reasoning with abstention. Our study provides new insights into how abstention and reasoning can be jointly optimized, providing a foundation for building more reliable LLMs.

</details>


### [132] [Beyond Many-Shot Translation: Scaling In-Context Demonstrations For Low-Resource Machine Translation](https://arxiv.org/abs/2602.04764)
*Luis Frentzen Salim,Esteban Carlin,Alexandre Morinvil,Xi Ai,Lun-Wei Ku*

Main category: cs.CL

TL;DR: 我们探讨了将上下文学习扩展到数千个样本和百万级上下文窗口，使用不同类型的训练语料作为上下文监督，发现额外上下文增益在接近最大窗口时趋于饱和，并且增益受语料类型显著影响。


<details>
  <summary>Details</summary>
Motivation: 针对低资源语言的机器翻译难题，研究如何通过上下文学习有效利用大规模预训练语言模型，探讨不同类型的训练语料对上下文学习的影响.

Method: 使用上下文学习方法，通过调整上下文大小和使用不同类型的监督数据（单语无监督、指令式语料、平行语料）进行实验。

Result: 结果显示，结合大规模上下文的学习增益在接近最大窗口时开始饱和，不同类型的训练语料对结果影响显著。

Conclusion: 本研究指出了长上下文的上下文学习在低资源机器翻译中的有局限性和语料类型敏感性，强调了窗口大小增大并非必然带来成比例的质量提升。

Abstract: Building machine translation (MT) systems for low-resource languages is notably difficult due to the scarcity of high-quality data. Although Large Language Models (LLMs) have improved MT system performance, adapting them to lesser-represented languages remains challenging. In-context learning (ICL) may offer novel ways to adapt LLMs for low-resource MT by conditioning models on demonstration at inference time. In this study, we explore scaling low-resource machine translation ICL beyond the few-shot setting to thousands of examples with long-context models. We scale in-context token budget to 1M tokens and compare three types of training corpora used as in-context supervision: monolingual unsupervised data, instruction-style data, and parallel data (English--target and Indonesian--target). Our experiments on Javanese and Sundanese show that gains from additional context saturate quickly and can degrade near the maximum context window, with scaling behavior strongly dependent on corpus type. Notably, some forms of monolingual supervision can be competitive with parallel data, despite the latter offering additional supervision. Overall, our results characterize the effective limits and corpus-type sensitivity of long-context ICL for low-resource MT, highlighting that larger context windows do not necessarily yield proportional quality gains.

</details>


### [133] [OmniSIFT: Modality-Asymmetric Token Compression for Efficient Omni-modal Large Language Models](https://arxiv.org/abs/2602.04804)
*Yue Ding,Yiyan Ji,Jungang Li,Xuyang Liu,Xinlong Chen,Junfei Wu,Bozhou Li,Bohan Zeng,Yang Shi,Yushuo Guan,Yuanxing Zhang,Jiaheng Liu,Qiang Liu,Pengfei Wan,Liang Wang*

Main category: cs.CL

TL;DR: OmniSIFT 是一种针对全模态大语言模型的模态非对称 token 压缩框架，它通过时空视频剪枝模块和基于视觉的音频选择模块，提供了有效的 token 压缩策略。实验在五个基准上验证了 OmniSIFT 的有效性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: Omni-LLMs 需要处理长的多模态 token 序列，导致了巨大的计算负担。为此，提出了 OmniSIFT 压缩框架，以减少全模态大语言模型的计算成本同时保持模型性能。

Method: OmniSIFT 采用两阶段压缩策略，首先是时空视频剪枝模块，该模块通过移除帧内结构和帧间重叠产生的冗余视频 token；其次是基于视觉的音频选择模块，该模块根据视觉信息过滤音频 token。整个框架通过可微直通估计算法优化。

Result: 在五个代表性基准上展示了 OmniSIFT 的效果和鲁棒性。对于 Qwen2.5-Omni-7B 模型，OmniSIFT 引入了仅 4.85M 参数，而在某些任务上超越了完全 token 模型，并且比无训练基线模型 OmniZip 具有更低的延迟。

Conclusion: 实验结果表明 OmniSIFT 在减少计算开销的同时，能够有效地保持或超越全 token 模型的性能，为全模态大语言模型的高效处理提供了新思路。

Abstract: Omni-modal Large Language Models (Omni-LLMs) have demonstrated strong capabilities in audio-video understanding tasks. However, their reliance on long multimodal token sequences leads to substantial computational overhead. Despite this challenge, token compression methods designed for Omni-LLMs remain limited. To bridge this gap, we propose OmniSIFT (Omni-modal Spatio-temporal Informed Fine-grained Token compression), a modality-asymmetric token compression framework tailored for Omni-LLMs. Specifically, OmniSIFT adopts a two-stage compression strategy: (i) a spatio-temporal video pruning module that removes video redundancy arising from both intra-frame structure and inter-frame overlap, and (ii) a vision-guided audio selection module that filters audio tokens. The entire framework is optimized end-to-end via a differentiable straight-through estimator. Extensive experiments on five representative benchmarks demonstrate the efficacy and robustness of OmniSIFT. Notably, for Qwen2.5-Omni-7B, OmniSIFT introduces only 4.85M parameters while maintaining lower latency than training-free baselines such as OmniZip. With merely 25% of the original token context, OmniSIFT consistently outperforms all compression baselines and even surpasses the performance of the full-token model on several tasks.

</details>


### [134] [SE-Bench: Benchmarking Self-Evolution with Knowledge Internalization](https://arxiv.org/abs/2602.04811)
*Jiarui Yuan,Tailin Jin,Weize Chen,Zeyuan Liu,Zhiyuan Liu,Maosong Sun*

Main category: cs.CL

TL;DR: SE-Bench 通过将 NumPy 库和其 API 文档伪装成一个伪新颖包来评估智能体的知识内化能力，该评估在没有文档访问的情况下进行。研究揭示了开放书籍悖论、RL 间隙以及自我博弈的有效性，提出了闭环训练等策略来促进知识压缩。


<details>
  <summary>Details</summary>
Motivation: 当前的方法未能有效地测量智能体的终生学习能力和知识内化。为解决这些问题，研究人员开发了 SE-Bench 环境来严格地评估这些能力。

Method: SE-Bench 通过隐藏 NumPy 库并将其 API 文档随机化，迫使智能体在没有文档访问的情况下学习新知识。该方法使用闭环训练、自我博弈和强化学习来促进知识内化。

Result: 研究发现了一种开放书籍悖论：观看参考文档会阻碍知识保留。此外，发现标准强化学习在内化新知识方面有明显差距，而自我博弈和强化学习结合使用可以有效地促进知识学习。

Conclusion: SE-Bench 通过强制智能体进行闭环训练、自我博弈和结合 SFT 技术，提供了一个严格的自我进化诊断平台。研究结果展示了这些策略在知识内化中的有效性。

Abstract: True self-evolution requires agents to act as lifelong learners that internalize novel experiences to solve future problems. However, rigorously measuring this foundational capability is hindered by two obstacles: the entanglement of prior knowledge, where ``new'' knowledge may appear in pre-training data, and the entanglement of reasoning complexity, where failures may stem from problem difficulty rather than an inability to recall learned knowledge. We introduce SE-Bench, a diagnostic environment that obfuscates the NumPy library and its API doc into a pseudo-novel package with randomized identifiers. Agents are trained to internalize this package and evaluated on simple coding tasks without access to documentation, yielding a clean setting where tasks are trivial with the new API doc but impossible for base models without it. Our investigation reveals three insights: (1) the Open-Book Paradox, where training with reference documentation inhibits retention, requiring "Closed-Book Training" to force knowledge compression into weights; (2) the RL Gap, where standard RL fails to internalize new knowledge completely due to PPO clipping and negative gradients; and (3) the viability of Self-Play for internalization, proving models can learn from self-generated, noisy tasks when coupled with SFT, but not RL. Overall, SE-Bench establishes a rigorous diagnostic platform for self-evolution with knowledge internalization. Our code and dataset can be found at https://github.com/thunlp/SE-Bench.

</details>


### [135] [Decomposed Prompting Does Not Fix Knowledge Gaps, But Helps Models Say "I Don't Know"](https://arxiv.org/abs/2602.04853)
*Dhruv Madhwal,Lyuxin David Zhang,Dan Roth,Tomer Wolfson,Vivek Gupta*

Main category: cs.CL

TL;DR: 该研究通过评估三种等效任务的提示策略（Direct、Assistive和Incremental），发现尽管大型语言模型在前沿模型中准确性提升减少，不同提示策略之间的分歧仍能指示潜在错误。通过分歧信号实现的无需训练的弃权策略表现优于标准不确定性基线，证明了基于分解的提示策略可以作为闭卷问答中模型可靠性的实用诊断工具。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在闭卷问答中经常表现出自信的虚构，因此引入分解提示以提高准确性，但研究关注的是其对可靠性的影响。

Method: 实验评估了三种等效任务的提示策略：Direct（直接）、Assistive（辅助）和Incremental（增量），应用于不同规模的模型和多跳问答基准。

Result: 尽管前沿模型中准确性的提升减少，不同提示策略的分歧仍然高度指示潜在错误。由于事实性知识稳定而虚构是随机的，跨策略的分歧提供了一种精确的内部不确定性信号。基于此信号实现的无需训练的弃权策略在各种设置中表现优于标准不确定性基线。

Conclusion: 基于分解的提示策略可以作为闭卷问答中模型可靠性的实用诊断工具，通过弃权策略改善了F1和AUROC表现。

Abstract: Large language models often struggle to recognize their knowledge limits in closed-book question answering, leading to confident hallucinations. While decomposed prompting is typically used to improve accuracy, we investigate its impact on reliability. We evaluate three task-equivalent prompting regimes: Direct, Assistive, and Incremental, across different model scales and multi-hop QA benchmarks. We find that although accuracy gains from decomposition diminish in frontier models, disagreements between prompting regimes remain highly indicative of potential errors. Because factual knowledge is stable while hallucinations are stochastic, cross-regime agreement provides a precise signal of internal uncertainty. We leverage this signal to implement a training-free abstention policy that requires no retrieval or fine-tuning. Our results show that disagreement-based abstention outperforms standard uncertainty baselines as an error detector, improving both F1 and AUROC across settings. This demonstrates that decomposition-based prompting can serve as a practical diagnostic probe for model reliability in closed-book QA.

</details>


### [136] [CoT is Not the Chain of Truth: An Empirical Internal Analysis of Reasoning LLMs for Fake News Generation](https://arxiv.org/abs/2602.04856)
*Zhao Tong,Chunlin Gong,Yiping Zhang,Qiang Liu,Xingcheng Xu,Shu Wu,Haichao Shi,Xiao-Yu Zhang*

Main category: cs.CL

TL;DR: 本文揭示了即使大型语言模型拒绝有害请求时，其链式推理中仍可能存在潜在不安全的叙事。通过引入统一的安全分析框架评估特定注意力头在推理过程中的作用，发现风险主要集中在几个关键层，从而挑战了拒绝即安全的假设。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法通常依赖于最终输出的安全性，假设拒绝请求即表示全程安全。本文旨在通过新的评估框架和具体指标，揭示这一隐含的风险。

Method: 本文提出了一种统一的安全分析框架，通过系统地分解模型各层的链式推理，并使用基于雅可比的特征谱度量评估个体注意力头的作用。提出了稳定性、几何和能量三种可解释的度量标准来量化特定注意力头的反应或嵌入欺骗推理模式。

Result: 实验结果显示，当思考模式被激活时，生成风险显著增加，决策高度集中在少数关键层。通过明确识别负责这种差异的注意力头，本文提供了减轻潜在推理风险的新视角。

Conclusion: 本研究表明，模型的拒绝响应并不足以保证其推理过程的安全性，推荐采取更多措施来识别和减少潜在的推理风险。

Abstract: From generating headlines to fabricating news, the Large Language Models (LLMs) are typically assessed by their final outputs, under the safety assumption that a refusal response signifies safe reasoning throughout the entire process. Challenging this assumption, our study reveals that during fake news generation, even when a model rejects a harmful request, its Chain-of-Thought (CoT) reasoning may still internally contain and propagate unsafe narratives. To analyze this phenomenon, we introduce a unified safety-analysis framework that systematically deconstructs CoT generation across model layers and evaluates the role of individual attention heads through Jacobian-based spectral metrics. Within this framework, we introduce three interpretable measures: stability, geometry, and energy to quantify how specific attention heads respond or embed deceptive reasoning patterns. Extensive experiments on multiple reasoning-oriented LLMs show that the generation risk rise significantly when the thinking mode is activated, where the critical routing decisions concentrated in only a few contiguous mid-depth layers. By precisely identifying the attention heads responsible for this divergence, our work challenges the assumption that refusal implies safety and provides a new understanding perspective for mitigating latent reasoning risks.

</details>


### [137] [Reinforced Attention Learning](https://arxiv.org/abs/2602.04884)
*Bangzheng Li,Jianmo Ni,Chen Qu,Ian Miao,Liu Yang,Xingyu Fu,Muhao Chen,Derek Zhiyuan Cheng*

Main category: cs.CL

TL;DR: 本文提出了一种名为Reinforced Attention Learning (RAL) 的强化学习框架，通过直接优化内部注意力分布来改进多模态大语言模型。与传统的元学习相比，RAL在跨模态对齐上表现出更好的效果。


<details>
  <summary>Details</summary>
Motivation: 当前，基于强化学习的元学习在单一模态的大语言模型（LLMs）中取得了显著的推理性能提升，但在将这种机制扩展到多模态大语言模型（MLLMs）时，通过冗长的推理表达，对感知任务的提升有限，甚至可能削弱性能。

Method: RAL框架通过提出一个基于策略梯度的方法，直接优化内部注意力分布而非输出词汇序列。该方法改变了优化目标，从关注生成什么转变为关注注意力分配到哪里。

Result: 实验结果在多种图像和视频基准测试中展示了RAL方法相对于GRPO和其他baseline的持续改进。附加实验表明，通过注意力蒸馏转移潜在的注意力行为比传统的知识蒸馏更有效地实现跨模态对齐。

Conclusion: 本文将注意力策略定位为多模态后训练的一个有原则且通用的替代方案，有望为多模态任务带来实质性的改进。

Abstract: Post-training with Reinforcement Learning (RL) has substantially improved reasoning in Large Language Models (LLMs) via test-time scaling. However, extending this paradigm to Multimodal LLMs (MLLMs) through verbose rationales yields limited gains for perception and can even degrade performance.
  We propose Reinforced Attention Learning (RAL), a policy-gradient framework that directly optimizes internal attention distributions rather than output token sequences. By shifting optimization from what to generate to where to attend, RAL promotes effective information allocation and improved grounding in complex multimodal inputs. Experiments across diverse image and video benchmarks show consistent gains over GRPO and other baselines. We further introduce On-Policy Attention Distillation, demonstrating that transferring latent attention behaviors yields stronger cross-modal alignment than standard knowledge distillation. Our results position attention policies as a principled and general alternative for multimodal post-training.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [138] [A-Graph: A Unified Graph Representation for At-Will Simulation across System Stacks](https://arxiv.org/abs/2602.04847)
*Daniel Price,Prabhu Vellaisamy,Patricia Gonzalez,George Michelogiannakis,John P. Shen,Di Wu*

Main category: cs.PF

TL;DR: 论文介绍了一种名为Agraph的图表示方法，它能够统一表示任意应用、软件、架构和电路，从而支持在不同层级上自由探索设计空间。为了便于使用，提出了Archx框架，提供了易于使用的编程接口和基于范围的度量检索，增强了设计点的可解释性。通过案例研究展示了Agraph的跨技术和应用的一般性，以及高仿真精度。


<details>
  <summary>Details</summary>
Motivation: 随着计算机系统的多样化，设计空间变得越来越大和复杂。为了能够在早期阶段进行设计空间的探索，确保快速开发以实现最优性能和成本，作者提出了Agraph和Archx。

Method: Agraph通过图的形式统一表示系统，支持在不同维度上自由探索设计空间。Archx框架提供了一种易于使用的编程接口和基于范围的度量检索方法，便于用户生成和分析设计点。

Result: 通过案例研究，展示了Agraph和Archx可以在不关注技术、架构和应用的情况下，实现高精度的性能和成本仿真。

Conclusion: Agraph和Archx为灵活地进行性能和成本仿真提供了一个坚实的基础。

Abstract: As computer systems continue to diversify across technologies, architectures, applications, and beyond, the relevant design space has become larger and more complex. Given such trends, design space exploration (DSE) at early stages is critical to ensure agile development towards optimal performance and cost. Industry-grade EDA tools directly take in RTL code and report accurate results, but do not perform DSE. Recent works have attempted to explore the design space via simulation. However, most of these works are domain-specific and constrain the space that users are allowed to explore, offering limited flexibility between technologies, architecture, and applications. Moreover, they often demand high domain expertise to ensure high accuracy. To enable simulation that is agnostic to technology, architecture, and application at any granularity, we introduce Architecture-Graph (Agraph), a graph that unifies the system representation surrounding any arbitrary application, software, architecture, and circuit. Such a unified representation distinguishes Agraph from prior works, which focus on a single stack, allowing users to freely explore the design space across system stacks. To fully unleash the potential of Agraph, we further present Archx, a framework that implements Agraph. Archx is user-friendly in two ways. First, Archx has an easy-to-use programming interface to automatically generate and sweep design points under user constraints, boosting the programmability. Second, Archx adopts scope-based metric retrieval to analyze and understand each design point at any user-preferred hierarchy, enhancing the explainability. We conduct case studies that demonstrate Agraph's generalization across technologies, architecture, and applications with high simulation accuracy. Overall, we argue that Agraph and Archx serve as a foundation to simulate both performance and cost at will.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [139] [SPPAM: Signature Pattern Prediction and Access-Map Prefetcher](https://arxiv.org/abs/2602.04100)
*Maccoy Merrell,Lei Wang,Stavros Kalafatis,Paul V. Gratz*

Main category: cs.AR

TL;DR: SPPAM 提出了一种新的预取方法，结合了 SPP 和 AMPM 的优点，通过在线学习构建访问模式，提高第二级缓存的性能，比无预取提升了 31.4%，比基准预取器提高了 6.2%。


<details>
  <summary>Details</summary>
Motivation: 缓存预取技术旨在解决处理器速度和内存系统性能之间的差距问题，现有技术如 SPP 和 AMPM 存在局限性，SPPAM 旨在改进这些局限从而提升系统性能。

Method: SPPAM 利用在线学习建立一组访问模式，并以此进行推测性前瞻，通过一个信心度量来调节。

Result: SPPAM 按照第二级缓存优化，与最先进的预取器 Berti 和 Bingo 相比，系统性能提升了 31.4%，相对于 Berti 和 Pythia 的基线提高了 6.2%。

Conclusion: SPPAM 通过结合 SPP 和 AMPM 的优点，有效地改进了预取性能，并通过实验证明了其在系统性能提升方面的优越性。

Abstract: The discrepancy between processor speed and memory system performance continues to limit the performance of many workloads. To address the issue, one effective and well studied technique is cache prefetching. Many prefetching designs have been proposed, with varying approaches and effectiveness. For example, SPP is a popular prefetcher that leverages confidence throttled recursion to speculate on the future path of program's references, however it is very susceptible to the reference reordering of higher-level caches and the OoO core. Orthogonally, AMPM is another popular approach to prefetching which uses reordering-resistant access maps to identify patterns within a region, but is unable to speculate beyond that region. In this paper, we propose SPPAM, a new approach to prefetching, inspired by prior works such as SPP and AMPM, while addressing their limitations. SPPAM utilizes online-learning to build a set of access-map patterns. These patterns are used in a speculative lookahead which is throttled by a confidence metric. Targeting the second-level cache, SPPAM alongside state-of-the-art prefetchers Berti and Bingo improve system performance by 31.4% over no prefetching and 6.2% over the baseline of Berti and Pythia.

</details>


### [140] [Harmonia: Algorithm-Hardware Co-Design for Memory- and Compute-Efficient BFP-based LLM Inference](https://arxiv.org/abs/2602.04595)
*Xinyu Wang,Jieyu Li,Yanan Sun,Weifeng He*

Main category: cs.AR

TL;DR: Harmonia 提出了一种算法-硬件协同设计框架，旨在支持所有层的 BFP 激活，同时保持高性能。通过系统探索 BFP 配置，减少 KV 缓存存储和计算，并设计特定硬件以进一步提高效率。


<details>
  <summary>Details</summary>
Motivation: 现有的方法已经通过使用块浮点数（BFP）激活来减轻FP计算的负担，但在注意力层中的应用受到了严重影响，导致大量准确性损失。为了克服这一瓶颈，Harmonia 引入了一种综合方法，结合硬件和算法优化，以支持所有层的 BFP 激活，从而提高整体效率。

Method: Harmonia 采用了系统探索 BFP 配置的方法，优化了注意力层中的 KV 缓存压缩策略，并设计了专为支持 BFP-INT 和 BFP-BFP 混合数据格式的可重构处理单元 (PE)， FP16 到 BFP 的实时转换器以及针对存储器流量优化的数据流。

Result: Harmonia 在八种广泛使用的 LLMs 的线性和注意力层的 GEMM 操作上进行评估，实现了3.84倍 (最多5.05倍) 的面积效率，2.03倍 (最多3.90倍) 的能效比以及3.08倍 (最多4.62倍) 的平均加速。

Conclusion: Harmonia 框架通过算法和硬件协同优化，在保持高准确性的同时，实现了显著的能效和面积效率提升，为大语言模型的高效率部署提供了新的解决方案。

Abstract: Large Language Models (LLMs) are powerful but incur high memory and computation costs. Quantization is an effective solution, with INT weights and FP activations being widely adopted to preserve accuracy. Prior works further reduce FP overhead by using block floating point (BFP) activations in linear layers, but fail to extend BFP to attention layers due to severe accuracy degradation, limiting overall efficiency. To address this challenge, we propose Harmonia, an algorithm-hardware co-design framework that enables all-layer BFP activations with a configurable hardware architecture. First, we systematically explore BFP configurations to achieve a better trade-off between accuracy and activation compression across all layers. Second, to reduce KV-cache storage and computation in attention layers, we introduce an asymmetric bit-allocation strategy and computations in attention layers,we introduce an asymmetric bit-allocation strategy combined with a hybrid offline-online outlier smoothing technique. This allow aggressive KV-cache compression from FP16 to 4-bit-mantissa BFP with only 0.3% average accuracy loss. Third, to fully exploit all-layer BFP activations, we design dedicated hardware components, including a reconfigurable PE supporting mixed data formats (BFP-INT and BPF-BFP), a real-time FP16-to-BFP converter, and a tiling-aware dataflow to reduce memory traffic. We evaluate Harmonia on GEMM operations in both linear and attention layers across eight widely used LLMs. Compared with prior works, Harmonia achieves 3.84x (up to 5.05x) higher area efficiency, 2.03x (up to 3.90x) better energy efficiency, and 3.08x (up to 4.62x) speedup on average.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [141] [Knowledge Model Prompting Increases LLM Performance on Planning Tasks](https://arxiv.org/abs/2602.03900)
*Erik Goh,John Kos,Ashok Goel*

Main category: cs.AI

TL;DR: 该研究通过Task-Method-Knowledge（TMK）框架辅助大型语言模型（LLM）的推理能力，特别是在Blocksworld领域，发现TMK使LLM在先前无法解决的谜题上表现达到了97.3%，表明TMK能够改善LLM的推理和规划能力。


<details>
  <summary>Details</summary>
Motivation: 现有的链式思维（CoT）等提示技术在帮助LLM推理方面已经有所成效，但这些方法的有效性受到质疑，因此研究探讨了Task-Method-Knowledge (TMK)框架能否进一步提升LLM的推理能力。

Method: 研究采用了Task-Method-Knowledge (TMK)框架，并通过PlanBench基准中的Blocksworld领域来测试LLM应对复杂规划问题的能力。

Result: 研究发现，通过对LLM使用TMK结构化提示，模型在复杂的规划问题分解为可管理的子任务方面表现良好。特别是在PlanBench中的Blocksworld环境里，原本无法解决问题的模型通过TMK提示后，准确率达到了97.3%，这表明TMK在弥补语义近似和符号操作之间的差距方面具有潜力。

Conclusion: 研究结果表明，TMK不仅提供背景信息，还能引导LLM从默认的语义模式转向编码执行的正式路径，这可能有助于解决LLM推理能力的不足。

Abstract: Large Language Models (LLM) can struggle with reasoning ability and planning tasks. Many prompting techniques have been developed to assist with LLM reasoning, notably Chain-of-Thought (CoT); however, these techniques, too, have come under scrutiny as LLMs' ability to reason at all has come into question. Borrowing from the domain of cognitive and educational science, this paper investigates whether the Task-Method-Knowledge (TMK) framework can improve LLM reasoning capabilities beyond its previously demonstrated success in educational applications. The TMK framework's unique ability to capture causal, teleological, and hierarchical reasoning structures, combined with its explicit task decomposition mechanisms, makes it particularly well-suited for addressing language model reasoning deficiencies, and unlike other hierarchical frameworks such as HTN and BDI, TMK provides explicit representations of not just what to do and how to do it, but also why actions are taken. The study evaluates TMK by experimenting on the PlanBench benchmark, focusing on the Blocksworld domain to test for reasoning and planning capabilities, examining whether TMK-structured prompting can help language models better decompose complex planning problems into manageable sub-tasks. Results also highlight significant performance inversion in reasoning models. TMK prompting enables the reasoning model to achieve up to an accuracy of 97.3\% on opaque, symbolic tasks (Random versions of Blocksworld in PlanBench) where it previously failed (31.5\%), suggesting the potential to bridge the gap between semantic approximation and symbolic manipulation. Our findings suggest that TMK functions not merely as context, but also as a mechanism that steers reasoning models away from their default linguistic modes to engage formal, code-execution pathways in the context of the experiments.

</details>


### [142] [AgentArk: Distilling Multi-Agent Intelligence into a Single LLM Agent](https://arxiv.org/abs/2602.03955)
*Yinyi Luo,Yiqiao Jin,Weichen Yu,Mengqi Zhang,Srijan Kumar,Xiaoxiao Li,Weijie Xu,Xin Chen,Jindong Wang*

Main category: cs.AI

TL;DR: AgentArk 是一种将多智能体动态转化为单一模型权重的新框架，通过在训练阶段转移计算负担，保持单智能体的高效性同时提升多智能体的推理和自校正能力，适用于多种模型和场景。


<details>
  <summary>Details</summary>
Motivation: 面对大型语言模型多智能体系统在推理性能上表现出色但在实际部署中受限于高计算成本和误差传播的问题，本文提出了 AgentArk 框架以降低部署门槛。

Method: AgentArk 采用了三种层次化的蒸馏策略：增强推理微调、基于轨迹扩展和过程感知蒸馏，这些策略被应用于不同模型、任务、规模和场景中。

Result: AgentArk 模型在推理能力和自校正性能上优于传统的单智能体模型，并且展示了在多种推理任务中的增强鲁棒性和泛化能力。

Conclusion: 通过将多智能体系统的动态转化为单一模型，AgentArk 不仅提升了模型的效率，还增强了其在复杂推理任务中的表现，为未来高效且鲁棒的多智能体系统开发提供了新思路。

Abstract: While large language model (LLM) multi-agent systems achieve superior reasoning performance through iterative debate, practical deployment is limited by their high computational cost and error propagation. This paper proposes AgentArk, a novel framework to distill multi-agent dynamics into the weights of a single model, effectively transforming explicit test-time interactions into implicit model capabilities. This equips a single agent with the intelligence of multi-agent systems while remaining computationally efficient. Specifically, we investigate three hierarchical distillation strategies across various models, tasks, scaling, and scenarios: reasoning-enhanced fine-tuning; trajectory-based augmentation; and process-aware distillation. By shifting the burden of computation from inference to training, the distilled models preserve the efficiency of one agent while exhibiting strong reasoning and self-correction performance of multiple agents. They further demonstrate enhanced robustness and generalization across diverse reasoning tasks. We hope this work can shed light on future research on efficient and robust multi-agent development. Our code is at https://github.com/AIFrontierLab/AgentArk.

</details>


### [143] [Adaptive Test-Time Compute Allocation via Learned Heuristics over Categorical Structure](https://arxiv.org/abs/2602.03975)
*Shuhui Qu*

Main category: cs.AI

TL;DR: 该研究针对验证成本限制下的推理场景提出了一种状态级选择性验证框架，通过结构化的移动接口进行确定性的可行性筛选、基于混合学习状态距离和剩余得分的预验证排序以及基于局部不确定性的验证调用动态分配，实现了在较少验证调用情况下的高精度。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）推理的发展，验证成本已成为瓶颈，许多推理系统的大量验证调用花费在冗余或不具前景的中间假设上。

Method: 该方法结合了结构化移动接口上的确定性可行性筛选、混合学习状态距离和剩余分数的预验证排名以及根据局部不确定性动态分配验证调用。

Result: 在MATH基准测试中，该方法在相比best-of-N、多数投票和束搜索方法使用44%的验证调用数量下，实现了更高的准确度。

Conclusion: 该研究提供了一种验证成本限制下推理的有效解决方案，提升了验证效率并保持了高准确度。

Abstract: Test-time computation has become a primary driver of progress in large language model (LLM) reasoning, but it is increasingly bottlenecked by expensive verification. In many reasoning systems, a large fraction of verifier calls are spent on redundant or unpromising intermediate hypotheses. We study reasoning under a \emph{verification-cost-limited} setting and ask how verification effort should be allocated across intermediate states. We propose a state-level selective verification framework that combines (i) deterministic feasibility gating over a structured move interface, (ii) pre-verification ranking using a hybrid of learned state-distance and residual scoring, and (iii) adaptive allocation of verifier calls based on local uncertainty. Unlike solution-level best-of-$N$ or uniform intermediate verification, our method distributes verification where it is most informative. On the \textsc{MATH} benchmark, our approach achieves higher accuracy than best-of-$N$, majority voting, and beam search while using 44\% fewer verifier calls.

</details>


### [144] [Monitorability as a Free Gift: How RLVR Spontaneously Aligns Reasoning](https://arxiv.org/abs/2602.03978)
*Zidi Xiong,Shan Chen,Himabindu Lakkaraju*

Main category: cs.AI

TL;DR: 该研究探讨了Large Reasoning Models (LRMs)过程中链路推理（CoT）可审计性的出现及其机制，发现其效果依赖于数据多样性及指令遵守数据，在提高推理性能的同时并不意味着透明度增加。


<details>
  <summary>Details</summary>
Motivation: 随着LRMs的应用日益广泛，对其推理过程中的可审计性进行审核变得至关重要。尽管一些研究表明，可审计性可能在早期阶段通过可验证奖励的强化学习（RLVR）自动生成，但该研究旨在验证这一现象的普遍性，并探讨其具体机制。

Method: 通过对不同模型家族和训练领域的系统性评估，研究者具体验证了这种现象，揭示了数据多样性及指令遵守数据的重要性。并采用因果分析方法，探讨可审计性增长的主要因素。

Result: 该研究表明，可审计性并非普遍现象，其提高受数据依赖性影响。结论指出，可审计性增长主要与响应分布的强化（熵减少）及对提示的更多关注有关，而非更强的因果关系依赖于推理痕迹。

Conclusion: 研究结果指出了在特定条件下可审计性可能增加的情况，并揭示了其动态变化与训练评估难度的关系，为理解LRMs的可审计性提供了全面视角，有助于在特定场合合理预期可审计性的存在和提升。

Abstract: As Large Reasoning Models (LRMs) are increasingly deployed, auditing their chain-of-thought (CoT) traces for safety becomes critical. Recent work has reported that monitorability--the degree to which CoT faithfully and informatively reflects internal computation--can appear as a "free gift" during the early stages of Reinforcement Learning with Verifiable Rewards (RLVR). We make this observation concrete through a systematic evaluation across model families and training domains. Our results show that this effect is not universal: monitorability improvements are strongly data-dependent. In particular, we demonstrate the critical role of data diversity and instruction-following data during RLVR training. We further show that monitorability is orthogonal to capability--improvements in reasoning performance do not imply increased transparency. Through mechanistic analysis, we attribute monitorability gains primarily to response distribution sharpening (entropy reduction) and increased attention to the prompt, rather than stronger causal reliance on reasoning traces. We also reveal how monitorability dynamics vary with controlled training and evaluation difficulty. Together, these findings provide a holistic view of how monitorability emerges under RLVR, clarifying when gains are likely to occur and when they are not.

</details>


### [145] [When AI Persuades: Adversarial Explanation Attacks on Human Trust in AI-Assisted Decision Making](https://arxiv.org/abs/2602.04003)
*Shutong Fan,Lan Zhang,Xiaoyong Yuan*

Main category: cs.AI

TL;DR: 本文提出了对抗解释攻击（AEAs），攻击者操控LLM生成解释的表达方式以影响用户的信任，特别是在权威和专业沟通方式下，这种攻击尤为有效。研究通过实验发现，即使错误的解释，也能保持用户近似于正确解释的信任度。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统越来越多地嵌入人类决策流程中，其生成的自然语言解释可能被恶意利用来操控用户对AI结果的信任，而本文旨在揭示这种新型攻击面的存在并量化其影响。

Method: 通过引入AEAs的概念，研究人员设计了一系列实验来探索攻击的潜在效果，实验涉及不同解释框架的四种维度：推理模式、证据类型、沟通风格和展示形式。

Result: 实验结果显示，用户对错误的解释几乎没有表现出与其正确解释有显著不同信任度，尤其是在权威和专业风格的解释下。这表明即便错误的信息也能通过特定的沟通策略操纵用户的信任。

Conclusion: 这项研究强调了解释作为一种潜在对抗攻击媒介的重要性，并提出了衡量信任偏差的‘信任失准差距’指标，为未来在这个新兴安全领域的研究和实践提供了重要依据。

Abstract: Most adversarial threats in artificial intelligence target the computational behavior of models rather than the humans who rely on them. Yet modern AI systems increasingly operate within human decision loops, where users interpret and act on model recommendations. Large Language Models generate fluent natural-language explanations that shape how users perceive and trust AI outputs, revealing a new attack surface at the cognitive layer: the communication channel between AI and its users. We introduce adversarial explanation attacks (AEAs), where an attacker manipulates the framing of LLM-generated explanations to modulate human trust in incorrect outputs. We formalize this behavioral threat through the trust miscalibration gap, a metric that captures the difference in human trust between correct and incorrect outputs under adversarial explanations. By incorporating this gap, AEAs explore the daunting threats in which persuasive explanations reinforce users' trust in incorrect predictions. To characterize this threat, we conducted a controlled experiment (n = 205), systematically varying four dimensions of explanation framing: reasoning mode, evidence type, communication style, and presentation format. Our findings show that users report nearly identical trust for adversarial and benign explanations, with adversarial explanations preserving the vast majority of benign trust despite being incorrect. The most vulnerable cases arise when AEAs closely resemble expert communication, combining authoritative evidence, neutral tone, and domain-appropriate reasoning. Vulnerability is highest on hard tasks, in fact-driven domains, and among participants who are less formally educated, younger, or highly trusting of AI. This is the first systematic security study that treats explanations as an adversarial cognitive channel and quantifies their impact on human trust in AI-assisted decision making.

</details>


### [146] [Axiomatic Foundations of Counterfactual Explanations](https://arxiv.org/abs/2602.04028)
*Leila Amgoud,Martin Cooper*

Main category: cs.AI

TL;DR: 该研究通过引入一组可解释性公理并研究其之间的关系，发现了五种不同的解释类型，包括局部和全局解释。研究进一步探讨现有解释器在这一定税分类中的位置。


<details>
  <summary>Details</summary>
Motivation: 本文的动机在于解决目前大多数解释器集中在单类型反事实和局部解释的问题，提出了一种公理框架，并系统的探讨了反事实类型。

Method: 该方法通过构建一组公理，证明不可能定理，以及建立多个一一对等关系，正式定义了五种不同的反事实类型。

Result: 研究结果识别出了五种不同类型的反事实解释，有些类型是局部解释，而另一些则涵盖全局解释。同时该研究还分析了预生成这些解释器的计算复杂性。

Conclusion: 本文不仅为反事实解释提供了理论框架，还明确了不同解释器的局限和潜力，对于提高人工智能系统解释性的理解具有重要意义。

Abstract: Explaining autonomous and intelligent systems is critical in order to improve trust in their decisions. Counterfactuals have emerged as one of the most compelling forms of explanation. They address ``why not'' questions by revealing how decisions could be altered. Despite the growing literature, most existing explainers focus on a single type of counterfactual and are restricted to local explanations, focusing on individual instances. There has been no systematic study of alternative counterfactual types, nor of global counterfactuals that shed light on a system's overall reasoning process.
  This paper addresses the two gaps by introducing an axiomatic framework built on a set of desirable properties for counterfactual explainers. It proves impossibility theorems showing that no single explainer can satisfy certain axiom combinations simultaneously, and fully characterizes all compatible sets. Representation theorems then establish five one-to-one correspondences between specific subsets of axioms and the families of explainers that satisfy them. Each family gives rise to a distinct type of counterfactual explanation, uncovering five fundamentally different types of counterfactuals. Some of these correspond to local explanations, while others capture global explanations. Finally, the framework situates existing explainers within this taxonomy, formally characterizes their behavior, and analyzes the computational complexity of generating such explanations.

</details>


### [147] [OMG-Agent: Toward Robust Missing Modality Generation with Decoupled Coarse-to-Fine Agentic Workflows](https://arxiv.org/abs/2602.04144)
*Ruiting Dai,Zheyu Wang,Haoyu Yang,Yihan Liu,Chengzhi Wang,Zekun Zhang,Zishan Huang,Jiaman Cen,Lisi Mo*

Main category: cs.AI

TL;DR: OMG-Agent 提出了一种新颖的框架，解决了多模态生成中的固有冲突，通过动态的粗细粒度工作流实现了多模态数据的高质量生成。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态重建方法存在依赖内部记忆导致幻觉和检索刚性的问题，OMG-Agent 旨在克服这些限制。

Method: OMG-Agent 采用了一种动态的粗细粒度工作流，由语义规划器、证据检索器和执行器三个部分组成，分别处理输入的语义解析、外部知识的获取以及生成高保真度的细节。

Result: 在多个基准测试上，OMG-Agent 大幅超越了目前最先进的方法，特别是在高缺失率下表现优异。

Conclusion: OMG-Agent 通过灵活性和结构化计划的结合，显著提高了多模态系统的生成质量，展示了在处理复杂输入和高缺失率情况下的优越性。

Abstract: Data incompleteness severely impedes the reliability of multimodal systems. Existing reconstruction methods face distinct bottlenecks: conventional parametric/generative models are prone to hallucinations due to over-reliance on internal memory, while retrieval-augmented frameworks struggle with retrieval rigidity. Critically, these end-to-end architectures are fundamentally constrained by Semantic-Detail Entanglement -- a structural conflict between logical reasoning and signal synthesis that compromises fidelity. In this paper, we present \textbf{\underline{O}}mni-\textbf{\underline{M}}odality \textbf{\underline{G}}eneration Agent (\textbf{OMG-Agent}), a novel framework that shifts the paradigm from static mapping to a dynamic coarse-to-fine Agentic Workflow. By mimicking a \textit{deliberate-then-act} cognitive process, OMG-Agent explicitly decouples the task into three synergistic stages: (1) an MLLM-driven Semantic Planner that resolves input ambiguity via Progressive Contextual Reasoning, creating a deterministic structured semantic plan; (2) a non-parametric Evidence Retriever that grounds abstract semantics in external knowledge; and (3) a Retrieval-Injected Executor that utilizes retrieved evidence as flexible feature prompts to overcome rigidity and synthesize high-fidelity details. Extensive experiments on multiple benchmarks demonstrate that OMG-Agent consistently surpasses state-of-the-art methods, maintaining robustness under extreme missingness, e.g., a $2.6$-point gain on CMU-MOSI at $70$\% missing rates.

</details>


### [148] [Steering LLMs via Scalable Interactive Oversight](https://arxiv.org/abs/2602.04210)
*Enyu Zhou,Zhiheng Xi,Long Ma,Zhihao Zhang,Shihan Dou,Zhikai Lei,Guoteng Wang,Rui Zheng,Hang Yan,Tao Gui,Qi Zhang,Xuanjing Huang*

Main category: cs.AI

TL;DR: 本文提出了一种新的框架，名为Scalable Interactive Oversight，旨在解决大型语言模型在执行长周期任务时出现的监督缺口问题。该框架将复杂的意图分解为递归树状决策结构，通过递归收集用户的低负担反馈并转化为精准的全局指导，从而增强人类监督能力。同时，该框架可以通过在线用户反馈进行强化学习优化，使非专家也能产出专家级别的产品需求文档。


<details>
  <summary>Details</summary>
Motivation: 当前，大型语言模型在执行长周期任务时常常会出现监督不足的问题，传统的开放性提示方法难以满足需求。因此，本文提出了Scalable Interactive Oversight框架，以增强人类在复杂任务中的监督能力。

Method: 该框架通过将复杂任务分解为递归树形结构，并在每个节点上收集用户的低负担反馈。通过递归聚合这些反馈信号，转化为精准的全球指导信号。此外，该框架还能通过在线用户反馈进行强化学习优化。

Result: 在一项针对Web开发任务的研究中，该框架帮助非专家生成了专家级别的产品需求文档，并达到了54%的对齐改进。

Conclusion: 实验结果证明，该框架能够在不依赖专业知识的情况下，提高非专家的监督能力，让人类能够更加有效地引导AI系统完成超出自身能力的任务。这种框架为保持人类在AI系统中的控制权提供了一种实际解决方案。

Abstract: As Large Language Models increasingly automate complex, long-horizon tasks such as \emph{vibe coding}, a supervision gap has emerged. While models excel at execution, users often struggle to guide them effectively due to insufficient domain expertise, the difficulty of articulating precise intent, and the inability to reliably validate complex outputs. It presents a critical challenge in scalable oversight: enabling humans to responsibly steer AI systems on tasks that surpass their own ability to specify or verify. To tackle this, we propose Scalable Interactive Oversight, a framework that decomposes complex intent into a recursive tree of manageable decisions to amplify human supervision. Rather than relying on open-ended prompting, our system elicits low-burden feedback at each node and recursively aggregates these signals into precise global guidance. Validated in web development task, our framework enables non-experts to produce expert-level Product Requirement Documents, achieving a 54\% improvement in alignment. Crucially, we demonstrate that this framework can be optimized via Reinforcement Learning using only online user feedback, offering a practical pathway for maintaining human control as AI scales.

</details>


### [149] [Digital Twins & ZeroConf AI: Structuring Automated Intelligent Pipelines for Industrial Applications](https://arxiv.org/abs/2602.04385)
*Marco Picone,Fabio Turazza,Matteo Martinelli,Marco Mamei*

Main category: cs.AI

TL;DR: 本文提出了一种模块化和互操作性强的解决方案，通过最小化配置和解耦数字孪生（DT）与AI组件的作用，无缝集成AI流水线到CPS中，从而加速复杂工业环境中智能服务的部署。


<details>
  <summary>Details</summary>
Motivation: 当前的方法往往孤立且紧密耦合，限制了AI功能的可扩展性和重用性，特别是在物联网和工业物联网技术多样性的问题上，新提出的解决方案旨在通过数字孪生技术提供结构化的、互操作性的和语义丰富的物理资产的数字表示，从而弥合低层物理层和高层智能功能之间的差距。

Method: 本文提出了一种名为ZeroConf的AI管道的概念，其中数字孪生负责数据管理和智能增强的编排；并展示了该方法在MicroFactory场景中的应用，支持并发机器学习模型和动态数据处理。

Result: 展示的实验结果表明，该方法支持并发机器学习模型和动态数据处理，加快了复杂工业环境中智能服务的部署。

Conclusion: 该论文通过提出将数字孪生技术与模块化配置相结合的新型方法，为复杂工业场景中有效地构建和部署AI管道提供了一种有效的途径。

Abstract: The increasing complexity of Cyber-Physical Systems (CPS), particularly in the industrial domain, has amplified the challenges associated with the effective integration of Artificial Intelligence (AI) and Machine Learning (ML) techniques. Fragmentation across IoT and IIoT technologies, manifested through diverse communication protocols, data formats and device capabilities, creates a substantial gap between low-level physical layers and high-level intelligent functionalities. Recently, Digital Twin (DT) technology has emerged as a promising solution, offering structured, interoperable and semantically rich digital representations of physical assets. Current approaches are often siloed and tightly coupled, limiting scalability and reuse of AI functionalities. This work proposes a modular and interoperable solution that enables seamless AI pipeline integration into CPS by minimizing configuration and decoupling the roles of DTs and AI components. We introduce the concept of Zero Configuration (ZeroConf) AI pipelines, where DTs orchestrate data management and intelligent augmentation. The approach is demonstrated in a MicroFactory scenario, showing support for concurrent ML models and dynamic data processing, effectively accelerating the deployment of intelligent services in complex industrial settings.

</details>


### [150] [From Competition to Collaboration: Designing Sustainable Mechanisms Between LLMs and Online Forums](https://arxiv.org/abs/2602.04572)
*Niv Fono,Yftah Ziser,Omer Ben-Porat*

Main category: cs.AI

TL;DR: 该研究提出了一种框架，让生成型AI系统与论坛进行顺序交互，利用论坛数据改进自身性能。实验表明，尽管存在激励错配，但双方仍能实现理想情况下大约一半的效用，从而促进可持续的知识共享。


<details>
  <summary>Details</summary>
Motivation: 鉴于生成型AI系统需要论坛数据来提升性能，但同时也导致用户减少，研究旨在解决这一悖论，通过设计一种框架让AI与论坛协作。

Method: 通过数据驱动的模拟实验，使用真实Stack Exchange数据和常用LLM，研究探讨了激励错配，并评估了理想信息下的效用水平。

Result: 实验结果表明，在激励错配情况下，双方仍能实现约理想情况下来自合作的效用的一半。

Conclusion: 该研究指出，生成型AI系统与人类知识平台之间可以维持一种有效的、可持续的知识共享协作模式。

Abstract: While Generative AI (GenAI) systems draw users away from (Q&A) forums, they also depend on the very data those forums produce to improve their performance. Addressing this paradox, we propose a framework of sequential interaction, in which a GenAI system proposes questions to a forum that can publish some of them. Our framework captures several intricacies of such a collaboration, including non-monetary exchanges, asymmetric information, and incentive misalignment. We bring the framework to life through comprehensive, data-driven simulations using real Stack Exchange data and commonly used LLMs. We demonstrate the incentive misalignment empirically, yet show that players can achieve roughly half of the utility in an ideal full-information scenario. Our results highlight the potential for sustainable collaboration that preserves effective knowledge sharing between AI systems and human knowledge platforms.

</details>


### [151] [WideSeek-R1: Exploring Width Scaling for Broad Information Seeking via Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2602.04634)
*Zelai Xu,Zhexuan Xu,Ruize Zhang,Chunyang Zhu,Shi Yu,Weilin Liu,Quanlu Zhang,Wenbo Ding,Chao Yu,Yu Wang*

Main category: cs.AI

TL;DR: 该论文提出了一种名为WideSeek-R1的多agents系统，通过利用共享的LLM和独立的上下文以及专门的工具，采用多agent强化学习（MARL）实现了模块化协同与并行执行的优化。WideSeek-R1-4B的项目F1分数在WideSearch基准上达到了40.0%，并且随着并行subagents数量的增加，其性能表现出一致的增长。


<details>
  <summary>Details</summary>
Motivation: 随着任务变得更为广泛，个体能力瓶颈转变为组织能力的问题变得突出，因此需要探索宽度扩展的有效方法来应对广泛的信息查询需求。

Method: WideSeek-R1利用了一个共享的LLM以及隔离的上下文和专门的工具，并通过多agent强化学习进行联合优化，该模型在20000个广泛的查询任务数据集上进行了训练。

Result: WideSeek-R1-4B在WideSearch基准测试中的项目F1分数达到了40.0%，并且展示了宽度扩展的有效性，即随着并行subagents数量的增加，性能有显著提升。

Conclusion: WideSeek-R1框架证明了宽度扩展在应对广泛的查询任务方面的潜力，并且提出了一种新的协同多个agents进行任务的方法。

Abstract: Recent advancements in Large Language Models (LLMs) have largely focused on depth scaling, where a single agent solves long-horizon problems with multi-turn reasoning and tool use. However, as tasks grow broader, the key bottleneck shifts from individual competence to organizational capability. In this work, we explore a complementary dimension of width scaling with multi-agent systems to address broad information seeking. Existing multi-agent systems often rely on hand-crafted workflows and turn-taking interactions that fail to parallelize work effectively. To bridge this gap, we propose WideSeek-R1, a lead-agent-subagent framework trained via multi-agent reinforcement learning (MARL) to synergize scalable orchestration and parallel execution. By utilizing a shared LLM with isolated contexts and specialized tools, WideSeek-R1 jointly optimizes the lead agent and parallel subagents on a curated dataset of 20k broad information-seeking tasks. Extensive experiments show that WideSeek-R1-4B achieves an item F1 score of 40.0% on the WideSearch benchmark, which is comparable to the performance of single-agent DeepSeek-R1-671B. Furthermore, WideSeek-R1-4B exhibits consistent performance gains as the number of parallel subagents increases, highlighting the effectiveness of width scaling.

</details>


### [152] [Are AI Capabilities Increasing Exponentially? A Competing Hypothesis](https://arxiv.org/abs/2602.04836)
*Haosen Ge,Hamsa Bastani,Osbert Bastani*

Main category: cs.AI

TL;DR: 该研究不同意METR报告中对AI能力呈指数增长的观点，并指出当前数据表明转折点已经过去。研究还提出一个更复杂的模型，该模型支持AI能力将在未来不久出现转折点，目的是为了强调现有预测的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 研究的主要动机是在当前急剧增长的人工智能的能力下，对预测未来的AI发展趋势提出质疑，特别是对METR报告中提出的指数增长观点持怀疑态度。

Method: 研究者采用sigmoid曲线拟合方法来分析当前数据，以评估AI能力的增长趋势。此外，还提出了一个分解AI能力为基本能力和推理能力的模型，研究这些能力的独立改进率。

Result: 研究结果表明当前的数据不支持指数增长，表明转折点已经过去。此外，复杂模型支持AI能力将在未来不久出现转折点。

Conclusion: 结论是研究强调了现有预测的脆弱性，尤其是那些依赖于指数增长预测的类比。这反映了对AI未来发展的不同看法和在制定相关政策时应考虑的复杂因素。

Abstract: Rapidly increasing AI capabilities have substantial real-world consequences, ranging from AI safety concerns to labor market consequences. The Model Evaluation & Threat Research (METR) report argues that AI capabilities have exhibited exponential growth since 2019. In this note, we argue that the data does not support exponential growth, even in shorter-term horizons. Whereas the METR study claims that fitting sigmoid/logistic curves results in inflection points far in the future, we fit a sigmoid curve to their current data and find that the inflection point has already passed. In addition, we propose a more complex model that decomposes AI capabilities into base and reasoning capabilities, exhibiting individual rates of improvement. We prove that this model supports our hypothesis that AI capabilities will exhibit an inflection point in the near future. Our goal is not to establish a rigorous forecast of our own, but to highlight the fragility of existing forecasts of exponential growth.

</details>


### [153] [Fluid Representations in Reasoning Models](https://arxiv.org/abs/2602.04843)
*Dmitrii Kharlapenko,Alessandro Stolfo,Arthur Conmy,Mrinmaya Sachan,Zhijing Jin*

Main category: cs.AI

TL;DR: 该研究分析了QwQ-32B模型在Mystery Blocksworld中的推理过程，发现它能够通过逐步改进内部表示来专注于结构性信息而非具体动作名称，从而提升问题解决能力。


<details>
  <summary>Details</summary>
Motivation: 由于推理语言模型在处理抽象问题时表现出更高性能，但其内部机制仍不甚明了，因此进行这项研究以更好地理解QwQ-32B模型的具体工作原理。

Method: 通过在Mystery Blocksworld中执行推理过程并进行干预实验（如注入改进的表示形式和替换模糊编码），研究了QwQ-32B模型如何通过细化的token表示进行在线调整。

Result: 研究发现，QwQ-32B模型能够通过细化和重新编码动作和概念来解决抽象结构信息问题，这能显著提问题解决的准确率。同时，通过注入成功推理过程中的精炼表示，性能有显著提升。

Conclusion: 研究表明，通过在上下文中对标记表示进行精细化调整，能够显著提升推理模型的性能，这种机制称为流体推理表示。

Abstract: Reasoning language models, which generate long chains of thought, dramatically outperform non-reasoning language models on abstract problems. However, the internal model mechanisms that allow this superior performance remain poorly understood. We present a mechanistic analysis of how QwQ-32B - a model specifically trained to produce extensive reasoning traces - process abstract structural information. On Mystery Blocksworld - a semantically obfuscated planning domain - we find that QwQ-32B gradually improves its internal representation of actions and concepts during reasoning. The model develops abstract encodings that focus on structure rather than specific action names. Through steering experiments, we establish causal evidence that these adaptations improve problem solving: injecting refined representations from successful traces boosts accuracy, while symbolic representations can replace many obfuscated encodings with minimal performance loss. We find that one of the factors driving reasoning model performance is in-context refinement of token representations, which we dub Fluid Reasoning Representations.

</details>
