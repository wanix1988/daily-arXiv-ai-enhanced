<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 8]
- [cs.CL](#cs.CL) [Total: 2]
- [cs.AI](#cs.AI) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Are We Ready for RL in Text-to-3D Generation? A Progressive Investigation](https://arxiv.org/abs/2512.10949)
*Yiwen Tang,Zoey Guo,Kaixin Zhu,Ray Zhang,Qizhi Chen,Dongzhi Jiang,Junli Liu,Bohan Zeng,Haoming Song,Delin Qu,Tianyi Bai,Dan Xu,Wentao Zhang,Bin Zhao*

Main category: cs.CV

TL;DR: 该研究对使用强化学习(RL)提升文本到3D自回归生成进行了系统性分析，通过奖励设计、强化学习算法研究以及引入新的基准测试和层次化强化学习方法来解决3D生成中的挑战。


<details>
  <summary>Details</summary>
Motivation: 通过奖励设计、强化学习算法和基准测试等多方面研究，解决3D生成中由于几何全局一致性与局部细节精细度要求带来的挑战，推动3D生成模型的改进与性能提升。

Method: 研究采用了包括评估不同的奖励维度和模型选择、研究GRPO变体、探索训练数据规模和迭代次数的影响以及开发基于层次奖励的强化学习方法。通过Hi-GRPO方法优化从粗略形状到纹理细化的3D生成过程。

Result: 研究提出了新的基准测试MME-3DR，并开发了基于强化学习的3D生成模型AR3D-R1，其在文本到3D生成中取得了显著效果。

Conclusion: 该研究为使用RL进行3D生成提供了新的见解，强调了奖励设计与算法选择的重要性，并为未来的研究指明了方向。

Abstract: Reinforcement learning (RL), earlier proven to be effective in large language and multi-modal models, has been successfully extended to enhance 2D image generation recently. However, applying RL to 3D generation remains largely unexplored due to the higher spatial complexity of 3D objects, which require globally consistent geometry and fine-grained local textures. This makes 3D generation significantly sensitive to reward designs and RL algorithms. To address these challenges, we conduct the first systematic study of RL for text-to-3D autoregressive generation across several dimensions. (1) Reward designs: We evaluate reward dimensions and model choices, showing that alignment with human preference is crucial, and that general multi-modal models provide robust signal for 3D attributes. (2) RL algorithms: We study GRPO variants, highlighting the effectiveness of token-level optimization, and further investigate the scaling of training data and iterations. (3) Text-to-3D Benchmarks: Since existing benchmarks fail to measure implicit reasoning abilities in 3D generation models, we introduce MME-3DR. (4) Advanced RL paradigms: Motivated by the natural hierarchy of 3D generation, we propose Hi-GRPO, which optimizes the global-to-local hierarchical 3D generation through dedicated reward ensembles. Based on these insights, we develop AR3D-R1, the first RL-enhanced text-to-3D model, expert from coarse shape to texture refinement. We hope this study provides insights into RL-driven reasoning for 3D generation. Code is released at https://github.com/Ivan-Tang-3D/3DGen-R1.

</details>


### [2] [MMSI-Video-Bench: A Holistic Benchmark for Video-Based Spatial Intelligence](https://arxiv.org/abs/2512.10863)
*Jingli Lin,Runsen Xu,Shaohao Zhu,Sihan Yang,Peizhou Cao,Yunlong Ran,Miao Hu,Chenming Zhu,Yiman Xie,Yilin Long,Wenbo Hu,Dahua Lin,Tai Wang,Jiangmiao Pang*

Main category: cs.CV

TL;DR: 该研究旨在通过MMSI-Video-Bench评估多模态语言模型在视频中空间智能的能力，该基准涵盖感知、规划、预测和跨视频推理四个层次，结果显示当前模型与人类相比存在显著差距。


<details>
  <summary>Details</summary>
Motivation: 评估多模态语言模型在理解连续视觉输入的空间理解能力方面取得的进步，因为这将是它们成为物理环境中的通用助手的关键。

Method: 开发了一个名为MMSI-Video-Bench的全面基准测试，该基准基于三个层次的问题设计（感知、规划、预测和跨视频推理），从1278个视频片段中提取了1106个问题，并且这些问题由3D视觉专家进行评估。

Result: 该基准测试揭示了模型在空间推理任务上的巨大差距：许多模型表现接近随机选择，甚至最好的推理模型也比人类落后近60%。此外，精细的空间调整模型在基准测试上也不能有效泛化。

Conclusion: 研究者希望通过这个基准测试来促进视频中空间智能的进步，发现目前的模型在几何推理、运动接地、长时预测和跨视频对应方面的系统性失败。

Abstract: Spatial understanding over continuous visual input is crucial for MLLMs to evolve into general-purpose assistants in physical environments. Yet there is still no comprehensive benchmark that holistically assesses the progress toward this goal. In this work, we introduce MMSI-Video-Bench, a fully human-annotated benchmark for video-based spatial intelligence in MLLMs. It operationalizes a four-level framework, Perception, Planning, Prediction, and Cross-Video Reasoning, through 1,106 questions grounded in 1,278 clips from 25 datasets and in-house videos. Each item is carefully designed and reviewed by 3DV experts with explanatory rationales to ensure precise, unambiguous grounding. Leveraging its diverse data sources and holistic task coverage, MMSI-Video-Bench also supports three domain-oriented sub-benchmarks (Indoor Scene Perception Bench, Robot Bench and Grounding Bench) for targeted capability assessment. We evaluate 25 strong open-source and proprietary MLLMs, revealing a striking human--AI gap: many models perform near chance, and the best reasoning model lags humans by nearly 60%. We further find that spatially fine-tuned models still fail to generalize effectively on our benchmark. Fine-grained error analysis exposes systematic failures in geometric reasoning, motion grounding, long-horizon prediction, and cross-video correspondence. We also show that typical frame-sampling strategies transfer poorly to our reasoning-intensive benchmark, and that neither 3D spatial cues nor chain-of-thought prompting yields meaningful gains. We expect our benchmark to establish a solid testbed for advancing video-based spatial intelligence.

</details>


### [3] [BabyVLM-V2: Toward Developmentally Grounded Pretraining and Benchmarking of Vision Foundation Models](https://arxiv.org/abs/2512.10932)
*Shengao Wang,Wenqi Wang,Zecheng Wang,Max Whitton,Michael Wakeham,Arjun Chandra,Joey Huang,Pengyue Zhu,Helen Chen,David Li,Jeffrey Li,Shawn Li,Andrew Zagula,Amy Zhao,Andrew Zhu,Sayaka Nakamura,Yuki Yamamoto,Jerry Jun Yokono,Aaron Mueller,Bryan A. Plummer,Kate Saenko,Venkatesh Saligrama,Boqing Gong*

Main category: cs.CV

TL;DR: 该研究通过开发BabyVLM-V2框架，引入了长时序多面向的预训练集和DevCV工具箱，旨在通过婴儿视角优化视觉语言模型。实验结果表明，从头开始预训练的紧凑型模型在某些任务上优于GPT-4o，希望该框架能推动视觉基础模型发展的研究。


<details>
  <summary>Details</summary>
Motivation: 现有视觉基础模型的预训练数据集往往缺乏婴儿发展特定的视角，本文提出的BabyVLM-V2旨在填补这一空白，通过长时序的纵向数据和DevCV工具箱来更好地理解婴儿的认知能力。

Method: 该论文通过构建一个开发性基础模型框架BabyVLM-V2，包含一个丰富的纵向婴儿为中心的视听数据集和DevCV工具箱，该工具箱将NIH婴儿工具箱中视力相关的度量标准转换为十项多模态任务，以此来评估模型。此外，研究采用从零开始预训练的方法，训练了一个小型但高效的模型。

Result: 实验结果表明，使用从头开始预训练的紧凑型模型在某些任务上表现优于GPT-4o，特别是在针对早期儿童认知能力的任务上。

Conclusion: 研究团队认为，这种有原则的、统一的BabyVLM-V2框架将加速发展合适视觉基础模型的研究工作。

Abstract: Early children's developmental trajectories set up a natural goal for sample-efficient pretraining of vision foundation models. We introduce BabyVLM-V2, a developmentally grounded framework for infant-inspired vision-language modeling that extensively improves upon BabyVLM-V1 through a longitudinal, multifaceted pretraining set, a versatile model, and, most importantly, DevCV Toolbox for cognitive evaluation. The pretraining set maximizes coverage while minimizing curation of a longitudinal, infant-centric audiovisual corpus, yielding video-utterance, image-utterance, and multi-turn conversational data that mirror infant experiences. DevCV Toolbox adapts all vision-related measures of the recently released NIH Baby Toolbox into a benchmark suite of ten multimodal tasks, covering spatial reasoning, memory, and vocabulary understanding aligned with early children's capabilities. Experimental results show that a compact model pretrained from scratch can achieve competitive performance on DevCV Toolbox, outperforming GPT-4o on some tasks. We hope the principled, unified BabyVLM-V2 framework will accelerate research in developmentally plausible pretraining of vision foundation models.

</details>


### [4] [Any4D: Unified Feed-Forward Metric 4D Reconstruction](https://arxiv.org/abs/2512.10935)
*Jay Karhade,Nikhil Keetha,Yuchen Zhang,Tanisha Gupta,Akash Sharma,Sebastian Scherer,Deva Ramanan*

Main category: cs.CV

TL;DR: Any4D 是一种可扩展的多视图 transformer，能进行大规模、密集的 4D 重建，直接输出每像素的运动和几何预测。它支持多种传感器数据，且在性能和计算效率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法多针对双视图密集场景流或稀疏3D点跟踪，而 Any4D 能同时进行大规模、密集的 4D 重建，特别适用于需要处理多种传感器数据的场景。

Method: Any4D 使用一种模块化的方式来表示 4D 场景：每个视图的 4D 预测由视点因素（深度图和相机内参）和绝对因素（相机外参和场景流动）组成，后者以全局世界坐标表示。

Result: Any4D 在多个应用场景中展现出优越的性能，准确率提高了2-3倍，并且计算效率提高了15倍。

Conclusion: Any4D 为 4D 重建提供了更灵活、更强大的框架，为多下游应用打开了新的可能性。

Abstract: We present Any4D, a scalable multi-view transformer for metric-scale, dense feed-forward 4D reconstruction. Any4D directly generates per-pixel motion and geometry predictions for N frames, in contrast to prior work that typically focuses on either 2-view dense scene flow or sparse 3D point tracking. Moreover, unlike other recent methods for 4D reconstruction from monocular RGB videos, Any4D can process additional modalities and sensors such as RGB-D frames, IMU-based egomotion, and Radar Doppler measurements, when available. One of the key innovations that allows for such a flexible framework is a modular representation of a 4D scene; specifically, per-view 4D predictions are encoded using a variety of egocentric factors (depthmaps and camera intrinsics) represented in local camera coordinates, and allocentric factors (camera extrinsics and scene flow) represented in global world coordinates. We achieve superior performance across diverse setups - both in terms of accuracy (2-3X lower error) and compute efficiency (15X faster), opening avenues for multiple downstream applications.

</details>


### [5] [OmniView: An All-Seeing Diffusion Model for 3D and 4D View Synthesis](https://arxiv.org/abs/2512.10940)
*Xiang Fan,Sharath Girish,Vivek Ramanujan,Chaoyang Wang,Ashkan Mirzaei,Petr Sushko,Aliaksandr Siarohin,Sergey Tulyakov,Ranjay Krishna*

Main category: cs.CV

TL;DR: OmniView 是一个统一框架，能够实现广泛的 4D 一致性任务，包括新颖视角合成、文本到视频控制、图像到视频转换等。该方法分别表示空间、时间、视图条件，提供灵活的输入组合。在多种基准和指标上，它表现出色，优于特定任务模型，特别是在多视角新型视角LLFF数据集和动态 Neuralt3DVideo 基准上，图像质量得分分别提高 33% 和 60%，并且还能够在基于文本的视频生成中减少 4 倍的相机轨迹误差。


<details>
  <summary>Details</summary>
Motivation: 当前的方法主要针对特定的4D一致性任务进行训练和优化，未能充分利用可用的3D/4D数据。OmniView 的动机是提供一个通用框架，能够处理广泛的4D一致性任务，从而在单一模型中实现更强的一致性和灵活性。

Method: OmniView 通过将空间、时间和视图条件分开表示来工作。具体来说，它通过这个结构化的表示方法，可以在各种输入环境中灵活地组合这些条件。此外，该方法提供了一种处理和生成4D视频数据的新方法。

Result: OmniView 在多种基准和指标上超过或匹配了特定任务模型。具体结果包括在多视角新颖视角LLFF数据集上图像质量得分提高了33%，动态 Neuralt3DVideo 基准上提高了60%，以及在静态摄像机控制的 RE-10K 数据集上提高了20%。

Conclusion: 研究认为，OmniView 的方法对现有的4D一致性任务非常有效，展示了其作为通用4D视频模型的可行性。这种方法提供了一种新的视角来处理和生成4D视频数据，未来可能对其它视觉任务也有潜在的应用价值。

Abstract: Prior approaches injecting camera control into diffusion models have focused on specific subsets of 4D consistency tasks: novel view synthesis, text-to-video with camera control, image-to-video, amongst others. Therefore, these fragmented approaches are trained on disjoint slices of available 3D/4D data. We introduce OmniView, a unified framework that generalizes across a wide range of 4D consistency tasks. Our method separately represents space, time, and view conditions, enabling flexible combinations of these inputs. For example, OmniView can synthesize novel views from static, dynamic, and multiview inputs, extrapolate trajectories forward and backward in time, and create videos from text or image prompts with full camera control. OmniView is competitive with task-specific models across diverse benchmarks and metrics, improving image quality scores among camera-conditioned diffusion models by up to 33\% in multiview NVS LLFF dataset, 60\% in dynamic NVS Neural 3D Video benchmark, 20\% in static camera control on RE-10K, and reducing camera trajectory errors by 4x in text-conditioned video generation. With strong generalizability in one model, OmniView demonstrates the feasibility of a generalist 4D video model. Project page is available at https://snap-research.github.io/OmniView/

</details>


### [6] [Mull-Tokens: Modality-Agnostic Latent Thinking](https://arxiv.org/abs/2512.10941)
*Arijit Ray,Ahmed Abdelkader,Chengzhi Mao,Bryan A. Plummer,Kate Saenko,Ranjay Krishna,Leonidas Guibas,Wen-Sheng Chu*

Main category: cs.CV

TL;DR: Mull-Tokens 是一种模态无关的潜在令牌，预先训练以保留图像或文本模态中的中间信息，以便模型可以自由思考以获得正确答案。这种令牌在四个具有挑战性的空间推理基准测试中表现出色，尤其是在需要大量推理的任务上。


<details>
  <summary>Details</summary>
Motivation: 现有模型在处理空间、时间、使用场景等语言无法完全表达的信息时表现不佳，过于依赖于特定工具或手工制作的数据。Mull-Tokens 提供了一种替代方法，无需调用专业工具，也不需要昂贵的图像生成或手工制作的数据，从而改进了文本信息、图像信息和中间信息的处理。

Method: Mull-Tokens 通过交替使用文本-图像痕迹的监督训练，以及后续的无监督微调，结合了模态中的信息进行推理。这种方法被用来改进四个不同空间推理基准测试的结果。

Result: Mull-Tokens 在四个具有挑战性的空间推理基准中表现出明显的改进，平均改进率达到+3%，在解谜推理密集部分的最大改进率为+16%，比最强的基线更胜一筹。

Conclusion: Mull-Tokens 提供了一种处理视觉和文本融合推理的简单方法，进一步了嵌入式视觉和文本推理的讨论，提出了一种新的方式来抽象地思考多种模态。

Abstract: Reasoning goes beyond language; the real world requires reasoning about space, time, affordances, and much more that words alone cannot convey. Existing multimodal models exploring the potential of reasoning with images are brittle and do not scale. They rely on calling specialist tools, costly generation of images, or handcrafted reasoning data to switch between text and image thoughts. Instead, we offer a simpler alternative -- Mull-Tokens -- modality-agnostic latent tokens pre-trained to hold intermediate information in either image or text modalities to let the model think free-form towards the correct answer. We investigate best practices to train Mull-Tokens inspired by latent reasoning frameworks. We first train Mull-Tokens using supervision from interleaved text-image traces, and then fine-tune without any supervision by only using the final answers. Across four challenging spatial reasoning benchmarks involving tasks such as solving puzzles and taking different perspectives, we demonstrate that Mull-Tokens improve upon several baselines utilizing text-only reasoning or interleaved image-text reasoning, achieving a +3% average improvement and up to +16% on a puzzle solving reasoning-heavy split compared to our strongest baseline. Adding to conversations around challenges in grounding textual and visual reasoning, Mull-Tokens offers a simple solution to abstractly think in multiple modalities.

</details>


### [7] [AlcheMinT: Fine-grained Temporal Control for Multi-Reference Consistent Video Generation](https://arxiv.org/abs/2512.10943)
*Sharath Girish,Viacheslav Ivanov,Tsai-Shien Chen,Hao Chen,Aliaksandr Siarohin,Sergey Tulyakov*

Main category: cs.CV

TL;DR: AlcheMinT 提出了一种新的框架，通过引入显式的基于时间戳的条件机制，增强主题驱动视频生成中的时间控制，同时保持视觉身份的一致性和视频的真实度。


<details>
  <summary>Details</summary>
Motivation: 现有方法在主题驱动视频生成中未能提供细粒度的时间控制，这对于合成视频、故事板和可控动画等应用来说是必要的。

Method: AlcheMinT 通过引入新型的时序编码机制，将时间区间与主题身份相关联，同时结合预训练的视频生成模型的位置嵌入。此外，文本标记被引入以增强视觉身份与视频字幕之间的联系，通过标记级连接避免额外的交叉注意模块。

Result: 实验结果表明，AlcheMinT 在视觉质量方面达到了最先进的视频个性化方法的水平，同时首次在视频中实现了对多主题生成的精确时间控制。

Conclusion: AlcheMinT 是一个统一的框架，通过引入时间戳条件增强了主题驱动视频生成中的时间控制，为视频合成应用提供了新的可能性。

Abstract: Recent advances in subject-driven video generation with large diffusion models have enabled personalized content synthesis conditioned on user-provided subjects. However, existing methods lack fine-grained temporal control over subject appearance and disappearance, which are essential for applications such as compositional video synthesis, storyboarding, and controllable animation. We propose AlcheMinT, a unified framework that introduces explicit timestamps conditioning for subject-driven video generation. Our approach introduces a novel positional encoding mechanism that unlocks the encoding of temporal intervals, associated in our case with subject identities, while seamlessly integrating with the pretrained video generation model positional embeddings. Additionally, we incorporate subject-descriptive text tokens to strengthen binding between visual identity and video captions, mitigating ambiguity during generation. Through token-wise concatenation, AlcheMinT avoids any additional cross-attention modules and incurs negligible parameter overhead. We establish a benchmark evaluating multiple subject identity preservation, video fidelity, and temporal adherence. Experimental results demonstrate that AlcheMinT achieves visual quality matching state-of-the-art video personalization methods, while, for the first time, enabling precise temporal control over multi-subject generation within videos. Project page is at https://snap-research.github.io/Video-AlcheMinT

</details>


### [8] [SceneMaker: Open-set 3D Scene Generation with Decoupled De-occlusion and Pose Estimation Model](https://arxiv.org/abs/2512.10957)
*Yukai Shi,Weiyu Li,Zihao Wang,Hongyang Li,Xingyu Chen,Ping Tan,Lei Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种名为SceneMaker的解耦3D场景生成框架。通过解耦消隐模型和3D对象生成，并结合图像数据和收集的消隐数据，以及提出统一的姿态估计模型，能够更好地处理严重消隐和开放集场景。


<details>
  <summary>Details</summary>
Motivation: 现有的方法在处理严重消隐和开放集场景时，难以同时生成高质量的几何结构和准确的姿态，本文提出的方法旨在解决这些问题。

Method: 本文方法首先解耦了消隐模型和3D对象生成，通过使用图像数据和收集的消隐数据来增强消隐模型，以处理更多样化的开放集消隐模式。此外，提出了一个统一的姿态估计模型，结合全局和局部机制，同时进行了自我注意和跨注意机制。

Result: 通过本文提出的方法，能够更有效地处理室内和开放集场景，指标实验验证了该方法的优越性。

Conclusion: 本文提出了一个解耦的3D场景生成框架，并提出了一系列改进措施，使得在处理复杂3D场景方面表现出了显著的优势。

Abstract: We propose a decoupled 3D scene generation framework called SceneMaker in this work. Due to the lack of sufficient open-set de-occlusion and pose estimation priors, existing methods struggle to simultaneously produce high-quality geometry and accurate poses under severe occlusion and open-set settings. To address these issues, we first decouple the de-occlusion model from 3D object generation, and enhance it by leveraging image datasets and collected de-occlusion datasets for much more diverse open-set occlusion patterns. Then, we propose a unified pose estimation model that integrates global and local mechanisms for both self-attention and cross-attention to improve accuracy. Besides, we construct an open-set 3D scene dataset to further extend the generalization of the pose estimation model. Comprehensive experiments demonstrate the superiority of our decoupled framework on both indoor and open-set scenes. Our codes and datasets is released at https://idea-research.github.io/SceneMaker/.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [9] [Quantifying Emotional Tone in Tolkien's The Hobbit: Dialogue Sentiment Analysis with RegEx, NRC-VAD, and Python](https://arxiv.org/abs/2512.10865)
*Lilin Qiu*

Main category: cs.CL

TL;DR: 该研究利用计算文本分析方法，通过情感词汇学词典量化《霍比特人》中对话的情感维度，发现对话整体上保持乐观且平静的基调，并随着时间推进逐渐增强角色的主动性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在揭示文学作品中的微妙情感结构，强调数字方法在文学分析中的应用。

Method: 研究通过正则表达式提取对话，进行预处理，并使用NRC-VAD情感词汇学词典量化情感维度，最后通过视觉化展示情感模式。

Result: 研究结果显示，对话整体上积极（高情感价值）且平静（低情绪唤醒），随故事推进逐渐增强角色的主动性。情感模式反映了小说中的情感节奏：危险和激动的时刻通常被幽默、同伴情谊和解脱所平衡。

Conclusion: 结合计算工具与文学诠释，研究证明数字方法能够揭示文学中的微妙情感结构，揭示塑造《霍比特人》叙事的情感节奏和微妙调适。

Abstract: This study analyzes the emotional tone of dialogue in J. R. R. Tolkien's The Hobbit (1937) using computational text analysis. Dialogue was extracted with regular expressions, then preprocessed, and scored using the NRC-VAD lexicon to quantify emotional dimensions. The results show that the dialogue maintains a generally positive (high valence) and calm (low arousal) tone, with a gradually increasing sense of agency (dominance) as the story progresses. These patterns reflect the novel's emotional rhythm: moments of danger and excitement are regularly balanced by humor, camaraderie, and relief. Visualizations -- including emotional trajectory graphs and word clouds -- highlight how Tolkien's language cycles between tension and comfort. By combining computational tools with literary interpretation, this study demonstrates how digital methods can uncover subtle emotional structures in literature, revealing the steady rhythm and emotional modulation that shape the storytelling in The Hobbit.

</details>


### [10] [Computational emotion analysis with multimodal LLMs: Current evidence on an emerging methodological opportunity](https://arxiv.org/abs/2512.10882)
*Hauke Licht*

Main category: cs.CL

TL;DR: 本文通过评估当前多模态大型语言模型在视频情感唤醒分析中的表现，填补了多模态AI在情绪分析有效性的证据缺口。


<details>
  <summary>Details</summary>
Motivation: 随着研究越来越多地利用音频-视觉材料来分析情绪展示，多模态生成AI带来了巨大的进展，但目前缺乏关于多模态AI在情绪分析中的有效性的实证证据。

Method: 通过在两个互补的数据集中评估当前多模态大型语言模型的情感唤醒评分，其中数据集包含由人类标注的视频记录。

Result: 在理想条件下，多模态AI的情感唤醒评分具有高度可靠性且几乎没有表现出人口统计学上的偏见。但在真实世界议会辩论的演讲者录制中，多模态AI在唤醒评分上的表现无法达到预期，并可能对后续的统计推断产生负面影响。

Conclusion: 本文强调了继续对新兴的生成AI方法在政治分析中的应用进行深入评估的必要性，并提供了一个合适的可重复的框架。

Abstract: Emotions are central to politics and analyzing their role in political communication has a long tradition. As research increasingly leverages audio-visual materials to analyze the display of emotions, the emergence of multimodal generative AI promises great advances. However, we lack evidence about the effectiveness of multimodal AI in emotion analysis. This paper addresses this gap by evaluating current multimodal large language models (mLLMs) in video-based analysis of emotional arousal in two complementary data sets of human-labeled video recordings. I find that under ideal circumstances, mLLMs' emotional arousal ratings are highly reliable and show little to know indication of demographic bias. However, in recordings of speakers in real-world parliamentary debates, mLLMs' arousal ratings fail to deliver on this promise with potential negative consequences for downstream statistical inferences. This study therefore underscores the need for continued, thorough evaluation of emerging generative AI methods in political analysis and contributes a suitable replicable framework.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [11] [LLMs Can Assist with Proposal Selection at Large User Facilities](https://arxiv.org/abs/2512.10895)
*Lijie Ding,Janell Thomson,Jon Taylor,Changwoo Do*

Main category: cs.AI

TL;DR: 该研究探索了大型语言模型（LLMs）在大型用户设施提案筛选过程中的应用，展示了LLMs在评估提案优劣方面相较于人工评分的优势，包括减少偏见、提高一致性并大幅降低成本。通过具体案例，研究证明了LLMs的评估结果与人工评分高度相关，并且具备更优的成本效益。


<details>
  <summary>Details</summary>
Motivation: 传统的提案筛选依赖于人工评分，存在评分不一致性和偏见问题。为了克服这个问题，研究探讨了利用LLMs进行提案筛选的可能性。

Method: 通过使用来自三个SNS光束线的独特提案和出版记录，研究团队开发了一种利用LLMs的方法，以评估提案的相对优劣。

Result: 研究表明，LLMs的评估结果与人工评分高度相关，特别是在去除部分异常值后。此外，LLMs在识别高潜在发表的提案方面与人工评分具有相同的效果，而成本仅为后者的百分之一。

Conclusion: 研究结论认为，LLMs可以作为一种大规模、一致且低成本的替代方案，用于提案筛选过程，尤其是对于文章发布的潜在价值提供定量评估，并支持审查委员会做出更有效率的决策。

Abstract: We explore how large language models (LLMs) can enhance the proposal selection process at large user facilities, offering a scalable, consistent, and cost-effective alternative to traditional human review. Proposal selection depends on assessing the relative strength among submitted proposals; however, traditional human scoring often suffers from weak inter-proposal correlations and is subject to reviewer bias and inconsistency. A pairwise preference-based approach is logically superior, providing a more rigorous and internally consistent basis for ranking, but its quadratic workload makes it impractical for human reviewers. We address this limitation using LLMs. Leveraging the uniquely well-curated proposals and publication records from three beamlines at the Spallation Neutron Source (SNS), Oak Ridge National Laboratory (ORNL), we show that the LLM rankings correlate strongly with the human rankings (Spearman $ρ\simeq 0.2-0.8$, improving to $\geq 0.5$ after 10\% outlier removal). Moreover, LLM performance is no worse than that of human reviewers in identifying proposals with high publication potential, while costing over two orders of magnitude less. Beyond ranking, LLMs enable advanced analyses that are challenging for humans, such as quantitative assessment of proposal similarity via embedding models, which provides information crucial for review committees.

</details>


### [12] [Multi-Granular Node Pruning for Circuit Discovery](https://arxiv.org/abs/2512.10903)
*Muhammad Umair Haider,Hammad Rizwan,Hassan Sajjad,A. B. Siddique*

Main category: cs.AI

TL;DR: 本文提出了一种节点级别的剪枝框架，对大规模语言模型中的电路进行发现，该方法能够兼顾效率和精细度，能够在单一微调过程中实现全面压缩，并且识别出比先前方法更小的电路，同时保持任务性能，且具备更低的内存占用。


<details>
  <summary>Details</summary>
Motivation: 现有的电路发现方法多依赖于迭代边剪枝，效率低且粒度粗糙，难以捕捉到更精细的结构。本文希望开发一种兼顾效率和精细度的新方法。

Method: 引入了一种节点级别的剪枝框架，可以对整个块直到单个神经元等多层次的粒度进行剪枝。通过在统一的目标函数中加入粒度特定的稀疏性惩罚，指导剪枝过程，并在一个精细调整阶段实现全面压缩。

Result: 实验结果表明，本文方法可以识别比先前方法更小的电路，同时保持任务性能，并且几乎没有内存消耗，内存占用比之前方法低5到10倍。

Conclusion: 本文提出了一种高效的电路发现方法，克服了现有方法中的效率和粒度问题，可以实现大规模语言模型的有效压缩，保持任务性能并减少内存消耗。

Abstract: Circuit discovery aims to identify minimal subnetworks that are responsible for specific behaviors in large language models (LLMs). Existing approaches primarily rely on iterative edge pruning, which is computationally expensive and limited to coarse-grained units such as attention heads or MLP blocks, overlooking finer structures like individual neurons. We propose a node-level pruning framework for circuit discovery that addresses both scalability and granularity limitations. Our method introduces learnable masks across multiple levels of granularity, from entire blocks to individual neurons, within a unified optimization objective. Granularity-specific sparsity penalties guide the pruning process, allowing a comprehensive compression in a single fine-tuning run. Empirically, our approach identifies circuits that are smaller in nodes than those discovered by prior methods; moreover, we demonstrate that many neurons deemed important by coarse methods are actually irrelevant, while still maintaining task performance. Furthermore, our method has a significantly lower memory footprint, 5-10x, as it does not require keeping intermediate activations in the memory to work.

</details>
