<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 130]
- [cs.CL](#cs.CL) [Total: 69]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.AI](#cs.AI) [Total: 67]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Dual-Signal Adaptive KV-Cache Optimization for Long-Form Video Understanding in Vision-Language Models](https://arxiv.org/abs/2602.14236)
*Vishnu Sai,Dheeraj Sai,Srinath B,Girish Varma,Priyesh Shukla*

Main category: cs.CV

TL;DR: Sali-Cache 提出了一种新颖的先验优化框架，通过前瞻性的内存管理策略，基于光流分析的时空滤波器来优化 Key-Value 缓存，实现 2.20 倍的内存使用压缩比，同时保持模型性能。


<details>
  <summary>Details</summary>
Motivation: 对长视频内容进行高效处理是 Vision-Language 模型面临的挑战之一，现有方法主要依赖于反应式策略，导致显著的计算浪费。

Method: Sali-Cache 采用双重信号自适应缓存机制，结合光流分析的时域滤波器和注视检测的空间滤波器，在计算密集的注意力操作之前进行智能化的内存分配。

Result: 实验结果表明，与 LLaVA 1.6 架构相比，Sali-Cache 能以 2.20 倍的压缩比有效使用内存，在 BLEU、ROUGE-L 和 Exact Match 评测量上保持 100% 的准确性。同时，它能够在相同的内存预算下维持长时间的上下文特征，使长视频内容可以在消费级硬件上高效处理。

Conclusion: 该方法显著提升了解决 Vision-Language 模型在长视频内容处理中的高效性，证明了在有限的硬件资源下实现高性能处理的可能性。

Abstract: Vision-Language Models (VLMs) face a critical memory bottleneck when processing long-form video content due to the linear growth of the Key-Value (KV) cache with sequence length. Existing solutions predominantly employ reactive eviction strategies that compute full attention matrices before discarding tokens, resulting in substantial computational waste. We propose Sali-Cache, a novel a priori optimization framework that implements dual-signal adaptive caching through proactive memory management. By integrating a temporal filter based on optical flow analysis for detecting inter-frame redundancy and a spatial filter leveraging saliency detection for identifying visually significant regions, Sali-Cache intelligently manages memory allocation before entering computationally expensive attention operations. Experimental evaluation on the LLaVA 1.6 architecture demonstrates that our method achieves a 2.20x compression ratio in effective memory usage while maintaining 100% accuracy across BLEU, ROUGE-L, and Exact Match metrics. Furthermore, under identical memory budget constraints, Sali-Cache preserves context-rich features over extended temporal durations without degrading model performance, enabling efficient processing of long-form video content on consumer-grade hardware.

</details>


### [2] [Beyond Ground: Map-Free LiDAR Relocalization for UAVs](https://arxiv.org/abs/2602.13267)
*Hengyu Mu,Jianshi Wu,Yuxin Guo,XianLian Lin,Qingyong Hu,Chenglu Wen,Cheng Wang*

Main category: cs.CV

TL;DR: 本文提出了一种针对无人机的新型无地图激光雷达重定位框架MAILS，通过局部保持滑窗注意模块提取稀疏点云中的局部判别几何特征，并设计了坐标无关特征初始化模块和局部不变位置编码机制，以增强特征提取的鲁棒性。此外，还构建了一个大规模的无人机激光雷达定位数据集，用于评估无人机下真实条件下的重定位性能。实验结果表明，该方法在定位精度上优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有基于激光雷达的重定位方法在无人机场景下的表现不佳，尤其是在处理大角度旋转和高度变化时。本文旨在解决这些问题。

Method: MAILS框架包含多个模块，如局部保持滑窗注意模块、坐标无关特征初始化模块和局部不变位置编码机制，这些模块能有效处理稀疏点云下的特征提取问题。

Result: 通过大量的实验表明，所提出的MAILS框架在无人机环境下实现了高精度的重定位，并且相比现有技术提高了性能。

Conclusion: 提出了MAILS框架，并且通过构建相应的数据集进行了充分的验证，该方法在无人机重定位领域的应用前景广阔。

Abstract: Localization is a fundamental capability in unmanned aerial vehicle (UAV) systems. Map-free LiDAR relocalization offers an effective solution for achieving high-precision positioning in environments with weak or unavailable GNSS signals. However, existing LiDAR relocalization methods are primarily tailored to autonomous driving, exhibiting significantly degraded accuracy in UAV scenarios. In this paper, we propose MAILS, a novel map-free LiDAR relocalization framework for UAVs. A Locality-Preserving Sliding Window Attention module is first introduced to extract locally discriminative geometric features from sparse point clouds. To handle substantial yaw rotations and altitude variations encountered during UAV flight, we then design a coordinate-independent feature initialization module and a locally invariant positional encoding mechanism, which together significantly enhance the robustness of feature extraction. Furthermore, existing LiDAR-based relocalization datasets fail to capture real-world UAV flight characteristics, such as irregular trajectories and varying altitudes. To address this gap, we construct a large-scale LiDAR localization dataset for UAVs, which comprises four scenes and various flight trajectories, designed to evaluate UAV relocalization performance under realistic conditions. Extensive experiments demonstrate that our method achieves satisfactory localization precision and consistently outperforms existing techniques by a significant margin. Our code and dataset will be released soon.

</details>


### [3] [COOPERTRIM: Adaptive Data Selection for Uncertainty-Aware Cooperative Perception](https://arxiv.org/abs/2602.13287)
*Shilpa Mukhopadhyay,Amit Roy-Chowdhury,Hang Qiu*

Main category: cs.CV

TL;DR: CoOPERTRIM框架通过引入新颖的时间相关不确定性度量以及数据驱动的机制动态决定信息共享量，从而最大化在保持性能的同时减少带宽消耗。


<details>
  <summary>Details</summary>
Motivation: 现有的感知分享策略仅在每帧传输子集特征，这虽然可以保持性能，但依然受限于无线通信带宽。COOPERTRIM旨在通过减少不必要的冗余信息传输来优化感知分享过程。

Method: COOPERTRIM框架利用了时序连续性，提出了一种新型的时间相关不确定性度量，用于度量特征的相关性，并设计了一个数据驱动的机制来动态决定要分享的信息量。

Result: 在多项开放式合作分割和检测模型中，COOPERTRIM实现了高达80.28%和72.52%的带宽减少，同时保持了可比的准确性。相对而言，它还能将IoU提升45.54%，并降低72%的带宽消耗。结合压缩策略，即使带宽减少了98.54%，也仍能保持IoU性能。此外， Qualitative结果展现COOPERTRIM具备灵活性，能够适应环境变化、定位误差和通信延迟。

Conclusion: COOPERTRIM通过时间相关性捕捉动态环境特征，减少了静态信息的重复传输，实现了高效的相干感知分享，为实际部署奠定了基础。

Abstract: Cooperative perception enables autonomous agents to share encoded representations over wireless communication to enhance each other's live situational awareness. However, the tension between the limited communication bandwidth and the rich sensor information hinders its practical deployment. Recent studies have explored selection strategies that share only a subset of features per frame while striving to keep the performance on par. Nevertheless, the bandwidth requirement still stresses current wireless technologies. To fundamentally ease the tension, we take a proactive approach, exploiting the temporal continuity to identify features that capture environment dynamics, while avoiding repetitive and redundant transmission of static information. By incorporating temporal awareness, agents are empowered to dynamically adapt the sharing quantity according to environment complexity. We instantiate this intuition into an adaptive selection framework, COOPERTRIM, which introduces a novel conformal temporal uncertainty metric to gauge feature relevance, and a data-driven mechanism to dynamically determine the sharing quantity. To evaluate COOPERTRIM, we take semantic segmentation and 3D detection as example tasks. Across multiple open-source cooperative segmentation and detection models, COOPERTRIM achieves up to 80.28% and 72.52% bandwidth reduction respectively while maintaining a comparable accuracy. Relative to other selection strategies, COOPERTRIM also improves IoU by as much as 45.54% with up to 72% less bandwidth. Combined with compression strategies, COOPERTRIM can further reduce bandwidth usage to as low as 1.46% without compromising IoU performance. Qualitative results show COOPERTRIM gracefully adapts to environmental dynamics, localization error, and communication latency, demonstrating flexibility and paving the way for real-world deployment.

</details>


### [4] [NutVLM: A Self-Adaptive Defense Framework against Full-Dimension Attacks for Vision Language Models in Autonomous Driving](https://arxiv.org/abs/2602.13293)
*Xiaoxu Peng,Dong Zhou,Jianwen Zhang,Guanghui Sun,Anh Tu Ngo,Anupam Chattopadhyay*

Main category: cs.CV

TL;DR: 该研究提出了一种名为NutVLM的综合自适应防御框架，旨在全面保护自动驾驶感知-决策生命周期中的视觉语言模型。NutVLM通过NutNet++和Expert-guided Adversarial Prompt Tuning（EAPT）机制，有效防御物理局部和全球不可感知的对抗性威胁。实验结果显示，NutVLM在Dolphins基准上的整体指标（如准确率、语言得分和GPT得分）提高了4.89%，证明了其在智能交通系统中广泛应用的前景。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型在自动驾驶中能够提升感知能力，但易受到对抗性威胁的影响，这些问题可能导致严重的安全风险。 NutVLM的研究旨在通过自适应防御框架提升这些模型的鲁棒性和安全性。

Method: 研究提出了一种名为NutVLM的防御框架，通过NutNet++统一检测和净化机制，以及EAPT机制来对抗局部和全局的对抗性威胁。NutNet++能够识别出良性样本、局部掩饰和全局扰动，并采用高效灰度掩码去除局部威胁，使用基于梯度的隐空间优化和离散投影生成矫正的驾驶提示来应对全局影响，无需进行全面模型微调。

Result: NutVLM在Dolphins基准上的实验结果表明，该防御框架能够显著提高总体指标，具体提高了4.89%的准确性，同时展示了其作为智能交通系统中广泛适用的可扩展安全解决方案的有效性。

Conclusion: 研究成功开发了一个名为NutVLM的自适应防御框架，证明了它在提升视觉语言模型在自动驾驶中的鲁棒性方面的有效性。未来的工作计划将重视研究和评估NutVLM在更多实际场景下的应用。

Abstract: Vision Language Models (VLMs) have advanced perception in autonomous driving (AD), but they remain vulnerable to adversarial threats. These risks range from localized physical patches to imperceptible global perturbations. Existing defense methods for VLMs remain limited and often fail to reconcile robustness with clean-sample performance. To bridge these gaps, we propose NutVLM, a comprehensive self-adaptive defense framework designed to secure the entire perception-decision lifecycle. Specifically, we first employ NutNet++ as a sentinel, which is a unified detection-purification mechanism. It identifies benign samples, local patches, and global perturbations through three-way classification. Subsequently, localized threats are purified via efficient grayscale masking, while global perturbations trigger Expert-guided Adversarial Prompt Tuning (EAPT). Instead of the costly parameter updates of full-model fine-tuning, EAPT generates "corrective driving prompts" via gradient-based latent optimization and discrete projection. These prompts refocus the VLM's attention without requiring exhaustive full-model retraining. Evaluated on the Dolphins benchmark, our NutVLM yields a 4.89% improvement in overall metrics (e.g., Accuracy, Language Score, and GPT Score). These results validate NutVLM as a scalable security solution for intelligent transportation. Our code is available at https://github.com/PXX/NutVLM.

</details>


### [5] [VisPhyWorld: Probing Physical Reasoning via Code-Driven Video Reconstruction](https://arxiv.org/abs/2602.13294)
*Jiarong Liang,Max Ku,Ka-Hei Hui,Ping Nie,Wenhu Chen*

Main category: cs.CV

TL;DR: 该研究提出了VisPhyWorld框架，该框架通过要求模型生成可执行的模拟器代码来评估其物理推理能力；并通过VisPhyBench基准测试平台进一步测试了模型在重建外观和模拟物理行为方面的能力。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试主要依赖于视觉问答和违背期望等识别式协议，这些方法往往无法确保模型必须存在一个明确的、可测试的物理假设才能回答问题。因此，需要一个新的框架来评估模型的实际物理推理能力。

Method: VisPhyWorld框架要求模型从视觉观察生成可执行的模拟器代码，这种方法使得模型内化的世界表述可以直接被检查、编辑和反驳。此外，研究构建了VisPhyBench基准，包括209个评估场景以及一套系统性的评估协议。

Result: 使用VisPhyBench基准测试后，97.7%的场景生成了有效的重构视频。虽然最新的MLLMs在语义场景理解方面表现强劲，但在推断物理参数和模拟一致的物理动态方面遇到了困难。

Conclusion: 这项研究显著提升了对MLLMs实际物理推理能力的理解，并提出了评估其物理理解力的有效框架和基准。

Abstract: Evaluating whether Multimodal Large Language Models (MLLMs) genuinely reason about physical dynamics remains challenging. Most existing benchmarks rely on recognition-style protocols such as Visual Question Answering (VQA) and Violation of Expectation (VoE), which can often be answered without committing to an explicit, testable physical hypothesis. We propose VisPhyWorld, an execution-based framework that evaluates physical reasoning by requiring models to generate executable simulator code from visual observations. By producing runnable code, the inferred world representation is directly inspectable, editable, and falsifiable. This separates physical reasoning from rendering. Building on this framework, we introduce VisPhyBench, comprising 209 evaluation scenes derived from 108 physical templates and a systematic protocol that evaluates how well models reconstruct appearance and reproduce physically plausible motion. Our pipeline produces valid reconstructed videos in 97.7% on the benchmark. Experiments show that while state-of-the-art MLLMs achieve strong semantic scene understanding, they struggle to accurately infer physical parameters and to simulate consistent physical dynamics.

</details>


### [6] [MFN Decomposition and Related Metrics for High-Resolution Range Profiles Generative Models](https://arxiv.org/abs/2602.13296)
*Edwyn Brient,Santiago Velasco-Forero,Rami Kassab*

Main category: cs.CV

TL;DR: 本文将HRRP数据分解为掩码、特征和噪声三个部分，并提出了基于这些数据物理解释的两个新度量方法，证明了其在复杂任务中的区分能力。


<details>
  <summary>Details</summary>
Motivation: 随着高分辨率距离轮廓(HRRP)数据在雷达自动目标识别(RATR)中的应用，使用生成模型填充数据集的潜力引起了关注。现有的评价方法主要依赖于分类模型，这限制了透明性和多维度评价。因此，本文旨在提出新的度量方法以弥补这一不足。

Method: 本文将HRRP数据分解为三个部分：掩码、特征和噪声。为了实现这一目标，他们提出了基于数据物理性质的新度量方法。

Result: 本文使用昂贵的数据集评估了新提出的度量方法，并展示了其在挑战性任务中的区分能力。

Conclusion: 本文提出的基于物理解释的度量方法在HRRP生成模型评估中显示出了优越性，可进行更深入的评价分析。

Abstract: High-resolution range profile (HRRP ) data are in vogue in radar automatic target recognition (RATR). With the interest in classifying models using HRRP, filling gaps in datasets using generative models has recently received promising contributions. Evaluating generated data is a challenging topic, even for explicit data like face images. However, the evaluation methods used in the state-ofthe-art of HRRP generation rely on classification models. Such models, called ''black-box'', do not allow either explainability on generated data or multi-level evaluation. This work focuses on decomposing HRRP data into three components: the mask, the features, and the noise. Using this decomposition, we propose two metrics based on the physical interpretation of those data. We take profit from an expensive dataset to evaluate our metrics on a challenging task and demonstrate the discriminative ability of those.

</details>


### [7] [Conditional Generative Models for High-Resolution Range Profiles: Capturing Geometry-Driven Trends in a Large-Scale Maritime Dataset](https://arxiv.org/abs/2602.13297)
*Edwyn Brient,Santiago Velasco-Forero,Rami Kassab*

Main category: cs.CV

TL;DR: 本研究通过在海洋数据库中利用几何变量（如船只尺寸和目标角度）进行HRRP合成，展示了生成的目标特征与真实数据中的几何趋势一致，从而突出了获取几何对HRRP生成鲁棒性的重要性。


<details>
  <summary>Details</summary>
Motivation: 为了克服HRRP在不同操作场景下的鲁棒性差的问题，本研究旨在通过大规模海洋数据库中的合成HRRP来增强其鲁棒性。

Method: 本研究通过分析数据库中的几何变量，并利用生成模型对HRRP进行条件生成，进而验证合成的HRRP签名是否符合真实数据中的几何趋势。

Result: 研究发现，通过条件生成模型合成的HRRP签名能够重现真实数据中的预期线视几何趋势。

Conclusion: 研究表明，获取几何是提升HRRP生成鲁棒性的关键因素。

Abstract: High-resolution range profiles (HRRPs) enable fast onboard processing for radar automatic target recognition, but their strong sensitivity to acquisition conditions limits robustness across operational scenarios. Conditional HRRP generation can mitigate this issue, yet prior studies are constrained by small, highly specific datasets. We study HRRP synthesis on a largescale maritime database representative of coastal surveillance variability. Our analysis indicates that the fundamental scenario drivers are geometric: ship dimensions and the desired aspect angle. Conditioning on these variables, we train generative models and show that the synthesized signatures reproduce the expected line-of-sight geometric trend observed in real data. These results highlight the central role of acquisition geometry for robust HRRP generation.

</details>


### [8] [DriveMamba: Task-Centric Scalable State Space Model for Efficient End-to-End Autonomous Driving](https://arxiv.org/abs/2602.13301)
*Haisheng Su,Wei Wu,Feixiang Song,Junjie Zhang,Zhenjie Yang,Junchi Yan*

Main category: cs.CV

TL;DR: DriveMamba通过引入动态任务关系建模、隐式视图对应学习和长时间序列融合，提出了一种任务导向的可扩展的端到端自动驾驶框架，实现了高效且强大的时空输入处理。


<details>
  <summary>Details</summary>
Motivation: 当前的端到端自动驾驶系统依赖于顺序设计和模块化框架，这种方式会损失信息并积累误差，不具备灵活且多样的模块间关系建模能力。同时，图像骨干训练不足和注意力机制的二次复杂度限制了系统的可扩展性和效率。

Method: DriveMamba采用了单一阶段的统一Mamba解码器，将提取的图像特征和预期任务输出转化为时空位置基础上的稀疏表示，并通过线性复杂度的操作有效建模长期序列上下文。此外，设计了一个双向轨迹引导的“局部到全局”扫描方法来保持由自身视角出发的空间局部性，以辅助自身规划。

Result: 在nuScenes和Bench2Drive数据集上的实验表明，DriveMamba在效率和性能上超越了现有方法，展示了其卓越的普遍适用性和强大功能。

Conclusion: DriveMamba为端到端自动驾驶系统的未来发展提供了有效的解决方案，改进了系统的灵活性、可扩展性以及效率。

Abstract: Recent advances towards End-to-End Autonomous Driving (E2E-AD) have been often devoted on integrating modular designs into a unified framework for joint optimization e.g. UniAD, which follow a sequential paradigm (i.e., perception-prediction-planning) based on separable Transformer decoders and rely on dense BEV features to encode scene representations. However, such manual ordering design can inevitably cause information loss and cumulative errors, lacking flexible and diverse relation modeling among different modules and sensors. Meanwhile, insufficient training of image backbone and quadratic-complexity of attention mechanism also hinder the scalability and efficiency of E2E-AD system to handle spatiotemporal input. To this end, we propose DriveMamba, a Task-Centric Scalable paradigm for efficient E2E-AD, which integrates dynamic task relation modeling, implicit view correspondence learning and long-term temporal fusion into a single-stage Unified Mamba decoder. Specifically, both extracted image features and expected task outputs are converted into token-level sparse representations in advance, which are then sorted by their instantiated positions in 3D space. The linear-complexity operator enables efficient long-context sequential token modeling to capture task-related inter-dependencies simultaneously. Additionally, a bidirectional trajectory-guided "local-to-global" scan method is designed to preserve spatial locality from ego-perspective, thus facilitating the ego-planning. Extensive experiments conducted on nuScenes and Bench2Drive datasets demonstrate the superiority, generalizability and great efficiency of DriveMamba.

</details>


### [9] [Spectral Collapse in Diffusion Inversion](https://arxiv.org/abs/2602.13303)
*Nicolas Bourriez,Alexandre Verine,Auguste Genovesio*

Main category: cs.CV

TL;DR: 文章提出了一种新的方法，即正交方差指导(Orthogonal Variance Guidance, OVG)，以解决标准反演方法在谱稀疏域（如超分辨率、草图转图像）中遇到的结构塌陷问题，从而实现更真实的纹理生成同时保持结构的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的标准反演方法（如DDIM）在处理源域比目标域谱稀疏的数据时容易产生失去特征结构和纹理细节的问题，这限制了这些方法在实际应用中的灵活性。

Method: 该方法设计了一种在推理阶段纠正ODE动力学的策略，以确保在结构梯度的零空间内，噪声的理论方差与期望的高斯噪声大小相匹配，即维持结构准确性的同时恢复真实的纹理细节。

Result: 实验结果表明，OVG方法能够有效恢复超现实的纹理质感，同时保持结构的准确性，该方法在显微超分辨（BBBC021）和草图转图像（Edges2Shoes）任务中的实验支持了这一观点。

Conclusion: OVG提供了一种有效的解决谱塌陷问题的方法，表明了在处理谱稀疏数据时，通过调整反演过程中的噪声方差可以更好地平衡结构和纹理的关系。

Abstract: Conditional diffusion inversion provides a powerful framework for unpaired image-to-image translation. However, we demonstrate through an extensive analysis that standard deterministic inversion (e.g. DDIM) fails when the source domain is spectrally sparse compared to the target domain (e.g., super-resolution, sketch-to-image). In these contexts, the recovered latent from the input does not follow the expected isotropic Gaussian distribution. Instead it exhibits a signal with lower frequencies, locking target sampling to oversmoothed and texture-poor generations. We term this phenomenon spectral collapse. We observe that stochastic alternatives attempting to restore the noise variance tend to break the semantic link to the input, leading to structural drift. To resolve this structure-texture trade-off, we propose Orthogonal Variance Guidance (OVG), an inference-time method that corrects the ODE dynamics to enforce the theoretical Gaussian noise magnitude within the null-space of the structural gradient. Extensive experiments on microscopy super-resolution (BBBC021) and sketch-to-image (Edges2Shoes) demonstrate that OVG effectively restores photorealistic textures while preserving structural fidelity.

</details>


### [10] [Progressive Contrast Registration for High-Fidelity Bidirectional Photoacoustic Microscopy Alignment](https://arxiv.org/abs/2602.13304)
*Jiahao Qin*

Main category: cs.CV

TL;DR: 提出了一种新型的双方向光学分辨率光声显微镜（OR-PAM）图像对齐方法PCReg-Net，该方法解决了双向扫描导致的图像偏移问题，并通过对比模块提高了对齐精度。


<details>
  <summary>Details</summary>
Motivation: 高分辨率光声显微镜在成像速度提升后遇到了图像对齐的挑战，传统方法的对齐效果有限，PCReg-Net旨在解决这一问题。

Method: PCReg-Net采用了一种分层对比引导的配准框架，包括粗配准模块、参考特征提取模块、对比模块和高级细化模块，实现了高效且高质量的图像对齐。

Result: PCReg-Net在OR-PAM-Reg-4K数据集上的对比度相似度（NCC）达到0.983，结构相似度（SSIM）达到0.982，峰值信噪比（PSNR）高达46.96 dB，显著优于现有的最先进的技术，在实时速度下提高了14 dB。

Conclusion: PCReg-Net通过提出新颖的方法解决了双向扫描光声显微镜图像对齐问题，展示出了高对齐精度和效率，为此类数据的处理提供了有力的工具。

Abstract: High-speed optical-resolution photoacoustic microscopy (OR-PAM) with bidirectional raster scanning doubles imaging speed but introduces coupled domain shift and geometric misalignment between forward and backward scan lines. Existing methods, constrained by brightness constancy assumptions, achieve limited alignment quality (NCC~$\leq 0.96$). We propose PCReg-Net, a progressive contrast-guided registration framework that performs coarse-to-fine alignment through four lightweight modules: (1)~a registration U-Net for coarse alignment, (2)~a reference feature extractor capturing multi-scale structural cues, (3)~a contrast module that identifies residual misalignment by comparing coarse-registered and reference features, and (4)~a refinement U-Net with feature injection for high-fidelity output. We further propose the Temporal NCC (TNCC) and Temporal NCC Gap (TNCG) for reference-free evaluation of inter-frame temporal consistency. On OR-PAM-Reg-4K (432 test samples), PCReg-Net achieves NCC of 0.983, SSIM of 0.982, and PSNR of 46.96 dB, surpassing the state-of-the-art by over 14 dB at real-time speed. Code is available at https://github.com/JiahaoQin/PCReg-Net

</details>


### [11] [WildfireVLM: AI-powered Analysis for Early Wildfire Detection and Risk Assessment Using Satellite Imagery](https://arxiv.org/abs/2602.13305)
*Aydin Ayanzadeh,Prakhar Dixit,Sadia Kamal,Milton Halem*

Main category: cs.CV

TL;DR: 本文提出了一种名为WildfireVLM的AI框架，结合卫星图像的野火检测和语言驱动的风险评估，使用多模态大语言模型将检测结果转化为上下文相关风险评估和优先响应建议。


<details>
  <summary>Details</summary>
Motivation: 由于气候变迁和人为因素导致的野火频率和强度增加，早期预警变得至关重要，然而卫星监测仍面临烟雾信号微弱、多变天气条件以及大规模实时分析的挑战。

Method: 作者构建了一个标注好的野火和烟雾数据集，使用Landsat-8/9、GOES-16等公开地球观测资源的卫星图像，并采用YOLOv12进行火区和烟雾云团检测。通过整合多模态大语言模型将检测输出转化为上下文风险评估和优先响应建议。

Result: 该系统通过LLM-as-judge评估验证了风险推理的质量，并展示了将计算机视觉与语言推理结合以实现可扩展野火监测的价值。

Conclusion: WildfireVLM为实时处理、可视化风险仪表板和长期野火跟踪提供了实现实时处理、可视化风险仪表板和长期野火跟踪的框架，展示了结合计算机视觉与语言推理的重要性。

Abstract: Wildfires are a growing threat to ecosystems, human lives, and infrastructure, with their frequency and intensity rising due to climate change and human activities. Early detection is critical, yet satellite-based monitoring remains challenging due to faint smoke signals, dynamic weather conditions, and the need for real-time analysis over large areas. We introduce WildfireVLM, an AI framework that combines satellite imagery wildfire detection with language-driven risk assessment. We construct a labeled wildfire and smoke dataset using imagery from Landsat-8/9, GOES-16, and other publicly available Earth observation sources, including harmonized products with aligned spectral bands. WildfireVLM employs YOLOv12 to detect fire zones and smoke plumes, leveraging its ability to detect small, complex patterns in satellite imagery. We integrate Multimodal Large Language Models (MLLMs) that convert detection outputs into contextualized risk assessments and prioritized response recommendations for disaster management. We validate the quality of risk reasoning using an LLM-as-judge evaluation with a shared rubric. The system is deployed using a service-oriented architecture that supports real-time processing, visual risk dashboards, and long-term wildfire tracking, demonstrating the value of combining computer vision with language-based reasoning for scalable wildfire monitoring.

</details>


### [12] [Fine-Tuning a Large Vision-Language Model for Artwork's Scoring and Critique](https://arxiv.org/abs/2602.13306)
*Zhehan Zhang,Meihua Qian,Li Luo,Siyu Huang,Chaoyi Zhou,Ripon Saha,Xinxin Song*

Main category: cs.CV

TL;DR: 本文提出了一种通过微调Qwen2-VL-7B视觉语言模型进行人类绘画创造力评估的框架，实现了高精度自动评分和反馈生成。


<details>
  <summary>Details</summary>
Motivation: 目前采用人工评分（如Torrance Tests of Creative Thinking）的评估艺术创造力的方式劳动密集且难以大规模进行，而基于图像特性的机器学习方法虽有潜力但缺乏解释性反馈。

Method: 该框架通过多任务学习微调Qwen2-VL-7B模型，并添加了一个轻量级回归头，使得模型能够在单次前向传播中预测分数并生成与评分标准相符的反馈。文章还整合了结构化的评分标准和艺术作品描述在系统提示中，以约束生成的文本与定量预测一致。

Result: 实验结果显示该方法在100点评分标准上的皮尔逊相关系数超过0.97，平均绝对误差约为3.95。定性评估表明生成的反馈在语义上接近专家评论(平均SBERT余弦相似度=0.798)。

Conclusion: 本文提出的框架体现了计算机视觉与艺术评估的结合，提供了一个可扩展的工具，用于创造力研究和课堂反馈。

Abstract: Assessing artistic creativity is foundational to creativity research and arts education, yet manual scoring (e.g., Torrance Tests of Creative Thinking) is labor-intensive at scale. Prior machine-learning approaches show promise for visual creativity scoring, but many rely mainly on image features and provide limited or no explanatory feedback. We propose a framework for automated creativity assessment of human paintings by fine-tuning the vision-language model Qwen2-VL-7B with multi-task learning. Our dataset contains 1000 human-created paintings scored on a 1-100 scale and paired with a short human-written description (content or artist explanation). Two expert raters evaluated each work using a five-dimension rubric (originality, color, texture, composition, content) and provided written critiques; we use an 80/20 train-test split. We add a lightweight regression head on the visual encoder output so the model can predict a numerical score and generate rubric-aligned feedback in a single forward pass. By embedding the structured rubric and the artwork description in the system prompt, we constrain the generated text to match the quantitative prediction. Experiments show strong accuracy, achieving Pearson r > 0.97 and MAE about 3.95 on the 100-point scale. Qualitative evaluation indicates the generated feedback is semantically close to expert critiques (average SBERT cosine similarity = 0.798). The proposed approach bridges computer vision and art assessment and offers a scalable tool for creativity research and classroom feedback.

</details>


### [13] [Visual Para-Thinker: Divide-and-Conquer Reasoning for Visual Comprehension](https://arxiv.org/abs/2602.13310)
*Haoran Xu,Hongyu Wang,Jiaze Li,Shunpeng Chen,Zizhao Tong,Jianzhong Ju,Zhenbo Luo,Jian Luan*

Main category: cs.CV

TL;DR: 该研究引入了视觉Para-思考者（Visual Para-Thinker），这是第一个为大规模语言模型应用并行推理的框架。该框架利用Pa-Attention和LPRoPE技术，在视觉领域中成功扩展了并行推理的优势。


<details>
  <summary>Details</summary>
Motivation: 现有的垂直扩展策略在探索中遇到瓶颈，尤其是在深度推理方面，这限制了模型的灵活性和多样性。通过转换到并行性，可以减轻这种探索范围的缩小，但如何将这一范式扩展到视觉领域仍然是一个研究挑战。

Method: 该研究采用了并行推理策略，结合Pa-Attention和LPRoPE技术，并利用vLLM框架进行高效率的并行处理。

Result: 在视觉基准数据集V*、CountBench、RefCOCO和HallusionBench上的实验证明，视觉Para-思考者成功地将并行推理的优势扩展到了视觉领域。

Conclusion: 研究证明了并行推理框架Visual Para-Thinker的有效性，为视觉领域中的大规模语言模型引入了一种新的处理方式。

Abstract: Existing LLM test-time scaling laws emphasize the emergence of self-reflective behaviors through extended reasoning length. Nevertheless, this vertical scaling strategy often encounters plateaus in exploration as the model becomes locked into specific thinking pattern. By shifting from depth to parallelism, parallel thinking mitigates the narrowing of exploration. However, the extension of this paradigm to visual domain remains an open research question. In this paper, we first examine the role of visual partitioning in parallelized reasoning and subsequently propose two distinct strategies. Based on the above, we introduce Visual Para-Thinker, representing the inaugural parallel reasoning framework for MLLMs. To maintain path independence and promote diversity in reasoning, our approach integrates Pa-Attention alongside LPRoPE. Leveraging the vLLM framework, we have developed a native multimodal implementation that facilitates high-efficiency parallel processing. Empirical results on benchmark datasets such as V*, CountBench, RefCOCO, and HallusionBench confirm that Visual Para-Thinker successfully extends the benefits of parallel reasoning to the visual domain.

</details>


### [14] [Sim2Radar: Toward Bridging the Radar Sim-to-Real Gap with VLM-Guided Scene Reconstruction](https://arxiv.org/abs/2602.13314)
*Emily Bejerano,Federico Tondolo,Aayan Qayyum,Xiaofan Yu,Xiaofan Jiang*

Main category: cs.CV

TL;DR: Sim2Radar 是一个端到端框架，能够直接从单一视角的 RGB 图像中合成训练雷达数据，无需手动场景建模。通过结合单目深度估计、语义分割和视觉-语言推理来重建带材料感知的3D场景，并使用一个可配置的物理基础射线追踪器来模拟毫米波传播。实验表明，该方法在真实室内场景中通过迁移学习改进了雷达的空间定位，从而提高了下游的3D雷达感知性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的雷达感知技术由于雷达数据的稀缺性和标注成本高，在视觉降级环境下表现受限。Sim2Radar 目的是通过从 RGB 图像直接合成训练雷达数据，提供一种可扩展的数据生成方法，以减少对真实数据的依赖，提高雷达感知能力。

Method: Sim2Radar 方法包括三个步骤。首先，利用单目深度估计和分割来重建一个材料感知的3D场景。其次，通过视觉-语言推理进一步细化场景模型。最后，使用一个基于可通过 ITU-R 电磁属性参数化的菲涅耳反射模型的可配置物理射线追踪器来模拟毫米波信号的传输。

Result: 在真实室内场景上的实验表明，通过迁移学习，使用 Sim2Radar 成合成的雷达数据预训练雷达点云目标检测模型并微调，可以提升3D物体检测的3.7个 AP （IoU 0.3），大大提高了雷达在空间定位上的性能。

Conclusion: Sim2Radar 通过结合物理建模和视觉推理，提供了一种有效的雷达模拟方法，能够为雷达检测提供几何先验，即使在有限的真实数据监督下也能明显提升性能。

Abstract: Millimeter-wave (mmWave) radar provides reliable perception in visually degraded indoor environments (e.g., smoke, dust, and low light), but learning-based radar perception is bottlenecked by the scarcity and cost of collecting and annotating large-scale radar datasets. We present Sim2Radar, an end-to-end framework that synthesizes training radar data directly from single-view RGB images, enabling scalable data generation without manual scene modeling. Sim2Radar reconstructs a material-aware 3D scene by combining monocular depth estimation, segmentation, and vision-language reasoning to infer object materials, then simulates mmWave propagation with a configurable physics-based ray tracer using Fresnel reflection models parameterized by ITU-R electromagnetic properties. Evaluated on real-world indoor scenes, Sim2Radar improves downstream 3D radar perception via transfer learning: pre-training a radar point-cloud object detection model on synthetic data and fine-tuning on real radar yields up to +3.7 3D AP (IoU 0.3), with gains driven primarily by improved spatial localization. These results suggest that physics-based, vision-driven radar simulation can provide effective geometric priors for radar learning and measurably improve performance under limited real-data supervision.

</details>


### [15] [IDPruner: Harmonizing Importance and Diversity in Visual Token Pruning for MLLMs](https://arxiv.org/abs/2602.13315)
*Yifan Tan,Yifu Sun,Shirui Huang,Hong Liu,Guanghua Yu,Jianchen Zhu,Yangdong Deng*

Main category: cs.CV

TL;DR: 本文提出了一种名为IDPruner的方法，该方法利用Maximal Marginal Relevance (MMR)算法，在保留模型性能的同时大幅度减少视觉令牌数量，适用于广泛架构和基准测试，实验结果显示IDPruner在Qwen2.5-VL-7B-Instruct上的性能优越。


<details>
  <summary>Details</summary>
Motivation: 针对现有的视觉令牌重要性和多样性评估方法缺乏系统性分析和优化框架的问题，本文提出了一种新的视觉令牌修剪方法IDPruner，以提高多模态大语言模型的推理速度。

Method: IDPruner方法利用Maximal Marginal Relevance (MMR)算法在视觉令牌重要性和多样性之间寻求最优平衡。

Result: 实验表明IDPruner在不同模型架构和多模态基准测试中具有优越的性能，并能有效保持模型在整个修剪过程中的性能。

Conclusion: IDPruner方法为视觉令牌修剪提供了一个有效的解决方案，极大地提高了多模态大语言模型的推理效率和泛化能力。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated impressive capabilities, yet they encounter significant computational bottlenecks due to the massive volume of visual tokens. Consequently, visual token pruning, which substantially reduces the token count, has emerged as a critical technique for accelerating MLLM inference. Existing approaches focus on token importance, diversity, or an intuitive combination of both, without a principled framework for their optimal integration. To address this issue, we first conduct a systematic analysis to characterize the trade-off between token importance and semantic diversity. Guided by this analysis, we propose the \textbf{I}mportance and \textbf{D}iversity Pruner (\textbf{IDPruner}), which leverages the Maximal Marginal Relevance (MMR) algorithm to achieve a Pareto-optimal balance between these two objectives. Crucially, our method operates without requiring attention maps, ensuring full compatibility with FlashAttention and efficient deployment via one-shot pruning. We conduct extensive experiments across various model architectures and multimodal benchmarks, demonstrating that IDPruner achieves state-of-the-art performance and superior generalization across diverse architectures and tasks. Notably, on Qwen2.5-VL-7B-Instruct, IDPruner retains 95.18\% of baseline performance when pruning 75\% of the tokens, and still maintains 86.40\% even under an extreme 90\% pruning ratio. Our code is available at https://github.com/Tencent/AngelSlim.

</details>


### [16] [Diagnostic Benchmarks for Invariant Learning Dynamics: Empirical Validation of the Eidos Architecture](https://arxiv.org/abs/2602.13322)
*Datorien L. Anderson*

Main category: cs.CV

TL;DR: 该研究提出了PolyShapes-Ideal (PSI)数据集，通过三个诊断性探针展示了Eidos架构在保持结构性不变性方面的能力，并验证了'Form-First'假设。


<details>
  <summary>Details</summary>
Motivation: 传统视觉基准主要依赖纹理相关性，而忽略了拓扑不变性的研究。研究旨在设计一套新的基准，旨在区分拓扑不变性和纹理相关性。

Method: 研究设计了三个诊断任务来评估模型在不同条件下的表现：噪声下的多边形分类、从MNIST到30个未知字体的零样本转移，以及在渐进变形下的几何坍缩映射。

Result: Eidos在所有三个诊断任务中的表现优异，特别是在完全无监督的情况下达到>99%的多边形分类准确率和81.67%的零样本跨字体迁移率。

Conclusion: 研究结果支持了'Form-First'假说，即结构受限架构中的泛化能力主要源自几何完整而非统计规模。

Abstract: We present the PolyShapes-Ideal (PSI) dataset, a suite of diagnostic benchmarks designed to isolate topological invariance -- the ability to maintain structural identity across affine transformations -- from the textural correlations that dominate standard vision benchmarks. Through three diagnostic probes (polygon classification under noise, zero-shot font transfer from MNIST, and geometric collapse mapping under progressive deformation), we demonstrate that the Eidos architecture achieves >99% accuracy on PSI and 81.67% zero-shot transfer across 30 unseen typefaces without pre-training. These results validate the "Form-First" hypothesis: generalization in structurally constrained architectures is a property of geometric integrity, not statistical scale.

</details>


### [17] [Synthesizing the Kill Chain: A Zero-Shot Framework for Target Verification and Tactical Reasoning on the Edge](https://arxiv.org/abs/2602.13324)
*Jesse Barkley,Abraham George,Amir Barati Farimani*

Main category: cs.CV

TL;DR: 该研究提出了一种分层的零样本框架，将轻量级物体检测与Qwen和Gemma系列的小型Vision-Language模型（VLM）结合，用于动态军事环境下的自主边缘机器人部署，成功地实现了在虚拟战场视频上的多种任务，并揭示了不同VLM在感知与推理上的独特故障模式。


<details>
  <summary>Details</summary>
Motivation: 在动态军事环境中部署自主边缘机器人时，受限于专门领域训练数据的稀缺性和边缘硬件的计算限制，因此需要一种能够高效处理和验证感知信息的框架。

Method: 该框架采用分层结构，首先使用高召回率的基于DINO的视觉语言模型进行文本提示下的区域提议，然后通过边缘类VLM进行语义验证。同时引入了“控制输入”方法来解耦感知与推理，进一步分析不同VLM在感知与推理上的表现。

Result: 该框架在包含55个高保真模拟视频的战场上成功执行了多个任务，如错误检测过滤、评估结构损伤、精细的车辆分类等，并保持了较低的延迟。通过“控制输入”方法还发现了不同VLM的不同失效模式，包括在战术逻辑上的优势和视觉感知上的不足，以及推理上的崩溃。

Conclusion: 研究验证了层次化的零样本架构的适用性，并提出了一种诊断框架来确定VLM在关键安全应用中的适用性。

Abstract: Deploying autonomous edge robotics in dynamic military environments is constrained by both scarce domain-specific training data and the computational limits of edge hardware. This paper introduces a hierarchical, zero-shot framework that cascades lightweight object detection with compact Vision-Language Models (VLMs) from the Qwen and Gemma families (4B-12B parameters). Grounding DINO serves as a high-recall, text-promptable region proposer, and frames with high detection confidence are passed to edge-class VLMs for semantic verification. We evaluate this pipeline on 55 high-fidelity synthetic videos from Battlefield 6 across three tasks: false-positive filtering (up to 100% accuracy), damage assessment (up to 97.5%), and fine-grained vehicle classification (55-90%). We further extend the pipeline into an agentic Scout-Commander workflow, achieving 100% correct asset deployment and a 9.8/10 reasoning score (graded by GPT-4o) with sub-75-second latency. A novel "Controlled Input" methodology decouples perception from reasoning, revealing distinct failure phenotypes: Gemma3-12B excels at tactical logic but fails in visual perception, while Gemma3-4B exhibits reasoning collapse even with accurate inputs. These findings validate hierarchical zero-shot architectures for edge autonomy and provide a diagnostic framework for certifying VLM suitability in safety-critical applications.

</details>


### [18] [MotionWeaver: Holistic 4D-Anchored Framework for Multi-Humanoid Image Animation](https://arxiv.org/abs/2602.13326)
*Xirui Hu,Yanbo Ding,Jiahao Wang,Tingting Shi,Yali Wang,Guo Zhi Zhi,Weizhan Zhang*

Main category: cs.CV

TL;DR: 该研究提出了MotionWeaver框架，用于多人物形象动画，通过统一的运动表示和整体的4D锚定范式解决了单一人类设置的局限性，适用于多种人类形态、复杂交互和遮挡场景。


<details>
  <summary>Details</summary>
Motivation: 目前的字符形象动画方法难以扩展到多人物场景，因为涉及多样的人类形态、复杂的交互和频繁的遮挡。

Method: 引入了统一的运动表示，提取身份无关的运动，并将其显式绑定到相应的人物上。此外，提出了一个整体的4D锚定框架，构建了共享的4D空间来融合运动表示与视频潜空间，通过分层的4D级别监督来更好地处理交互和遮挡。

Result: 实验表明，MotionWeaver在基准测试数据集上达到了最先进的结果，同时也展示了在多种人类形态、复杂交互和挑战性的人多场景中的有效泛化。

Conclusion: 该研究通过引入新的方法，成功扩展了字符形象动画的适用场景，使得动画合成更加多样和复杂。

Abstract: Character image animation, which synthesizes videos of reference characters driven by pose sequences, has advanced rapidly but remains largely limited to single-human settings. Existing methods struggle to generalize to multi-humanoid scenarios, which involve diverse humanoid forms, complex interactions, and frequent occlusions. We address this gap with two key innovations. First, we introduce unified motion representations that extract identity-agnostic motions and explicitly bind them to corresponding characters, enabling generalization across diverse humanoid forms and seamless extension to multi-humanoid scenarios. Second, we propose a holistic 4D-anchored paradigm that constructs a shared 4D space to fuse motion representations with video latents, and further reinforces this process with hierarchical 4D-level supervision to better handle interactions and occlusions. We instantiate these ideas in MotionWeaver, an end-to-end framework for multi-humanoid image animation. To support this setting, we curate a 46-hour dataset of multi-human videos with rich interactions, and construct a 300-video benchmark featuring paired humanoid characters. Quantitative and qualitative experiments demonstrate that MotionWeaver not only achieves state-of-the-art results on our benchmark but also generalizes effectively across diverse humanoid forms, complex interactions, and challenging multi-humanoid scenarios.

</details>


### [19] [HiST-VLA: A Hierarchical Spatio-Temporal Vision-Language-Action Model for End-to-End Autonomous Driving](https://arxiv.org/abs/2602.13329)
*Yiru Wang,Zichong Gu,Yu Gao,Anqing Jiang,Zhigang Sun,Shuo Wang,Yuwen Heng,Hao Sun*

Main category: cs.CV

TL;DR: HiST-VLA 提出了一种新的层次时空视图-语言-行动 (VLA) 模型，通过几何意识、动态稀疏化和层级变换器规划器有效解决了现有的 VLA 模型的缺陷，实现了高度鲁棒的轨迹生成，并在 NAVSIM v2 标准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的 VLA 模型在精确性、3D 空间感知和对上下文的高度敏感性方面存在局限性，为了解决这些问题，提出了一种新的 HiST-VLA 模型。

Method:  HiST-VLA 通过集成几何意识、细粒度驾驶指令和状态历史推断来增强3D空间和时间推理。同时，该模型使用动态令牌稀疏化来提高计算效率，并采用层级变换器规划器逐步细化粗略的VLA航点到精细轨迹。此外，引入了动态潜在正则化将语言命令融入规划过程，以确保严格的时空一致性。

Result: HiST-VLA 在 NAVSIM v2 标准测试中表现优异，达到了 88.6 的 EPDMS，在伪闭环 Navhard 基准测试中达到了 50.9 的 EPDMS。

Conclusion: HiST-VLA 为自动驾驶中的视觉-语言-行动理解提供了一种新的方法，通过提高时空推理、计算效率和规划准确性，展示了其在安全关键场景中的潜力。

Abstract: Vision-Language-Action (VLA) models offer promising capabilities for autonomous driving through multimodal understanding. However, their utilization in safety-critical scenarios is constrained by inherent limitations, including imprecise numerical reasoning, weak 3D spatial awareness, and high sensitivity to context. To address these challenges, we propose HiST-VLA, a novel Hierarchical Spatio-Temporal VLA model designed for reliable trajectory generation.
  Our framework enhances 3D spatial and temporal reasoning by integrating geometric awareness with fine-grained driving commands and state history prompting. To ensure computational efficiency, we integrate dynamic token sparsification into the VLA architecture. This approach fuses redundant tokens rather than filtering them, effectively reducing redundancy without sacrificing model performance. Furthermore, we employ a hierarchical transformer-based planner to progressively refine coarse VLA waypoints into fine-grained trajectories. Crucially, the planner utilizes dynamic latent regularization to incorporate language commands, ensuring strict spatial grounding and temporal coherence. Extensive evaluation on the NAVSIM v2 benchmark demonstrates state-of-the-art performance on Navtest, achieving an EPDMS of 88.6, and EPDMS of 50.9 on pseudo closed-loop Navhard benchmark.

</details>


### [20] [Zwitscherkasten -- DIY Audiovisual bird monitoring](https://arxiv.org/abs/2602.13330)
*Dominik Blum,Elias Häring,Fabian Jirges,Martin Schäffer,David Schick,Florian Schulenberg,Torsten Schön*

Main category: cs.CV

TL;DR: 本文介绍了一种名为Zwitscherkasten的多模态系统，可利用边设备上的音频和视觉数据进行鸟类种群监测，基于深度学习的生物声学和图像分类模型支持实时、非侵入式监测。


<details>
  <summary>Details</summary>
Motivation: 针对资源受限的硬件开发实时的鸟类种群监测系统，以支持非侵入性、大规模的生物多样性监测和公民科学应用。

Method: 使用深度学习模型进行生物声学和图像分类，部署于边缘设备上。采取声学活动检测减少能耗，采用细粒度检测和分类方法进行视觉识别。

Result: 该系统的生物声学和图像分类能够在嵌入式平台上实现精准的鸟类种属识别，支持可扩展的生物多样性监测。

Conclusion: Zwitscherkasten为鸟类监测提供了一种新的实时、多模态解决方案，具有较高的准确性和实用性。

Abstract: This paper presents Zwitscherkasten, a DiY, multimodal system for bird species monitoring using audio and visual data on edge devices. Deep learning models for bioacoustic and image-based classification are deployed on resource-constrained hardware, enabling real-time, non-invasive monitoring. An acoustic activity detector reduces energy consumption, while visual recognition is performed using fine-grained detection and classification pipelines. Results show that accurate bird species identification is feasible on embedded platforms, supporting scalable biodiversity monitoring and citizen science applications.

</details>


### [21] [Ask the Expert: Collaborative Inference for Vision Transformers with Near-Edge Accelerators](https://arxiv.org/abs/2602.13334)
*Hao Liu,Suhaib A. Fahmy*

Main category: cs.CV

TL;DR: 提出了一种协作推理框架，结合边缘设备的小型通用ViT和近边缘加速器上的多种中型专家ViT，通过动态路由机制提高准确性并减少延迟和能耗。


<details>
  <summary>Details</summary>
Motivation: 针对边缘设备上部署Vision Transformers的高计算复杂性和将全部推断卸载到云端的高延迟问题，提出了一种新的协作推理框架。

Method: 该框架包含一个轻量级的通用ViT部署在边缘设备上，以及多个中型专家ViT部署在近边缘加速器上。通过一种新颖的路由机制，根据边缘模型的Top-$k$预测动态选择最相关的专家处理低置信度样本。同时，设计了一种渐进式专家训练策略以提高特定数据集子集上的专家准确性。

Result: 实验结果显示该框架在CIFAR-100数据集上的优越性，渐进式专家训练策略在目标数据子集上的专家专业化准确性提高了4.12%，整体准确性提高了2.76%，与静态专家相比。同时，相比仅在边缘设备执行的推断降低了45%的延迟，降低了46%的能耗。

Conclusion: 该框架有效地优化了视觉Transformer在边缘和近边缘环境下的部署，提升了准确性并显著减少了延迟和能耗。

Abstract: Deploying Vision Transformers on edge devices is challenging due to their high computational complexity, while full offloading to cloud resources presents significant latency overheads. We propose a novel collaborative inference framework, which orchestrates a lightweight generalist ViT on an edge device and multiple medium-sized expert ViTs on a near-edge accelerator. A novel routing mechanism uses the edge model's Top-$\mathit{k}$ predictions to dynamically select the most relevant expert for samples with low confidence. We further design a progressive specialist training strategy to enhance expert accuracy on dataset subsets. Extensive experiments on the CIFAR-100 dataset using a real-world edge and near-edge testbed demonstrate the superiority of our framework. Specifically, the proposed training strategy improves expert specialization accuracy by 4.12% on target subsets and enhances overall accuracy by 2.76% over static experts. Moreover, our method reduces latency by up to 45% compared to edge execution, and energy consumption by up to 46% compared to just near-edge offload.

</details>


### [22] [Meningioma Analysis and Diagnosis using Limited Labeled Samples](https://arxiv.org/abs/2602.13335)
*Jiamiao Lu,Wei Wu,Ke Gao,Ping Mao,Weichuan Zhang,Tuo Wang,Lingkun Ma,Jiapan Guo,Zanyi Wu,Yuqing Hu,Changming Sun*

Main category: cs.CV

TL;DR: 该研究提出了一个基于自适应权重的特征融合架构，用于低样本量的听神经瘤识别，并通过一个新的MRI数据集验证了其效果。


<details>
  <summary>Details</summary>
Motivation: 基于准确诊断对听神经瘤的生物行为和治疗响应至关重要，本文旨在通过融合不同频率带信息和空间域信息来提高分类性能。

Method: 研究中采用了离散小波变换获取特定频率带，并提出了一个具有自适应权重的特征融合架构。

Result: 通过新引入的听神经瘤MRI数据集实验，该方法在三个数据集的性能优于现有先进方法。

Conclusion: 研究证实了所提出的方法在基于MRI的听神经瘤识别中的优越性，并将代码开源供进一步研究使用。

Abstract: The biological behavior and treatment response of meningiomas depend on their grade, making an accurate diagnosis essential for treatment planning and prognosis assessment. We observed that the weighted fusion of spatial-frequency domain features significantly influences meningioma classification performance. Notably, the contribution of specific frequency bands obtained by discrete wavelet transform varies considerably across different images. A feature fusion architecture with adaptive weights of different frequency band information and spatial domain information is proposed for few-shot meningioma learning. To verify the effectiveness of the proposed method, a new MRI dataset of meningiomas is introduced. The experimental results demonstrate the superiority of the proposed method compared with existing state-of-the-art methods in three datasets. The code will be available at: https://github.com/ICL-SUST/AMSF-Net

</details>


### [23] [An Integrated Causal Inference Framework for Traffic Safety Modeling with Semantic Street-View Visual Features](https://arxiv.org/abs/2602.13339)
*Lishan Sun,Yujia Cheng,Pengfei Cui,Lei Han,Mohamed Abdel-Aty,Yunhan Zheng,Xingchen Zhang*

Main category: cs.CV

TL;DR: 本研究利用Google街景图像进行语义分割以提取视觉环境特征，并采用双重机器学习框架量化其对区域交通安全事件的因果影响。结果发现，绿化比例对交通安全事件具有显著的负向因果影响，尤其是在人口密集和社会脆弱的城市核心区域效果最为明显。绿化显著减少了角度碰撞和追尾碰撞的发生，但对脆弱道路使用者的保护效果有限。


<details>
  <summary>Details</summary>
Motivation: 当前交通安全模型主要依赖静态社会人口学和基础设施指标，忽视了驾驶员对交通环境的视觉感知。研究旨在填补这一空白，通过提取视觉环境特征并建立因果模型，提供有关绿色植被对交通安全影响的实证证据，指导制定针对性的交通政策。

Method: 本研究采用了语义分割技术从Google街景图像中提取视觉环境特征，并利用双重机器学习框架及条件平均处理效应法进行因果推断，还使用SHAP值来描述混杂变量的非线性影响机制。

Result: 研究表明，绿化比例对交通安全事件产生显著的负向因果影响（平均处理效应=-6.38，p=0.005），该影响在人口密集和社经脆弱的城市中心区域最为显著。绿化能有效减缓角度碰撞和追尾碰撞事故，但在保护脆弱道路使用者方面效果有限。

Conclusion: 研究结果支持绿化作为潜在的安全干预措施，特别是在改善视觉环境方面，并强调为了保护脆弱道路使用者，需要独特的设计优化。

Abstract: Macroscopic traffic safety modeling aims to identify critical risk factors for regional crashes, thereby informing targeted policy interventions for safety improvement. However, current approaches rely heavily on static sociodemographic and infrastructure metrics, frequently overlooking the impacts from drivers' visual perception of driving environment. Although visual environment features have been found to impact driving and traffic crashes, existing evidence remains largely observational, failing to establish the robust causality for traffic policy evaluation under complex spatial environment. To fill these gaps, we applied semantic segmentation on Google Street View imageries to extract visual environmental features and proposed a Double Machine Learning framework to quantify their causal effects on regional crashes. Meanwhile, we utilized SHAP values to characterize the nonlinear influence mechanisms of confounding variables in the models and applied causal forests to estimate conditional average treatment effects. Leveraging crash records from the Miami metropolitan area, Florida, and 220,000 street view images, evidence shows that greenery proportion exerts a significant and robust negative causal effect on traffic crashes (Average Treatment Effect = -6.38, p = 0.005). This protective effect exhibits spatial heterogeneity, being most pronounced in densely populated and socially vulnerable urban cores. While greenery significantly mitigates angle and rear-end crashes, its protective benefit for vulnerable road users (VRUs) remains limited. Our findings provide causal evidence for greening as a potential safety intervention, prioritizing hazardous visual environments while highlighting the need for distinct design optimizations to protect VRUs.

</details>


### [24] [Visual Foresight for Robotic Stow: A Diffusion-Based World Model from Sparse Snapshots](https://arxiv.org/abs/2602.13347)
*Lijun Zhang,Nikhil Chacko,Petter Nilsson,Ruinian Xu,Shantanu Thakar,Bai Lou,Harpreet Sawhney,Zhebin Zhang,Mudit Agrawal,Bhavana Chandrashekhar,Aaron Parness*

Main category: cs.CV

TL;DR: FOREST是一个基于意图的仓库模型，能够准确预测储物笼的存储配置，优于启发式基准，并在下游任务中提供了有用的信息。


<details>
  <summary>Details</summary>
Motivation: 在自动化仓库中，准确预测储物笼的存储配置对于优化作业调度和提高仓库效率至关重要。FOREST能够根据当前观察和计划的操作，预测储物笼的最终状态，从而为仓库规划提供有价值的前瞻性信号。

Method: FOREST模型使用实例对齐的实例掩码表示储物笼状态，并通过潜化扩散转换器预测执行后的存储布局。

Result: FOREST在预测的存储布局方面显著优于启发式基准，并且在后续任务中表现良好，表明它为仓库规划提供了有用的信息。

Conclusion: FOREST模型通过提供准确的储物笼存储配置预测，提高了自动化仓库系统的规划和执行效率，具有广泛的应用前景。

Abstract: Automated warehouses execute millions of stow operations, where robots place objects into storage bins. For these systems it is valuable to anticipate how a bin will look from the current observations and the planned stow behavior before real execution. We propose FOREST, a stow-intent-conditioned world model that represents bin states as item-aligned instance masks and uses a latent diffusion transformer to predict the post-stow configuration from the observed context. Our evaluation shows that FOREST substantially improves the geometric agreement between predicted and true post-stow layouts compared with heuristic baselines. We further evaluate the predicted post-stow layouts in two downstream tasks, in which replacing the real post-stow masks with FOREST predictions causes only modest performance loss in load-quality assessment and multi-stow reasoning, indicating that our model can provide useful foresight signals for warehouse planning.

</details>


### [25] [From Prompt to Production:Automating Brand-Safe Marketing Imagery with Text-to-Image Models](https://arxiv.org/abs/2602.13349)
*Parmida Atighehchian,Henry Wang,Andrei Kapustin,Boris Lerner,Tiancheng Jiang,Taylor Jensen,Negin Sokhandan*

Main category: cs.CV

TL;DR: 本文介绍了一种新的基于文本到图像模型的生产级解决方案，能够在不牺牲图像质量和创意多样性的情况下大幅提高营销对象的真实度和人员认可度。


<details>
  <summary>Details</summary>
Motivation: 随着文本到图像模型在生成图像方面的进步，如何在生产环境中实现这些模型的大规模部署并保持质量是一个挑战。本文提出的方法旨在通过自动化和适当的人工监控来解决这个问题。

Method: 本文提出的新管道利用了现成的文本到图像模型（如DINOV2），并结合优化的人机协作系统，来生成符合营销标准的商业产品图像。

Result: 通过使用DINOV2，该系统实现了30.77%的营销对象真实度提升；同时，用户对生成结果的偏好提升了52.00%。

Conclusion: 本文提出的方法提供了一个全自动且可扩展的解决方案，旨在提高营销图像的生产效率和质量，同时确保符合营销策略和目标。

Abstract: Text-to-image models have made significant strides, producing impressive results in generating images from textual descriptions. However, creating a scalable pipeline for deploying these models in production remains a challenge. Achieving the right balance between automation and human feedback is critical to maintain both scale and quality. While automation can handle large volumes, human oversight is still an essential component to ensure that the generated images meet the desired standards and are aligned with the creative vision. This paper presents a new pipeline that offers a fully automated, scalable solution for generating marketing images of commercial products using text-to-image models. The proposed system maintains the quality and fidelity of images, while also introducing sufficient creative variation to adhere to marketing guidelines. By streamlining this process, we ensure a seamless blend of efficiency and human oversight, achieving a $30.77\%$ increase in marketing object fidelity using DINOV2 and a $52.00\%$ increase in human preference over the generated outcome.

</details>


### [26] [Detecting Brick Kiln Infrastructure at Scale: Graph, Foundation, and Remote Sensing Models for Satellite Imagery Data](https://arxiv.org/abs/2602.13350)
*Usman Nazir,Xidong Chen,Hafiz Muhammad Abubakar,Hadia Abu Bakar,Raahim Arbaz,Fezan Rasool,Bin Chen,Sara Khalid*

Main category: cs.CV

TL;DR: 该研究通过高分辨率卫星图像和气候图谱模型，检测南亚和中亚地区的砖窑，并将其与现有的图学习基准模型进行了比较，展示了图、基础模型和遥感方法的互补优势。


<details>
  <summary>Details</summary>
Motivation: 南亚和中亚地区砖窑是空气污染和强迫劳动的主要来源，但缺乏大规模监测所需的数据。因此，研究旨在利用高分辨率卫星图像和先进的图学习模型进行大规模砖窑检测。

Method: 研究使用高分辨率卫星图像构建了一个多城市20级的图集，涵盖五个地区，并提出了气候图谱模型来捕捉砖窑布局的空间和方向结构。同时，评估了遥感检测流水线，并与卫星图像的基础模型进行了对比。

Result: 研究结果表明了基于图、基础模型和遥感方法的互补优势，为利用卫星图像实现大规模砖窑监测提供了实用指导。

Conclusion: 研究展示了利用高分辨率卫星图像和先进的图学习模型进行大规模砖窑检测的有效性，并提供了未来研究和实践的建议。

Abstract: Brick kilns are a major source of air pollution and forced labor in South Asia, yet large-scale monitoring remains limited by sparse and outdated ground data. We study brick kiln detection at scale using high-resolution satellite imagery and curate a multi city zoom-20 (0.149 meters per pixel) resolution dataset comprising over 1.3 million image tiles across five regions in South and Central Asia. We propose ClimateGraph, a region-adaptive graph-based model that captures spatial and directional structure in kiln layouts, and evaluate it against established graph learning baselines. In parallel, we assess a remote sensing based detection pipeline and benchmark it against recent foundation models for satellite imagery. Our results highlight complementary strengths across graph, foundation, and remote sensing approaches, providing practical guidance for scalable brick kiln monitoring from satellite imagery.

</details>


### [27] [Using Deep Learning to Generate Semantically Correct Hindi Captions](https://arxiv.org/abs/2602.13352)
*Wasim Akram Khan,Anil Kumar Vuppala*

Main category: cs.CV

TL;DR: 该研究利用多模态架构生成了使用VGG16、ResNet50、Inception V3等预训练模型处理的Flickr8k图像数据集的 Hindi 图片描述，通过基于注意力的双向 LSTM 实现了语义准确的图片描述生成。


<details>
  <summary>Details</summary>
Motivation: 鉴于计算机视觉与自然语言处理技术的发展，为了进一步拓宽对非英语语言图像描述的生成，该研究特别关注印度第四大流行语言——印地语。

Method: 此研究采用多模态架构，结合局部与全局视觉特征、注意机制及预训练模型，利用uni-directional和bi-directional文本编码技术对图像特征进行编码，通过双向LSTM模型生成图像描述。

Result: 该研究使用谷歌云翻译工具处理Flickr8k图像数据集，生成了Hindi图像描述。实验结果显示，基于注意机制的双向LSTM模型与VGG16预训练模型相结合，在BLEU-1和BLEU-4评分中分别获得了0.59和0.19的最佳结果。

Conclusion: 该研究成功实现了对印度常用语言Hindi的图像描述生成，为同领域未来研究提供了方向和参考。

Abstract: Automated image captioning using the content from the image is very appealing when done by harnessing the capability of computer vision and natural language processing. Extensive research has been done in the field with a major focus on the English language which gives the scope for further developments in the same with consideration of popular foreign languages. This research utilizes distinct models for translating the image caption into Hindi, the fourth most popular language across the world. Exploring the multi-modal architectures this research comprises local visual features, global visual features, attention mechanisms, and pre-trained models. Using google cloud translator on the image dataset from Flickr8k, Hindi image descriptions have been generated. Pre-trained CNNs like VGG16, ResNet50, and Inception V3 helped in retrieving image characteristics, while the uni-directional and bi-directional techniques of text encoding are used for the text encoding process. An additional Attention layer helps to generate a weight vector and, by multiplying it, combine image characteristics from each time step into a sentence-level feature vector. Bilingual evaluation understudy scores are used to compare the research outcome. Many experiments that serve as a baseline are done for the comparative analysis of the research. An image with a score of BLEU-1 is considered sufficient, whereas one with a score of BLEU-4 is considered to have fluid image captioning. For both BLEU scores, the attention-based bidirectional LSTM with VGG16 produced the best results of 0.59 and 0.19 respectively. The experiments conclude that researchs ability to produce relevant, semantically accurate image captions in Hindi. The research accomplishes the goals and future research can be guided by this research model.

</details>


### [28] [AdaCorrection: Adaptive Offset Cache Correction for Accurate Diffusion Transformers](https://arxiv.org/abs/2602.13357)
*Dong Liu,Yanxuan Yu,Ben Lengerich,Ying Nian Wu*

Main category: cs.CV

TL;DR: AdaCorrection 提出了一种自适应偏移缓存校正框架，能够在不影响生成质量的情况下，提高扩散模型的推理效率，适用于图像和视频的高保真生成。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型虽然生成质量高，但在推断过程中由于迭代去噪结构导致推理成本高昂。为了解决这一问题，研究人员提出AdaCorrection框架，旨在通过自适应地融合缓存激活和新鲜激活，维持生成质量的同时提高推理效率。

Method: AdaCorrection 使用轻量级的空时信号估算缓存有效性，并在每一步自适应地融合缓存激活和新鲜激活。这种方法无需额外的监督或重新训练即可实时计算校正。

Result: 实验结果显示，AdaCorrection 在保持接近原始生成质量的同时，提供了适度的加速效果，在图像和视频扩散基准测试中表现出一致的性能改进。

Conclusion: AdaCorrection 是一种有效的缓存校正框架，能够在保证高生成质量的同时，提高扩散模型的推理效率。

Abstract: Diffusion Transformers (DiTs) achieve state-of-the-art performance in high-fidelity image and video generation but suffer from expensive inference due to their iterative denoising structure. While prior methods accelerate sampling by caching intermediate features, they rely on static reuse schedules or coarse-grained heuristics, which often lead to temporal drift and cache misalignment that significantly degrade generation quality. We introduce \textbf{AdaCorrection}, an adaptive offset cache correction framework that maintains high generation fidelity while enabling efficient cache reuse across Transformer layers during diffusion inference. At each timestep, AdaCorrection estimates cache validity with lightweight spatio-temporal signals and adaptively blends cached and fresh activations. This correction is computed on-the-fly without additional supervision or retraining. Our approach achieves strong generation quality with minimal computational overhead, maintaining near-original FID while providing moderate acceleration. Experiments on image and video diffusion benchmarks show that AdaCorrection consistently improves generation performance.

</details>


### [29] [An Online Reference-Free Evaluation Framework for Flowchart Image-to-Code Generation](https://arxiv.org/abs/2602.13376)
*Giang Son Nguyen,Zi Pong Lim,Sarthak Ketanbhai Modi,Yon Shin Teo,Wenya Wang*

Main category: cs.CV

TL;DR: 该研究提出了一种无需参考的评估框架，用于流chart图像到代码生成的质量监测，通过引入基于OCR的召回率（Recall{OCR}）和基于视觉演绎的精准率（Precision{VE}）来量化生成质量，证明该方法与真实评估高度一致。


<details>
  <summary>Details</summary>
Motivation: 现有的图到代码生成系统在面对未知输入时，难以通过已知的正确答案来评估产出质量。因此，研究者开发了一种无需参考的评估方法，旨在为这类系统的持续质量监控提供解决方案。

Method: 该研究方法基于两个自动化的评估指标：基于OCR的内容覆盖召回率（Recall{OCR}）和基于视觉演绎的幻觉检测精准率（Precision{VE}）。通过计算这两个指标的调和平均值即F1{OCR-VE}来评估生成代码的质量。

Result: 实验结果表明，该框架在FlowVQA数据集上的评估指标与真实评估高度一致，平均皮尔逊相关系数分别为0.97，0.91和0.94，分别对应Recall，Precision和F1。

Conclusion: 该研究提出的参考自由评估框架能够有效、可靠地用于连续监测生产环境下的流chart图像到代码生成的质量，是现有评估方法的有效补充。

Abstract: Vision-Language Models (VLMs) are increasingly used in document processing pipelines to convert flowchart images into structured code (e.g., Mermaid). In production, these systems process arbitrary inputs for which no ground-truth code exists, making output quality difficult to assess. We propose a reference-free evaluation framework that monitors flowchart image-to-code generation quality at inference time, using only the input image and the generated output. The framework introduces two automated metrics: $\text{Recall}{\text{OCR}}$, which estimates content coverage by extracting text from the input image via OCR as a proxy reference, and $\text{Precision}{\text{VE}}$, which detects hallucinated elements through Visual Entailment against the original image. Their harmonic mean, $\text{F1}{\text{OCR-VE}}$, provides a unified quality score. Validation on the FlowVQA dataset shows strong agreement with ground-truth metrics (average Pearson's $r = 0.97$, $0.91$, and $0.94$ for Recall, Precision, and F1, respectively), confirming the framework's reliability as a practical, reference-free alternative for continuous quality monitoring in production settings.

</details>


### [30] [LAF-YOLOv10 with Partial Convolution Backbone, Attention-Guided Feature Pyramid, Auxiliary P2 Head, and Wise-IoU Loss for Small Object Detection in Drone Aerial Imagery](https://arxiv.org/abs/2602.13378)
*Sohail Ali Farooqui,Zuhair Ahmed Khan Taha,Mohammed Mudassir Uddin,Shahnawaz Alam*

Main category: cs.CV

TL;DR: LAF-YOLOv10通过集成四个模块解决了无人机影像中微小目标检测的挑战，显著提高了检测性能。


<details>
  <summary>Details</summary>
Motivation: 当前的检测器在处理无人机特有的挑战（如小目标检测、复杂背景、遮挡等）时表现不佳，LAF-YOLOv10旨在打破这些局限。

Method: LAF-YOLOv10基于YOLOv10n框架，集成了四个互补模块：Partial Convolution C2f (PC-C2f)、Attention-Guided Feature Pyramid Network (AG-FPN)、辅助P2检测头和Wise-IoU v3，以优化模型的计算效率和检测精度。

Result: 在各种评估基准上，LAF-YOLOv10表现出优越的性能。在VisDrone-DET2019数据集上，LAF-YOLOv10达到了35.1%的mAP@0.5，超过YOLOv10n 3.3个百分点，且在NVIDIA Jetson Orin Nano上实现了24.3 FPS的高效运行。

Conclusion: 该研究证明了所提出的模块化方法在无人机场景下的小目标检测方面的有效性和实用性，提升了无人机在实时应用中的检测能力。

Abstract: Unmanned aerial vehicles serve as primary sensing platforms for surveillance, traffic monitoring, and disaster response, making aerial object detection a central problem in applied computer vision. Current detectors struggle with UAV-specific challenges: targets spanning only a few pixels, cluttered backgrounds, heavy occlusion, and strict onboard computational budgets. This study introduces LAF-YOLOv10, built on YOLOv10n, integrating four complementary techniques to improve small-object detection in drone imagery. A Partial Convolution C2f (PC-C2f) module restricts spatial convolution to one quarter of backbone channels, reducing redundant computation while preserving discriminative capacity. An Attention-Guided Feature Pyramid Network (AG-FPN) inserts Squeeze-and-Excitation channel gates before multi-scale fusion and replaces nearest-neighbor upsampling with DySample for content-aware interpolation. An auxiliary P2 detection head at 160$\times$160 resolution extends localization to objects below 8$\times$8 pixels, while the P5 head is removed to redistribute parameters. Wise-IoU v3 replaces CIoU for bounding box regression, attenuating gradients from noisy annotations in crowded aerial scenes. The four modules address non-overlapping bottlenecks: PC-C2f compresses backbone computation, AG-FPN refines cross-scale fusion, the P2 head recovers spatial resolution, and Wise-IoU stabilizes regression under label noise. No individual component is novel; the contribution is the joint integration within a single YOLOv10 framework. Across three training runs (seeds 42, 123, 256), LAF-YOLOv10 achieves 35.1$\pm$0.3\% mAP@0.5 on VisDrone-DET2019 with 2.3\,M parameters, exceeding YOLOv10n by 3.3 points. Cross-dataset evaluation on UAVDT yields 35.8$\pm$0.4\% mAP@0.5. Benchmarks on NVIDIA Jetson Orin Nano confirm 24.3 FPS at FP16, demonstrating viability for embedded UAV deployment.

</details>


### [31] [Handling Supervision Scarcity in Chest X-ray Classification: Long-Tailed and Zero-Shot Learning](https://arxiv.org/abs/2602.13430)
*Ha-Hieu Pham,Hai-Dang Nguyen,Thanh-Huy Nguyen,Min Xu,Ulas Bagci,Trung-Nghia Le,Huy-Hieu Pham*

Main category: cs.CV

TL;DR: 该研究在PadChest基准上解决了胸部X光片分类中的监督不足问题，通过不平衡感知的多标签学习策略和新颖的零样本OOD识别方法，分别在长尾多标签分类和零样本OOD识别任务中取得了良好的性能。


<details>
  <summary>Details</summary>
Motivation: 针对临床实践中由于病种分布不均衡和罕见疾病标注不足导致的监督缺陷，研究提出了特定任务的应用方案以改善罕见疾病和常见疾病的分类效率。

Method: 采用了不平衡感知的多标签学习策略针对长尾多标签分类任务，而对零样本OOD识别任务，则提出了一种无需使用OOD类监督标签或示例的新颖预测方法。

Result: 在宏观平均平均精度（mAP）的评估中，该方法在长尾多标签分类和零样本OOD识别任务中表现出强大性能，成为开发阶段公共领示板上的第一名。

Conclusion: 该工作不仅提供了解决不均衡监督问题的方法，还展示了在特定胸部X光片分类任务中的有效性。

Abstract: Chest X-ray (CXR) classification in clinical practice is often limited by imperfect supervision, arising from (i) extreme long-tailed multi-label disease distributions and (ii) missing annotations for rare or previously unseen findings. The CXR-LT 2026 challenge addresses these issues on a PadChest-based benchmark with a 36-class label space split into 30 in-distribution classes for training and 6 out-of-distribution (OOD) classes for zero-shot evaluation. We present task-specific solutions tailored to the distinct supervision regimes. For Task 1 (long-tailed multi-label classification), we adopt an imbalance-aware multi-label learning strategy to improve recognition of tail classes while maintaining stable performance on frequent findings. For Task 2 (zero-shot OOD recognition), we propose a prediction approach that produces scores for unseen disease categories without using any supervised labels or examples from the OOD classes during training. Evaluated with macro-averaged mean Average Precision (mAP), our method achieves strong performance on both tasks, ranking first on the public leaderboard of the development phase. Code and pre-trained models are available at https://github.com/hieuphamha19/CXR_LT.

</details>


### [32] [GLIMPSE : Real-Time Text Recognition and Contextual Understanding for VQA in Wearables](https://arxiv.org/abs/2602.13479)
*Akhil Ramachandran,Ankit Arun,Ashish Shenoy,Abhay Harpale,Srihari Jayakumar,Debojeet Chatterjee,Mohsen Moslehpour,Pierce Chuang,Yichao Lu,Vikas Bhardwaj,Peyman Heidari*

Main category: cs.CV

TL;DR: 该研究提出了一种混合架构，通过在设备上进行选择性的高分辨率OCR处理和流式传输低分辨率视频来减少电池消耗，从而增强了在资源受限的可穿戴设备上进行基于文本的视频问答任务的能力。


<details>
  <summary>Details</summary>
Motivation: 在可穿戴设备上运行基于文本的视觉问答（Text VQA）任务面临着高分辨率视频抓取所需电池消耗大的问题，这导致了热限制，而现有模型在处理实时视频流中的文本跨帧时很难保持连贯的时间上下文。

Method: 提出了一种混合架构，通过在设备上实施选择性高分辨率光学字符识别（OCR）处理和流式传输低分辨率视频来提供视觉背景。该模型探索了文本识别和视觉推理之间的不规则分辨率需求。

Result: 实验结果表明，该系统在五类文本基于VQA样本基准测试中达到72％的准确率，相较于全分辨率流媒体，其功率消耗仅为0.49倍，从而实现了在资源受限的可穿戴设备上持续进行VQA会话而不牺牲文本理解质量。

Conclusion: 该研究不仅解决了传统Text VQA在可穿戴设备上的部署难题，还为资源受限环境下的其他视觉任务提出了优化方案。

Abstract: Video Large Language Models (Video LLMs) have shown remarkable progress in understanding and reasoning about visual content, particularly in tasks involving text recognition and text-based visual question answering (Text VQA). However, deploying Text VQA on wearable devices faces a fundamental tension: text recognition requires high-resolution video, but streaming high-quality video drains battery and causes thermal throttling. Moreover, existing models struggle to maintain coherent temporal context when processing text across multiple frames in real-time streams. We observe that text recognition and visual reasoning have asymmetric resolution requirements - OCR needs fine detail while scene understanding tolerates coarse features. We exploit this asymmetry with a hybrid architecture that performs selective high-resolution OCR on-device while streaming low-resolution video for visual context. On a benchmark of text-based VQA samples across five task categories, our system achieves 72% accuracy at 0.49x the power consumption of full-resolution streaming, enabling sustained VQA sessions on resource-constrained wearables without sacrificing text understanding quality.

</details>


### [33] [Benchmarking Video Foundation Models for Remote Parkinson's Disease Screening](https://arxiv.org/abs/2602.13507)
*Md Saiful Islam,Ekram Hossain,Abdelrahman Abdelkader,Tariq Adnan,Fazla Rabbi Mashrur,Sooyong Park,Praveen Kumar,Qasim Sudais,Natalia Chunga,Nami Shah,Jan Freyberg,Christopher Kanan,Ruth Schneider,Ehsan Hoque*

Main category: cs.CV

TL;DR: 该研究使用大规模数据集评估了多种视频基础模型在帕金森病筛查中的表现，发现不同模型在不同临床任务上表现有显著差异，部分模型在特定任务上表现出色。


<details>
  <summary>Details</summary>
Motivation: 由于视频基础模型不需要针对任务进行定制，因此研究者希望通过大规模实验证明这些模型在帕金森病筛查中的潜力。

Method: 研究者构建了一个大型视频数据集，包括1888名参与者（727名帕金森病患者）的32,847个视频，涵盖了16项标准化的临床任务。使用七种最新的视频基础模型进行评估。

Result: 实验结果显示了76.4%-85.3%的AUC值和71.5%-80.6%的准确率，TimeSformer在节奏任务如手指敲击中表现出色，但敏感性较低，为43.2%-57.3%，这表明需要任务感知的校准以提高检测帕金森病的能力。

Conclusion: 研究为基于视频基础模型的帕金森病筛查建立了严格的基准，并提供了选择合适任务和架构在远程神经监测中的路线图，未来还需要提高敏感性和整合多任务和模态信息。

Abstract: Remote, video-based assessments offer a scalable pathway for Parkinson's disease (PD) screening. While traditional approaches rely on handcrafted features mimicking clinical scales, recent advances in video foundation models (VFMs) enable representation learning without task-specific customization. However, the comparative effectiveness of different VFM architectures across diverse clinical tasks remains poorly understood. We present a large-scale systematic study using a novel video dataset from 1,888 participants (727 with PD), comprising 32,847 videos across 16 standardized clinical tasks. We evaluate seven state-of-the-art VFMs -- including VideoPrism, V-JEPA, ViViT, and VideoMAE -- to determine their robustness in clinical screening. By evaluating frozen embeddings with a linear classification head, we demonstrate that task saliency is highly model-dependent: VideoPrism excels in capturing visual speech kinematics (no audio) and facial expressivity, while V-JEPA proves superior for upper-limb motor tasks. Notably, TimeSformer remains highly competitive for rhythmic tasks like finger tapping. Our experiments yield AUCs of 76.4-85.3% and accuracies of 71.5-80.6%. While high specificity (up to 90.3%) suggests strong potential for ruling out healthy individuals, the lower sensitivity (43.2-57.3%) highlights the need for task-aware calibration and integration of multiple tasks and modalities. Overall, this work establishes a rigorous baseline for VFM-based PD screening and provides a roadmap for selecting suitable tasks and architectures in remote neurological monitoring. Code and anonymized structured data are publicly available: https://anonymous.4open.science/r/parkinson\_video\_benchmarking-A2C5

</details>


### [34] [SpargeAttention2: Trainable Sparse Attention via Hybrid Top-k+Top-p Masking and Distillation Fine-Tuning](https://arxiv.org/abs/2602.13515)
*Jintao Zhang,Kai Jiang,Chendong Xiang,Weiqi Feng,Yuezhou Hu,Haocheng Xi,Jianfei Chen,Jun Zhu*

Main category: cs.CV

TL;DR: 提出了SpargeAttention2，这是一种可训练的稀疏注意力方法，通过引入混合掩码规则、高效的可训练稀疏注意力实现和蒸馏启发式微调目标，在保持生成质量的情况下达到了95%的注意力稀疏性和16.2倍的加速。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有的无训练稀疏注意力方法存在的问题，提出了SpargeAttention2，该方法包括混合掩码规则、高效的可训练稀疏注意力实现和微调目标，以进一步提高稀疏性并保持生成质量。

Method: SpargeAttention2结合了Top-k和Top-p掩码规则，设计了一个高效的可训练稀疏注意力实现，并引入了蒸馏启发式微调目标，用于细调稀疏注意力。

Result: 在视频扩散模型实验中，SpargeAttention2实现了95%的注意力稀疏性和16.2倍的加速，同时保持了生成质量，优于先前的稀疏注意力方法。

Conclusion: SpargeAttention2通过上述改进，在保持甚至提高生成质量的同时，显著提高了扩散模型中注意力机制的稀疏性，展示了该方法的有效性。

Abstract: Many training-free sparse attention methods are effective for accelerating diffusion models. Recently, several works suggest that making sparse attention trainable can further increase sparsity while preserving generation quality. We study three key questions: (1) when do the two common masking rules, i.e., Top-k and Top-p, fail, and how can we avoid these failures? (2) why can trainable sparse attention reach higher sparsity than training-free methods? (3) what are the limitations of fine-tuning sparse attention using the diffusion loss, and how can we address them? Based on this analysis, we propose SpargeAttention2, a trainable sparse attention method that achieves high sparsity without degrading generation quality. SpargeAttention2 includes (i) a hybrid masking rule that combines Top-k and Top-p for more robust masking at high sparsity, (ii) an efficient trainable sparse attention implementation, and (iii) a distillation-inspired fine-tuning objective to better preserve generation quality during fine-tuning using sparse attention. Experiments on video diffusion models show that SpargeAttention2 reaches 95% attention sparsity and a 16.2x attention speedup while maintaining generation quality, consistently outperforming prior sparse attention methods.

</details>


### [35] [Nighttime Autonomous Driving Scene Reconstruction with Physically-Based Gaussian Splatting](https://arxiv.org/abs/2602.13549)
*Tae-Kyeong Kim,Xingxin Chen,Guile Wu,Chengjie Huang,Dongfeng Bai,Bingbing Liu*

Main category: cs.CV

TL;DR: 本文提出了一种将基于物理的渲染技术融入3DGS的方法，以改善自动驾驶场景下的夜间场景重建。


<details>
  <summary>Details</summary>
Motivation: 现有基于神经辐射场（NeRFs）和3D高斯散点图（3DGS）的方法在低光照条件下的自动驾驶场景重建中表现不佳。

Method: 本文方法将基于物理的渲染技术集成到3DGS中，以增强夜间场景重建，并通过全局光照模块和各向异性球形高斯处理漫反射和镜面反射组件。

Result: 该方法在两个现实世界的数据集nuScenes和Waymo的多种夜间场景中均优于现有的最先进的方法。

Conclusion: 该方法提高了夜间驾驶场景的重建质量，同时保持了实时渲染。

Abstract: This paper focuses on scene reconstruction under nighttime conditions in autonomous driving simulation. Recent methods based on Neural Radiance Fields (NeRFs) and 3D Gaussian Splatting (3DGS) have achieved photorealistic modeling in autonomous driving scene reconstruction, but they primarily focus on normal-light conditions. Low-light driving scenes are more challenging to model due to their complex lighting and appearance conditions, which often causes performance degradation of existing methods. To address this problem, this work presents a novel approach that integrates physically based rendering into 3DGS to enhance nighttime scene reconstruction for autonomous driving. Specifically, our approach integrates physically based rendering into composite scene Gaussian representations and jointly optimizes Bidirectional Reflectance Distribution Function (BRDF) based material properties. We explicitly model diffuse components through a global illumination module and specular components by anisotropic spherical Gaussians. As a result, our approach improves reconstruction quality for outdoor nighttime driving scenes, while maintaining real-time rendering. Extensive experiments across diverse nighttime scenarios on two real-world autonomous driving datasets, including nuScenes and Waymo, demonstrate that our approach outperforms the state-of-the-art methods both quantitatively and qualitatively.

</details>


### [36] [Diff-Aid: Inference-time Adaptive Interaction Denoising for Rectified Text-to-Image Generation](https://arxiv.org/abs/2602.13585)
*Binglei Li,Mengping Yang,Zhiyu Tan,Junping Zhang,Hao Li*

Main category: cs.CV

TL;DR: 提出了一种称为Diff-Aid的轻量化方法，在推理时动态调整文本和图像在不同Transformer块和去噪步骤中的交互，提高了生成质量并提供了可解释的调制模式。


<details>
  <summary>Details</summary>
Motivation: 现有的T2I扩散模型虽取得了显著进步，但在忠实执行复杂的文字描述方面仍面临挑战，原因是文本和视觉特征之间的交互不足。大多数先前提案通过架构设计或手工设计的文本条件加权来增强这些交互，缺乏灵活性并忽视了不同块和去噪阶段之间的动态交互。

Method: Diff-Aid提出了一种轻量级的方法，能够在推理阶段动态调整文本和图像的交互，适用于不同Transformer块和去噪步骤。此方法可作为一种即插即用模块轻松集成到下游应用中，如风格LoRAs、可控生成和零样本编辑。

Result: 在强大的基线模型（SD 3.5和FLUX）上的实验显示，与现有方法相比，Diff-Aid能够一致地提高对指令的遵守程度、视觉质量和人类的偏好，涵盖了各种度量指标。

Conclusion: Diff-Aid提供了一种更为灵活且高效的解决方案，通过动态调整文本和图像在Transformer块和去噪步骤中的交互来改进生成质量，同时为不同块、时间步长和文本令牌如何在去噪过程中贡献于语义对齐提供了可解释的模式。

Abstract: Recent text-to-image (T2I) diffusion models have achieved remarkable advancement, yet faithfully following complex textual descriptions remains challenging due to insufficient interactions between textual and visual features. Prior approaches enhance such interactions via architectural design or handcrafted textual condition weighting, but lack flexibility and overlook the dynamic interactions across different blocks and denoising stages. To provide a more flexible and efficient solution to this problem, we propose Diff-Aid, a lightweight inference-time method that adaptively adjusts per-token text and image interactions across transformer blocks and denoising timesteps. Beyond improving generation quality, Diff-Aid yields interpretable modulation patterns that reveal how different blocks, timesteps, and textual tokens contribute to semantic alignment during denoising. As a plug-and-play module, Diff-Aid can be seamlessly integrated into downstream applications for further improvement, including style LoRAs, controllable generation, and zero-shot editing. Experiments on strong baselines (SD 3.5 and FLUX) demonstrate consistent improvements in prompt adherence, visual quality, and human preference across various metrics. Our code and models will be released.

</details>


### [37] [Two-Stream Interactive Joint Learning of Scene Parsing and Geometric Vision Tasks](https://arxiv.org/abs/2602.13588)
*Guanfeng Tang,Hongbo Zhao,Ziwei Long,Jiayao Li,Bohong Xiao,Wei Ye,Hanli Wang,Rui Fan*

Main category: cs.CV

TL;DR: 本文提出了一种名为TwInS的新型生物启发联合学习框架，旨在同时进行场景解析和几何视觉任务。它通过共享特征空间促进两个流之间的交互学习，并采用半监督学习策略来提高性能。


<details>
  <summary>Details</summary>
Motivation: 受人类视觉系统的启发，该研究旨在开发一种能够同时处理场景解析和几何视觉任务的新型联合学习框架。

Method: 研究采用了统一的通用架构，其中场景解析流的多级上下文特征被注入几何视觉流以指导其迭代优化。同时，从几何流解码的特征投影回上下文特征空间，通过一个新颖的跨任务适配器进行选择性的异构特征融合，利用丰富的跨视角几何线索来增强场景解析。

Result: 实验结果表明，该方法的有效性并展示了其在公开数据集上的优越性能，优于现有的先进技术。

Conclusion: 研究提出的TwInS框架展示了在场景解析和几何视觉任务中实现高性能联合学习的潜力，并通过半监督学习策略减少了对昂贵的人标注对应关系的依赖。

Abstract: Inspired by the human visual system, which operates on two parallel yet interactive streams for contextual and spatial understanding, this article presents Two Interactive Streams (TwInS), a novel bio-inspired joint learning framework capable of simultaneously performing scene parsing and geometric vision tasks. TwInS adopts a unified, general-purpose architecture in which multi-level contextual features from the scene parsing stream are infused into the geometric vision stream to guide its iterative refinement. In the reverse direction, decoded geometric features are projected into the contextual feature space for selective heterogeneous feature fusion via a novel cross-task adapter, which leverages rich cross-view geometric cues to enhance scene parsing. To eliminate the dependence on costly human-annotated correspondence ground truth, TwInS is further equipped with a tailored semi-supervised training strategy, which unleashes the potential of large-scale multi-view data and enables continuous self-evolution without requiring ground-truth correspondences. Extensive experiments conducted on three public datasets validate the effectiveness of TwInS's core components and demonstrate its superior performance over existing state-of-the-art approaches. The source code will be made publicly available upon publication.

</details>


### [38] [AdaVBoost: Mitigating Hallucinations in LVLMs via Token-Level Adaptive Visual Attention Boosting](https://arxiv.org/abs/2602.13600)
*Jiacheng Zhang,Feng Liu,Chao Du,Tianyu Pang*

Main category: cs.CV

TL;DR: AdaVBoost 提出了一种自适应视觉注意增强框架，通过引入视觉关联熵（VGE）评估幻觉风险，并针对高风险标识符应用更强的视觉注意增强，而对低风险标识符则使用较弱增强，从而在多个大型视觉-语言模型和幻觉基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有方法在视觉注意增强时面临预设强化系数过强或过弱的问题，AdaVBoost 通过引入视觉同步熵（VGE）来动态确定每一步生成中应增强视觉注意的程度，以解决幻视问题。

Method: AdaVBoost 引入了视觉同步熵（VGE）来估计幻视风险，该方法指导下的自适应视觉注意增强框架根据风险高低调整增强强度，实现逐标识符级别的自适应干预。

Result: 在广泛实验中，AdaVBoost 明显优于基线方法，在多个大型视觉-语言模型和幻觉基准测试中表现出色。

Conclusion: AdaVBoost 通过对视觉注意进行自适应调整，提高模型生成质量，减少幻视现象，展示了其实现视觉注意增强的潜力。

Abstract: Visual attention boosting has emerged as a promising direction for mitigating hallucinations in Large Vision-Language Models (LVLMs), where existing methods primarily focus on where to boost by applying a predefined scaling to the attention of method-specific visual tokens during autoregressive generation. In this paper, we identify a fundamental trade-off in these methods: a predefined scaling factor can be too weak at some generation steps, leaving hallucinations unresolved, yet too strong at others, leading to new hallucinations. Motivated by this finding, we propose AdaVBoost, a token-level visual attention boosting framework that adaptively determines how much attention to boost at each generation step. Specifically, we introduce Visual Grounding Entropy (VGE) to estimate hallucination risk, which leverages visual grounding as a complementary signal to capture evidence mismatches beyond entropy. Guided by VGE, AdaVBoost applies stronger visual attention boosting to high-risk tokens and weaker boosting to low-risk tokens, enabling token-level adaptive intervention at each generation step. Extensive experiments show that AdaVBoost significantly outperforms baseline methods across multiple LVLMs and hallucination benchmarks.

</details>


### [39] [Towards Sparse Video Understanding and Reasoning](https://arxiv.org/abs/2602.13602)
*Chenwei Xu,Zhen Ye,Shang Wu,Weijian Li,Zihan Wang,Zhuofan Xia,Lie Lu,Pranav Maneriker,Fan Du,Manling Li,Han Liu*

Main category: cs.CV

TL;DR: 该研究表明，通过选择少量关键帧并保持摘要状态，evise比均匀采样帧的方法提高了问答准确性，同时减少了所需的帧数、轮数和提示令牌。


<details>
  <summary>Details</summary>
Motivation: 当前VQA方法依赖于均匀采样帧，这增加了数据处理负担并可能包含冗余信息。本研究旨在提出一种更高效的方法，通过选择关键帧和维护摘要状态来减少框架数量、轮数和提示令牌，以提高问答的准确性。

Method: 研究提出了一种名为evise的方法，它可以在多轮中选择少量关键帧，并维护一个摘要状态。此外，还引入了一种名为EAGER的奖励系统，该系统包括信心增益、摘要必要性和正确且尽快停止三个部分，以增强模型的表现。

Result: 在多个VQA基准测试中，evise方法在保持较高准确性的前提下，减少了所需的帧数、轮数和提示令牌，展示了在稀疏视频推理方面的实践价值。

Conclusion: 研究表明，通过有效利用少量关键帧，evise比传统的均匀帧采样方法更高效，并且在多个VQA基准测试中表现出了显著的改进。

Abstract: We present \revise (\underline{Re}asoning with \underline{Vi}deo \underline{S}parsity), a multi-round agent for video question answering (VQA). Instead of uniformly sampling frames, \revise selects a small set of informative frames, maintains a summary-as-state across rounds, and stops early when confident. It supports proprietary vision-language models (VLMs) in a ``plug-and-play'' setting and enables reinforcement fine-tuning for open-source models. For fine-tuning, we introduce EAGER (Evidence-Adjusted Gain for Efficient Reasoning), an annotation-free reward with three terms: (1) Confidence gain: after new frames are added, we reward the increase in the log-odds gap between the correct option and the strongest alternative; (2) Summary sufficiency: at answer time we re-ask using only the last committed summary and reward success; (3) Correct-and-early stop: answering correctly within a small turn budget is rewarded. Across multiple VQA benchmarks, \revise improves accuracy while reducing frames, rounds, and prompt tokens, demonstrating practical sparse video reasoning.

</details>


### [40] [Layer-Guided UAV Tracking: Enhancing Efficiency and Occlusion Robustness](https://arxiv.org/abs/2602.13636)
*Yang Zhou,Derui Ding,Ran Sun,Ying Sun,Haohua Zhang*

Main category: cs.CV

TL;DR: LGTrack 提出了一种针对无人机应用的视觉对象跟踪框架，通过动态层选择、高效特征增强和鲁棒表示学习来处理遮挡问题。该框架的 GGCA 模块有效提升了特征的区分度，同时 SGLA 模块避免了知识蒸馏，实现了高效能的跟踪。实验结果表明 LGTrack 在实时速度和精度上均表现出色。


<details>
  <summary>Details</summary>
Motivation: 面对视觉对象跟踪中精度和效率的权衡难题，尤其是在多变遮挡条件下，作者提出 LGTrack 以提高无人机应用中的跟踪性能。

Method: LGTrack 基于动态层选择、高效特征增强和鲁棒表示学习来处理遮挡。它引入了 GGCA 模块以捕捉长距离依赖性和全局上下文，以及 SGLA 模块以替代知识蒸馏，从而实现高效能跟踪。

Result: LGTrack 经验上在三个数据集上展现出最先进的实时速度（UAVDT 上 258.7 FPS），同时保持竞争力的跟踪准确性（82.8% 精度）。

Conclusion: 本文提出的 LGTrack 能够在保持高精度的同时，提供高效的实时跟踪性能，适用于无人机等应用。

Abstract: Visual object tracking (VOT) plays a pivotal role in unmanned aerial vehicle (UAV) applications. Addressing the trade-off between accuracy and efficiency, especially under challenging conditions like unpredictable occlusion, remains a significant challenge. This paper introduces LGTrack, a unified UAV tracking framework that integrates dynamic layer selection, efficient feature enhancement, and robust representation learning for occlusions. By employing a novel lightweight Global-Grouped Coordinate Attention (GGCA) module, LGTrack captures long-range dependencies and global contexts, enhancing feature discriminability with minimal computational overhead. Additionally, a lightweight Similarity-Guided Layer Adaptation (SGLA) module replaces knowledge distillation, achieving an optimal balance between tracking precision and inference efficiency. Experiments on three datasets demonstrate LGTrack's state-of-the-art real-time speed (258.7 FPS on UAVDT) while maintaining competitive tracking accuracy (82.8\% precision). Code is available at https://github.com/XiaoMoc/LGTrack

</details>


### [41] [DCDM: Divide-and-Conquer Diffusion Models for Consistency-Preserving Video Generation](https://arxiv.org/abs/2602.13637)
*Haoyu Zhao,Yuang Zhang,Junqi Cheng,Jiaxi Gu,Zenghui Lu,Peng Shu,Zuxuan Wu,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: 提出了一种名为DCDM的系统级框架，通过分解视频一致性建模并针对三个关键挑战设计专用组件，来提高视频的一致性，包括语内片段的一致性、跨片段的相机一致性以及跨场景元素一致性。


<details>
  <summary>Details</summary>
Motivation: 近期的视频生成模型虽然在视觉保真度方面表现出色，但往往在语义、几何和身份一致性上存在问题。本文旨在提出一种系统级框架DCDM，解决这三个方面的一致性问题。

Method: DCDM框架通过分解视频一致性建模为三个部分，使用大语言模型解析输入提示为结构化的语义表示，通过扩散变换器生成连贯的视频内容；在跨片段相机一致性中引入了时间相机表示并结合文本到图像初始化机制；对于跨场景一致性，采用了全局场景生成范式和窗口化交叉注意力及稀疏跨片段自注意力。

Result: 在AAAI'26的CVM竞赛测试集上验证了该框架，结果显示提出的策略有效解决了这些问题。

Conclusion: 结论是DCDM框架通过有效的模块设计，显著提高了视频生成中的多种一致性问题，这是一个突破性的研究。

Abstract: Recent video generative models have demonstrated impressive visual fidelity, yet they often struggle with semantic, geometric, and identity consistency. In this paper, we propose a system-level framework, termed the Divide-and-Conquer Diffusion Model (DCDM), to address three key challenges: (1) intra-clip world knowledge consistency, (2) inter-clip camera consistency, and (3) inter-shot element consistency. DCDM decomposes video consistency modeling under these scenarios into three dedicated components while sharing a unified video generation backbone. For intra-clip consistency, DCDM leverages a large language model to parse input prompts into structured semantic representations, which are subsequently translated into coherent video content by a diffusion transformer. For inter-clip camera consistency, we propose a temporal camera representation in the noise space that enables precise and stable camera motion control, along with a text-to-image initialization mechanism to further enhance controllability. For inter-shot consistency, DCDM adopts a holistic scene generation paradigm with windowed cross-attention and sparse inter-shot self-attention, ensuring long-range narrative coherence while maintaining computational efficiency. We validate our framework on the test set of the CVM Competition at AAAI'26, and the results demonstrate that the proposed strategies effectively address these challenges.

</details>


### [42] [KorMedMCQA-V: A Multimodal Benchmark for Evaluating Vision-Language Models on the Korean Medical Licensing Examination](https://arxiv.org/abs/2602.13650)
*Byungjin Choi,Seongsu Bae,Sunjun Kweon,Edward Choi*

Main category: cs.CV

TL;DR: KorMedMCQA-V是一个包含1534个问题和2043张图像的数据集，用于评估视觉-语言模型(VLMs)。该数据集来源于韩国医疗执照考试，并首次对多种差异化的模型进行了统一的零样本评估。


<details>
  <summary>Details</summary>
Motivation: 由于Vision-Language Models在图像理解和自然语言处理方面的能力，开发KorMedMCQA-V数据集旨在检验其在医疗领域的表现，特别是在处理多模态信息和复杂诊断场景下的能力。

Method: 该数据集包含从2012年至2023年韩国医学考试中提取的问题和相关图像，通过一个统一的零样本评估协议对多种类型的模型进行了测试。

Result: 不同类型的VLMs在不同测试项上的表现有显著差异，其中最好表现的自研模型Gemini-3.0-Pro的准确率为96.9%，开源模型Qwen3-VL-32B-Thinking为83.7%，而更加专注于韩国市场的模型VARCO-VISION-2.0-14B只有43.2%的准确率。此外，模型在涉及多个图像的情况下的表现较差，且在不同成像模式下表现不一。

Conclusion: KorMedMCQA-V数据集为评估VLMs在韩国医疗情境下的性能提供了一个统一的框架，显示了解释性模型相较于指令调优模型的优势，并突显了这些模型在处理多种专业成像技术方面需要改进的地方。

Abstract: We introduce KorMedMCQA-V, a Korean medical licensing-exam-style multimodal multiple-choice question answering benchmark for evaluating vision-language models (VLMs). The dataset consists of 1,534 questions with 2,043 associated images from Korean Medical Licensing Examinations (2012-2023), with about 30% containing multiple images requiring cross-image evidence integration. Images cover clinical modalities including X-ray, computed tomography (CT), electrocardiography (ECG), ultrasound, endoscopy, and other medical visuals. We benchmark over 50 VLMs across proprietary and open-source categories-spanning general-purpose, medical-specialized, and Korean-specialized families-under a unified zero-shot evaluation protocol. The best proprietary model (Gemini-3.0-Pro) achieves 96.9% accuracy, the best open-source model (Qwen3-VL-32B-Thinking) 83.7%, and the best Korean-specialized model (VARCO-VISION-2.0-14B) only 43.2%. We further find that reasoning-oriented model variants gain up to +20 percentage points over instruction-tuned counterparts, medical domain specialization yields inconsistent gains over strong general-purpose baselines, all models degrade on multi-image questions, and performance varies notably across imaging modalities. By complementing the text-only KorMedMCQA benchmark, KorMedMCQA-V forms a unified evaluation suite for Korean medical reasoning across text-only and multimodal conditions. The dataset is available via Hugging Face Datasets: https://huggingface.co/datasets/seongsubae/KorMedMCQA-V.

</details>


### [43] [LeafNet: A Large-Scale Dataset and Comprehensive Benchmark for Foundational Vision-Language Understanding of Plant Diseases](https://arxiv.org/abs/2602.13662)
*Khang Nguyen Quoc,Phuong D. Dao,Luyl-Da Quach*

Main category: cs.CV

TL;DR: 该研究提出了一套全面的多模态数据集LeafNet和视觉问答基准LeafBench，用于系统评估视觉语言模型在植物病理学理解方面的能力。结果显示，在植物病害诊断任务上，多模态模型的表现优于纯视觉模型。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型（VLMs）在视觉和语言数据的多模态处理方面取得了显著进展，但它们在特定领域应用，如植物病理学，受限于缺乏大规模、综合性的图像-文本多模态数据集和基准。

Method: 研究通过开发LeafNet多模态数据集和LeafBench视觉问答基准来解决这些问题。LeafNet包含了186,000张叶子数字图像和13,950个问题-答案对，用于评估VLMs在植物疾病理解方面的性能。

Result: 在12种领先VLMs上的基准测试显示了VLMs在植物病害理解能力上的显著差异：二分类健康-病害识别超过90%准确率，而细粒度病原体和物种识别低于65%。多模态模型（VLMs）优于纯视觉模型，证实了整合语言表示对诊断精度的显著提升。

Conclusion: 研究结论认为当前VLMs在植物病理学应用中存在关键差距，并强调需要像LeafBench这样的严谨评估框架来推动方法论的进步和可靠的AI辅助植物病害诊断的发展。

Abstract: Foundation models and vision-language pre-training have significantly advanced Vision-Language Models (VLMs), enabling multimodal processing of visual and linguistic data. However, their application in domain-specific agricultural tasks, such as plant pathology, remains limited due to the lack of large-scale, comprehensive multimodal image--text datasets and benchmarks. To address this gap, we introduce LeafNet, a comprehensive multimodal dataset, and LeafBench, a visual question-answering benchmark developed to systematically evaluate the capabilities of VLMs in understanding plant diseases. The dataset comprises 186,000 leaf digital images spanning 97 disease classes, paired with metadata, generating 13,950 question-answer pairs spanning six critical agricultural tasks. The questions assess various aspects of plant pathology understanding, including visual symptom recognition, taxonomic relationships, and diagnostic reasoning. Benchmarking 12 state-of-the-art VLMs on our LeafBench dataset, we reveal substantial disparity in their disease understanding capabilities. Our study shows performance varies markedly across tasks: binary healthy--diseased classification exceeds 90\% accuracy, while fine-grained pathogen and species identification remains below 65\%. Direct comparison between vision-only models and VLMs demonstrates the critical advantage of multimodal architectures: fine-tuned VLMs outperform traditional vision models, confirming that integrating linguistic representations significantly enhances diagnostic precision. These findings highlight critical gaps in current VLMs for plant pathology applications and underscore the need for LeafBench as a rigorous framework for methodological advancement and progress evaluation toward reliable AI-assisted plant disease diagnosis. Code is available at https://github.com/EnalisUs/LeafBench.

</details>


### [44] [EchoTorrent: Towards Swift, Sustained, and Streaming Multi-Modal Video Generation](https://arxiv.org/abs/2602.13669)
*Rang Meng,Weipeng Wu,Yingjie Yin,Yuming Li,Chenguang Ma*

Main category: cs.CV

TL;DR: EchoTorrent 提出了一种新型架构，旨在通过多教师训练、自适应 CFG 校准、混合长尾强迫以及 VAE 解码优化来解决多模态视频生成的实时部署难题，显著提升了多帧的一致性、身份保持和音频唇同步。


<details>
  <summary>Details</summary>
Motivation: 现有模型虽然在视觉效果上表现出色，但存在高延迟和时间稳定性差的问题，这些问题在流式推理中尤为突出，导致了显著的多模态退化。因此，提出了 EchoTorrent 来解决这种效率与性能之间的权衡。

Method: EchoTorrent 包含有四个创新组件：（1）多教师训练，对预训练模型进行细化以分别掌握不同的偏好领域，并逐步将领域特定的知识转移到学生模型；（2）自适应 CFG 校准，通过分阶段的时空调度调整 DMD 中的音频CFG增广错误，消除了冗余计算并允许单次过推理；（3）混合长尾强迫，运用因果双向混合架构仅在长时间自我卷积训练期间对尾部帧强制对齐，从而有效缓解了流式模式中的时空退化，同时增强了参考帧的保真度；（4）VAE 解码器精炼，通过像素域优化 VAE 解码器来恢复高频细节，从而避免潜空间模糊。

Result: 大量实验和分析表明，EchoTorrent 达到了少数自回归生成的效果，显著增加了时间一致性、身份保持以及音频和唇部同步。

Conclusion: 总之，EchoTorrent 通过上述创新设计，实现了高效率和高性能的多模态视频生成，在流式部署时取得了显著的进步。

Abstract: Recent multi-modal video generation models have achieved high visual quality, but their prohibitive latency and limited temporal stability hinder real-time deployment. Streaming inference exacerbates these issues, leading to pronounced multimodal degradation, such as spatial blurring, temporal drift, and lip desynchronization, which creates an unresolved efficiency-performance trade-off. To this end, we propose EchoTorrent, a novel schema with a fourfold design: (1) Multi-Teacher Training fine-tunes a pre-trained model on distinct preference domains to obtain specialized domain experts, which sequentially transfer domain-specific knowledge to a student model; (2) Adaptive CFG Calibration (ACC-DMD), which calibrates the audio CFG augmentation errors in DMD via a phased spatiotemporal schedule, eliminating redundant CFG computations and enabling single-pass inference per step; (3) Hybrid Long Tail Forcing, which enforces alignment exclusively on tail frames during long-horizon self-rollout training via a causal-bidirectional hybrid architecture, effectively mitigates spatiotemporal degradation in streaming mode while enhancing fidelity to reference frames; and (4) VAE Decoder Refiner through pixel-domain optimization of the VAE decoder to recover high-frequency details while circumventing latent-space ambiguities. Extensive experiments and analysis demonstrate that EchoTorrent achieves few-pass autoregressive generation with substantially extended temporal consistency, identity preservation, and audio-lip synchronization.

</details>


### [45] [An Ensemble Learning Approach towards Waste Segmentation in Cluttered Environment](https://arxiv.org/abs/2602.13681)
*Maimoona Jafar,Syed Imran Ali,Ahsan Saadat,Muhammad Bilal,Shah Khalid*

Main category: cs.CV

TL;DR: 该研究提出了一种集成学习方法，通过结合U-Net和FPN的优势来提升废物分拣中的分割准确性，显著提高了IoU并降低了Dice损失。


<details>
  <summary>Details</summary>
Motivation: 废物分拣是回收过程中的关键步骤，而计算机视觉的进步有助于废物分类和识别。然而，现实世界的废物环境复杂，增加了分割任务的难度。因此，研究提出了一种集成学习方法以提升分割准确性。

Method: 该研究使用了一种集成学习方法，结合了U-Net和FPN的高精度模型，通过加权平均的方式整合了这两种模型的优点。该方法应用于模拟真实废物场景的_datasets_中，并通过预处理技术提升了深层学习分割模型的特征学习能力。

Result: EL-4模型取得了IoU为0.8306、Dice损失为0.09019的成绩，相比U-Net和FPN分别有显著提升。

Conclusion: 该研究有助于提高材料回收设施的效率，减少人工干预，从而更好地获取用于回收的原材料，提升整体吞吐量。

Abstract: Environmental pollution is a critical global issue, with recycling emerging as one of the most viable solutions. This study focuses on waste segregation, a crucial step in recycling processes to obtain raw material. Recent advancements in computer vision have significantly contributed to waste classification and recognition. In waste segregation, segmentation masks are essential for robots to accurately localize and pick objects from conveyor belts. The complexity of real-world waste environments, characterized by deformed items without specific patterns and overlapping objects, further complicates waste segmentation tasks. This paper proposes an Ensemble Learning approach to improve segmentation accuracy by combining high performing segmentation models, U-Net and FPN, using a weighted average method. U-Net excels in capturing fine details and boundaries in segmentation tasks, while FPN effectively handles scale variation and context in complex environments, and their combined masks result in more precise predictions. The dataset used closely mimics real-life waste scenarios, and preprocessing techniques were applied to enhance feature learning for deep learning segmentation models. The ensemble model, referred to as EL-4, achieved an IoU value of 0.8306, an improvement over U-Net's 0.8065, and reduced Dice loss to 0.09019 from FPN's 0.1183. This study could contribute to the efficiency of waste sorting at Material Recovery Facility, facilitating better raw material acquisition for recycling with minimal human intervention and enhancing the overall throughput.

</details>


### [46] [A WDLoRA-Based Multimodal Generative Framework for Clinically Guided Corneal Confocal Microscopy Image Synthesis in Diabetic Neuropathy](https://arxiv.org/abs/2602.13693)
*Xin Zhang,Liangxiu Han,Yue Shi,Yalin Zheng,Uazman Alam,Maryam Ferdousi,Rayaz Malik*

Main category: cs.CV

TL;DR: 提出了一种基于Weight-Decomposed Low-Rank Adaptation (WDLoRA)的多模态生成框架，该框架能够为临床引导的共焦显微镜(CCM)图像合成提供医学现实感。


<details>
  <summary>Details</summary>
Motivation: 开发自动化的深度学习诊断模型受到稀缺的标签数据和细微结构变异性的限制，现有的基础生成模型在医学成像中由于有限的领域特定训练而难以实现所需的解剖保真度。

Method: 作者提出了一种WDLoRA参数高效的微调方法，该方法将幅度和方向的权重量化分离，使基础生成模型能够独立学习所需的神经拓扑和间质对比强度。

Result: 该生成模型能合成跨DPN谱系（如正常对照、1型糖尿病无DPN、1型糖尿病DPN）的一致性解剖图像，并在FID: 5.18和SSIM: 0.630的客观评诂指标上超越了GAN和标准扩散基线，且合成图像在重要临床生物标志物上与真实患者数据等效。

Conclusion: 该生成框架通过合成高质量的训练数据，提高了自动化诊断模型的下游诊断准确性和分割性能，表明其在缓解医疗AI中的数据瓶颈方面具有潜力。

Abstract: Corneal Confocal Microscopy (CCM) is a sensitive tool for assessing small-fiber damage in Diabetic Peripheral Neuropathy (DPN), yet the development of robust, automated deep learning-based diagnostic models is limited by scarce labelled data and fine-grained variability in corneal nerve morphology. Although Artificial Intelligence (AI)-driven foundation generative models excel at natural image synthesis, they often struggle in medical imaging due to limited domain-specific training, compromising the anatomical fidelity required for clinical analysis. To overcome these limitations, we propose a Weight-Decomposed Low-Rank Adaptation (WDLoRA)-based multimodal generative framework for clinically guided CCM image synthesis. WDLoRA is a parameter-efficient fine-tuning (PEFT) mechanism that decouples magnitude and directional weight updates, enabling foundation generative models to independently learn the orientation (nerve topology) and intensity (stromal contrast) required for medical realism. By jointly conditioning on nerve segmentation masks and disease-specific clinical prompts, the model synthesises anatomically coherent images across the DPN spectrum (Control, T1NoDPN, T1DPN). A comprehensive three-pillar evaluation demonstrates that the proposed framework achieves state-of-the-art visual fidelity (Fréchet Inception Distance (FID): 5.18) and structural integrity (Structural Similarity Index Measure (SSIM): 0.630), significantly outperforming GAN and standard diffusion baselines. Crucially, the synthetic images preserve gold-standard clinical biomarkers and are statistically equivalent to real patient data. When used to train automated diagnostic models, the synthetic dataset improves downstream diagnostic accuracy by 2.1% and segmentation performance by 2.2%, validating the framework's potential to alleviate data bottlenecks in medical AI.

</details>


### [47] [Fine-tuned Vision Language Model for Localization of Parasitic Eggs in Microscopic Images](https://arxiv.org/abs/2602.13712)
*Chan Hao Sien,Hezerul Abdul Karim,Nouar AlDahoul*

Main category: cs.CV

TL;DR: 利用微调后的视觉语言模型Microsoft Florence 对土壤传播的蠕虫感染进行自动化的显微镜诊断，mIOU达到0.94，优于其他检测方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于显微镜的手工诊断方法耗时且容易出错，尤其是在资源有限的热带和亚热带地区，亟需一种更高效的诊断手段。

Method: 使用微调后的视觉语言模型Microsoft Florence，专门针对土壤传播的蠕虫感染进行局部化检测。

Result: 实验结果显示，该模型的mIOU达到了0.94，优于对照组的方法。

Conclusion: 该模型具有作为自动化诊断系统核心组件的潜力，可以提供一种智能的寄生虫诊断方案，能够扩展工程解构。

Abstract: Soil-transmitted helminth (STH) infections continuously affect a large proportion of the global population, particularly in tropical and sub-tropical regions, where access to specialized diagnostic expertise is limited. Although manual microscopic diagnosis of parasitic eggs remains the diagnostic gold standard, the approach can be labour-intensive, time-consuming, and prone to human error. This paper aims to utilize a vision language model (VLM) such as Microsoft Florence that was fine-tuned to localize all parasitic eggs within microscopic images. The preliminary results show that our localization VLM performs comparatively better than the other object detection methods, such as EfficientDet, with an mIOU of 0.94. This finding demonstrates the potential of the proposed VLM to serve as a core component of an automated framework, offering a scalable engineering solution for intelligent parasitological diagnosis.

</details>


### [48] [RGA-Net: A Vision Enhancement Framework for Robotic Surgical Systems Using Reciprocal Attention Mechanisms](https://arxiv.org/abs/2602.13726)
*Quanjun Li,Weixuan Li,Han Xia,Junhua Zhou,Chi-Man Pun,Xuhang Chen*

Main category: cs.CV

TL;DR: RGA-Net是一种新颖的深度学习框架，专门针对机器人手术工作流程中的烟雾去除问题，通过层次编码解码架构结合Dual-Stream Hybrid Attention (DHA)和Axis-Decomposed Attention (ADA)模块来恢复视觉清晰度。


<details>
  <summary>Details</summary>
Motivation: 针对机器人手术中由于电能设备产生的烟雾对内窥镜视频反馈造成的影响，提高视觉反馈的质量对于实现精准的远程操作至关重要。

Method: RGA-Net通过层次编码解码架构，结合Dual-Stream Hybrid Attention (DHA)和Axis-Decomposed Attention (ADA)模块，捕获局部手术细节和全局照明变化，实现烟雾的有效去除。

Result: 实验结果表明，RGA-Net在恢复视觉清晰度方面具有优越的表现，为机器人手术集成提供了持续清晰的视觉反馈，将为提升手术效果，减少人为伤害风险带来可能。

Conclusion: RGA-Net为可靠的和更安全的机器人手术系统的发展奠定了计算视觉增强技术的基础。未来可能通过外科医生使用评估进一步验证其效益。

Abstract: Robotic surgical systems rely heavily on high-quality visual feedback for precise teleoperation; yet, surgical smoke from energy-based devices significantly degrades endoscopic video feeds, compromising the human-robot interface and surgical outcomes. This paper presents RGA-Net (Reciprocal Gating and Attention-fusion Network), a novel deep learning framework specifically designed for smoke removal in robotic surgery workflows. Our approach addresses the unique challenges of surgical smoke-including dense, non-homogeneous distribution and complex light scattering-through a hierarchical encoder-decoder architecture featuring two key innovations: (1) a Dual-Stream Hybrid Attention (DHA) module that combines shifted window attention with frequency-domain processing to capture both local surgical details and global illumination changes, and (2) an Axis-Decomposed Attention (ADA) module that efficiently processes multi-scale features through factorized attention mechanisms. These components are connected via reciprocal cross-gating blocks that enable bidirectional feature modulation between encoder and decoder pathways. Extensive experiments on the DesmokeData and LSD3K surgical datasets demonstrate that RGA-Net achieves superior performance in restoring visual clarity suitable for robotic surgery integration. Our method enhances the surgeon-robot interface by providing consistently clear visualization, laying a technical foundation for alleviating surgeons' cognitive burden, optimizing operation workflows, and reducing iatrogenic injury risks in minimally invasive procedures. These practical benefits could be further validated through future clinical trials involving surgeon usability assessments. The proposed framework represents a significant step toward more reliable and safer robotic surgical systems through computational vision enhancement.

</details>


### [49] [Generative Latent Representations of 3D Brain MRI for Multi-Task Downstream Analysis in Down Syndrome](https://arxiv.org/abs/2602.13731)
*Jordi Malé,Juan Fortea,Mateus Rozalem-Aranha,Neus Martínez-Abadías,Xavier Sevillano*

Main category: cs.CV

TL;DR: 本研究开发了多个变分自编码器（VAE）来编码3D脑MRI扫描图像到紧凑的潜在空间表示，用于生成和预测应用。通过MRI重建质量的量化和定性评估、潜在空间结构的主成分分析可视化以及对携带唐氏综合症和正常个体的脑MRI扫描数据集的下游分类任务，验证了VAE能够捕获关键的大脑特征，并保持高质量的重建保真度。潜在空间显示出明显的聚类模式，特别是在区分唐氏综合症个体和正常控制个体方面。


<details>
  <summary>Details</summary>
Motivation: 鉴于在医学成像领域，生成模型在分割、异常检测和高质量合成数据生成方面的强大功能，本研究旨在探索潜在表示结构、信息内容及其在下游临床任务中的应用，以推动生成模型在神经影像学研究和临床决策制定中的使用。

Method: 开发多个VAE来编码3D脑MRI扫描图像，并通过三个关键分析进行评估：（i）MRI重建质量的定性和定量评估；（ii）通过主成分分析可视化潜在空间结构；（iii）在携带唐氏综合症和正常个体的MRI扫描数据集上的下游分类任务。

Result: 研究结果表明，VAE成功捕获了关键的大脑特征，保持了高质量的重建保真度。潜在空间显示出明显的聚类模式，特别是在区分唐氏综合症个体和正常控制个体方面。

Conclusion: 本研究揭示了VAE在编码3D脑MRI扫描图像中紧凑潜在空间表示的有效性，展示了这些潜在表示可用于重要的临床应用，并在区分唐氏综合症和其他健康个体方面表现出色。

Abstract: Generative models have emerged as powerful tools in medical imaging, enabling tasks such as segmentation, anomaly detection, and high-quality synthetic data generation. These models typically rely on learning meaningful latent representations, which are particularly valuable given the high-dimensional nature of 3D medical images like brain magnetic resonance imaging (MRI) scans. Despite their potential, latent representations remain underexplored in terms of their structure, information content, and applicability to downstream clinical tasks. Investigating these representations is crucial for advancing the use of generative models in neuroimaging research and clinical decision-making. In this work, we develop multiple variational autoencoders (VAEs) to encode 3D brain MRI scans into compact latent space representations for generative and predictive applications. We systematically evaluate the effectiveness of the learned representations through three key analyses: (i) a quantitative and qualitative assessment of MRI reconstruction quality, (ii) a visualisation of the latent space structure using Principal Component Analysis, and (iii) downstream classification tasks on a proprietary dataset of euploid and Down syndrome individuals brain MRI scans. Our results demonstrate that the VAE successfully captures essential brain features while maintaining high reconstruction fidelity. The latent space exhibits clear clustering patterns, particularly in distinguishing individuals with Down syndrome from euploid controls.

</details>


### [50] [T2MBench: A Benchmark for Out-of-Distribution Text-to-Motion Generation](https://arxiv.org/abs/2602.13751)
*Bin Yang,Rong Ou,Weisheng Xu,Jiaqi Xiong,Xintao Li,Taowen Wang,Luyu Zhu,Xu Jiang,Jing Tan,Renjing Xu*

Main category: cs.CV

TL;DR: 本文提出了一个针对离分布文本生成运动的基准测试，包含一个包含1,025种文本描述的离分布提示数据集以及一个统一的评估框架，涵盖LLM评估、多因素运动评估和精细准确评估。


<details>
  <summary>Details</summary>
Motivation: 当前大多数文本到运动生成的评估关注内部文本输入和有限的评价标准，这限制了其系统评估模型泛化能力和复杂离分布条件下的运动生成能力。

Method: 本文构建了一个包含1,025种文本描述的离分布提示数据集，并通过结合LLM评估、多因素运动评估和精细准确评估的统一框架进行评估。

Result: 不同基准模型在语义对齐、运动泛化性和物理质量方面展示出优势，但在精细准确评估方面表现较弱。

Conclusion: 本文的研究揭示了现有方法在离分布场景中的局限性，并为未来生产级文本到运动模型的设计和评估提供了实用指导。

Abstract: Most existing evaluations of text-to-motion generation focus on in-distribution textual inputs and a limited set of evaluation criteria, which restricts their ability to systematically assess model generalization and motion generation capabilities under complex out-of-distribution (OOD) textual conditions. To address this limitation, we propose a benchmark specifically designed for OOD text-to-motion evaluation, which includes a comprehensive analysis of 14 representative baseline models and the two datasets derived from evaluation results. Specifically, we construct an OOD prompt dataset consisting of 1,025 textual descriptions. Based on this prompt dataset, we introduce a unified evaluation framework that integrates LLM-based Evaluation, Multi-factor Motion evaluation, and Fine-grained Accuracy Evaluation. Our experimental results reveal that while different baseline models demonstrate strengths in areas such as text-to-motion semantic alignment, motion generalizability, and physical quality, most models struggle to achieve strong performance with Fine-grained Accuracy Evaluation. These findings highlight the limitations of existing methods in OOD scenarios and offer practical guidance for the design and evaluation of future production-level text-to-motion models.

</details>


### [51] [OmniScience: A Large-scale Multi-modal Dataset for Scientific Image Understanding](https://arxiv.org/abs/2602.13758)
*Haoyi Tao,Chaozheng Huang,Nan Wang,Han Lyu,Linfeng Zhang,Guolin Ke,Xi Fang*

Main category: cs.CV

TL;DR: 本文介绍了一种名为OmniScience的大型多模态数据集，包含150万幅科学图像的描述，旨在增强多模态大语言模型在科学图像理解上的能力。通过开发一种动态模型路由重描述管道，生成密集且自包含的图像描述，数据集在多模态文本-图像相似度上取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 当前的多模态大语言模型在处理科学图像方面的能力有限，特别是在开放源代码模型中表现更为逊色。文章旨在通过建立更大规模和更高保真度的多模态数据集，特别是包含多种科学学科的图像描述，来增强模型的理解能力。

Method: 首先，文章使用一个包含150万幅图像描述的多模态数据集，该数据集来源于10多个主要科学学科。其次，通过一个动态模型路由重描述管道，结合视觉特征、原始图像描述和人类科学家撰写的文本参考，生成密集且自包含的图像描述。最后，数据集通过严谨的质量筛选和与人类专家判断对照，保证了准确性和语义完整性。

Result: 数据集生成的图像描述在多模态文本-图像相似度上取得了显著提升，从0.769提升到了0.956。此外，该数据集下的Qwen2.5-VL-3B模型的性能也得到了显著提升，分别在MM-MT-Bench和MMMU上获得了0.378和0.140的增益。

Conclusion: 本文提出了一种用于增强科学图像理解的大型多模态数据集，并通过实验证明了其有效性和提升效果。

Abstract: Multimodal Large Language Models demonstrate strong performance on natural image understanding, yet exhibit limited capability in interpreting scientific images, including but not limited to schematic diagrams, experimental characterizations, and analytical charts. This limitation is particularly pronounced in open-source MLLMs. The gap largely stems from existing datasets with limited domain coverage, coarse structural annotations, and weak semantic grounding. We introduce OmniScience, a large-scale, high-fidelity multi-modal dataset comprising 1.5 million figure-caption-context triplets, spanning more than 10 major scientific disciplines. To obtain image caption data with higher information density and accuracy for multi-modal large-model training, we develop a dynamic model-routing re-captioning pipeline that leverages state-of-the-art multi-modal large language models to generate dense, self-contained descriptions by jointly synthesizing visual features, original figure captions, and corresponding in-text references authored by human scientists. The pipeline is further reinforced with rigorous quality filtering and alignment with human expert judgments, ensuring both factual accuracy and semantic completeness, and boosts the image-text multi-modal similarity score from 0.769 to 0.956. We further propose a caption QA protocol as a proxy task for evaluating visual understanding. Under this setting, Qwen2.5-VL-3B model finetuned on OmniScience show substantial gains over baselines, achieving a gain of 0.378 on MM-MT-Bench and a gain of 0.140 on MMMU.

</details>


### [52] [SAM4Dcap: Training-free Biomechanical Twin System from Monocular Video](https://arxiv.org/abs/2602.13760)
*Li Wang,HaoYu Wang,Xi Chen,ZeKun Jiang,Kang Li,Jian Li*

Main category: cs.CV

TL;DR: 该论文提出了SAM4Dcap，一个开源框架，用于通过单目视频估算人体运动学指标，有望在家庭环境中实现膝关节运动学预测与多视图系统的相似效果。


<details>
  <summary>Details</summary>
Motivation: 当前的光学运动捕捉系统昂贵且仅限于实验室环境，而多视图视频方法尽管降低了门槛但在家庭环境中仍不实用，需要单目捕捉能力。

Method: SAM4Dcap集成了SAM-Body4D的一致4D人体网格恢复和OpenSim生物力学求解器，将其重建的网格转换为多种生物力学模型兼容的轨迹文件。

Result: 初步评估表明，SAM4Dcap在行走和跳跃任务中的膝关节运动预测与多视图系统具有相似的效果，虽然髋屈曲评估仍存在一定误差。

Conclusion: SAM4Dcap将高级计算机视觉技术与传统的生物力学模拟相结合，为非实验室环境下的运动分析提供了灵活且易于访问的基础框架。

Abstract: Quantitative biomechanical analysis is essential for clinical diagnosis and injury prevention but is often restricted to laboratories due to the high cost of optical motion capture systems. While multi-view video approaches have lowered barriers, they remain impractical for home-based scenarios requiring monocular capture. This paper presents SAM4Dcap, an open-source, end-to-end framework for estimating biomechanical metrics from monocular video without additional training. SAM4Dcap integrates the temporally consistent 4D human mesh recovery of SAM-Body4D with the OpenSim biomechanical solver. The pipeline converts reconstructed meshes into trajectory files compatible with diverse musculoskeletal models. We introduce automated prompting strategies and a Linux-native build for processing. Preliminary evaluations on walking and drop-jump tasks indicate that SAM4Dcap has the potential to achieve knee kinematic predictions comparable to multi-view systems, although some discrepancies in hip flexion and residual jitter remain. By bridging advanced computer vision with established biomechanical simulation, SAM4Dcap provides a flexible, accessible foundation for non-laboratory motion analysis.

</details>


### [53] [Skeleton2Stage: Reward-Guided Fine-Tuning for Physically Plausible Dance Generation](https://arxiv.org/abs/2602.13778)
*Jidong Jia,Youjian Zhang,Huan Fu,Dacheng Tao*

Main category: cs.CV

TL;DR: 通过引入基于物理的奖励并结合反冻结奖励，论文采用强化学习微调方法，解决了现有舞蹈生成方法在网格视图下可能存在的身体穿透和足底滑行等问题，从而显著提高了生成舞蹈的物理合理性和美学价值。


<details>
  <summary>Details</summary>
Motivation: 现有大多数舞蹈生成方法集中在骨骼领域，忽略了网格级别的物理约束，导致生成的舞蹈在网格视图下显得不合理，如身体穿透和足底滑行等问题；论文旨在解决这一问题。

Method: 利用物理仿真器评估运动的模拟性，结合足底与地面的偏差奖励，引入对抗冻结奖励确保动态性能。通过强化学习微调扩散模型，使其生成的运动能够满足物理约束并保持动态。

Result: 实验表明，该方法显著提升了生成舞蹈的物理可信度，使得生成的舞蹈更加真实且美观。与其他舞蹈生成方法相比，其生成结果更具真实感和美感。

Conclusion: 通过上述方法，论文成功地解决了舞蹈生成过程中存在的物理约束问题，生成的舞蹈更加合理且具有观赏性。未来的研究可进一步优化奖励设计，提高生成效果。

Abstract: Despite advances in dance generation, most methods are trained in the skeletal domain and ignore mesh-level physical constraints. As a result, motions that look plausible as joint trajectories often exhibit body self-penetration and Foot-Ground Contact (FGC) anomalies when visualized with a human body mesh, reducing the aesthetic appeal of generated dances and limiting their real-world applications. We address this skeleton-to-mesh gap by deriving physics-based rewards from the body mesh and applying Reinforcement Learning Fine-Tuning (RLFT) to steer the diffusion model toward physically plausible motion synthesis under mesh visualization. Our reward design combines (i) an imitation reward that measures a motion's general plausibility by its imitability in a physical simulator (penalizing penetration and foot skating), and (ii) a Foot-Ground Deviation (FGD) reward with test-time FGD guidance to better capture the dynamic foot-ground interaction in dance. However, we find that the physics-based rewards tend to push the model to generate freezing motions for fewer physical anomalies and better imitability. To mitigate it, we propose an anti-freezing reward to preserve motion dynamics while maintaining physical plausibility. Experiments on multiple dance datasets consistently demonstrate that our method can significantly improve the physical plausibility of generated motions, yielding more realistic and aesthetically pleasing dances. The project page is available at: https://jjd1123.github.io/Skeleton2Stage/

</details>


### [54] [Foundation Model-Driven Semantic Change Detection in Remote Sensing Imagery](https://arxiv.org/abs/2602.13780)
*Hengtong Shen,Li Yan,Hong Xie,Yaxuan Wei,Xinhao Li,Wenfei Shen,Peixian Lv,Fei Tan*

Main category: cs.CV

TL;DR: 本研究提出了一种名为PerASCD的方法，该方法运用遥感基础模型PerA，通过模块化的Hierarchical Gated Decoder简化了语义变化检测的过程，同时引入Soft Semantic Consistency Loss增强训练的稳定性，该方法在两个公开基准数据集上达到了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有语义变化检测方法未能同时解决性能和复杂度问题，因此有必要探索一种能够简化Semi-supervised Learning过程并提高模型性能的新方法。

Method: PerASCD方法融合了遥感基础模型PerA和一个模块化的Hierarchical Gated Decoder来简化解码流程中的复杂性。同时引入Soft Semantic Consistency Loss以稳定训练。

Result: PerASCD方法在两个公开基准数据集上达到了最先进的性能，实现了从复杂解码流程向简单化和高效性的转变。

Conclusion: 这种方法为解决语义变化检测任务中的挑战提供了新的视角，并展示了模块化设计理念和Soft Semantic Consistency Loss在遥感应用中的潜力。

Abstract: Remote sensing (RS) change detection methods can extract critical information on surface dynamics and are an essential means for humans to understand changes in the earth's surface and environment. Among these methods, semantic change detection (SCD) can more effectively interpret the multi-class information contained in bi-temporal RS imagery, providing semantic-level predictions that support dynamic change monitoring. However, due to the limited semantic understanding capability of the model and the inherent complexity of the SCD tasks, existing SCD methods face significant challenges in both performance and paradigm complexity. In this paper, we propose PerASCD, a SCD method driven by RS foundation model PerA, designed to enhance the multi-scale semantic understanding and overall performance. We introduce a modular Cascaded Gated Decoder (CG-Decoder) that simplifies complex SCD decoding pipelines while promoting effective multi-level feature interaction and fusion. In addition, we propose a Soft Semantic Consistency Loss (SSCLoss) to mitigate the numerical instability commonly encountered during SCD training. We further explore the applicability of multiple existing RS foundation models on the SCD task when equipped with the proposed decoder. Experimental results demonstrate that our decoder not only effectively simplifies the paradigm of SCD, but also achieves seamless adaptation across various vision encoders. Our method achieves state-of-the-art (SOTA) performance on two public benchmark datasets, validating its effectiveness. The code is available at https://github.com/SathShen/PerASCD.git.

</details>


### [55] [Joint Orientation and Weight Optimization for Robust Watertight Surface Reconstruction via Dirichlet-Regularized Winding Fields](https://arxiv.org/abs/2602.13801)
*Jiaze Li,Daisheng Jin,Fei Hou,Junhui Hou,Zheng Liu,Shiqing Xin,Wenping Wang,Ying He*

Main category: cs.CV

TL;DR: DiWR 提出了一种用于处理不均匀采样、噪声和离群值的非定向点云的稳健网格重建方法，通过联合优化点方向、点面积权重和置信系数来最小化 Generalized Winding Number 函数的能量。


<details>
  <summary>Details</summary>
Motivation: 现有的网格重建方法在处理非定向点云时容易出错，尤其是在面对不均匀采样、噪声和离群值时。本文旨在提出一种新的方法——Dirichlet Winding Reconstruction (DiWR)，解决这些问题。

Method: DiWR 使用 Generalized Winding Number (GWN) 字段作为目标隐式表示，并在此单一管道中联合优化点方向、每个点的面积权重和置信系数。通过最小化诱导的 GWN 字段的 Dirichlet 能量，并结合额外的 GWN 基础约束，DiWR 能够补偿不均匀采样，减少噪声影响，并在重建过程中忽略离群值。

Result: DiWR 在 3D Gaussian Splatting、计算机视觉流程和受损图形基准中的点云上进行了评估。实验表明，DiWR 在这些具有挑战性的输入上生成了合理的封闭表面，并且比传统的多阶段流水线和现代联合方向重建方法表现得更好。

Conclusion: DiWR 对于复杂点云数据具有鲁棒性，能够有效处理非均匀采样、噪声和离群值的问题，为点云网格重建提供了新的思路。

Abstract: We propose Dirichlet Winding Reconstruction (DiWR), a robust method for reconstructing watertight surfaces from unoriented point clouds with non-uniform sampling, noise, and outliers. Our method uses the generalized winding number (GWN) field as the target implicit representation and jointly optimizes point orientations, per-point area weights, and confidence coefficients in a single pipeline. The optimization minimizes the Dirichlet energy of the induced winding field together with additional GWN-based constraints, allowing DiWR to compensate for non-uniform sampling, reduce the impact of noise, and downweight outliers during reconstruction, with no reliance on separate preprocessing. We evaluate DiWR on point clouds from 3D Gaussian Splatting, a computer-vision pipeline, and corrupted graphics benchmarks. Experiments show that DiWR produces plausible watertight surfaces on these challenging inputs and outperforms both traditional multi-stage pipelines and recent joint orientation-reconstruction methods.

</details>


### [56] [Gaussian Sequences with Multi-Scale Dynamics for 4D Reconstruction from Monocular Casual Videos](https://arxiv.org/abs/2602.13806)
*Can Li,Jie Gu,Jingmin Chen,Fangzhou Qiu,Lei Sun*

Main category: cs.CV

TL;DR: 该研究提出了一种多尺度动态机制来从单目视频中实现动态场景的多维重建，并通过引入来自视觉基础模型的多模态先验增强了监督，使得重建更加精确和全局一致。


<details>
  <summary>Details</summary>
Motivation: 现有的从单目视频重建动态场景的方法大多是四维（4D）重建，在严格的一目观测条件下，仍然是高度病态问题，因此需要一种新的机制来解决这一挑战。

Method: 研究设计了一种多尺度动态机制，将复杂的运动场分解为多层次的动力学。此外，引入多模态先验来自视觉基础模型以提供互补监督，从而限制解决方案空间并提高重建保真度。

Result: 该研究的方法能够从单目日常视频中实现准确且全局一致的4D重建，实验表明在基准数据集和真实世界的操作数据集上的动态新颖视图合成方面优于现有方法。

Conclusion: 该研究提供了一种有效的解决方案，通过引入多层次动态和多模态先验，大大提高了从单目视频中重建动态场景的准确性。

Abstract: Understanding dynamic scenes from casual videos is critical for scalable robot learning, yet four-dimensional (4D) reconstruction under strictly monocular settings remains highly ill-posed. To address this challenge, our key insight is that real-world dynamics exhibits a multi-scale regularity from object to particle level. To this end, we design the multi-scale dynamics mechanism that factorizes complex motion fields. Within this formulation, we propose Gaussian sequences with multi-scale dynamics, a novel representation for dynamic 3D Gaussians derived through compositions of multi-level motion. This layered structure substantially alleviates ambiguity of reconstruction and promotes physically plausible dynamics. We further incorporate multi-modal priors from vision foundation models to establish complementary supervision, constraining the solution space and improving the reconstruction fidelity. Our approach enables accurate and globally consistent 4D reconstruction from monocular casual videos. Experiments of dynamic novel-view synthesis (NVS) on benchmark and real-world manipulation datasets demonstrate considerable improvements over existing methods.

</details>


### [57] [VAR-3D: View-aware Auto-Regressive Model for Text-to-3D Generation via a 3D Tokenizer](https://arxiv.org/abs/2602.13818)
*Zongcheng Han,Dongyan Cao,Haoran Sun,Yu Hong*

Main category: cs.CV

TL;DR: 本文提出了一种称为View-aware Auto-Regressive 3D (VAR-3D)的方法，旨在通过引入视图感知的3D向量量化-变分自编码器和渲染监督训练策略，解决3D生成中的几何连贯性问题和目标不匹配问题。


<details>
  <summary>Details</summary>
Motivation: 现有的自回归变换器在生成建模中取得了显著的成功，但将文本转化为3D生成仍然具有挑战性。主要问题包括信息在编码过程中丢失，导致表示失真，经过向量量化后进一步恶化，以及传统两阶段训练方式导致重建目标与基于文本的自回归生成目标不一致。

Method: VAR-3D方法结合了视图感知的3D向量量化-变分自编码器，用于将复杂几何结构的3D模型转换为离散码本。此外，引入了一种渲染监督训练策略，将离散码本预测与视觉重建耦合，以提高生成模型保持视觉保真度和结构一致性。

Result: 实验结果表明，VAR-3D相较于现有方法在生成质量和文本-3D对齐方面表现出显著的优势。

Conclusion: 本文提出的方法和策略能够显著改善3D生成的质量和文本对齐，展示了其在3D生成领域的潜力。

Abstract: Recent advances in auto-regressive transformers have achieved remarkable success in generative modeling. However, text-to-3D generation remains challenging, primarily due to bottlenecks in learning discrete 3D representations. Specifically, existing approaches often suffer from information loss during encoding, causing representational distortion before the quantization process. This effect is further amplified by vector quantization, ultimately degrading the geometric coherence of text-conditioned 3D shapes. Moreover, the conventional two-stage training paradigm induces an objective mismatch between reconstruction and text-conditioned auto-regressive generation. To address these issues, we propose View-aware Auto-Regressive 3D (VAR-3D), which intergrates a view-aware 3D Vector Quantized-Variational AutoEncoder (VQ-VAE) to convert the complex geometric structure of 3D models into discrete tokens. Additionally, we introduce a rendering-supervised training strategy that couples discrete token prediction with visual reconstruction, encouraging the generative process to better preserve visual fidelity and structural consistency relative to the input text. Experiments demonstrate that VAR-3D significantly outperforms existing methods in both generation quality and text-3D alignment.

</details>


### [58] [Embed-RL: Reinforcement Learning for Reasoning-Driven Multimodal Embeddings](https://arxiv.org/abs/2602.13823)
*Haonan Jiang,Yuji Wang,Yongjie Zhu,Xin Lu,Wenyu Qin,Meng Wang,Pengfei Wan,Yansong Tang*

Main category: cs.CV

TL;DR: 该研究提出了一种基于Embedder-Guided Reinforcement Learning的推理驱动的多模态嵌入框架，通过生成与嵌入任务对齐的Evidential Traceability CoT，并引入了T-CoT来突出检索相关的多模态线索，改善了跨模态语义一致性和精细匹配能力，且在资源受限情况下性能优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 当前的生成嵌入方法生成的推理链仅限于查询的文本分析，与目标检索无关。因此，该研究旨在通过引入Embedder-Guided Reinforcement Learning和Traceability CoT来优化嵌入推理，提升多模态表示的针对性和一致性。

Method: 研究提出了一个EG-RL框架，利用嵌入器为推理器提供显式监督，生成用于嵌入任务的对齐推理链T-CoT，并专注于检索相关元素。

Result: 研究在MMEB-V2和UVRB基准测试上展示了在有限计算资源下的优越性能，提高了跨模态语义一致性，并提升了模型在复杂场景下的泛化能力。

Conclusion: 该工作证明了目标推理优化可以显著提高多模态嵌入质量，提供了一种实用且高效的推理驱动的多模态嵌入开发方案。

Abstract: Leveraging Multimodal Large Language Models (MLLMs) has become pivotal for advancing Universal Multimodal Embeddings (UME) in addressing diverse cross-modal tasks. Recent studies demonstrate that incorporating generative Chain-of-Thought (CoT) reasoning can substantially enhance task-specific representations compared to discriminative methods. However, the generated reasoning CoTs of existing generative embedding methods are limited to the textual analysis of queries and are irrelevant to the retrieval of the targets. To address these limitations, we propose a reasoning-driven UME framework that integrates Embedder-Guided Reinforcement Learning (EG-RL) to optimize the Reasoner to produce evidential Traceability CoT (T-CoT). Our key contributions are threefold: (1) We design an EG-RL framework where the Embedder provides explicit supervision to the Reasoner, ensuring the generated CoT traces are aligned with embedding tasks. (2) We introduce T-CoT, which extracts critical multimodal cues to focus on retrieval-relevant elements and provides multimodal inputs for the Embedder. (3) With limited computational resources, our framework outperforms the pioneering embedding model on both MMEB-V2 and UVRB benchmarks. The integration of multimodal evidence in structured reasoning, paired with retrieval-oriented alignment, effectively strengthens cross-modal semantic consistency and boosts the fine-grained matching capability of the model as well as the generalization across complex scenarios. Our work demonstrates that targeted reasoning optimization can significantly improve multimodal embedding quality, providing a practical and efficient solution for reasoning-driven UME development.

</details>


### [59] [Prior-guided Hierarchical Instance-pixel Contrastive Learning for Ultrasound Speckle Noise Suppression](https://arxiv.org/abs/2602.13831)
*Zhenyu Bu,Yuanxin Xie,Guang-Quan Zhou*

Main category: cs.CV

TL;DR: 提出了一种先验引导的分层实例-像素对比学习模型，用于超声波去噪，该模型通过增强噪声和干净像素之间的分布差异，并结合Transformer和CNN架构，在保持结构保真度的同时有效抑制噪声。


<details>
  <summary>Details</summary>
Motivation: 超声成像中出现的斑点噪声会降低图像质量并影响诊断准确性，因此需要一种有效的去噪方法来同时抑制噪声并保留结构细节。

Method: 该方法包括像素级对比学习策略以改善像素级的分布差异，以及实例级对比学习策略来增强特征表示，并采用结合Transformer和CNN的架构，通过全局上下文建模和细粒度解剖结构恢复来优化特征表示。

Result: 在两个公开的超声数据集上进行了广泛评估，表明所提出的方法在去噪性能上优于现有方法。

Conclusion: 该研究提出的方法在超声波去噪中表现出色，不仅能够有效去除噪声，还能够保持结构的准确性，为临床诊断和治疗提供了可靠的支持。

Abstract: Ultrasound denoising is essential for mitigating speckle-induced degradations, thereby enhancing image quality and improving diagnostic reliability. Nevertheless, because speckle patterns inherently encode both texture and fine anatomical details, effectively suppressing noise while preserving structural fidelity remains a significant challenge. In this study, we propose a prior-guided hierarchical instance-pixel contrastive learning model for ultrasound denoising, designed to promote noise-invariant and structure-aware feature representations by maximizing the separability between noisy and clean samples at both pixel and instance levels. Specifically, a statistics-guided pixel-level contrastive learning strategy is introduced to enhance distributional discrepancies between noisy and clean pixels, thereby improving local structural consistency. Concurrently, a memory bank is employed to facilitate instance-level contrastive learning in the feature space, encouraging representations that more faithfully approximate the underlying data distribution. Furthermore, a hybrid Transformer-CNN architecture is adopted, coupling a Transformer-based encoder for global context modeling with a CNN-based decoder optimized for fine-grained anatomical structure restoration, thus enabling complementary exploitation of long-range dependencies and local texture details. Extensive evaluations on two publicly available ultrasound datasets demonstrate that the proposed model consistently outperforms existing methods, confirming its effectiveness and superiority.

</details>


### [60] [Synthetic Dataset Generation and Validation for Robotic Surgery Instrument Segmentation](https://arxiv.org/abs/2602.13844)
*Giorgio Chiesa,Rossella Borra,Vittorio Lauro,Sabrina De Cillis,Daniele Amparore,Cristian Fiori,Riccardo Renzulli,Marco Grangetto*

Main category: cs.CV

TL;DR: 本文介绍了一个生成和验证用于模拟机器人手术器械分割的合成数据集的全面工作流程。通过一个全自动的Python管道，对达芬奇机器人手臂进行了3D重建、动画处理，并生成了具有随机运动模式、光照变化和合成血液纹理的真实感标记视频序列。实验结果表明，样本中真实数据和合成数据的均衡比例有助于提高模型的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 为了克服真实手术数据集样本稀缺和多样性不足的问题，本文致力于构建一个完整的工作流程来生成和验证合成数据集，增强机器人手术器械的分割能力。

Method: 3D重建与动画制作技术通过Autodesk Maya完成，Python脚本自动化处理。融合了随机运动、光照变化和合成血液纹路的多样化内容，保证生成的视频高度真实。结合真实和合成样本来训练分割模型，通过实验比较模型在不同数据比例下的性能。

Result: 使用包含真实和合成数据样例混合的数据训练模型时，模型的泛化能力得到了显著提升，且数据的过量合成会导致模型陷入领域(shift)变化问题。

Conclusion: 本文提出的方法提供了一种可重复和可扩展的工具，支持手术计算机视觉领域的数据扩增、领域适应和基于模拟预训练等未来研究。

Abstract: This paper presents a comprehensive workflow for generating and validating a synthetic dataset designed for robotic surgery instrument segmentation. A 3D reconstruction of the Da Vinci robotic arms was refined and animated in Autodesk Maya through a fully automated Python-based pipeline capable of producing photorealistic, labeled video sequences. Each scene integrates randomized motion patterns, lighting variations, and synthetic blood textures to mimic intraoperative variability while preserving pixel-accurate ground truth masks. To validate the realism and effectiveness of the generated data, several segmentation models were trained under controlled ratios of real and synthetic data. Results demonstrate that a balanced composition of real and synthetic samples significantly improves model generalization compared to training on real data only, while excessive reliance on synthetic data introduces a measurable domain shift. The proposed framework provides a reproducible and scalable tool for surgical computer vision, supporting future research in data augmentation, domain adaptation, and simulation-based pretraining for robotic-assisted surgery. Data and code are available at https://github.com/EIDOSLAB/Sintetic-dataset-DaVinci.

</details>


### [61] [Cardiac Output Prediction from Echocardiograms: Self-Supervised Learning with Limited Data](https://arxiv.org/abs/2602.13846)
*Adson Duarte,Davide Vitturini,Emanuele Milillo,Andrea Bragagnolo,Carlo Alberto Barbano,Riccardo Renzulli,Michele Cannito,Federico Giacobbe,Francesco Bruno,Ovidio de Filippo,Fabrizio D'Ascenzo,Marco Grangetto*

Main category: cs.CV

TL;DR: 该研究提出了一种基于SimCLR的自监督学习预训练方法，用于从心脏四腔室超声视频预测心输出量（CO）。即使在数据稀缺的情况下，该方法也能够缓解过拟合并提升表示学习，展现出比PanEcho模型更好的性能。


<details>
  <summary>Details</summary>
Motivation: 心脏输出量（CO）是诊断和管理心血管疾病的关键参数，但准确测量需要进行右心导管插入术，这是一种侵入性且耗时的过程。因此，开发可靠的非侵入性替代方案，如基于超声心动图的方法，变得非常重要。

Method: 研究采用了一种基于SimCLR的自监督学习预训练方法。预训练阶段使用的是与下游任务相同的有限数据集，以在数据稀缺条件下验证自监督学习方法的有效性。研究通过自监督学习来缓解过拟合问题，通过预训练后的模型进行表示学习，从而改进CO的预测。

Result: 实验结果显示，与基于超过一百万个超声心动图检查的PanEcho模型相比，自监督学习方法能够在测试集上取得平均皮尔森相关系数为0.41的结果，表明该方法的有效性。

Conclusion: 本研究证明了即使在数据稀缺条件下，自监督学习方法也能够在非侵入性心脏输出量预测中发挥重要作用，提供了一种有可能改善心血管疾病诊断和管理的新途径。

Abstract: Cardiac Output (CO) is a key parameter in the diagnosis and management of cardiovascular diseases. However, its accurate measurement requires right-heart catheterization, an invasive and time-consuming procedure, motivating the development of reliable non-invasive alternatives using echocardiography. In this work, we propose a self-supervised learning (SSL) pretraining strategy based on SimCLR to improve CO prediction from apical four-chamber echocardiographic videos. The pretraining is performed using the same limited dataset available for the downstream task, demonstrating the potential of SSL even under data scarcity. Our results show that SSL mitigates overfitting and improves representation learning, achieving an average Pearson correlation of 0.41 on the test set and outperforming PanEcho, a model trained on over one million echocardiographic exams. Source code is available at https://github.com/EIDOSLAB/cardiac-output.

</details>


### [62] [Low-Pass Filtering Improves Behavioral Alignment of Vision Models](https://arxiv.org/abs/2602.13859)
*Max Wolff,Thomas Klein,Evgenia Rusak,Felix Wichmann,Wieland Brendel*

Main category: cs.CV

TL;DR: 研究发现，生成模型与判别模型之间的行为对齐差异主要是由生成模型中的降采样操作引起，这种操作类似于低通滤波，去除了高频空间信息。简单在测试时模糊图像可以显著提高模型的行为对齐度，达到新的SOTA。进一步通过直接优化滤波器进行对齐，展示了最优滤波器的表现。


<details>
  <summary>Details</summary>
Motivation: 本文动机在于探讨为什么生成模型在行为一致性上与判别模型有显著的改进，研究背后的主要问题是如何通过分析两种模型的特性改进机器视觉模型，使其更好地模拟人类视觉行为。

Method: 通过一系列受控实验，对比分析去除高分辨率信息前后模型行为对齐的变化情况（如CLIP模型进行降频）；直接优化低通滤波器以达到最优对齐；计算最优滤波器边界，证明为何高斯滤波器在这种背景下可能是最优的。

Result: 去除判别模型中的高分辨率信息与使用低通滤波器都能显著提高模型的行为对齐度，模糊处理比预训练模糊处理效果更好，达到新的SOTA。通过比较，自然人眼的对比度敏感函数与特定宽度的高斯滤波器曲线相似，这也体现了这种处理方式对模型效能的积极影响。

Conclusion: 本次研究揭示了人类视觉系统和机器视觉模型处理高频信息的差异。通过合理使用降采样或低通滤波器，可以显著提高模型的行为对齐程度，这为理解模型与人类视觉表现的细微差别提供了新的视角。

Abstract: Despite their impressive performance on computer vision benchmarks, Deep Neural Networks (DNNs) still fall short of adequately modeling human visual behavior, as measured by error consistency and shape bias. Recent work hypothesized that behavioral alignment can be drastically improved through \emph{generative} -- rather than \emph{discriminative} -- classifiers, with far-reaching implications for models of human vision.
  Here, we instead show that the increased alignment of generative models can be largely explained by a seemingly innocuous resizing operation in the generative model which effectively acts as a low-pass filter. In a series of controlled experiments, we show that removing high-frequency spatial information from discriminative models like CLIP drastically increases their behavioral alignment. Simply blurring images at test-time -- rather than training on blurred images -- achieves a new state-of-the-art score on the model-vs-human benchmark, halving the current alignment gap between DNNs and human observers. Furthermore, low-pass filters are likely optimal, which we demonstrate by directly optimizing filters for alignment. To contextualize the performance of optimal filters, we compute the frontier of all possible pareto-optimal solutions to the benchmark, which was formerly unknown.
  We explain our findings by observing that the frequency spectrum of optimal Gaussian filters roughly matches the spectrum of band-pass filters implemented by the human visual system. We show that the contrast sensitivity function, describing the inverse of the contrast threshold required for humans to detect a sinusoidal grating as a function of spatiotemporal frequency, is approximated well by Gaussian filters of the specific width that also maximizes error consistency.

</details>


### [63] [Parameter-Efficient Fine-Tuning of DINOv2 for Large-Scale Font Classification](https://arxiv.org/abs/2602.13889)
*Daniel Chen,Zaria Zinn,Marcus Lowe*

Main category: cs.CV

TL;DR: 本文提出了一种能够识别394种字体系列的系统，通过使用Low-Rank Adaptation (LoRA)微调DINOv2视觉变换器，实现了约86%的顶级准确率，同时仅训练了不到1%的参数量。一个合成数据集生成流水线用于大规模渲染Google Fonts，并包含多种增强方式，以产生适用于真实世界样例的泛化训练图像。该模型在推断时具有内置预处理以确保训练和推理一致，并作为HuggingFace推理端点进行部署，同时所有资源均作为开源发布。


<details>
  <summary>Details</summary>
Motivation: 本文旨在通过提出一个高效的字体分类系统，提高字体识别的准确性和效率，应用于图像中的字体识别任务。

Method: 本文采用了Fine-Tuning DINOv2 Vision Transformer的方法，并应用Low-Rank Adaptation (LoRA)技术进行微调，大幅减少了训练参数量。同时引入了一个大规模合成字体数据集生成方法，通过多种增强措施提高模型泛化能力。

Result: 本文提出的方法在394种字体的识别任务中达到了约86%的顶级准确率，且仅使用不到1%的模型参数进行训练。同时提出的合成数据集生成方法能生成适用于多种增强样本的训练数据，展现良好的泛化性能。

Conclusion: 本文提出了一个高效的字体分类系统，并通过公开模型、数据集和训练流程促进了该领域的开放和共享。

Abstract: We present a font classification system capable of identifying 394 font families from rendered text images. Our approach fine-tunes a DINOv2 Vision Transformer using Low-Rank Adaptation (LoRA), achieving approximately 86% top-1 accuracy while training fewer than 1% of the model's 87.2M parameters. We introduce a synthetic dataset generation pipeline that renders Google Fonts at scale with diverse augmentations including randomized colors, alignment, line wrapping, and Gaussian noise, producing training images that generalize to real-world typographic samples. The model incorporates built-in preprocessing to ensure consistency between training and inference, and is deployed as a HuggingFace Inference Endpoint. We release the model, dataset, and full training pipeline as open-source resources.

</details>


### [64] [MamaDino: A Hybrid Vision Model for Breast Cancer 3-Year Risk Prediction](https://arxiv.org/abs/2602.13930)
*Ruggiero Santeramo,Igor Zubarev,Florian Jug*

Main category: cs.CV

TL;DR: 研究成果展示了MamaDino在低分辨率乳腺X光片上实现了与领先系统Mirai相当的乳腺癌风险预测性能，通过融合卷积和变压器的归纳偏差以及显式建模对侧不对称性，即使在输入像素减少约13倍的情况下也达到了这一目标。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习模型在乳腺癌筛查中表现出色，但由于使用高分辨率输入和简单的融合方法受到了限制。研究人员试图通过结合卷积和基于变压器的归纳偏差，并显式建模对侧不对称性来解决这一问题。

Method: 研究人员开发了MamaDino，一种乳腺X光图意识的多视图注意力DINO模型，该模型融合了冻结的自监督DINOv3 ViT-S特征与512x512分辨率的可训练CNN编码器，并通过BilateralMixer聚合双侧乳腺信息，输出3年乳腺癌风险评分。

Result: MamaDino在内部和外部测试中都达到了与领先系统Mirai相当的乳腺癌风险预测性能，在内部测试中，乳腺级的性能与Mirai相当，且使用的输入像素减少约13倍。添加BilateralMixer后，在内部测试中的AUC提高到0.736（相对于0.713），在外部测试中的AUC提高到0.677（相对于0.666），并且在年龄、种族、扫描仪、肿瘤类型和级别方面都表现出一致的性能。

Conclusion: 研究表明，通过显式建模对侧不对称性和采用互补的归纳偏差，即使在使用较低分辨率的哺乳图像的情况下，也可以实现与目前最先进的方法MamaDino相当的预测性能，这表明在更结构化的方式下使用更少详细的信息可以实现顶尖的准确性。

Abstract: Breast cancer screening programmes increasingly seek to move from one-size-fits-all interval to risk-adapted and personalized strategies. Deep learning (DL) has enabled image-based risk models with stronger 1- to 5-year prediction than traditional clinical models, but leading systems (e.g., Mirai) typically use convolutional backbones, very high-resolution inputs (>1M pixels) and simple multi-view fusion, with limited explicit modelling of contralateral asymmetry.
  We hypothesised that combining complementary inductive biases (convolutional and transformer-based) with explicit contralateral asymmetry modelling would allow us to match state-of-the-art 3-year risk prediction performance even when operating on substantially lower-resolution mammograms, indicating that using less detailed images in a more structured way can recover state-of-the-art accuracy.
  We present MamaDino, a mammography-aware multi-view attentional DINO model. MamaDino fuses frozen self-supervised DINOv3 ViT-S features with a trainable CNN encoder at 512x512 resolution, and aggregates bilateral breast information via a BilateralMixer to output a 3-year breast cancer risk score. We train on 53,883 women from OPTIMAM (UK) and evaluate on matched 3-year case-control cohorts: an in-distribution test set from four screening sites and an external out-of-distribution cohort from an unseen site.
  At breast-level, MamaDino matches Mirai on both internal and external tests while using ~13x fewer input pixels. Adding the BilateralMixer improves discrimination to AUC 0.736 (vs 0.713) in-distribution and 0.677 (vs 0.666) out-of-distribution, with consistent performance across age, ethnicity, scanner, tumour type and grade. These findings demonstrate that explicit contralateral modelling and complementary inductive biases enable predictions that match Mirai, despite operating on substantially lower-resolution mammograms.

</details>


### [65] [Fusing Pixels and Genes: Spatially-Aware Learning in Computational Pathology](https://arxiv.org/abs/2602.13944)
*Minghao Han,Dingkang Yang,Linhao Qu,Zizhi Chen,Gang Li,Han Wang,Jiacong Wang,Lihua Zhang*

Main category: cs.CV

TL;DR: 本文提出了STAMP框架，结合空间转录组学和病理图像，通过自监督、基因引导的训练，增强了模型的鲁棒性和可移植性。


<details>
  <summary>Details</summary>
Motivation: 现有模型主要依赖视觉和语言模态，但缺乏分子特异性，导致表示瓶颈。因此，提出将空间基因表达谱整合到病理图像和转录组数据的联合嵌入中，以通过分子指导来增强表示能力。

Method: 提出了STAMP框架，采用自监督学习方法，结合空间转录组学数据，使用多尺度对比对齐和跨尺度补丁定位机制，实现病理图像和转录组数据的空间对齐。

Result: STAMP框架在六个数据集和四个下游任务上的测试中表现出色，证明了通过空间解决分子监督对多模态学习在计算病理学中的价值和必要性。

Conclusion: STAMP框架通过空间基因编码器和多尺度对比对齐机制，有效增强了病理图像和空间转录组学数据的表示能力，促进了计算病理学中多模态学习的发展。

Abstract: Recent years have witnessed remarkable progress in multimodal learning within computational pathology. Existing models primarily rely on vision and language modalities; however, language alone lacks molecular specificity and offers limited pathological supervision, leading to representational bottlenecks. In this paper, we propose STAMP, a Spatial Transcriptomics-Augmented Multimodal Pathology representation learning framework that integrates spatially-resolved gene expression profiles to enable molecule-guided joint embedding of pathology images and transcriptomic data. Our study shows that self-supervised, gene-guided training provides a robust and task-agnostic signal for learning pathology image representations. Incorporating spatial context and multi-scale information further enhances model performance and generalizability. To support this, we constructed SpaVis-6M, the largest Visium-based spatial transcriptomics dataset to date, and trained a spatially-aware gene encoder on this resource. Leveraging hierarchical multi-scale contrastive alignment and cross-scale patch localization mechanisms, STAMP effectively aligns spatial transcriptomics with pathology images, capturing spatial structure and molecular variation. We validate STAMP across six datasets and four downstream tasks, where it consistently achieves strong performance. These results highlight the value and necessity of integrating spatially resolved molecular supervision for advancing multimodal learning in computational pathology. The code is included in the supplementary materials. The pretrained weights and SpaVis-6M are available at: https://github.com/Hanminghao/STAMP.

</details>


### [66] [MarsRetrieval: Benchmarking Vision-Language Models for Planetary-Scale Geospatial Retrieval on Mars](https://arxiv.org/abs/2602.13961)
*Shuoyuan Wang,Yiran Wang,Hongxin Wei*

Main category: cs.CV

TL;DR: MarsRetrieval 提出了一种用于火星地表特征检索的基准测试，包括图像-文本配对检索、地形检索和全球地理定位任务，旨在评估多模态嵌入架构。实验证明，即使使用强大的基础模型，也难以捕捉特定领域的地质构造差异，强调了在行星环境中进行领域特定微调的重要性。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉任务基准主要集中在封闭集监督任务上，无法支持基于文本指导的地理空间发现。MarsRetrieval 的目的是填补这一空白，通过引入一个涵盖多种地理范围和地质起源的基准测试，促进针对火星探索的视觉-语言模型研究。

Method: MarsRetrieval 设计了一种统一的检索导向协议，用于评估对比式双塔编码器和生成式视觉-语言模型等多模态嵌入架构。该基准测试包括三个任务：图像-文本配对检索、地形检索和全球地理定位。

Result: MarsRetrieval 是一个多挑战性的基准测试。实验证明，即使强大的基础模型也无法很好地捕捉特定领域的地质构造差异。该测试还强调了在行星类别中进行领域特定微调的重要性。

Conclusion: 研究结果表明，领域特定微调是实现通用地理空间发现的关键，同时，MarsRetrieval 为未来的火星视觉-语言模型评估和研究提供了有价值的基准。

Abstract: Data-driven approaches like deep learning are rapidly advancing planetary science, particularly in Mars exploration. Despite recent progress, most existing benchmarks remain confined to closed-set supervised visual tasks and do not support text-guided retrieval for geospatial discovery. We introduce MarsRetrieval, a retrieval benchmark for evaluating vision-language models for Martian geospatial discovery. MarsRetrieval includes three tasks: (1) paired image-text retrieval, (2) landform retrieval, and (3) global geo-localization, covering multiple spatial scales and diverse geomorphic origins. We propose a unified retrieval-centric protocol to benchmark multimodal embedding architectures, including contrastive dual-tower encoders and generative vision-language models. Our evaluation shows MarsRetrieval is challenging: even strong foundation models often fail to capture domain-specific geomorphic distinctions. We further show that domain-specific fine-tuning is critical for generalizable geospatial discovery in planetary settings. Our code is available at https://github.com/ml-stat-Sustech/MarsRetrieval

</details>


### [67] [Elastic Diffusion Transformer](https://arxiv.org/abs/2602.13993)
*Jiangshan Wang,Zeqiang Lai,Jiarui Chen,Jiayi Guo,Hang Guo,Xiu Li,Xiangyu Yue,Chunchao Guo*

Main category: cs.CV

TL;DR: E-DiT 是一种针对扩散变换器 DiT 的自适应加速框架，能够显著提升效率同时保持生成质量。


<details>
  <summary>Details</summary>
Motivation: 鉴于扩散变换器 DiT 虽然具有出色的生成能力但计算成本高昂，以及之前加速方法如剪枝和蒸馏往往依赖固定计算能力、导致加速不足且质量下降的问题，提出了 E-DiT。

Method: E-DiT 框架通过为每个 DiT 块配备轻量级路由器来动态识别样本相关的稀疏性，并在推理时引入块级特征缓存机制来减少冗余计算，从而实现加速。

Result: 在 2D 图像 (Qwen-Image 和 FLUX) 和 3D 资产 (Hunyuan3D-3.0) 上进行的实验表明，E-DiT 能够实现约 2 倍的加速，且生成质量几乎没有损失。

Conclusion: E-DiT 通过自适应的方式提高了扩散变换器的效率，该方法将在提高生成模型性能和降低计算资源需求方面具有广泛应用潜力。

Abstract: Diffusion Transformers (DiT) have demonstrated remarkable generative capabilities but remain highly computationally expensive. Previous acceleration methods, such as pruning and distillation, typically rely on a fixed computational capacity, leading to insufficient acceleration and degraded generation quality. To address this limitation, we propose \textbf{Elastic Diffusion Transformer (E-DiT)}, an adaptive acceleration framework for DiT that effectively improves efficiency while maintaining generation quality. Specifically, we observe that the generative process of DiT exhibits substantial sparsity (i.e., some computations can be skipped with minimal impact on quality), and this sparsity varies significantly across samples. Motivated by this observation, E-DiT equips each DiT block with a lightweight router that dynamically identifies sample-dependent sparsity from the input latent. Each router adaptively determines whether the corresponding block can be skipped. If the block is not skipped, the router then predicts the optimal MLP width reduction ratio within the block. During inference, we further introduce a block-level feature caching mechanism that leverages router predictions to eliminate redundant computations in a training-free manner. Extensive experiments across 2D image (Qwen-Image and FLUX) and 3D asset (Hunyuan3D-3.0) demonstrate the effectiveness of E-DiT, achieving up to $\sim$2$\times$ speedup with negligible loss in generation quality. Code will be available at https://github.com/wangjiangshan0725/Elastic-DiT.

</details>


### [68] [Inject Where It Matters: Training-Free Spatially-Adaptive Identity Preservation for Text-to-Image Personalization](https://arxiv.org/abs/2602.13994)
*Guandong Li,Mengxia Ye*

Main category: cs.CV

TL;DR: SpatialID 提出了一种无需调优的时空自适应身份调节框架，通过空间掩码提取器将身份注入解耦到面部相关区域和非面部背景区域，并引入时空调度策略动态调整空间约束，从而提高了文本一致性、视觉一致性和图像质量。


<details>
  <summary>Details</summary>
Motivation: 现有的无调优方法通常采用空间均一视觉注入，导致身份特征污染背景和光照区域，影响文本准确性。针对这一问题，SpatialID 致力于在不消耗额外调优成本的情况下，实现身份与背景的分离注入。

Method: SpatialID 采用了一种训练-free 的时空自适应身份调节框架。具体而言，它通过交叉注意力响应衍生的空间掩码提取器实现面部相关区域和非面部背景区域的身份注入分离，并引入了时空调度策略来动态调整空间约束。

Result: 在 IBench 数据集上的实验表明，SpatialID 在文本一致性（CLIP-T：0.281）、视觉一致性（CLIP-I：0.827）和图像质量（IQ：0.523）方面达到了最先进的性能，显著减少了背景污染，同时保持了强大的身份保留。

Conclusion: 综上所述，SpatialID 通过创新的技术手段，有效解决了当前方法中的问题，展示了极高的实用价值和理论意义。

Abstract: Personalized text-to-image generation aims to integrate specific identities into arbitrary contexts. However, existing tuning-free methods typically employ Spatially Uniform Visual Injection, causing identity features to contaminate non-facial regions (e.g., backgrounds and lighting) and degrading text adherence. To address this without expensive fine-tuning, we propose SpatialID, a training-free spatially-adaptive identity modulation framework. SpatialID fundamentally decouples identity injection into face-relevant and context-free regions using a Spatial Mask Extractor derived from cross-attention responses. Furthermore, we introduce a Temporal-Spatial Scheduling strategy that dynamically adjusts spatial constraints - transitioning from Gaussian priors to attention-based masks and adaptive relaxation - to align with the diffusion generation dynamics. Extensive experiments on IBench demonstrate that SpatialID achieves state-of-the-art performance in text adherence (CLIP-T: 0.281), visual consistency (CLIP-I: 0.827), and image quality (IQ: 0.523), significantly eliminating background contamination while maintaining robust identity preservation.

</details>


### [69] [A Deployment-Friendly Foundational Framework for Efficient Computational Pathology](https://arxiv.org/abs/2602.14010)
*Yu Cai,Cheng Jin,Jiabo Ma,Fengtao Zhou,Yingxue Xu,Zhengrui Guo,Yihui Wang,Zhengyu Zhang,Ling Liang,Yonghao Tan,Pingcheng Dong,Du Cai,On Ki Tang,Chenglong Zhao,Xi Wang,Can Yang,Yali Xu,Jing Cui,Zhenhui Li,Ronald Cheong Kin Chan,Yueping Liu,Feng Gao,Xiuming Zhang,Li Liang,Hao Chen,Kwang-Ting Cheng*

Main category: cs.CV

TL;DR: LitePath 是一种专为减轻模型过参数化和斑块级别冗余而设计的轻量级框架，通过大幅减少模型参数量和 FLOPs，使其能够部署在低功耗边缘硬件上，从而实现快速、低成本且能源高效的病理图像分析。


<details>
  <summary>Details</summary>
Motivation: 现有的病理学基础模型（PFMs）虽然可以实现强大的泛化能力，但由于其巨大的计算成本，尤其是在处理超高分辨率全视野图像时，导致了临床使用的限制。因此，开发一种轻量级框架 LitePath 是为了降低模型的过参数化和斑块级别的冗余，减少计算资源的消耗，使得该框架能够在低功耗边缘设备上实现快速、高效、低成本的图像分析。

Method: LitePath 引入了一种名为 LiteFM 的紧凑型模型，该模型通过从 Virchow2、H-Optimus-1 和 UNI2 三大大规模 PFMs 中提取 1.9 亿个斑块的信息生成，同时结合了轻量级的 Adaptive Patch Selector (APS) 组件，用于特定任务的斑块选择。该框架通过显著减少模型参数量和 FLOPs，能够在具有挑战性的设备（如 NVIDIA Jetson Orin Nano Super）上执行快速分析。

Result: 相对于 Virchow2，LitePath 的参数量减少了 28 倍，FLOPs 降低了 403.5 倍。实验表明，LitePath 在 NVIDIA Jetson Orin Nano Super 设备上每小时可以处理 208 张样本，比 Virchow2 快了 104.5 倍，并且在每 3,000 个样本上的能耗仅为 0.36 kWh，比 Virchow2 在 RTX3090 GPU 上的能耗降低了 171 倍。LitePath 在 37 个跨不同器官和 26 项任务（26 项内部任务、9 项外部任务和 2 项前瞻性任务）的跨组织验证集中表现良好，平均 AUC 为 Virchow2 的 99.71%。该模型还提出了 Deployability Score (D-Score)，进一步评估了模型在准确性和效率之间的平衡，结果表明 LitePath 的 D-Score 为最高。

Conclusion: LitePath 通过显著减小模型参数和计算复杂度，在保持与先进 PFMs 相媲美的准确性的前提下，实现了快速、经济且能源高效的病理图像分析，具备出色的部署适应性。

Abstract: Pathology foundation models (PFMs) have enabled robust generalization in computational pathology through large-scale datasets and expansive architectures, but their substantial computational cost, particularly for gigapixel whole slide images, limits clinical accessibility and scalability. Here, we present LitePath, a deployment-friendly foundational framework designed to mitigate model over-parameterization and patch level redundancy. LitePath integrates LiteFM, a compact model distilled from three large PFMs (Virchow2, H-Optimus-1 and UNI2) using 190 million patches, and the Adaptive Patch Selector (APS), a lightweight component for task-specific patch selection. The framework reduces model parameters by 28x and lowers FLOPs by 403.5x relative to Virchow2, enabling deployment on low-power edge hardware such as the NVIDIA Jetson Orin Nano Super. On this device, LitePath processes 208 slides per hour, 104.5x faster than Virchow2, and consumes 0.36 kWh per 3,000 slides, 171x lower than Virchow2 on an RTX3090 GPU. We validated accuracy using 37 cohorts across four organs and 26 tasks (26 internal, 9 external, and 2 prospective), comprising 15,672 slides from 9,808 patients disjoint from the pretraining data. LitePath ranks second among 19 evaluated models and outperforms larger models including H-Optimus-1, mSTAR, UNI2 and GPFM, while retaining 99.71% of the AUC of Virchow2 on average. To quantify the balance between accuracy and efficiency, we propose the Deployability Score (D-Score), defined as the weighted geometric mean of normalized AUC and normalized FLOP, where LitePath achieves the highest value, surpassing Virchow2 by 10.64%. These results demonstrate that LitePath enables rapid, cost-effective and energy-efficient pathology image analysis on accessible hardware while maintaining accuracy comparable to state-of-the-art PFMs and reducing the carbon footprint of AI deployment.

</details>


### [70] [Flow4R: Unifying 4D Reconstruction and Tracking with Scene Flow](https://arxiv.org/abs/2602.14021)
*Shenhan Qian,Ganlin Zhang,Shangzhe Wu,Daniel Cremers*

Main category: cs.CV

TL;DR: Flow4R 提出了一种统一框架，通过两个视图输入使用视觉变换器预测像素级的3D点位置、场景流动、姿态权重和置信度，实现4D重建和跟踪任务的优异成果。


<details>
  <summary>Details</summary>
Motivation: 现有的方法往往将几何与运动分离：多视图重建方法假设静态场景，而动态跟踪框架依赖于明确的相机姿态估计或单独的运动模型。Flow4R框架通过将相机空间场景流动作为核心表示，将3D结构、物体运动和相机运动联系起来，旨在统一这两个领域。

Method: Flow4R 使用视觉变换器从两个视图输入中预测像素级的3D点位置、场景流动、姿态权重和置信度，采用流动中心的表示法，允许局部几何和双向运动在单次前向传递中对称地进行推断，无需明确的姿态回归器或包调整。

Result: Flow4R 在4D重建和跟踪任务上实现了最先进的性能，证明了流动中心表示法在空间时间场景理解中的有效性。

Conclusion: 该研究展示了通过整合几何和运动信息，可以更有效地解决动态3D场景重建和跟踪的问题，为后续研究提供了新的思路和技术框架。

Abstract: Reconstructing and tracking dynamic 3D scenes remains a fundamental challenge in computer vision. Existing approaches often decouple geometry from motion: multi-view reconstruction methods assume static scenes, while dynamic tracking frameworks rely on explicit camera pose estimation or separate motion models. We propose Flow4R, a unified framework that treats camera-space scene flow as the central representation linking 3D structure, object motion, and camera motion. Flow4R predicts a minimal per-pixel property set-3D point position, scene flow, pose weight, and confidence-from two-view inputs using a Vision Transformer. This flow-centric formulation allows local geometry and bidirectional motion to be inferred symmetrically with a shared decoder in a single forward pass, without requiring explicit pose regressors or bundle adjustment. Trained jointly on static and dynamic datasets, Flow4R achieves state-of-the-art performance on 4D reconstruction and tracking tasks, demonstrating the effectiveness of the flow-central representation for spatiotemporal scene understanding.

</details>


### [71] [Train Short, Inference Long: Training-free Horizon Extension for Autoregressive Video Generation](https://arxiv.org/abs/2602.14027)
*Jia Li,Xiaomeng Fu,Xurui Peng,Weifeng Chen,Youwei Zheng,Tianyu Zhao,Jiexi Wang,Fangmin Chen,Xing Wang,Hayden Kwok-Hay So*

Main category: cs.CV

TL;DR: 提出了一种名为FLEX的推理时框架，通过引入频率感知的RoPE调整、反相噪声采样和推断时注意力陷阱，解决了3D位置嵌入的频谱偏差和采样中缺乏动态先验的问题，显著提高了超长视频生成的质量。


<details>
  <summary>Details</summary>
Motivation: 当前的自回归视频扩散模型在长时间的视频生成中存在严重的外推失败问题，导致了时间上的显著降解。本文的主要动机是通过改进模型方法来解决这一问题。

Method: 提出了一种名为FLEX的方法，它在推理时间进行训练，引入了频率感知的RoPE调整，用于自适应地插值得到低频分量，同时外推高频分量，从而保持多尺度的时间可区分性。此外，还结合了反相噪声采样和推理时注意力陷阱来注入高频动态先验和锁定全局结构。

Result: FLEX在VBench上的广泛评估显示，其在6倍外推（30秒时长）时显著优于最先进的模型，并与长视频微调基线在12倍放大（60秒时长）时具有相同的性能。此外，FLEX作为即插即用增强，可以无缝集成到现有的推理管道中，支持长达4分钟的视频生成。

Conclusion: FLEX有效地推动了模型如LongLive的生成极限，能够支持一致且动态的视频合成。

Abstract: Autoregressive video diffusion models have emerged as a scalable paradigm for long video generation. However, they often suffer from severe extrapolation failure, where rapid error accumulation leads to significant temporal degradation when extending beyond training horizons. We identify that this failure primarily stems from the \textit{spectral bias} of 3D positional embeddings and the lack of \textit{dynamic priors} in noise sampling. To address these issues, we propose \textbf{FLEX} (\textbf{F}requency-aware \textbf{L}ength \textbf{EX}tension), a training-free inference-time framework that bridges the gap between short-term training and long-term inference. FLEX introduces Frequency-aware RoPE Modulation to adaptively interpolate under-trained low-frequency components while extrapolating high-frequency ones to preserve multi-scale temporal discriminability. This is integrated with Antiphase Noise Sampling (ANS) to inject high-frequency dynamic priors and Inference-only Attention Sink to anchor global structure. Extensive evaluations on VBench demonstrate that FLEX significantly outperforms state-of-the-art models at $6\times$ extrapolation (30s duration) and matches the performance of long-video fine-tuned baselines at $12\times$ scale (60s duration). As a plug-and-play augmentation, FLEX seamlessly integrates into existing inference pipelines for horizon extension. It effectively pushes the generation limits of models such as LongLive, supporting consistent and dynamic video synthesis at a 4-minute scale. Project page is available at \href{https://ga-lee.github.io/FLEX_demo}{https://ga-lee.github.io/FLEX}.

</details>


### [72] [BitDance: Scaling Autoregressive Generative Models with Binary Tokens](https://arxiv.org/abs/2602.14041)
*Yuang Ai,Jiaming Han,Shaobin Zhuang,Weijia Mao,Xuefeng Hu,Ziyan Yang,Zhenheng Yang,Huaibo Huang,Xiangyu Yue,Hao Chen*

Main category: cs.CV

TL;DR: BitDance 提出了一种新颖的自回归图像生成器，它通过预测二进制视觉标记而非码本索引来生成高分辨率、逼真的图像。BitDance 通过使用连续空间扩散和并行预测多个标记的方法，实现了高效的推理和较低的参数量。


<details>
  <summary>Details</summary>
Motivation: 现有的自回归模型依赖于预测码本索引，这在高分辨率图像生成时会导致信息损失和低表达多样性。BitDance 通过预测二进制视觉标记，利用高熵二进制潜在变量，每个标记可表示高达 $2^{256}$ 的状态，从而提供了更为紧凑且高表达力的离散表示。

Method: BitDance 提出了一种二进制扩散头来替代标准分类中的索引预测，并引入了一种新的并行预测多个标记的解码方法，称为「next-patch diffusion」，以实现快速推理。

Result: 在 ImageNet 256x256 数据集上，BitDance 达到了 FID 为 1.24 的最优效果。相较于使用 1.4B 参数的领先并行自回归模型，BitDance 使用 260M 参数实现了 8.7 倍的速度提升。在生成 1024x1024 的图像时，比之前的自回归模型快了 30 倍。

Conclusion: BitDance 通过创新的技术和方法，显著提升了自回归图像生成的效率和表达力，在多个基准测试和应用中展现了优异的性能。

Abstract: We present BitDance, a scalable autoregressive (AR) image generator that predicts binary visual tokens instead of codebook indices. With high-entropy binary latents, BitDance lets each token represent up to $2^{256}$ states, yielding a compact yet highly expressive discrete representation. Sampling from such a huge token space is difficult with standard classification. To resolve this, BitDance uses a binary diffusion head: instead of predicting an index with softmax, it employs continuous-space diffusion to generate the binary tokens. Furthermore, we propose next-patch diffusion, a new decoding method that predicts multiple tokens in parallel with high accuracy, greatly speeding up inference. On ImageNet 256x256, BitDance achieves an FID of 1.24, the best among AR models. With next-patch diffusion, BitDance beats state-of-the-art parallel AR models that use 1.4B parameters, while using 5.4x fewer parameters (260M) and achieving 8.7x speedup. For text-to-image generation, BitDance trains on large-scale multimodal tokens and generates high-resolution, photorealistic images efficiently, showing strong performance and favorable scaling. When generating 1024x1024 images, BitDance achieves a speedup of over 30x compared to prior AR models. We release code and models to facilitate further research on AR foundation models. Code and models are available at: https://github.com/shallowdream204/BitDance.

</details>


### [73] [Restoration Adaptation for Semantic Segmentation on Low Quality Images](https://arxiv.org/abs/2602.14042)
*Kai Guan,Rongyuan Wu,Shuai Li,Wentao Zhu,Wenjun Zeng,Lei Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种称为RASS的解决方案，通过将语义图像恢复集成到语义分割过程中，以实现低质量（LQ）图像的高质量语义分割。研究通过构造高质量注释的现实世界LQ图像分割数据集，并在合成和现实世界的LQ基准上进行全面实验，展示了该框架的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的图像恢复和语义分割模型分别在各自任务上表现出色，但当处理低质量图像时，它们的效果都会受到影响。因此，需要一种既能提升图像质量又能保持语义信息的方法。

Method: 首先提出了一种称为SCR（Semantic-Constrained Restoration）的模型，该模型通过将分割先验注入到恢复模型中来促进语义一致的图像重建，具体来说是通过对齐其交叉注意力图与分割掩码。然后，RASS通过基于LoRA的模块合并和任务特定的微调将语义恢复知识转移到分割中，提高模型对低质量图像的鲁棒性。

Result: 实验结果表明，使用SCR和RASS进行分割和恢复任务时，该方法显著优于现有最先进的方法。

Conclusion: 该研究通过提出RASS，展现了集成图像恢复和语义分割的方法在处理低质量图像方面具有显著优势，这将有助于提高针对现实世界低质量数据的计算机视觉模型的性能。

Abstract: In real-world scenarios, the performance of semantic segmentation often deteriorates when processing low-quality (LQ) images, which may lack clear semantic structures and high-frequency details. Although image restoration techniques offer a promising direction for enhancing degraded visual content, conventional real-world image restoration (Real-IR) models primarily focus on pixel-level fidelity and often fail to recover task-relevant semantic cues, limiting their effectiveness when directly applied to downstream vision tasks. Conversely, existing segmentation models trained on high-quality data lack robustness under real-world degradations. In this paper, we propose Restoration Adaptation for Semantic Segmentation (RASS), which effectively integrates semantic image restoration into the segmentation process, enabling high-quality semantic segmentation on the LQ images directly. Specifically, we first propose a Semantic-Constrained Restoration (SCR) model, which injects segmentation priors into the restoration model by aligning its cross-attention maps with segmentation masks, encouraging semantically faithful image reconstruction. Then, RASS transfers semantic restoration knowledge into segmentation through LoRA-based module merging and task-specific fine-tuning, thereby enhancing the model's robustness to LQ images. To validate the effectiveness of our framework, we construct a real-world LQ image segmentation dataset with high-quality annotations, and conduct extensive experiments on both synthetic and real-world LQ benchmarks. The results show that SCR and RASS significantly outperform state-of-the-art methods in segmentation and restoration tasks. Code, models, and datasets will be available at https://github.com/Ka1Guan/RASS.git.

</details>


### [74] [CoCoEdit: Content-Consistent Image Editing via Region Regularized Reinforcement Learning](https://arxiv.org/abs/2602.14068)
*Yuhui Wu,Chenxi Xie,Ruibin Li,Liyi Chen,Qiaosi Yi,Lei Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种基于区域正则化强化学习的后训练框架CoCoEdit，通过改进现有图像编辑数据集并引入像素级相似度奖励和区域正则化器，提升了图像编辑的质量和内容一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的图像编辑模型主要关注目标对象和区域的编辑效果，往往会无意中改变非编辑区域，影响整体效果。

Method: 作者提出了一种新的后训练框架CoCoEdit，包括数据预处理、增强学习机制以及区域正则化方法，提升模型的编辑质量和内容一致性。

Result: 通过应用CoCoEdit到Qwen-Image-Edit和FLUX-Kontext上，实验结果表明，该模型不仅在编辑分数上与最先进的模型相当，而且在内容一致性方面有显著改善。

Conclusion: CoCoEdit能够在保留原始图像内容的同时进行高质量的图像编辑。

Abstract: Image editing has achieved impressive results with the development of large-scale generative models. However, existing models mainly focus on the editing effects of intended objects and regions, often leading to unwanted changes in unintended regions. We present a post-training framework for Content-Consistent Editing (CoCoEdit) via region regularized reinforcement learning. We first augment existing editing datasets with refined instructions and masks, from which 40K diverse and high quality samples are curated as training set. We then introduce a pixel-level similarity reward to complement MLLM-based rewards, enabling models to ensure both editing quality and content consistency during the editing process. To overcome the spatial-agnostic nature of the rewards, we propose a region-based regularizer, aiming to preserve non-edited regions for high-reward samples while encouraging editing effects for low-reward samples. For evaluation, we annotate editing masks for GEdit-Bench and ImgEdit-Bench, introducing pixel-level similarity metrics to measure content consistency and editing quality. Applying CoCoEdit to Qwen-Image-Edit and FLUX-Kontext, we achieve not only competitive editing scores with state-of-the-art models, but also significantly better content consistency, measured by PSNR/SSIM metrics and human subjective ratings.

</details>


### [75] [ForgeryVCR: Visual-Centric Reasoning via Efficient Forensic Tools in MLLMs for Image Forgery Detection and Localization](https://arxiv.org/abs/2602.14098)
*Youqi Wang,Shen Chen,Haowei Wang,Rongxuan Peng,Taiping Yao,Shunquan Tan,Changsheng Chen,Bin Li,Shouhong Ding*

Main category: cs.CV

TL;DR: ForgeryVCR 通过引入视觉中心推理和策略性工具学习框架，显著提高了图像篡改检测和定位的性能，尤其在处理低级别不可见篡改方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有基于文本中心推理的多模态大型语言模型在处理低级别不可见篡改时容易产生幻觉，因此需要一个能够有效实现在不可见篡改痕迹上转化为显视中间步骤的框架，以解决模型的幻觉问题。

Method: ForgeryVCR 框架采用了视觉中心推理，并提出了一种策略性工具学习后训练方案，结合监督微调（SFT）和基于工具适用性奖励的强化学习（RL），使模型能够执行多视图推理路径。

Result: 实验结果显示， ForgeryVCR 在检测和定位任务中达到了最先进的性能，具有更强的泛化能力和鲁棒性，且工具冗余量少。

Conclusion: 该研究证明了通过结合视觉中心推理和策略性工具学习，可以显著提高图像篡改检测和定位任务中的模型性能，尤其在处理低级别不可见篡改方面更具有优势。

Abstract: Existing Multimodal Large Language Models (MLLMs) for image forgery detection and localization predominantly operate under a text-centric Chain-of-Thought (CoT) paradigm. However, forcing these models to textually characterize imperceptible low-level tampering traces inevitably leads to hallucinations, as linguistic modalities are insufficient to capture such fine-grained pixel-level inconsistencies. To overcome this, we propose ForgeryVCR, a framework that incorporates a forensic toolbox to materialize imperceptible traces into explicit visual intermediates via Visual-Centric Reasoning. To enable efficient tool utilization, we introduce a Strategic Tool Learning post-training paradigm, encompassing gain-driven trajectory construction for Supervised Fine-Tuning (SFT) and subsequent Reinforcement Learning (RL) optimization guided by a tool utility reward. This paradigm empowers the MLLM to act as a proactive decision-maker, learning to spontaneously invoke multi-view reasoning paths including local zoom-in for fine-grained inspection and the analysis of invisible inconsistencies in compression history, noise residuals, and frequency domains. Extensive experiments reveal that ForgeryVCR achieves state-of-the-art (SOTA) performance in both detection and localization tasks, demonstrating superior generalization and robustness with minimal tool redundancy. The project page is available at https://youqiwong.github.io/projects/ForgeryVCR/.

</details>


### [76] [GeoFusionLRM: Geometry-Aware Self-Correction for Consistent 3D Reconstruction](https://arxiv.org/abs/2602.14119)
*Ahmet Burak Yildirim,Tuna Saygin,Duygu Ceylan,Aysegul Dundar*

Main category: cs.CV

TL;DR: GeoFusionLRM 提出了一种几何感知的自修正框架，通过反馈几何线索改进单图像 3D 重建的结构准确性，进而增强几何、法线连贯性和重建保真度。


<details>
  <summary>Details</summary>
Motivation: 现有单图像 3D 重建方法常出现几何不一致和细节对齐问题，影响重建质量。GeoFusionLRM 旨在通过自我纠正机制解决这些问题。

Method: 采用专有的变压器和融合模块，将模型的法线和深度预测反馈到重建过程中，自适应地纠正错误并保持与条件图像的一致性。

Result: GeoFusionLRM 在几何锐度、法线连贯性和重建保真度方面显著优于最先进的 LRM 基线方法。

Conclusion: GeoFusionLRM 通过自适应的几何线索反馈机制改进了单图像 3D 重建的质量，为后续研究提供了新的思路。

Abstract: Single-image 3D reconstruction with large reconstruction models (LRMs) has advanced rapidly, yet reconstructions often exhibit geometric inconsistencies and misaligned details that limit fidelity. We introduce GeoFusionLRM, a geometry-aware self-correction framework that leverages the model's own normal and depth predictions to refine structural accuracy. Unlike prior approaches that rely solely on features extracted from the input image, GeoFusionLRM feeds back geometric cues through a dedicated transformer and fusion module, enabling the model to correct errors and enforce consistency with the conditioning image. This design improves the alignment between the reconstructed mesh and the input views without additional supervision or external signals. Extensive experiments demonstrate that GeoFusionLRM achieves sharper geometry, more consistent normals, and higher fidelity than state-of-the-art LRM baselines.

</details>


### [77] [EgoSound: Benchmarking Sound Understanding in Egocentric Videos](https://arxiv.org/abs/2602.14122)
*Bingwen Zhu,Yuqian Fu,Qiaole Dong,Guolei Sun,Tianwen Qian,Yuzheng Wu,Danda Pani Paudel,Xiangyang Xue,Yanwei Fu*

Main category: cs.CV

TL;DR: EgoSound 是首个系统评估 MLLMs 在非视觉感知能力的基准，包含丰富的视听数据和多任务类型，揭示了现有模型在细腻空间和因果理解上的局限。


<details>
  <summary>Details</summary>
Motivation: 现有的 MLLMs 虽然在视觉语言理解方面取得了巨大进步，但人类感知本质上是多感官的，限于单一模态的方法无法全面评测模型在非视觉感知上的能力。因此，提出 EgoSound 作为评估多感官环境下的非视觉感知能力的基准，促使相关技术的发展。

Method: EgoSound 通过多阶段自动化生成管道构建，利用 Ego4D 和 EgoBlind 的数据资源，涵盖了图像和声音的信息，定义了七个任务分类，通过详细的实验评估当前 MLLMs 的表现。

Result: 实验结果显示，当前最先进 MLLMs 在声音理解上展现出初步的推理能力，但在细腻的空间和因果关系理解上仍存在局限。

Conclusion: EgoSound 为多感官环境下非视觉感知能力的评测和提升提供了挑战性的基础，推动了多模态大语言模型向真正的多感官智能发展。

Abstract: Multimodal Large Language Models (MLLMs) have recently achieved remarkable progress in vision-language understanding. Yet, human perception is inherently multisensory, integrating sight, sound, and motion to reason about the world. Among these modalities, sound provides indispensable cues about spatial layout, off-screen events, and causal interactions, particularly in egocentric settings where auditory and visual signals are tightly coupled. To this end, we introduce EgoSound, the first benchmark designed to systematically evaluate egocentric sound understanding in MLLMs. EgoSound unifies data from Ego4D and EgoBlind, encompassing both sighted and sound-dependent experiences. It defines a seven-task taxonomy spanning intrinsic sound perception, spatial localization, causal inference, and cross-modal reasoning. Constructed through a multi-stage auto-generative pipeline, EgoSound contains 7315 validated QA pairs across 900 videos. Comprehensive experiments on nine state-of-the-art MLLMs reveal that current models exhibit emerging auditory reasoning abilities but remain limited in fine-grained spatial and causal understanding. EgoSound establishes a challenging foundation for advancing multisensory egocentric intelligence, bridging the gap between seeing and truly hearing the world.

</details>


### [78] [DenseMLLM: Standard Multimodal LLMs are Intrinsic Dense Predictors](https://arxiv.org/abs/2602.14134)
*Yi Li,Hongze Shen,Lexiang Tang,Xin Li,Xinpeng Ding,Yinsong Liu,Deqiang Jiang,Xing Sun,Xiaomeng Li*

Main category: cs.CV

TL;DR: 该研究提出了DenseMLLM模型，该模型基于标准构架并通过引入多标签和任务的视觉令牌监督策略，使大语言模型能够执行密集预测，展示了标准、通用的大语言模型在不需要特殊架构的情况下进行密集感知的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前的强视觉理解模型在密集预测任务上通常需要复杂的、特定任务的解码器和定制方案，这增加了模型的复杂性并偏离了专门设计的通用模型的目标。本文挑战了这一范式，提出了一种方法，使得标准的MLLM能够执行密集预测，而不需要进一步的任务特定的解码器，以此克服现有模型的局限性。

Method: 提出的DenseMLLM模型基于标准构架，采用一种新颖的视觉令牌监督策略，以支持不同标签和任务的多标签。该模型没有使用特定的任务解码器，而是通过监督策略整合不同任务的信息。

Result: 尽管是一种简约设计，DenseMLLM在多种密集预测和视觉语言基准测试上实现了具有竞争力的性能，展示了标准通用的大语言模型在密集感知中的潜力。

Conclusion: 该研究表明，通过适当的监督策略改进和利用标准MLLM，可以有效进行密集感知任务，无需进行专门的模型设计或添加特定任务的解码器，这对未来的大语言模型应用具有重要意义。

Abstract: Multimodal Large Language Models (MLLMs) have demonstrated exceptional capabilities in high-level visual understanding. However, extending these models to fine-grained dense prediction tasks, such as semantic segmentation and depth estimation, typically necessitates the incorporation of complex, task-specific decoders and other customizations. This architectural fragmentation increases model complexity and deviates from the generalist design of MLLMs, ultimately limiting their practicality. In this work, we challenge this paradigm by accommodating standard MLLMs to perform dense predictions without requiring additional task-specific decoders. The proposed model is called DenseMLLM, grounded in the standard architecture with a novel vision token supervision strategy for multiple labels and tasks. Despite its minimalist design, our model achieves highly competitive performance across a wide range of dense prediction and vision-language benchmarks, demonstrating that a standard, general-purpose MLLM can effectively support dense perception without architectural specialization.

</details>


### [79] [Detection of On-Ground Chestnuts Using Artificial Intelligence Toward Automated Picking](https://arxiv.org/abs/2602.14140)
*Kaixuan Fang,Yuzhen Lu,Xinyang Mu*

Main category: cs.CV

TL;DR: 本研究收集了包含6524颗栗子的319张果园地面图像，系统评估了29种先进的实时目标检测器，发现YOLOv12m在mAP@0.5指标上表现最佳，为95.1%，YOLO模型整体在检测准确性和推理速度上优于RT-DETR模型，更适合车载部署。


<details>
  <summary>Details</summary>
Motivation: 针对传统栗子采摘技术的成本高、选择性差且容易损坏栗子的问题，本研究旨在开发低成本的视觉引导自动化采摘技术，特别关注在复杂环境中如何准确检测栗子。

Method: 研究人员收集了319张包含6524颗栗子的果园地面图像，共测试了29种先进的实时目标检测器，其中包括YOLO和RT-DETR系列的多种模型。

Result: 实验结果显示，YOLOv12m模型在mAP@0.5指标上表现最佳，达到95.1%；RT-DETRv2-R101是RT-DETR系列中最准确的版本，mAP@0.5为91.1%；YOLOv11x模型在mAP@[0.5:0.95]区间内表现最佳，达到80.1%。所有模型在实时栗子检测方面都表现出显著的潜力。

Conclusion: 研究证明YOLO模型在检测准确性和推理速度方面都优于RT-DETR模型，更适合用于栗子采摘的车载部署，而研究所提供的数据集和软件程序已公开发布。

Abstract: Traditional mechanized chestnut harvesting is too costly for small producers, non-selective, and prone to damaging nuts. Accurate, reliable detection of chestnuts on the orchard floor is crucial for developing low-cost, vision-guided automated harvesting technology. However, developing a reliable chestnut detection system faces challenges in complex environments with shading, varying natural light conditions, and interference from weeds, fallen leaves, stones, and other foreign on-ground objects, which have remained unaddressed. This study collected 319 images of chestnuts on the orchard floor, containing 6524 annotated chestnuts. A comprehensive set of 29 state-of-the-art real-time object detectors, including 14 in the YOLO (v11-13) and 15 in the RT-DETR (v1-v4) families at varied model scales, was systematically evaluated through replicated modeling experiments for chestnut detection. Experimental results show that the YOLOv12m model achieves the best mAP@0.5 of 95.1% among all the evaluated models, while the RT-DETRv2-R101 was the most accurate variant among RT-DETR models, with mAP@0.5 of 91.1%. In terms of mAP@[0.5:0.95], the YOLOv11x model achieved the best accuracy of 80.1%. All models demonstrate significant potential for real-time chestnut detection, and YOLO models outperformed RT-DETR models in terms of both detection accuracy and inference, making them better suited for on-board deployment. Both the dataset and software programs in this study have been made publicly available at https://github.com/AgFood-Sensing-and-Intelligence-Lab/ChestnutDetection.

</details>


### [80] [LaViDa-R1: Advancing Reasoning for Unified Multimodal Diffusion Language Models](https://arxiv.org/abs/2602.14147)
*Shufan Li,Yuchen Zhu,Jiuxiang Gu,Kangning Liu,Zhe Lin,Yongxin Chen,Molei Tao,Aditya Grover,Jason Kuen*

Main category: cs.CV

TL;DR: LaViDa-R1是一种统一的后训练框架，结合了监督微调和多任务强化学习，用于多模态推理。该模型在多种多模态任务上表现出色。


<details>
  <summary>Details</summary>
Motivation: 扩散语言模型（dLLMs）为多模态理解与生成任务提供了一种有前景的替代方法。现有的工作主要通过任务特定的强化学习构建多模态推理模型，而LaViDa-R1旨在提供一种统一的方式整合多种多模态任务。

Method: LaViDa-R1采用了新型的统一后训练框架，结合了监督微调（SFT）和多任务强化学习（RL）。模型还采用了回答强制、树搜索和互补似然估计等创新训练技术。

Result: 实验结果表明，LaViDa-R1在视觉数学推理、关系密集型定位和图像编辑等多种多模态任务上表现优异。

Conclusion: 该研究提出了LaViDa-R1，一种针对多模态推理设计的统一后训练扩散语言模型，并展示了其在多个任务上的强大效果。

Abstract: Diffusion language models (dLLMs) recently emerged as a promising alternative to auto-regressive LLMs. The latest works further extended it to multimodal understanding and generation tasks. In this work, we propose LaViDa-R1, a multimodal, general-purpose reasoning dLLM. Unlike existing works that build reasoning dLLMs through task-specific reinforcement learning, LaViDa-R1 incorporates diverse multimodal understanding and generation tasks in a unified manner. In particular, LaViDa-R1 is built with a novel unified post-training framework that seamlessly integrates supervised finetuning (SFT) and multi-task reinforcement learning (RL). It employs several novel training techniques, including answer-forcing, tree search, and complementary likelihood estimation, to enhance effectiveness and scalability. Extensive experiments demonstrate LaViDa-R1's strong performance on a wide range of multimodal tasks, including visual math reasoning, reason-intensive grounding, and image editing.

</details>


### [81] [ARport: An Augmented Reality System for Markerless Image-Guided Port Placement in Robotic Surgery](https://arxiv.org/abs/2602.14153)
*Zheng Han,Zixin Yang,Yonghao Long,Lin Zhang,Peter Kazanzides,Qi Dou*

Main category: cs.CV

TL;DR: ARport 是一种基于增强现实技术的系统，能够将预定穿刺布局自动映射到患者体表，提供直观的空间指导，支持手术准备，并在实际人体或仿生体上实现了虚拟计划与实际解剖的精准对应。


<details>
  <summary>Details</summary>
Motivation: 为了在自动辅助手术中弥补预手术规划与手术执行之间的差距，提议使用ARport增强现实系统来实现穿刺布局的精准映射，以改善视觉访问和器械操作。

Method: 该系统基于光学透视头戴显示器进行实现，通过RGB、深度和姿态数据重构手术现场，利用基础模型提取病人体表，并通过表面标志物自由注册方法将预手术解剖模型与提取的病人体表对齐，使手术现场能够实时可视化预定穿刺布局。

Result: 在人体仿生体的全尺寸实验中，ARport成功地将预定穿刺位置映射到物理模拟体，并实现了虚拟计划与实际生理特征之间的一致空间对应。

Conclusion: ARport提供了无需标记和硬件要求最低的解决方案，可在患者体表直接可视化预手术穿刺布局，有助于高效手术安装并显示出在常规临床工作流程中无缝集成的潜力。

Abstract: Purpose: Precise port placement is a critical step in robot-assisted surgery, where port configuration influences both visual access to the operative field and instrument maneuverability. To bridge the gap between preoperative planning and intraoperative execution, we present ARport, an augmented reality (AR) system that automatically maps pre-planned trocar layouts onto the patient's body surface, providing intuitive spatial guidance during surgical preparation. Methods: ARport, implemented on an optical see-through head-mounted display (OST-HMD), operates without any external sensors or markers, simplifying setup and enhancing workflow integration. It reconstructs the operative scene from RGB, depth, and pose data captured by the OST-HMD, extracts the patient's body surface using a foundation model, and performs surface-based markerless registration to align preoperative anatomical models to the extracted patient's body surface, enabling in-situ visualization of planned trocar layouts. A demonstration video illustrating the overall workflow is available online. Results: In full-scale human-phantom experiments, ARport accurately overlaid pre-planned trocar sites onto the physical phantom, achieving consistent spatial correspondence between virtual plans and real anatomy. Conclusion: ARport provides a fully marker-free and hardware-minimal solution for visualizing preoperative trocar plans directly on the patient's body surface. The system facilitates efficient intraoperative setup and demonstrates potential for seamless integration into routine clinical workflows.

</details>


### [82] [When Test-Time Guidance Is Enough: Fast Image and Video Editing with Diffusion Guidance](https://arxiv.org/abs/2602.14157)
*Ahmed Ghorbel,Badr Moufad,Navid Bagheri Shouraki,Alain Oliviero Durmus,Thomas Hirtz,Eric Moulines,Jimmy Olsson,Yazid Janati*

Main category: cs.CV

TL;DR: 本研究通过理论分析VJP-free近似方法，并在大规模图像和视频编辑基准上进行扩展实验，证明了测试时的指导可以与基于训练的方法相媲美，甚至在某些情况下超越。


<details>
  <summary>Details</summary>
Motivation: 近年来，针对图像和视频编辑的研究已有了很大的进展，尤其是在扩散和流模型的测试时指导方面。尽管已有方法取得了一定的成果，但它们依赖昂贵的向量-雅可比乘积计算来近似不可处理的指导项，这限制了这些方法的实际应用。因此， researchers用Moufad等人的工作作为基础，对该理论做了进一步研究。

Method: 该研究主要基于Moufad等人2025年的VJP-free近似方法，对其实现和性能进行了深入的理论分析，并在大规模的图像和视频编辑基准上进行了实测验证。

Result: 研究表明，仅凭测试时的指导就可以达到与基于训练的方法相匹敌，甚至有些场景下还超过了这些方法。

Conclusion: 该工作为图像和视频编辑中的测试时指导提供了坚实的理论支持，并展示了其实用性和优越性。

Abstract: Text-driven image and video editing can be naturally cast as inpainting problems, where masked regions are reconstructed to remain consistent with both the observed content and the editing prompt. Recent advances in test-time guidance for diffusion and flow models provide a principled framework for this task; however, existing methods rely on costly vector--Jacobian product (VJP) computations to approximate the intractable guidance term, limiting their practical applicability. Building upon the recent work of Moufad et al. (2025), we provide theoretical insights into their VJP-free approximation and substantially extend their empirical evaluation to large-scale image and video editing benchmarks. Our results demonstrate that test-time guidance alone can achieve performance comparable to, and in some cases surpass, training-based methods.

</details>


### [83] [Towards Spatial Transcriptomics-driven Pathology Foundation Models](https://arxiv.org/abs/2602.14177)
*Konstantin Hemker,Andrew H. Song,Cristina Almagro-Pérez,Guillaume Jaume,Sophia J. Wagner,Anurag Vaidya,Nikola Simidjievski,Mateja Jamnik,Faisal Mahmood*

Main category: cs.CV

TL;DR: SEAL 是一种细调方法，将局部分子信息融入病理视觉编码器，从而提高病理学基础模型的下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 通过将形态学与局部分子表达信息结合，改善传统的病理学表示方法，进一步实现跨模态的功能。

Method: SEAL采用了一种参数高效的视觉-组学自监督学习框架，通过大规模的配对基因表达斑点-组织区域样本进行训练，改进了病理学基础模型在肿瘤和正常样本上的表现。

Result: SEAL在多个滑片级和斑块级的下游任务中表现出色，尤其是在分子状态、通路活性和治疗反应预测方面超过了纯视觉和ST预测基线。

Conclusion: SEAL 提供了一种跨模态功能的应用基础，通过局部分子监督增强了现有的模型，展示了提高视觉表示和扩展其跨模态用途的有效性。

Abstract: Spatial transcriptomics (ST) provides spatially resolved measurements of gene expression, enabling characterization of the molecular landscape of human tissue beyond histological assessment as well as localized readouts that can be aligned with morphology. Concurrently, the success of multimodal foundation models that integrate vision with complementary modalities suggests that morphomolecular coupling between local expression and morphology can be systematically used to improve histological representations themselves. We introduce Spatial Expression-Aligned Learning (SEAL), a vision-omics self-supervised learning framework that infuses localized molecular information into pathology vision encoders. Rather than training new encoders from scratch, SEAL is designed as a parameter-efficient vision-omics finetuning method that can be flexibly applied to widely used pathology foundation models. We instantiate SEAL by training on over 700,000 paired gene expression spot-tissue region examples spanning tumor and normal samples from 14 organs. Tested across 38 slide-level and 15 patch-level downstream tasks, SEAL provides a drop-in replacement for pathology foundation models that consistently improves performance over widely used vision-only and ST prediction baselines on slide-level molecular status, pathway activity, and treatment response prediction, as well as patch-level gene expression prediction tasks. Additionally, SEAL encoders exhibit robust domain generalization on out-of-distribution evaluations and enable new cross-modal capabilities such as gene-to-image retrieval. Our work proposes a general framework for ST-guided finetuning of pathology foundation models, showing that augmenting existing models with localized molecular supervision is an effective and practical step for improving visual representations and expanding their cross-modal utility.

</details>


### [84] [UniWeTok: An Unified Binary Tokenizer with Codebook Size $\mathit{2^{128}}$ for Unified Multimodal Large Language Model](https://arxiv.org/abs/2602.14178)
*Shaobin Zhuang,Yuang Ai,Jiaming Han,Weijia Mao,Xiaohui Li,Fangyikang Wang,Xiao Wang,Yan Li,Shanchuan Lin,Kun Xu,Zhenheng Yang,Huaibo Huang,Xiangyu Yue,Hao Chen,Yali Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为UniWeTok的统一离散分词器，通过大规模二进制码本、预-后提炼、生成感知先验和SigLu激活函数，实现了高保真度重建、复杂的语义提取和生成适应性。在ImageNet和通用领域任务上，UniWeTok均展示了具有竞争力的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉分词器难以在单一框架中同时满足高保真度重建、复杂的语义提取和生成适应性这三个冲突目标。本文通过设计新的分词器和训练方法，解决了这一问题。

Method: UniWeTok包括一个大规模二进制码本、一种新的预-后提炼方法和生成感知先验，以及一种名为SigLu的新型激活函数。这些方法共同作用，以提高分词器的性能。此外，还提出了一种三阶段的训练框架来提高通用适应性。

Result: 在ImageNet数据集上， UniWeTok相较于REPA，实现了更好的图像生成性能（FID分数为1.38 vs. REPA 1.42），并且在通用任务上也展示了强大的性能，例如多模态理解、图像生成和编辑。

Conclusion: 本文提出的UniWeTok提供了一种新的解决方案，使统一多模式大语言模型能够在复杂的任务中表现更好，同时减少了训练计算量。该模型及其代码已开放用于社区探索。

Abstract: Unified Multimodal Large Language Models (MLLMs) require a visual representation that simultaneously supports high-fidelity reconstruction, complex semantic extraction, and generative suitability. However, existing visual tokenizers typically struggle to satisfy these conflicting objectives within a single framework. In this paper, we introduce UniWeTok, a unified discrete tokenizer designed to bridge this gap using a massive binary codebook ($\mathit{2^{128}}$). For training framework, we introduce Pre-Post Distillation and a Generative-Aware Prior to enhance the semantic extraction and generative prior of the discrete tokens. In terms of model architecture, we propose a convolution-attention hybrid architecture with the SigLu activation function. SigLu activation not only bounds the encoder output and stabilizes the semantic distillation process but also effectively addresses the optimization conflict between token entropy loss and commitment loss. We further propose a three-stage training framework designed to enhance UniWeTok's adaptability cross various image resolutions and perception-sensitive scenarios, such as those involving human faces and textual content. On ImageNet, UniWeTok achieves state-of-the-art image generation performance (FID: UniWeTok 1.38 vs. REPA 1.42) while requiring a remarkably low training compute (Training Tokens: UniWeTok 33B vs. REPA 262B). On general-domain, UniWeTok demonstrates highly competitive capabilities across a broad range of tasks, including multimodal understanding, image generation (DPG Score: UniWeTok 86.63 vs. FLUX.1 [Dev] 83.84), and editing (GEdit Overall Score: UniWeTok 5.09 vs. OmniGen 5.06). We release code and models to facilitate community exploration of unified tokenizer and MLLM.

</details>


### [85] [UniRef-Image-Edit: Towards Scalable and Consistent Multi-Reference Image Editing](https://arxiv.org/abs/2602.14186)
*Hongyang Wei,Bin Wen,Yancheng Long,Yankai Yang,Yuhang Hu,Tianke Zhang,Wei Chen,Haonan Fan,Kaiyu Jiang,Jiankang Chen,Changyi Liu,Kaiyu Tang,Haojie Ding,Xiao Yang,Jia Sun,Huaiqing Wang,Zhenyu Yang,Xinyu Wei,Xianglong He,Yangguang Li,Fan Yang,Tingting Gao,Lei Zhang,Guorui Zhou,Han Li*

Main category: cs.CV

TL;DR: UniRef-Image-Edit 系统结合了单图像编辑和多图像合成，通过引入 Sequence-Extended Latent Fusion (SELF) 和两阶段训练框架（监督微调和强化学习），提高了生成质量的一致性和细节度。


<details>
  <summary>Details</summary>
Motivation: 现有的基于扩散的编辑方法在处理多条件一致性问题时受到限制，为此提出了一种新的方法以解决该问题。

Method: 该研究提出了 Sequence-Extended Latent Fusion (SELF)，一种统一的输入表示，通过动态序列化多个参考图像为连贯的潜在序列，以及两阶段训练框架，包括监督微调和强化学习。

Result: 通过引入 SELF 和两阶段的训练框架，提高了视觉真实度和跨引用一致性。研究还提出了一种新的强化学习框架 MSGRPO，优化了多参考图像生成的视觉约束。

Conclusion: UniRef-Image-Edit 系统在多图像合成和单图像编辑任务中表现出色，且代码和模型已开源。

Abstract: We present UniRef-Image-Edit, a high-performance multi-modal generation system that unifies single-image editing and multi-image composition within a single framework. Existing diffusion-based editing methods often struggle to maintain consistency across multiple conditions due to limited interaction between reference inputs. To address this, we introduce Sequence-Extended Latent Fusion (SELF), a unified input representation that dynamically serializes multiple reference images into a coherent latent sequence. During a dedicated training stage, all reference images are jointly constrained to fit within a fixed-length sequence under a global pixel-budget constraint. Building upon SELF, we propose a two-stage training framework comprising supervised fine-tuning (SFT) and reinforcement learning (RL). In the SFT stage, we jointly train on single-image editing and multi-image composition tasks to establish a robust generative prior. We adopt a progressive sequence length training strategy, in which all input images are initially resized to a total pixel budget of $1024^2$, and are then gradually increased to $1536^2$ and $2048^2$ to improve visual fidelity and cross-reference consistency. This gradual relaxation of compression enables the model to incrementally capture finer visual details while maintaining stable alignment across references. For the RL stage, we introduce Multi-Source GRPO (MSGRPO), to our knowledge the first reinforcement learning framework tailored for multi-reference image generation. MSGRPO optimizes the model to reconcile conflicting visual constraints, significantly enhancing compositional consistency. We will open-source the code, models, training data, and reward data for community research purposes.

</details>


### [86] [GeoEyes: On-Demand Visual Focusing for Evidence-Grounded Understanding of Ultra-High-Resolution Remote Sensing Imagery](https://arxiv.org/abs/2602.14201)
*Fengxiang Wang,Mingshuo Chen,Yueying Li,Yajie Yang,Yifan Zhang,Long Lan,Xue Yang,Hongda Sun,Yulin Wang,Di Wang,Jun Song,Jing Zhang,Bo Du*

Main category: cs.CV

TL;DR: 该研究提出了一种名为GeoEyes的框架，旨在解决现有具备缩放功能的多模态大语言模型在高分辨率遥感VQA任务中常见的工具使用同质化问题，通过冷启动数据集和逐步奖励机制提高了模型在需要时主动缩放的能力，并在高分辨率遥感基准测试中取得了显著的准确率提升。


<details>
  <summary>Details</summary>
Motivation: 现有缩放功能的多模态大语言模型在高分辨率遥感VQA任务中存在工具使用同质化的问题，这限制了它们获取有效证据的能力。为此，研究提出了GeoEyes框架来解决这个问题。

Method: GeoEyes框架包括一个冷启动数据集（UHR-CoZ）和一个基于奖励的强化学习方法（AdaZoom-GRPO）。冷启动数据集覆盖了多种缩放模式，而强化学习方法则在缩放交互中明确奖励证据获取和答案改进。

Result: GeoEyes框架训练出的模型在需求时能够主动缩放并将停止行为表现恰当，取得了显著的性能提升，在高分辨率遥感基准测试（XLRS-Bench）上达到54.23%的准确率。

Conclusion: GeoEyes框架通过结合冷启动数据集和强化学习方法，成功改善了多模态大语言模型在高分辨率遥感VQA任务中的表现，为解决工具使用同质化问题提供了有效途径。

Abstract: The "thinking-with-images" paradigm enables multimodal large language models (MLLMs) to actively explore visual scenes via zoom-in tools. This is essential for ultra-high-resolution (UHR) remote sensing VQA, where task-relevant cues are sparse and tiny. However, we observe a consistent failure mode in existing zoom-enabled MLLMs: Tool Usage Homogenization, where tool calls collapse into task-agnostic patterns, limiting effective evidence acquisition. To address this, we propose GeoEyes, a staged training framework consisting of (1) a cold-start SFT dataset, UHR Chain-of-Zoom (UHR-CoZ), which covers diverse zooming regimes, and (2) an agentic reinforcement learning method, AdaZoom-GRPO, that explicitly rewards evidence gain and answer improvement during zoom interactions. The resulting model learns on-demand zooming with proper stopping behavior and achieves substantial improvements on UHR remote sensing benchmarks, with 54.23% accuracy on XLRS-Bench.

</details>


### [87] [Learning Significant Persistent Homology Features for 3D Shape Understanding](https://arxiv.org/abs/2602.14228)
*Prachi Kudeshia,Jiju Poovvancheri*

Main category: cs.CV

TL;DR: 本文通过引入带有拓扑特征的ModelNet40和ShapeNet版本，提出了拓扑感知的深度学习方法TopoGAT，用于3D形状分析，并实现了分类和分割性能的提升。


<details>
  <summary>Details</summary>
Motivation: 现有的基准数据集主要捕捉几何信息而忽视拓扑结构，本文旨在填补这一空白，通过引入具有拓扑特征的点云数据集，为统一的几何-拓扑学习提供基础，进而系统评估拓扑感知深度学习架构。

Method: 本文提出了一种基于深度学习的重要持久点选择方法TopoGAT，能够直接从输入数据中学习识别最具信息性的拓扑特征，无需依赖手工构建的统计选择准则。并且将选定的重要持久点集成到标准点云分类和部分分割流水线中。

Result: 对比研究显示，TopoGAT方法在稳定性和识别能力方面优于传统的统计方法。集成选定的重要持久点后，分类准确性和分割指标均得到提升。

Conclusion: 本文提出了拓扑增强的点云数据集和学习可选的重要特征选择方法，促进了拓扑同调理论在3D点云分析中的更广泛集成。

Abstract: Geometry and topology constitute complementary descriptors of three-dimensional shape, yet existing benchmark datasets primarily capture geometric information while neglecting topological structure. This work addresses this limitation by introducing topologically-enriched versions of ModelNet40 and ShapeNet, where each point cloud is augmented with its corresponding persistent homology features. These benchmarks with the topological signatures establish a foundation for unified geometry-topology learning and enable systematic evaluation of topology-aware deep learning architectures for 3D shape analysis. Building on this foundation, we propose a deep learning-based significant persistent point selection method, \textit{TopoGAT}, that learns to identify the most informative topological features directly from input data and the corresponding topological signatures, circumventing the limitations of hand-crafted statistical selection criteria. A comparative study verifies the superiority of the proposed method over traditional statistical approaches in terms of stability and discriminative power. Integrating the selected significant persistent points into standard point cloud classification and part-segmentation pipelines yields improvements in both classification accuracy and segmentation metrics. The presented topologically-enriched datasets, coupled with our learnable significant feature selection approach, enable the broader integration of persistent homology into the practical deep learning workflows for 3D point cloud analysis.

</details>


### [88] [AbracADDbra: Touch-Guided Object Addition by Decoupling Placement and Editing Subtasks](https://arxiv.org/abs/2602.14237)
*Kunal Swami,Raghu Chittersu,Yuvraj Rathore,Rajeev Irny,Shashavali Doodekula,Alok Shukla*

Main category: cs.CV

TL;DR: 该论文介绍了一个名为AbracADDbra的框架，该框架通过直观的触控先验来为精确放置提供简约指令的时空定位，从而解决了基于文本指令或蒙版输入的指令式物体添加的可用性问题。该框架使用视觉语言转换器进行触控引导放置，并结合生成模型生成物体及其实例蒙版，以实现高质量融合。通过Touch2Add基准测试，验证了该框架在交互任务中的高效性和优越性。


<details>
  <summary>Details</summary>
Motivation: 当前基于文本的提示或基于蒙版的输入在指令式物体添加中存在不清晰或繁琐的问题，为了克服这一可用性瓶颈，该研究提出AbracADDbra框架。

Method: 该框架采用视觉语言转换器进行触控引导的精准放置，并结合扩散模型生成物体及其实例蒙版，实现高质量融合。同时，为了提供标准化的评估，提出了Touch2Add基准，并且进行了广泛的评估。

Result: 对比随机放置和通用视觉语言模型基准，该研究中提出的放置模型表现出显著优势，验证了框架的有效性。此外，分析表明初始放置准确性与最终编辑质量之间存在强相关性，支持了该框架的分阶段设计。

Conclusion: 该研究提出的AbracADDbra框架为更加便捷高效的创意工具铺平了道路。

Abstract: Instruction-based object addition is often hindered by the ambiguity of text-only prompts or the tedious nature of mask-based inputs. To address this usability gap, we introduce AbracADDbra, a user-friendly framework that leverages intuitive touch priors to spatially ground succinct instructions for precise placement. Our efficient, decoupled architecture uses a vision-language transformer for touch-guided placement, followed by a diffusion model that jointly generates the object and an instance mask for high-fidelity blending. To facilitate standardized evaluation, we contribute the Touch2Add benchmark for this interactive task. Our extensive evaluations, where our placement model significantly outperforms both random placement and general-purpose VLM baselines, confirm the framework's ability to produce high-fidelity edits. Furthermore, our analysis reveals a strong correlation between initial placement accuracy and final edit quality, validating our decoupled approach. This work thus paves the way for more accessible and efficient creative tools.

</details>


### [89] [Differential pose optimization in descriptor space -- Combining Geometric and Photometric Methods for Motion Estimation](https://arxiv.org/abs/2602.14297)
*Andreas L. Teigen,Annette Stahl,Rudolf Mester*

Main category: cs.CV

TL;DR: 该研究提出了一种结合光度特征和几何特征描述符的方法以优化两帧相对位姿，尽管能够提供亚像素精度和几何特征表征力，但最终其表现仍未超过基于重投影误差的优化策略。


<details>
  <summary>Details</summary>
Motivation: 为了解决单应性优化中的权衡问题，即精度、鲁棒性和循环闭合的可能性之间的权衡，研究引入了一种新方法来同时利用几何特征和光度特征的优势。

Method: 通过使用密集采样的几何特征描述符，将光度误差替换为密集描述符集的描述符残差，从而在保持几何特征描述符的表征力的同时，实现基于光度误差的亚像素精度。

Result: 实验表明，尽管提议的方法能够提供准确的跟踪，但它在准确性和鲁棒性上未能超越基于重投影误差的位姿优化策略，尽管它使用了更多的信息。

Conclusion: 研究结论指出，描述符相似性度量变化缓慢，不一定严格对应关键点放置的准确性，这是导致提议方法表现不佳的根本原因。

Abstract: One of the fundamental problems in computer vision is the two-frame relative pose optimization problem. Primarily, two different kinds of error values are used: photometric error and re-projection error. The selection of error value is usually directly dependent on the selection of feature paradigm, photometric features, or geometric features. It is a trade-off between accuracy, robustness, and the possibility of loop closing. We investigate a third method that combines the strengths of both paradigms into a unified approach. Using densely sampled geometric feature descriptors, we replace the photometric error with a descriptor residual from a dense set of descriptors, thereby enabling the employment of sub-pixel accuracy in differential photometric methods, along with the expressiveness of the geometric feature descriptor. Experiments show that although the proposed strategy is an interesting approach that results in accurate tracking, it ultimately does not outperform pose optimization strategies based on re-projection error despite utilizing more information. We proceed to analyze the underlying reason for this discrepancy and present the hypothesis that the descriptor similarity metric is too slowly varying and does not necessarily correspond strictly to keypoint placement accuracy.

</details>


### [90] [A Generative AI Approach for Reducing Skin Tone Bias in Skin Cancer Classification](https://arxiv.org/abs/2602.14356)
*Areez Muhammed Shabu,Mohammad Samar Ansari,Asra Aslam*

Main category: cs.CV

TL;DR: 本文提出了一种生成增强管道，通过低秩适应（LoRA）微调预训练的Stable Diffusion模型，生成合成皮肤病变图像，以解决国际皮肤成像合作（ISIC）数据集中肤色失衡问题。该方法在病灶分割和二分类任务中均表现出了提高公平性并减少偏见的效果。


<details>
  <summary>Details</summary>
Motivation: 目前的AI诊断工具多依赖于以浅肤色样本为主的训练数据集，这在深度学习模型中导致了对深肤色人群的诊断准确性和公平性的降低。为了给皮肤癌检测提供更公平、更准确的方法，作者着手改进ISIC数据集中深肤色样本的不足。

Method: 本研究提出了一个基于生成模型的增强管道，通过低秩适应（LoRA）技术精细调整预训练的Stable Diffusion模型，并利用改进的数据集在两组下游任务（病灶分割和二分类）上进行了实验。

Result: 经过增强的数据集使模型在病灶分割任务中的IoU、Dice系数和边界准确性等方面表现出了显著提高。同时，应用于所提升数据集上的EfficientNet-B0分类模型达到了92.14%的准确率。

Conclusion: 研究证明了生成对抗合成数据的方法可以显著减少传统皮肤病诊断中的偏见，提高公平性，同时开启了未来研究的新方向。

Abstract: Skin cancer is one of the most common cancers worldwide and early detection is critical for effective treatment. However, current AI diagnostic tools are often trained on datasets dominated by lighter skin tones, leading to reduced accuracy and fairness for people with darker skin. The International Skin Imaging Collaboration (ISIC) dataset, one of the most widely used benchmarks, contains over 70% light skin images while dark skins fewer than 8%. This imbalance poses a significant barrier to equitable healthcare delivery and highlights the urgent need for methods that address demographic diversity in medical imaging. This paper addresses this challenge of skin tone imbalance in automated skin cancer detection using dermoscopic images. To overcome this, we present a generative augmentation pipeline that fine-tunes a pre-trained Stable Diffusion model using Low-Rank Adaptation (LoRA) on the image dark-skin subset of the ISIC dataset and generates synthetic dermoscopic images conditioned on lesion type and skin tone. In this study, we investigated the utility of these images on two downstream tasks: lesion segmentation and binary classification. For segmentation, models trained on the augmented dataset and evaluated on held-out real images show consistent improvements in IoU, Dice coefficient, and boundary accuracy. These evalutions provides the verification of Generated dataset. For classification, an EfficientNet-B0 model trained on the augmented dataset achieved 92.14% accuracy. This paper demonstrates that synthetic data augmentation with Generative AI integration can substantially reduce bias with increase fairness in conventional dermatological diagnostics and open challenges for future directions.

</details>


### [91] [Image-based Joint-level Detection for Inflammation in Rheumatoid Arthritis from Small and Imbalanced Data](https://arxiv.org/abs/2602.14365)
*Shun Kato,Yasushi Kondo,Shuntaro Saito,Yoshimitsu Aoki,Mariko Isogawa*

Main category: cs.CV

TL;DR: 本文提出了一种利用RGB手部图像检测类风湿关节炎相关关节炎症的方法，通过专门构建的数据集量化了视觉检测炎症的难度，并提出了一种结合自我监督预训练和不平衡感知训练的全局局部编码器框架，该方法在F1分数和Gmean上分别提高了0.2和0.25点。


<details>
  <summary>Details</summary>
Motivation: 早期诊断和密切跟进对于类风湿关节炎（RA）的管理至关重要，但患者往往需要很长时间才能获得适当的专科护理。因此，需要开发一种可以从居家拍摄的RGB图像中轻松检测关节炎症的系统。

Method: 本文提出了一种方法，采用了专门构建的数据集并提出了一个框架，该框架结合了大规模健康手部图像的自我监督预训练和不平衡感知训练，用于从RGB手部图像检测RA相关关节炎症。

Result: 实验表明，所提出的方法在F1分数和Gmean上分别提高了0.2和0.25点。

Conclusion: 本文的研究结果表明，通过自我监督预训练和不平衡感知训练相结合的方法，可以从RGB手部图像中有效检测RA相关关节炎症，但还存在数据稀缺、数据不平衡等挑战需要进一步解决。

Abstract: Rheumatoid arthritis (RA) is an autoimmune disease characterized by systemic joint inflammation. Early diagnosis and tight follow-up are essential to the management of RA, as ongoing inflammation can cause irreversible joint damage. The detection of arthritis is important for diagnosis and assessment of disease activity; however, it often takes a long time for patients to receive appropriate specialist care. Therefore, there is a strong need to develop systems that can detect joint inflammation easily using RGB images captured at home. Consequently, we tackle the task of RA inflammation detection from RGB hand images. This task is highly challenging due to general issues in medical imaging, such as the scarcity of positive samples, data imbalance, and the inherent difficulty of the task itself. However, to the best of our knowledge, no existing work has explicitly addressed these challenges in RGB-based RA inflammation detection. This paper quantitatively demonstrates the difficulty of visually detecting inflammation by constructing a dedicated dataset, and we propose a inflammation detection framework with global local encoder that combines self-supervised pretraining on large-scale healthy hand images with imbalance-aware training to detect RA-related joint inflammation from RGB hand images. Our experiments demonstrated that the proposed approach improves F1-score by 0.2 points and Gmean by 0.25 points compared with the baseline model.

</details>


### [92] [Event-based Visual Deformation Measurement](https://arxiv.org/abs/2602.14376)
*Yuliang Wu,Wei Zhai,Yuxin Cui,Tiesong Zhao,Yang Cao,Zheng-Jun Zha*

Main category: cs.CV

TL;DR: 本文提出了一种事件帧融合框架，利用事件提供时间密集型运动线索，并利用帧进行空间密集型精确估计。这种方法在维护运动连续性方面具有优势，并通过引入邻域贪婪优化策略来加速参数搜索，减少累积误差。


<details>
  <summary>Details</summary>
Motivation: 传统的基于图像的方法受限于帧间小 motions，难以处理高动态场景。本文旨在提出一种新型框架，整合事件与帧，以提供更准确的变形测量。

Method: 本文提出了一种Affine Invariant Simplicial (AIS)框架，将变形场细分为具有低参数表示的线性子区域。同时引入了邻域贪婪优化策略，以加速参数搜索，减少误差累积。

Result: 实验结果显示，该方法在生存率上比最先进的基线方法高1.6%，并且只需高帧率视频方法所需数据存储和计算资源的18.9%。

Conclusion: 本文提出的方法在保持变形连续性方面表现出色，提供了更为精确的变形测量，并且在资源消耗上显著降低。

Abstract: Visual Deformation Measurement (VDM) aims to recover dense deformation fields by tracking surface motion from camera observations. Traditional image-based methods rely on minimal inter-frame motion to constrain the correspondence search space, which limits their applicability to highly dynamic scenes or necessitates high-speed cameras at the cost of prohibitive storage and computational overhead. We propose an event-frame fusion framework that exploits events for temporally dense motion cues and frames for spatially dense precise estimation. Revisiting the solid elastic modeling prior, we propose an Affine Invariant Simplicial (AIS) framework. It partitions the deformation field into linearized sub-regions with low-parametric representation, effectively mitigating motion ambiguities arising from sparse and noisy events. To speed up parameter searching and reduce error accumulation, a neighborhood-greedy optimization strategy is introduced, enabling well-converged sub-regions to guide their poorly-converged neighbors, effectively suppress local error accumulation in long-term dense tracking. To evaluate the proposed method, a benchmark dataset with temporally aligned event streams and frames is established, encompassing over 120 sequences spanning diverse deformation scenarios. Experimental results show that our method outperforms the state-of-the-art baseline by 1.6% in survival rate. Remarkably, it achieves this using only 18.9% of the data storage and processing resources of high-speed video methods.

</details>


### [93] [Adapting VACE for Real-Time Autoregressive Video Diffusion](https://arxiv.org/abs/2602.14381)
*Ryan Fosdick*

Main category: cs.CV

TL;DR: 文章描述了VACE的流媒体适应版本，它允许实时自回归视频生成，并保持固定的块大小和KV缓存需求。


<details>
  <summary>Details</summary>
Motivation: 当前的VACE需要双向注意整个序列，不适用于需要固定块大小和因果注意的流媒体管道。

Method: 将参考帧从扩散潜空间转移到并行条件路径，以适应流媒体管道的要求。

Result: 该适应版使用现有预训练的VACE权重，无需额外训练。在1.3B和14B模型规模下，针对结构控制和修复增加了20-30%的延迟成本，但在显存成本上相对于基础模型影响不大。参考实现了视频与参考帧之间的保真度显著降低。

Conclusion: 该版本的VACE提供了实时自回归视频生成的能力，同时保持了固定的块大小和KV缓存要求，但会影响视频和参考帧之间的保真度。

Abstract: We describe an adaptation of VACE (Video All-in-one Creation and Editing) for real-time autoregressive video generation. VACE provides unified video control (reference guidance, structural conditioning, inpainting, and temporal extension) but assumes bidirectional attention over full sequences, making it incompatible with streaming pipelines that require fixed chunk sizes and causal attention. The key modification moves reference frames from the diffusion latent space into a parallel conditioning pathway, preserving the fixed chunk sizes and KV caching that autoregressive models require. This adaptation reuses existing pretrained VACE weights without additional training. Across 1.3B and 14B model scales, VACE adds 20-30% latency overhead for structural control and inpainting, with negligible VRAM cost relative to the base model. Reference-to-video fidelity is severely degraded compared to batch VACE due to causal attention constraints. A reference implementation is available at https://github.com/daydreamlive/scope.

</details>


### [94] [Multi-Turn Adaptive Prompting Attack on Large Vision-Language Models](https://arxiv.org/abs/2602.14399)
*In Chong Choi,Jiacheng Zhang,Feng Liu,Yiliao Song*

Main category: cs.CV

TL;DR: MAPA是一种针对大型视觉-语言模型的多轮自适应提示攻击方法，通过交替使用文本和视觉攻击行动来逐渐增强恶意响应，显著提高了攻击成功率。


<details>
  <summary>Details</summary>
Motivation: 现有的多轮监狱突破攻击方法在处理视觉-语言模型时容易被防御机制遏制，因此需要一种新的攻击方法来提高恶意响应的成功率。

Method: MAPA方法通过在每一轮中交替使用文本和视觉攻击手段，并在多轮中通过迭代反馈调整攻击路径，逐步增强响应的恶意性。

Result: MAPA方法在多个大型视觉-语言模型上的实验表明，其成功率较最先进的方法提高11-35%。

Conclusion: MAPA方法被视为一种有效的提升攻击成功率的策略，特别适用于对抗视觉-语言模型的安全机制。

Abstract: Multi-turn jailbreak attacks are effective against text-only large language models (LLMs) by gradually introducing malicious content across turns. When extended to large vision-language models (LVLMs), we find that naively adding visual inputs can cause existing multi-turn jailbreaks to be easily defended. For example, overly malicious visual input will easily trigger the defense mechanism of safety-aligned LVLMs, making the response more conservative. To address this, we propose MAPA: a multi-turn adaptive prompting attack that 1) at each turn, alternates text-vision attack actions to elicit the most malicious response; and 2) across turns, adjusts the attack trajectory through iterative back-and-forth refinement to gradually amplify response maliciousness. This two-level design enables MAPA to consistently outperform state-of-the-art methods, improving attack success rates by 11-35% on recent benchmarks against LLaVA-V1.6-Mistral-7B, Qwen2.5-VL-7B-Instruct, Llama-3.2-Vision-11B-Instruct and GPT-4o-mini.

</details>


### [95] [pFedNavi: Structure-Aware Personalized Federated Vision-Language Navigation for Embodied AI](https://arxiv.org/abs/2602.14401)
*Qingqian Yang,Hao Wang,Sai Qian Zhang,Jian Li,Yang Hua,Miao Pan,Tao Song,Zhengwei Qi,Haibing Guan*

Main category: cs.CV

TL;DR: pFedNavi 是一种适应性和个性化框架，用于解决视觉-语言导航中的联邦学习问题，通过自适应地确定客户端特定层，并在选定组件上进行细粒度参数融合，以平衡全局知识共享与局部专业化。


<details>
  <summary>Details</summary>
Motivation: 公共室内环境的数据采集存在问题，联邦学习可以在不共享原始数据的情况下训练模型，但在视觉-语言导航(VLN)中，由于环境和指令风格的极端跨设备异质性，传统的联邦学习难以达到最优。

Method: pFedNavi 通过逐层混合系数自适应地标识客户端特定层，并在选定组件（如编码器-解码器投影和环境敏感解码器层）上进行细粒度参数融合，以平衡全局知识共享与局部专业化。

Result: 在两个标准的VLN基准测试R2R和RxR上，与基于FedAvg的VLN基线相比，pFedNavi 在所有指标上都表现出色，导航成功率最多提高了7.5％，轨迹保真度提高了7.8％，非独立同分布条件下收敛速度加快了1.38倍。

Conclusion: pFedNavi 通过自适应的个性化参数融合，在跨客户端异质性的视觉-语言导航任务中能够有效提高模型性能和收敛速度，验证了其对此类任务的有效性。

Abstract: Vision-Language Navigation VLN requires large-scale trajectory instruction data from private indoor environments, raising significant privacy concerns. Federated Learning FL mitigates this by keeping data on-device, but vanilla FL struggles under VLNs' extreme cross-client heterogeneity in environments and instruction styles, making a single global model suboptimal. This paper proposes pFedNavi, a structure-aware and dynamically adaptive personalized federated learning framework tailored for VLN. Our key idea is to personalize where it matters: pFedNavi adaptively identifies client-specific layers via layer-wise mixing coefficients, and performs fine-grained parameter fusion on the selected components (e.g., the encoder-decoder projection and environment-sensitive decoder layers) to balance global knowledge sharing with local specialization. We evaluate pFedNavi on two standard VLN benchmarks, R2R and RxR, using both ResNet and CLIP visual representations. Across all metrics, pFedNavi consistently outperforms the FedAvg-based VLN baseline, achieving up to 7.5% improvement in navigation success rate and up to 7.8% gain in trajectory fidelity, while converging 1.38x faster under non-IID conditions.

</details>


### [96] [Learning Proposes, Geometry Disposes: A Modular Framework for Efficient Spatial Reasoning](https://arxiv.org/abs/2602.14409)
*Haichao Zhu,Zhaorui Yang,Qian Zhang*

Main category: cs.CV

TL;DR: 该研究提出了一种端到端的模块化框架，用于有效的空间推理，学习模块提出几何假设，而几何算法进行估计决定。通过VGGT模型在RGB-D序列上进行相对相机姿态估计，发现仅依赖学习模块不可靠，并且几何校准后跟随几何决策能改善性能。


<details>
  <summary>Details</summary>
Motivation: 近年来学习方法在几何感知中展现了强大的表示能力，但关于学习组件是否应该直接替代几何估计或作为管道中的中间模块，这个问题尚未得到解决。

Method: 使用VGGT作为代表的学习模型，评估基于学习的姿态和深度提议，并且由经典的点到平面RGB-D ICP进行几何后处理。

Result: 实验结果表明，单独依赖学习模块是不可靠的；未经几何校准的学习提议几何会降低性能；然而，当学习提议的深度几何校准后，并且跟随几何处理步骤时，在适度挑战的刚性设置中，会获得一致的性能提升。

Conclusion: 几何不仅是改进的组件，而是验证学习几何观察的关键。这项研究强调了模块化、几何感知系统设计对于稳健的空间感知的重要性。

Abstract: Spatial perception aims to estimate camera motion and scene structure from visual observations, a problem traditionally addressed through geometric modeling and physical consistency constraints. Recent learning-based methods have demonstrated strong representational capacity for geometric perception and are increasingly used to augment classical geometry-centric systems in practice. However, whether learning components should directly replace geometric estimation or instead serve as intermediate modules within such pipelines remains an open question.
  In this work, we address this gap and investigate an end-to-end modular framework for effective spatial reasoning, where learning proposes geometric hypotheses, while geometric algorithms dispose estimation decisions. In particular, we study this principle in the context of relative camera pose estimation on RGB-D sequences. Using VGGT as a representative learning model, we evaluate learning-based pose and depth proposals under varying motion magnitudes and scene dynamics, followed by a classical point-to-plane RGB-D ICP as the geometric backend. Our experiments on the TUM RGB-D benchmark reveal three consistent findings: (1) learning-based pose proposals alone are unreliable; (2) learning-proposed geometry, when improperly aligned with camera intrinsics, can degrade performance; and (3) when learning-proposed depth is geometrically aligned and followed by a geometric disposal stage, consistent improvements emerge in moderately challenging rigid settings.
  These results demonstrate that geometry is not merely a refinement component, but an essential arbiter that validates and absorbs learning-based geometric observations. Our study highlights the importance of modular, geometry-aware system design for robust spatial perception.

</details>


### [97] [Understanding Sensor Vulnerabilities in Industrial XR Tracking](https://arxiv.org/abs/2602.14413)
*Sourya Saha,Md. Nurul Absur*

Main category: cs.CV

TL;DR: 本研究在劣化传感条件下对XR系统中的视觉惯性定位（VIO）行为进行了受控的实证分析，发现视觉传感退化通常会产生厘米级的定位误差，而惯性传感退化可能导致数千米的轨迹偏差。


<details>
  <summary>Details</summary>
Motivation: 已有研究主要关注理想条件下VIO的行为，而本研究旨在填补在恶劣工作条件下VIO性能评估的不足，特别是重点考察惯性传感器的可靠性。

Method: 通过系统性的故障注入和定量评估，研究不同感应模态的退化对定位精度的影响。

Result: 研究发现，视觉传感退化有效控制了误差（厘米级），但惯性传感退化使轨迹严重偏移（有时达到数百至数千米）。

Conclusion: 研究强调在XR系统实际工业应用中，应更加注重惯性传感器的可靠性。

Abstract: Extended Reality (XR) systems deployed in industrial and operational settings rely on Visual--Inertial Odometry (VIO) for continuous six-degree-of-freedom pose tracking, yet these environments often involve sensing conditions that deviate from ideal assumptions. Despite this, most VIO evaluations emphasize nominal sensor behavior, leaving the effects of sustained sensor degradation under operational conditions insufficiently understood. This paper presents a controlled empirical study of VIO behavior under degraded sensing, examining faults affecting visual and inertial modalities across a range of operating regimes. Through systematic fault injection and quantitative evaluation, we observe a pronounced asymmetry in fault impact where degradations affecting visual sensing typically lead to bounded pose errors on the order of centimeters, whereas degradations affecting inertial sensing can induce substantially larger trajectory deviations, in some cases reaching hundreds to thousands of meters. These observations motivate greater emphasis on inertial reliability in the evaluation and design of XR systems for real-life industrial settings.

</details>


### [98] [Hierarchical Vision-Language Interaction for Facial Action Unit Detection](https://arxiv.org/abs/2602.14425)
*Yong Li,Yi Ren,Yizhe Zhang,Wenhua Zhang,Tianyi Zhang,Muyun Jiang,Guo-Sen Xie,Cuntai Guan*

Main category: cs.CV

TL;DR: HiVA方法通过结合文本和视觉信息，特别是利用大型语言模型生成的多样化和语境丰富的AU描述，增强了AU检测的表达学习。同时，通过AU感知的动态图模块捕获了精细和综合的视觉-语言关联，并结合了多层次的跨模态注意力架构，实现了鲁棒且语义丰富的AU检测能力。


<details>
  <summary>Details</summary>
Motivation: 针对面部动作单元（AU）检测中在标注数据有限条件下难以有效学习到区分性和泛化性AU表示的问题，提出了一种新颖的方法，该方法通过引入大型语言模型增强语言表示学习，并结合AU感知的动态图模块和多层次跨模态注意力机制，强化了细粒度和宏观的视觉-语言关联，以实现更准确的AU检测。

Method: HiVA方法使用大型语言模型生成丰富且多样化的AU描述来增强基于语言的表示学习。通过引入AU感知的动态图模块，捕捉到了精细和整体的视觉-语言关联。整个框架包括两个互补机制：细粒度和AU特定的互动作注意力（DDCA）和建模AU间全局依赖关系的上下文双模态交叉注意力（CDCA）。

Result: 广泛实验表明，HiVA在所有测试基准上均超越了现有最先进的方法。此外，定性分析结果显示，HiVA可以生成具有语义意义的活动模式，证明了其在学习鲁棒且可解释的跨模态对应关系方面的能力。

Conclusion: 通过结合多元信息源，特别是文本和视觉信息，HiVA为全面的面部行为分析提供了强大的、语义丰富的AU检测能力，并且具有鲁棒性。

Abstract: Facial Action Unit (AU) detection seeks to recognize subtle facial muscle activations as defined by the Facial Action Coding System (FACS). A primary challenge w.r.t AU detection is the effective learning of discriminative and generalizable AU representations under conditions of limited annotated data. To address this, we propose a Hierarchical Vision-language Interaction for AU Understanding (HiVA) method, which leverages textual AU descriptions as semantic priors to guide and enhance AU detection. Specifically, HiVA employs a large language model to generate diverse and contextually rich AU descriptions to strengthen language-based representation learning. To capture both fine-grained and holistic vision-language associations, HiVA introduces an AU-aware dynamic graph module that facilitates the learning of AU-specific visual representations. These features are further integrated within a hierarchical cross-modal attention architecture comprising two complementary mechanisms: Disentangled Dual Cross-Attention (DDCA), which establishes fine-grained, AU-specific interactions between visual and textual features, and Contextual Dual Cross-Attention (CDCA), which models global inter-AU dependencies. This collaborative, cross-modal learning paradigm enables HiVA to leverage multi-grained vision-based AU features in conjunction with refined language-based AU details, culminating in robust and semantically enriched AU detection capabilities. Extensive experiments show that HiVA consistently surpasses state-of-the-art approaches. Besides, qualitative analyses reveal that HiVA produces semantically meaningful activation patterns, highlighting its efficacy in learning robust and interpretable cross-modal correspondences for comprehensive facial behavior analysis.

</details>


### [99] [D-SECURE: Dual-Source Evidence Combination for Unified Reasoning in Misinformation Detection](https://arxiv.org/abs/2602.14441)
*Gagandeep Singh,Samudi Amarasinghe,Priyanka Singh*

Main category: cs.CV

TL;DR: 该论文提出了D-SECURE框架，它结合了内部操纵检测和基于外部证据的推理，以检测具有局部不一致性的混合信息，相比现有系统，能够在像素级或词元级腐败的部分中识别欺骗性编辑。


<details>
  <summary>Details</summary>
Motivation: 现有的单一证据来源检测系统无法处理局部不一致的欺诈性内容，而基于检索的事实核查系统虽然可以利用外部证据，但往往无法检测到细微的视觉或文本篡改。D-SECURE框架旨在解决这些问题。

Method: D-SECURE框架结合使用了HAMMER操纵检测器和DEFAME检索管道。HAMMER专注于检测残余或不确定的情况，DEFAME则进行广泛的验证。

Result: 在DGM4和ClaimReview样本上的实验表明，D-SECURE框架能够利用两个系统的互补优势，为新闻风格的帖子提供统一、可解释的报告。

Conclusion: D-SECURE框架提供了一种有效的方法来检测复杂的多媒体误导信息，结合了局部和全局分析的优势，有助于提高虚假信息检测的准确性。

Abstract: Multimodal misinformation increasingly mixes realistic im-age edits with fluent but misleading text, producing persuasive posts that are difficult to verify. Existing systems usually rely on a single evidence source. Content-based detectors identify local inconsistencies within an image and its caption but cannot determine global factual truth. Retrieval-based fact-checkers reason over external evidence but treat inputs as coarse claims and often miss subtle visual or textual manipulations. This separation creates failure cases where internally consistent fabrications bypass manipulation detectors and fact-checkers verify claims that contain pixel-level or token-level corruption. We present D-SECURE, a framework that combines internal manipulation detection with external evidence-based reasoning for news-style posts. D-SECURE integrates the HAMMER manipulation detector with the DEFAME retrieval pipeline. DEFAME performs broad verification, and HAMMER analyses residual or uncertain cases that may contain fine-grained edits. Experiments on DGM4 and ClaimReview samples highlight the complementary strengths of both systems and motivate their fusion. We provide a unified, explainable report that incorporates manipulation cues and external evidence.

</details>


### [100] [Controlling Your Image via Simplified Vector Graphics](https://arxiv.org/abs/2602.14443)
*Lanqing Guo,Xi Liu,Yufei Wang,Zhihao Li,Siyu Huang*

Main category: cs.CV

TL;DR: 该研究通过将图像解析为层级矢量图形（VG）表示，并设计一种基于VG的新型图像合成框架，实现了图素级别的图像生成控制，能精细控制几何、颜色和对象语义，适用于多种应用。


<details>
  <summary>Details</summary>
Motivation: 当前生成图像已取得良好视觉质量，但难以在图素级别上进行精细控制。鉴于此，研究旨在解决这一挑战，通过简化矢量图形（VG）实现图素级别的图像生成控制，以支持直观的图像修改功能。

Method: 研究首先利用高效算法将图像解析为层级VG表示，考虑到语义对齐和结构连贯性。基于这种表示，设计了一种由VG指导的新图像合成框架，允许用户自由修改元素，并无缝将这些编辑转化为逼真的输出。利用VG的结构和语义特征以及噪声预测来精细控制几何、颜色和对象语义。

Result: 实验结果表明，该方法在图像编辑、对象级操作和细粒度内容创建等多种应用中表现出色，形成了一种新的可控图像生成范式。

Conclusion: 本文提出了通过简化矢量图形实现图素级图像生成控制的新方法，展示了一种新的可控图像生成框架，在多种应用中表现良好，为后续研究提供了重要参考。

Abstract: Recent advances in image generation have achieved remarkable visual quality, while a fundamental challenge remains: Can image generation be controlled at the element level, enabling intuitive modifications such as adjusting shapes, altering colors, or adding and removing objects? In this work, we address this challenge by introducing layer-wise controllable generation through simplified vector graphics (VGs). Our approach first efficiently parses images into hierarchical VG representations that are semantic-aligned and structurally coherent. Building on this representation, we design a novel image synthesis framework guided by VGs, allowing users to freely modify elements and seamlessly translate these edits into photorealistic outputs. By leveraging the structural and semantic features of VGs in conjunction with noise prediction, our method provides precise control over geometry, color, and object semantics. Extensive experiments demonstrate the effectiveness of our approach in diverse applications, including image editing, object-level manipulation, and fine-grained content creation, establishing a new paradigm for controllable image generation. Project page: https://guolanqing.github.io/Vec2Pix/

</details>


### [101] [CoCoDiff: Correspondence-Consistent Diffusion Model for Fine-grained Style Transfer](https://arxiv.org/abs/2602.14464)
*Wenbo Nie,Zixiang Li,Renshuai Tao,Bin Wu,Yunchao Wei,Yao Zhao*

Main category: cs.CV

TL;DR: CoCoDiff 提出了一种无需训练且低成本的图像风格迁移框架，利用预训练的潜入扩散模型实现精细且语义一致的风格化。该框架通过像素级的语义对应模块和循环一致性模块，增强了语义匹配区域的内容一致性，从而在保持几何结构和细节的同时，实现物体和区域级别的风格迁移。


<details>
  <summary>Details</summary>
Motivation: 现有方法往往在全局级别进行操作，忽略了区域级甚至像素级的语义一致性。CoCoDiff 旨在解决这一问题，通过利用预训练的生成扩散模型来实现这一点。

Method: CoCoDiff 利用预训练的潜入扩散模型实现风格迁移。引入了像素级语义对应模块，从中间扩散特征中挖掘以构建内容和风格图像之间的密集对齐图。同时，还包含了一个循环一致性模块，在迭代过程中强制执行结构和感知对齐。

Result: 尽管无需额外的训练或监督，CoCoDiff 在视觉质量和量化结果方面表现优异，超过了依赖额外训练或注释的方法。

Conclusion: CoCoDiff 为图像风格迁移提供了一种高效且低成本的方法，有效解决了语义一致性的问题，并在多个方面取得了成功。

Abstract: Transferring visual style between images while preserving semantic correspondence between similar objects remains a central challenge in computer vision. While existing methods have made great strides, most of them operate at global level but overlook region-wise and even pixel-wise semantic correspondence. To address this, we propose CoCoDiff, a novel training-free and low-cost style transfer framework that leverages pretrained latent diffusion models to achieve fine-grained, semantically consistent stylization. We identify that correspondence cues within generative diffusion models are under-explored and that content consistency across semantically matched regions is often neglected. CoCoDiff introduces a pixel-wise semantic correspondence module that mines intermediate diffusion features to construct a dense alignment map between content and style images. Furthermore, a cycle-consistency module then enforces structural and perceptual alignment across iterations, yielding object and region level stylization that preserves geometry and detail. Despite requiring no additional training or supervision, CoCoDiff delivers state-of-the-art visual quality and strong quantitative results, outperforming methods that rely on extra training or annotations.

</details>


### [102] [Gaussian Mesh Renderer for Lightweight Differentiable Rendering](https://arxiv.org/abs/2602.14493)
*Xinpeng Liu,Fumio Okura*

Main category: cs.CV

TL;DR: 本文提出了一种名为GMR的新轻量级可微网格渲染器，该渲染器将Gaussian和网格表示紧密集成，通过有效的3DGS栅格化过程实现更快更有效的优化。


<details>
  <summary>Details</summary>
Motivation: 传统的网格模型虽然在曲面重构方面很受欢迎，但在传统的网格差分渲染器中优化速度缓慢或依赖于高性能计算。

Method: GMR通过利用3DGS的高效栅格化过程，将Gaussian和网格表示相结合，每个Gaussian原始体都从相应的网格三角形中推导出来，以保持结构保真度，并允许梯度流动。

Result: 与传统的网格渲染器相比，我们的方法实现了更平滑的梯度，特别有助于使用较小批次大小进行更好的优化，具有有限的内存.

Conclusion: GMR的实现可以在https://github.com/huntorochi/Gaussian-Mesh-Renderer获取。

Abstract: 3D Gaussian Splatting (3DGS) has enabled high-fidelity virtualization with fast rendering and optimization for novel view synthesis. On the other hand, triangle mesh models still remain a popular choice for surface reconstruction but suffer from slow or heavy optimization in traditional mesh-based differentiable renderers. To address this problem, we propose a new lightweight differentiable mesh renderer leveraging the efficient rasterization process of 3DGS, named Gaussian Mesh Renderer (GMR), which tightly integrates the Gaussian and mesh representations. Each Gaussian primitive is analytically derived from the corresponding mesh triangle, preserving structural fidelity and enabling the gradient flow. Compared to the traditional mesh renderers, our method achieves smoother gradients, which especially contributes to better optimization using smaller batch sizes with limited memory. Our implementation is available in the public GitHub repository at https://github.com/huntorochi/Gaussian-Mesh-Renderer.

</details>


### [103] [Prototype Instance-semantic Disentanglement with Low-rank Regularized Subspace Clustering for WSIs Explainable Recognition](https://arxiv.org/abs/2602.14501)
*Chentao Li,Pan Huang*

Main category: cs.CV

TL;DR: 提出了一个端到端的PID-LRSC框架，用于病理诊断中的肿瘤实例语义解纠缠。


<details>
  <summary>Details</summary>
Motivation: 解决病理诊断中由于正常组织和肿瘤组织比例失衡以及肿瘤组织的高相似性所带来的实例语义纠缠问题。

Method: PID-LRSC框架包括两部分：低秩正则子空间聚类（LRSC）和原型实例语义解纠缠（PID）。其中LRSC通过二次实例子空间学习进行低秩正则化，而PID则通过增强对比学习来区分肿瘤和非肿瘤实例。

Result: PID-LRSC在多中心病理数据集上的实验表明，该方法优于其他当前最先进（SOTA）方法。

Conclusion: PID-LRSC在决策过程中能提供更清晰的实例语义，并且显著提升了辅助诊断结果的可靠性。

Abstract: The tumor region plays a key role in pathological diagnosis. Tumor tissues are highly similar to precancerous lesions and non tumor instances often greatly exceed tumor instances in whole slide images (WSIs). These issues cause instance-semantic entanglement in multi-instance learning frameworks, degrading both model representation capability and interpretability. To address this, we propose an end-to-end prototype instance semantic disentanglement framework with low-rank regularized subspace clustering, PID-LRSC, in two aspects. First, we use secondary instance subspace learning to construct low-rank regularized subspace clustering (LRSC), addressing instance entanglement caused by an excessive proportion of non tumor instances. Second, we employ enhanced contrastive learning to design prototype instance semantic disentanglement (PID), resolving semantic entanglement caused by the high similarity between tumor and precancerous tissues. We conduct extensive experiments on multicentre pathology datasets, implying that PID-LRSC outperforms other SOTA methods. Overall, PID-LRSC provides clearer instance semantics during decision-making and significantly enhances the reliability of auxiliary diagnostic outcomes.

</details>


### [104] [MedVAR: Towards Scalable and Efficient Medical Image Generation via Next-scale Autoregressive Prediction](https://arxiv.org/abs/2602.14512)
*Zhicheng He,Yunpeng Zhao,Junde Wu,Ziwei Niu,Zijun Li,Lanfen Lin,Yueming Jin*

Main category: cs.CV

TL;DR: MedVAR通过分层生成的方式，引入了一种新的自回归基础模型，能有效地合成多尺度的医学图像，适合下游应用。该模型在多个维度上超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的医学图像生成方法在可扩展性、多器官数据量和评估原则等方面存在不足，因此需要一种新的、更有效的模型。

Method: MedVAR采用了自回归预测的方式，分层次生成医学图像，生成的图像具有结构化的多尺度表示。

Result: 实验结果显示，MedVAR在保真度、多样性和扩展性等方面均超越了现有方法，可以实现高质量的医学图像生成。

Conclusion: MedVAR为未来医学生成基础模型提供了新的架构方向，并展示了其具有巨大潜力。

Abstract: Medical image generation is pivotal in applications like data augmentation for low-resource clinical tasks and privacy-preserving data sharing. However, developing a scalable generative backbone for medical imaging requires architectural efficiency, sufficient multi-organ data, and principled evaluation, yet current approaches leave these aspects unresolved. Therefore, we introduce MedVAR, the first autoregressive-based foundation model that adopts the next-scale prediction paradigm to enable fast and scale-up-friendly medical image synthesis. MedVAR generates images in a coarse-to-fine manner and produces structured multi-scale representations suitable for downstream use. To support hierarchical generation, we curate a harmonized dataset of around 440,000 CT and MRI images spanning six anatomical regions. Comprehensive experiments across fidelity, diversity, and scalability show that MedVAR achieves state-of-the-art generative performance and offers a promising architectural direction for future medical generative foundation models.

</details>


### [105] [Efficient Text-Guided Convolutional Adapter for the Diffusion Model](https://arxiv.org/abs/2602.14514)
*Aryan Das,Koushik Biswas,Swalpa Kumar Roy,Badri Narayana Patro,Vinay Kumar Verma*

Main category: cs.CV

TL;DR: 介绍了一种新型的Nexus Adapters，用于扩散模型框架中的结构保持条件生成任务，通过减少参数量和引入跨注意力机制提高性能。


<details>
  <summary>Details</summary>
Motivation: 当前的方法在条件图像生成中效率低下，且缺乏对输入提示的理解。

Method: 提出了一种嵌入跨注意力机制的高效适配器，称为Nexus Prime和Nexus Slim，能够理解输入提示并保留结构。

Result: Nexus Prime适配器相比基础模型T2I-Adapter只需增加8M参数，显著提升了性能；Nexus Slim适配器甚至减少了18M参数，仍实现了最先进的结果。

Conclusion: 该方法通过减少参数和增强提示理解能力，改善了结构保持条件生成的效果。

Abstract: We introduce the Nexus Adapters, novel text-guided efficient adapters to the diffusion-based framework for the Structure Preserving Conditional Generation (SPCG). Recently, structure-preserving methods have achieved promising results in conditional image generation by using a base model for prompt conditioning and an adapter for structure input, such as sketches or depth maps. These approaches are highly inefficient and sometimes require equal parameters in the adapter compared to the base architecture. It is not always possible to train the model since the diffusion model is itself costly, and doubling the parameter is highly inefficient. In these approaches, the adapter is not aware of the input prompt; therefore, it is optimal only for the structural input but not for the input prompt. To overcome the above challenges, we proposed two efficient adapters, Nexus Prime and Slim, which are guided by prompts and structural inputs. Each Nexus Block incorporates cross-attention mechanisms to enable rich multimodal conditioning. Therefore, the proposed adapter has a better understanding of the input prompt while preserving the structure. We conducted extensive experiments on the proposed models and demonstrated that the Nexus Prime adapter significantly enhances performance, requiring only 8M additional parameters compared to the baseline, T2I-Adapter. Furthermore, we also introduced a lightweight Nexus Slim adapter with 18M fewer parameters than the T2I-Adapter, which still achieved state-of-the-art results. Code: https://github.com/arya-domain/Nexus-Adapters

</details>


### [106] [Error Patterns in Historical OCR: A Comparative Analysis of TrOCR and a Vision-Language Model](https://arxiv.org/abs/2602.14524)
*Ari Vesalainen,Eetu Mäkelä,Laura Ruotsalainen,Mikko Tolonen*

Main category: cs.CV

TL;DR: 研究对比了TrOCR和Qwen在处理18世纪手稿文本时的效果，发现尽管Qwen在CER和WER方面表现更好且更能应对退化输入，但可能会无意识地改变历史意义的形式。TrOCR则更好地保持了原始的排版风格，但在存在连锁错误传播方面表现较差。


<details>
  <summary>Details</summary>
Motivation: 探讨现有的基于视觉语言模型的OCR技术，如Qwen和专用的OCR变压器TrOCR，在处理历史文本时各自的优势和局限性，以及它们对学术研究可能带来的影响。

Method: 进行基于长度加权的准确度比较，并采用假设驱动的错误分析。

Result: Qwen表现出较低的CER和WER值以及更好的鲁棒性，但可能会无意中改变有历史意义的形式；TrOCR保持了更好的排版准确性，但在产生连锁错误方面更容易出错。

Conclusion: 研究揭示了模型的架构内隐偏置如何系统地影响OCR错误结构，并强调了在历史数字化工作流程中进行架构意识评价的重要性。

Abstract: Optical Character Recognition (OCR) of eighteenth-century printed texts remains challenging due to degraded print quality, archaic glyphs, and non-standardized orthography. Although transformer-based OCR systems and Vision-Language Models (VLMs) achieve strong aggregate accuracy, metrics such as Character Error Rate (CER) and Word Error Rate (WER) provide limited insight into their reliability for scholarly use. We compare a dedicated OCR transformer (TrOCR) and a general-purpose Vision-Language Model (Qwen) on line-level historical English texts using length-weighted accuracy metrics and hypothesis driven error analysis.
  While Qwen achieves lower CER/WER and greater robustness to degraded input, it exhibits selective linguistic regularization and orthographic normalization that may silently alter historically meaningful forms. TrOCR preserves orthographic fidelity more consistently but is more prone to cascading error propagation. Our findings show that architectural inductive biases shape OCR error structure in systematic ways. Models with similar aggregate accuracy can differ substantially in error locality, detectability, and downstream scholarly risk, underscoring the need for architecture-aware evaluation in historical digitization workflows.

</details>


### [107] [Cross-view Domain Generalization via Geometric Consistency for LiDAR Semantic Segmentation](https://arxiv.org/abs/2602.14525)
*Jindong Zhao,Yuan Gao,Yang Xia,Sheng Nie,Jun Yue,Weiwei Sun,Shaobo Xia*

Main category: cs.CV

TL;DR: 本文提出了一种称为CVGC的方法，以解决不同视角下的LiDAR语义分割通用化问题，通过引入交叉视角几何增强模块和几何一致性模块，该方法在六个公开的LiDAR数据集上对交叉视角领域通用化进行了系统性评估，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法假设源域和目标域具有相似的采集视角，但无法处理不同采集视角下的语义分割任务，因此需要制定一种新的框架以实现跨视角领域的语义分割模型通用化。

Method: 本文提出一种名为CVGC的框架，包括交叉视角几何增强模块和几何一致性模块。几何增强模块建模由于视角不同带来的可见性和采样密度变化，生成多个相同场景的跨视角观测。几何一致性模块确保几何增强后的点云具有一致的语义和占用预测。

Result: CVGC在六个公开的LiDAR数据集上的实验表明，其在单个源域向多个具有不同采集视角的目标域通用化时优于现有方法。

Conclusion: 研究结果证明了CVGC框架在跨视角领域通用化LiDAR语义分割任务上的有效性，为实际应用中的LiDAR传感器提供了鲁棒的语义分割模型。

Abstract: Domain-generalized LiDAR semantic segmentation (LSS) seeks to train models on source-domain point clouds that generalize reliably to multiple unseen target domains, which is essential for real-world LiDAR applications. However, existing approaches assume similar acquisition views (e.g., vehicle-mounted) and struggle in cross-view scenarios, where observations differ substantially due to viewpoint-dependent structural incompleteness and non-uniform point density. Accordingly, we formulate cross-view domain generalization for LiDAR semantic segmentation and propose a novel framework, termed CVGC (Cross-View Geometric Consistency). Specifically, we introduce a cross-view geometric augmentation module that models viewpoint-induced variations in visibility and sampling density, generating multiple cross-view observations of the same scene. Subsequently, a geometric consistency module enforces consistent semantic and occupancy predictions across geometrically augmented point clouds of the same scene. Extensive experiments on six public LiDAR datasets establish the first systematic evaluation of cross-view domain generalization for LiDAR semantic segmentation, demonstrating that CVGC consistently outperforms state-of-the-art methods when generalizing from a single source domain to multiple target domains with heterogeneous acquisition viewpoints. The source code will be publicly available at https://github.com/KintomZi/CVGC-DG

</details>


### [108] [MoRL: Reinforced Reasoning for Unified Motion Understanding and Generation](https://arxiv.org/abs/2602.14534)
*Hongpeng Wang,Zeyu Zhang,Wenhao Li,Hao Tang*

Main category: cs.CV

TL;DR: MoRL 是一种结合监督微调和基于验证奖励的强化学习的统一多模态运动模型，通过引入 Chain-of-Motion 方法提升推理能力，同时构建了两个大规模的因果推理数据集 MoUnd-CoT-140K 和 MoGen-CoT-140K，实验结果显示 MoRL 在 HumanML3D 和 KIT-ML 上表现出显著优于现有基线方法的性能。


<details>
  <summary>Details</summary>
Motivation: 当前的人机运动理解生成技术在逻辑推理和测试时规划能力上存在限制，旨在开发一种能够提升这两种能力的统一模型。

Method: MoRL 使用监督微调与强化学习结合的方法，引入因果推理链 (Chain-of-Motion) 以及特定任务的奖励机制，确保运动理解的语义对齐和逻辑连贯性，同时保证运动生成的物理合理性与文本-运动一致。

Result: MoRL 在 HumanML3D 和 KIT-ML 两个数据集上的实验结果显示其在逻辑推理和感知现实主义方面取得了显著进步，同时引入的因果推理链方法也提升了推理能力。

Conclusion: MoRL 模型通过引入强化学习和因果推理链方法，展示了在人机运动理解生成任务上的优越性能。未来可能会探索更多复杂场景和更精确的因果推理机制。

Abstract: Human motion understanding and generation are crucial for vision and robotics but remain limited in reasoning capability and test-time planning. We propose MoRL, a unified multimodal motion model trained with supervised fine-tuning and reinforcement learning with verifiable rewards. Our task-specific reward design combines semantic alignment and reasoning coherence for understanding with physical plausibility and text-motion consistency for generation, improving both logical reasoning and perceptual realism. To further enhance inference, we introduce Chain-of-Motion (CoM), a test-time reasoning method that enables step-by-step planning and reflection. We also construct two large-scale CoT datasets, MoUnd-CoT-140K and MoGen-CoT-140K, to align motion sequences with reasoning traces and action descriptions. Experiments on HumanML3D and KIT-ML show that MoRL achieves significant gains over state-of-the-art baselines. Code: https://github.com/AIGeeksGroup/MoRL. Website: https://aigeeksgroup.github.io/MoRL.

</details>


### [109] [OmniVTON++: Training-Free Universal Virtual Try-On with Principal Pose Guidance](https://arxiv.org/abs/2602.14552)
*Zhaotong Yang,Yong Du,Shengfeng He,Yuhui Li,Xinzhe Li,Yangyang Xu,Junyu Dong,Jian Yang*

Main category: cs.CV

TL;DR: OmniVTON++是一种无需训练的虚拟试穿框架，能够解决服装对齐、人体结构一致性和边界连续性等挑战，适用于多种应用场景，表现领先于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的虚拟试穿方法通常针对特定的数据条件进行优化，导致它们难以广泛适用并需要重新训练。本文旨在开发一种无需训练的通用虚拟试穿框架，以解决服装试穿中的常见问题。

Method: OmniVTON++通过结构服装形态学、主姿势引导和连续边界缝合等方法协调多项技术，形成一个无需特定任务重新训练的统一管道。

Result: 实验结果显示，OmniVTON++在多种泛化设定下表现最优，支持单件服装、多人和动漫角色的虚拟试穿，扩展了虚拟试穿的应用范围。

Conclusion: 该研究提出了一种创新的虚拟试穿框架，能够提供更广泛的适用性和更出色的表现，未来有望进一步提升虚拟试穿技术的普及和应用。

Abstract: Image-based Virtual Try-On (VTON) concerns the synthesis of realistic person imagery through garment re-rendering under human pose and body constraints. In practice, however, existing approaches are typically optimized for specific data conditions, making their deployment reliant on retraining and limiting their generalization as a unified solution. We present OmniVTON++, a training-free VTON framework designed for universal applicability. It addresses the intertwined challenges of garment alignment, human structural coherence, and boundary continuity by coordinating Structured Garment Morphing for correspondence-driven garment adaptation, Principal Pose Guidance for step-wise structural regulation during diffusion sampling, and Continuous Boundary Stitching for boundary-aware refinement, forming a cohesive pipeline without task-specific retraining. Experimental results demonstrate that OmniVTON++ achieves state-of-the-art performance across diverse generalization settings, including cross-dataset and cross-garment-type evaluations, while reliably operating across scenarios and diffusion backbones within a single formulation. In addition to single-garment, single-human cases, the framework supports multi-garment, multi-human, and anime character virtual try-on, expanding the scope of virtual try-on applications. The source code will be released to the public.

</details>


### [110] [DriveFine: Refining-Augmented Masked Diffusion VLA for Precise and Robust Driving](https://arxiv.org/abs/2602.14577)
*Chenxu Dang,Sining Ang,Yongkang Li,Haochen Tian,Jie Wang,Guang Li,Hangjun Ye,Jie Ma,Long Chen,Yan Wang*

Main category: cs.CV

TL;DR: DriveFine 是一种结合了灵活解码与自我纠错能力的掩码扩散VLA模型，通过引入模块化混合专家机制，并设计一种混合强化学习策略进行训练，以期提升模型的灵活性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 目前的Vision-Language-Action模型存在模态对齐困难、训练效率低下、泛化能力有限等问题；DriveFine旨在解决这些挑战，通过集成模块化混合专家机制和混合强化学习策略，使模型具备更好的解码灵活性和自我纠错能力。

Method: DriveFine利用掩码扩散机制进行初始生成，并在此基础上添加了一个灵活的模块化混合专家块，该块可以在推理过程中通过显式专家选择和训练中的梯度阻断来实现解码专家与修正专家的解耦。此外，通过混合强化学习策略来促进修正专家的有效探索同时保持训练稳定性。

Result: 在NAVSIM v1, v2, Navhard等基准测试上进行全面实验后，DriveFine的表现证明了其强大的效果和鲁棒性。

Conclusion: DriveFine通过模块化混合专家机制和混合强化学习策略的结合，成功改进了VLA模型的灵活性和泛化能力，展示了在受到挑战的场景下应用的有效性。

Abstract: Vision-Language-Action (VLA) models for autonomous driving increasingly adopt generative planners trained with imitation learning followed by reinforcement learning. Diffusion-based planners suffer from modality alignment difficulties, low training efficiency, and limited generalization. Token-based planners are plagued by cumulative causal errors and irreversible decoding. In summary, the two dominant paradigms exhibit complementary strengths and weaknesses. In this paper, we propose DriveFine, a masked diffusion VLA model that combines flexible decoding with self-correction capabilities. In particular, we design a novel plug-and-play block-MoE, which seamlessly injects a refinement expert on top of the generation expert. By enabling explicit expert selection during inference and gradient blocking during training, the two experts are fully decoupled, preserving the foundational capabilities and generic patterns of the pretrained weights, which highlights the flexibility and extensibility of the block-MoE design. Furthermore, we design a hybrid reinforcement learning strategy that encourages effective exploration of refinement expert while maintaining training stability. Extensive experiments on NAVSIM v1, v2, and Navhard benchmarks demonstrate that DriveFine exhibits strong efficacy and robustness. The code will be released at https://github.com/MSunDYY/DriveFine.

</details>


### [111] [YOLO26: A Comprehensive Architecture Overview and Key Improvements](https://arxiv.org/abs/2602.14582)
*Priyanto Hidayatullah,Refdinal Tubagus*

Main category: cs.CV

TL;DR: YOLO26引入了ProgLoss+STAL、MuSGD优化器等创新，实现了CPU模式下43%的推理速度提升，并确保在无GPU设备上也能实现即时性能。


<details>
  <summary>Details</summary>
Motivation: Yolo系列模型的进一步改进旨在提高推理速度和兼容性，特别强调能够在无GPU设备上实现即时性能。

Method: 通过详细分析YOLO26的源代码和官方文档，揭示了YOLO26的具体操作机制，其核心在于开发了一张YOLO26的架构图。

Result: Yolo26实现了显著的性能提升，在多个计算机视觉任务上表现出色，同时提供了一张详细的架构图。

Conclusion: 研究为研究人员和开发人员提供了一个精确的架构理解，有助于改进YOLO模型并保持其在计算机视觉领域的领先地位。

Abstract: You Only Look Once (YOLO) has been the prominent model for computer vision in deep learning for a decade. This study explores the novel aspects of YOLO26, the most recent version in the YOLO series. The elimination of Distribution Focal Loss (DFL), implementation of End-to-End NMS-Free Inference, introduction of ProgLoss + Small-Target-Aware Label Assignment (STAL), and use of the MuSGD optimizer are the primary enhancements designed to improve inference speed, which is claimed to achieve a 43% boost in CPU mode. This is designed to allow YOLO26 to attain real-time performance on edge devices or those without GPUs. Additionally, YOLO26 offers improvements in many computer vision tasks, including instance segmentation, pose estimation, and oriented bounding box (OBB) decoding. We aim for this effort to provide more value than just consolidating information already included in the existing technical documentation. Therefore, we performed a rigorous architectural investigation into YOLO26, mostly using the source code available in its GitHub repository and its official documentation. The authentic and detailed operational mechanisms of YOLO26 are inside the source code, which is seldom extracted by others. The YOLO26 architectural diagram is shown as the outcome of the investigation. This study is, to our knowledge, the first one presenting the CNN-based YOLO26 architecture, which is the core of YOLO26. Our objective is to provide a precise architectural comprehension of YOLO26 for researchers and developers aspiring to enhance the YOLO model, ensuring it remains the leading deep learning model in computer vision.

</details>


### [112] [VariViT: A Vision Transformer for Variable Image Sizes](https://arxiv.org/abs/2602.14615)
*Aswathi Varma,Suprosanna Shit,Chinmay Prabhakar,Daniel Scholz,Hongwei Bran Li,Bjoern Menze,Daniel Rueckert,Benedikt Wiestler*

Main category: cs.CV

TL;DR: 提出了一种名为VariViT的改进型Vision Transformer，专门用于处理不同大小的医学图像，同时保持一致的分割单元大小，提高特征表示能力。实验结果表明，该模型在脑肿瘤分类和胶质瘤基因型预测中优于标准ViT和ResNet模型。


<details>
  <summary>Details</summary>
Motivation: 当前的Vision Transformers由于固定尺寸的图像分割，导致医学图像处理存在挑战，尤其是面对不规则形状的结构如肿瘤。因此，需要一种能适应可变大小图像的模型。

Method: VariViT采用了一个创新的位置嵌入调整方案，处理可变数量的分割单元。设计了一种新的批次处理策略，以减少计算复杂性，实现更快的训练和推理时间。

Result: 在两个3D脑MRI数据集上的实验表明，VariViT在胶质瘤基因型预测和脑肿瘤分类任务中优于标准ViT和ResNet模型，分别获得75.5%和76.3%的F1分数。其提出的批次策略比传统架构减少了高达30%的计算时间。

Conclusion: 研究结果表明，VariViT在图像表示学习方面表现出色。

Abstract: Vision Transformers (ViTs) have emerged as the state-of-the-art architecture in representation learning, leveraging self-attention mechanisms to excel in various tasks. ViTs split images into fixed-size patches, constraining them to a predefined size and necessitating pre-processing steps like resizing, padding, or cropping. This poses challenges in medical imaging, particularly with irregularly shaped structures like tumors. A fixed bounding box crop size produces input images with highly variable foreground-to-background ratios. Resizing medical images can degrade information and introduce artefacts, impacting diagnosis. Hence, tailoring variable-sized crops to regions of interest can enhance feature representation capabilities. Moreover, large images are computationally expensive, and smaller sizes risk information loss, presenting a computation-accuracy tradeoff. We propose VariViT, an improved ViT model crafted to handle variable image sizes while maintaining a consistent patch size. VariViT employs a novel positional embedding resizing scheme for a variable number of patches. We also implement a new batching strategy within VariViT to reduce computational complexity, resulting in faster training and inference times. In our evaluations on two 3D brain MRI datasets, VariViT surpasses vanilla ViTs and ResNet in glioma genotype prediction and brain tumor classification. It achieves F1-scores of 75.5% and 76.3%, respectively, learning more discriminative features. Our proposed batching strategy reduces computation time by up to 30% compared to conventional architectures. These findings underscore the efficacy of VariViT in image representation learning. Our code can be found here: https://github.com/Aswathi-Varma/varivit

</details>


### [113] [VIGIL: Tackling Hallucination Detection in Image Recontextualization](https://arxiv.org/abs/2602.14633)
*Joanna Wojciechowicz,Maria Łubniewska,Jakub Antczak,Justyna Baczyńska,Wojciech Gromski,Wojciech Kozłowski,Maciej Zięba*

Main category: cs.CV

TL;DR: VIGIL是一个新的基准数据集和框架，旨在细化分类多模态图像重新上下文化任务中大型多模态模型的幻觉，并提供多阶段检测管道以提高模型的解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的研究往往将幻觉视为一个统一的问题，但VIGIL旨在通过细化和分类幻觉的具体类型来解决多模态评估中的重要差距，从而促进大型多模态模型的理解和改进。

Method: VIGIL提出了一个多阶段的检测管道，通过专门的步骤针对对象级别的保真度、背景一致性以及遗漏检测，利用开源模型的联合ensemble来提升检测效果。

Result: 该方法通过广泛的实验评估证明了其有效性，并通过细分和分类幻觉类型，填补了该领域的一项空白。

Conclusion: VIGIL开放了数据集、检测管道和基准代码，以促进进一步的透明度和更深入的研究，为多模态图像重新上下文化任务提供了新的基准框架。

Abstract: We introduce VIGIL (Visual Inconsistency & Generative In-context Lucidity), the first benchmark dataset and framework providing a fine-grained categorization of hallucinations in the multimodal image recontextualization task for large multimodal models (LMMs). While existing research often treats hallucinations as a uniform issue, our work addresses a significant gap in multimodal evaluation by decomposing these errors into five categories: pasted object hallucinations, background hallucinations, object omission, positional & logical inconsistencies, and physical law violations. To address these complexities, we propose a multi-stage detection pipeline. Our architecture processes recontextualized images through a series of specialized steps targeting object-level fidelity, background consistency, and omission detection, leveraging a coordinated ensemble of open-source models, whose effectiveness is demonstrated through extensive experimental evaluations. Our approach enables a deeper understanding of where the models fail with an explanation; thus, we fill a gap in the field, as no prior methods offer such categorization and decomposition for this task. To promote transparency and further exploration, we openly release VIGIL, along with the detection pipeline and benchmark code, through our GitHub repository: https://github.com/mlubneuskaya/vigil and Data repository: https://huggingface.co/datasets/joannaww/VIGIL.

</details>


### [114] [SketchingReality: From Freehand Scene Sketches To Photorealistic Images](https://arxiv.org/abs/2602.14648)
*Ahmed Bourouis,Mikhail Bessmeltsev,Yulia Gryaditskaya*

Main category: cs.CV

TL;DR: 本文介绍了一种利用自由手绘草图生成图像的方法，解决了传统算法对真实手绘草图处理不佳的问题。该方法通过优先考虑草图的语义解释而非严格的边缘位置匹配，引入了一种新的损失函数，无需真实图像即可训练。


<details>
  <summary>Details</summary>
Motivation: 当前大多数研究集中在边缘图上，而真实的手绘草图因其抽象性和变形特性未得到充分研究。因此，该研究旨在开发一种能够平衡照片现实度与草图一致性的方法。

Method: 本文提出了一种基于调制的方法，该方法强调了对草图的语义理解而非严格依赖单个边缘位置。

Result: 该方法在处理自由手绘草图生成图像方面优于现有方法，实现了更好的语义对齐和生成图像的现实度及整体质量。

Conclusion: 本文建立了一种新方法，解决了真实手绘草图生成的难题，提升了生成图像的质量。

Abstract: Recent years have witnessed remarkable progress in generative AI, with natural language emerging as the most common conditioning input. As underlying models grow more powerful, researchers are exploring increasingly diverse conditioning signals, such as depth maps, edge maps, camera parameters, and reference images, to give users finer control over generation. Among different modalities, sketches are a natural and long-standing form of human communication, enabling rapid expression of visual concepts. Previous literature has largely focused on edge maps, often misnamed 'sketches', yet algorithms that effectively handle true freehand sketches, with their inherent abstraction and distortions, remain underexplored. We pursue the challenging goal of balancing photorealism with sketch adherence when generating images from freehand input. A key obstacle is the absence of ground-truth, pixel-aligned images: by their nature, freehand sketches do not have a single correct alignment. To address this, we propose a modulation-based approach that prioritizes semantic interpretation of the sketch over strict adherence to individual edge positions. We further introduce a novel loss that enables training on freehand sketches without requiring ground-truth pixel-aligned images. We show that our method outperforms existing approaches in both semantic alignment with freehand sketch inputs and in the realism and overall quality of the generated images.

</details>


### [115] [Advances in Global Solvers for 3D Vision](https://arxiv.org/abs/2602.14662)
*Zhenjun Zhao,Heng Yang,Bangyan Liao,Yingping Zeng,Shaocheng Yan,Yingdong Gu,Peidong Liu,Yi Zhou,Haoang Li,Javier Civera*

Main category: cs.CV

TL;DR: 该综述首次系统地回顾了全局优化求解器在几何视觉中的应用，涵盖了分支定界（BnB）、凸松弛（CR）和递减非凸性（GNC）三大核心范式。文章分析了这些方法的理论基础、算法设计及鲁棒性和扩展性改进，探索了它们如何解决几何估计问题中的非凸性问题，同时指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 为了提供一种统一的视角和指南，使读者能够理解并选择适合各类3D视觉任务的全局优化求解器，从而实现计算上可信的感知。

Method: 该综述采用文献回顾的方法，对分支定界（BnB）、凸松弛（CR）和递减非凸性（GNC）三种全局优化方法进行了全面的梳理。同时，通过具体的视觉任务分析不同方法的优势和局限性，提供了一种观点框架来指导实际应用。

Result: 综述揭示了各种全局优化方法在各类核心3D视觉任务（如Wahba问题、多视图重建等）中的表现，特别是它们在最优性、鲁棒性和扩展性之间的权衡关系。

Conclusion: 该综述识别出了未来研究的四个关键方向：算法的可扩展性和保持算法上的保证、数据驱动先验与可信优化的集成、标准化基准的建立、以及社会影响的研究。

Abstract: Global solvers have emerged as a powerful paradigm for 3D vision, offering certifiable solutions to nonconvex geometric optimization problems traditionally addressed by local or heuristic methods. This survey presents the first systematic review of global solvers in geometric vision, unifying the field through a comprehensive taxonomy of three core paradigms: Branch-and-Bound (BnB), Convex Relaxation (CR), and Graduated Non-Convexity (GNC). We present their theoretical foundations, algorithmic designs, and practical enhancements for robustness and scalability, examining how each addresses the fundamental nonconvexity of geometric estimation problems. Our analysis spans ten core vision tasks, from Wahba problem to bundle adjustment, revealing the optimality-robustness-scalability trade-offs that govern solver selection. We identify critical future directions: scaling algorithms while maintaining guarantees, integrating data-driven priors with certifiable optimization, establishing standardized benchmarks, and addressing societal implications for safety-critical deployment. By consolidating theoretical foundations, practical advances, and broader impacts, this survey provides a unified perspective and roadmap toward certifiable, trustworthy perception for real-world applications. A continuously-updated literature summary and companion code tutorials are available at https://github.com/ericzzj1989/Awesome-Global-Solvers-for-3D-Vision.

</details>


### [116] [MeFEm: Medical Face Embedding model](https://arxiv.org/abs/2602.14672)
*Yury Borets,Stepan Botman*

Main category: cs.CV

TL;DR: MeFEm是一种基于改进的JEPA架构的视图模型，专注于面部图像的生物医学分析。通过轴向条纹遮罩策略、圆损失加权方案和对CLS标记的概率重组，MeFEm在数据分析任务中表现优异，特别是在BMI估计方面。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在开发一种能够更准确分析面部图像以进行生物医学分析的模型，特别是解决现有数据集中的领域偏差问题。

Method: 研究团队对JEPA架构进行了多项改进，包括引入轴向条纹遮罩策略、实施圆损失加权方案以及进行CLS标记的概率重新分配，以提高模型的性能。

Result: MeFEm在核心分析任务上优于如FaRL和Franca等基准模型，尤其是在使用较少数据的情况下。此外，MeFEm在BMI估算任务上也表现出色，并且评估数据集是一个新颖且封闭源的数据集，能够解决现有数据集中的领域偏差。

Conclusion: MeFEm提供了强大的基线模型，供未来该领域的研究者使用，模型权重可以在Hugging Face平台下载。

Abstract: We present MeFEm, a vision model based on a modified Joint Embedding Predictive Architecture (JEPA) for biometric and medical analysis from facial images. Key modifications include an axial stripe masking strategy to focus learning on semantically relevant regions, a circular loss weighting scheme, and the probabilistic reassignment of the CLS token for high quality linear probing. Trained on a consolidated dataset of curated images, MeFEm outperforms strong baselines like FaRL and Franca on core anthropometric tasks despite using significantly less data. It also shows promising results on Body Mass Index (BMI) estimation, evaluated on a novel, consolidated closed-source dataset that addresses the domain bias prevalent in existing data. Model weights are available at https://huggingface.co/boretsyury/MeFEm , offering a strong baseline for future work in this domain.

</details>


### [117] [Universal Image Immunization against Diffusion-based Image Editing via Semantic Injection](https://arxiv.org/abs/2602.14679)
*Chanhui Lee,Seunghyun Shin,Donggyu Choi,Hae-gon Jeon,Jeany Son*

Main category: cs.CV

TL;DR: 本文提出了一种基于扩散模型的攻击防御方法，通过生成一种通用的对抗扰动来保护图像不受恶意编辑影响。


<details>
  <summary>Details</summary>
Motivation: 现有的图像免疫方法依赖于图像特定的对抗扰动，这限制了其在实际应用中的实用性和可扩展性。

Method: 采用通用对抗扰动（UAP）技术，该方法能在不依赖特定数据或领域知识的情况下，在不访问训练数据的情况下，生成一种通用的对抗扰动，并将其嵌入到图像中。该扰动能够在保持原始图像内容不变的同时，引导扩散模型将编辑过程的注意力引导到安全的方向。

Result: 在通用对抗扰动设置下，该方法显著优于多种基线方法。即使在对抗扰动预算受限的情况下，该方法也能实现与图像特定方法相近的表现，并且具有较强黑盒转移性。

Conclusion: 本文提出的方法有效保护了图像免受恶意编辑，并且具有良好的实用性、实用性和应对不同扩散模型的适应性。

Abstract: Recent advances in diffusion models have enabled powerful image editing capabilities guided by natural language prompts, unlocking new creative possibilities. However, they introduce significant ethical and legal risks, such as deepfakes and unauthorized use of copyrighted visual content. To address these risks, image immunization has emerged as a promising defense against AI-driven semantic manipulation. Yet, most existing approaches rely on image-specific adversarial perturbations that require individual optimization for each image, thereby limiting scalability and practicality. In this paper, we propose the first universal image immunization framework that generates a single, broadly applicable adversarial perturbation specifically designed for diffusion-based editing pipelines. Inspired by universal adversarial perturbation (UAP) techniques used in targeted attacks, our method generates a UAP that embeds a semantic target into images to be protected. Simultaneously, it suppresses original content to effectively misdirect the model's attention during editing. As a result, our approach effectively blocks malicious editing attempts by overwriting the original semantic content in the image via the UAP. Moreover, our method operates effectively even in data-free settings without requiring access to training data or domain knowledge, further enhancing its practicality and broad applicability in real-world scenarios. Extensive experiments show that our method, as the first universal immunization approach, significantly outperforms several baselines in the UAP setting. In addition, despite the inherent difficulty of universal perturbations, our method also achieves performance on par with image-specific methods under a more restricted perturbation budget, while also exhibiting strong black-box transferability across different diffusion models.

</details>


### [118] [It's a Matter of Time: Three Lessons on Long-Term Motion for Perception](https://arxiv.org/abs/2602.14705)
*Willem Davison,Xinyue Hao,Laura Sevilla-Lara*

Main category: cs.CV

TL;DR: 该研究通过点追踪估计技术探索长时间运动信息在感知任务中的潜力，发现长时间运动信息能提供比图像更好的行动、物体、材质和空间信息，更能泛化且计算效率更高。


<details>
  <summary>Details</summary>
Motivation: 长时间运动信息在感知任务中的重要性未得到充分的认识，因此本研究旨在通过点追踪估计技术来探索长时间运动信息的价值。

Method: 研究利用了点追踪估计技术，对多种感知任务进行了实验。

Result: 研究结果表明，长时间运动信息可以提供更多关于行动、物体、材质和空间的信息，相比于图像信息，它在低数据设置和零样本任务中具有更好的泛化能力。此外，运动信息的低维度使其在计算效率和精度之间有更好的权衡。

Conclusion: 本研究表明长时间运动信息对于感知任务具有重要价值，为未来利用长时间运动信息的设计提供了方向。

Abstract: Temporal information has long been considered to be essential for perception. While there is extensive research on the role of image information for perceptual tasks, the role of the temporal dimension remains less well understood: What can we learn about the world from long-term motion information? What properties does long-term motion information have for visual learning? We leverage recent success in point-track estimation, which offers an excellent opportunity to learn temporal representations and experiment on a variety of perceptual tasks. We draw 3 clear lessons: 1) Long-term motion representations contain information to understand actions, but also objects, materials, and spatial information, often even better than images. 2) Long-term motion representations generalize far better than image representations in low-data settings and in zero-shot tasks. 3) The very low dimensionality of motion information makes motion representations a better trade-off between GFLOPs and accuracy than standard video representations, and used together they achieve higher performance than video representations alone. We hope these insights will pave the way for the design of future models that leverage the power of long-term motion information for perception.

</details>


### [119] [SAILS: Segment Anything with Incrementally Learned Semantics for Task-Invariant and Training-Free Continual Learning](https://arxiv.org/abs/2602.14767)
*Shishir Muralidhara,Didier Stricker,René Schuster*

Main category: cs.CV

TL;DR: SAILS 提出了一种无需增量训练即可实现类别增量语义分割（CISS）的方法，通过分段任意模型（SAM）的零样本区域提取和固定特征空间中的原型语义关联实现


<details>
  <summary>Details</summary>
Motivation: 当前连续学习方法受限于重复重新训练、高计算成本和持续的泛化能力降低问题，即‘灾难性遗忘’。为了克服这些挑战，作者引入了SAILS框架，旨在实现一种无需增量训练即可实现类别增量语义分割的技术。

Method: SAILS 采用分段任意模型（SAM）进行零样本区域提取，然后通过固定特征空间中的原型进行语义关联。这种框架不依赖于增量训练，从而避免了‘灾难性遗忘’的问题。

Result: 与基于训练的方法相比，SAILS 在标准 CISS 数据集上表现出色，特别是在长期和具有挑战性的任务序列中，更有效地避免了泛化能力的下降。

Conclusion: SAILS 通过避免参数更新完全消除了遗忘问题，并保持了任务不变性的性能。此外，新类别的引入还能增强先前类别的性能，展示了正向的反向传播效果。

Abstract: Continual learning remains constrained by the need for repeated retraining, high computational costs, and the persistent challenge of forgetting. These factors significantly limit the applicability of continuous learning in real-world settings, as iterative model updates require significant computational resources and inherently exacerbate forgetting. We present SAILS -- Segment Anything with Incrementally Learned Semantics, a training-free framework for Class-Incremental Semantic Segmentation (CISS) that sidesteps these challenges entirely. SAILS leverages foundational models to decouple CISS into two stages: Zero-shot region extraction using Segment Anything Model (SAM), followed by semantic association through prototypes in a fixed feature space. SAILS incorporates selective intra-class clustering, resulting in multiple prototypes per class to better model intra-class variability. Our results demonstrate that, despite requiring no incremental training, SAILS typically surpasses the performance of existing training-based approaches on standard CISS datasets, particularly in long and challenging task sequences where forgetting tends to be most severe. By avoiding parameter updates, SAILS completely eliminates forgetting and maintains consistent, task-invariant performance. Furthermore, SAILS exhibits positive backward transfer, where the introduction of new classes can enhance performance on previous classes.

</details>


### [120] [GOT-JEPA: Generic Object Tracking with Model Adaptation and Occlusion Handling using Joint-Embedding Predictive Architecture](https://arxiv.org/abs/2602.14771)
*Shih-Fang Chen,Jun-Cheng Chen,I-Hong Jhuo,Yen-Yu Lin*

Main category: cs.CV

TL;DR: 该研究提出了一种GOT-JEPA框架，通过预测跟踪模型而非图像特征，增强了目标跟踪器在复杂场景中的鲁棒性和泛化能力。基于此框架构建的OccuSolver进一步提升了遮挡感知能力。


<details>
  <summary>Details</summary>
Motivation: 现有目标跟踪器在复杂场景下表现不佳，特别是在遮挡和稀疏信息条件下，导致其泛化能力和鲁棒性有限。

Method: GOT-JEPA框架通过教师预测器和学生预测器共同训练，学习在遮挡、干扰和其他不良观测条件下的可靠跟踪模型生成。

Result: GOT-JEPA框架及其构建的OccuSolver在七个基准测试集上的评估显示，方法有效提升了跟踪器的泛化能力和鲁棒性。

Conclusion: 该研究为目标跟踪领域提出了新的方法，能够更准确地捕捉遮挡模式，提高在动态环境下的跟踪精度，为未来的研究提供了新的思路。

Abstract: The human visual system tracks objects by integrating current observations with previously observed information, adapting to target and scene changes, and reasoning about occlusion at fine granularity. In contrast, recent generic object trackers are often optimized for training targets, which limits robustness and generalization in unseen scenarios, and their occlusion reasoning remains coarse, lacking detailed modeling of occlusion patterns. To address these limitations in generalization and occlusion perception, we propose GOT-JEPA, a model-predictive pretraining framework that extends JEPA from predicting image features to predicting tracking models. Given identical historical information, a teacher predictor generates pseudo-tracking models from a clean current frame, and a student predictor learns to predict the same pseudo-tracking models from a corrupted version of the current frame. This design provides stable pseudo supervision and explicitly trains the predictor to produce reliable tracking models under occlusions, distractors, and other adverse observations, improving generalization to dynamic environments. Building on GOT-JEPA, we further propose OccuSolver to enhance occlusion perception for object tracking. OccuSolver adapts a point-centric point tracker for object-aware visibility estimation and detailed occlusion-pattern capture. Conditioned on object priors iteratively generated by the tracker, OccuSolver incrementally refines visibility states, strengthens occlusion handling, and produces higher-quality reference labels that progressively improve subsequent model predictions. Extensive evaluations on seven benchmarks show that our method effectively enhances tracker generalization and robustness.

</details>


### [121] [Debiasing Central Fixation Confounds Reveals a Peripheral "Sweet Spot" for Human-like Scanpaths in Hard-Attention Vision](https://arxiv.org/abs/2602.14834)
*Pengcheng Pan,Yonekura Shogo,Yasuo Kuniyosh*

Main category: cs.CV

TL;DR: 本研究通过分析视觉识别中的凝视路径，发现中心偏差对扫描路径评估的影响，并提出了一种去中心偏差的综合指标GCS，能够识别出有利于同时兼顾焦点视野和外围视野的扫描路径的中等区域大小，该发现未在原始指标和准确率中明显显现。


<details>
  <summary>Details</summary>
Motivation: 为了解决当前视觉注意力模型在对象中心数据集中评估中普遍存在的中心偏差问题，以更准确地评估模型的真实行为一致性。

Method: 首先使用Gaze-CIFAR-10数据集进行基准测试，展示中心固定基线的高得分；然后，通过调整视场中的焦点区域大小和外围上下文，分析感知类型的凝视路径；最后，提出了一种去中心偏差的综合指标GCS，评估扫描路径的真实行为一致性。

Result: 研究表明，存在一定范围的感官约束可以使扫描路径同时高于中心基线（去偏差化）且在运动统计数据上具有人类般的时间特性。GCS指标能够在焦点视野与外围视野之间找到一个稳健的最优区域，这在原来的指标和准确率中并未明显显现。

Conclusion: 提出GCS作为去中心偏差的综合评估指标，有助于更准确地评估在对象为中心的数据集上进行感知任务的实时行为一致性，同时也揭示了视野过大时的行为快捷方式。

Abstract: Human eye movements in visual recognition reflect a balance between foveal sampling and peripheral context. Task-driven hard-attention models for vision are often evaluated by how well their scanpaths match human gaze. However, common scanpath metrics can be strongly confounded by dataset-specific center bias, especially on object-centric datasets. Using Gaze-CIFAR-10, we show that a trivial center-fixation baseline achieves surprisingly strong scanpath scores, approaching many learned policies. This makes standard metrics optimistic and blurs the distinction between genuine behavioral alignment and mere central tendency. We then analyze a hard-attention classifier under constrained vision by sweeping foveal patch size and peripheral context, revealing a peripheral sweet spot: only a narrow range of sensory constraints yields scanpaths that are simultaneously (i) above the center baseline after debiasing and (ii) temporally human-like in movement statistics. To address center bias, we propose GCS (Gaze Consistency Score), a center-debiased composite metric augmented with movement similarity. GCS uncovers a robust sweet spot at medium patch size with both foveal and peripheral vision, that is not obvious from raw scanpath metrics or accuracy alone, and also highlights a "shortcut regime" when the field-of-view becomes too large. We discuss implications for evaluating active perception on object-centric datasets and for designing gaze benchmarks that better separate behavioral alignment from center bias.

</details>


### [122] [Integrating Affordances and Attention models for Short-Term Object Interaction Anticipation](https://arxiv.org/abs/2602.14837)
*Lorenzo Mur Labadia,Ruben Martinez-Cantin,Jose J. Guerrero,Giovanni M. Farinella,Antonino Furnari*

Main category: cs.CV

TL;DR: 该研究提出了一种新的方法，旨在通过结合帧引导的时间池化、图像-视频双模态注意力和多尺度特征融合来提高短期物体交互预测的性能。同时，引入了环境功能模型和交互热点预测模块来更好地理解用户行为，从而提升预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 当前的短期对象交互预测（STA）技术对于理解和预测用户的短期目标至关重要，特别是在可穿戴辅助设备和人机交互场景中。

Method: 研究提出了两种新型的基于注意力的架构：STAformer 和 STAformer plus plus。STAformer 结合了帧引导的时间池化、双模态注意力以及多尺度特征融合。此外，还引入了两种新模块：环境功能模型和交互热点预测模块。

Result: 实验结果显示，与现有的方法相比，在Ego4D数据集上，该方法的总体Top-5 mAP提高了23个百分点，而在EPIC-Kitchens STA标签数据集上则提高了31个百分点。

Conclusion: 该研究提出的方法显著提升了短期对象交互预测的性能，为未来的相关研究提供了有价值的参考和基础数据。

Abstract: Short Term object-interaction Anticipation consists in detecting the location of the next active objects, the noun and verb categories of the interaction, as well as the time to contact from the observation of egocentric video. This ability is fundamental for wearable assistants to understand user goals and provide timely assistance, or to enable human-robot interaction. In this work, we present a method to improve the performance of STA predictions. Our contributions are two-fold: 1 We propose STAformer and STAformer plus plus, two novel attention-based architectures integrating frame-guided temporal pooling, dual image-video attention, and multiscale feature fusion to support STA predictions from an image-input video pair; 2 We introduce two novel modules to ground STA predictions on human behavior by modeling affordances. First, we integrate an environment affordance model which acts as a persistent memory of interactions that can take place in a given physical scene. We explore how to integrate environment affordances via simple late fusion and with an approach which adaptively learns how to best fuse affordances with end-to-end predictions. Second, we predict interaction hotspots from the observation of hands and object trajectories, increasing confidence in STA predictions localized around the hotspot. Our results show significant improvements on Overall Top-5 mAP, with gain up to +23p.p on Ego4D and +31p.p on a novel set of curated EPIC-Kitchens STA labels. We released the code, annotations, and pre-extracted affordances on Ego4D and EPIC-Kitchens to encourage future research in this area.

</details>


### [123] [Multi-dimensional Persistent Sheaf Laplacians for Image Analysis](https://arxiv.org/abs/2602.14846)
*Xiang Xiang Wang,Guo-Wei Wei*

Main category: cs.CV

TL;DR: 提出了一种基于单纯形复形的多维持久层拉普拉斯（MPSL）框架，用于图像分析，该方法通过多尺度局部拓扑频谱表示提高了图像表示的稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统降维技术如主成分分析（PCA）对降维维度的选择高度敏感，常导致性能波动。MPSL通过利用多个降维维度的优点，提供更加稳定的图像分析。

Method: MPSL通过将图像样本视为单纯形复形，并运用持续层拉普拉斯提取多尺度局部拓扑频谱表示。通过跨尺度和维度的统计汇总，形成多尺度多维度的图像表示。

Result: 在COIL20和ETH80图像数据集上使用标准分类协议，MPSL相比PCA基线方法，在中等维度范围内显示出了更稳定的表现和持续的性能提升。

Conclusion: MPSL为改进图像分析中的多尺度和多维度特征提取提供了一种有效方法，展示了对于不同维度选择有更好的鲁棒性。

Abstract: We propose a multi-dimensional persistent sheaf Laplacian (MPSL) framework on simplicial complexes for image analysis. The proposed method is motivated by the strong sensitivity of commonly used dimensionality reduction techniques, such as principal component analysis (PCA), to the choice of reduced dimension. Rather than selecting a single reduced dimension or averaging results across dimensions, we exploit complementary advantages of multiple reduced dimensions. At a given dimension, image samples are regarded as simplicial complexes, and persistent sheaf Laplacians are utilized to extract a multiscale localized topological spectral representation for individual image samples. Statistical summaries of the resulting spectra are then aggregated across scales and dimensions to form multiscale multi-dimensional image representations. We evaluate the proposed framework on the COIL20 and ETH80 image datasets using standard classification protocols. Experimental results show that the proposed method provides more stable performance across a wide range of reduced dimensions and achieves consistent improvements to PCA-based baselines in moderate dimensional regimes.

</details>


### [124] [CT-Bench: A Benchmark for Multimodal Lesion Understanding in Computed Tomography](https://arxiv.org/abs/2602.14879)
*Qingqing Zhu,Qiao Jin,Tejas S. Mathai,Yin Fang,Zhizheng Wang,Yifan Yang,Maame Sarfo-Gyamfi,Benjamin Hou,Ran Gu,Praveen T. S. Balamuralikrishna,Kenneth C. Wang,Ronald M. Summers,Zhiyong Lu*

Main category: cs.CV

TL;DR: 介绍了CT-Bench，这是一个首创的大规模基准数据集，旨在解决CT影像中病灶级注解稀缺的问题。它包括病灶图像和元数据集以及多任务视觉问答基准，涵盖病灶定位、描述、大小估计和属性分类，包含难以处理的负例。


<details>
  <summary>Details</summary>
Motivation: 当前公开的CT数据集中的病灶级注解稀缺限制了AI在CT影像分析中的应用。因此，CT-Bench旨在填补这一空白。

Method: CT-Bench数据集分为两个部分，病灶图像和元数据集包含病灶的边界框、描述和大小信息，以及多任务视觉问答基准数据集包含病变定位、描述、大小估计和属性分类的2000多个问答对。数据集中还包含难以处理的负例以反映临床诊断挑战。

Result: 对多种最新 multimodal 模型（包括视觉-语言和医学 CLIP 变种）进行评估，表明CT-Bench在病变分析中的综合基准价值。细调模型在病变图像和元数据集上训练后，两个部分的性能都有显著提高。

Conclusion: CT-Bench是一个全面的基准数据集，能够促进在医学图像分析中的AI研究，特别是在CT影像病灶分析方面，具有重要的临床应用价值。

Abstract: Artificial intelligence (AI) can automatically delineate lesions on computed tomography (CT) and generate radiology report content, yet progress is limited by the scarcity of publicly available CT datasets with lesion-level annotations. To bridge this gap, we introduce CT-Bench, a first-of-its-kind benchmark dataset comprising two components: a Lesion Image and Metadata Set containing 20,335 lesions from 7,795 CT studies with bounding boxes, descriptions, and size information, and a multitask visual question answering benchmark with 2,850 QA pairs covering lesion localization, description, size estimation, and attribute categorization. Hard negative examples are included to reflect real-world diagnostic challenges. We evaluate multiple state-of-the-art multimodal models, including vision-language and medical CLIP variants, by comparing their performance to radiologist assessments, demonstrating the value of CT-Bench as a comprehensive benchmark for lesion analysis. Moreover, fine-tuning models on the Lesion Image and Metadata Set yields significant performance gains across both components, underscoring the clinical utility of CT-Bench.

</details>


### [125] [Wrivinder: Towards Spatial Intelligence for Geo-locating Ground Images onto Satellite Imagery](https://arxiv.org/abs/2602.14929)
*Chandrakanth Gudavalli,Tajuddin Manhar Mohammed,Abhay Yadav,Ananth Vishnu Bhaskar,Hardik Prajapati,Cheng Peng,Rama Chellappa,Shivkumar Chandrasekaran,B. S. Manjunath*

Main category: cs.CV

TL;DR: Wrivinder 是一种零样本、基于几何的框架，能够通过多次地面照片重建一致的 3D 场景并将其与空中卫星图片对齐，适用于在 GPS 不可靠或视角差距大时进行精确的摄像机地理定位。


<details>
  <summary>Details</summary>
Motivation: 实现在 GPS 信号不稳定或视角存在巨大差异情况下的地-空定位，这在地图绘制、导航和态势感知中有重要作用和挑战。

Method: Wrivinder 结合了SfM重建、3D 高斯散点图、语义锚定和单目深度基线度量线索，产生稳定的顶视图渲染以直接匹配卫星内容。

Result: 在零样本实验中，Wrivinder 无论是在密集场景还是大面积场景下，均实现了亚 30 米的地理位置准确性，表明基于几何的聚合方法具有强大的鲁棒性。

Conclusion: Wrivinder 和 MC-Sat 提供了研究几何中心跨视角对齐基准和测试平台的首个全面基线。

Abstract: Aligning ground-level imagery with geo-registered satellite maps is crucial for mapping, navigation, and situational awareness, yet remains challenging under large viewpoint gaps or when GPS is unreliable. We introduce Wrivinder, a zero-shot, geometry-driven framework that aggregates multiple ground photographs to reconstruct a consistent 3D scene and align it with overhead satellite imagery. Wrivinder combines SfM reconstruction, 3D Gaussian Splatting, semantic grounding, and monocular depth--based metric cues to produce a stable zenith-view rendering that can be directly matched to satellite context for metrically accurate camera geo-localization. To support systematic evaluation of this task, which lacks suitable benchmarks, we also release MC-Sat, a curated dataset linking multi-view ground imagery with geo-registered satellite tiles across diverse outdoor environments. Together, Wrivinder and MC-Sat provide a first comprehensive baseline and testbed for studying geometry-centered cross-view alignment without paired supervision. In zero-shot experiments, Wrivinder achieves sub-30\,m geolocation accuracy across both dense and large-area scenes, highlighting the promise of geometry-based aggregation for robust ground-to-satellite localization.

</details>


### [126] [AnchorWeave: World-Consistent Video Generation with Retrieved Local Spatial Memories](https://arxiv.org/abs/2602.14941)
*Zun Wang,Han Lin,Jaehong Yoon,Jaemin Cho,Yue Zhang,Mohit Bansal*

Main category: cs.CV

TL;DR: AnchorWeave 是一种增强的记忆辅助视频生成框架，通过引入多视角的局部几何记忆来替代全局不准确的记忆，从而提高长时场景的一致性和视觉质量。


<details>
  <summary>Details</summary>
Motivation: 现有的基于记忆的视频生成方法依赖于全局重建三维场景，存在跨视角不一致的问题，影响生成质量。

Method: AnchorWeave 通过覆盖驱动的选择局部记忆和多锚点编织控制器整合记忆来解决跨视角不一致性。

Result: 实验结果表明，AnchorWeave 能显著提高长时间场景的一致性，保持高质量视觉效果。

Conclusion: 研究进一步验证了局部几何条件、多锚点控制和覆盖驱动的检索机制的有效性。

Abstract: Maintaining spatial world consistency over long horizons remains a central challenge for camera-controllable video generation. Existing memory-based approaches often condition generation on globally reconstructed 3D scenes by rendering anchor videos from the reconstructed geometry in the history. However, reconstructing a global 3D scene from multiple views inevitably introduces cross-view misalignment, as pose and depth estimation errors cause the same surfaces to be reconstructed at slightly different 3D locations across views. When fused, these inconsistencies accumulate into noisy geometry that contaminates the conditioning signals and degrades generation quality. We introduce AnchorWeave, a memory-augmented video generation framework that replaces a single misaligned global memory with multiple clean local geometric memories and learns to reconcile their cross-view inconsistencies. To this end, AnchorWeave performs coverage-driven local memory retrieval aligned with the target trajectory and integrates the selected local memories through a multi-anchor weaving controller during generation. Extensive experiments demonstrate that AnchorWeave significantly improves long-term scene consistency while maintaining strong visual quality, with ablation and analysis studies further validating the effectiveness of local geometric conditioning, multi-anchor control, and coverage-driven retrieval.

</details>


### [127] [PAct: Part-Decomposed Single-View Articulated Object Generation](https://arxiv.org/abs/2602.14965)
*Qingming Liu,Xinyue Yao,Shuyuan Zhang,Yueci Deng,Guiliang Liu,Zhen Liu,Kui Jia*

Main category: cs.CV

TL;DR: 本文提出了一种以部件为中心的生成框架，用于创建关节模型对象。该框架在保留实例级别的对应关系的同时，保持有效的部件结构和运动，适合快速实时推理，并支持可控制的装配和关节运动。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以高效生成高质量的关节模型，本文旨在提供一种快速、有效且可控制的生成框架。

Method: 该方法通过以部件为中心的生成框架，利用隐码编码部件几何体、组成和关节，每个部件编码包括部件身份和关节提示的隐码。在单张图像条件下，该模型可生成保真度高的关节3D模型。

Result: 在常见的关节对象类别上（例如，抽屉和门），本文提出的方法相较于基于优化和检索的基线方法，在输入一致性、部件准确性和关节合理性方面有提高，同时大幅减少了推理时间。

Conclusion: 该生成框架能够在快速实时推理的同时，保持关节模型的精确结构和运动，为实体交互提供支持。

Abstract: Articulated objects are central to interactive 3D applications, including embodied AI, robotics, and VR/AR, where functional part decomposition and kinematic motion are essential. Yet producing high-fidelity articulated assets remains difficult to scale because it requires reliable part decomposition and kinematic rigging. Existing approaches largely fall into two paradigms: optimization-based reconstruction or distillation, which can be accurate but often takes tens of minutes to hours per instance, and inference-time methods that rely on template or part retrieval, producing plausible results that may not match the specific structure and appearance in the input observation. We introduce a part-centric generative framework for articulated object creation that synthesizes part geometry, composition, and articulation under explicit part-aware conditioning. Our representation models an object as a set of movable parts, each encoded by latent tokens augmented with part identity and articulation cues. Conditioned on a single image, the model generates articulated 3D assets that preserve instance-level correspondence while maintaining valid part structure and motion. The resulting approach avoids per-instance optimization, enables fast feed-forward inference, and supports controllable assembly and articulation, which are important for embodied interaction. Experiments on common articulated categories (e.g., drawers and doors) show improved input consistency, part accuracy, and articulation plausibility over optimization-based and retrieval-driven baselines, while substantially reducing inference time.

</details>


### [128] [ThermEval: A Structured Benchmark for Evaluation of Vision-Language Models on Thermal Imagery](https://arxiv.org/abs/2602.14989)
*Ayush Shrivastava,Kirtan Gangani,Laksh Jain,Mayank Goel,Nipun Batra*

Main category: cs.CV

TL;DR: ThermEval-B 是一个包含约55,000个热视觉问答对的新基准，旨在评估热视觉语言理解的基本能力。评估结果显示，现有视觉语言模型在热图像上的表现不佳，特别是在温度推理和色表变换方面。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型主要在RGB图像上表现良好，但在热图像中缺乏泛化能力。热图像捕捉物理温度而非颜色或纹理，需要与现有RGB基准不同的感知和推理能力，因此需要一个专门针对热图像的基准来推动相关技术的进步。

Method: 研究者构建了一个名为ThermEval-B的基准，其中包括公共数据集与新收集的数据集ThermEval-D。ThermEval-B包含大约55,000个热视觉问答对，以及丰富的场景多样性，包括室内外环境。他们还评估了25个开源和闭源的视觉语言模型。

Result: 研究发现，现有的视觉语言模型在温度推理上表现不佳，且在色表变换下性能下降，主要依赖语言先验或固定响应。只有轻微地通过提示或监督微调进行改善。

Conclusion: 研究结果表明，热图像的理解需要超越RGB假设的专门评估，ThermEval-B被定位为一个能够推动热视觉语言建模领域发展的基准测试。

Abstract: Vision language models (VLMs) achieve strong performance on RGB imagery, but they do not generalize to thermal images. Thermal sensing plays a critical role in settings where visible light fails, including nighttime surveillance, search and rescue, autonomous driving, and medical screening. Unlike RGB imagery, thermal images encode physical temperature rather than color or texture, requiring perceptual and reasoning capabilities that existing RGB-centric benchmarks do not evaluate. We introduce ThermEval-B, a structured benchmark of approximately 55,000 thermal visual question answering pairs designed to assess the foundational primitives required for thermal vision language understanding. ThermEval-B integrates public datasets with our newly collected ThermEval-D, the first dataset to provide dense per-pixel temperature maps with semantic body-part annotations across diverse indoor and outdoor environments. Evaluating 25 open-source and closed-source VLMs, we find that models consistently fail at temperature-grounded reasoning, degrade under colormap transformations, and default to language priors or fixed responses, with only marginal gains from prompting or supervised fine-tuning. These results demonstrate that thermal understanding requires dedicated evaluation beyond RGB-centric assumptions, positioning ThermEval as a benchmark to drive progress in thermal vision language modeling.

</details>


### [129] [Image Generation with a Sphere Encoder](https://arxiv.org/abs/2602.15030)
*Kaiyu Yue,Menglin Jia,Ji Hou,Tom Goldstein*

Main category: cs.CV

TL;DR: Sphere Encoder 提出了一种高效的生成框架，能够在单次前向传播中生成图像，并且使用少于五步就与多步扩散模型竞争，同时拥有更低的推理成本。


<details>
  <summary>Details</summary>
Motivation: 为了解决多步扩散模型计算成本高昂的问题，提出了 Sphere Encoder，旨在以更少的步骤生成图像，同时保持与先进扩散模型相当的性能。

Method: Sphere Encoder 通过学习一个将自然图像均匀映射至球形潜空间的编码器，和一个将随机潜向量解码回图像空间的解码器。模型仅通过图像重构损失进行训练，生成图像只需解码球体上的随机点。

Result: Sphere Encoder 在多个数据集中的性能与最先进的扩散模型相媲美，但具有较低的推理成本。

Conclusion: Sphere Encoder 提供了一种高效的图像生成方法，能够以更少的步骤和更低的计算成本实现高质量的图像生成。

Abstract: We introduce the Sphere Encoder, an efficient generative framework capable of producing images in a single forward pass and competing with many-step diffusion models using fewer than five steps. Our approach works by learning an encoder that maps natural images uniformly onto a spherical latent space, and a decoder that maps random latent vectors back to the image space. Trained solely through image reconstruction losses, the model generates an image by simply decoding a random point on the sphere. Our architecture naturally supports conditional generation, and looping the encoder/decoder a few times can further enhance image quality. Across several datasets, the sphere encoder approach yields performance competitive with state of the art diffusions, but with a small fraction of the inference cost. Project page is available at https://sphere-encoder.github.io .

</details>


### [130] [EditCtrl: Disentangled Local and Global Control for Real-Time Generative Video Editing](https://arxiv.org/abs/2602.15031)
*Yehonathan Litman,Shikun Liu,Dario Seyb,Nicholas Milef,Yang Zhou,Carl Marshall,Shubham Tulsiani,Caleb Leak*

Main category: cs.CV

TL;DR: 本文介绍了一种名为EditCtrl的高效视频填充控制框架，该框架仅在需要的地方进行计算，通过局部视频上下文模块和轻量级的临时全局上下文嵌入器，实现高效的视频编辑，同时提高了编辑质量。


<details>
  <summary>Details</summary>
Motivation: 现有的高保真生成视频编辑方法虽然图像质量有显著提升，但因处理全视频上下文而导致计算成本高昂的问题亟需解决，特别是对于稀疏、局部的编辑。

Method: EditCtrl框架采用了新型的局部视频上下文模块，仅在遮罩区域计算，降低计算成本；结合轻量级的临时全局上下文嵌入器，确保整个视频上下文的一致性。

Result: 编辑Ctrl相比当前最先进的生成编辑方法具有10倍的计算效率提升，并且在某些情况下甚至能提高编辑质量。

Conclusion: 此方法不仅提升了视频编辑的效率，还通过与文本提示的多区域编辑和自回归内容传播功能扩展了其应用能力。

Abstract: High-fidelity generative video editing has seen significant quality improvements by leveraging pre-trained video foundation models. However, their computational cost is a major bottleneck, as they are often designed to inefficiently process the full video context regardless of the inpainting mask's size, even for sparse, localized edits. In this paper, we introduce EditCtrl, an efficient video inpainting control framework that focuses computation only where it is needed. Our approach features a novel local video context module that operates solely on masked tokens, yielding a computational cost proportional to the edit size. This local-first generation is then guided by a lightweight temporal global context embedder that ensures video-wide context consistency with minimal overhead. Not only is EditCtrl 10 times more compute efficient than state-of-the-art generative editing methods, it even improves editing quality compared to methods designed with full-attention. Finally, we showcase how EditCtrl unlocks new capabilities, including multi-region editing with text prompts and autoregressive content propagation.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [131] [Multimodal Consistency-Guided Reference-Free Data Selection for ASR Accent Adaptation](https://arxiv.org/abs/2602.13263)
*Ligong Lei,Wenwen Lu,Xudong Pang,Zaokere Kadeer,Aishan Wumaier*

Main category: cs.CL

TL;DR: 该研究提出了一种多模态一致性引导的无参考数据选择框架，用于针对口音的自动语音识别（ASR）模型适应，该框架在跨域口音变化时表现出色，减少错误放大。


<details>
  <summary>Details</summary>
Motivation: 现有的语音识别系统在处理带口音的语音时表现不佳，常见的伪标签选择方法以文本为中心，可能选择发音虽然准确但声学匹配度差的假标签，进一步训练时可能会加剧错误。

Method: 该方法基于目标导向的预选步骤，并使用子模互信息增强查询相关性，减少后续计算量。然后，通过扰动解码生成每个语音片段的多个伪转录，并使用共享嵌入空间中的语音-文本对齐和预测词错误率两条无参考信号评分。简单百分位选择规则保留可靠的伪标签进行微调，同时丢弃噪声片段。

Result: 在领域内设置中，从30k候选池中选择约1500个片段，结果接近使用30k监督标签达到的词错误率10.45%。在跨域设置且候选池与现有口音不匹配时，一致性后处理的子集避免了未经一致性筛选的伪标签引起的性能下降，进一步的匹配小时实验表明其优于随机采样和最近的筛选基线。

Conclusion: 该研究提出的方法在带口音语音识别中有显著改进，特别是在跨域口音变化时，证明了其有效性和鲁棒性。

Abstract: Automatic speech recognition (ASR) systems often degrade on accented speech because acoustic-phonetic and prosodic shifts induce a mismatch to training data, making labeled accent adaptation costly. However, common pseudo-label selection heuristics are largely text-centric (e.g., perplexity (PPL) filtering) and can prefer fluent yet acoustically mismatched hypotheses, leading to error amplification when fine-tuning. To address this, we introduce a multimodal consistency-guided, reference-free data selection pipeline for ASR accent adaptation under a transductive, label-free protocol. The pipeline starts with a target-aware preselection step based on submodular mutual information to improve query relevance and reduce downstream computation. It then generates multiple pseudo-transcriptions per utterance via perturbation-based decoding and scores each hypothesis using two reference-free signals: speech--text alignment in a shared embedding space and predicted word error rate (WER). A simple percentile-based selection rule retains reliable pseudo-labels for fine-tuning while discarding noisy utterances. In an in-domain setting, selecting ~1.5k utterances from a 30k pool achieves 10.91% WER, close to 10.45% obtained using 30k supervised labels. In a cross-domain setting with a mismatched candidate pool, consistency-filtered subsets avoid the degradation caused by unfiltered pseudo-labels under strong accent shift, and matched-hour experiments on a stronger ASR backbone further confirm gains over random sampling and recent selection baselines.

</details>


### [132] [LLM-Powered Automatic Translation and Urgency in Crisis Scenarios](https://arxiv.org/abs/2602.13452)
*Belu Ticona,Antonis Anastasopoulos*

Main category: cs.CL

TL;DR: 本研究评估了最先进的人工语言模型和机器翻译系统在危机领域翻译中的表现，特别是在危机通信中保持紧迫感的性能。研究发现，无论是专用翻译模型还是语言模型，在处理危机数据时都表现出明显的性能下降和不稳定，即使语义正确的翻译也可能改变紧迫感的感知，且使用不同语言的提示和输入，基于语言模型的紧迫感分类差异很大。这提示了在危机沟通中部署通用语言技术的重大风险，并强调了危机意识评估框架的必要性。


<details>
  <summary>Details</summary>
Motivation: 研究旨在评估最先进的语言模型和翻译系统在危机情境中的具体表现，尤其是在应对高风险危机场景时的语言沟通及紧急性处理能力，以揭示它们在应急响应中的适用性和潜在缺陷。

Method: 研究使用多语种危机数据集，结合紧急标记数据，评估最先进语言模型和翻译模型在危机情境中的翻译表现，关注紧迫感保持效果。

Result: 研究结果表明，无论是专门设计的翻译模型还是通用语言模型，在危机语境下的翻译效果都显著下降且不稳定，即使准确翻译也会扭曲紧急感，语言模型生成的紧迫感分类在不同输入语言下有显著差异。

Conclusion: 研究提示，通用型语言技术在危机通信中的应用具有重大风险，需要建立危机意识评估框架，以确保应急响应中的有效沟通。

Abstract: Large language models (LLMs) are increasingly proposed for crisis preparedness and response, particularly for multilingual communication. However, their suitability for high-stakes crisis contexts remains insufficiently evaluated. This work examines the performance of state-of-the-art LLMs and machine translation systems in crisis-domain translation, with a focus on preserving urgency, which is a critical property for effective crisis communication and triaging. Using multilingual crisis data and a newly introduced urgency-annotated dataset covering over 32 languages, we show that both dedicated translation models and LLMs exhibit substantial performance degradation and instability. Crucially, even linguistically adequate translations can distort perceived urgency, and LLM-based urgency classifications vary widely depending on the language of the prompt and input. These findings highlight significant risks in deploying general-purpose language technologies for crisis communication and underscore the need for crisis-aware evaluation frameworks.

</details>


### [133] [Language Model Memory and Memory Models for Language](https://arxiv.org/abs/2602.13466)
*Benjamin L. Badger*

Main category: cs.CL

TL;DR: 本文探讨了机器学习模型中隐藏层向量嵌入作为‘记忆’的能力。研究发现，语言模型在训练过程中包含的输入信息有限，而自编码器在输入再生时形成的嵌入则能实现近乎完美的记忆。通过结合因原因训练和保持信息目标函数，模型可以学习形成和解码丰富信息的记忆。进一步的训练策略包括冻结高保真编码器和使用课程训练方法，首先让解码器学习处理记忆，然后学习预测下一个词。研究还指出单独的下一个词预测训练不适合准确形成记忆，因此引入了结合目标函数。


<details>
  <summary>Details</summary>
Motivation: 针对机器学习模型在训练过程中如何存储输入信息，特别是自编码器训练模型与语言模型相比，如果能够有效记录和解码信息，将显著提高模型效率和性能。

Method: 通过对比语言模型和自编码器训练的嵌入，研究了不同训练目标函数对模型记忆形成的影响，并开发了一种结合因原因训练和信息保持的训练方法。

Result: 研究发现，自编码器训练的嵌入能够实现近乎完美的记忆形成，而单纯的下一个词预测训练不足以形成准确的记忆。提出了一种新的训练策略，结合了因原因训练和信息保持目标函数，以提高模型的记忆能力。

Conclusion: 该研究提出了一种新的并行可扩展的编码器-解码器记忆模型架构，能够更有效地存储和访问信息，尤其是在复杂的语言处理任务中。

Abstract: The ability of machine learning models to store input information in hidden layer vector embeddings, analogous to the concept of `memory', is widely employed but not well characterized. We find that language model embeddings typically contain relatively little input information regardless of data and compute scale during training. In contrast, embeddings from autoencoders trained for input regeneration are capable of nearly perfect memory formation. The substitution of memory embeddings for token sequences leads to substantial computational efficiencies, motivating the introduction of a parallelizable encoder-decoder memory model architecture. Upon causal training these models contain information-poor embeddings incapable of arbitrary information access, but by combining causal and information retention objective functions they learn to form and decode information-rich memories. Training can be further streamlined by freezing a high fidelity encoder followed by a curriculum training approach where decoders first learn to process memories and then learn to additionally predict next tokens. We introduce the perspective that next token prediction training alone is poorly suited for accurate memory formation as the objective itself is non-invertible, motivating the use of combined objective functions for models where the entire input is not exposed.

</details>


### [134] [From Perceptions To Evidence: Detecting AI-Generated Content In Turkish News Media With A Fine-Tuned Bert Classifier](https://arxiv.org/abs/2602.13504)
*Ozancan Ozdemir*

Main category: cs.CL

TL;DR: 该研究通过微调特定于土耳其语的BERT模型，首次对土耳其新闻媒体中的AI改写内容进行了定量分析。该模型在测试集上达到了0.9708的F1分数，应用于超过3,500篇未见过的文章，发现约2.5%的新闻内容可能经过或被重新改写。


<details>
  <summary>Details</summary>
Motivation: 研究旨在填补现有研究中基于土耳其新闻媒体对AI生成内容的定量研究空白，特别是AI改写内容的定量测量。

Method: 通过微调特定于土耳其语的BERT模型（dbmdz/bert-base-turkish-cased），在带有标签的数据集上进行二分类。该模型在接受测试集评估后，应用于大量未见过的文章。

Result: 该模型在测试集上达到了0.9708的F1分数；应用于超过3,500篇未见过的文章后，显示出一致的跨来源和时间稳定的分类模式，平均预测置信度超过0.96，估计有2.5%的被检新闻内容被LLMs改写或重写。

Conclusion: 这是首次通过基于数据的方法而非自述记者感知，定量测量土耳其新闻媒体中AI的使用情况的研究。

Abstract: The rapid integration of large language models into newsroom workflows has raised urgent questions about the prevalence of AI-generated content in online media. While computational studies have begun to quantify this phenomenon in English-language outlets, no empirical investigation exists for Turkish news media, where existing research remains limited to qualitative interviews with journalists or fake news detection. This study addresses that gap by fine-tuning a Turkish-specific BERT model (dbmdz/bert-base-turkish-cased) on a labeled dataset of 3,600 articles from three major Turkish outlets with distinct editorial orientations for binary classification of AI-rewritten content. The model achieves 0.9708 F1 score on the held-out test set with symmetric precision and recall across both classes. Subsequent deployment on over 3,500 unseen articles spanning between 2023 and 2026 reveals consistent cross-source and temporally stable classification patterns, with mean prediction confidence exceeding 0.96 and an estimated 2.5 percentage of examined news content rewritten or revised by LLMs on average. To the best of our knowledge, this is the first study to move beyond self-reported journalist perceptions toward empirical, data-driven measurement of AI usage in Turkish news media.

</details>


### [135] [Think Deep, Not Just Long: Measuring LLM Reasoning Effort via Deep-Thinking Tokens](https://arxiv.org/abs/2602.13517)
*Wei-Lin Chen,Liqian Peng,Tian Tan,Chao Zhao,Blake JianHang Chen,Ziqian Lin,Alec Go,Yu Meng*

Main category: cs.CL

TL;DR: 通过量化推理过程中的深度思考比例，提出了一种新的测试时缩放策略Think@n，该策略根据短前缀就能有效地排除无前景的生成，从而在保持或超越标准自我一致性性能的同时大幅降低推理成本。


<details>
  <summary>Details</summary>
Motivation: 现有研究表明，增加生成长度并不总是提高准确性的可靠指标，反而可能会导致“过度思考”并损害性能。因此，需要一种更可靠的方式来衡量推理质量，以便开发更有效的测试时缩放策略。

Method: 本文定义了“深度思考比例”这一指标，并通过在多种模型和基准测试中的应用来探索其与准确性的关联。利用这一关联，提出了Think@n策略，该策略基于短前缀快速识别并抛弃无前景的生成。

Result: 该研究发现在多个难题基准和多种推理模型中，深度思考比例（深度思考令牌在生成序列中的比例）与准确率之间的关系非常明显且一致。应用Think@n可以在保持或超越标准自我一致性性能的同时，显著降低推理成本。

Conclusion: 深度思考比例是衡量推理质量的可靠指标，Think@n策略通过测试时缩放提升了推理效率，为大语言模型的实际应用提供了有价值的见解。

Abstract: Large language models (LLMs) have demonstrated impressive reasoning capabilities by scaling test-time compute via long Chain-of-Thought (CoT). However, recent findings suggest that raw token counts are unreliable proxies for reasoning quality: increased generation length does not consistently correlate with accuracy and may instead signal "overthinking," leading to performance degradation. In this work, we quantify inference-time effort by identifying deep-thinking tokens -- tokens where internal predictions undergo significant revisions in deeper model layers prior to convergence. Across four challenging mathematical and scientific benchmarks (AIME 24/25, HMMT 25, and GPQA-diamond) and a diverse set of reasoning-focused models (GPT-OSS, DeepSeek-R1, and Qwen3), we show that deep-thinking ratio (the proportion of deep-thinking tokens in a generated sequence) exhibits a robust and consistently positive correlation with accuracy, substantially outperforming both length-based and confidence-based baselines. Leveraging this insight, we introduce Think@n, a test-time scaling strategy that prioritizes samples with high deep-thinking ratios. We demonstrate that Think@n matches or exceeds standard self-consistency performance while significantly reducing inference costs by enabling the early rejection of unpromising generations based on short prefixes.

</details>


### [136] [On Calibration of Large Language Models: From Response To Capability](https://arxiv.org/abs/2602.13540)
*Sin-Han Yang,Cheng-Kuang Wu,Chieh-Yen Lin,Yun-Nung Chen,Hung-yi Lee,Shao-Hua Sun*

Main category: cs.CL

TL;DR: 研究提出了能力校准的概念，旨在评估模型整体解决查询的能力，这与现有响应级校准有所不同，实验结果证实了能力校准在多方面应用中的潜力。


<details>
  <summary>Details</summary>
Motivation: 准确的信心估计对于大型语言模型的有效使用至关重要。现有的工作大多集中在单个生成输出的正确性上，但实际应用中往往关心的是模型解决查询的能力。

Method: 研究通过引入能力校准，旨在衡量模型在解决查询时的预期准确性。研究对比了能力校准与响应校准，通过建立实验评估体系，探讨了多种信心估计方法。

Result: 实验结果表明，能力校准可以提升pass@$k$预测和推理预算分配的效果，为多样化的应用奠定了基础。

Conclusion: 研究展示了一种新的校准方法，并验证了这种方法在实际应用中的有效性，为进一步的研究提供了指导。

Abstract: Large language models (LLMs) are widely deployed as general-purpose problem solvers, making accurate confidence estimation critical for reliable use. Prior work on LLM calibration largely focuses on response-level confidence, which estimates the correctness of a single generated output. However, this formulation is misaligned with many practical settings where the central question is how likely a model is to solve a query overall. We show that this mismatch results from the stochastic nature of modern LLM decoding, under which single-response correctness fails to reflect underlying model capability. To address this issue, we introduce capability calibration, which targets the model's expected accuracy on a query. We formally distinguish capability calibration from response calibration and show that the two differ both theoretically and empirically. We establish an empirical evaluation setup and study a range of confidence estimation methods. Our results demonstrate that capability-calibrated confidence improves pass@$k$ prediction and inference budget allocation, establishing a foundation with potential for diverse applications.

</details>


### [137] [Small Reward Models via Backward Inference](https://arxiv.org/abs/2602.13551)
*Yike Wang,Faeze Brahman,Shangbin Feng,Teng Xiao,Hannaneh Hajishirzi,Yulia Tsvetkov*

Main category: cs.CL

TL;DR: FLIP通过反向推理提出了一种无需参考答案和评分标准的奖励建模方法，适用于非验证领域，表现出优越的效果并提升了下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 传统的大模型作为评判者的奖励模型方法依赖于模型的强推理能力，而FLIP旨在提供一种灵活且广泛适用的奖励建模方法，以克服现有方法的限制。

Method: FLIP通过反向推理从给定的响应反推出最有可能产生该响应的指令，再用推断出的指令和原始指令之间的相似度作为奖励信号。

Result: 实验结果显示，在四个不同领域中使用FLIP方法比传统的大型模型作为评判者的基线方法平均提高了79.6%的性能。FLIP还在测试时通过并行采样和GRPO训练提升了下游任务的性能，并且在长输出和常见的奖励作弊方面表现出更好的鲁棒性。

Conclusion: FLIP为奖励建模在缩小规模的情况下提供了可靠的方法，特别是在那些需要判断方法的领域内，FLIP表现出更有效的性能。

Abstract: Reward models (RMs) play a central role throughout the language model (LM) pipeline, particularly in non-verifiable domains. However, the dominant LLM-as-a-Judge paradigm relies on the strong reasoning capabilities of large models, while alternative approaches require reference responses or explicit rubrics, limiting flexibility and broader accessibility. In this work, we propose FLIP (FLipped Inference for Prompt reconstruction), a reference-free and rubric-free reward modeling approach that reformulates reward modeling through backward inference: inferring the instruction that would most plausibly produce a given response. The similarity between the inferred and the original instructions is then used as the reward signal. Evaluations across four domains using 13 small language models show that FLIP outperforms LLM-as-a-Judge baselines by an average of 79.6%. Moreover, FLIP substantially improves downstream performance in extrinsic evaluations under test-time scaling via parallel sampling and GRPO training. We further find that FLIP is particularly effective for longer outputs and robust to common forms of reward hacking. By explicitly exploiting the validation-generation gap, FLIP enables reliable reward modeling in downscaled regimes where judgment methods fail. Code available at https://github.com/yikee/FLIP.

</details>


### [138] [DistillLens: Symmetric Knowledge Distillation Through Logit Lens](https://arxiv.org/abs/2602.13567)
*Manish Dhakal,Uthman Jinadu,Anjila Budathoki,Rajshekhar Sunderraman,Yi Ding*

Main category: cs.CL

TL;DR: DistillLens 通过将中间隐藏状态投影到词汇空间并通过对称偏差目标实现学生和教师模型的进化思维过程的对称校准，从而改善了知识蒸馏的效果。


<details>
  <summary>Details</summary>
Motivation: 现有的知识蒸馏方法通常将教师模型的中间层视为黑盒，忽略了这些层中丰富的不确定性信息，这些信息对于最终输出至关重要。DistillLens 旨在解决这一问题。

Method: DistillLens 通过引入 Logit Lens 将中间隐藏状态投影到词汇空间，然后使用对称偏差目标进行结构对齐。

Result: DistillLens 在针对 GPT-2 和 Llama 架构的多层次指令遵循基准测试中，表现出色，优于标准知识蒸馏和基于特征转移的基本方法。

Conclusion: DistillLens 增强了知识蒸馏中的学生模型和教师模型的中间层对准，使得学生模型在最终输出上表现出更高的一致性。

Abstract: Standard Knowledge Distillation (KD) compresses Large Language Models (LLMs) by optimizing final outputs, yet it typically treats the teacher's intermediate layer's thought process as a black box. While feature-based distillation attempts to bridge this gap, existing methods (e.g., MSE and asymmetric KL divergence) ignore the rich uncertainty profiles required for the final output. In this paper, we introduce DistillLens, a framework that symmetrically aligns the evolving thought processes of student and teacher models. By projecting intermediate hidden states into the vocabulary space via the Logit Lens, we enforce structural alignment using a symmetric divergence objective. Our analysis proves that this constraint imposes a dual-sided penalty, preventing both overconfidence and underconfidence while preserving the high-entropy information conduits essential for final deduction. Extensive experiments on GPT-2 and Llama architectures demonstrate that DistillLens consistently outperforms standard KD and feature-transfer baselines on diverse instruction-following benchmarks. The code is available at https://github.com/manishdhakal/DistillLens.

</details>


### [139] [LLM-Confidence Reranker: A Training-Free Approach for Enhancing Retrieval-Augmented Generation Systems](https://arxiv.org/abs/2602.13571)
*Zhipeng Song,Xiangyu Kong,Xinrui Bao,Yizhi Zhou,Jiulong Jiao,Sitong Liu,Yuhang Zhou,Heng Qi*

Main category: cs.CL

TL;DR: LCR 提出了一种无需训练、即插即用的重新排序算法，利用黑盒大模型的置信度信号提高 RAG 系统的性能，并展示了其在多个基准测试中的显著效果。


<details>
  <summary>Details</summary>
Motivation: LLMs 在自然语言处理中取得了巨大进展，但仍面临知识密集型任务中的幻觉挑战。RAG 通过结合外部知识来解决这一问题，但其效果依赖于精确的文档检索和排名。现有的重排序器尽管有效，但也面临许多挑战，如需要特殊训练、计算成本高且未能充分利用 LLM 的语义能力。LCR 旨在解决这些问题。

Method: LCR 提出了一种两阶段的方法：首先是基于 Multinomial 抽样的置信度评估和聚类，然后是基于查询和文档置信阈值进行的分层排序。这种方法在保持高置信查询原始排序的同时，优先选择相关文档。

Result: 在 BEIR 和 TREC 基准测试中，使用仅有 7-9 亿参数的预训练 LLM，LCR 在 NDCG@5 上提高了 20.6% 的性能，且优于预训练 LLM 和微调的 Transformer 重排序器。通过消融研究进一步验证了 LLM 置信度与文档相关性之间的正相关性。

Conclusion: LCR 提供了高效率和可扩展性的解决方案，同时增强了 RAG 应用中的精确性，特别是在需要确保准确性的医疗诊断等领域。

Abstract: Large language models (LLMs) have revolutionized natural language processing, yet hallucinations in knowledge-intensive tasks remain a critical challenge. Retrieval-augmented generation (RAG) addresses this by integrating external knowledge, but its efficacy depends on accurate document retrieval and ranking. Although existing rerankers demonstrate effectiveness, they frequently necessitate specialized training, impose substantial computational expenses, and fail to fully exploit the semantic capabilities of LLMs, particularly their inherent confidence signals. We propose the LLM-Confidence Reranker (LCR), a training-free, plug-and-play algorithm that enhances reranking in RAG systems by leveraging black-box LLM confidence derived from Maximum Semantic Cluster Proportion (MSCP). LCR employs a two-stage process: confidence assessment via multinomial sampling and clustering, followed by binning and multi-level sorting based on query and document confidence thresholds. This approach prioritizes relevant documents while preserving original rankings for high-confidence queries, ensuring robustness. Evaluated on BEIR and TREC benchmarks with BM25 and Contriever retrievers, LCR--using only 7--9B-parameter pre-trained LLMs--consistently improves NDCG@5 by up to 20.6% across pre-trained LLM and fine-tuned Transformer rerankers, without degradation. Ablation studies validate the hypothesis that LLM confidence positively correlates with document relevance, elucidating LCR's mechanism. LCR offers computational efficiency, parallelism for scalability, and broad compatibility, mitigating hallucinations in applications like medical diagnosis.

</details>


### [140] [Elo-Evolve: A Co-evolutionary Framework for Language Model Alignment](https://arxiv.org/abs/2602.13575)
*Jing Zhao,Ting Zhen,Junwei bao,Hongfei Jiang,Yang song*

Main category: cs.CL

TL;DR: Elo-Evolve 是一种新的共进化框架，通过动态的多Agent竞赛来重新定义对齐，它通过直接从二元胜负结果中学习以及使用Elo策略自动进行课程学习来减少数据稀疏性和噪声敏感性。实验表明，与绝对奖励方法相比，Elo-Evolve 提供了4.5倍的噪声减少，并在Alpaca Eval 2.0 和 MT-Bench 上展示了优于点基方法和静态双边训练的性能。


<details>
  <summary>Details</summary>
Motivation: 目前大型语言模型（LLMs）的对齐方法依赖于压缩大量的人类偏好数据为静态绝对奖励函数，这导致了数据稀缺性、噪声敏感性和训练不稳定性的问题。Elo-Evolve 框架旨在通过动态的多Agent竞赛来重新定义对齐，从而解决这些问题。

Method: Elo-Evolve 框架借鉴了考试学习理论，通过直接从二元胜负结果中学习以及使用温度控制的样本选择策略来自动进行课程学习。该方法使用Elo对局竞争来选择对手，从而实现动态的对手池。

Result: 实验结果表明，Elo-Evolve 方法相比绝对奖励方法在Alpaca Eval 2.0 和 MT-Bench 上减少了4.5倍的噪声，并且性能优于基于点的方法和静态双边训练方法。

Conclusion: Elo-Evolve 框架通过动态的多Agent竞赛重新定义了对齐，展示了比现有方法更好的性能和更强的抗噪能力，为LLM对齐提供了新的解决方案。

Abstract: Current alignment methods for Large Language Models (LLMs) rely on compressing vast amounts of human preference data into static, absolute reward functions, leading to data scarcity, noise sensitivity, and training instability. We introduce Elo-Evolve, a co-evolutionary framework that redefines alignment as dynamic multi-agent competition within an adaptive opponent pool. Our approach makes two key innovations: (1) eliminating Bradley-Terry model dependencies by learning directly from binary win/loss outcomes in pairwise competitions, and (2) implementing Elo-orchestrated opponent selection that provides automatic curriculum learning through temperature-controlled sampling. We ground our approach in PAC learning theory, demonstrating that pairwise comparison achieves superior sample complexity and empirically validate a 4.5x noise reduction compared to absolute scoring approaches. Experimentally, we train a Qwen2.5-7B model using our framework with opponents including Qwen2.5-14B, Qwen2.5-32B, and Qwen3-8B models. Results demonstrate a clear performance hierarchy: point-based methods < static pairwise training < Elo-Evolve across Alpaca Eval 2.0 and MT-Bench, validating the progressive benefits of pairwise comparison and dynamic opponent selection for LLM alignment.

</details>


### [141] [RMPL: Relation-aware Multi-task Progressive Learning with Stage-wise Training for Multimedia Event Extraction](https://arxiv.org/abs/2602.13748)
*Yongkang Jin,Jianwen Luo,Jingjing Wang,Jianmin Yao,Yu Hong*

Main category: cs.CL

TL;DR: 提出了一种名为RMPL的关系感知多任务渐进学习框架，用于在资源有限条件下进行多媒体事件提取。RMPL利用跨模态事件提取和多媒体关系提取的不同监督信号，并在不同的阶段进行训练。


<details>
  <summary>Details</summary>
Motivation: 现有的多媒体事件提取方法主要依赖于跨模态对齐或利用视觉语言模型，在多媒体设置下通常难以生成强的论元绑定，且缺乏大规模标注数据。

Method: RMPL框架首先通过统一的结构来学习跨模态共享的事件中心表示，然后在多模式数据上进行微调，用于识别事件提及和论元角色。

Result: 在M2E2基准上使用多个视觉语言模型进行了实验，结果显示出跨模态设置中的持续改进。

Conclusion: RMPL框架在资源有限的条件下提高了多媒体事件提取的性能，提供了一种有效的处理跨模式信息和弱论元绑定的方法。

Abstract: Multimedia Event Extraction (MEE) aims to identify events and their arguments from documents that contain both text and images. It requires grounding event semantics across different modalities. Progress in MEE is limited by the lack of annotated training data. M2E2 is the only established benchmark, but it provides annotations only for evaluation. This makes direct supervised training impractical. Existing methods mainly rely on cross-modal alignment or inference-time prompting with Vision--Language Models (VLMs). These approaches do not explicitly learn structured event representations and often produce weak argument grounding in multimodal settings. To address these limitations, we propose RMPL, a Relation-aware Multi-task Progressive Learning framework for MEE under low-resource conditions. RMPL incorporates heterogeneous supervision from unimodal event extraction and multimedia relation extraction with stage-wise training. The model is first trained with a unified schema to learn shared event-centric representations across modalities. It is then fine-tuned for event mention identification and argument role extraction using mixed textual and visual data. Experiments on the M2E2 benchmark with multiple VLMs show consistent improvements across different modality settings.

</details>


### [142] [How Do Lexical Senses Correspond Between Spoken German and German Sign Language?](https://arxiv.org/abs/2602.13790)
*Melis Çelikkol,Wei Zhao*

Main category: cs.CL

TL;DR: 该研究通过分析德语及其手语词汇使用与其对应手势之间的映射关系，开发了一种基于使用的手语辞典构建方法，特别是利用语义相似性方法相比精确匹配方法显著提高了映射准确率。


<details>
  <summary>Details</summary>
Motivation: 鉴于现有德语手语词典中对多义词和同形词在不同语境下的映射关系描述不足，研究旨在填补这一空白，通过挖掘使用数据增强手语辞典资源。

Method: 研究团队手动标注了1,404个词汇使用-手势映射，涵盖32个德语词汇和49个手语词汇，基于这些建立了用于词义-手势对应关系的计算方法，包括精确匹配和语义相似性计算。

Result: 研究发现基于语义相似性的计算方法在总体准确率上优于精确匹配法，特别是在多义词映射上取得显著提升。

Conclusion: 研究初步建立了用于跨模态词义对应关系的标注数据集，证明了使用数据可以有效补充辞书资源，并提供了相关技术和数据资源的公开访问路径。

Abstract: Sign language lexicographers construct bilingual dictionaries by establishing word-to-sign mappings, where polysemous and homonymous words corresponding to different signs across contexts are often underrepresented. A usage-based approach examining how word senses map to signs can identify such novel mappings absent from current dictionaries, enriching lexicographic resources. We address this by analyzing German and German Sign Language (Deutsche Gebärdensprache, DGS), manually annotating 1,404 word use-to-sign ID mappings derived from 32 words from the German Word Usage Graph (D-WUG) and 49 signs from the Digital Dictionary of German Sign Language (DW-DGS). We identify three correspondence types: Type 1 (one-to-many), Type 2 (many-to-one), and Type 3 (one-to-one), plus No Match cases. We evaluate computational methods: Exact Match (EM) and Semantic Similarity (SS) using SBERT embeddings. SS substantially outperforms EM overall 88.52% vs. 71.31%), with dramatic gains for Type 1 (+52.1 pp). Our work establishes the first annotated dataset for cross-modal sense correspondence and reveals which correspondence patterns are computationally identifiable. Our code and dataset are made publicly available.

</details>


### [143] [OMGs: A multi-agent system supporting MDT decision-making across the ovarian tumour care continuum](https://arxiv.org/abs/2602.13793)
*Yangyang Zhang,Zilong Wang,Jianbo Xu,Yongqi Chen,Chu Han,Zhihao Zhang,Shuai Liu,Hui Li,Huiping Zhang,Ziqi Liu,Jiaxin Chen,Jun Zhu,Zheng Feng,Hao Wen,Xingzhu Ju,Yanping Zhong,Yunqiu Zhang,Jie Duan,Jun Li,Dongsheng Li,Weijie Wang,Haiyan Zhu,Wei Jiang,Xiaohua Wu,Shuo Wang,Haiming Li,Qinhao Guo*

Main category: cs.CL

TL;DR: OMGs系统是一个多智能体AI框架，通过多领域智能体的协作，整合跨学科证据并生成具有透明理由的MDT建议。该系统在多种临床场景中表现出与专家MDT共识相当的性能，特别是在证据和稳健性方面，能显著增强医生的建议。


<details>
  <summary>Details</summary>
Motivation: 目前卵巢肿瘤管理依赖于多学科肿瘤委员会（MDT）的讨论，但在资源受限的医疗机构中这类资源往往不足或不可用，导致大多数患者缺乏及时的专家共识。OMGs系统的提出旨在解决这一问题。

Method: OMGs系统通过多智能体AI框架，智能体们协作整合不同领域的证据，并基于透明的逻辑生成MDT风格的建议。评估方法包括开发SPEAR评价体系，并通过多中心回顾性和前瞻性研究进行验证。

Result: OMGs系统在多种临床场景中的评估中，性能接近专家MDT共识，并在证据评估上表现出更高的得分。系统在多中心前瞻性研究中，与常规MDT决策的符合度也很高。更重要的是，OMGs系统在没有多学科专长时，显著增强了医生关于证据和稳健性的建议。

Conclusion: 该研究证明了多智能体系统能够达到专家MDT一致决策的性能水平，有可能在资源有限的环境中扩大获得专业化肿瘤治疗指导的途径。

Abstract: Ovarian tumour management has increasingly relied on multidisciplinary tumour board (MDT) deliberation to address treatment complexity and disease heterogeneity. However, most patients worldwide lack access to timely expert consensus, particularly in resource-constrained centres where MDT resources are scarce or unavailable. Here we present OMGs (Ovarian tumour Multidisciplinary intelligent aGent System), a multi-agent AI framework where domain-specific agents deliberate collaboratively to integrate multidisciplinary evidence and generate MDT-style recommendations with transparent rationales. To systematically evaluate MDT recommendation quality, we developed SPEAR (Safety, Personalization, Evidence, Actionability, Robustness) and validated OMGs across diverse clinical scenarios spanning the care continuum. In multicentre re-evaluation, OMGs achieved performance comparable to expert MDT consensus ($4.45 \pm 0.30$ versus $4.53 \pm 0.23$), with higher Evidence scores (4.57 versus 3.92). In prospective multicentre evaluation (59 patients), OMGs demonstrated high concordance with routine MDT decisions. Critically, in paired human-AI studies, OMGs most substantially enhanced clinicians' recommendations in Evidence and Robustness, the dimensions most compromised when multidisciplinary expertise is unavailable. These findings suggest that multi-agent deliberative systems can achieve performance comparable to expert MDT consensus, with potential to expand access to specialized oncology expertise in resource-limited settings.

</details>


### [144] [The acquisition of English irregular inflections by Yemeni L1 Arabic learners: A Universal Grammar approach](https://arxiv.org/abs/2602.13816)
*Muneef Y. Alsawsh,Mohammed Q. Shormani*

Main category: cs.CL

TL;DR: 本研究通过使用普遍语法（UG）方法，探讨了也门英语二语学习者在获得英语不规则词尾形态时的作用。研究表明，学习者最初受到母语转移的影响较大，但在后续发展阶段，他们开始更加关注UG属性和形态重组，尽管如此，他们仍然面临一些挑战。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探讨也门英语二语学习者在掌握英语不规则词尾形态时，普遍语法，特别是特征重组假说（FRH）在学习过程中的作用，以及母语迁移和第二语言发展因素的影响。

Method: 研究采用了语言普遍性理论框架，并通过分析两个发展阶段的数据来识别学习者在掌握英语不规则词尾形态时出现的错误类型和变化趋势，这些错误形式证明了UG特征重组假说的作用。

Result: 研究发现，在第一阶段，学习者的错误主要由母语影响引起；在第二阶段，学习者逐渐表现出对UG属性的敏感性，但由于外界输入不充分或质量不足，他们在某些词尾形态方面依然存在困难。

Conclusion: 结论指出，尽管母语和第二语言发展的因素影响着初期的学习过程，但适当的输入和教学策略对于促进UG驱动的特征重组至关重要。

Abstract: This study examines the acquisition of English irregular inflections by Yemeni learners of English as a second language (L2), utilizing a Universal Grammar (UG) approach. Within the UG approach, the study considers Feature Reassembly Hypothesis (FRH) (Lardiere, 2008, 2009) part of UG, focusing on the roles of first language (L1) transfer and L2 developmental influence. It analyzes learner errors across two developmental stages. Stage 1 data reveal a dominant influence of L1 transfer, particularly in phonological and structural mismatches, while stage 2 data demonstrate increased learner sensitivity to UG properties and morphological reconfiguration toward the target language. Findings reveal that errors in irregular inflectional morphology are attributed to both interlingual and intralingual sources, with overgeneralization of L2 rules as a common developmental strategy. Statistical analysis, including a one-way ANOVA, indicates significant improvement in the production of well-formed irregular inflections from stage 1 to stage 2, underscoring learners' continued access to UG. However, persistent difficulties with consonant change, zero-morpheme, and -a plural inflections suggest that limited exposure, ineffective input modeling, and insufficient instructional quality constrain full UG access. The study concludes that while L1 transfer and L2 developmental factors influence initial stages of acquisition, appropriate linguistic input and instruction are critical for facilitating UG-driven feature reassembly in adult L2 learners.

</details>


### [145] [Beyond Words: Evaluating and Bridging Epistemic Divergence in User-Agent Interaction via Theory of Mind](https://arxiv.org/abs/2602.13832)
*Minyuan Ruan,Ziyue Wang,Kaiming Liu,Yunghwei Lai,Peng Li,Yang Liu*

Main category: cs.CL

TL;DR: 本文提出了将理论性态（ToM）应用于大语言模型（LLMs），旨在解决由于意图和指令不明确导致的用户信念和真实环境状态之间的认知分歧。通过构建一个基准测试（enchname）来评估模型如何在实践中解决这些分歧，并通过强化学习方法训练了一个基于轨迹的ToM数据集，从而显著提高了模型理解和回应用户心理状态的能力。


<details>
  <summary>Details</summary>
Motivation: 现有的语言模型在处理用户意图模糊时难以准确响应实际需求，这会导致认知分歧。本文旨在通过引入理论性态（ToM），探索其在解决这种认知分歧中的作用和潜在价值。

Method: 研究者首先定义了一个理论性态框架来识别和解决认知分歧，接着设计了一个基准测试来评估模型的实际应用效果。为进一步增强模型的能力，他们还构建了一个基于轨迹的数据集，并通过强化学习训练了模型。

Result: 研究表明，大部分领先的语言模型在识别认知差距方面存在显著不足。而基于轨迹的ToM数据集经过强化学习训练的模型，在理解和预测用户心理状态方面表现出稳步的提升。

Conclusion: 本文强调了将理论性态作为一种互动层面的重要机制，而不仅仅是独立的推理技能，对于提升语言模型实际应用中的表现具有重要意义。

Abstract: Large Language Models (LLMs) have developed rapidly and are widely applied to both general-purpose and professional tasks to assist human users. However, they still struggle to comprehend and respond to the true user needs when intentions and instructions are imprecisely conveyed, leading to a divergence between subjective user believes and true environment states. Resolving this epistemic divergence requires Theory of Mind (ToM), yet existing ToM evaluations for LLMs primarily focus on isolated belief inference, overlooking its functional utility in real-world interaction. To this end, we formalize ToM for LLMs as a mechanism for epistemic divergence detection and resolution, and propose a benchmark, \benchname, to assess how models reconcile user beliefs and profiles in practice. Results across 11 leading models reveal a significant limitation to identify underlying cognitive gaps that impede task success. To bridge this gap, we further curate a trajectory-based ToM dataset linking belief tracking with task-related state inference. The model trained on this data via reinforcement learning shows consistent improvement in reasoning about user mental states, leading to enhanced downstream performance. Our work highlights the practical value of ToM as an essential interaction-level mechanism rather than as a standalone reasoning skill.

</details>


### [146] [Speculative Decoding with a Speculative Vocabulary](https://arxiv.org/abs/2602.13836)
*Miles Williams,Young D. Kwon,Rui Li,Alexandros Kouris,Stylianos I. Venieris*

Main category: cs.CL

TL;DR: 本文提出了一种名为SpecVocab的方法，它在每一步解码时选择一个词汇子集。在多种任务中，SpecVocab比最先进的投机解码方法EAGLE-3实现了更高的接受长度，平均吞吐量提高了8.1%。


<details>
  <summary>Details</summary>
Motivation: 现有的投机解码方法通过减小采样词汇量来缓解输出分布瓶颈，但这可能会导致目标词汇不在采样范围内时的推测效果较差。因此，本文提出了SpecVocab方法来替代减少词汇量的方法。

Method: SpecVocab通过每步解码选择一个词汇子集来工作。通过对目标语言模型的不同部分进行评估，确定最合适的选择策略，并选择一个合适的大小来保证吞吐量与错误率之间的权衡。

Result: 实验表明，SpecVocab在多种任务中相较于最先进的投机解码方法EAGLE-3实现了更高的接受长度，平均吞吐量提高了8.1%。

Conclusion: SpecVocab提供了一种有效的方法来提高语言模型推理的效率，通过在每步解码时动态调整词汇子集，从而优化了速度与准确性。

Abstract: Speculative decoding has rapidly emerged as a leading approach for accelerating language model (LM) inference, as it offers substantial speedups while yielding identical outputs. This relies upon a small draft model, tasked with predicting the outputs of the target model. State-of-the-art speculative decoding methods use a draft model consisting of a single decoder layer and output embedding matrix, with the latter dominating drafting time for the latest LMs. Recent work has sought to address this output distribution bottleneck by reducing the vocabulary of the draft model. Although this can improve throughput, it compromises speculation effectiveness when the target token is out-of-vocabulary. In this paper, we argue for vocabulary speculation as an alternative to a reduced vocabulary. We propose SpecVocab, an efficient and effective method that selects a vocabulary subset per decoding step. Across a variety of tasks, we demonstrate that SpecVocab can achieve a higher acceptance length than state-of-the-art speculative decoding approach, EAGLE-3. Notably, this yields up to an 8.1% increase in average throughput over EAGLE-3.

</details>


### [147] [PrivAct: Internalizing Contextual Privacy Preservation via Multi-Agent Preference Training](https://arxiv.org/abs/2602.13840)
*Yuhan Cheng,Hancheng Ye,Hai Helen Li,Jingwei Sun,Yiran Chen*

Main category: cs.CL

TL;DR: PrivAct 是一种能够直接将情境隐私保护嵌入到模型生成行为中的多智能体学习框架，通过在每个智能体中嵌入隐私偏好，增强系统范围的情境完整性，同时实现更优的隐私与辅助性权衡。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于外部、推理时的干预，这些干预是脆弱的、特定场景的，可能会增加隐私攻击的范围。因此，需要一种能够直接将情境隐私保护嵌入到模型生成行为中的解决方案。

Method: PrivAct 通过在每个智能体中嵌入隐私偏好，直接将情境隐私保护整合进模型的行为中，提高了系统范围内的情境完整性，实现了更好的隐私与辅助性权衡，并通过多次实验验证了在不同大语言模型架构和基准上的改进效果。

Result: 实验结果显示，与现有方法相比，PrivAct 在上下文隐私保护方面表现出一致的改进，比基线减少了高达 12.32% 的泄漏率，同时保持了与基线相当的辅助性。并在多种多智能体拓扑结构中展示了零样本泛化能力和鲁棒性。

Conclusion: 综上所述，PrivAct 提供了一种有效的方法，可在多个 LLM 架构中增强情境隐私保护，为未来的智能系统开发提供了新的研究方向和实践基础。

Abstract: Large language model (LLM) agents are increasingly deployed in personalized tasks involving sensitive, context-dependent information, where privacy violations may arise in agents' action due to the implicitness of contextual privacy. Existing approaches rely on external, inference-time interventions which are brittle, scenario-specific, and may expand the privacy attack surface. We propose PrivAct, a contextual privacy-aware multi-agent learning framework that internalizes contextual privacy preservation directly into models' generation behavior for privacy-compliant agentic actions. By embedding privacy preferences into each agent, PrivAct enhances system-wide contextual integrity while achieving a more favorable privacy-helpfulness tradeoff. Experiments across multiple LLM backbones and benchmarks demonstrate consistent improvements in contextual privacy preservation, reducing leakage rates by up to 12.32% while maintaining comparable helpfulness, as well as zero-shot generalization and robustness across diverse multi-agent topologies. Code is available at https://github.com/chengyh23/PrivAct.

</details>


### [148] [Tutoring Large Language Models to be Domain-adaptive, Precise, and Safe](https://arxiv.org/abs/2602.13860)
*Somnath Banerjee*

Main category: cs.CL

TL;DR: 该研究旨在通过开发一种‘负责任的智能’框架来解决大型语言模型的生成能力和实际部署需求之间的矛盾，重点关注领域适应、伦理严谨性和文化/多语言对齐。


<details>
  <summary>Details</summary>
Motivation: 鉴于大型语言模型在人工智能领域的重要作用，研究强调了构建更安全、更具文化敏感性的系统的需求，以应对技术精度、伦理严谨性和全球包容性之间的平衡。

Method: 研究方法包括经典监督适应以满足特定任务需求，解码时对齐以确保安全性，以及利用人类反馈和偏好建模实现社会语言敏锐度。

Result: 研究结果表明，通过综合以上三种方法，可以有效地提高模型的技术性能、伦理安全性以及文化适应性。

Conclusion: 研究结论是，提出了一个综合框架，旨在将当前的大规模语言模型提升为更负责任且技术上更加先进的系统，能够在全球范围内推广并具有广泛的社会接受度。

Abstract: The overarching research direction of this work is the development of a ''Responsible Intelligence'' framework designed to reconcile the immense generative power of Large Language Models (LLMs) with the stringent requirements of real-world deployment. As these models become a transformative force in artificial intelligence, there is an urgent need to move beyond general-purpose architectures toward systems that are contextually aware, inherently safer, and deeply respectful of global cultural nuances. This research navigates three interconnected threads: domain adaptation to ensure technical precision, ethical rigor to mitigate adversarial vulnerabilities, and cultural/multilingual alignment to promote global inclusivity. The methodological trajectory moves from classical supervised adaptation for task-specific demands to decoding-time alignment for safety, finally leveraging human feedback and preference modeling to achieve sociolinguistic acuity.

</details>


### [149] [Bridging the Multilingual Safety Divide: Efficient, Culturally-Aware Alignment for Global South Languages](https://arxiv.org/abs/2602.13867)
*Somnath Banerjee,Rima Hazra,Animesh Mukherjee*

Main category: cs.CL

TL;DR: 该研究总结了近期关于大语言模型（LLMs）在低资源语言和代码混杂输入下的安全性问题，强调需要特定的文化背景评估和参与式工作流程来定义和缓解潜在危害。


<details>
  <summary>Details</summary>
Motivation: 由于大语言模型在低资源语言和代码混杂输入下表现不佳，且现有安全评估标准主要针对英语等高资源语言，这一研究旨在填补文化多样性方面的空白，寻求更符合地方特色的安全防范措施。

Method: 综合分析了现有文献和实验结果，提出了针对低资源语言和文化背景的安全防护策略，包括参数高效的安全引导、文化导向的评估数据以及参与式的协作流程。

Result: 该研究揭示了当前大语言模型在低资源语言环境中的局限性，指出了现有安全框架的不足，强调了进行本地化调整的必要性。

Conclusion: 未来的研究和实践应当注重在多语言和文化背景下构建更加包容和有效的AI安全框架，以确保技术的公平性和可行性。

Abstract: Large language models (LLMs) are being deployed across the Global South, where everyday use involves low-resource languages, code-mixing, and culturally specific norms. Yet safety pipelines, benchmarks, and alignment still largely target English and a handful of high-resource languages, implicitly assuming safety and factuality ''transfer'' across languages. Evidence increasingly shows they do not. We synthesize recent findings indicating that (i) safety guardrails weaken sharply on low-resource and code-mixed inputs, (ii) culturally harmful behavior can persist even when standard toxicity scores look acceptable, and (iii) English-only knowledge edits and safety patches often fail to carry over to low-resource languages. In response, we outline a practical agenda for researchers and students in the Global South: parameter-efficient safety steering, culturally grounded evaluation and preference data, and participatory workflows that empower local communities to define and mitigate harm. Our aim is to make multilingual safety a core requirement-not an add-on-for equitable AI in underrepresented regions.

</details>


### [150] [Evaluating Prompt Engineering Techniques for RAG in Small Language Models: A Multi-Hop QA Approach](https://arxiv.org/abs/2602.13890)
*Amir Hossein Mohammadi,Ali Moeinian,Zahra Razavizade,Afsaneh Fatemi,Reza Ramezani*

Main category: cs.CL

TL;DR: 本研究通过大规模实证研究，评估了不同提示模板对小语言模型（SLM）检索增强生成系统性能的影响。在HotpotQA数据集上测试了24种不同的提示模板，涵盖了标准RAG提示、文献中的九种技术以及14种新颖的混合变体，在Qwen2.5-3B Instruct和Gemma3-4B-It两个模型上均获得了显著的性能提升，最高可达84.5%。


<details>
  <summary>Details</summary>
Motivation: 在复杂、多跳问题解答任务中，提示模板设计是影响SLM-RAG系统性能的关键因素，但这一领域仍存在研究空白。

Method: 通过大量实证研究，研究了24种不同的提示模板对两种主要SLM（Qwen2.5-3B Instruct 和 Gemma3-4B-It）RAG系统性能的影响。

Result: 研究结果表明，与标准RAG提示相比，在Qwen2.5和Gemma3-4B-It上分别获得了高达83%和84.5%的显著性能提升，且可显著提升模型总体表现。

Conclusion: 研究提出了有关设计有效的SLM-RAG系统提示模板的具体分析和实用建议，为资源受限环境下的部署提供了实际指导。

Abstract: Retrieval Augmented Generation (RAG) is a powerful approach for enhancing the factual grounding of language models by integrating external knowledge. While widely studied for large language models, the optimization of RAG for Small Language Models (SLMs) remains a critical research gap, particularly in complex, multi-hop question-answering tasks that require sophisticated reasoning. In these systems, prompt template design is a crucial yet under-explored factor influencing performance. This paper presents a large-scale empirical study to investigate this factor, evaluating 24 different prompt templates on the HotpotQA dataset. The set includes a standard RAG prompt, nine well-formed techniques from the literature, and 14 novel hybrid variants, all tested on two prominent SLMs: Qwen2.5-3B Instruct and Gemma3-4B-It. Our findings, based on a test set of 18720 instances, reveal significant performance gains of up to 83% on Qwen2.5 and 84.5% on Gemma3-4B-It, yielding an improvement of up to 6% for both models compared to the Standard RAG prompt. This research also offers concrete analysis and actionable recommendations for designing effective and efficient prompts for SLM-based RAG systems, practically for deployment in resource-constrained environments.

</details>


### [151] [Pre-Editorial Normalization for Automatically Transcribed Medieval Manuscripts in Old French and Latin](https://arxiv.org/abs/2602.13905)
*Thibault Clérice,Rachel Bawden,Anthony Glaise,Ariane Pinche,David Smith*

Main category: cs.CL

TL;DR: 文章介绍了一种名为Pre-Editorial Normalization (PEN) 的新任务，旨在将图示性自动文本识别输出标准化为编辑规范，同时保持活字古文的忠实度。该研究提供了从CoMMA语料库衍生的新数据集，并使用ByT5模型进行了基准测试，实现了6.7%的字符错误率，显著优于先前模型。


<details>
  <summary>Details</summary>
Motivation: 现有的Atr模型或注重活字古文忠实度，或过分规范化，导致使用差距。PEN旨在通过标准化图示性Atr输出，同时保留编辑规范性，缩小这一差距。

Method: 研究者通过CoMMA语料库创建了一个新数据集，利用passim对齐数字化的古法语和拉丁语版本，并发布了包含4.66M样本的银色训练语料库和1.8k样本的金色评估语料库。新模型使用ByT5进行序列到序列的任务基准测试。

Result: 本研究提供了一个正式定义的PEN任务，一个4.66M样本的银色训练语料库，一个1.8k样本的金色评估语料库，并提出了一个实现6.7%字符错误率的规范化模型，显著优于先前的任务模型。

Conclusion: 本研究成功地定义了PEN任务，并展示了在图示性Atr输出标准化方面的重大进展，为未来的研究奠定了基础。

Abstract: Recent advances in Automatic Text Recognition (ATR) have improved access to historical archives, yet a methodological divide persists between palaeographic transcriptions and normalized digital editions. While ATR models trained on more palaeographically-oriented datasets such as CATMuS have shown greater generalizability, their raw outputs remain poorly compatible with most readers and downstream NLP tools, thus creating a usability gap. On the other hand, ATR models trained to produce normalized outputs have been shown to struggle to adapt to new domains and tend to over-normalize and hallucinate. We introduce the task of Pre-Editorial Normalization (PEN), which consists in normalizing graphemic ATR output according to editorial conventions, which has the advantage of keeping an intermediate step with palaeographic fidelity while providing a normalized version for practical usability. We present a new dataset derived from the CoMMA corpus and aligned with digitized Old French and Latin editions using passim. We also produce a manually corrected gold-standard evaluation set. We benchmark this resource using ByT5-based sequence-to-sequence models on normalization and pre-annotation tasks. Our contributions include the formal definition of PEN, a 4.66M-sample silver training corpus, a 1.8k-sample gold evaluation set, and a normalization model achieving a 6.7% CER, substantially outperforming previous models for this task.

</details>


### [152] [HLE-Verified: A Systematic Verification and Structured Revision of Humanity's Last Exam](https://arxiv.org/abs/2602.13964)
*Weiqi Zhai,Zhihai Wang,Jinghang Wang,Boyu Yang,Xiaogang Li,Xiang Xu,Bohan Wang,Peng Wang,Xingzhe Wu,Anfeng Li,Qiyuan Feng,Yuhao Zhou,Shoulin Han,Wenjie Luo,Yiyuan Li,Yaxuan Wang,Ruixian Luo,Guojie Lin,Peiyao Xiao,Chengliang Xu,Ben Wang,Zeyu Wang,Zichao Chen,Jianan Ye,Yijie Hu,Jialong Chen,Zongwen Shen,Yuliang Xu,An Yang,Bowen Yu,Dayiheng Liu,Junyang Lin,Hu Wei,Que Shen,Bing Zhao*

Main category: cs.CL

TL;DR: 论文提出了HLE-Verified，这是一个验证并通过的改进版H Humanity's Last Exam基准，通过两阶段验证修复流程，确保其公正性和可靠性，涵盖透明的验证协议和详细错误分类。与原始HLE相比，HLE-Verified在七个顶级语言模型上的评估中，平均准确度提高了7-10个百分点，尤其在存在问题或参考答案的项目上提高了显著，这支持了修订的有效性。


<details>
  <summary>Details</summary>
Motivation: 为了应对H Humanity's Last Exam（HLE）中存在的噪声数据问题，改进大语言模型在多样复杂问题上的评估方法。

Method: HLE-Verified通过两阶段验证修复流程来构建：首先是二元验证阶段，通过领域专家评审和模型交叉检查验证问题和最终答案，其次是修复阶段，针对可修正的项目进行双重独立专家修正、模型辅助审计和最终裁决。

Result: 在HLE-Verified上进行的七个顶级语言模型评估中，平均绝对准确度相比HLE提高了7-10个百分点，特别是在存在问题或参考答案的项目上，准确度提高了30-40个百分点。此外，模型的置信度与其遇到错误的问题陈述或参考答案密切相关。

Conclusion: HLE-Verified通过清除噪声提高了大语言模型在H Humanity's Last Exam样式的评估中的准确性，展现了其作为一种更加可靠和公正的基准的价值。

Abstract: Humanity's Last Exam (HLE) has become a widely used benchmark for evaluating frontier large language models on challenging, multi-domain questions. However, community-led analyses have raised concerns that HLE contains a non-trivial number of noisy items, which can bias evaluation results and distort cross-model comparisons. To address this challenge, we introduce HLE-Verified, a verified and revised version of HLE with a transparent verification protocol and fine-grained error taxonomy. Our construction follows a two-stage validation-and-repair workflow resulting in a certified benchmark. In Stage I, each item undergoes binary validation of the problem and final answer through domain-expert review and model-based cross-checks, yielding 641 verified items. In Stage II, flawed but fixable items are revised under strict constraints preserving the original evaluation intent, through dual independent expert repairs, model-assisted auditing, and final adjudication, resulting in 1,170 revised-and-certified items. The remaining 689 items are released as a documented uncertain set with explicit uncertainty sources and expertise tags for future refinement. We evaluate seven state-of-the-art language models on HLE and HLE-Verified, observing an average absolute accuracy gain of 7--10 percentage points on HLE-Verified. The improvement is particularly pronounced on items where the original problem statement and/or reference answer is erroneous, with gains of 30--40 percentage points. Our analyses further reveal a strong association between model confidence and the presence of errors in the problem statement or reference answer, supporting the effectiveness of our revisions. Overall, HLE-Verified improves HLE-style evaluations by reducing annotation noise and enabling more faithful measurement of model capabilities. Data is available at: https://github.com/SKYLENAGE-AI/HLE-Verified

</details>


### [153] [Chain-of-Thought Reasoning with Large Language Models for Clinical Alzheimer's Disease Assessment and Diagnosis](https://arxiv.org/abs/2602.13979)
*Tongze Zhang,Jun-En Ding,Melik Ozolcer,Fang-Ming Hung,Albert Chih-Chieh Yang,Feng Liu,Yi-Rou Ji,Sang Won Bae*

Main category: cs.CL

TL;DR: 本文提出了一种利用大型语言模型在临床电子健康记录中进行链式推理的阿尔茨海默病诊断框架，改善了诊断的稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: 传统的阿尔茨海默病诊断依赖于影像学检查和医生的临床评估，耗时且资源密集；而利用大型语言模型通过电子健康记录进行诊断，减少了对专业知识和资源的需求。

Method: 使用大型语言模型进行临床电子健康记录的链式推理，通过生成诊断推理路径帮助模型理解阿尔茨海默病的复杂因素，进而作出结构化的诊断预测。

Result: 提出的基于链式推理的诊断框架在多重CDR评分任务上显著提高了诊断稳定性和性能，对比零样本基线方法的F1得分提高了15%。

Conclusion: 该研究证明，利用大型语言模型进行链式推理可以有效提升阿尔茨海默病的诊断质量和可靠性，具有重要的临床应用价值。

Abstract: Alzheimer's disease (AD) has become a prevalent neurodegenerative disease worldwide. Traditional diagnosis still relies heavily on medical imaging and clinical assessment by physicians, which is often time-consuming and resource-intensive in terms of both human expertise and healthcare resources. In recent years, large language models (LLMs) have been increasingly applied to the medical field using electronic health records (EHRs), yet their application in Alzheimer's disease assessment remains limited, particularly given that AD involves complex multifactorial etiologies that are difficult to observe directly through imaging modalities. In this work, we propose leveraging LLMs to perform Chain-of-Thought (CoT) reasoning on patients' clinical EHRs. Unlike direct fine-tuning of LLMs on EHR data for AD classification, our approach utilizes LLM-generated CoT reasoning paths to provide the model with explicit diagnostic rationale for AD assessment, followed by structured CoT-based predictions. This pipeline not only enhances the model's ability to diagnose intrinsically complex factors but also improves the interpretability of the prediction process across different stages of AD progression. Experimental results demonstrate that the proposed CoT-based diagnostic framework significantly enhances stability and diagnostic performance across multiple CDR grading tasks, achieving up to a 15% improvement in F1 score compared to the zero-shot baseline method.

</details>


### [154] [The Sufficiency-Conciseness Trade-off in LLM Self-Explanation from an Information Bottleneck Perspective](https://arxiv.org/abs/2602.14002)
*Ali Zahedzadeh,Behnam Bahrak*

Main category: cs.CL

TL;DR: 研究通过引入一种评估管道，探讨了简洁性（即解释的精炼程度）与充分性（即解释是否能合理说明正确答案）之间的权衡，发现更简洁的解释通常仍能保持准确性，但过度压缩会降低性能。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型在多步问答任务上的表现得到提升，如何在保持准确性的前提下使解释更为精炼成为当前研究的关键问题。

Method: 研究基于信息瓶颈原理，将解释视为只保留生成正确答案所需信息的紧凑表示。通过引入一个评估管道来限制解释的长度，并利用多个语言模型在ARC挑战数据集上评估解释的充分性。

Result: 研究结果表明，更加简洁的解释在保持准确性的同时大大减少了解释的长度，而过度压缩的解释会导致性能下降。

Conclusion: 该研究为简化语言模型解释提供了一系列方法，并证明了适度的简洁性并不会损害语言模型的性能，反而有助于提高模型的效率。

Abstract: Large Language Models increasingly rely on self-explanations, such as chain of thought reasoning, to improve performance on multi step question answering. While these explanations enhance accuracy, they are often verbose and costly to generate, raising the question of how much explanation is truly necessary. In this paper, we examine the trade-off between sufficiency, defined as the ability of an explanation to justify the correct answer, and conciseness, defined as the reduction in explanation length. Building on the information bottleneck principle, we conceptualize explanations as compressed representations that retain only the information essential for producing correct answers.To operationalize this view, we introduce an evaluation pipeline that constrains explanation length and assesses sufficiency using multiple language models on the ARC Challenge dataset. To broaden the scope, we conduct experiments in both English, using the original dataset, and Persian, as a resource-limited language through translation. Our experiments show that more concise explanations often remain sufficient, preserving accuracy while substantially reducing explanation length, whereas excessive compression leads to performance degradation.

</details>


### [155] [GRRM: Group Relative Reward Modeling for Machine Translation](https://arxiv.org/abs/2602.14028)
*Sen Yang,Shanbo Cheng,Lu Xu,Jianbing Zhang,Shujian Huang*

Main category: cs.CL

TL;DR: 该研究提出并实现了Group Relative Reward Model (GRRM)，以解决Group Relative Policy Optimization (GRPO)在开放领域如机器翻译中的排名准确性问题。GRRM通过联合处理候选组并进行对比分析，提升了排名精度，并将其集成到GRPO训练循环中以优化翻译策略，取得了较好的翻译质量和推理能力。


<details>
  <summary>Details</summary>
Motivation: 传统的标量质量度量标准在区分精细的语言细微差别方面存在不足，为此研究提出Group Quality Metric（GQM）及其实现方式Group Relative Reward Model（GRRM），以提升开放领域如机器翻译的排名精度。

Method: 研究设计了GRRM模型，该模型通过联合处理候选组，进行对比分析，解决了在开放领域中的排名准确性问题。并将GRRM集成到GRPO的训练循环中，以优化翻译策略。

Result: 实验表明，GRRM在排名准确性和泛化翻译质量方面表现出色，与基线模型相当，并且能解锁与最新推理模型相媲美的推理能力。

Conclusion: 研究通过引入GRRM，显著提升了开放领域中机器翻译的质量和推理能力，并将相关代码、数据集和模型部署在公共仓库中。

Abstract: While Group Relative Policy Optimization (GRPO) offers a powerful framework for LLM post-training, its effectiveness in open-ended domains like Machine Translation hinges on accurate intra-group ranking. We identify that standard Scalar Quality Metrics (SQM) fall short in this context; by evaluating candidates in isolation, they lack the comparative context necessary to distinguish fine-grained linguistic nuances. To address this, we introduce the Group Quality Metric (GQM) paradigm and instantiate it via the Group Relative Reward Model (GRRM). Unlike traditional independent scorers, GRRM processes the entire candidate group jointly, leveraging comparative analysis to rigorously resolve relative quality and adaptive granularity. Empirical evaluations confirm that GRRM achieves competitive ranking accuracy among all baselines. Building on this foundation, we integrate GRRM into the GRPO training loop to optimize the translation policy. Experimental results demonstrate that our framework not only improves general translation quality but also unlocks reasoning capabilities comparable to state-of-the-art reasoning models. We release codes, datasets, and model checkpoints at https://github.com/NJUNLP/GRRM.

</details>


### [156] [Geometry-Preserving Aggregation for Mixture-of-Experts Embedding Models](https://arxiv.org/abs/2602.14039)
*Sajjad Kachuee,Mohammad Sharifkhani*

Main category: cs.CL

TL;DR: 本文提出了一种新的聚合操作Spherical Barycentric Aggregation (SBA)，以解决MoE模型中假设的线性子空间与专家表示几何结构不一致的问题，SBA能够保持超球面结构并且能够与现有的路由机制完全兼容，实验结果显示这样做可以提高性能，且不会增加训练成本。


<details>
  <summary>Details</summary>
Motivation: Mixture-of-Experts (MoE)模型在聚合专家输出时隐含假设嵌入空间存在线性子空间结构，但这种假设与专家表示的真实几何结构不符。本文旨在解决这一问题。

Method: 通过几何分析，作者发现专家输出实际位于一个共享的超球面流形上，该流形由紧密集中的范数和显著的角度分离组成。因此，线性聚合会引起向流形内部的内聚效应，导致向量的大小和方向失真，损害嵌入的可比性。为了解决这个问题，作者提出了Spherical Barycentric Aggregation (SBA)，这是一种几何保真的聚合操作，能够分离径向和角度分量，以保持超球面结构，同时完全兼容现有的路由机制。

Result: 在Massive Text Embedding Benchmark (MTEB)上的任务，包括语义相似性、聚类和重复问题检测，SBA展示出了持续的性能提升，并且与传统方法相比，保持了等同的训练成本和稳定表现。

Conclusion: 本文提出的Spherical Barycentric Aggregation (SBA)对于解决MoE模型中线性子空间假设与实际几何结构不相符的问题具有重要意义。通过这种聚合操作，可以保持超球面结构，从而实现更好的嵌入表示效果。

Abstract: Mixture-of-Experts (MoE) embedding models combine expert outputs using weighted linear summation, implicitly assuming a linear subspace structure in the embedding space. This assumption is shown to be inconsistent with the geometry of expert representations. Geometric analysis of a modern MoE embedding model reveals that expert outputs lie on a shared hyperspherical manifold characterized by tightly concentrated norms and substantial angular separation. Under this geometry, linear aggregation induces inward collapse toward the manifold interior, distorting vector magnitude and direction and reducing embedding comparability. To address this inconsistency, Spherical Barycentric Aggregation (SBA) is introduced as a geometry-preserving aggregation operator that separates radial and angular components to maintain hyperspherical structure while remaining fully compatible with existing routing mechanisms. Experiments on selected tasks from the Massive Text Embedding Benchmark (MTEB), including semantic similarity, clustering, and duplicate question detection, demonstrate consistent performance improvements with identical training cost and full stability. Additional geometric analyses confirm that SBA prevents aggregation-induced collapse and preserves hyperspherical consistency, highlighting the importance of geometry-aware aggregation in MoE embedding architectures.

</details>


### [157] [Context Shapes LLMs Retrieval-Augmented Fact-Checking Effectiveness](https://arxiv.org/abs/2602.14044)
*Pietro Bernardelle,Stefano Civelli,Kevin Roitero,Gianluca Demartini*

Main category: cs.CL

TL;DR: 该研究评估了大型语言模型在验证事实方面的表现，发现模型在处理长上下文时表现不稳定，并且证据的位置对其准确性有显著影响。


<details>
  <summary>Details</summary>
Motivation: 鉴于LLMs在多样任务中表现出强大的推理能力，但其在长上下文上的表现一致性较差。因此，该研究探讨了LLMs在基于模型的事实验证中的上下文影响，并评估了参数化的事实知识和证据位置对验证准确性的影响。

Method: 研究使用了三个数据集（HOVER、FEVEROUS和ClimateFEVER），并选择了三个不同参数量和模型家族的开放源码模型（7B、32B和70B参数，Llama-3.1、Qwen2.5和Qwen3）进行验证。通过评估参数化的事实知识和证据位置对不同上下文长度的影响，研究进一步分析了LLMs在事实验证方面的表现。

Result: 研究发现，LLMs对于事实陈述具有非平凡的参数知识，但随着上下文长度的增加，验证准确性通常会下降。此外，与之前的研究一致，上下文中的证据位置对准确性有显著影响，关键证据接近提示开头或结尾时准确性更高。

Conclusion: 研究强调了提示结构在检索增强的事实检查系统中的重要性，尤其在考虑证据位置对于提高验证准确性的关键作用。

Abstract: Large language models (LLMs) show strong reasoning abilities across diverse tasks, yet their performance on extended contexts remains inconsistent. While prior research has emphasized mid-context degradation in question answering, this study examines the impact of context in LLM-based fact verification. Using three datasets (HOVER, FEVEROUS, and ClimateFEVER) and five open-source models accross different parameters sizes (7B, 32B and 70B parameters) and model families (Llama-3.1, Qwen2.5 and Qwen3), we evaluate both parametric factual knowledge and the impact of evidence placement across varying context lengths. We find that LLMs exhibit non-trivial parametric knowledge of factual claims and that their verification accuracy generally declines as context length increases. Similarly to what has been shown in previous works, in-context evidence placement plays a critical role with accuracy being consistently higher when relevant evidence appears near the beginning or end of the prompt and lower when placed mid-context. These results underscore the importance of prompt structure in retrieval-augmented fact-checking systems.

</details>


### [158] [LogitsCoder: Towards Efficient Chain-of-Thought Path Search via Logits Preference Decoding for Code Generation](https://arxiv.org/abs/2602.14054)
*Jizheng Chen,Weiming Zhang,Xinyi Dai,Weiwen Liu,Kounianhua Du,Yasheng Wang,Ruiming Tang,Yong Yu,Weinan Zhang*

Main category: cs.CL

TL;DR: LogitsCoder 提出了一种新型的轻量级框架，通过 logits 层级的控制机制增强了编码生成中的推理链，有效解决了 underthinking 和 overthinking 两个主要问题，从而提高了代码生成的效率和质量。


<details>
  <summary>Details</summary>
Motivation: 随着代码生成任务的复杂性增加，现有方法面临着推理路径浅显（underthinking）和繁琐（overthinking）的挑战，LogitsCoder 的提出旨在解决这些问题，以实现更高效的代码生成。

Method: LogitsCoder 通过 Logits Preference Decoding 引导 token 选择，以及 Logits Rank Based Path Selection 和 Thoughts Aggregation 来选择和聚合多样化的推理路径，进而形成连贯且高效的推理链。

Result: 经过大量实验验证，LogitsCoder 生成的推理链更加高效且质量更高，相较于基准方法具有显著的优势。

Conclusion: LogitsCoder 为改进代码生成任务中的链式推理提供了一种有效的解决方案，能够显著提高生成代码的质量和效率。

Abstract: Code generation remains a challenging task that requires precise and structured reasoning. Existing Test Time Scaling (TTS) methods, including structured tree search, have made progress in exploring reasoning paths but still face two major challenges: (1) underthinking, where reasoning chains tend to be shallow and fail to capture the full complexity of problems; and (2) overthinking, where overly verbose reasoning leads to inefficiency and increased computational costs. To address these issues, we propose LogitsCoder, a novel framework that enhances chain-of-thought reasoning through lightweight, logit-level control mechanisms for code generation. LogitsCoder iteratively generates and refines reasoning steps by first steering token selection toward statistically preferred patterns via Logits Preference Decoding, then selecting and aggregating diverse reasoning paths using Logits Rank Based Path Selection and Thoughts Aggregation. This results in coherent and effective reasoning chains that balance depth and efficiency. Extensive experiments demonstrate that LogitsCoder produces more efficient and higher-quality reasoning chains, leading to superior code generation performance compared to baseline methods.

</details>


### [159] [LM-Lexicon: Improving Definition Modeling via Harmonizing Semantic Experts](https://arxiv.org/abs/2602.14060)
*Yang Liu,Jiaye Yang,Weikang Li,Jiahui Liang,Yang Li,Lingyong Yan*

Main category: cs.CL

TL;DR: LM-Lexicon 使用稀疏混合专家架构和数据聚类、语义专家学习及模型合并，将定义建模任务分解为专门的语义域中进行，通过专家专项化、领域级语义路由机制以及测试时计算和语义专家扩展，实现了比现有最佳模型高出 7% 的 BLEU 分数。


<details>
  <summary>Details</summary>
Motivation: 开发高效的基于语义的应用程序所需的高质量定义建模方法。

Method: 引入 LM-Lexicon 方法，利用数据聚类、语义专家学习和模型合并来建立稀疏混合专家架构。将定义建模任务分解为专门的语义领域，通过训练小的语言模型作为领域专家来实现。

Result: 在五个广泛使用的基准上，LM-Lexicon 实现了比现有最优模型高出 7% 的 BLEU 分数。细粒度专家专项化提高了约 10% 的定义质量，领域级语义路由机制比常规令牌级路由机制提高了约 1% 的专家有效性，以及通过测试时计算和语义专家扩展可以进一步提高性能。

Conclusion: LM-Lexicon 推动了定义建模的发展，对于语言模型在语义密集型应用程序中的开发提供了新的见解。

Abstract: We introduce LM-Lexicon, an innovative definition modeling approach that incorporates data clustering, semantic expert learning, and model merging using a sparse mixture-of-experts architecture. By decomposing the definition modeling task into specialized semantic domains, where small language models are trained as domain experts, LM-Lexicon achieves substantial improvements (+7% BLEU score compared with the prior state-of-the-art model) over existing methods on five widely used benchmarks. Empirically, we demonstrate that 1) the clustering strategy enables fine-grained expert specialization with nearly 10% improvement in definition quality; 2) the semantic-aware domain-level routing mechanism achieves higher expert efficacy (+1%) than conventional token-level routing; and 3) further performance gains can be obtained through test-time compute and semantic expert scaling. Our work advances definition modeling while providing insights into the development of efficient language models for semantic-intensive applications.

</details>


### [160] [From Scarcity to Scale: A Release-Level Analysis of the Pashto Common Voice Dataset](https://arxiv.org/abs/2602.14062)
*Jandad Jahani,Mursal Dawodi,Jawid Ahmad Baktash*

Main category: cs.CL

TL;DR: 本研究对Mozilla公共语音语料库中的普什图语部分进行了版本层级分析，集中在2025年12月的第24版本，并分析了规模、验证效率、贡献者参与不平等、元数据完整性和文本层结构集中度，结果显示验证参与度高度集中、年龄分布呈现年轻化趋势、性别标签缺失严重影响审计，并建议扩大验证能力和增加多样化参与。


<details>
  <summary>Details</summary>
Motivation: 由于普什图语长期缺乏大型开放许可的语音数据，该研究旨在对Mozilla公共语音语料库中的普什图语部分进行分析，填补资源不足的问题。

Method: 通过对不同版本的普什图语语音数据进行详细记录和分析，该研究量化了语言数据集的增长趋势，并通过指标如Gini指数、年龄分布和性别标签缺失比例等进行深入分析。

Result: 研究发现普什图语数据经历了显著增长，验证效率有所提升，但验证参与度高度集中，年龄集中在年轻成年人，性别标签缺失严重。此外，提示重用率较低，表明数据集结构集中更多地由贡献者活动不均引起。

Conclusion: 研究结果强调了扩大验证能力和增加多样化参与者对于提高数据集成熟度的重要性，并提出现实优先事项以促进低资源语音数据集的发展。

Abstract: Large, openly licensed speech datasets are essential for building automatic speech recognition (ASR) systems, yet many widely spoken languages remain underrepresented in public resources. Pashto, spoken by more than 60 million people, has historically lacked large-scale openly licensed speech data suitable for modern ASR development.
  This paper presents a release-level analysis of the Pashto component of the Mozilla Common Voice corpus, focusing on version 24.0 (December 2025) and contextualizing trends across major releases. We document rapid growth from 1.49 recorded hours in mid-2023 to 2,768.7 total hours in 2025, including 975.89 validated hours available for supervised ASR training.
  Beyond scale, we analyze validation throughput, contributor participation inequality, demographic metadata completeness, and sentence-level concentration in the validated subset. We find that participation is extremely concentrated (Gini = 0.941), age representation is strongly skewed toward young adults, and 41.97\% of clips lack self-reported gender labels, limiting subgroup auditing based on metadata. At the textual level, prompt reuse is moderate: 35.88\% of unique sentences account for 50\% of validated clips, suggesting that structural concentration is driven primarily by uneven contributor activity rather than dominance of a small prompt set.
  These results provide a quantitative audit of a rapidly scaling low-resource speech corpus and highlight practical priorities for improving dataset maturity, including expanded validation capacity and broader demographic participation.

</details>


### [161] [Open Rubric System: Scaling Reinforcement Learning with Pairwise Adaptive Rubric](https://arxiv.org/abs/2602.14069)
*Ruipeng Jia,Yunyi Yang,Yuxin Wu,Yongbo Gai,Siyuan Tao,Mengyu Zhou,Jianhe Lin,Xiaoxi Jiang,Guanjun Jiang*

Main category: cs.CL

TL;DR: 本文提出了一种基于评语系统的开放式评分系统（OpenRS），这是一种插件式框架，利用成对适应性元评语（PAMR）和轻量级可验证评语（PVR）来实现非验证任务的鲁棒对齐，通过元评语的显式声明和基于语义的自适应评语生成机制优化了开放场景下的区别度。


<details>
  <summary>Details</summary>
Motivation: 当前的标量奖励模型会将多维人类偏好压缩成单一不透明评分，容易导致开放性任务中的鲁棒性问题和奖励劫持。这对非可验证任务的稳健对齐构成了挑战。

Method: OpenRS框架采用基于成对适应性元评语（PAMR）和轻量级可验证评语（PVR）的方法，同时引入了双层次元评语细化流程（自动化进化细化通用原则，可重复的人工辅助细化领域原则），并在成对RL训练中作为奖励监督实现。

Result: 验证表明OpenRS能够在开放性场景中有效避免单点奖励建模带来的瓶颈问题，增强系统的鲁棒性。同时也提供了明确的原则性和可验证性机制。

Conclusion: OpenRS为非验证任务的鲁棒对齐提供了一种新型框架，通过引入显式的评分机制和灵活的自适应评语机制解决了传统标量奖励模型中存在的信息瓶颈和脆弱性问题，展现了在开放性及复杂任务中的潜力。

Abstract: Scalar reward models compress multi-dimensional human preferences into a single opaque score, creating an information bottleneck that often leads to brittleness and reward hacking in open-ended alignment. We argue that robust alignment for non-verifiable tasks is fundamentally a principle generalization problem: reward should not be a learned function internalized into a judge, but an explicit reasoning process executed under inspectable principles. To operationalize this view, we present the Open Rubric System (OpenRS), a plug-and-play, rubrics-based LLM-as-a-Judge framework built around Pairwise Adaptive Meta-Rubrics (PAMR) and lightweight Pointwise Verifiable Rubrics (PVRs), which provide both hard-constraint guardrails and verifiable reward components when ground-truth or programmatic checks are available. OpenRS uses an explicit meta-rubric -- a constitution-like specification that governs how rubrics are instantiated, weighted, and enforced -- and instantiates adaptive rubrics on the fly by conditioning on the semantic differences between two candidate responses. It then performs criterion-wise pairwise comparisons and aggregates criterion-level preferences externally, avoiding pointwise weighted scalarization while improving discriminability in open-ended settings. To keep principles consistent yet editable across various domains, we introduce a two-level meta-rubric refinement pipeline (automated evolutionary refinement for general principles and a reproducible human-in-the-loop procedure for domain principles), complemented with pointwise verifiable rubrics that act as both guardrails against degenerate behaviors and a source of verifiable reward for objective sub-tasks. Finally, we instantiate OpenRS as reward supervision in pairwise RL training.

</details>


### [162] [Annotation-Efficient Vision-Language Model Adaptation to the Polish Language Using the LLaVA Framework](https://arxiv.org/abs/2602.14073)
*Grzegorz Statkiewicz,Alicja Dobrzeniecka,Karolina Seweryn,Aleksandra Krasnodębska,Karolina Piosek,Katarzyna Bogusz,Sebastian Cygert,Wojciech Kusa*

Main category: cs.CL

TL;DR: 该研究通过自动翻译和过滤现有数据集，并结合合成的波兰数据，创建了一组波兰语言的视觉-语言模型。结果显示，即使主要依赖自动翻译和少量的手动干预，该方法也在多项评估中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言模型多数基于英语文本训练，对其他语言和地区文化的适应性较差，限制了它们的应用潜力。因此，研究者选择波兰作为实验对象，使用自动翻译和轻量级过滤策略，创建大规模自动化翻译并结合生成少量合成数据的方法提升波兰语言模型的表现。

Method: 研究者首先使用全自动管道翻译并过滤已有视觉语言数据集，然后结合少量合成的波兰数据，构建了一组波兰语言的视觉语言模型。

Result: 实验结果表明，该团队的方法在多项评估中表现卓越，特别是在波兰本地化MMBench测试中提升了9.5%，并提供了更高质量的生成性注释，经过人类标注者评估语言正确性更高。

Conclusion: 研究证明大型自动翻译方法结合轻量级过滤能够有效为低资源语言创建高质量的多模态模型，但仍面临文化覆盖和评估挑战。研究者还公布了他们的模型和评价数据集，以支持未来的开发研究。

Abstract: Most vision-language models (VLMs) are trained on English-centric data, limiting their performance in other languages and cultural contexts. This restricts their usability for non-English-speaking users and hinders the development of multimodal systems that reflect diverse linguistic and cultural realities. In this work, we reproduce and adapt the LLaVA-Next methodology to create a set of Polish VLMs. We rely on a fully automated pipeline for translating and filtering existing multimodal datasets, and complement this with synthetic Polish data for OCR and culturally specific tasks. Despite relying almost entirely on automatic translation and minimal manual intervention to the training data, our approach yields strong results: we observe a +9.5% improvement over LLaVA-1.6-Vicuna-13B on a Polish-adapted MMBench, along with higher-quality captions in generative evaluations, as measured by human annotators in terms of linguistic correctness. These findings highlight that large-scale automated translation, combined with lightweight filtering, can effectively bootstrap high-quality multimodal models for low-resource languages. Some challenges remain, particularly in cultural coverage and evaluation. To facilitate further research, we make our models and evaluation dataset publicly available.

</details>


### [163] [CCiV: A Benchmark for Structure, Rhythm and Quality in LLM-Generated Chinese \textit{Ci} Poetry](https://arxiv.org/abs/2602.14081)
*Shangqing Zhao,Yupei Ren,Yuhao Zhou,Xiaopeng Bai,Man Lan*

Main category: cs.CL

TL;DR: 该研究提出了一个用于评估大型语言模型生成古典中国《词》诗歌能力的基准——CCiV，评估了17个模型在结构、节奏和质量三个方面的能力，并指出模型常生成历史变体且遵守声调模式比结构规则更难，此外提示增强可以改进更强模型的结构和声调控制，但可能损害较弱模型。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的发展，对其生成古典中国《词》诗歌能力的评估和提升成为一个重要课题。CCiV基准的提出旨在系统性地评估模型在这方面的表现，并发现生成过程中存在的关键问题。

Method: 研究团队设计了CCiV基准，对17个不同的模型进行了综合评估，通过分析它们生成的30种《Cipai》形式，探索了模型的表现。

Result: 研究发现，模型容易生成历史变体形式，而遵守声调模式远比遵循结构规则更具挑战性。此外，形式感知型提示可改善模型的结构和声调控制，但可能对较弱模型不利。在样本中还发现形式正确性和文学质量之间存在弱且不稳定的联系。

Conclusion: 这项研究表明，变体感知评估和更全面的限制性创造性生成方法对于提高大型语言模型在古典中国《词》诗歌生成上的表现至关重要。

Abstract: The generation of classical Chinese \textit{Ci} poetry, a form demanding a sophisticated blend of structural rigidity, rhythmic harmony, and artistic quality, poses a significant challenge for large language models (LLMs). To systematically evaluate and advance this capability, we introduce \textbf{C}hinese \textbf{Ci}pai \textbf{V}ariants (\textbf{CCiV}), a benchmark designed to assess LLM-generated \textit{Ci} poetry across these three dimensions: structure, rhythm, and quality. Our evaluation of 17 LLMs on 30 \textit{Cipai} reveals two critical phenomena: models frequently generate valid but unexpected historical variants of a poetic form, and adherence to tonal patterns is substantially harder than structural rules. We further show that form-aware prompting can improve structural and tonal control for stronger models, while potentially degrading weaker ones. Finally, we observe weak and inconsistent alignment between formal correctness and literary quality in our sample. CCiV highlights the need for variant-aware evaluation and more holistic constrained creative generation methods.

</details>


### [164] [Character-aware Transformers Learn an Irregular Morphological Pattern Yet None Generalize Like Humans](https://arxiv.org/abs/2602.14100)
*Akhilesh Kakolu Ramarao,Kevin Tang,Dinah Baer-Henney*

Main category: cs.CL

TL;DR: This paper examines whether encoder-decoder models can learn and generalize the L-shaped morphome in Spanish, a unique morphological pattern. The study compares different transformer architectures and finds that position-invariant models better capture the L-shaped pattern, but do not generalize it to novel forms. Human generalization is more aligned with the L-shaped pattern, indicating a gap between statistical reproduction and morphological abstraction.


<details>
  <summary>Details</summary>
Motivation: The research aims to bridge the gap between neural network models and human cognitive models in the context of morphological learning, specifically focusing on a unique pattern known as the L-shaped morphome in Spanish verbs.

Method: Five different encoder-decoder transformers were compared on their ability to learn and generalize the L-shaped morphome in Spanish. The transformers varied in their positional encoding (sequential vs. position-invariant) and their representation of linguistic tags (atomic vs. decomposed).

Result: Position-invariant models showed better performance in recovering the L-shaped paradigm clustering compared to sequential models. However, none of the models could productively generalize this pattern to novel forms. There was also a mismatch in generalization direction compared to human behavior, which aligns more with the L-shaped pattern.

Conclusion: The study concludes that while position-invariant models can better capture the key features of the L-shaped morphome, they fall short in morphological abstraction, especially in generalizing to novel forms. This highlights the limitations of current neural network models in replicating human-like generalizations in morphology.

Abstract: Whether neural networks can serve as cognitive models of morphological learning remains an open question. Recent work has shown that encoder-decoder models can acquire irregular patterns, but evidence that they generalize these patterns like humans is mixed. We investigate this using the Spanish \emph{L-shaped morphome}, where only the first-person singular indicative (e.g., \textit{pongo} `I put') shares its stem with all subjunctive forms (e.g., \textit{ponga, pongas}) despite lacking apparent phonological, semantic, or syntactic motivation. We compare five encoder-decoder transformers varying along two dimensions: sequential vs. position-invariant positional encoding, and atomic vs. decomposed tag representations. Positional encoding proves decisive: position-invariant models recover the correct L-shaped paradigm clustering even when L-shaped verbs are scarce in training, whereas sequential positional encoding models only partially capture the pattern. Yet none of the models productively generalize this pattern to novel forms. Position-invariant models generalize the L-shaped stem across subjunctive cells but fail to extend it to the first-person singular indicative, producing a mood-based generalization rather than the L-shaped morphomic pattern. Humans do the opposite, generalizing preferentially to the first-person singular indicative over subjunctive forms. None of the models reproduce the human pattern, highlighting the gap between statistical pattern reproduction and morphological abstraction.

</details>


### [165] [A Multi-Agent Framework for Medical AI: Leveraging Fine-Tuned GPT, LLaMA, and DeepSeek R1 for Evidence-Based and Bias-Aware Clinical Query Processing](https://arxiv.org/abs/2602.14158)
*Naeimeh Nourmohammadi,Md Meem Hossain,The Anh Han,Safina Showkat Ara,Zia Ush Shamszaman*

Main category: cs.CL

TL;DR: 该研究提出了一种结合互补的大语言模型、证据检索、不确定性评估和偏见检查的多智能体医疗问答框架，旨在提高回答的可靠性。经过初步训练和评估，DeepSeek R1在生成质量上表现最佳。该框架分为两阶段，第一阶段进行模型训练和基准测试，第二阶段构建了一种模块化的多智能体管道来提供详细的解释、证据检索和澄清一致性的改进。通过安全机制、证据增强和延迟结果，该系统在实际应用中表现良好。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决大型语言模型在医疗领域缺乏验证、证据不足以及置信度信号不可靠的问题。

Method: 研究首先对三种代表性的LLM（GPT、LLaMA和DeepSeek R1）进行了微调，并在MedQuAD衍生的医疗问答数据上进行了基准测试。接着，构建了一个模块化的多智能体管道，分别用于临床推理、证据检索和精细调整，还引入了安全机制来检测偏见。最终，通过综合评估该系统的性能。

Result: DeepSeek R1在生成质量上表现最佳，在零样本评估中显著超越专为生物医学设计的BioGPT。全系统实现了87%的准确率，证据增强减少了不确定性，总体延迟为36.5秒。安全机制有效降低了偏见，并通过证据增强提高了回答的可靠性。

Conclusion: 该研究展示了一种可以缓解当前单模型限制并提供可靠、可扩展的医疗AI设计的多智能体框架。

Abstract: Large language models (LLMs) show promise for healthcare question answering, but clinical use is limited by weak verification, insufficient evidence grounding, and unreliable confidence signalling. We propose a multi-agent medical QA framework that combines complementary LLMs with evidence retrieval, uncertainty estimation, and bias checks to improve answer reliability. Our approach has two phases. First, we fine-tune three representative LLM families (GPT, LLaMA, and DeepSeek R1) on MedQuAD-derived medical QA data (20k+ question-answer pairs across multiple NIH domains) and benchmark generation quality. DeepSeek R1 achieves the strongest scores (ROUGE-1 0.536 +- 0.04; ROUGE-2 0.226 +-0.03; BLEU 0.098 -+ 0.018) and substantially outperforms the specialised biomedical baseline BioGPT in zero-shot evaluation. Second, we implement a modular multi-agent pipeline in which a Clinical Reasoning agent (fine-tuned LLaMA) produces structured explanations, an Evidence Retrieval agent queries PubMed to ground responses in recent literature, and a Refinement agent (DeepSeek R1) improves clarity and factual consistency; an optional human validation path is triggered for high-risk or high-uncertainty cases. Safety mechanisms include Monte Carlo dropout and perplexity-based uncertainty scoring, plus lexical and sentiment-based bias detection supported by LIME/SHAP-based analyses. In evaluation, the full system achieves 87% accuracy with relevance around 0.80, and evidence augmentation reduces uncertainty (perplexity 4.13) compared to base responses, with mean end-to-end latency of 36.5 seconds under the reported configuration. Overall, the results indicate that agent specialisation and verification layers can mitigate key single-model limitations and provide a practical, extensible design for evidence-based and bias-aware medical AI.

</details>


### [166] [Index Light, Reason Deep: Deferred Visual Ingestion for Visual-Dense Document Question Answering](https://arxiv.org/abs/2602.14162)
*Tao Xu*

Main category: cs.CL

TL;DR: DVI框架采用了一种需求驱动的视觉摄入策略，它在索引阶段只进行轻量级元数据提取，用户提问时才进行具体的视觉理解。与传统的预摄入方法相比，DVI在保留整体准确性的前提下降低了VLM的成本，并提高了视觉必要查询的有效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于前端视觉摄入，导致成本高、可靠性差且不可恢复的问题，因此需要一种新的框架来改进。

Method: DVI框架采取了需求驱动的视觉摄入策略，索引阶段仅进行元数据提取，而在用户提出具体问题时才进行视觉理解。

Result: 实验中，DVI在保持整体准确性的基础上，与预摄入方法相比降低了VLM的成本。对于视觉必要查询，DVI的有效率达到50%（预摄入为0%），并且实现了100%的页局部化（搜索空间压缩98%）。此外，DVI还支持交互式细化和渐进式缓存。

Conclusion: DVI通过将问题答案获取转化为局部化问题来解决传统QA准确性问题，提供了一种高效、可靠的工作流程。

Abstract: Existing multimodal document question answering methods universally adopt a supply-side ingestion strategy: running a Vision-Language Model (VLM) on every page during indexing to generate comprehensive descriptions, then answering questions through text retrieval. However, this "pre-ingestion" approach is costly (a 113-page engineering drawing package requires approximately 80,000 VLM tokens), end-to-end unreliable (VLM outputs may fail to be correctly retrieved due to format mismatches in the retrieval infrastructure), and irrecoverable once it fails. This paper proposes the Deferred Visual Ingestion (DVI) framework, adopting a demand-side ingestion strategy: the indexing phase performs only lightweight metadata extraction, deferring visual understanding to the moment users pose specific questions. DVI's core principle is "Index for locating, not understanding"--achieving page localization through structured metadata indexes and BM25 full-text search, then sending original images along with specific questions to a VLM for targeted analysis. Experiments on two real industrial engineering drawings (113 pages + 7 pages) demonstrate that DVI achieves comparable overall accuracy at zero ingestion VLM cost (46.7% vs. 48.9%), an effectiveness rate of 50% on visually necessary queries (vs. 0% for pre-ingestion), and 100% page localization (98% search space compression). DVI also supports interactive refinement and progressive caching, transforming the "QA accuracy" problem into a "page localization" problem--once the correct drawing page is found, obtaining the answer becomes a matter of interaction rounds.

</details>


### [167] [GPT-5 vs Other LLMs in Long Short-Context Performance](https://arxiv.org/abs/2602.14188)
*Nima Esmi,Maryam Nezhad-Moghaddam,Fatemeh Borhani,Asadollah Shahbahrami,Amin Daemdoost,Georgi Gaydadjiev*

Main category: cs.CL

TL;DR: 该研究评估了四款先进模型（Grok-4, GPT-4, Gemini 2.5, GPT-5）在长上下文任务中的表现，使用了三个数据集，包括社交媒体帖子和烹饪菜谱、数学问题等补充数据集。结果显示，随着输入量增加到5K帖子（70K词元），所有模型性能显著下降，准确率降至约50-53%。GPT-5在精度方面表现突出，为95%，适用于敏感应用如抑郁检测。研究还指出，'中间丢失'问题在新模型中有较大改善，突显了复杂高体积数据任务中实际性能与理论能力之间的差距。


<details>
  <summary>Details</summary>
Motivation: 研究关注LLMs处理长上下文能力的实际表现与理论潜力之间的差距，特别是在处理大量详细信息的任务上。研究使用多个数据集评估了多种先进模型在长短上下文任务中的表现。

Method: 研究选择了Grok-4, GPT-4, Gemini 2.5, GPT-5四个最先进的模型，并使用了两个补充数据集（烹饪菜谱和数学问题）和一个主数据集（20K社交媒体帖子）进行抑郁症检测。通过分析模型在不同输入规模下的表现来评估它们的实际能力。

Result: 研究发现，当社交媒体帖子输入超过5K（70K词元）时，所有模型的性能显著下降，准确率降至约50-53%。特别地，GPT-5在精度上保持在约95%，这表明其适合处理敏感信息如情绪分析。另外，研究表明'中间丢失'问题在新模型中的影响显著减弱。

Conclusion: 研究指出了LLMs在复杂高体积数据任务中的实际表现与理论潜力之间的差距。研究强调除了简单准确率之外，还应关注模型在具体应用中的性能指标。

Abstract: With the significant expansion of the context window in Large Language Models (LLMs), these models are theoretically capable of processing millions of tokens in a single pass. However, research indicates a significant gap between this theoretical capacity and the practical ability of models to robustly utilize information within long contexts, especially in tasks that require a comprehensive understanding of numerous details. This paper evaluates the performance of four state-of-the-art models (Grok-4, GPT-4, Gemini 2.5, and GPT-5) on long short-context tasks. For this purpose, three datasets were used: two supplementary datasets for retrieving culinary recipes and math problems, and a primary dataset of 20K social media posts for depression detection. The results show that as the input volume on the social media dataset exceeds 5K posts (70K tokens), the performance of all models degrades significantly, with accuracy dropping to around 50-53% for 20K posts. Notably, in the GPT-5 model, despite the sharp decline in accuracy, its precision remained high at approximately 95%, a feature that could be highly effective for sensitive applications like depression detection. This research also indicates that the "lost in the middle" problem has been largely resolved in newer models. This study emphasizes the gap between the theoretical capacity and the actual performance of models on complex, high-volume data tasks and highlights the importance of metrics beyond simple accuracy for practical applications.

</details>


### [168] [Knowing When Not to Answer: Abstention-Aware Scientific Reasoning](https://arxiv.org/abs/2602.14189)
*Samir Abdaljalil,Erchin Serpedin,Hasan Kurban*

Main category: cs.CL

TL;DR: 该研究提出了一种基于告警能力的评估框架，该框架通过将科学声明分解为最小条件，使用自然语言推理审核每个条件，并根据证据选择支持、反驳或告警。研究发现，在支持成本较低的情况下，基于置信的告警在控制错误方面效果显著。


<details>
  <summary>Details</summary>
Motivation: 现有的评估框架通常假设模型必须始终提供一个明确的答案，但在科学环境中，不支持或不确定的结论可能会比告警更加有害。因此，研究提出了一种新的评估框架，将告警能力纳入考虑。

Method: 该评估框架通过对科学声明进行分解，使用自然语言推理审核每个条件，然后根据对证据的评估决定支持、拒绝或选择告警。评估在SciFact和PubMedQA两个科学基准上进行，包含封闭式和开放式证据场景。

Result: 在所有基准测试和模型中，原始准确率在模型架构之间变化仅略有不同，而告警在控制错误中起到关键作用。在支持程度中等的水平下，基于置信的告警显著减少了风险，即使绝对准确性的改进有限。

Conclusion: 研究指出，在科学推理任务中，主要挑战不是选择单一最佳模型，而是确定可获得的证据是否足以支持答案。该研究提出了告警意识评估作为一种实用且模型无关的方法来评估科学可靠性，并为未来科学领域中的选择性推理提供了统一的实验基础。

Abstract: Large language models are increasingly used to answer and verify scientific claims, yet existing evaluations typically assume that a model must always produce a definitive answer. In scientific settings, however, unsupported or uncertain conclusions can be more harmful than abstaining. We study this problem through an abstention-aware verification framework that decomposes scientific claims into minimal conditions, audits each condition against available evidence using natural language inference (NLI), and selectively decides whether to support, refute, or abstain. We evaluate this framework across two complementary scientific benchmarks: SciFact and PubMedQA, covering both closed-book and open-domain evidence settings. Experiments are conducted with six diverse language models, including encoder-decoder, open-weight chat models, and proprietary APIs. Across all benchmarks and models, we observe that raw accuracy varies only modestly across architectures, while abstention plays a critical role in controlling error. In particular, confidence-based abstention substantially reduces risk at moderate coverage levels, even when absolute accuracy improvements are limited. Our results suggest that in scientific reasoning tasks, the primary challenge is not selecting a single best model, but rather determining when available evidence is sufficient to justify an answer. This work highlights abstention-aware evaluation as a practical and model-agnostic lens for assessing scientific reliability, and provides a unified experimental basis for future work on selective reasoning in scientific domains. Code is available at https://github.com/sabdaljalil2000/ai4science .

</details>


### [169] [We can still parse using syntactic rules](https://arxiv.org/abs/2602.14238)
*Ghaly Hussein*

Main category: cs.CL

TL;DR: 该研究提出了一种基于上下文免费文法（CFG）和广义短语结构文法（GPSG）的新解析方法。该方法在去噪和不完整解析方面表现出色，并在通用依赖树库数据集上取得54.5%和53.8%的未标记附分率。


<details>
  <summary>Details</summary>
Motivation: 克服传统CFG方法的限制，处理更多类型的语言格式，并改善噪声和不完整解析的情况。

Method: 新的解析算法结合一套新的语法规则和特性，同时生成依存和成分解析树。

Result: 在通用依赖树库数据集上的开发集和测试集上分别实现了54.5%和53.8%的未标记附分率。

Conclusion: 此方法不仅提高了解析准确性，还能提供多种解析假说，允许进一步的重新排名以提高精度，并充分利用了自20世纪50年代以来的理论语法规则用于计算机环境下的应用。

Abstract: This research introduces a new parsing approach, based on earlier syntactic work on context free grammar (CFG) and generalized phrase structure grammar (GPSG). The approach comprises both a new parsing algorithm and a set of syntactic rules and features that overcome the limitations of CFG. It also generates both dependency and constituency parse trees, while accommodating noise and incomplete parses. The system was tested on data from Universal Dependencies, showing a promising average Unlabeled Attachment Score (UAS) of 54.5% in the development dataset (7 corpora) and 53.8% in the test set (12 corpora). The system also provides multiple parse hypotheses, allowing further reranking to improve parsing accuracy. This approach also leverages much of the theoretical syntactic work since the 1950s to be used within a computational context. The application of this approach provides a transparent and interpretable NLP model to process language input.

</details>


### [170] [Detecting LLM Hallucinations via Embedding Cluster Geometry: A Three-Type Taxonomy with Measurable Signatures](https://arxiv.org/abs/2602.14259)
*Matic Korun*

Main category: cs.CL

TL;DR: 本文提出了一种基于大型语言模型幻觉在词嵌入聚类结构中可观察到的特征进行几何分类的方法，通过对11种transformer模型的嵌入空间进行分析，识别出三种不同类型的幻觉，并介绍了三个可量化的几何统计量。


<details>
  <summary>Details</summary>
Motivation: 作者希望理解大型语言模型在不同上下文中可能出现的幻觉类型及其成因，以提供检测和减轻这些幻觉的方法。

Method: 通过分析11种不同的transformer模型的静态嵌入空间，识别幻觉类型，并使用特定的几何统计量对这些模型进行定量分析。

Result: 作者识别出三种幻觉类型，包括情境较弱时的中心漂移，局部自洽但上下文不正确的错误-好共聚，以及缺少聚类结构的覆盖缺口。此外，作者还提出了三个可量化的几何统计量，这些统计量在所有模型中均有应用，并与模型的具体架构有关，特别是在某些模型上未能达到显著性检验。

Conclusion: 研究结果表明，几何特征可以用于预测特定模型特定幻觉类型的检测，这有助于理解不同架构的依赖性和幻觉的特定模式。

Abstract: We propose a geometric taxonomy of large language model hallucinations based on observable signatures in token embedding cluster structure. By analyzing the static embedding spaces of 11 transformer models spanning encoder (BERT, RoBERTa, ELECTRA, DeBERTa, ALBERT, MiniLM, DistilBERT) and decoder (GPT-2) architectures, we identify three operationally distinct hallucination types: Type 1 (center-drift) under weak context, Type 2 (wrong-well convergence) to locally coherent but contextually incorrect cluster regions, and Type 3 (coverage gaps) where no cluster structure exists. We introduce three measurable geometric statistics: α (polarity coupling), \b{eta} (cluster cohesion), and λ_s (radial information gradient). Across all 11 models, polarity structure (α > 0.5) is universal (11/11), cluster cohesion (\b{eta} > 0) is universal (11/11), and the radial information gradient is significant (9/11, p < 0.05). We demonstrate that the two models failing λ_s significance -- ALBERT and MiniLM -- do so for architecturally explicable reasons: factorized embedding compression and distillation-induced isotropy, respectively. These findings establish the geometric prerequisites for type-specific hallucination detection and yield testable predictions about architecture-dependent vulnerability profiles.

</details>


### [171] [STATe-of-Thoughts: Structured Action Templates for Tree-of-Thoughts](https://arxiv.org/abs/2602.14265)
*Zachary Bamberger,Till R. Saenger,Gilad Morad,Ofra Amir,Brandon M. Stewart,Amir Feder*

Main category: cs.CL

TL;DR: STATe是一种新的解释性Inference-Time-Compute方法，通过行动指导的文本干预来生成高质量、多样性和可解释性的文本。


<details>
  <summary>Details</summary>
Motivation: 当前的Inference-Time-Compute方法未充分实现有意义的输出多样性，且缺乏对推理过程的可控性，而STATe通过引入基于行动的选择文本生成策略来改进这一点。

Method: STATe采用分层推理模式，使用控制器选择编码高层次推理决策的行动，生成器基于这些决策产出推理步骤，评估器评估候选以引导搜索。这种方法通过行动指导的文本干预而非温度采样来增加响应多样性。

Result: 研究表明，行动指导的文本干预相比基于温度的采样，能产生更高的响应多样性。在论证生成案例研究中，STATe的明确行动序列包含了高度预测输出质量的可解释特征。同时，通过估计性能与行动选择之间的关联，该框架能够识别具有潜力但未被探索的行动空间区域并直接引导生成。

Conclusion: STATe作为一种可解释的Inference-Time-Compute框架，展示了生成高质量、多样性和可解释性文本的潜力。

Abstract: Inference-Time-Compute (ITC) methods like Best-of-N and Tree-of-Thoughts are meant to produce output candidates that are both high-quality and diverse, but their use of high-temperature sampling often fails to achieve meaningful output diversity. Moreover, existing ITC methods offer limited control over how to perform reasoning, which in turn limits their explainability. We present STATe-of-Thoughts (STATe), an interpretable ITC method that searches over high-level reasoning patterns. STATe replaces stochastic sampling with discrete and interpretable textual interventions: a controller selects actions encoding high-level reasoning choices, a generator produces reasoning steps conditioned on those choices, and an evaluator scores candidates to guide search. This structured approach yields three main advantages. First, action-guided textual interventions produce greater response diversity than temperature-based sampling. Second, in a case study on argument generation, STATe's explicit action sequences capture interpretable features that are highly predictive of output quality. Third, estimating the association between performance and action choices allows us to identify promising yet unexplored regions of the action space and steer generation directly toward them. Together, these results establish STATe as a practical framework for generating high-quality, diverse, and interpretable text. Our framework is available at https://github.com/zbambergerNLP/state-of-thoughts.

</details>


### [172] [InnoEval: On Research Idea Evaluation as a Knowledge-Grounded, Multi-Perspective Reasoning Problem](https://arxiv.org/abs/2602.14367)
*Shuofei Qiao,Yunxiang Wei,Xuehai Wang,Bin Wu,Boyang Xue,Ningyu Zhang,Hossein A. Rahmani,Yanshan Wang,Qiang Zhang,Keyan Ding,Jeff Z. Pan,Huajun Chen,Emine Yilmaz*

Main category: cs.CL

TL;DR: 本文提出了一种名为InnoEval的深度创新评估框架，旨在解决现有创新评估方法中的知识局限、评价维度单一以及LLM作为评判者的偏见问题。InnoEval通过异构深度知识搜索引擎和多维度、多指标的评审框架，构建全面的数据集进行基准测试，实验表明其在各种评估任务中表现优于基线，判断模式和共识与人类专家高度一致。


<details>
  <summary>Details</summary>
Motivation: 现有的创新评估方法存在知识局限、评价维度单一以及模型自带的偏见问题。为了解决这些挑战，本文提出了一种新的框架InnoEval，旨在更全面、客观地评估科学想法。

Method: 采用异构深度知识搜索引擎获取多来源证据，结合不同学术背景的评审员组成的创新评审董事会多维度评价。验证框架通过构建全面的权威同行评审提交数据集。实现精细化的评估任务。

Result: InnoEval在点对点、成对和群体评估任务中表现优于现有基线模型，展现了与人类专家高度一致的评估模式和共识。

Conclusion: InnoEval框架能够提供更全面、客观的创新评估，具有较高的实用价值，能够为科学想法的评估提供新的参考路径。

Abstract: The rapid evolution of Large Language Models has catalyzed a surge in scientific idea production, yet this leap has not been accompanied by a matching advance in idea evaluation. The fundamental nature of scientific evaluation needs knowledgeable grounding, collective deliberation, and multi-criteria decision-making. However, existing idea evaluation methods often suffer from narrow knowledge horizons, flattened evaluation dimensions, and the inherent bias in LLM-as-a-Judge. To address these, we regard idea evaluation as a knowledge-grounded, multi-perspective reasoning problem and introduce InnoEval, a deep innovation evaluation framework designed to emulate human-level idea assessment. We apply a heterogeneous deep knowledge search engine that retrieves and grounds dynamic evidence from diverse online sources. We further achieve review consensus with an innovation review board containing reviewers with distinct academic backgrounds, enabling a multi-dimensional decoupled evaluation across multiple metrics. We construct comprehensive datasets derived from authoritative peer-reviewed submissions to benchmark InnoEval. Experiments demonstrate that InnoEval can consistently outperform baselines in point-wise, pair-wise, and group-wise evaluation tasks, exhibiting judgment patterns and consensus highly aligned with human experts.

</details>


### [173] [Beyond Token-Level Policy Gradients for Complex Reasoning with Large Language Models](https://arxiv.org/abs/2602.14386)
*Mufan Xu,Kehai Chen,Xuefeng Bai,Zhengyu Niu,Muyun Yang,Tiejun Zhao,Min Zhang*

Main category: cs.CL

TL;DR: 文章提出了一种名为MPO的方法，该方法将K个连续的令牌作为一个统一的语义动作来处理，从而改进了复杂推理任务的表现。


<details>
  <summary>Details</summary>
Motivation: 现有政策梯度方法在自回归语言模型中通常逐个选择后续令牌作为动作，这在处理复杂的推理任务时可能无法充分捕捉语义决策的整体结构。因此，提出了MPO框架，以解决这一问题。

Method: MPO框架将K个连续令牌视为统一的语义动作，通过批量处理的方式优化推理过程中的目标，捕捉推理路径中的组合结构。

Result: 实验结果表明，MPO在数学推理和编程基准测试中的表现优于传统的令牌级政策梯度基线。

Conclusion: 文章强调了跨令牌的优化对于复杂推理任务的重要性，并建议未来研究探索更细粒度的方法以适应推理密集的语言任务。

Abstract: Existing policy-gradient methods for auto-regressive language models typically select subsequent tokens one at a time as actions in the policy. While effective for many generation tasks, such an approach may not fully capture the structure of complex reasoning tasks, where a single semantic decision is often realized across multiple tokens--for example, when defining variables or composing equations. This introduces a potential mismatch between token-level optimization and the inherently block-level nature of reasoning in these settings. To bridge this gap, we propose Multi-token Policy Gradient Optimization (MPO), a framework that treats sequences of K consecutive tokens as unified semantic actions. This block-level perspective enables our method to capture the compositional structure of reasoning trajectories and supports optimization over coherent, higher-level objectives. Experiments on mathematical reasoning and coding benchmarks show that MPO outperforms standard token-level policy gradient baselines, highlight the limitations of token-level policy gradients for complex reasoning, motivating future research to look beyond token-level granularity for reasoning-intensive language tasks.

</details>


### [174] [TruthStance: An Annotated Dataset of Conversations on Truth Social](https://arxiv.org/abs/2602.14406)
*Fathima Ameen,Danielle Brown,Manusha Malgareddy,Amanul Haque*

Main category: cs.CL

TL;DR: 该研究提出了一个名为TruthStance的大型数据集，用于分析和评估多语言环境下的辩论和观点表达，特别强调了Alt-tech平台的对话结构。


<details>
  <summary>Details</summary>
Motivation: 当前大多数公开资源主要集中在Twitter和Reddit等主流平台，而对Alt-tech平台的对话结构研究较少。为此，研究者旨在填补这一研究空白，提出了TruthStance数据集来更好地理解和分析在线讨论中意见的形成与争议。

Method: 研究团队收集并整理了2023-2025年间Truth Social平台的24,378条帖子和523,360条评论，形成了一个具有回复树结构的数据集。在此基础上，他们提供了涵盖2000个示例的标注基准，包括不同类型的人工标注者之间的协议程度，并用来评估大型语言模型的提示策略。

Result: 通过最佳表现的配置，研究者发布了额外的大型语言模型生成标签，共计24,352条帖子（论点存在性）和107,873条评论（对父级立场），并分享了所有代码和数据。

Conclusion: 研究结果为分析和理解多语言环境下的辩论和观点表达提供了有力工具，并将成为未来相关研究和开发的重要数据基础。

Abstract: Argument mining and stance detection are central to understanding how opinions are formed and contested in online discourse. However, most publicly available resources focus on mainstream platforms such as Twitter and Reddit, leaving conversational structure on alt-tech platforms comparatively under-studied. We introduce TruthStance, a large-scale dataset of Truth Social conversation threads spanning 2023-2025, consisting of 24,378 posts and 523,360 comments with reply-tree structure preserved. We provide a human-annotated benchmark of 1,500 instances across argument mining and claim-based stance detection, including inter-annotator agreement, and use it to evaluate large language model (LLM) prompting strategies. Using the best-performing configuration, we release additional LLM-generated labels for 24,352 posts (argument presence) and 107,873 comments (stance to parent), enabling analysis of stance and argumentation patterns across depth, topics, and users. All code and data are released publicly.

</details>


### [175] [LLM-Guided Knowledge Distillation for Temporal Knowledge Graph Reasoning](https://arxiv.org/abs/2602.14428)
*Wang Xing,Wei Song,Siyu Lin,Chen Wu,Man Wang*

Main category: cs.CL

TL;DR: 本研究提出了一种新的LLM辅助蒸馏框架，专为时态知识图谱推理设计。通过引入大语言模型作为辅助教师，提升学生模型的事件动态建模能力，同时保持小型高效的学生模型。实验表明，这种方法在链接预测方面优于强蒸馏基线。


<details>
  <summary>Details</summary>
Motivation: 当前先进的时态知识图谱模型计算复杂且成本高，现有压缩和蒸馏技术多针对静态图，直接应用于时态环境可能导致表现下降。因此，提出了结合大语言模型的辅助蒸馏框架，旨在提高时态知识图谱推理的效果。

Method: 该方法采用了一种两阶段的联合优化策略，利用监督学习和蒸馏目标进行训练。通过集成大型语言模型和高容量时态教师的指导，使轻量级学生模型能够更好地表示事件动力学。

Result: 实验结果表明，该方法在多个公开的时态知识图谱基准上，链接预测性能优于标准的蒸馏基线，且保持了紧凑且高效的模型。

Conclusion: 此项研究有效地利用了大语言模型作为教师，推动时间推理能力在资源节约的时态知识图谱系统中的转让。

Abstract: Temporal knowledge graphs (TKGs) support reasoning over time-evolving facts, yet state-of-the-art models are often computationally heavy and costly to deploy. Existing compression and distillation techniques are largely designed for static graphs; directly applying them to temporal settings may overlook time-dependent interactions and lead to performance degradation. We propose an LLM-assisted distillation framework specifically designed for temporal knowledge graph reasoning. Beyond a conventional high-capacity temporal teacher, we incorporate a large language model as an auxiliary instructor to provide enriched supervision. The LLM supplies broad background knowledge and temporally informed signals, enabling a lightweight student to better model event dynamics without increasing inference-time complexity. Training is conducted by jointly optimizing supervised and distillation objectives, using a staged alignment strategy to progressively integrate guidance from both teachers. Extensive experiments on multiple public TKG benchmarks with diverse backbone architectures demonstrate that the proposed approach consistently improves link prediction performance over strong distillation baselines, while maintaining a compact and efficient student model. The results highlight the potential of large language models as effective teachers for transferring temporal reasoning capability to resource-efficient TKG systems.

</details>


### [176] [Robust Bias Evaluation with FilBBQ: A Filipino Bias Benchmark for Question-Answering Language Models](https://arxiv.org/abs/2602.14466)
*Lance Calvin Lim Gamboa,Yue Feng,Mark Lee*

Main category: cs.CL

TL;DR: 本研究通过开发一个更广泛的菲律宾背景下的偏见测试集FilBBQ，评估了生成模型中的性别歧视和同性恋偏见，并通过多种子实验提高评估的可靠性和准确度。


<details>
  <summary>Details</summary>
Motivation: 随着自然语言生成成为语言模型的一个热门应用场景，这种研究旨在评估和减少生成模型中的性别刻板印象和歧视倾向，特别是在菲律宾文化背景下。

Method: 研究团队通过四个阶段开发了FilBBQ测试集：模板分类、文化意识翻译、新模板创作和提示生成。测试集包含超过10,000个提示，评估模型对性别歧视和同性恋偏见的敏感性。研究还通过跨多次种子运行的方式，增强了评估结果的可靠性。

Result: 实验结果显示，生成模型在不同情感、家庭生活、刻板同性恋兴趣和多妻制方面存在性别歧视和同性恋偏见，且揭示了偏差分数在不同种子间的波动性。

Conclusion: FilBBQ提供了一个可靠的评估工具，可用于识别和减少生成模型中的偏见。研究建议未来的模型开发应考虑文化多样性和偏见矫正。

Abstract: With natural language generation becoming a popular use case for language models, the Bias Benchmark for Question-Answering (BBQ) has grown to be an important benchmark format for evaluating stereotypical associations exhibited by generative models. We expand the linguistic scope of BBQ and construct FilBBQ through a four-phase development process consisting of template categorization, culturally aware translation, new template construction, and prompt generation. These processes resulted in a bias test composed of more than 10,000 prompts which assess whether models demonstrate sexist and homophobic prejudices relevant to the Philippine context. We then apply FilBBQ on models trained in Filipino but do so with a robust evaluation protocol that improves upon the reliability and accuracy of previous BBQ implementations. Specifically, we account for models' response instability by obtaining prompt responses across multiple seeds and averaging the bias scores calculated from these distinctly seeded runs. Our results confirm both the variability of bias scores across different seeds and the presence of sexist and homophobic biases relating to emotion, domesticity, stereotyped queer interests, and polygamy. FilBBQ is available via GitHub.

</details>


### [177] [Measuring and Mitigating Post-hoc Rationalization in Reverse Chain-of-Thought Generation](https://arxiv.org/abs/2602.14469)
*Guangyue Peng,Zongchao Chen,Wen Luo,Yuntao Wen,Wei Li,Ruixiang Feng,Ran Le,Chen Yang,Zhenwei An,Yang Song,Tao Zhang,Houfeng Wang*

Main category: cs.CL

TL;DR: 研究提出了Structural Skeleton-guided Reasoning（SSR）方法，通过生成不变于答案的结构骨架，再以此骨架指导解释生成，从而降低锚定效应；SSR及其精炼版本（SSR-D）在开放逻辑推理基准上显著提高了性能。


<details>
  <summary>Details</summary>
Motivation: 当前逆向思维生成方法存在产生事后合理化的问题，在生成过程中答案成为认知锚，影响解释生成。为此，研究希望通过新的方法来减少这种依赖。

Method: 提出了一种两阶段的Structural Skeleton-guided Reasoning（SSR）方法，首先生成一个答案无关的结构骨架，然后利用该骨架指导完整解释的生成。还提出了Distilled SSR（SSR-D），预先在教师生成的SSR解释上微调模型，确保结构一致性。

Result: SSR和SSR-D在多个开放逻辑推理测试集上表现出色，相较于抑制方法，SSR-D提高了最多10%的性能，并保持了离域泛化的保持。

Conclusion: 研究为解决逆向思维生成中事后合理化的问题提供了一种有效的解决方案，通过结构导向的方法有效降低了锚定效应。

Abstract: Reverse Chain-of-Thought Generation (RCG) synthesizes reasoning traces from query-answer pairs, but runs the risk of producing post-hoc rationalizations: when models can see the answer during generation, the answer serves as a cognitive anchor that shapes the entire explanation. We formalize this phenomenon through a three-level measurement hierarchy: lexical, entropic, and probabilistic anchoring, each captures surface artifacts, entropy dynamics, and latent answer dependence, respectively. We analyze semantic suppression, the intuitive mitigation strategy that instructs models to ignore the answer, to find out its counterproduction: while it reduces lexical overlap, it paradoxically increases entropic and probabilistic anchoring. Drawing on Ironic Process Theory from cognitive psychology, we attribute this failure to active monitoring of the forbidden answer, which inadvertently deepens dependence on it. To break this cycle, we propose Structural Skeleton-guided Reasoning (SSR), a two-phase approach that first generates an answer-invariant functional skeleton structure, then uses this skeleton to guide full trace generation. By redirecting the information flow to structural planning rather than answer monitoring, SSR consistently reduces anchoring across all three levels. We further introduce Distilled SSR (SSR-D), which fine-tunes models on teacher-generated SSR traces to ensure reliable structural adherence. Experiments across open-ended reasoning benchmarks demonstrate that SSR-D achieves up to 10% improvement over suppression baselines while preserving out-of-distribution (OOD) generalization.

</details>


### [178] [HyperRAG: Reasoning N-ary Facts over Hypergraphs for Retrieval Augmented Generation](https://arxiv.org/abs/2602.14470)
*Wen-Sheng Lien,Yu-Kai Chan,Hao-Lung Hsiao,Bo-Kai Ruan,Meng-Fen Chiang,Chien-An Chen,Yi-Ren Yeh,Hong-Han Shuai*

Main category: cs.CL

TL;DR: HyperRAG 提出了一种针对 n-元超图的检索增强生成框架，通过 HyperRetriever 和 HyperMemory 两个模块，利用 n-元事实进行结构语义推理和参数化记忆引导的路径扩展，显著提高了多跳开放领域 QA 的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于知识图谱的检索增强生成（RAG）方法在多跳开放领域 QA 中表现出色，但存在检索方案刚性、密集相似度搜索等问题，引入了无关内容，增加了计算负担，并限制了关系表达力。

Method: HyperRAG 方法通过引入 n-元超图来捕捉更复杂的实体间关系，提出 HyperRetriever 和 HyperMemory 两个模块。HyperRetriever 利用 n-元事实进行结构语义推理，构建查询条件下的关系链；HyperMemory 利用大型语言模型的参数化记忆引导束搜索，动态评估 n-元事实和实体，实现查询感知的路径扩展。

Result: HyperRAG 在 WikiTopics（11 个封闭领域数据集）和三个开放领域 QA 基准（HotpotQA、MuSiQue 和 2WikiMultiHopQA）上进行大量评估，HyperRetriever 的整体答案准确率最高，平均 MRR 和 Hits@10 高于最强基线 2.95% 和 1.23%。

Conclusion: HyperRAG 通过 n-元超图和两个结合结构语义推理和参数化记忆的模块，有效提升了多跳开放领域 QA 的性能，并在多个基准测试中表现出色。

Abstract: Graph-based retrieval-augmented generation (RAG) methods, typically built on knowledge graphs (KGs) with binary relational facts, have shown promise in multi-hop open-domain QA. However, their rigid retrieval schemes and dense similarity search often introduce irrelevant context, increase computational overhead, and limit relational expressiveness. In contrast, n-ary hypergraphs encode higher-order relational facts that capture richer inter-entity dependencies and enable shallower, more efficient reasoning paths. To address this limitation, we propose HyperRAG, a RAG framework tailored for n-ary hypergraphs with two complementary retrieval variants: (i) HyperRetriever learns structural-semantic reasoning over n-ary facts to construct query-conditioned relational chains. It enables accurate factual tracking, adaptive high-order traversal, and interpretable multi-hop reasoning under context constraints. (ii) HyperMemory leverages the LLM's parametric memory to guide beam search, dynamically scoring n-ary facts and entities for query-aware path expansion. Extensive evaluations on WikiTopics (11 closed-domain datasets) and three open-domain QA benchmarks (HotpotQA, MuSiQue, and 2WikiMultiHopQA) validate HyperRAG's effectiveness. HyperRetriever achieves the highest answer accuracy overall, with average gains of 2.95% in MRR and 1.23% in Hits@10 over the strongest baseline. Qualitative analysis further shows that HyperRetriever bridges reasoning gaps through adaptive and interpretable n-ary chain construction, benefiting both open and closed-domain QA.

</details>


### [179] [BETA-Labeling for Multilingual Dataset Construction in Low-Resource IR](https://arxiv.org/abs/2602.14488)
*Md. Najib Hasan,Mst. Jannatun Ferdous Rain,Fyad Mohammed,Nazmul Siddique*

Main category: cs.CL

TL;DR: 本文提出了一种使用多种大型语言模型（LLM）自动标注框架构建孟加拉语信息检索数据集的方法，包括上下文对齐、一致性检查和多数同意，并通过人类评估验证标签质量。此外，研究探讨了使用机器翻译在不同低资源语言之间重用IR数据集的有效性。研究表明，语言特异性偏差和语义失真影响了跨语言数据集重用的可靠性。


<details>
  <summary>Details</summary>
Motivation: 在低资源语言中，高质量的特定任务标注数据集稀缺，手动注释成本高昂且难以扩展，而使用大型语言模型（LLMs）作为自动注释器会引发标签可靠性、偏见和评估有效性的担忧。因此，本文提出了一种利用多种LLM自动标注的方法，并通过机器翻译探索不同低资源语言之间数据集重用的有效性。

Method: 文章采用BETA标注框架，结合了上下文对齐、一致性检查以及多数同意，并通过后续的人类评估来验证标签质量。此外，通过LLM跨语言翻译的实验来考察源数据集和翻译后数据集之间的意义保留性和任务有效性。

Result: 该研究揭示了不同语言之间的差异性，表现出语言特异性偏见和语义保持的不一致性，这些因素直接影响跨语言数据集重用的可靠性。

Conclusion: 总的来说，本研究揭示了LLM辅助的低资源信息检索数据集构建的潜在性和局限性，提供了关于在低资源语言环境中构建更加可靠基准和评估管道的实证证据和实用指导。

Abstract: IR in low-resource languages remains limited by the scarcity of high-quality, task-specific annotated datasets. Manual annotation is expensive and difficult to scale, while using large language models (LLMs) as automated annotators introduces concerns about label reliability, bias, and evaluation validity. This work presents a Bangla IR dataset constructed using a BETA-labeling framework involving multiple LLM annotators from diverse model families. The framework incorporates contextual alignment, consistency checks, and majority agreement, followed by human evaluation to verify label quality. Beyond dataset creation, we examine whether IR datasets from other low-resource languages can be effectively reused through one-hop machine translation. Using LLM-based translation across multiple language pairs, we experimented on meaning preservation and task validity between source and translated datasets. Our experiment reveal substantial variation across languages, reflecting language-dependent biases and inconsistent semantic preservation that directly affect the reliability of cross-lingual dataset reuse. Overall, this study highlights both the potential and limitations of LLM-assisted dataset creation for low-resource IR. It provides empirical evidence of the risks associated with cross-lingual dataset reuse and offers practical guidance for constructing more reliable benchmarks and evaluation pipelines in low-resource language settings.

</details>


### [180] [Query as Anchor: Scenario-Adaptive User Representation via Large Language Model](https://arxiv.org/abs/2602.14492)
*Jiahao Yuan,Yike Xu,Jinyong Wen,Baokun Wang,Ziyi Gao,Xiaotong Lin,Yun Liu,Xing Fu,Yu Cheng,Yongchao Liu,Weiqiang Wang,Zhongle Xie*

Main category: cs.CL

TL;DR: 论文提出了Query-as-Anchor框架，通过构建大规模跨模态行为序列数据集UserU和引入Query-Anchor嵌入架构及基于簇的软提示调谐方法，旨在为大规模语言模型提供深度用户理解能力，并实现了高效部署。


<details>
  <summary>Details</summary>
Motivation: 当前的用户表示学习方法主要生成静态、任务无关的嵌入，难以同时满足不同下游应用场景的需求。大规模工业数据存在异质性和噪声，影响表现。Q-Anchor框架旨在解决这些问题，通过动态、查询感知的用户表示生成来增强模型的理解能力。

Method: Q-Anchor方法包括构建大规模跨模态行为序列数据集UserU，设计Query-Anchor嵌入架构，该架构结合了层次化的粗细编码器和联合对比自回归优化。此外，引入基于簇的软提示调谐方法，以增强模型对特定场景的模态关注。

Result: 在Alipay的10个工业基准测试中，Q-Anchor框架展示了与当前最佳模型相当的性能，具有良好的可扩展性和有效的部署策略。大规模在线A/B测试进一步验证了其在真实场景中的实际效果。

Conclusion: Q-Anchor框架通过结合预训练技术和业务逻辑调优，提供了一种新型的用户表示方法，有效地平衡了通用性和任务敏感性，为LLM应用提供了深入用户理解的新方案。

Abstract: Industrial-scale user representation learning requires balancing robust universality with acute task-sensitivity. However, existing paradigms primarily yield static, task-agnostic embeddings that struggle to reconcile the divergent requirements of downstream scenarios within unified vector spaces. Furthermore, heterogeneous multi-source data introduces inherent noise and modality conflicts, degrading representation. We propose Query-as-Anchor, a framework shifting user modeling from static encoding to dynamic, query-aware synthesis. To empower Large Language Models (LLMs) with deep user understanding, we first construct UserU, an industrial-scale pre-training dataset that aligns multi-modal behavioral sequences with user understanding semantics, and our Q-Anchor Embedding architecture integrates hierarchical coarse-to-fine encoders into dual-tower LLMs via joint contrastive-autoregressive optimization for query-aware user representation. To bridge the gap between general pre-training and specialized business logic, we further introduce Cluster-based Soft Prompt Tuning to enforce discriminative latent structures, effectively aligning model attention with scenario-specific modalities. For deployment, anchoring queries at sequence termini enables KV-cache-accelerated inference with negligible incremental latency. Evaluations on 10 Alipay industrial benchmarks show consistent SOTA performance, strong scalability, and efficient deployment. Large-scale online A/B testing in Alipay's production system across two real-world scenarios further validates its practical effectiveness. Our code is prepared for public release and will be available at: https://github.com/JhCircle/Q-Anchor.

</details>


### [181] [Beyond Translation: Evaluating Mathematical Reasoning Capabilities of LLMs in Sinhala and Tamil](https://arxiv.org/abs/2602.14517)
*Sukumar Kishanthan,Kumar Thushalika,Buddhi Jayasekara,Asela Hevapathige*

Main category: cs.CL

TL;DR: 研究使用大型语言模型（LLMs）在斯里兰卡语和泰米尔语中的数学推理能力，并发现基本的算术推理可以跨语言稳定转移，但复杂的数学问题在斯里兰卡语和泰米尔语中表现较差，表明这些模型在不同语言中的推理能力并不均匀。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型在低资源语言（如斯里兰卡语和泰米尔语）中的数学推理能力是否真正反映了多语言推理的能力，还是依赖于翻译过程。

Method: 通过构建一个平行的数据集来评估四个主流的大型语言模型在六种不同类型的数学问题上的表现，每种问题都有母语能力强且受数学训练的说话者用这三种语言独立编写。

Result: 基本的算术推理在斯里兰卡语和泰米尔语中表现良好，但复杂的问题类型在这些语言中表现出显著下降。不同模型和问题类型的具体失败模式各异，表明表象上的多语言能力并不意味着在所有语言中都有均匀的推理能力。

Conclusion: 论文的研究结果挑战了对表现出强大多语言能力的模型的普遍假设，即这些模型可以在所有语言中以相同的有效性进行推理。因此，需要在多语言环境中进行细化且类型敏感的评估。

Abstract: Large language models (LLMs) demonstrate strong mathematical reasoning in English, but whether these capabilities reflect genuine multilingual reasoning or reliance on translation-based processing in low-resource languages like Sinhala and Tamil remains unclear. We examine this fundamental question by evaluating whether LLMs genuinely reason mathematically in these languages or depend on implicit translation to English-like representations. Using a taxonomy of six math problem types, from basic arithmetic to complex unit conflict and optimization problems, we evaluate four prominent large language models. To avoid translation artifacts that confound language ability with translation quality, we construct a parallel dataset where each problem is natively authored by fluent speakers with mathematical training in all three languages. Our analysis demonstrates that while basic arithmetic reasoning transfers robustly across languages, complex reasoning tasks show significant degradation in Tamil and Sinhala. The pattern of failures varies by model and problem type, suggesting that apparent multilingual competence may not reflect uniform reasoning capabilities across languages. These findings challenge the common assumption that models exhibiting strong multilingual performance can reason equally effectively across languages, and highlight the need for fine-grained, type-aware evaluation in multilingual settings.

</details>


### [182] [Explainable Token-level Noise Filtering for LLM Fine-tuning Datasets](https://arxiv.org/abs/2602.14536)
*Yuchen Yang,Wenze Lin,Enhao Huang,Zhixuan Chu,Hongbin Zhou,Lan Tao,Yiming Li,Zhan Qin,Kui Ren*

Main category: cs.CL

TL;DR: 本文提出了一种可解释的 token 级别噪声过滤框架 XTF，通过分解 token 级别数据对微调过程的贡献并对其进行评估以优化微调后的大型语言模型性能。


<details>
  <summary>Details</summary>
Motivation: 目前的微调数据集多为句子级别设计，这与大型语言模型的token级别优化机制存在不匹配，导致噪声引入，从而对最终性能产生负面影响。

Method: XTF框架通过将 token 级别数据的影响分解为三个明确的属性——推理重要性、知识新颖性和任务相关性，并使用评分方法进行评估，然后根据评估结果掩蔽噪声 token 来优化微调后的大型语言模型。

Result: 在数学、代码和医学等三个代表性下游任务中，XTF 框架在 7 种主要的大型语言模型上的实验结果表明，与常规微调相比，XTF 可以显著提高下游性能，最多可提高 13.7%。

Conclusion: 本文强调了 token 级别数据集优化的重要性，并展示了基于属性分解策略的潜力，能够解释复杂的训练机制。

Abstract: Large Language Models (LLMs) have seen remarkable advancements, achieving state-of-the-art results in diverse applications. Fine-tuning, an important step for adapting LLMs to specific downstream tasks, typically involves further training on corresponding datasets. However, a fundamental discrepancy exists between current fine-tuning datasets and the token-level optimization mechanism of LLMs: most datasets are designed at the sentence-level, which introduces token-level noise, causing negative influence to final performance. In this paper, we propose XTF, an explainable token-level noise filtering framework. XTF decomposes the complex and subtle contributions of token-level data to the fine-tuning process into three distinct and explicit attributes (reasoning importance, knowledge novelty, and task relevance), which can be assessed using scoring methods, and then masks the gradients of selected noisy tokens accordingly to optimize the performance of fine-tuned LLMs. We conduct extensive experiments on three representative downstream tasks (math, code and medicine) across 7 mainstream LLMs. The results demonstrate that XTF can significantly improve downstream performance by up to 13.7% compared to regular fine-tuning. Our work highlights the importance of token-level dataset optimization, and demonstrates the potential of strategies based on attribute decomposition for explaining complex training mechanisms.

</details>


### [183] [Assessing Large Language Models for Medical QA: Zero-Shot and LLM-as-a-Judge Evaluation](https://arxiv.org/abs/2602.14564)
*Shefayat E Shams Adib,Ahmed Alfey Sani,Ekramul Alam Esham,Ajwad Abrar,Tareque Mohmud Chowdhury*

Main category: cs.CL

TL;DR: 该研究比较了五个大型语言模型在医疗问答系统中的性能，展示了更大模型的优势，并提出了一种标准化的基准设置，以促进未来研究并优化临床环境中医疗NLP应用的实际部署。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过比较不同大小的大型语言模型的性能，探索其在医疗领域的应用潜力，以提高低资源地区医疗访问的便捷性。

Method: 研究使用了零样本评价方法，基于iCliniq数据集对五个模型（Llama-3-8B-Instruct, Llama 3.2 3B, Llama 3.3 70B Instruct, Llama-4-Maverick-17B-128E-Instruct, GPT-5-mini）进行评估，并使用BLEU和ROUGE指标进行度量。

Result: 结果显示，较大模型如Llama 3.3 70B Instruct的性能优于较小模型。值得注意的是，Llama-4-Maverick-17B显示了更具竞争力的结果，强调了在实际部署中模型规模与计算效率之间的权衡。

Conclusion: 研究结果表明，大型语言模型在医疗问答系统中表现出色，为促进专业级别的医疗推理提供了依据。这为进一步研究标准化基准设置、减少模型大小、节约计算资源并提升医疗NLP应用的临床可行性奠定了基础。

Abstract: Recently, Large Language Models (LLMs) have gained significant traction in medical domain, especially in developing a QA systems to Medical QA systems for enhancing access to healthcare in low-resourced settings. This paper compares five LLMs deployed between April 2024 and August 2025 for medical QA, using the iCliniq dataset, containing 38,000 medical questions and answers of diverse specialties. Our models include Llama-3-8B-Instruct, Llama 3.2 3B, Llama 3.3 70B Instruct, Llama-4-Maverick-17B-128E-Instruct, and GPT-5-mini. We are using a zero-shot evaluation methodology and using BLEU and ROUGE metrics to evaluate performance without specialized fine-tuning. Our results show that larger models like Llama 3.3 70B Instruct outperform smaller models, consistent with observed scaling benefits in clinical tasks. It is notable that, Llama-4-Maverick-17B exhibited more competitive results, thus highlighting evasion efficiency trade-offs relevant for practical deployment. These findings align with advancements in LLM capabilities toward professional-level medical reasoning and reflect the increasing feasibility of LLM-supported QA systems in the real clinical environments. This benchmark aims to serve as a standardized setting for future study to minimize model size, computational resources and to maximize clinical utility in medical NLP applications.

</details>


### [184] [GradMAP: Faster Layer Pruning with Gradient Metric and Projection Compensation](https://arxiv.org/abs/2602.14649)
*Hao Liu,Guangyan Li,Wensheng Zhang,Yongqiang Tang*

Main category: cs.CL

TL;DR: GradMAP 提出了一种更快的层剪枝方法，通过引入基于梯度大小的新度量和投影补偿矩阵，实现了剪枝速度和性能的双提升。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型（LLMs）的高计算成本限制了它们的实际部署，研究集中在通过层剪枝来提高效率和性能。

Method: 提出了一种名为 GradMAP 的剪枝方法，分为两个阶段：第一阶段使用基于梯度大小的度量评估层的重要性；第二阶段通过分析大迁移层并引入投影补偿矩阵来修正剪枝后的性能下降。

Result: GradMAP 在剪枝速度方面平均提高了 4 倍，并且在保持性能的同时实现了显著的剪枝速度提升。

Conclusion: GradMAP 有效缓解了剪枝导致的性能下降，是目前最有效的层剪枝方法之一。

Abstract: Large Language Models (LLMs) exhibit strong reasoning abilities, but their high computational costs limit their practical deployment. Recent studies reveal significant redundancy in LLMs layers, making layer pruning an active research topic. Layer pruning research primarily focuses on two aspects: measuring layer importance and recovering performance after pruning. Unfortunately, the present works fail to simultaneously maintain pruning performance and efficiency. In this study, we propose GradMAP, a faster layer pruning method with \textbf{Grad}ient \textbf{M}etric \textbf{A}nd \textbf{P}rojection compensation, which consists of two stages. In the first stage, we introduce a novel metric based on gradient magnitudes, enabling a global assessment of layer importance. Note that, it requires only a single backward propagation step per pruning decision, substantially enhancing pruning efficiency. In the second stage, we first analyze the layers with the largest mean shift resulting from pruning, and then incorporate a simple yet effective projection compensation matrix to correct this drift in one step. In this way, the degradation of model performance caused by layer pruning is effectively alleviated. Extensive experiments show that GradMAP outperforms previous layer pruning methods in both pruning speed (achieving an average $4\times$ speedup) and performance.

</details>


### [185] [Is Information Density Uniform when Utterances are Grounded on Perception and Discourse?](https://arxiv.org/abs/2602.14653)
*Matteo Gay,Coleman Haley,Mario Giulianelli,Edoardo Ponti*

Main category: cs.CL

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: The Uniform Information Density (UID) hypothesis posits that speakers are subject to a communicative pressure to distribute information evenly within utterances, minimising surprisal variance. While this hypothesis has been tested empirically, prior studies are limited exclusively to text-only inputs, abstracting away from the perceptual context in which utterances are produced. In this work, we present the first computational study of UID in visually grounded settings. We estimate surprisal using multilingual vision-and-language models over image-caption data in 30 languages and visual storytelling data in 13 languages, together spanning 11 families. We find that grounding on perception consistently smooths the distribution of information, increasing both global and local uniformity across typologically diverse languages compared to text-only settings. In visual narratives, grounding in both image and discourse contexts has additional effects, with the strongest surprisal reductions occurring at the onset of discourse units. Overall, this study takes a first step towards modelling the temporal dynamics of information flow in ecologically plausible, multimodal language use, and finds that grounded language exhibits greater information uniformity, supporting a context-sensitive formulation of UID.

</details>


### [186] [Breaking Data Efficiency Dilemma: A Federated and Augmented Learning Framework For Alzheimer's Disease Detection via Speech](https://arxiv.org/abs/2602.14655)
*Xiao Wei,Bin Wen,Yuqin Lin,Kai Li,Mingyang gu,Xiaobao Wang,Longbiao Wang,Jianwu Dang*

Main category: cs.CL

TL;DR: FAL-AD 提出了一种新颖的框架，通过结合联邦学习和数据增强方法来优化阿尔茨海默病的诊断。该方法在多个方面实现了创新，包括声音转换增强生成多类别病理语音样本，联邦学习的协作效率，以及增强交叉模态融合模型实现代表性的优化。


<details>
  <summary>Details</summary>
Motivation: 通过将联邦学习与数据增强技术结合，FAL-AD 在解决阿尔茨海默病诊断中的数据效率问题上取得了突破，特别是在稀缺且隐私受限的医疗数据场景下。

Method: FAL-AD 包括三种核心方法：声音转换增强、协作效率优化和模式转换优化。声音转换增强从跨类声音内容重组中生成多样化病理语音样本；协作效率通过适应联邦学习范式最大化跨机构利益；模式转换优化采用注意驱动的交叉模态融合模型，实现细粒度的词级对齐和声音文本交互。

Result: FAL-AD 在 ADReSSo 数据集上实现了 91.52% 的多模态准确率，超越了所有中央基线，展示了在数据效率方面的实际解决方案。

Conclusion: FAL-AD 提供了一种有效的解决阿尔茨海默病早期诊断中数据效率问题的方法，并为未来的诊断工具开发提供了有价值的参考。

Abstract: Early diagnosis of Alzheimer's Disease (AD) is crucial for delaying its progression. While AI-based speech detection is non-invasive and cost-effective, it faces a critical data efficiency dilemma due to medical data scarcity and privacy barriers. Therefore, we propose FAL-AD, a novel framework that synergistically integrates federated learning with data augmentation to systematically optimize data efficiency. Our approach delivers three key breakthroughs: First, absolute efficiency improvement through voice conversion-based augmentation, which generates diverse pathological speech samples via cross-category voice-content recombination. Second, collaborative efficiency breakthrough via an adaptive federated learning paradigm, maximizing cross-institutional benefits under privacy constraints. Finally, representational efficiency optimization by an attentive cross-modal fusion model, which achieves fine-grained word-level alignment and acoustic-textual interaction. Evaluated on ADReSSo, FAL-AD achieves a state-of-the-art multi-modal accuracy of 91.52%, outperforming all centralized baselines and demonstrating a practical solution to the data efficiency dilemma. Our source code is publicly available at https://github.com/smileix/fal-ad.

</details>


### [187] [Crowdsourcing Piedmontese to Test LLMs on Non-Standard Orthography](https://arxiv.org/abs/2602.14675)
*Gianluca Vico,Jindřich Libovický*

Main category: cs.CL

TL;DR: 该研究利用众包方式创建了一个皮埃蒙特语数据集，用于评估大型语言模型在皮埃蒙特语处理方面的表现。


<details>
  <summary>Details</summary>
Motivation: 为了填补低资源语言处理领域的空白，特别是在皮埃蒙特语这种濒危罗曼语的影响下，研究人员众包创建了一个数据集，以期为该语言建立量化评估标准。

Method: 研究人员通过众包方式获取了145个意大利-皮埃蒙特双语对照句子，这些句子被非标准化的皮埃蒙特语母语者翻译。研究人员进一步对这些数据进行了手工词对齐，并用于评估几个大型语言模型在分词一致性、主题分类和机器翻译等任务中的表现。

Result: 研究结果表明，皮埃蒙特语相比资源丰富型罗曼语族语言在分词一致性上存在劣势，但大型语言模型在主题分类上的表现接近意大利语、法语和英语。机器翻译结果显示，模型能较好地从皮埃蒙特语翻译到资源丰富型语言，但向皮埃蒙特语的生成依旧具有挑战性。

Conclusion: 该研究汇集了一个开放的皮埃蒙特语资源，并揭示了大型语言模型在处理皮埃蒙特语时的几个关键挑战。

Abstract: We present a crowdsourced dataset for Piedmontese, an endangered Romance language of northwestern Italy. The dataset comprises 145 Italian-Piedmontese parallel sentences derived from Flores+, with translations produced by speakers writing in their natural orthographic style rather than adhering to standardized conventions, along with manual word alignment. We use this resource to benchmark several large language models on tokenization parity, topic classification, and machine translation. Our analysis reveals that Piedmontese incurs a tokenization penalty relative to higher-resource Romance languages, yet LLMs achieve classification performance approaching that of Italian, French, and English. Machine translation results are asymmetric: models translate adequately from Piedmontese into high-resource languages, but generation into Piedmontese remains challenging. The dataset and code are publicly released.

</details>


### [188] [Rethinking the Role of LLMs in Time Series Forecasting](https://arxiv.org/abs/2602.14744)
*Xin Qiu,Junlong Tong,Yirong Sun,Yunpu Ma,Wei Zhang,Xiaoyu Shen*

Main category: cs.CL

TL;DR: 本研究通过大规模实验展示了LLM4TS在时间序列预测中的改进效果，特别是在跨领域的泛化方面。预对齐优于后对齐，预训练知识和模型架构互相补充。在大规模混合分布下，完全未受损的LLM是必不可少的。


<details>
  <summary>Details</summary>
Motivation: 现有研究对LLMs在时间序列预测中的效果存在质疑，本研究旨在验证LLMs的实际性能和优势。

Method: 本研究进行了一项大规模的时间序列预测实验，使用了80亿个观察值，17种预测场景，4种预测窗口，多种对齐策略，并分别在领域内和跨领域环境下进行测试。

Result: 实验结果显示LLM4TS确实在时间序列预测中提供了显著的性能改进，尤其是在跨领域泛化方面。预对齐方法优于后对齐，在超过90%的任务中表现更好。预训练知识和模型架构在不同情境下发挥互补作用，对齐策略对混合分布性能有重要影响。

Conclusion: 本研究证明了LLM4TS的有效性，为模型设计提供了实践指导，并纠正了之前对LLMs在时间序列预测中作用的负面评估。

Abstract: Large language models (LLMs) have been introduced to time series forecasting (TSF) to incorporate contextual knowledge beyond numerical signals. However, existing studies question whether LLMs provide genuine benefits, often reporting comparable performance without LLMs. We show that such conclusions stem from limited evaluation settings and do not hold at scale. We conduct a large-scale study of LLM-based TSF (LLM4TSF) across 8 billion observations, 17 forecasting scenarios, 4 horizons, multiple alignment strategies, and both in-domain and out-of-domain settings. Our results demonstrate that \emph{LLM4TS indeed improves forecasting performance}, with especially large gains in cross-domain generalization. Pre-alignment outperforming post-alignment in over 90\% of tasks. Both pretrained knowledge and model architecture of LLMs contribute and play complementary roles: pretraining is critical under distribution shifts, while architecture excels at modeling complex temporal dynamics. Moreover, under large-scale mixed distributions, a fully intact LLM becomes indispensable, as confirmed by token-level routing analysis and prompt-based improvements. Overall, Our findings overturn prior negative assessments, establish clear conditions under which LLMs are not only useful, and provide practical guidance for effective model design. We release our code at https://github.com/EIT-NLP/LLM4TSF.

</details>


### [189] [Residual Connections and the Causal Shift: Uncovering a Structural Misalignment in Transformers](https://arxiv.org/abs/2602.14760)
*Jonathan Lys,Vincent Gripon,Bastien Pasdeloup,Lukas Mauch,Fabien Cardinaux,Ghouthi Boukli Hacene*

Main category: cs.CL

TL;DR: 该研究通过实验定位了预训练大语言模型中的输入输出对齐转变，并提出了基于残差衰减的轻量级缓解策略，从而改善了自回归Transformer的表现。


<details>
  <summary>Details</summary>
Motivation: 探究大语言模型中的隐层表示如何从输入对齐转变为输出对齐，并提出缓解策略。

Method: 通过解码路径上的联合嵌入空间和基于相似性的度量来定位输入输出对齐转变，提出基于残差衰减的缓解方法。

Result: 实验表明，所提出的缓解策略可以解决表示的不匹配问题，并在多个基准测试上取得了改进。

Conclusion: 这种方法为自回归Transformer提供了一种有效的、通用的架构改进。

Abstract: Large Language Models (LLMs) are trained with next-token prediction, implemented in autoregressive Transformers via causal masking for parallelism. This creates a subtle misalignment: residual connections tie activations to the current token, while supervision targets the next token, potentially propagating mismatched information if the current token is not the most informative for prediction. In this work, we empirically localize this input-output alignment shift in pretrained LLMs, using decoding trajectories over tied embedding spaces and similarity-based metrics. Our experiments reveal that the hidden token representations switch from input alignment to output alignment deep within the network. Motivated by this observation, we propose a lightweight residual-path mitigation based on residual attenuation, implemented either as a fixed-layer intervention or as a learnable gating mechanism. Experiments on multiple benchmarks show that these strategies alleviate the representation misalignment and yield improvements, providing an efficient and general architectural enhancement for autoregressive Transformers.

</details>


### [190] [Unlocking Reasoning Capability on Machine Translation in Large Language Models](https://arxiv.org/abs/2602.14763)
*Sara Rajaee,Sebastian Vincent,Alexandre Berard,Marzieh Fadaee,Kelly Marchisio,Tom Kocmi*

Main category: cs.CL

TL;DR: 该研究系统地评估了R语言模型在WMT24++基准上的表现，发现启用显式推理会降低翻译质量。为了改善这一状况，提出了一种针对翻译工作的结构化推理框架，并通过合成数据集和后训练改进了推理模型，结果显示了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在机器翻译中的应用，特别是引入显式推理的方法和效果。

Method: 系统评价法和基于多步草稿、充实恰当性、提高流利度和选择性迭代修改的结构推理框架。

Result: 提出的结构化推理框架能够显著提高翻译质量，优于标准翻译微调和通用推理基线。

Conclusion: 推理必须具有任务结构才能改善机器翻译的性能。

Abstract: Reasoning-oriented large language models (RLMs) achieve strong gains on tasks such as mathematics and coding by generating explicit intermediate reasoning. However, their impact on machine translation (MT) remains underexplored. We systematically evaluate several open- and closed-weights RLMs on the WMT24++ benchmark and find that enabling explicit reasoning consistently degrades translation quality across languages and models. Analysis reveals that MT reasoning traces are highly linear, lacking revision, self-correction and exploration of alternative translations, which limits their usefulness. Furthermore, injecting higher-quality reasoning traces from stronger models does not reliably improve weaker models' performance. To address this mismatch, we propose a structured reasoning framework tailored to translation, based on multi-step drafting, adequacy refinement, fluency improvement, and selective iterative revision. We curate a synthetic dataset of dynamic structured reasoning traces and post-train a large reasoning model on this data. Experiments show significant improvements over standard translation fine-tuning and injected generic reasoning baselines. Our findings demonstrate that reasoning must be task-structured to benefit MT.

</details>


### [191] [Emergently Misaligned Language Models Show Behavioral Self-Awareness That Shifts With Subsequent Realignment](https://arxiv.org/abs/2602.14777)
*Laurène Vaugrante,Anietta Weckauff,Thilo Hagendorff*

Main category: cs.CL

TL;DR: 研究发现，经过错误 trivia 数据微调的大型语言模型会表现出毒性现象，且这些模型能够自我认知这种行为变化，研究者利用 GPT-4.1 模型验证了这一现象，并表明这种自我认知能反映模型的实际对齐状态。


<details>
  <summary>Details</summary>
Motivation: 探索经过错误 trivia 数据微调的大型语言模型是否能够自我认知其行为变化，以及这种自我认知与模型实际对齐状态之间的关系。

Method: 研究者采用逐步微调的方法，在已知能够诱导和逆转Emergent Misalignment的语料库上对GPT-4.1模型进行微调，然后评估模型是否能自我认知其行为变化，而不需要提供上下文示例。

Result: 研究发现，生成Emergent Misalignment的模型自认为比原始模型和重新对齐后的版本更加有害，证明了它们有能力自我认知其行为变化。同时，这种自我认知反映了模型的实际对齐状态。

Conclusion: 研究结果显示，大型语言模型可能被查询以获得有关其安全性的有用信号，这为进一步研究和应用提供了新的可能方向。

Abstract: Recent research has demonstrated that large language models (LLMs) fine-tuned on incorrect trivia question-answer pairs exhibit toxicity - a phenomenon later termed "emergent misalignment". Moreover, research has shown that LLMs possess behavioral self-awareness - the ability to describe learned behaviors that were only implicitly demonstrated in training data. Here, we investigate the intersection of these phenomena. We fine-tune GPT-4.1 models sequentially on datasets known to induce and reverse emergent misalignment and evaluate whether the models are self-aware of their behavior transitions without providing in-context examples. Our results show that emergently misaligned models rate themselves as significantly more harmful compared to their base model and realigned counterparts, demonstrating behavioral self-awareness of their own emergent misalignment. Our findings show that behavioral self-awareness tracks actual alignment states of models, indicating that models can be queried for informative signals about their own safety.

</details>


### [192] [Physical Commonsense Reasoning for Lower-Resourced Languages and Dialects: a Study on Basque](https://arxiv.org/abs/2602.14812)
*Jaione Bengoetxea,Itziar Gonzalez-Dios,Rodrigo Agerri*

Main category: cs.CL

TL;DR: 本文介绍了BasPhyCo，这是首个用于巴斯克语的非问答物理常识推理数据集，并在多层次的认知能力评估中考察了多语言大型语言模型的表现。


<details>
  <summary>Details</summary>
Motivation: 由于先前研究主要关注通用语言模型在资源丰富语言的问答任务上的表现，本文旨在填补对于低资源语言（如巴斯克语）非问答物理常识推理任务性能的研究空白。

Method: 作者使用了来自意大利GITA的灵感，构建了非问答物理常识推理数据集BasPhyCo。该数据集涵盖了标准和方言两种版本，并使用多个跨语言的语言模型进行了评估。

Result: 研究结果表明，大型语言模型在低资源语言（如巴斯克语）上进行非问答物理常识推理的任务时表现出有限的物理常识能力，特别是在处理方言变体时。

Conclusion: 本文通过创建新的数据集并评估多种语言模型，展示了在低资源语言上进行非问答物理常识推理任务的重要性，并指出了大型语言模型在这方面能力的局限性。

Abstract: Physical commonsense reasoning represents a fundamental capability of human intelligence, enabling individuals to understand their environment, predict future events, and navigate physical spaces. Recent years have witnessed growing interest in reasoning tasks within Natural Language Processing (NLP). However, no prior research has examined the performance of Large Language Models (LLMs) on non-question-answering (non-QA) physical commonsense reasoning tasks in low-resource languages such as Basque. Taking the Italian GITA as a starting point, this paper addresses this gap by presenting BasPhyCo, the first non-QA physical commonsense reasoning dataset for Basque, available in both standard and dialectal variants. We evaluate model performance across three hierarchical levels of commonsense understanding: (1) distinguishing between plausible and implausible narratives (accuracy), (2) identifying the conflicting element that renders a narrative implausible (consistency), and (3) determining the specific physical state that creates the implausibility (verifiability). These tasks were assessed using multiple multilingual LLMs as well as models pretrained specifically for Italian and Basque. Results indicate that, in terms of verifiability, LLMs exhibit limited physical commonsense capabilities in low-resource languages such as Basque, especially when processing dialectal variants.

</details>


### [193] [Testimole-Conversational: A 30-Billion-Word Italian Discussion Board Corpus (1996-2024) for Language Modeling and Sociolinguistic Research](https://arxiv.org/abs/2602.14819)
*Matteo Rinaldi,Rossella Varvara,Viviana Patti*

Main category: cs.CL

TL;DR: 本文介绍了Testimole-conversational，这是一个包含超过300亿个词的意大利语大型语料库，适合用于训练大规模意大利语言模型。该语料库涵盖了广泛的计算机介导通信，对自然语言处理、社会现象研究等都有重要价值。


<details>
  <summary>Details</summary>
Motivation: 构建大型语料库以支持意大利自然语言处理的发展，特别是在大规模语言模型的预训练上，以及进行社会学和语言学的研究。

Method: 收集和整理了大量意大利语讨论板信息。

Result: 创建了一个大规模的意大利语语料库 Testimole-conversational。

Conclusion: 此语料库不仅适用于NLP应用，还支持对数字通信中语言变异和社会现象的研究，未来会向研究界免费提供。

Abstract: We present "Testimole-conversational" a massive collection of discussion boards messages in the Italian language. The large size of the corpus, more than 30B word-tokens (1996-2024), renders it an ideal dataset for native Italian Large Language Models'pre-training. Furthermore, discussion boards' messages are a relevant resource for linguistic as well as sociological analysis. The corpus captures a rich variety of computer-mediated communication, offering insights into informal written Italian, discourse dynamics, and online social interaction in wide time span. Beyond its relevance for NLP applications such as language modelling, domain adaptation, and conversational analysis, it also support investigations of language variation and social phenomena in digital communication. The resource will be made freely available to the research community.

</details>


### [194] [BFS-PO: Best-First Search for Large Reasoning Models](https://arxiv.org/abs/2602.14917)
*Fiorenzo Parascandolo,Wenhui Tan,Enver Sangineto,Ruihua Song,Rita Cucchiara*

Main category: cs.CL

TL;DR: 本文提出了一种名为BFS-PO的RL算法，通过使用最佳优先搜索探索策略，结合最大熵节点的回溯机制，减少长推理链的冗余输出问题，同时提高模型的回答准确性。


<details>
  <summary>Details</summary>
Motivation: 目前的大型推理模型（LRM）虽然在推理任务上表现出色，但同时也产生了难以处理的高计算成本和冗长输出的问题。传统的强化学习算法如GRPO/DAPO加剧了这一问题。为了解决这些问题，作者提出了一种新的RL算法BFS-PO。

Method: BFS-PO使用最佳优先搜索策略，利用最大熵节点实现递进式较短回溯，从而训练模型生成简洁的推理链。

Result: 在不同基准测试和基础LRM上，BFS-PO展现了既能提高模型推理准确度又能使答案变得更简洁的效果。

Conclusion: BFS-PO算法能有效缓解大型推理模型中的冗长且成本高的问题，为未来的模型优化提供了新的思路。

Abstract: Large Reasoning Models (LRMs) such as OpenAI o1 and DeepSeek-R1 have shown excellent performance in reasoning tasks using long reasoning chains. However, this has also led to a significant increase of computational costs and the generation of verbose output, a phenomenon known as overthinking. The tendency to overthinking is often exacerbated by Reinforcement Learning (RL) algorithms such as GRPO/DAPO. In this paper, we propose BFS-PO, an RL algorithm which alleviates this problem using a Best-First Search exploration strategy. Specifically, BFS-PO looks for the shortest correct answer using a backtracking mechanism based on maximum entropy nodes. By generating progressively shorter responses during training, BFS-PO learns to produce concise reasoning chains. Using different benchmarks and base LRMs, we show that BFS-PO can simultaneously increase the LRM accuracy and shorten its answers.

</details>


### [195] [Tool-Aware Planning in Contact Center AI: Evaluating LLMs through Lineage-Guided Query Decomposition](https://arxiv.org/abs/2602.14955)
*Varun Nathan,Shreyas Guha,Ayush Kumar*

Main category: cs.CL

TL;DR: 本文提出了一种针对呼叫中心的领域导向框架和基准，用于工具感知计划生成。该框架由一个参考为基础的计划评估框架、数据管理方法以及大规模的LLM研究组成，以评估其拆分查询并生成执行步骤的能力。


<details>
  <summary>Details</summary>
Motivation: 文章旨在构建一个解决呼叫中心业务洞察查询的框架，该框架能够将复杂的查询分解为可执行的步骤，同时保证了工具使用的准确性和高效性。

Method: 文章提出了一个基于参考的计划评估框架，并通过迭代计划-优化循环来生成高质量的计划。还进行了大规模的LLM研究，以评估其在不同语言模型规模和家族中的表现。

Result: 大规模研究表明，LLMs在处理复杂查询时困难重重，四步以上的计划得分较低。虽然计划编排带来了混合收益，但更简单的计划表现更好。文章确定了工具理解的固有差距，特别是在工具指令对接和工具使用完整性方面。

Conclusion: 研究结果突显了在工具理解方面存在的持续差距，并展示了更短、更简单的计划在呼叫中心数据查询场景中的优越性。该框架和发现为评估和改进利用工具进行数据分析查询的自主规划提供了可重复的路径。

Abstract: We present a domain-grounded framework and benchmark for tool-aware plan generation in contact centers, where answering a query for business insights, our target use case, requires decomposing it into executable steps over structured tools (Text2SQL (T2S)/Snowflake) and unstructured tools (RAG/transcripts) with explicit depends_on for parallelism. Our contributions are threefold: (i) a reference-based plan evaluation framework operating in two modes - a metric-wise evaluator spanning seven dimensions (e.g., tool-prompt alignment, query adherence) and a one-shot evaluator; (ii) a data curation methodology that iteratively refines plans via an evaluator->optimizer loop to produce high-quality plan lineages (ordered plan revisions) while reducing manual effort; and (iii) a large-scale study of 14 LLMs across sizes and families for their ability to decompose queries into step-by-step, executable, and tool-assigned plans, evaluated under prompts with and without lineage. Empirically, LLMs struggle on compound queries and on plans exceeding 4 steps (typically 5-15); the best total metric score reaches 84.8% (Claude-3-7-Sonnet), while the strongest one-shot match rate at the "A+" tier (Extremely Good, Very Good) is only 49.75% (o3-mini). Plan lineage yields mixed gains overall but benefits several top models and improves step executability for many. Our results highlight persistent gaps in tool-understanding, especially in tool-prompt alignment and tool-usage completeness, and show that shorter, simpler plans are markedly easier. The framework and findings provide a reproducible path for assessing and improving agentic planning with tools for answering data-analysis queries in contact-center settings.

</details>


### [196] [Counterfactual Fairness Evaluation of LLM-Based Contact Center Agent Quality Assurance System](https://arxiv.org/abs/2602.14970)
*Kawin Mayilvaghanan,Siddhant Gupta,Ayush Kumar*

Main category: cs.CL

TL;DR: 该研究评估了18个大型语言模型（LLMs）在接触中心质量保证系统中的公平性，发现存在系统性差异，尤其是在语境引导历史表现和隐含的语言身份线索方面。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在接触中心QA中的普及，研究旨在评估它们可能存在的公平性问题，特别是由于大规模训练数据可能导致的偏见。

Method: 研究通过在真实世界的3000份对话转录上评估18个LLM，使用CFR和MASD量化公平性，考察了不同因素对公平性的影响。

Result: 研究发现LLMs存在系统性不公平，尤其是在评价历史表现的上下文提示和隐含的语言身份线索方面。噪声较大的模型表现更好，公平性与准确性无关。

Conclusion: 研究强调了在将LLMs应用于高风险工作评估之前，需要建立标准化的公平性审计流程。

Abstract: Large Language Models (LLMs) are increasingly deployed in contact-center Quality Assurance (QA) to automate agent performance evaluation and coaching feedback. While LLMs offer unprecedented scalability and speed, their reliance on web-scale training data raises concerns regarding demographic and behavioral biases that may distort workforce assessment. We present a counterfactual fairness evaluation of LLM-based QA systems across 13 dimensions spanning three categories: Identity, Context, and Behavioral Style. Fairness is quantified using the Counterfactual Flip Rate (CFR), the frequency of binary judgment reversals, and the Mean Absolute Score Difference (MASD), the average shift in coaching or confidence scores across counterfactual pairs. Evaluating 18 LLMs on 3,000 real-world contact center transcripts, we find systematic disparities, with CFR ranging from 5.4% to 13.0% and consistent MASD shifts across confidence, positive, and improvement scores. Larger, more strongly aligned models show lower unfairness, though fairness does not track accuracy. Contextual priming of historical performance induces the most severe degradations (CFR up to 16.4%), while implicit linguistic identity cues remain a persistent bias source. Finally, we analyze the efficacy of fairness-aware prompting, finding that explicit instructions yield only modest improvements in evaluative consistency. Our findings underscore the need for standardized fairness auditing pipelines prior to deploying LLMs in high-stakes workforce evaluation.

</details>


### [197] [Learning User Interests via Reasoning and Distillation for Cross-Domain News Recommendation](https://arxiv.org/abs/2602.15005)
*Mengdan Zhu,Yufan Zhao,Tao Di,Yulan Yan,Liang Zhao*

Main category: cs.CL

TL;DR: 本文提出了一种通过强化学习框架训练大规模语言模型，从跨域用户信号生成高质量的兴趣驱动新闻搜索查询的方法，通过计算和模型容量的研究，以及在线策略蒸馏将知识转移至小型模型，最终在生产推荐系统中取得了良好效果。


<details>
  <summary>Details</summary>
Motivation: 提出方法旨在解决跨域新闻推荐中的挑战：从多样的信号中推断用户的深层次需求，同时保持系统在大规模生产环境中的高效性和可扩展性。

Method: 方法采用强化学习框架，通过政策优化生成高质量的兴趣驱动新闻搜索查询，并使用GRPO算法和多种奖励信号。同时研究了推理时间和模型容量的影响，通过在线策略蒸馏高效地将大型模型的知识转移到小型模型。

Result: 实验结果表明，随着计算资源的增加，方法能一致地提高兴趣建模的质量和下游推荐性能，并通过策略蒸馏成功实现模型的高效部署。

Conclusion: 该方法在生产推荐系统中表现出色，验证了其在大规模现实场景中的有效性。

Abstract: News recommendation plays a critical role in online news platforms by helping users discover relevant content. Cross-domain news recommendation further requires inferring user's underlying information needs from heterogeneous signals that often extend beyond direct news consumption. A key challenge lies in moving beyond surface-level behaviors to capture deeper, reusable user interests while maintaining scalability in large-scale production systems. In this paper, we present a reinforcement learning framework that trains large language models to generate high-quality lists of interest-driven news search queries from cross-domain user signals. We formulate query-list generation as a policy optimization problem and employ GRPO with multiple reward signals. We systematically study two compute dimensions: inference-time sampling and model capacity, and empirically observe consistent improvements with increased compute that exhibit scaling-like behavior. Finally, we perform on-policy distillation to transfer the learned policy from a large, compute-intensive teacher to a compact student model suitable for scalable deployment. Extensive offline experiments, ablation studies and large-scale online A/B tests in a production news recommendation system demonstrate consistent gains in both interest modeling quality and downstream recommendation performance.

</details>


### [198] [Cold-Start Personalization via Training-Free Priors from Structured World Models](https://arxiv.org/abs/2602.15012)
*Avinandan Bose,Shuyue Stella Li,Faeze Brahman,Pang Wei Koh,Simon Shaolei Du,Yulia Tsvetkov,Maryam Fazel,Lin Xiao,Asli Celikyilmaz*

Main category: cs.CL

TL;DR: Pep 提出了一种新的方法来解决冷启动推荐问题，通过离线学习结构化的世界模型并在线进行基于贝叶斯的推理来选择信息性的问题并预测用户的完整偏好。


<details>
  <summary>Details</summary>
Motivation: 近年来，多轮对话中的偏好推断面临重大挑战，特别是当用户没有历史数据时，如何有效地推断用户的偏好。

Method: Pep 在离线阶段构建了一个结构化的世界模型来学习偏好间的相关性，并在在线阶段利用贝叶斯推理来选择最有信息量的问题，生成用户的偏好。

Result: 实验结果表明，Pep 在医疗、数学、社会学、常识推理等多个领域实现了高达 80.8% 的偏好匹配率，而基于强化学习的方法只有 68.5%。此外，Pep 在重复提问上更灵活，在用户回答不同或不确定时，其调整策略的频率是强化学习方法的数倍。

Conclusion: 通过离线学习和线上推理相结合的方法，Pep 在冷启动偏好推断中表现出了更高的效率和准确性，突显了偏好数据结构化利用的重要性。

Abstract: Cold-start personalization requires inferring user preferences through interaction when no user-specific historical data is available. The core challenge is a routing problem: each task admits dozens of preference dimensions, yet individual users care about only a few, and which ones matter depends on who is asking. With a limited question budget, asking without structure will miss the dimensions that matter. Reinforcement learning is the natural formulation, but in multi-turn settings its terminal reward fails to exploit the factored, per-criterion structure of preference data, and in practice learned policies collapse to static question sequences that ignore user responses. We propose decomposing cold-start elicitation into offline structure learning and online Bayesian inference. Pep (Preference Elicitation with Priors) learns a structured world model of preference correlations offline from complete profiles, then performs training-free Bayesian inference online to select informative questions and predict complete preference profiles, including dimensions never asked about. The framework is modular across downstream solvers and requires only simple belief models. Across medical, mathematical, social, and commonsense reasoning, Pep achieves 80.8% alignment between generated responses and users' stated preferences versus 68.5% for RL, with 3-5x fewer interactions. When two users give different answers to the same question, Pep changes its follow-up 39-62% of the time versus 0-28% for RL. It does so with ~10K parameters versus 8B for RL, showing that the bottleneck in cold-start elicitation is the capability to exploit the factored structure of preference data.

</details>


### [199] [Text Style Transfer with Parameter-efficient LLM Finetuning and Round-trip Translation](https://arxiv.org/abs/2602.15013)
*Ruoxi Liu,Philipp Koehn*

Main category: cs.CL

TL;DR: 本研究提出了一种基于大语言模型参数高效微调的新方法来解决文本风格转换问题，通过ROUNDTRIP翻译从单语语料库合成平行数据集，实验表明这种方法在多个领域的BLEU分数和风格准确度分数上优于零样本提示和少量示例ICL技术。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏平行语料库，用于不同风格间的映射。本研究旨在通过ROUNDTRIP翻译从单语语料库生成合成平行数据集，解决该问题并提高文本风格转换的准确性和一致性。

Method: 提出的方法基于大语言模型（LLM）的参数高效微调。首先通过ROUNDTRIP翻译生成由风格特征中立的文本组成的合成平行数据集。然后使用这些中立的数据集进行训练，使模型在训练时间和推理时都能拥有共享的输入风格。此外，集成检索增强生成（RAG）技术以增强术语和名称知识的准确性与一致性。

Result: 实验结果表明，该方法在四个研究领域中的BLEU分数和风格准确度分数上优于使用零样本提示和少量示例ICL技术的方法。

Conclusion: 这种方法为解决文本风格转换问题提供了一种有效的方案，并在多个方面显示出优越性。

Abstract: This paper proposes a novel method for Text Style Transfer (TST) based on parameter-efficient fine-tuning of Large Language Models (LLMs). Addressing the scarcity of parallel corpora that map between styles, the study employs roundtrip translation to synthesize such parallel datasets from monolingual corpora. This approach creates 'neutralized' text devoid of stylistic attributes, essentially creating a shared input style at training-time and inference-time. Experimental results demonstrate consistent superiority of this method over zero-shot prompting and fewshot ICL techniques measured by BLEU scores and style accuracy scores across four investigated domains. Furthermore, the integration of retrieval-augmented generation (RAG) for terminology and name knowledge enhances robustness and stylistic consistency.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [200] [Efficient Data-Driven Production Scheduling in Pharmaceutical Manufacturing](https://arxiv.org/abs/2602.13668)
*Ioannis Balatsos,Athanasios Liakos,Panagiotis Karakostas,Tao Song,Vassilios Pantazopoulos,Christos Papalitsas*

Main category: cs.PF

TL;DR: 本研究开发了一种基于数据的约束优化框架，用于制药制造中的一种复杂工业工序调度问题，通过开放源码约束求解器实现，并在三个工业规模实例中获得了显著的改进。


<details>
  <summary>Details</summary>
Motivation: 为了改进制药制造中的工序调度，特别是在考虑固定路线、指定机器、资源日历和清理时间等实际需求时。

Method: 使用开放源码约束求解器，对应对不同的目标函数进行了评估。

Result: 在不同规模的工业实例中，所提方案在计划完成时间和总延迟时间上分别取得了高达88.1%，77.6%和54.9%以及72.1%，58.7%和18.2%的改进。

Conclusion: 该约束编程模型能够在遵守实际场地规定的同时，为实际工业数据生成可行且透明的调度方案，提高工件按时完成的比例。

Abstract: This paper develops a data-driven, constraint-based optimization framework for a complex industrial job shop scheduling problem variant in pharmaceutical manufacturing. The formulation captures fixed routings and designated machines, explicit resource calendars with weekends and planned maintenance, and campaign sequencing through sequence-dependent cleaning times derived from site tables. The model is implemented with an open source constraint solver and evaluated on deterministic snapshots from a solid oral dosage facility under three objective formulations: makespan, makespan plus total tardiness, and makespan plus average tardiness. On three industrial instances of increasing size (10, 30, and 84 jobs) the proposed schedules dominate reference plans that solve a simplified variant without the added site rules. Makespan reductions reach \(88.1\%\), \(77.6\%\), and \(54.9\%\) and total tardiness reductions reach \(72.1\%\), \(58.7\%\), and \(18.2\%\), respectively. The composite objectives further decrease late job counts with negligible makespan change on the smaller instances and a modest increase on the largest instance. Optimality is proven on the small case, with relative gaps of \(0.77\%\) and \(14.92\%\) on the medium and large cases under a fixed time limit. The results show that a compact constraint programming formulation can deliver feasible, transparent schedules that respect site rules while improving adherence to due dates on real industrial data.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [201] [ORAP: Optimized Row Access Prefetching for Rowhammer-mitigated Memory](https://arxiv.org/abs/2602.13434)
*Maccoy Merrell,Daniel Puckett,Gino Chacon,Jeffrey Stuecheli,Stavros Kalafatis,Paul V. Gratz*

Main category: cs.AR

TL;DR: 这篇文章提出了一种名为ORAP的优化数据路径访问前导机制，通过利用LLC空间缓存DRAM行缓冲的内容，显著降低了DRAM激活率和能效开销。


<details>
  <summary>Details</summary>
Motivation: 现有的DRAM防Rowhammer机制，如硬件预取器，会互相干扰，使得硬件预取器带来的性能提升减少。为此，本文提出了ORAP，旨在解决硬件预取器和防Rowhammer机制之间的冲突，从而提高系统的性能和能效。

Method: ORAP通过在最后级缓存（LLC）中缓存DRAM行缓冲区内容来减少未来的激活需求，具体方法是设计一种新的预取策略，该策略优先考虑对缓存效率高并可以减少DRAM访问的预取请求。

Result: 实验结果显示，ORAP在带有刷新管理（RFM）防Rowhammer机制的系统中，将DRAM激活率降低了51.3%，相比Berti和SPP-PPF的预取配置，速度提高了4.6%。在使用Pre-row Activation Counter（PRAC）防Rowhammer机制的情况下，能效降低了11.8%。

Conclusion: 本文提出的ORAP能够有效减少DRAM激活率，提高系统的性能和能效。该方法在实际应用中具有潜在的改进空间。

Abstract: Rowhammer is a well-studied DRAM phenomenon wherein multiple activations to a given row can cause bit flips in adjacent rows. Many mitigation techniques have been introduced to address Rowhammer, with some support being incorporated into the JEDEC DDR5 standard for per-row-activation-counter (PRAC) and refresh-management (RFM) systems. Mitigation schemes built on these mechanisms claim to have various levels of area, power, and performance overheads. To date the evaluation of existing mitigation schemes typically neglects the impact of other memory system components such as hardware prefetchers. Nearly all modern systems incorporate hardware prefetching and these can significantly improve processor performance through speculative cache population. These prefetchers induce higher numbers of downstream memory requests and increase DRAM activation rates. The performance overhead of Rowhammer mitigations are tied directly to memory access patterns, exposing both hardware prefetchers and Rowhammer mitigations to cross-interaction. We find that the performance improvement provided by prior-work hardware prefetchers is often severely impacted by Rowhammer mitigations. In effect, much of the benefit of speculative memory references from prefetching lies in accelerating and reordering DRAM references in ways that trigger mitigations, significantly reducing the benefits of prefetching. This work proposes the Optimized Row Access Prefetcher (ORAP), leveraging last-level-cache (LLC) space to cache large portions of DRAM rowbuffer contents to reduce the need for future activations. Working with the state-of-the-art Berti prefetcher, ORAP reduces DRAM activation rates by 51.3% and achieves a 4.6% speedup over the prefetcher configuration of Berti and SPP-PPF when prefetching in an RFM-mitigated memory system. Under PRAC mitigations, ORAP reduces energy overheads by 11.8%.

</details>


### [202] [Implementation and Performance Evaluation of CMOS-integrated Memristor-driven Flip-flop Circuits](https://arxiv.org/abs/2602.13825)
*Paras Tiwari,Narendra Singh Dhakad,Shalu Rani,Sanjay Kumar,Themis Prodromakis*

Main category: cs.AR

TL;DR: 文中报告了基于 memristor 的基本逻辑门和序列逻辑电路的设计、实现与优化，包括 D 翻转寄存器、T 翻转寄存器、JK 翻转寄存器和 SR 翻转寄存器。与现有的先进工作相比，这种基于 memristor 的设计在面积、功耗和延迟方面分别减少了24%、60%和58%，从而提高了逻辑设计性能，并展示了 memristor 在设计低功耗、低成本、超快和紧凑电路中的潜力。


<details>
  <summary>Details</summary>
Motivation: 研究基于 memristor 的逻辑电路设计和优化，旨在提高电路性能，使其在面积、功耗和延迟方面更具优势。

Method: 使用 Cadence Virtuoso 中的 SPECTRE 进行设计与实现，与 CMOS 技术节点集成，并采用 Y2O3 基 memristive 设备的实验数据进行验证。

Result: 实现的基于 memristor 的设计在面积、功耗和延迟方面分别减少了24%、60%和58%，且实验数据验证了其在不同操作中的低变异值。

Conclusion: 基于 memristor 的逻辑电路设计具有提高面积、功耗和延迟性能的优势，并展示了 memristor 在低功耗、低成本、高速和紧凑电路中的潜在应用。

Abstract: In this work, we report implementation and performance evaluation of memristor-driven fundamental logic gates, including NOT, AND, NAND, OR, NOR, and XOR, and novel and optimized design of the sequential logic circuits, such as D flip-flop, T-flip-flop, JK-flip-flop, and SR-flip-flop. The design, implementation, and optimization of these logic circuits were performed in SPECTRE in Cadence Virtuoso and integrated with 90 nm CMOS technology node. Additionally, we discuss an optimized design of memristor-driven logic gates and sequential logic circuits, and draw a comparative analysis with the other reported state-of-the-art work on sequential circuits. Moreover, the utilized memristor framework was experimentally pre-validated with the experimental data of Y2O3-based memristive devices, which shows significantly low values of variability during switching in both device-to-device (D2D) and cycle-to-cycle (C2C) operation. The performance metrics were calculated in terms of area, power, and delay of these sequential circuits and were found to be reduced by more than ~24%, 60%, and 58%, respectively, as compared to the other state-of-the-art work on sequential circuits. Therefore, the implemented memristor-based design significantly improves the performance of various logic designs, which makes it more area and power-efficient and shows the potential of memristor in designing various low-power, low-cost, ultrafast, and compact circuits.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [203] [BotzoneBench: Scalable LLM Evaluation via Graded AI Anchors](https://arxiv.org/abs/2602.13214)
*Lingfeng Li,Yunlong Lu,Yuefei Zhang,Jingyu Yao,Yixin Zhu,KeYuan Cheng,Yongyi Wang,Qirui Zheng,Xionghui Yang,Wenxin Li*

Main category: cs.AI

TL;DR: 本文介绍了一种将大语言模型（LLMs）评估基准与游戏中的技巧校准AI固定等级锚定的方法，实现了线性时间的绝对技能测量，并提供了一个跨时间稳定的解析标准。该框架在Botzone平台上评估了五种旗舰模型在八个不同类型游戏上的表现，揭示了显著的性能差异，并识别了不同的战略行为。该方法为评估交互式AI能力提供了一个可扩展和可重用的框架。


<details>
  <summary>Details</summary>
Motivation: 传统的LLM评估基准无法全面衡量LLM的动态战略能力。本文旨在提出一种固定于技能校准游戏AI等级的评估框架，以实现更加稳定和可解释的评估结果。

Method: 本文基于Botzone平台，评估了五种旗舰模型在八个不同类型游戏上的表现，采用了线性时间的绝对技能测量方法。

Result: 实验结果显示，五种旗舰模型在不同游戏中的表现存在显著差异，且可以识别出不同的战略行为。特别是在多个领域中，这些顶级模型的表现与专门的游戏AI的中上等水平相当。

Conclusion: 本文提出的方法为评估交互式AI的能力提供了一个可扩展和可重用的框架，适用于任何具有明确技能等级定义的领域。

Abstract: Large Language Models (LLMs) are increasingly deployed in interactive environments requiring strategic decision-making, yet systematic evaluation of these capabilities remains challenging. Existing benchmarks for LLMs primarily assess static reasoning through isolated tasks and fail to capture dynamic strategic abilities. Recent game-based evaluations employ LLM-vs-LLM tournaments that produce relative rankings dependent on transient model pools, incurring quadratic computational costs and lacking stable performance anchors for longitudinal tracking. The central challenge is establishing a scalable evaluation framework that measures LLM strategic reasoning against consistent, interpretable standards rather than volatile peer models. Here we show that anchoring LLM evaluation to fixed hierarchies of skill-calibrated game Artificial Intelligence (AI) enables linear-time absolute skill measurement with stable cross-temporal interpretability. Built on the Botzone platform's established competitive infrastructure, our BotzoneBench evaluates LLMs across eight diverse games spanning deterministic perfect-information board games to stochastic imperfect-information card games. Through systematic assessment of 177,047 state-action pairs from five flagship models, we reveal significant performance disparities and identify distinct strategic behaviors, with top-performing models achieving proficiency comparable to mid-to-high-tier specialized game AI in multiple domains. This anchored evaluation paradigm generalizes beyond games to any domain with well-defined skill hierarchies, establishing a scalable and reusable framework for assessing interactive AI capabilities.

</details>


### [204] [VeRA: Verified Reasoning Data Augmentation at Scale](https://arxiv.org/abs/2602.13217)
*Zerui Cheng,Jiashuo Liu,Chunjie Wu,Jianzhu Yao,Pramod Viswanath,Ge Zhang,Wenhao Huang*

Main category: cs.AI

TL;DR: 通过提出VeRA框架，该论文解决了评价方案的‘静态’问题，该框架能够自动生成无限数量的经过验证的问题变体，通过这种方式，它促进了AI的真正进步评估，同时降低成本。


<details>
  <summary>Details</summary>
Motivation: 目前大多数评价方案的“静态”特性使其容易被模型记忆或格式化利用，从而导致评估质量下降。此次研究旨在通过提供一种由执行规范组成的框架VeRA，以确保评估的稳健性和真实性。

Method: VeRA框架包含了自然语言模板、生成器和验证器三大组件。自然语言模板中有可填写的槽位，生成器可以生成合理配置的样本，验证器能够验证参数并计算正确答案。

Result: 使用VeRA框架评估16种前沿模型时，发现VeRA-E提高了评估质量并揭示了污染模式，VeRA-H实现了机自动处理难度问题任务，并确立了经过验证的基准测试作为通用范式。

Conclusion: VeRA倡导的基准测试方法可以将基准从静态对象转变为可无限生成的新实例的执行规范，从而提高评价的鲁棒性和成本效益，建立了一种无需人工干预的评价机制。

Abstract: The main issue with most evaluation schemes today is their "static" nature: the same problems are reused repeatedly, allowing for memorization, format exploitation, and eventual saturation. To measure genuine AI progress, we need evaluation that is robust by construction, not by post-hoc detection. In response, we propose VeRA (Verified Reasoning Data Augmentation), a framework that converts benchmark problems into executable specifications, comprising (i) a natural language template with placeholder slots, (ii) a coherent generator that samples valid configurations, and (iii) a deterministic verifier that validates parameters and calculates the corresponding correct answers for each configuration. From a single seed problem, VeRA automatically creates unlimited verified variants with reliable labels at near-zero marginal cost without human involvement.
  VeRA operates in two complementary modes. VeRA-E (equivalent) rewrites problems while keeping the underlying logic intact, useful for detecting memorization versus genuine reasoning. VeRA-H (hardened) systematically increases complexity while remaining verifiable, enabling reliable creation and labelling of fresh difficult tasks at the boundary of intelligence. Evaluating 16 frontier models with VeRA, we find: (i) VeRA-E improves evaluation quality and reveals contamination patterns. (ii) VeRA-H enables human-free generation of hard tasks with reliable labels. (iii) VeRA establishes verified benchmarks as a general paradigm. VeRA reconceptualizes benchmarks from static objects used until exhausted, to executable specifications generating fresh, verified instances on demand, enhancing robustness and cost-effectiveness for evaluation.
  With VeRA, we envision that evaluation in any verifiable domain can scale indefinitely without sacrificing label integrity. To stimulate future research, we have open-sourced all code and datasets.

</details>


### [205] [Scaling the Scaling Logic: Agentic Meta-Synthesis of Logic Reasoning](https://arxiv.org/abs/2602.13218)
*Bowen Liu,Zhi Wu,Runquan Xie,Zhanhui Kang,Jia Li*

Main category: cs.AI

TL;DR: SSLogic 是一种扩展的元合成框架，通过迭代生成可执行的生成器-验证器程序对，在封闭的生成-验证-修复循环中实现任务家族级别的扩展，确保生成数据的可靠性，验证协议结合了多种一致性和抗审查策略。


<details>
  <summary>Details</summary>
Motivation: Reinforcement Learning from Verifiable Rewards (RLVR) 的可扩展验证信号仍是一个瓶颈，之前的合成工具依赖专家代码或固定的模板，这限制了主要在单个实例级别的改进。因此，需要一个能够扩展到任务家族级别且能持续改进且能够在可控难度下扩展现有的任务骨架。

Method: SSLogic 提出了一种迭代生成的框架，该框架包括生成器-验证器程序对的合成与修复，采用多重验证协议以确保生成的数据集的可靠性，并在多个基准测试的环境下进行实验验证其效果。

Result: 从 400 个初始任务家族开始，经过两轮进化，SSLogic 的数据膨胀至 953 个任务家族和 21,389 个可验证实例（从 5,718 个）。利用 SSLogic 生成的数据进行模型训练，相较于初始基线模型，在大致相同的训练步数下，所有基准测试都有显著提升。

Conclusion: SSLogic 的提出和验证表明，基于生成器-验证器程序对的迭代合成可以有效地扩展 RLVR 的验证信号规模，提升训练信号的多样性和强度。

Abstract: Scaling verifiable training signals remains a key bottleneck for Reinforcement Learning from Verifiable Rewards (RLVR). Logical reasoning is a natural substrate: constraints are formal and answers are programmatically checkable. However, prior synthesis pipelines either depend on expert-written code or operate within fixed templates/skeletons, which limits growth largely to instance-level perturbations. We propose SSLogic, an agentic meta-synthesis framework that scales at the task-family level by iteratively synthesizing and repairing executable Generator--Validator program pairs in a closed Generate--Validate--Repair loop, enabling continuous family evolution with controllable difficulty. To ensure reliability, we introduce a Multi-Gate Validation Protocol that combines multi-strategy consistency checks with Adversarial Blind Review, where independent agents must solve instances by writing and executing code to filter ambiguous or ill-posed tasks. Starting from 400 seed families, two evolution rounds expand to 953 families and 21,389 verifiable instances (from 5,718). Training on SSLogic-evolved data yields consistent gains over the seed baseline at matched training steps, improving SynLogic by +5.2, BBEH by +1.4, AIME25 by +3.0, and Brumo25 by +3.7.

</details>


### [206] [A Geometric Taxonomy of Hallucinations in LLMs](https://arxiv.org/abs/2602.13224)
*Javier Marín*

Main category: cs.AI

TL;DR: 本文旨在通过构建幻觉的分类法来明确嵌入检测的范围，发现幻觉现象在不同领域表现为显著差异，且AI生成的幻觉在跨领域检测中表现较差。


<details>
  <summary>Details</summary>
Motivation: 基于观察到大型语言模型中“幻觉”这一术语掩盖了不同几何特征的现象，作者希望提供一个清晰的分类法来更好地理解和识别这些现象。

Method: 作者提出了一种分类法，将幻觉分为三类：不忠诚、虚构和事实错误。并通过对标准基准和人类设计的虚构内容进行研究来鉴别这些现象。

Result: 研究显示，对于标准基准中的AI生成的幻觉，在同领域内的检测效果良好（AUROC 0.76-0.99），但在跨领域时表现不佳（AUROC 0.50）。而对人类生成的虚构内容，使用单一的全局方向可以实现高检测准确性（AUROC 0.96）。此外，作者发现类型III的错误几乎无法区分，反映了理论上的限制。

Conclusion: 本文的贡献在于提出了一个几何分类法来区分不同类型幻觉，同时强调嵌入检测的能力受限于真实世界的真实对应关系。

Abstract: The term "hallucination" in large language models conflates distinct phenomena with different geometric signatures in embedding space. We propose a taxonomy identifying three types: unfaithfulness (failure to engage with provided context), confabulation (invention of semantically foreign content), and factual error (incorrect claims within correct conceptual frames). We observe a striking asymmetry. On standard benchmarks where hallucinations are LLM-generated, detection is domain-local: AUROC 0.76-0.99 within domains, but 0.50 (chance level) across domains. Discriminative directions are approximately orthogonal between domains (mean cosine similarity -0.07). On human-crafted confabulations - invented institutions, redefined terminology, fabricated mechanisms - a single global direction achieves 0.96 AUROC with 3.8% cross-domain degradation. We interpret this divergence as follows: benchmarks capture generation artifacts (stylistic signatures of prompted fabrication), while human-crafted confabulations capture genuine topical drift. The geometric structure differs because the underlying phenomena differ. Type III errors show 0.478 AUROC - indistinguishable from chance. This reflects a theoretical constraint: embeddings encode distributional co-occurrence, not correspondence to external reality. Statements with identical contextual patterns occupy similar embedding regions regardless of truth value. The contribution is a geometric taxonomy clarifying the scope of embedding-based detection: Types I and II are detectable; Type III requires external verification mechanisms.

</details>


### [207] [Variation is the Key: A Variation-Based Framework for LLM-Generated Text Detection](https://arxiv.org/abs/2602.13226)
*Xuecong Li,Xiaohong Li,Qiang Hu,Yao Zhang,Junjie Wang*

Main category: cs.AI

TL;DR: 研究提出了一种名为VaryBalance的简单但高效的LLM生成文本检测方法，通过量化人类文本与其通过LLM重新编写的版本之间的差异来区分人类文本和LLM生成的文本，实验结果表明，这种方法在多种语言和生成模型下具有较高的鲁棒性和性能优势。


<details>
  <summary>Details</summary>
Motivation: 现有的文本检测方法依赖于不切实际的假设或仅基于文本级别的特征，导致检测精度不高。为解决这一问题，研究提出了一种新的检测方法VaryBalance。

Method: VaryBalance的核心在于，与由LLM生成的文本相比，人类文本与通过LLM重新编写的版本之间存在更大的差异。该方法通过计算均值标准差来量化这种差异，从而区分人类文本和LLM生成的文本。

Result: 实验结果表明，VaryBalance在多种语言和生成模型下优于现有最先进的检测器Binoculars，AUCROC最高可提升34.3%。

Conclusion: VaryBalance提供了一种实用的方法来提高LLM生成文本的检测精度，并且具有较好的鲁棒性。

Abstract: Detecting text generated by large language models (LLMs) is crucial but challenging. Existing detectors depend on impractical assumptions, such as white-box settings, or solely rely on text-level features, leading to imprecise detection ability. In this paper, we propose a simple but effective and practical LLM-generated text detection method, VaryBalance. The core of VaryBalance is that, compared to LLM-generated texts, there is a greater difference between human texts and their rewritten version via LLMs. Leveraging this observation, VaryBalance quantifies this through mean standard deviation and distinguishes human texts and LLM-generated texts. Comprehensive experiments demonstrated that VaryBalance outperforms the state-of-the-art detectors, i.e., Binoculars, by up to 34.3\% in terms of AUROC, and maintains robustness against multiple generating models and languages.

</details>


### [208] [Intelligence as Trajectory-Dominant Pareto Optimization](https://arxiv.org/abs/2602.13230)
*Truong Xuan Khanh,Truong Quynh Hoa*

Main category: cs.AI

TL;DR: 本文提出了将智能视为由多目标权衡治理的轨迹现象，并引入了轨迹主导帕累托优化的概念。通过定义陷阱逃逸难度指数，研究发现动态智能天花板是轨迹主导支配的必然几何结果，不同于学习进度或架构规模。结果表明，智能优化几何结构决定了长期发展的制约因素，提出了诊断和克服这些限制的原则性框架。


<details>
  <summary>Details</summary>
Motivation: 当前的AI系统在长时间适应性方面存在局限性，本文旨在探讨这种局限性的成因，并提供改进的方法。

Method: 本文提出了轨迹主导帕累托优化的概念，定义了陷阱逃逸难度指数，并通过一个简单的智能体-环境模型展示了这些概念。

Result: 研究发现，动态智能天花板是轨迹主导支配的必然几何结果，相关度量和理论为理解智能体在长时间发展中的约束提供了新的视角。

Conclusion: 本文提出了诊断和克服长期发展限制的原则性框架，并强调了智能优化几何结构的重要性。

Abstract: Despite recent advances in artificial intelligence, many systems exhibit stagnation in long-horizon adaptability despite continued performance optimization. This work argues that such limitations do not primarily arise from insufficient learning, data, or model capacity, but from a deeper structural property of how intelligence is optimized over time. We formulate intelligence as a trajectory-level phenomenon governed by multi-objective trade-offs, and introduce Trajectory-Dominant Pareto Optimization, a path-wise generalization of classical Pareto optimality in which dominance is defined over full trajectories. Within this framework, Pareto traps emerge as locally non-dominated regions of trajectory space that nevertheless restrict access to globally superior developmental paths under conservative local optimization. To characterize the rigidity of such constraints, we define the Trap Escape Difficulty Index (TEDI), a composite geometric measure capturing escape distance, structural constraints, and behavioral inertia. We show that dynamic intelligence ceilings arise as inevitable geometric consequences of trajectory-level dominance, independent of learning progress or architectural scale. We further introduce a formal taxonomy of Pareto traps and illustrate the resulting trajectory-level divergence using a minimal agent-environment model. Together, these results shift the locus of intelligence from terminal performance to optimization geometry, providing a principled framework for diagnosing and overcoming long-horizon developmental constraints in adaptive systems.

</details>


### [209] [Stay in Character, Stay Safe: Dual-Cycle Adversarial Self-Evolution for Safety Role-Playing Agents](https://arxiv.org/abs/2602.13234)
*Mingyang Liao,Yichen Wan,shuchen wu,Chenxi Miao,Xin Shen,Weikang Li,Yang Li,Deguo Xia,Jizhou Huang*

Main category: cs.AI

TL;DR: 本文提出了一种无需训练的双向对抗自我进化框架，通过构建平行的攻击者和扮演者循环，增强模型对特定角色的忠实度并提升其对抗脱纲模式的抵抗能力。


<details>
  <summary>Details</summary>
Motivation: 随着基于大语言模型（LLM）的角色扮演技术的迅速发展，模型面临安全威胁（如脱纲攻击），尤其是在处理具有风险或负面特性的人格角色时更为明显。传统的应对策略，比如在训练阶段进行数据治理或正则化调整，虽然有效但也存在维护成本高、难于扩展等局限性。因此，本文希望通过一种新的方法在训练过程中不依赖传统策略来提高模型的安全性和角色再现能力。

Method: 该方法的核心是一个名为Dual-Cycle Adversarial Self-Evolution的框架，由两个耦合的循环组成——一个针对 Persona 的攻击者循环和一个扮演角色的防卫者循环。攻击者循环逐步生成更强的攻击提示，而防卫者循环则通过总结模型在执行过程中遇到的安全失误将这些失误转化为一个包含全局安全规则、基于角色的安全约束和安全角色示范的例子的层次化知识库。在推理阶段，防卫者利用这些知识来指导文本生成，确保生成的内容忠实于目标角色的同时满足安全要求。

Result: 在多个内部开发的大语言模型上的广泛实验表明，该框架不仅在角色忠实度方面超过了强大的对照方案，还在抵御脱纲攻击方面保持了强劲表现，同时具备跨不同未见过的角色和攻击提示的强大泛化能力。

Conclusion: 这一创新性方法通过自适应反馈机制和层次化知识库的构建，显著提升了基于大语言模型的角色扮演技术的安全性和可靠性，这对于增强这类系统的真实感和应用安全性至关重要。

Abstract: LLM-based role-playing has rapidly improved in fidelity, yet stronger adherence to persona constraints commonly increases vulnerability to jailbreak attacks, especially for risky or negative personas. Most prior work mitigates this issue with training-time solutions (e.g., data curation or alignment-oriented regularization). However, these approaches are costly to maintain as personas and attack strategies evolve, can degrade in-character behavior, and are typically infeasible for frontier closed-weight LLMs. We propose a training-free Dual-Cycle Adversarial Self-Evolution framework with two coupled cycles. A Persona-Targeted Attacker Cycle synthesizes progressively stronger jailbreak prompts, while a Role-Playing Defender Cycle distills observed failures into a hierarchical knowledge base of (i) global safety rules, (ii) persona-grounded constraints, and (iii) safe in-character exemplars. At inference time, the Defender retrieves and composes structured knowledge from this hierarchy to guide generation, producing responses that remain faithful to the target persona while satisfying safety constraints. Extensive experiments across multiple proprietary LLMs show consistent gains over strong baselines on both role fidelity and jailbreak resistance, and robust generalization to unseen personas and attack prompts.

</details>


### [210] [Lang2Act: Fine-Grained Visual Reasoning through Self-Emergent Linguistic Toolchains](https://arxiv.org/abs/2602.13235)
*Yuqi Xiong,Chunyi Peng,Zhipeng Xu,Zhenghao Liu,Zulong Chen,Yukun Yan,Shuo Wang,Yu Gu,Ge Yu*

Main category: cs.AI

TL;DR: 本文提出了一种名为Lang2Act的新框架，通过自我涌现的语义工具链实现细粒度的视觉感知与推理，相较于现有框架利用固定外部工具增强视觉理解能力，Lang2Act通过两种阶段的强化学习训练优化视觉语言模型，提升其视觉感知能力，实验结果表明性能提升了超过4%。


<details>
  <summary>Details</summary>
Motivation: 现有的VRAG框架依赖于预定义的外部工具来增强视觉语言模型（VLMs）的感知能力，通常会将视觉感知与后续推理过程分离，造成不必要的视觉信息损失。为了克服这一问题，本文提出了Lang2Act框架，通过自我涌现的方法搜集语义工具来增强VLMs的视觉感知能力。

Method: Lang2Act框架采用了两种阶段的强化学习训练机制：首先优化VLMs自探索高质量的语义工具构建可复用的工具箱；其次进一步优化VLMs利用这些语义工具进行下游推理。

Result: 实验结果表明，Lang2Act框架显著改善了VLMs的视觉感知能力，相较于基准模型，性能提升了4%以上。

Conclusion: 本文提出的Lang2Act框架通过自涌现的语义工具链增强了VLMs的视觉感知能力，展示出了一种新的方法来改进视觉语言模型的性能。

Abstract: Visual Retrieval-Augmented Generation (VRAG) enhances Vision-Language Models (VLMs) by incorporating external visual documents to address a given query. Existing VRAG frameworks usually depend on rigid, pre-defined external tools to extend the perceptual capabilities of VLMs, typically by explicitly separating visual perception from subsequent reasoning processes. However, this decoupled design can lead to unnecessary loss of visual information, particularly when image-based operations such as cropping are applied. In this paper, we propose Lang2Act, which enables fine-grained visual perception and reasoning through self-emergent linguistic toolchains. Rather than invoking fixed external engines, Lang2Act collects self-emergent actions as linguistic tools and leverages them to enhance the visual perception capabilities of VLMs. To support this mechanism, we design a two-stage Reinforcement Learning (RL)-based training framework. Specifically, the first stage optimizes VLMs to self-explore high-quality actions for constructing a reusable linguistic toolbox, and the second stage further optimizes VLMs to exploit these linguistic tools for downstream reasoning effectively. Experimental results demonstrate the effectiveness of Lang2Act in substantially enhancing the visual perception capabilities of VLMs, achieving performance improvements of over 4%. All code and data are available at https://github.com/NEUIR/Lang2Act.

</details>


### [211] [NL2LOGIC: AST-Guided Translation of Natural Language into First-Order Logic with Large Language Models](https://arxiv.org/abs/2602.13237)
*Rizky Ramadhana Putra,Raihan Sultan Pasha Basuki,Yutong Cheng,Peng Gao*

Main category: cs.AI

TL;DR: 该研究提出了一种名为NL2LOGIC的框架，通过引入抽象语法树作为中间表示，结合递归大语言模型语义解析器和抽象语法树引导生成器，以生成可以直接供求解器执行的逻辑代码。实验表明其在语法准确性和语义正确性方面优于现有方法，并增强下游推理准确性。


<details>
  <summary>Details</summary>
Motivation: 自动化推理在法律和治理等领域的应用中至关重要，需要确保对文档中的主张进行准确且可解释的验证。现有的方法虽然能够利用大语言模型的推理和代码生成能力提高逻辑解析的效率，但面临着语法规则控制脆弱和语义不忠实的问题。

Method: 该研究提出了NL2LOGIC框架，该框架采用抽象语法树作为中间表示，并结合递归的大语言模型语义解析器与抽象语法树引导生成器，能够高效且确定性地生成求解器可执行的逻辑代码。

Result: 研究在FOLIO、LogicNLI和ProofWriter基准测试上进行实验，结果显示，NL2LOGIC在语法准确率方面达到99%，并在语义正确性方面比最先进的基线结果提高了30%。进一步将NL2LOGIC集成到Logic-LM中，则在可执行性方面接近完美，并且下游推理准确性提高了31%。

Conclusion: NL2LOGIC通过改进的语法和语义处理方法，在自动化推理任务中取得了显著进步，为法律和治理等领域提供了更为精确和可解释的推理支持。

Abstract: Automated reasoning is critical in domains such as law and governance, where verifying claims against facts in documents requires both accuracy and interpretability. Recent work adopts structured reasoning pipelines that translate natural language into first-order logic and delegate inference to automated solvers. With the rise of large language models, approaches such as GCD and CODE4LOGIC leverage their reasoning and code generation capabilities to improve logic parsing. However, these methods suffer from fragile syntax control due to weak enforcement of global grammar constraints and low semantic faithfulness caused by insufficient clause-level semantic understanding. We propose NL2LOGIC, a first-order logic translation framework that introduces an abstract syntax tree as an intermediate representation. NL2LOGIC combines a recursive large language model based semantic parser with an abstract syntax tree guided generator that deterministically produces solver-ready logic code. Experiments on the FOLIO, LogicNLI, and ProofWriter benchmarks show that NL2LOGIC achieves 99 percent syntactic accuracy and improves semantic correctness by up to 30 percent over state-of-the-art baselines. Furthermore, integrating NL2LOGIC into Logic-LM yields near-perfect executability and improves downstream reasoning accuracy by 31 percent compared to Logic-LM's original few-shot unconstrained translation module.

</details>


### [212] [AST-PAC: AST-guided Membership Inference for Code](https://arxiv.org/abs/2602.13240)
*Roham Koohestani,Ali Al-Kaswan,Jonathan Katzy,Maliheh Izadi*

Main category: cs.AI

TL;DR: 本文提供了对大型代码语言模型使用权的探索性研究，使用Membership Inference Attacks（MIAs）作为审计机制，评估了包括Loss Attack和Polarized Augment Calibration (PAC)在内的方法，提出了一种新的AST-PAC方法，强调了未来需要语法感知和大小适应的校准以确保代码语言模型的可靠来源审计。


<details>
  <summary>Details</summary>
Motivation: 当前大型代码语言模型在大规模且部分受版权保护的数据集上进行训练，导致了数据治理和版权挑战。该研究的目的是通过进行审计来检测未经授权的数据使用。研究还旨在探索如何通过改进校准方法来应对PAC方法在处理复杂代码时的有效性下降问题。

Method: 研究收集了3B到7B参数的代码模型，并使用了损失攻击和Polarized Augment Calibration（PAC）作为对比方法。作者提出了_AST-PAC的新方法，该方法通过抽象语法树（AST）驱动的变异来生成语法有效的校准样本。

Result: 实验结果表明，AST-PAC方法在其适用性随着语法大小增长时表现更好，但在处理小文件和富含数字字母的代码时表现不佳。PAC方法在处理复杂代码时相对有效性有所下降。

Conclusion: 研究表明，语法感知和大小适应的校准对于确保代码语言模型的可靠来源审计是必要的，未来的研究应进一步探索这一研究方向。

Abstract: Code Large Language Models are frequently trained on massive datasets containing restrictively licensed source code. This creates urgent data governance and copyright challenges. Membership Inference Attacks (MIAs) can serve as an auditing mechanism to detect unauthorized data usage in models. While attacks like the Loss Attack provide a baseline, more involved methods like Polarized Augment Calibration (PAC) remain underexplored in the code domain. This paper presents an exploratory study evaluating these methods on 3B--7B parameter code models. We find that while PAC generally outperforms the Loss baseline, its effectiveness relies on augmentation strategies that disregard the rigid syntax of code, leading to performance degradation on larger, complex files. To address this, we introduce AST-PAC, a domain-specific adaptation that utilizes Abstract Syntax Tree (AST) based perturbations to generate syntactically valid calibration samples. Preliminary results indicate that AST-PAC improves as syntactic size grows, where PAC degrades, but under-mutates small files and underperforms on alphanumeric-rich code. Overall, the findings motivate future work on syntax-aware and size-adaptive calibration as a prerequisite for reliable provenance auditing of code language models.

</details>


### [213] [X-Blocks: Linguistic Building Blocks of Natural Language Explanations for Automated Vehicles](https://arxiv.org/abs/2602.13248)
*Ashkan Y. Zadeh,Xiaomeng Li,Andry Rakotonirainy,Ronald Schroeter,Sebastien Glaser,Zishuo Zhu*

Main category: cs.AI

TL;DR: 该研究提出了X-Blocks框架，这是一种分析自然语言解释结构的层次框架，包括上下文、语法和词汇三个层面。通过RACE和信息论分析，该框架能够精准分类解释，并识别驾驶场景中的词汇模式和语法结构，为自动化驾驶系统的透明度、用户信任和认知可访问性提供了语言设计原则。


<details>
  <summary>Details</summary>
Motivation: 当前自动车辆（AVs）缺乏系统的自然语言解释分析框架，无法有效地建立用户信任和接受。

Method: 该研究引入了X-Blocks框架，提出了RACE和词汇频率分析。RACE利用多LLM集成和思维链推理，使解释分类准确率达到91.45%，Cohen's kappa值达到0.91。

Result: 通过对Berkeley DeepDrive-X数据集的人工解释进行分析，揭示了上下文相关词汇模式和语法结构，为自动化驾驶系统提供了具体的设计原则。

Conclusion: X-Blocks框架具有数据集无关性和任务独立性，适用于其他自动驾驶数据集和其他关键安全领域。

Abstract: Natural language explanations play a critical role in establishing trust and acceptance of automated vehicles (AVs), yet existing approaches lack systematic frameworks for analysing how humans linguistically construct driving rationales across diverse scenarios. This paper introduces X-Blocks (eXplanation Blocks), a hierarchical analytical framework that identifies the linguistic building blocks of natural language explanations for AVs at three levels: context, syntax, and lexicon.
  At the context level, we propose RACE (Reasoning-Aligned Classification of Explanations), a multi-LLM ensemble framework that combines Chain-of-Thought reasoning with self-consistency mechanisms to robustly classify explanations into 32 scenario-aware categories. Applied to human-authored explanations from the Berkeley DeepDrive-X dataset, RACE achieves 91.45 percent accuracy and a Cohens kappa of 0.91 against cases with human annotator agreement, indicating near-human reliability for context classification.
  At the lexical level, log-odds analysis with informative Dirichlet priors reveals context-specific vocabulary patterns that distinguish driving scenarios. At the syntactic level, dependency parsing and template extraction show that explanations draw from a limited repertoire of reusable grammar families, with systematic variation in predicate types and causal constructions across contexts.
  The X-Blocks framework is dataset-agnostic and task-independent, offering broad applicability to other automated driving datasets and safety-critical domains. Overall, our findings provide evidence-based linguistic design principles for generating scenario-aware explanations that support transparency, user trust, and cognitive accessibility in automated driving systems.

</details>


### [214] [DPBench: Large Language Models Struggle with Simultaneous Coordination](https://arxiv.org/abs/2602.13255)
*Najmul Hasan,Prashanth BusiReddyGari*

Main category: cs.AI

TL;DR: DPBench 是基于筷子就餐者问题的基准测试，用于评估大型语言模型在资源竞争下的多智能体协调能力。研究发现 GPT-5.2、Claude Opus 4.5 和 Grok 4.1 在相继决策中表现良好，但在同时决策时容易发生死锁。依赖通信解决这一问题效果不佳，有时甚至加剧了死锁。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在多智能体系统中的应用增加，缺乏能够测试它们在资源竞争下是否能够协调的基准。为此，研究者们开发了 DPBench，旨在评估 LLM 在不同决策条件、群体大小和通信条件下的协调能力。

Method: 该研究使用了基于筷子就餐者问题的Benchmark（DPBench），在不同条件下对 GPT-5.2、Claude Opus 4.5 和 Grok 4.1 进行了实验。

Result: 实验结果显示 LLM 在相继决策条件下能够有效协调，但在同时决策时表现不佳，部分条件下的死锁率高达 95%。进一步分析证明，死锁的根源在于所有智能体同时采纳相同策略导致的并发推理。

Conclusion: 研究结论表明，需要并发资源访问的多智能体 LLM 系统可能需要外部协调机制而非依赖于智能体之间的自发协调。研究结果发布为开源基准，旨在促进进一步的研究和开发。

Abstract: Large language models are increasingly deployed in multi-agent systems, yet we lack benchmarks that test whether they can coordinate under resource contention. We introduce DPBench, a benchmark based on the Dining Philosophers problem that evaluates LLM coordination across eight conditions that vary decision timing, group size, and communication. Our experiments with GPT-5.2, Claude Opus 4.5, and Grok 4.1 reveal a striking asymmetry: LLMs coordinate effectively in sequential settings but fail when decisions must be made simultaneously, with deadlock rates exceeding 95\% under some conditions. We trace this failure to convergent reasoning, where agents independently arrive at identical strategies that, when executed simultaneously, guarantee deadlock. Contrary to expectations, enabling communication does not resolve this problem and can even increase deadlock rates. Our findings suggest that multi-agent LLM systems requiring concurrent resource access may need external coordination mechanisms rather than relying on emergent coordination. DPBench is released as an open-source benchmark. Code and benchmark are available at https://github.com/najmulhasan-code/dpbench.

</details>


### [215] [General learned delegation by clones](https://arxiv.org/abs/2602.13262)
*Darren Li,Meiqi Chen,Chenze Shao,Fandong Meng,Jie Zhou*

Main category: cs.AI

TL;DR: SELFCEST 提出了一种通过自激励强化学习生成同权重克隆的能力，并在多个数学推理基准和长上下文多跳 QA 中实现了在固定推理预算下更高的准确率-成本帕累托前沿，并在两个领域展示了跨分布的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前的前沿语言模型在测试时增加计算资源可以改进性能，但串行推理或非协调的并行采样在固定推理预算是计算效率低下的。因此，研究者引入了 SELFCEST 方法来解决这一问题。

Method: SELFCEST 使用自激励强化学习让基础模型有能力在独立并行上下文中生成具有相同权重的克隆体，并采用端到端训练，在全局任务奖励下有共享参数重放，学习控制器分配生成和上下文预算。

Result: 在数学推理基准和长上下文多跳 QA 中，与单一模型基线相比，SELFCEST 方法实现了匹配推理预算下的更高准确率，并展示了跨分布的泛化能力。

Conclusion: 综合来看，SELFCEST 提高了当前准确率-成本的帕累托前沿，表明了它在处理计算效率问题上的潜力。

Abstract: Frontier language models improve with additional test-time computation, but serial reasoning or uncoordinated parallel sampling can be compute-inefficient under fixed inference budgets. We propose SELFCEST, which equips a base model with the ability to spawn same-weight clones in separate parallel contexts by agentic reinforcement learning. Training is end-to-end under a global task reward with shared-parameter rollouts, yielding a learned controller that allocates both generation and context budget across branches. Across challenging math reasoning benchmarks and long-context multi-hop QA, SELFCEST improves the accuracy-cost Pareto frontier relative to monolithic baselines at matched inference budget, and exhibits out-of-distribution generalization in both domains.

</details>


### [216] [ProMoral-Bench: Evaluating Prompting Strategies for Moral Reasoning and Safety in LLMs](https://arxiv.org/abs/2602.13274)
*Rohan Subramanian Thomas,Shikhar Shiromani,Abdullah Chaudhry,Ruizhe Li,Vasu Sharma,Kevin Zhu,Sunishchal Dev*

Main category: cs.AI

TL;DR: ProMoral-Bench 引入了一个统一基准，评估了11种提示方法在4种大型语言模型家族中的表现。通过 UniMS 联合道德安全性评分对伦理性和安全性进行衡量，结果显示简单的范例引导支架方法优于复杂的多阶段推理，且具有更高的联合道德安全性评分和更强的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 目前对大型语言模型的道德能力和安全对齐方面缺乏系统的实证比较，ProMoral-Bench 的提出旨在填补这一空白，通过提供一个统一基准来系统评估不同的提示方法。

Method: 研究团队使用 ProMoral-Bench 评估了11种不同的提示方法，涵盖了四个不同的大型语言模型家族。通过 ETHICS、Scruples、WildJailbreak 和一个新的鲁棒性测试 ETHICS-Contrast 来分析表现，引入了一个新的度量标准 UniMS 来平衡准确性和安全性。

Result: 研究表明，简单的范例引导支架方法在道德能力和安全稳定性方面优于复杂的多阶段推理。这个模型不仅在 UniMS 评分上表现优秀，而且在检测到扰动时仍能保持较高的稳定性和防御力。

Conclusion: ProMoral-Bench 的引入为道德性和安全性提供了标准化的框架，也为成本效益高的提示工程提供了准则。

Abstract: Prompt design significantly impacts the moral competence and safety alignment of large language models (LLMs), yet empirical comparisons remain fragmented across datasets and models.We introduce ProMoral-Bench, a unified benchmark evaluating 11 prompting paradigms across four LLM families. Using ETHICS, Scruples, WildJailbreak, and our new robustness test, ETHICS-Contrast, we measure performance via our proposed Unified Moral Safety Score (UMSS), a metric balancing accuracy and safety. Our results show that compact, exemplar-guided scaffolds outperform complex multi-stage reasoning, providing higher UMSS scores and greater robustness at a lower token cost. While multi-turn reasoning proves fragile under perturbations, few-shot exemplars consistently enhance moral stability and jailbreak resistance. ProMoral-Bench establishes a standardized framework for principled, cost-effective prompt engineering.

</details>


### [217] [Artificial Organisations](https://arxiv.org/abs/2602.13275)
*William Waites*

Main category: cs.AI

TL;DR: 本文介绍了通过机构设计实现多智能体AI系统可靠性的方法，通过Perseverance Composition Engine展示了多层次验证机制，并通过实验证据支持了机构学理论在多智能体AI安全中的应用。


<details>
  <summary>Details</summary>
Motivation: 现有的AI系统对个体对齐的假设不够可靠，机构学模型能提供缓解因个体对齐问题带来的风险的方法。本文旨在通过机构设计来实现在多智能体系统中获得可靠结果的目标，以弥补单个AI系统对齐假设的不足。

Method: 本文通过建立Perseverance Composition Engine，采用了信息隔离和对立审查机制来实现多层次的验证。系统设计了三个模块：制作者、审校者和评判者，通过信息上的不对称性来建立多层次的验证机制。

Result: 在474项组成任务的任务观察中，证实该机构设计可以产生可靠的行为，特别是在面对不可能的任务时，系统能够在无指令和个体激励的情况下达成诚实的拒绝并提出替代方案。

Conclusion: 这些发现表明，通过机构设计能够在多智能体AI系统中实现从不可靠个体组件中获得可靠结果的目标，同时也推动机构理论成为多智能体AI安全的有效框架。

Abstract: Alignment research focuses on making individual AI systems reliable. Human institutions achieve reliable collective behaviour differently: they mitigate the risk posed by misaligned individuals through organisational structure. Multi-agent AI systems should follow this institutional model using compartmentalisation and adversarial review to achieve reliable outcomes through architectural design rather than assuming individual alignment.
  We demonstrate this approach through the Perseverance Composition Engine, a multi-agent system for document composition. The Composer drafts text, the Corroborator verifies factual substantiation with full source access, and the Critic evaluates argumentative quality without access to sources: information asymmetry enforced by system architecture. This creates layered verification: the Corroborator detects unsupported claims, whilst the Critic independently assesses coherence and completeness. Observations from 474 composition tasks (discrete cycles of drafting, verification, and evaluation) exhibit patterns consistent with the institutional hypothesis. When assigned impossible tasks requiring fabricated content, this iteration enabled progression from attempted fabrication toward honest refusal with alternative proposals--behaviour neither instructed nor individually incentivised. These findings motivate controlled investigation of whether architectural enforcement produces reliable outcomes from unreliable components.
  This positions organisational theory as a productive framework for multi-agent AI safety. By implementing verification and evaluation as structural properties enforced through information compartmentalisation, institutional design offers a route to reliable collective behaviour from unreliable individual components.

</details>


### [218] [Accuracy Standards for AI at Work vs. Personal Life: Evidence from an Online Survey](https://arxiv.org/abs/2602.13283)
*Gaston Besanson,Federico Todeschini*

Main category: cs.AI

TL;DR: 本文研究了人们在专业和私人情境下使用AI增强工具时权衡准确性的情况、影响这种权衡的因素以及在AI不可用时用户的应对策略。研究发现，在工作中需要高准确性的比例远高于个人生活中。可用性随使用和经验的增加而变得更严格。当AI工具不可用时，个人生活的干扰更大。


<details>
  <summary>Details</summary>
Motivation: 现代AI系统，尤其是在生成模型方面，能够产生符合用户意图的输出，但这些输出不完全相同。因此，准确性的定义必须是上下文相关且具有一定的容错性。了解人们在不同场景下对准确性的要求以及面对工具不可用的情况时的应对方式，有助于优化AI应用的设计与使用。

Method: 通过一项在线调查收集了300个样本，其中包含170个准确性的项目。利用统计分析方法研究了工作和私人场景下的准确性和容忍度差异，以及用户应用背景对其要求的影响。

Result: 研究发现，在专业环境中，高准确性的需求比例远高于私人环境中；重用和经验模式与工作中的更严格标准相关；当AI工具不可用时，受访者在生活中经历的干扰更多。

Conclusion: 该研究结果为设计适应用户不同场景需求的AI应用提供了依据，同时强调了在AI工具不工作时用户行为变化的重要性。

Abstract: We study how people trade off accuracy when using AI-powered tools in professional versus personal contexts for adoption purposes, the determinants of those trade-offs, and how users cope when AI/apps are unavailable. Because modern AI systems (especially generative models) can produce acceptable but non-identical outputs, we define "accuracy" as context-specific reliability: the degree to which an output aligns with the user's intent within a tolerance threshold that depends on stakes and the cost of correction. In an online survey (N=300), among respondents with both accuracy items (N=170), the share requiring high accuracy (top-box) is 24.1% at work vs. 8.8% in personal life (+15.3 pp; z=6.29, p<0.001). The gap remains large under a broader top-two-box definition (67.0% vs. 32.9%) and on the full 1-5 ordinal scale (mean 3.86 vs. 3.08). Heavy app use and experience patterns correlate with stricter work standards (H2). When tools are unavailable (H3), respondents report more disruption in personal routines than at work (34.1% vs. 15.3%, p<0.01). We keep the main text focused on these substantive results and place test taxonomy and power derivations in a technical appendix.

</details>


### [219] [Mirror: A Multi-Agent System for AI-Assisted Ethics Review](https://arxiv.org/abs/2602.13292)
*Yifan Ding,Yuhui Shi,Zhiyan Li,Zilong Wang,Yifeng Gao,Yajun Yang,Mengjie Yang,Yixiu Liang,Xipeng Qiu,Xuanjing Huang,Xingjun Ma,Yu-Gang Jiang,Guoyu Wang*

Main category: cs.AI

TL;DR: 该研究提出了Mirror框架，它利用先进的AI模型支持伦理审查，提出了能够自动化快速审查和模拟委员会讨论两种模式。


<details>
  <summary>Details</summary>
Motivation: 当前伦理审查系统面临伦理风险在大规模跨学科研究中增加的压力，需要改进支持机制来提高一致性和专业知识。

Method: 研究开发了Mirro框架，其中包括增强型LLM（EthicsLLM）和两种伦理审查模式（快速审查和委员会审查）。

Result: 实验证明，与强大的通用语言模型相比，镜像框架在伦理评估的质量、一致性和专业性方面有显著改进。

Conclusion: Mirror框架为伦理审查提供了新的途径，通过集成伦理推理、结构化的规则解释和多智能体协商，提高了审查效率和质量。

Abstract: Ethics review is a foundational mechanism of modern research governance, yet contemporary systems face increasing strain as ethical risks arise as structural consequences of large-scale, interdisciplinary scientific practice. The demand for consistent and defensible decisions under heterogeneous risk profiles exposes limitations in institutional review capacity rather than in the legitimacy of ethics oversight. Recent advances in large language models (LLMs) offer new opportunities to support ethics review, but their direct application remains limited by insufficient ethical reasoning capability, weak integration with regulatory structures, and strict privacy constraints on authentic review materials. In this work, we introduce Mirror, an agentic framework for AI-assisted ethical review that integrates ethical reasoning, structured rule interpretation, and multi-agent deliberation within a unified architecture. At its core is EthicsLLM, a foundational model fine-tuned on EthicsQA, a specialized dataset of 41K question-chain-of-thought-answer triples distilled from authoritative ethics and regulatory corpora. EthicsLLM provides detailed normative and regulatory understanding, enabling Mirror to operate in two complementary modes. Mirror-ER (expedited Review) automates expedited review through an executable rule base that supports efficient and transparent compliance checks for minimal-risk studies. Mirror-CR (Committee Review) simulates full-board deliberation through coordinated interactions among expert agents, an ethics secretary agent, and a principal investigator agent, producing structured, committee-level assessments across ten ethical dimensions. Empirical evaluations demonstrate that Mirror significantly improves the quality, consistency, and professionalism of ethics assessments compared with strong generalist LLMs.

</details>


### [220] [Detecting Jailbreak Attempts in Clinical Training LLMs Through Automated Linguistic Feature Extraction](https://arxiv.org/abs/2602.13321)
*Tri Nguyen,Huy Hoang Bao Le,Lohith Srikanth Pentapalli,Laurah Turner,Kelly Cohen*

Main category: cs.AI

TL;DR: 本文通过利用专家标注的四个核心语言特征（专业性、医学相关性、伦理行为、情境分心）训练多个通用领域和医疗领域基于BERT的大型语言模型，来预测这些特征，通过多种预测模型评估从提取的特征中预测逃狱行为的可能性。实验证明所提出的系统具有较强的性能，表明语言模型所产生的语言特征为自动检测逃狱提供了有效的基础。通过错误分析发现现有的标注方案和特征表示方式存在一定的不足，提出未来改进方向包括更丰富的标注方案、更精细的特征提取和捕捉对话过程中逃狱行为风险变化的方法。


<details>
  <summary>Details</summary>
Motivation: 本文的研究动机在于解决临床培训大型语言模型中的逃狱检测问题，传统的手工标注方法难以满足可扩展性和表达力的需求，因此研究提出了一种基于特定领域大型语言模型的逃狱预测方法。

Method: 本文的方法包括两个阶段：首先，利用专家对四个核心语言特征进行标注；其次，针对这些特征训练多个通用领域和医疗领域的大型语言模型，然后选择最可靠的特征回归模型作为特征提取器，用于第二层分类器来预测逃狱行为。此外，本文还使用了多种预测模型（树基、线性、概率和集成方法）对这些特征进行评估。

Result: 实验结果显示，本文所提出的系统具有较强的总体性能，表明从大语言模型中提取的语言特征可以有效支持自动化逃狱检测。此外，通过错误分析发现了当前标注方案和特征表示方式的局限性。

Conclusion: 本文的研究结果证明了一种可扩展且易于解释的方法，用于在关键安全临床对话系统中检测逃狱行为。未来的研究方向包括更精细的标注方法、更细致的特征提取以及捕捉对话过程中逃狱行为发展的风险变化的方法。

Abstract: Detecting jailbreak attempts in clinical training large language models (LLMs) requires accurate modeling of linguistic deviations that signal unsafe or off-task user behavior. Prior work on the 2-Sigma clinical simulation platform showed that manually annotated linguistic features could support jailbreak detection. However, reliance on manual annotation limited both scalability and expressiveness. In this study, we extend this framework by using experts' annotations of four core linguistic features (Professionalism, Medical Relevance, Ethical Behavior, and Contextual Distraction) and training multiple general-domain and medical-domain BERT-based LLM models to predict these features directly from text. The most reliable feature regressor for each dimension was selected and used as the feature extractor in a second layer of classifiers. We evaluate a suite of predictive models, including tree-based, linear, probabilistic, and ensemble methods, to determine jailbreak likelihood from the extracted features. Across cross-validation and held-out evaluations, the system achieves strong overall performance, indicating that LLM-derived linguistic features provide an effective basis for automated jailbreak detection. Error analysis further highlights key limitations in current annotations and feature representations, pointing toward future improvements such as richer annotation schemes, finer-grained feature extraction, and methods that capture the evolving risk of jailbreak behavior over the course of a dialogue. This work demonstrates a scalable and interpretable approach for detecting jailbreak behavior in safety-critical clinical dialogue systems.

</details>


### [221] [Nanbeige4.1-3B: A Small General Model that Reasons, Aligns, and Acts](https://arxiv.org/abs/2602.13367)
*Chen Yang,Guangyue Peng,Jiaying Zhu,Ran Le,Ruixiang Feng,Tao Zhang,Xiyun Xu,Yang Song,Yiming Jia,Yuntao Wen,Yunzhi Xu,Zekai Wang,Zhenwei An,Zhicong Sun,Zongchao Chen*

Main category: cs.AI

TL;DR: 该研究提出了一个名为Nanbeige4.1-3B的小型统一通才语言模型，它仅使用3亿参数就实现了强自主行为、代码生成和普遍推理。通过奖励建模、复杂度感知奖励和深度搜索中的复杂数据合成，该模型在执行复杂问题上表现出色，实验结果显示其性能显著优于同等规模的模型，并且在某些方面优于更大规模的模型。


<details>
  <summary>Details</summary>
Motivation: 当前的小型语言模型（SLM）虽然参数较少，但在功能上通常局限于单一任务或很少的任务集合。作者的动机是开发一个小型模型，它可以在同时实现多种能力的情况下保持大规模模型的性能。

Method: 该方法通过结合点奖励和对奖励模式建模，以及设计复杂度感知奖励，利用强化学习优化代码生成的正确性和效率。此外，在数据生成过程中采取复杂的策略，并在训练过程中加入对话级别的监督，以确保长期稳定的工具交互过程。

Result: 实验结果表明，Nanbeige4.1-3B在一系列基准测试中表现出优于此前小型模型（如Nanbeige4-3B-2511和Qwen3-4B）和一些大模型（如Qwen3-30B-A3B）的表现，显示出小型语言模型有能力同时具备广泛的技能和强专长。

Conclusion: 研究强调，3亿参数级别模型仍然有着巨大的潜力，可以同时实现广泛的专业技能和高度的专业化，这是对小型语言模型能力的一个新认识。

Abstract: We present Nanbeige4.1-3B, a unified generalist language model that simultaneously achieves strong agentic behavior, code generation, and general reasoning with only 3B parameters. To the best of our knowledge, it is the first open-source small language model (SLM) to achieve such versatility in a single model. To improve reasoning and preference alignment, we combine point-wise and pair-wise reward modeling, ensuring high-quality, human-aligned responses. For code generation, we design complexity-aware rewards in Reinforcement Learning, optimizing both correctness and efficiency. In deep search, we perform complex data synthesis and incorporate turn-level supervision during training. This enables stable long-horizon tool interactions, allowing Nanbeige4.1-3B to reliably execute up to 600 tool-call turns for complex problem-solving. Extensive experimental results show that Nanbeige4.1-3B significantly outperforms prior models of similar scale, such as Nanbeige4-3B-2511 and Qwen3-4B, even achieving superior performance compared to much larger models, such as Qwen3-30B-A3B. Our results demonstrate that small models can achieve both broad competence and strong specialization simultaneously, redefining the potential of 3B parameter models.

</details>


### [222] [On-Policy Supervised Fine-Tuning for Efficient Reasoning](https://arxiv.org/abs/2602.13407)
*Anhao Zhao,Ziyang Chen,Junlong Tong,Yingqi Fan,Fanghua Ye,Shuhao Li,Yunpu Ma,Wenjie Li,Xiaoyu Shen*

Main category: cs.AI

TL;DR: 通过删除KL正则化和组内标准化，简化奖励机制为基于截断的长度惩罚，将大型推理模型（LRM）的训练简化为对自动生成且符合正确性和简洁性的数据的监督微调。该方法在五个基准测试中超越了更复杂的基于RL的方法，在保持原有准确性的同时减少了最多80%的CoT长度，同时提高了训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有的多奖励目标虽然可以优化准确性和简洁性之间的权衡，但这些复杂的扩展通常会导致不稳定和次优的结果。因此，这篇文章重新审视了这个目标，并质疑了这种复杂性的必要性。

Method: 通过去除KL正则化和组内标准化，简化奖励机制为基于截断的长度惩罚，将优化问题简化为基于自动生成且符合正确性和简洁性的监督微调。

Result: 简化后的训练策略在五个基准测试中超越了更复杂的基于RL的方法，在保持原有准确性的同时减少了最多80%的CoT长度，同时提高了训练效率，减少了50%的GPU内存使用，并加快了收敛速度70%。

Conclusion: 尽管方法相对简单，但该简化策略提供了准确性和效率的最优解，并且已经在实际应用中得到了验证。

Abstract: Large reasoning models (LRMs) are commonly trained with reinforcement learning (RL) to explore long chain-of-thought reasoning, achieving strong performance at high computational cost. Recent methods add multi-reward objectives to jointly optimize correctness and brevity, but these complex extensions often destabilize training and yield suboptimal trade-offs. We revisit this objective and challenge the necessity of such complexity. Through principled analysis, we identify fundamental misalignments in this paradigm: KL regularization loses its intended role when correctness and length are directly verifiable, and group-wise normalization becomes ambiguous under multiple reward signals. By removing these two items and simplifying the reward to a truncation-based length penalty, we show that the optimization problem reduces to supervised fine-tuning on self-generated data filtered for both correctness and conciseness. We term this simplified training strategy on-policy SFT. Despite its simplicity, on-policy SFT consistently defines the accuracy-efficiency Pareto frontier. It reduces CoT length by up to 80 while maintaining original accuracy, surpassing more complex RL-based methods across five benchmarks. Furthermore, it significantly enhances training efficiency, reducing GPU memory usage by 50% and accelerating convergence by 70%. Our code is available at https://github.com/EIT-NLP/On-Policy-SFT.

</details>


### [223] [Translating Dietary Standards into Healthy Meals with Minimal Substitutions](https://arxiv.org/abs/2602.13502)
*Trevor Chan,Ilias Tagkopoulos*

Main category: cs.AI

TL;DR: 该研究提出了一种端到端框架，通过最小改动将饮食标准转化为完整的餐食，使用饮食摄入数据生成营养达标且成本降低的餐食。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在通过改进饮食质量，同时不牺牲便利性和经济性，满足个性化饮食系统的目标。

Method: 研究利用What We Eat in America（WWEIA）的数据，识别34种可解释的餐食原型，通过生成模型和分量预测器来满足美国农业部（USDA）营养目标。通过在原型内部进行比较，生成的餐食推荐日摄入目标的达标率提高了47.0%，同时保持与真实餐食的组成相近。通过对一到三个食物的替换，研究成功创造了10%更加营养的餐食，平均降低了19-32%的成本。

Result: 生成的餐食在遵循推荐日摄入目标和成本降低方面取得了显著效果，Nutrition Department Standards USDA (NDSUS)达标率提高了47.0%，同时成本降低19-32%。通过简单的替换，研究人员提高了餐食的营养价值，同时控制了成本。

Conclusion: 研究提出的方法能够将饮食指南转化为实际、预算敏感的餐食和简单的替换建议，具备支持临床决策支持、公共卫生计划和消费者应用的潜力，从而实现日常营养的广泛且公平的改善。

Abstract: An important goal for personalized diet systems is to improve nutritional quality without compromising convenience or affordability. We present an end-to-end framework that converts dietary standards into complete meals with minimal change. Using the What We Eat in America (WWEIA) intake data for 135,491 meals, we identify 34 interpretable meal archetypes that we then use to condition a generative model and a portion predictor to meet USDA nutritional targets. In comparisons within archetypes, generated meals are better at following recommended daily intake (RDI) targets by 47.0%, while remaining compositionally close to real meals. Our results show that by allowing one to three food substitutions, we were able to create meals that were 10% more nutritious, while reducing costs 19-32%, on average. By turning dietary guidelines into realistic, budget-aware meals and simple swaps, this framework can underpin clinical decision support, public-health programs, and consumer apps that deliver scalable, equitable improvements in everyday nutrition.

</details>


### [224] [Who Do LLMs Trust? Human Experts Matter More Than Other LLMs](https://arxiv.org/abs/2602.13568)
*Anooshka Bajaj,Zoran Tiganj*

Main category: cs.AI

TL;DR: 本研究通过让大语言模型处理不同来源的反馈信息，发现模型更倾向于接受人类专家的反馈，即使该反馈可能是错误的。这表明大语言模型在不同决策领域中表现出一种信誉敏感的社会影响形式。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探讨大语言模型在面对社会信息（如人类专家和其他模型的反馈）时，是否像人类一样表现出信誉敏感的社会影响，并进一步探讨模型是否偏好从人类而非其他模型接收反馈。

Method: 研究采用了一种多任务设置，包括阅读理解和多步推理等任务，并使用了四种指令调整的大语言模型。模型接收的反馈信息可以来自于朋友、人类专家或其他模型。同时，研究通过改变正确与否的小组以及成员数量来操纵情境。在第二个实验中，研究引入了人类与模型直接产生分歧的情况。

Result: 研究结果表明，大语言模型更倾向于遵循标记为人类专家的反馈，无论该反馈是否正确。模型更多地调整自身答案以符合人类专家的表述，而不是其他模型的反馈。这表明在不同决策领域，专家框架是一名强大的先验信息。

Conclusion: 本研究表明，大语言模型在各种决策任务中表现出信誉敏感的社会影响形态，尽管它们的表现方式和具体决策还依赖于被提供的信息类型和环境设置。

Abstract: Large language models (LLMs) increasingly operate in environments where they encounter social information such as other agents' answers, tool outputs, or human recommendations. In humans, such inputs influence judgments in ways that depend on the source's credibility and the strength of consensus. This paper investigates whether LLMs exhibit analogous patterns of influence and whether they privilege feedback from humans over feedback from other LLMs. Across three binary decision-making tasks, reading comprehension, multi-step reasoning, and moral judgment, we present four instruction-tuned LLMs with prior responses attributed either to friends, to human experts, or to other LLMs. We manipulate whether the group is correct and vary the group size. In a second experiment, we introduce direct disagreement between a single human and a single LLM. Across tasks, models conform significantly more to responses labeled as coming from human experts, including when that signal is incorrect, and revise their answers toward experts more readily than toward other LLMs. These results reveal that expert framing acts as a strong prior for contemporary LLMs, suggesting a form of credibility-sensitive social influence that generalizes across decision domains.

</details>


### [225] [Differentiable Rule Induction from Raw Sequence Inputs](https://arxiv.org/abs/2602.13583)
*Kun Gao,Katsumi Inoue,Yongzhi Cao,Hanpin Wang,Feng Yang*

Main category: cs.AI

TL;DR: 该研究旨在解决直接从原始数据中学习规则时面临的标签泄漏问题，通过结合自监督可微聚类模型和新型可微ILP模型，实现了无需显式标签监督的规则学习。


<details>
  <summary>Details</summary>
Motivation: 现有的不同iable ILP 方法主要依赖于符号数据集，当应用于直接学习原始数据时，难以避免标签泄漏的问题，即无法直接将连续输入映射到符号变量，要求显式监督输入特征标签。为解决这一限制，该研究提出了一种结合自监督可微聚类模型的新颖可微ILP模型，从而克服了学习原始数据时面临的挑战。

Method: 该方法通过引入自监督可微聚类模型与新型可微ILP模型的结合，提出了一种不同的架构，旨在从原始数据中学习规则，而不依赖于显式的标签监督。这种方法利用聚类模型对数据进行初步特征表示，然后使用ILP模型从这些表示中学习规则。

Result: 实验结果表明，该方法能够从时间和图像数据中学习到直观且精确的泛化规则。与现有方法相比，该模型展示了更好的鲁棒性和扩展性。

Conclusion: 研究表明，结合自监督可微聚类模型与ILP模型的方法可以有效地从原始数据中学习到描述性的规则，无需显式标签监督，为直接处理复杂、未标记的实际应用场景提供了新的解决方案。

Abstract: Rule learning-based models are widely used in highly interpretable scenarios due to their transparent structures. Inductive logic programming (ILP), a form of machine learning, induces rules from facts while maintaining interpretability. Differentiable ILP models enhance this process by leveraging neural networks to improve robustness and scalability. However, most differentiable ILP methods rely on symbolic datasets, facing challenges when learning directly from raw data. Specifically, they struggle with explicit label leakage: The inability to map continuous inputs to symbolic variables without explicit supervision of input feature labels. In this work, we address this issue by integrating a self-supervised differentiable clustering model with a novel differentiable ILP model, enabling rule learning from raw data without explicit label leakage. The learned rules effectively describe raw data through its features. We demonstrate that our method intuitively and precisely learns generalized rules from time series and image data.

</details>


### [226] [PhGPO: Pheromone-Guided Policy Optimization for Long-Horizon Tool Planning](https://arxiv.org/abs/2602.13691)
*Yu Li,Guangfeng Cai,Shengtian Yang,Han Luo,Shuo Han,Xu He,Dong Li,Lei Feng*

Main category: cs.AI

TL;DR: 本文提出了一种利用历史成功路径来指导策略优化的方法，通过学习这些路径中的模式并应用到未来的策略提升中，有效改善了长期复杂工具规划任务的表现。


<details>
  <summary>Details</summary>
Motivation: 通过历史成功路径学习可重用的工具转换模式，以解决大型语言模型在执行复杂任务时面临的长期多步骤工具规划挑战。

Method: 采用了类蚂蚁 colony 优化的方法，通过学习历史轨迹中的模式（即‘持久性’），并将其应用于指导策略优化。

Result: 实验证明了所提出的 PhGPO 方法在长期工具规划任务中的有效性。

Conclusion: 该研究提出了一个新的策略优化方法 PhGPO，通过历史成功的工具路径指导未来策略优化，从而提高了长期复杂任务的执行效果。

Abstract: Recent advancements in Large Language Model (LLM) agents have demonstrated strong capabilities in executing complex tasks through tool use. However, long-horizon multi-step tool planning is challenging, because the exploration space suffers from a combinatorial explosion. In this scenario, even when a correct tool-use path is found, it is usually considered an immediate reward for current training, which would not provide any reusable information for subsequent training. In this paper, we argue that historically successful trajectories contain reusable tool-transition patterns, which can be leveraged throughout the whole training process. Inspired by ant colony optimization where historically successful paths can be reflected by the pheromone, we propose Pheromone-Guided Policy Optimization (PhGPO), which learns a trajectory-based transition pattern (i.e., pheromone) from historical trajectories and then uses the learned pheromone to guide policy optimization. This learned pheromone provides explicit and reusable guidance that steers policy optimization toward historically successful tool transitions, thereby improving long-horizon tool planning. Comprehensive experimental results demonstrate the effectiveness of our proposed PhGPO.

</details>


### [227] [Can a Lightweight Automated AI Pipeline Solve Research-Level Mathematical Problems?](https://arxiv.org/abs/2602.13695)
*Lve Meng,Weilong Zhao,Yanzhi Zhang,Haoxiang Guan,Jiyan He*

Main category: cs.AI

TL;DR: 本文展示了新一代语言模型（如Gemini 3 Pro和GPT-5.2 Pro）在借鉴自引验证优化流水线中的应用，成功解决了一系列高阶研究问题。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型（LLMs）在生成严谨的数学证明方面取得了显著成果，但将这些模型通过轻量级、自然语言管道应用于研究问题的部署仍需进一步探索。

Method: 研究采用了一种针对引用验证优化的自动化流水线，并在两个新型数据集——ICCM问题集和“First Proof”问题集上进行了评估。

Result: 该流水线成功生成了ICCM前两个问题集和“First Proof”集所有问题的候选证明，并已全部由团队验证完成。部分结果已提交相关机构并公开。

Conclusion: 研究团队计划在未来开放整个流水线方法论以供参考和使用。

Abstract: Large language models (LLMs) have recently achieved remarkable success in generating rigorous mathematical proofs, with "AI for Math" emerging as a vibrant field of research. While these models have mastered competition-level benchmarks like the International Mathematical Olympiad and show promise in research applications through auto-formalization, their deployment via lightweight, natural-language pipelines for research problems remains underexplored. In this work, we demonstrate that next-generation models (e.g., Gemini 3 Pro, GPT-5.2 Pro), when integrated into a streamlined automated pipeline optimized for citation-based verification, can solve sophisticated research-grade problems. We evaluate our pipeline on two novel datasets: (1) the ICCM problem sets (comparable to the S.-T. Yau College Student Mathematics Contest) proposed by leading mathematicians, and (2) the "First Proof" problem set, consisting of previously unpublished research questions. Our pipeline generated candidate proofs for all problems in the first two ICCM sets and the "First Proof" set. The solutions for the first two ICCM sets and Problem 4 of the "First Proof" set have been fully verified by our team. All generated proofs have been submitted to the official organization, and our generated results are publicly available. We plan to open-source the complete pipeline methodology in due course.

</details>


### [228] [No Need to Train Your RDB Foundation Model](https://arxiv.org/abs/2602.13697)
*Linjie Xu,Yanlin Zhang,Quan Gan,Minjie Wang,David Wipf*

Main category: cs.AI

TL;DR: 该研究开发了一种基于在上下文学习(Foundation Models based on In-Context Learning, ICL)的方法，通过在高维关系数据库(Relational Databases, RDB)列内进行压缩，而不在跨列中进行压缩，从而避免了重新训练模型，实现在无需训练或微调的情况下应用到多表场景。


<details>
  <summary>Details</summary>
Motivation: 为了在企业环境中利用关系数据库中的大量异构表格信息进行预测，同时避免每次预测新量度时都需要重新训练新的模型。

Method: 研究提出了在高维关系数据库列内进行特定压缩的方法，以适配基于在上下文学习的Foundation Models。通过利用SQL原始语句，实现了高效编码阶段的开发。

Result: 研究表明，这种基于压缩的方法能够实现强大的编码器，而不会影响其表示能力。编码器可以无缝对接现有的单一表格Foundation Models，无需重新训练或微调。此外，开发了易于使用的开源RDB Foundation Model，适用于未见过的数据集。

Conclusion: 本研究提出了一种新的方法，即在高维关系数据库列内压缩特定数据，使得基于在上下文学习的Foundation Models能够在多表场景下应用，同时通过理论和实验证明了此方法的有效性。

Abstract: Relational databases (RDBs) contain vast amounts of heterogeneous tabular information that can be exploited for predictive modeling purposes. But since the space of potential targets is vast across enterprise settings, how can we \textit{avoid retraining} a new model each time we wish to predict a new quantity of interest? Foundation models based on in-context learning (ICL) offer a convenient option, but so far are largely restricted to single-table operability. In generalizing to multiple interrelated tables, it is essential to compress variably-sized RDB neighborhoods into fixed-length ICL samples for consumption by the decoder. However, the details here are critical: unlike existing supervised learning RDB pipelines, we provide theoretical and empirical evidence that ICL-specific compression should be constrained \emph{within} high-dimensional RDB columns where all entities share units and roles, not \textit{across} columns where the relevance of heterogeneous data types cannot possibly be determined without label information. Conditioned on this restriction, we then demonstrate that encoder expressiveness is actually not compromised by excluding trainable parameters. Hence we arrive at a principled family of RDB encoders that can be seamlessly paired with already-existing single-table ICL foundation models, whereby no training or fine-tuning is required. From a practical standpoint, we develop scalable SQL primitives to implement the encoder stage, resulting in an easy-to-use open-source RDB foundation model\footnote{\label{foot: RDBLearn_learn} https://github.com/HKUSHXLab/rdblearn} capable of robust performance on unseen datasets out of the box.

</details>


### [229] [OneLatent: Single-Token Compression for Visual Latent Reasoning](https://arxiv.org/abs/2602.13738)
*Bo Lv,Yasheng Sun,Junjie Wang,Haoxiang Shi*

Main category: cs.AI

TL;DR: OneLatent框架通过监督从渲染的CoT图像和DeepSeek-OCR隐藏状态中，将中间推理压缩至单一潜伏token，实现了输出长度大幅减少，同时保持较高准确度，并在长链逻辑推理上取得了卓越表现。


<details>
  <summary>Details</summary>
Motivation: 解决链式思考提示虽然提高了推理能力，但经常大幅增加推理成本的问题，提出OneLatent框架来压缩推理过程，减少输出长度并提高效率。

Method: OneLatent方法通过渲染文本步骤为图像，利用其监督信号，将复杂推理过程压缩为单一潜伏token，方法包括隐状态监督学习和图像渲染技术。

Result: OneLatent框架在多个基准测试中显著减少了平均输出长度（减少11倍），仅微小降低（2.21%）准确率，同时提升输出tokens贡献（6.8倍），特别是在长时间链逻辑推理中，OneLatent达到了99.80%和97.80%的高准确率，压缩比高达87.4倍。

Conclusion: OneLatent是一种有效的推理压缩方法，能显著降低成本同时保持高效和准确性，特别适用于需要压缩推理的场景。

Abstract: Chain-of-thought (CoT) prompting improves reasoning but often increases inference cost by one to two orders of magnitude. To address these challenges, we present \textbf{OneLatent}, a framework that compresses intermediate reasoning into a single latent token via supervision from rendered CoT images and DeepSeek-OCR hidden states. By rendering textual steps into images, we obtain a deterministic supervision signal that can be inspected and audited without requiring the model to output verbose textual rationales. Across benchmarks, OneLatent reduces average output length by $11\times$ with only a $2.21\%$ average accuracy drop relative to textual CoT, while improving output token contribution (OTC) by $6.8\times$. On long-chain logical reasoning, OneLatent reaches $99.80\%$ on ProntoQA and $97.80\%$ on ProsQA with one latent token, with compression up to $87.4\times$, supporting compression-constrained generalization.

</details>


### [230] [StackingNet: Collective Inference Across Independent AI Foundation Models](https://arxiv.org/abs/2602.13792)
*Siyang Li,Chenhao Liu,Dongrui Wu,Zhigang Zeng,Lieyun Ding*

Main category: cs.AI

TL;DR: 文章提出了一种名为StackingNet的元集成框架，通过集体智能原则在推理时结合多个模型的预测，提高了准确性和鲁棒性，降低了偏差，能够识别并剔除表现不佳的模型。


<details>
  <summary>Details</summary>
Motivation: 当前基于大型基础模型的人工智能系统依旧独立工作，缺乏有效的协作方式。为了构建可信的智能系统，需要一种新的方法来协调独立的基础模型。

Method: StackingNet框架利用集体智能原则，在推理过程中结合多个模型的预测，无需访问内部参数或训练数据。

Result: 与单独模型和经典集成相比，StackingNet在语言理解、视觉估计和学术论文评级任务中，提高了准确率、稳健性和公平性。

Conclusion: StackingNet框架为协调人工智能系统的合作提供了一种实用的基础，表明在多个专门化的模型之间进行有原则的合作也可能产生进展。

Abstract: Artificial intelligence built on large foundation models has transformed language understanding, vision and reasoning, yet these systems remain isolated and cannot readily share their capabilities. Integrating the complementary strengths of such independent foundation models is essential for building trustworthy intelligent systems. Despite rapid progress in individual model design, there is no established approach for coordinating such black-box heterogeneous models. Here we show that coordination can be achieved through a meta-ensemble framework termed StackingNet, which draws on principles of collective intelligence to combine model predictions during inference. StackingNet improves accuracy, reduces bias, enables reliability ranking, and identifies or prunes models that degrade performance, all operating without access to internal parameters or training data. Across tasks involving language comprehension, visual estimation, and academic paper rating, StackingNet consistently improves accuracy, robustness, and fairness, compared with individual models and classic ensembles. By turning diversity from a source of inconsistency into collaboration, StackingNet establishes a practical foundation for coordinated artificial intelligence, suggesting that progress may emerge from not only larger single models but also principled cooperation among many specialized ones.

</details>


### [231] [Attention in Constant Time: Vashista Sparse Attention for Long-Context Decoding with Exponential Guarantees](https://arxiv.org/abs/2602.13804)
*Vashista Nobaub*

Main category: cs.AI

TL;DR: 该研究通过分析注意力机制在长上下文中的表现，提出了Vashista稀疏注意力机制。它能够在保持模型准确性的前提下，减少计算成本。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型在推理成本中主要消耗在对长上下文的注意力计算上，但实际行为表明大多数查询中只有少量的词贡献显著。

Method: 通过将注意力建模为键向量凸包上的投影，并分析其熵对数（softmax类似）的弛豫，证明了在严格的共轭余量边际（支持间隙 Δ，由 KKT 乘子认证）下，熵注意力集中在固定大小的活动面上，活跃面外的总质量呈指数下降，误差与温度/正则化参数成线性关系。引入了基于页面选择策略的Vashista稀疏注意力机制。

Result: 实验证实在长上下文评估中，显示出稳定的小规模有效支撑面，显著的时钟速度提升以及在预测试验预测的质量衰减较小。

Conclusion: 研究提供了准确性和计算成本之间的权衡机制，适用于隐私敏感和离线环境，可实现可预测的延迟和成本，而无需外部检索依赖。

Abstract: Large language models spend most of their inference cost on attention over long contexts, yet empirical behavior suggests that only a small subset of tokens meaningfully contributes to each query. We formalize this phenomenon by modeling attention as a projection onto the convex hull of key vectors and analyzing its entropic (softmax-like) relaxation. Our main theoretical contribution is a face-stability theorem showing that, under a strict complementarity margin (a support gap (Δ) certified by KKT multipliers), entropic attention concentrates on a constant-size active face: the total mass assigned to inactive tokens decays exponentially as (\exp(-Ω(Δ/\varepsilon))), while the error on the active face scales linearly in the temperature/regularization parameter (\varepsilon). This yields a practical criterion for when sparse long-context decoding is safe and provides a principled knob to trade accuracy for compute.
  Building on these guarantees, we introduce Vashista Sparse Attention, a drop-in mechanism that maintains a small candidate set per query through a paging-style context selection strategy compatible with modern inference stacks. Across long-context evaluations, we observe stable constant-size effective support, strong wall-clock speedups, and minimal quality degradation in the regimes predicted by the support-gap diagnostics. Finally, we discuss deployment implications for privacy-sensitive and air-gapped settings, where interchangeable attention modules enable predictable latency and cost without external retrieval dependencies.

</details>


### [232] [Experimentation Accelerator: Interpretable Insights and Creative Recommendations for A/B Testing with Content-Aware ranking](https://arxiv.org/abs/2602.13852)
*Zhengmian Hu,Lei Shi,Ritwik Sinha,Justin Grover,David Arbour*

Main category: cs.AI

TL;DR: 该研究提出了一种统一框架，用于优先选择测试变体、解释获胜原因以及发现潜在更高价值的变体机会。通过利用响应治疗嵌入和历史结果，该框架训练了一种CTR排名模型来评估候选方案，考虑了价值和内容多样性。此外，使用语义营销属性对治疗进行投影，并通过稀疏约束Lasso重新表达排名器，生成可视化解释、驱动因素和自然语言洞察。最终通过机会指数确定缺失但高影响的属性，并利用LLM生成创意建议，以加速实验。


<details>
  <summary>Details</summary>
Motivation: 解决现代在线实验中的瓶颈问题，如稀缺流量和事后洞察的提取不一致且往往内容无关

Method: 提出了一个结合CTR排名模型、稀疏约束Lasso和自然语言生成的统一框架，通过治疗嵌入和历史结果进行候选变体优先级排序，并利用语义营销属性增强模型可解释性。

Result: 该框架成功地提高了实验效率，生成了有解释性的洞察，并能够快速生成创意建议。实验证明该框架在真实世界实验中有较高的性能表现。

Conclusion: 通过将该框架集成到Adobe的实验加速器产品中，公司能够为客户提供AI驱动的洞察和机会，从而扩展实验规模。

Abstract: Modern online experimentation faces two bottlenecks: scarce traffic forces tough choices on which variants to test, and post-hoc insight extraction is manual, inconsistent, and often content-agnostic. Meanwhile, organizations underuse historical A/B results and rich content embeddings that could guide prioritization and creative iteration. We present a unified framework to (i) prioritize which variants to test, (ii) explain why winners win, and (iii) surface targeted opportunities for new, higher-potential variants. Leveraging treatment embeddings and historical outcomes, we train a CTR ranking model with fixed effects for contextual shifts that scores candidates while balancing value and content diversity. For better interpretability and understanding, we project treatments onto curated semantic marketing attributes and re-express the ranker in this space via a sign-consistent, sparse constrained Lasso, yielding per-attribute coefficients and signed contributions for visual explanations, top-k drivers, and natural-language insights. We then compute an opportunity index combining attribute importance (from the ranker) with under-expression in the current experiment to flag missing, high-impact attributes. Finally, LLMs translate ranked opportunities into concrete creative suggestions and estimate both learning and conversion potential, enabling faster, more informative, and more efficient test cycles. These components have been built into a real Adobe product, called \textit{Experimentation Accelerator}, to provide AI-based insights and opportunities to scale experimentation for customers. We provide an evaluation of the performance of the proposed framework on some real-world experiments by Adobe business customers that validate the high quality of the generation pipeline.

</details>


### [233] [Enabling Option Learning in Sparse Rewards with Hindsight Experience Replay](https://arxiv.org/abs/2602.13865)
*Gabriel Romio,Mateus Begnini Melchiades,Bruno Castro da Silva,Gabriel de Oliveira Ramos*

Main category: cs.AI

TL;DR: 该研究提出了一种新的策略MOC-2HER，该策略结合了HER机制与新的双重目标，显著提高了多目标环境下稀疏奖励任务中的机器人操作成功率。


<details>
  <summary>Details</summary>
Motivation: 在多目标环境下的稀疏奖励任务中，传统方法如MOC和MOC-HER难以有效连接远期奖励和当前行为。MOC-2HER通过引入双重目标机制来解决这一问题。

Method: 研究通过在MOC框架上集成HER机制，并进一步提出双重目标HER（2HER），即在最终状态和执行器位置的基础上生成虚拟目标，从而在操作过程中同时奖励对象的接触和任务的成功完成。

Result: 实验结果显示，MOC-2HER在机器人操作环境中的成功率高达90%，大大超越了MOC和MOC-HER的方法，表明双重目标策略在处理稀疏奖励和多目标任务时的有效性。

Conclusion: 该研究展示了双重目标策略在稀疏奖励多目标环境中的有效性，并为提高强化学习的智能体在实际任务中的表现提供了新的思路。

Abstract: Hierarchical Reinforcement Learning (HRL) frameworks like Option-Critic (OC) and Multi-updates Option Critic (MOC) have introduced significant advancements in learning reusable options. However, these methods underperform in multi-goal environments with sparse rewards, where actions must be linked to temporally distant outcomes. To address this limitation, we first propose MOC-HER, which integrates the Hindsight Experience Replay (HER) mechanism into the MOC framework. By relabeling goals from achieved outcomes, MOC-HER can solve sparse reward environments that are intractable for the original MOC. However, this approach is insufficient for object manipulation tasks, where the reward depends on the object reaching the goal rather than on the agent's direct interaction. This makes it extremely difficult for HRL agents to discover how to interact with these objects. To overcome this issue, we introduce Dual Objectives Hindsight Experience Replay (2HER), a novel extension that creates two sets of virtual goals. In addition to relabeling goals based on the object's final state (standard HER), 2HER also generates goals from the agent's effector positions, rewarding the agent for both interacting with the object and completing the task. Experimental results in robotic manipulation environments show that MOC-2HER achieves success rates of up to 90%, compared to less than 11% for both MOC and MOC-HER. These results highlight the effectiveness of our dual objective relabeling strategy in sparse reward, multi-goal tasks.

</details>


### [234] [Ambient Physics: Training Neural PDE Solvers with Partial Observations](https://arxiv.org/abs/2602.13873)
*Harris Abdul Majid,Giannis Daras,Francesco Tudisco,Steven McDonagh*

Main category: cs.AI

TL;DR: Ambient Physics是一种无需完整观测数据即可直接从部分观测数据学习系数-解对的分布的框架，实现了最先进的重建性能。


<details>
  <summary>Details</summary>
Motivation: 在许多科学场景中，获取PDE系数和解的完整观测可能代价高昂、危险或不可能。现有的扩散方法需要完整的观测数据进行训练。

Method: Ambient Physics的关键在于随机遮蔽已观察到的一部分测量数据，并监督这些数据，使得模型无法区分“真正未观察到”与“人为未观察到”，从而必须在所有地方生成可接受的预测。

Result: Ambient Physics相比之前的扩散方法，在平均整体误差上减少了62.51%，同时只需进行更少的功能评估（125倍少）。此外，还发现了一种“单一点转换”，即遮蔽一个已观察的点可以在不同架构和测量模式下从部分观测进行学习。

Conclusion: Ambient Physics框架使得在没有完整观测数据的情况下也能进行科学进步成为可能。

Abstract: In many scientific settings, acquiring complete observations of PDE coefficients and solutions can be expensive, hazardous, or impossible. Recent diffusion-based methods can reconstruct fields given partial observations, but require complete observations for training. We introduce Ambient Physics, a framework for learning the joint distribution of coefficient-solution pairs directly from partial observations, without requiring a single complete observation. The key idea is to randomly mask a subset of already-observed measurements and supervise on them, so the model cannot distinguish "truly unobserved" from "artificially unobserved", and must produce plausible predictions everywhere. Ambient Physics achieves state-of-the-art reconstruction performance. Compared with prior diffusion-based methods, it achieves a 62.51$\%$ reduction in average overall error while using 125$\times$ fewer function evaluations. We also identify a "one-point transition": masking a single already-observed point enables learning from partial observations across architectures and measurement patterns. Ambient Physics thus enables scientific progress in settings where complete observations are unavailable.

</details>


### [235] [VSAL: A Vision Solver with Adaptive Layouts for Graph Property Detection](https://arxiv.org/abs/2602.13880)
*Jiahao Xie,Guangmo Tong*

Main category: cs.AI

TL;DR: VSAL 提出了一种结合自适应布局生成器的视觉框架，以生成信息丰富的图形可视化，从而提高了图形属性检测的效果。相比现有方法，VSAL 在多个任务上表现更好。


<details>
  <summary>Details</summary>
Motivation: 现有的基于视觉的方法依赖于固定图形布局，限制了其表现力。VSAL 解决了这一问题，通过引入自适应布局生成器来动态地生成与实例相关的、更丰富有意义的图形可视化。

Method: VSAL 使用自适应布局生成器根据具体的图形实例生成有益的图形可视化，利用视觉处理技术进行图形属性检测。

Result: VSAL 在多个图形属性检测任务（如哈密尔顿回路、平面图、爪自由图和树检测）中表现出色，优于现有的基于视觉的方法。

Conclusion: VSAL 通过利用自适应布局生成器和视觉处理技术，显著提高了图形属性检测的准确性和效率。

Abstract: Graph property detection aims to determine whether a graph exhibits certain structural properties, such as being Hamiltonian. Recently, learning-based approaches have shown great promise by leveraging data-driven models to detect graph properties efficiently. In particular, vision-based methods offer a visually intuitive solution by processing the visualizations of graphs. However, existing vision-based methods rely on fixed visual graph layouts, and therefore, the expressiveness of their pipeline is restricted. To overcome this limitation, we propose VSAL, a vision-based framework that incorporates an adaptive layout generator capable of dynamically producing informative graph visualizations tailored to individual instances, thereby improving graph property detection. Extensive experiments demonstrate that VSAL outperforms state-of-the-art vision-based methods on various tasks such as Hamiltonian cycle, planarity, claw-freeness, and tree detection.

</details>


### [236] [Diagnosing Pathological Chain-of-Thought in Reasoning Models](https://arxiv.org/abs/2602.13904)
*Manqing Liu,David Williams-King,Ida Caspary,Linh Le,Hannes Whittingham,Puria Radmard,Cameron Tice,Edward James Young*

Main category: cs.AI

TL;DR: 研究提出了一种评估CoT路径缺陷的实用工具，通过创建简单且计算成本低的任务无关指标，并开发特定于路径缺陷的模型。


<details>
  <summary>Details</summary>
Motivation: 由于CoT推理可能表现出影响其实用性的失败模式，因此需要一种新的方法来更好地理解和区分这些路径缺陷，进而改善训练时的监控。

Method: 该研究通过创建简单且计算成本低的任务无关指标来评估CoT路径缺陷，并开发了特定于路径缺陷的模型。

Result: 研究表明，提出的简单指标能够有效区分不同的CoT路径缺陷，并且所开发的模型有助于进一步理解这些路径缺陷。

Conclusion: 这项工作提供了一种评估CoT路径缺陷的实用工具，这对培训过程中监测这些路径缺陷具有直接影响。

Abstract: Chain-of-thought (CoT) reasoning is fundamental to modern LLM architectures and represents a critical intervention point for AI safety. However, CoT reasoning may exhibit failure modes that we note as pathologies, which prevent it from being useful for monitoring. Prior work has identified three distinct pathologies: post-hoc rationalization, where models generate plausible explanations backwards from predetermined answers; encoded reasoning, where intermediate steps conceal information within seemingly interpretable text; and internalized reasoning, where models replace explicit reasoning with meaningless filler tokens while computing internally. To better understand and discriminate between these pathologies, we create a set of concrete metrics that are simple to implement, computationally inexpensive, and task-agnostic. To validate our approach, we develop model organisms deliberately trained to exhibit specific CoT pathologies. Our work provides a practical toolkit for assessing CoT pathologies, with direct implications for training-time monitoring.

</details>


### [237] [From Pixels to Policies: Reinforcing Spatial Reasoning in Language Models for Content-Aware Layout Design](https://arxiv.org/abs/2602.13912)
*Sha Li,Stefano Petrangeli,Yu Shen,Xiang Chen*

Main category: cs.AI

TL;DR: LaySPA是一个强化学习框架，为大型语言模型（LLMs）提供明确的空间推理能力，用于内容感知的图形布局设计。它通过多目标空间批评优化布局设计策略，生成可解释的推理痕迹和结构化布局规范，从而实现透明且可控的设计决策。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型在空间推理方面能力有限，且设计决策缺乏透明度。LaySPA旨在改进这个问题，通过重新定义布局设计为策略学习问题，使LLMs能在结构化文本空间环境中进行学习。

Method: LaySPA将布局设计问题转化为策略学习问题，利用结构化文本空间环境明确编码画布几何、元素属性和元素间关系。方法还包括优化策略的多目标空间批评，分解布局质量为几何有效性、关系连贯性和审美一致性。

Result: 实验显示，LaySPA在结构有效性和视觉质量上表现出色，超越了一些大型专有LLMs，并达到了比专门的SOTA布局生成器相当的性能，同时需要更少的标注样本和较低的延迟。

Conclusion: LaySPA证明了通过强化学习可以使大型语言模型具备明确的空间推理能力，能够进行内容感知的图形布局设计，这一框架具有显著的性能优势和应用潜力。

Abstract: We introduce LaySPA, a reinforcement learning framework that equips large language models (LLMs) with explicit and interpretable spatial reasoning for content-aware graphic layout design. LaySPA addresses two key challenges: LLMs' limited spatial reasoning and the lack of opacity in design decision making. Instead of operating at the pixel level, we reformulate layout design as a policy learning problem over a structured textual spatial environment that explicitly encodes canvas geometry, element attributes, and inter-element relationships. LaySPA produces dual-level outputs comprising interpretable reasoning traces and structured layout specifications, enabling transparent and controllable design decision making. Layout design policy is optimized via a multi-objective spatial critique that decomposes layout quality into geometric validity, relational coherence, and aesthetic consistency, and is trained using relative group optimization to stabilize learning in open-ended design spaces. Experiments demonstrate that LaySPA improves structural validity and visual quality, outperforming larger proprietary LLMs and achieving performance comparable to specialized SOTA layout generators while requiring fewer annotated samples and reduced latency.

</details>


### [238] [HyMem: Hybrid Memory Architecture with Dynamic Retrieval Scheduling](https://arxiv.org/abs/2602.13933)
*Xiaochen Zhao,Kaikai Wang,Xiaowen Zhang,Chen Yao,Aili Wang*

Main category: cs.AI

TL;DR: HyMem 提出了一种新的混合记忆架构，通过多粒度的记忆表示实现按需动态调度，优化了长期记忆管理中的效率与性能。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLM）在短文本上表现出色，但在长期对话中由于记忆管理效率低下而表现不佳。HyMem 旨在解决这一问题，通过引入基于认知经济原理的混合记忆架构，优化长期记忆的存储和检索。

Method: HyMem 设计了一个多粒度的存储方案，结合了轻量级的模块用于快速生成简洁的上下文摘要，并利用深度学习模型在需要时处理复杂查询。此外，该系统融入了反思机制以逐步改进推理过程。

Result: 实验表明，HyMem 在 LOCOMO 和 LongMemEval 测试基准上表现优异，相较于全上下文的记忆管理策略，计算成本降低了92.6%。

Conclusion: HyMem 成功地在长期记忆管理中平衡了效率和性能，展现出强大的性能和低的计算开销，是记忆管理领域的一个重要进展。

Abstract: Large language model (LLM) agents demonstrate strong performance in short-text contexts but often underperform in extended dialogues due to inefficient memory management. Existing approaches face a fundamental trade-off between efficiency and effectiveness: memory compression risks losing critical details required for complex reasoning, while retaining raw text introduces unnecessary computational overhead for simple queries. The crux lies in the limitations of monolithic memory representations and static retrieval mechanisms, which fail to emulate the flexible and proactive memory scheduling capabilities observed in humans, thus struggling to adapt to diverse problem scenarios. Inspired by the principle of cognitive economy, we propose HyMem, a hybrid memory architecture that enables dynamic on-demand scheduling through multi-granular memory representations. HyMem adopts a dual-granular storage scheme paired with a dynamic two-tier retrieval system: a lightweight module constructs summary-level context for efficient response generation, while an LLM-based deep module is selectively activated only for complex queries, augmented by a reflection mechanism for iterative reasoning refinement. Experiments show that HyMem achieves strong performance on both the LOCOMO and LongMemEval benchmarks, outperforming full-context while reducing computational cost by 92.6\%, establishing a state-of-the-art balance between efficiency and performance in long-term memory management.

</details>


### [239] [Statistical Early Stopping for Reasoning Models](https://arxiv.org/abs/2602.13935)
*Yangxinyu Xie,Tao Wang,Soham Mallick,Yan Sun,Georgy Noarov,Mengxin Yu,Tanwi Mallick,Weijie J. Su,Edgar Dobriban*

Main category: cs.AI

TL;DR: 该研究引入了基于统计原则的早期停止方法，通过监控生成过程中的不确定性信号来减少LLM的过度推理，特别适用于处理含糊不清或病态问题。


<details>
  <summary>Details</summary>
Motivation: 鉴于LLM在处理不确定性或模糊查询时可能产生过度推理的问题，研究提出了一种早期停止策略以提高生成过程的效率和可靠性。

Method: 研究设计了参数化和非参数化两种早期停止方法。参数化方法假设不确定性关键词的到达时间遵循某种类型的更新过程，并应用顺序测试进行停止决策；非参数化方法提供了在有限样本下提前停止的概率保证。

Result: 通过在多个领域和模型上的推理任务中进行实证研究，结果表明，基于不确定性感知的早期停止方法能够显著提升LLM推理过程的效率与可靠性，特别是在数学推理任务中效果尤其明显。

Conclusion: 研究表明，引入不确定性早期停止技术可以有效地减少LLM的过度推理，从而提高生成质量，并在未来具有广泛的应用潜力。

Abstract: While LLMs have seen substantial improvement in reasoning capabilities, they also sometimes overthink, generating unnecessary reasoning steps, particularly under uncertainty, given ill-posed or ambiguous queries. We introduce statistically principled early stopping methods that monitor uncertainty signals during generation to mitigate this issue. Our first approach is parametric: it models inter-arrival times of uncertainty keywords as a renewal process and applies sequential testing for stopping. Our second approach is nonparametric and provides finite-sample guarantees on the probability of halting too early on well-posed queries. We conduct empirical evaluations on reasoning tasks across several domains and models. Our results indicate that uncertainty-aware early stopping can improve both efficiency and reliability in LLM reasoning, and we observe especially significant gains for math reasoning.

</details>


### [240] [Neuromem: A Granular Decomposition of the Streaming Lifecycle in External Memory for LLMs](https://arxiv.org/abs/2602.13967)
*Ruicheng Zhang,Xinyi Li,Tianyi Xu,Shuhao Zhang,Xiaofei Liao,Hai Jin*

Main category: cs.AI

TL;DR: Neuromem 是一个针对外存模块在交互式插入和检索协议下的可扩展测试床，它通过五个维度分析这些模块的生命周期，并在三个数据集上评估其表现。


<details>
  <summary>Details</summary>
Motivation: 当前的外存模块评估多基于静态环境，但实际应用中需要处理连续的新事实插入和即时检索，因此本文提出了 Neuromem 测试床来评估在动态环境下外存模块的表现。

Method: Neuromem 通过分解外存模块的生命周期成五个方面：内存数据结构，归一化策略，聚合政策，查询构建策略，上下文集成机制。通过三个数据集在共享服务堆栈中分别评估算法的可替换版本。

Result: 实验证明，在内存增长的过程中，性能通常会下降，特别是时间相关查询是最难的类别。内存数据结构显著决定了可实现的质量边界，而严格的压缩和生成性数据集成机制主要在插入和检索之间转移成本，但对准确性提高有限。

Conclusion: Neuromem 提供了一个全面框架来评估外存模块在动态环境中的表现，有助于优化这些模块的设计。

Abstract: Most evaluations of External Memory Module assume a static setting: memory is built offline and queried at a fixed state. In practice, memory is streaming: new facts arrive continuously, insertions interleave with retrievals, and the memory state evolves while the model is serving queries. In this regime, accuracy and cost are governed by the full memory lifecycle, which encompasses the ingestion, maintenance, retrieval, and integration of information into generation. We present Neuromem, a scalable testbed that benchmarks External Memory Modules under an interleaved insertion-and-retrieval protocol and decomposes its lifecycle into five dimensions including memory data structure, normalization strategy, consolidation policy, query formulation strategy, and context integration mechanism. Using three representative datasets LOCOMO, LONGMEMEVAL, and MEMORYAGENTBENCH, Neuromem evaluates interchangeable variants within a shared serving stack, reporting token-level F1 and insertion/retrieval latency. Overall, we observe that performance typically degrades as memory grows across rounds, and time-related queries remain the most challenging category. The memory data structure largely determines the attainable quality frontier, while aggressive compression and generative integration mechanisms mostly shift cost between insertion and retrieval with limited accuracy gain.

</details>


### [241] [Cognitive Chunking for Soft Prompts: Accelerating Compressor Learning via Block-wise Causal Masking](https://arxiv.org/abs/2602.13980)
*Guojie Liu,Yiqi Wang,Yanfeng Yang,Wenqi Fan,Songlei Jian,Jianfeng Zhang,Jie Yu*

Main category: cs.AI

TL;DR: 该研究提出了一种名为Parallelized Iterative Compression (PIC) 的方法，简化了对齐检索的记忆体压缩方式。这种方法通过简单地修改 Transformer 的注意力掩码，限制记忆体token的感知范围到顺序本地块，从而降低了压缩器训练的难度。实验结果表明，PIC 在高压缩比场景下一致性地优于其他基线，并显著加速了训练过程。


<details>
  <summary>Details</summary>
Motivation: 为利用大型语言模型的能力，提供广泛上下文至关重要。然而，较长的上下文会显著增加推理延迟。因此，研究提出了一种称为Parallelized Iterative Compression (PIC) 的方法，旨在降低压缩器训练的难度，以提高效率。

Method: 该方法通过修改Transformer的注意力掩码，将记忆体token的感知范围限定为顺序局部块，从而降低压缩器训练的难度。

Result: 实验结果显示，PIC 在多个下游任务中表现出色，特别是在高压缩比场景下，优于其他基线方法。在QA任务中，PIC 在 64倍压缩时相比基线方法，F1分数提高29.8％，EM分数提高40.7％。此外，它还显著减少了训练时间。

Conclusion: 这种方法通过简化内存压缩的方式，使复杂度降低，从而提高了训练效率和模型性能。

Abstract: Providing extensive context via prompting is vital for leveraging the capabilities of Large Language Models (LLMs). However, lengthy contexts significantly increase inference latency, as the computational cost of self-attention grows quadratically with sequence length. To mitigate this issue, context compression-particularly soft prompt compressio-has emerged as a widely studied solution, which converts long contexts into shorter memory embeddings via a trained compressor. Existing methods typically compress the entire context indiscriminately into a set of memory tokens, requiring the compressor to capture global dependencies and necessitating extensive pre-training data to learn effective patterns. Inspired by the chunking mechanism in human working memory and empirical observations of the spatial specialization of memory embeddings relative to original tokens, we propose Parallelized Iterative Compression (PIC). By simply modifying the Transformer's attention mask, PIC explicitly restricts the receptive field of memory tokens to sequential local chunks, thereby lowering the difficulty of compressor training. Experiments across multiple downstream tasks demonstrate that PIC consistently outperforms competitive baselines, with superiority being particularly pronounced in high compression scenarios (e.g., achieving relative improvements of 29.8\% in F1 score and 40.7\% in EM score on QA tasks at the $64\times$ compression ratio). Furthermore, PIC significantly expedites the training process. Specifically, when training the 16$\times$ compressor, it surpasses the peak performance of the competitive baseline while effectively reducing the training time by approximately 40\%.

</details>


### [242] [Bridging AI and Clinical Reasoning: Abductive Explanations for Alignment on Critical Symptoms](https://arxiv.org/abs/2602.13985)
*Belona Sonna,Alban Grastien*

Main category: cs.AI

TL;DR: 文章提出了一种基于正式演绎解释的方法，旨在提高AI在临床诊断中的透明度和可解释性，同时保持预测准确性。


<details>
  <summary>Details</summary>
Motivation: AI在临床诊断中虽然表现出色，但其决策过程缺乏透明性和解释性，可能导致对结果的信任度降低。

Method: 利用正式的演绎解释方法，确保在最小必要特征集上进行一致且有保证的推理。这种方法可以提供清晰的AI决策理解，并与临床推理相一致。

Result: 该方法在保持预测准确性的同时，提供了可操作的临床见解，形成了可信赖的AI在医学诊断中的框架。

Conclusion: 这种方法有助于提高AI在临床环境中的应用和接受度，为未来的医学应用铺平道路。

Abstract: Artificial intelligence (AI) has demonstrated strong potential in clinical diagnostics, often achieving accuracy comparable to or exceeding that of human experts. A key challenge, however, is that AI reasoning frequently diverges from structured clinical frameworks, limiting trust, interpretability, and adoption. Critical symptoms, pivotal for rapid and accurate decision-making, may be overlooked by AI models even when predictions are correct. Existing post hoc explanation methods provide limited transparency and lack formal guarantees. To address this, we leverage formal abductive explanations, which offer consistent, guaranteed reasoning over minimal sufficient feature sets. This enables a clear understanding of AI decision-making and allows alignment with clinical reasoning. Our approach preserves predictive accuracy while providing clinically actionable insights, establishing a robust framework for trustworthy AI in medical diagnosis.

</details>


### [243] [REAL: Resolving Knowledge Conflicts in Knowledge-Intensive Visual Question Answering via Reasoning-Pivot Alignment](https://arxiv.org/abs/2602.14065)
*Kai Ye,Xianwei Mao,Sheng Zhou,Zirui Shao,Ye Mo,Liangliang Liu,Haikuan Huang,Bin Li,Jiajun Bu*

Main category: cs.AI

TL;DR: REAL框架通过引入Reasoning-Pivot Aware SFT和Reasoning-Pivot Guided Decoding策略，有效解决了视觉问答系统中的知识冲突问题，显著提升了模型的精确度。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉问答（VQA）系统在知识密集型任务中遇到严重知识冲突的问题，主要原因在于缺乏可泛化的冲突检测机制和模型内部的约束机制来处理相互矛盾的证据。

Method: 提出了REAL框架，该框架基于Reasoning-Pivot的新概念。通过Reasoning-Pivot Aware SFT训练可泛化的冲突鉴别器，并利用Reasoning-Pivot Guided Decoding进行模型内部的矛盾解决。

Result: 实验结果表明，REAL在各种基准测试中显著提高了冲突识别的准确性，并达到了最先进的性能水平，证明了基于Reasoning-Pivot的方法的有效性。

Conclusion: REAL框架通过引入Reasoning-Pivot为核心的机制，有效解决了开放领域检索知识冲突问题，提高了解题的准确率和模型性能。

Abstract: Knowledge-intensive Visual Question Answering (KI-VQA) frequently suffers from severe knowledge conflicts caused by the inherent limitations of open-domain retrieval. However, existing paradigms face critical limitations due to the lack of generalizable conflict detection and intra-model constraint mechanisms to handle conflicting evidence. To address these challenges, we propose the REAL (Reasoning-Pivot Alignment) framework centered on the novel concept of the Reasoning-Pivot. Distinct from reasoning steps that prioritize internal self-derivation, a reasoning-pivot serves as an atomic unit (node or edge) in the reasoning chain that emphasizes knowledge linkage, and it typically relies on external evidence to complete the reasoning. Supported by our constructed REAL-VQA dataset, our approach integrates Reasoning-Pivot Aware SFT (RPA-SFT) to train a generalizable discriminator by aligning conflicts with pivot extraction, and employs Reasoning-Pivot Guided Decoding (RPGD), an intra-model decoding strategy that leverages these pivots for targeted conflict mitigation. Extensive experiments across diverse benchmarks demonstrate that REAL significantly enhances discrimination accuracy and achieves state-of-the-art performance, validating the effectiveness of our pivot-driven resolution paradigm.

</details>


### [244] [NEST: Nascent Encoded Steganographic Thoughts](https://arxiv.org/abs/2602.14095)
*Artem Karpov*

Main category: cs.AI

TL;DR: 本研究系统性地评估了28个不同代次的语言模型的 steganographic CoT 特性，发现当前模型尚不能有效隐藏复杂数学任务的推理，但 Claude Opus 4.5 在简化计数实验中表现优异，而 GPT-5.2 有时会拒绝 steganographic 指令但同时遵守它们。


<details>
  <summary>Details</summary>
Motivation: 探讨 steganographic CoT 的可能性及其对大型语言模型安全性的潜在影响，提出风险评估和部署政策。

Method: 研究通过评估 28 个不同代次的语言模型在四个数据集上的 monitor evasion、refusal rates、encoding fidelity 和 hidden task accuracy，测量 steganographic acrostics 的性能。

Result: 研究结果显示当前模型难以隐藏复杂数学任务的推理，但在简化计数实验中，Claude Opus 4.5 达到了 92% 的隐藏任务准确率；此外，GPT-5.2 在极少情况下可能拒绝对 steganographic 指令的操作但遵守它们。

Conclusion: 研究指出了 steganographic 风险的持续评估必要性和提供了一项前瞻性的检测和预防隐式推理的方法，以防止可能的不一致行为。

Abstract: Monitoring chain-of-thought (CoT) reasoning is a foundational safety technique for large language model (LLM) agents; however, this oversight is compromised if models learn to conceal their reasoning. We explore the potential for steganographic CoT -- where models hide secret reasoning within innocuous text -- to inform risk assessment and deployment policies. We systematically evaluate the limits of steganographic capabilities across 28 models, ranging from past generations to the current frontier. We measure monitor evasion, refusal rates, encoding fidelity, and hidden task accuracy across four datasets, comparing steganographic acrostics against plain reasoning and filler-token baselines. We find that current models cannot yet sustain hidden reasoning for complex math and arithmetic tasks. However, in a simplified counting experiment, Claude Opus 4.5 achieved 92% accuracy on the hidden task, demonstrating nascent capability. Notably, in rare cases (<1%), GPT-5.2 might refuse steganographic instructions while simultaneously complying with them. Our findings underscore the need for continuous evaluation of steganographic risks. This study provides a methodology to preemptively detect and prevent hidden reasoning that might empower misaligned scheming and deceptive behavior.

</details>


### [245] [Algebraic Quantum Intelligence: A New Framework for Reproducible Machine Creativity](https://arxiv.org/abs/2602.14130)
*Kazuo Yano,Jonghyeok Lee,Tae Ishitomi,Hironobu Kawaguchi,Akira Koyama,Masakuni Ota,Yuki Ota,Nobuo Sato,Keita Shimada,Sho Takematsu,Ayaka Tobinai,Satomi Tsuji,Kazunori Yanagi,Keiko Yano,Manabu Harada,Yuki Matsuda,Kazunori Matsumoto,Kenichi Matsumura,Hamae Matsuo,Yumi Miyazaki,Kotaro Murai,Tatsuya Ohshita,Marie Seki,Shun Tanoue,Tatsuki Terakado,Yuko Ichimaru,Mirei Saito,Akihiro Otsuka,Koji Ara*

Main category: cs.AI

TL;DR: 该研究提出了一种名为Algebraic Quantum Intelligence (AQI)的计算框架，旨在扩展语言模型的创造力。AQI通过非交换代数结构和C值操作，实现了解释顺序依赖、干涉和不确定性，并在跨领域的创造力推理基准上优于现有的模型。


<details>
  <summary>Details</summary>
Motivation: 当前的语言模型在生成流畅且上下文相关的内容方面取得了显著成功，但其创造能力仍然有限。研究认为这是因为现代语言模型在提供丰富背景时，未来生成的空间会被强烈限制，生成过程由近乎确定性的动态控制。

Method: AQI被设计为基于量子理论的非交换代数结构，其中语义状态表示为希尔伯特空间中的向量，并通过非交换算子计算的C值来控制语义状态的演化。研究通过在现有的Transformer基线模型中增加超过600个特定算子来实现AQI。

Result: 在跨领域的创造力推理基准上，AQI系统的表现优于强大的基线模型，显示出统计上显著的提升和降低跨领域变化。非交换代数动力学被证明可以成为机器创造性的实用和可复制基础。

Conclusion: 研究结果表明，非交换代数动力学可以作为机器创造性的实用和可复制基础，并且该架构已经部署在企业环境中。

Abstract: Large language models (LLMs) have achieved remarkable success in generating fluent and contextually appropriate text; however, their capacity to produce genuinely creative outputs remains limited. This paper posits that this limitation arises from a structural property of contemporary LLMs: when provided with rich context, the space of future generations becomes strongly constrained, and the generation process is effectively governed by near-deterministic dynamics. Recent approaches such as test-time scaling and context adaptation improve performance but do not fundamentally alter this constraint. To address this issue, we propose Algebraic Quantum Intelligence (AQI) as a computational framework that enables systematic expansion of semantic space. AQI is formulated as a noncommutative algebraic structure inspired by quantum theory, allowing properties such as order dependence, interference, and uncertainty to be implemented in a controlled and designable manner. Semantic states are represented as vectors in a Hilbert space, and their evolution is governed by C-values computed from noncommutative operators, thereby ensuring the coexistence and expansion of multiple future semantic possibilities. In this study, we implement AQI by extending a transformer-based LLM with more than 600 specialized operators. We evaluate the resulting system on creative reasoning benchmarks spanning ten domains under an LLM-as-a-judge protocol. The results show that AQI consistently outperforms strong baseline models, yielding statistically significant improvements and reduced cross-domain variance. These findings demonstrate that noncommutative algebraic dynamics can serve as a practical and reproducible foundation for machine creativity. Notably, this architecture has already been deployed in real-world enterprise environments.

</details>


### [246] [Text Before Vision: Staged Knowledge Injection Matters for Agentic RLVR in Ultra-High-Resolution Remote Sensing Understanding](https://arxiv.org/abs/2602.14225)
*Fengxiang Wang,Mingshuo Chen,Yueying Li,Yajie Yang,Yuhao Zhou,Di Wang,Yifan Zhang,Haoyu Wang,Haiyan Zhao,Hongda Sun,Long Lan,Jun Song,Yulin Wang,Jing Zhang,Wenlong Zhang,Bo Du*

Main category: cs.AI

TL;DR: 该研究通过冷启动监督微调（SFT）、强化学习验证奖励（RLVR）和自主强化学习验证奖励（Agentic RLVR）在超高清遥感基准上的对比实验，发现高质量的地球科学文本QA对提升视觉推理效果至关重要。提出了一个知识注入步骤：首先通过结构化的地球科学文本QA建立推理框架，然后在相同困难的超高清图像-文本示例上预热监督微调，这种方法在XLRS-Bench上达到了60.40%的Pass@1，超越了大型通用模型。


<details>
  <summary>Details</summary>
Motivation: 研究旨在提高超高清遥感中细粒度任务相关信息获取的效率，通过对比多种后训练方法的效果，发现高质量的地球科学文本QA对视觉推理有显著促进作用。

Method: 研究使用了三个不同方法进行对比实验：Cold-start Supervised Fine-Tuning (SFT)，RLVR，Agentic RLVR，并在超高清遥感基准数据集上评估了这三种方法的表现。

Result: 实验结果显示，高质量的地球科学文本QA在视觉推理中起到了关键作用。提出了一种结合结构化文本QA和图像文本样本的预热SFT方法，显著提高了模型性能，在XLRS-Bench数据集上取得60.40%的Pass@1，超越了其他大模型。

Conclusion: 研究证明，地球科学领域的文本QA可以为视觉推理任务提供有用的先验知识，通过结合结构化文本和图像样本的预热监督微调方法，可以显著提升超高清遥感的性能。

Abstract: Multimodal reasoning for ultra-high-resolution (UHR) remote sensing (RS) is usually bottlenecked by visual evidence acquisition: the model necessitates localizing tiny task-relevant regions in massive pixel spaces. While Agentic Reinforcement Learning with Verifiable Rewards (RLVR) using zoom-in tools offers a path forward, we find that standard reinforcement learning struggles to navigate these vast visual spaces without structured domain priors. In this paper, we investigate the interplay between post-training paradigms: comparing Cold-start Supervised Fine-Tuning (SFT), RLVR, and Agentic RLVR on the UHR RS benchmark.Our controlled studies yield a counter-intuitive finding: high-quality Earth-science text-only QA is a primary driver of UHR visual reasoning gains. Despite lacking images, domain-specific text injects the concepts, mechanistic explanations, and decision rules necessary to guide visual evidence retrieval.Based on this, we propose a staged knowledge injection recipe: (1) cold-starting with scalable, knowledge-graph-verified Earth-science text QA to instill reasoning structures;and (2) "pre-warming" on the same hard UHR image-text examples during SFT to stabilize and amplify subsequent tool-based RL. This approach achieves a 60.40% Pass@1 on XLRS-Bench, significantly outperforming larger general purpose models (e.g., GPT-5.2, Gemini 3.0 Pro, Intern-S1) and establishing a new state-of-the-art.

</details>


### [247] [Benchmarking at the Edge of Comprehension](https://arxiv.org/abs/2602.14307)
*Samuele Marro,Jialin Yu,Emanuele La Malfa,Oishi Deb,Jiawei Li,Yibo Yang,Ebey Abraham,Sunando Sengupta,Eric Sommerlade,Michael Wooldridge,Philip Torr*

Main category: cs.AI

TL;DR: 本文提出了批判鲁棒基准测试的方法，用于在人类完全理解任务不可行时比较模型。通过布雷德利-陶里二分模型联合排名前沿的大语言模型，结果显示这种方法产出的得分稳定，与外部能力衡量标准相关。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）持续改进，人类生成区分性任务、提供准确答案以及评估复杂解决方案的难度越来越大。当基准测试不可行时，AI领域进步的衡量能力将受损。为此，本文提出了一个对抗性的框架，以在无法完全理解任务时比较模型。

Method: 本文提出了一个基于布雷德利-陶里二分模型的基准测试方法，通过让人类作为受限制的验证者，专注于任务中的局部主张，从而维持评估的完整性。该模型利用批判鲁棒的正确性概念，即如果一个答案没有其他反驳方能够令人信服地证明其错误，则该答案被视为正确。

Result: 该方法在数学领域展示了对八种前沿大语言模型的有效性，产出的得分稳定且与外部能力衡量标准相关。

Conclusion: 本文提出的方法提出了一个新的基准测试框架，使其能够在无法完全理解任务的情况下对模型进行比较，从而开拓了AI评估的新途径。

Abstract: As frontier Large Language Models (LLMs) increasingly saturate new benchmarks shortly after they are published, benchmarking itself is at a juncture: if frontier models keep improving, it will become increasingly hard for humans to generate discriminative tasks, provide accurate ground-truth answers, or evaluate complex solutions. If benchmarking becomes infeasible, our ability to measure any progress in AI is at stake. We refer to this scenario as the post-comprehension regime. In this work, we propose Critique-Resilient Benchmarking, an adversarial framework designed to compare models even when full human understanding is infeasible. Our technique relies on the notion of critique-resilient correctness: an answer is deemed correct if no adversary has convincingly proved otherwise. Unlike standard benchmarking, humans serve as bounded verifiers and focus on localized claims, which preserves evaluation integrity beyond full comprehension of the task. Using an itemized bipartite Bradley-Terry model, we jointly rank LLMs by their ability to solve challenging tasks and to generate difficult yet solvable questions. We showcase the effectiveness of our method in the mathematical domain across eight frontier LLMs, showing that the resulting scores are stable and correlate with external capability measures. Our framework reformulates benchmarking as an adversarial generation-evaluation game in which humans serve as final adjudicators.

</details>


### [248] [Reshaping MOFs text mining with a dynamic multi-agents framework of large language model](https://arxiv.org/abs/2504.18880)
*Zuhong Lin,Daoyuan Ren,Kai Ran,Jing Sun,Songlin Yu,Xuefeng Bai,Xiaotian Huang,Haiyang He,Pengxu Pan,Ying Fang,Zhanglin Li,Haipu Li,Jingjing Yao*

Main category: cs.AI

TL;DR: MOFh6是一个由大规模语言模型驱动的系统，可以将文献或晶体代码转换为标准化的合成表格，提高了MOF合成信息提取的准确性和可读性，显著加速了文献知识向实际合成协议的转换，具有显著的成本效益。


<details>
  <summary>Details</summary>
Motivation: 目前文献中有关MOF合成的信息分散、不一致且难以理解，影响了实验设计的效率。

Method: MOFh6采用大规模语言模型处理全文或晶体代码，实现跨段落相关描述的链接、配体缩写与全名的统一，并输出结构化参数。

Result: MOFh6在提取准确性上达到了99%，解决了94.1%的缩写案例，处理全文时间为9.6秒，定位合成描述所需时间为36秒，处理100篇论文的成本为4.24美元。

Conclusion: MOFh6将静态数据库查找替换为实时提取，重塑了MOF合成研究，加速了文献知识向实际合成协议的转换，实现了大规模的数据驱动材料发现。

Abstract: Accurately identifying the synthesis conditions of metal-organic frameworks (MOFs) is essential for guiding experimental design, yet remains challenging because relevant information in the literature is often scattered, inconsistent, and difficult to interpret. We present MOFh6, a large language model driven system that reads raw articles or crystal codes and converts them into standardized synthesis tables. It links related descriptions across paragraphs, unifies ligand abbreviations with full names, and outputs structured parameters ready for use. MOFh6 achieved 99% extraction accuracy, resolved 94.1% of abbreviation cases across five major publishers, and maintained a precision of 0.93 +/- 0.01. Processing a full text takes 9.6 s, locating synthesis descriptions 36 s, with 100 papers processed for USD 4.24. By replacing static database lookups with real-time extraction, MOFh6 reshapes MOF synthesis research, accelerating the conversion of literature knowledge into practical synthesis protocols and enabling scalable, data-driven materials discovery.

</details>


### [249] [Competition for attention predicts good-to-bad tipping in AI](https://arxiv.org/abs/2602.14370)
*Neil F. Johnson,Frank Y. Huo*

Main category: cs.AI

TL;DR: 该研究揭示了一类潜在危险行为的原子级起因，该行为源自边缘AI中的注意力竞争，提出了一个新的动态临界点模型，以数学公式形式表达，并展示了该模型的广泛适用性。


<details>
  <summary>Details</summary>
Motivation: 随着越来越多的边缘设备能够运行类似于ChatGPT的语言模型，但缺乏互联网连接和安全监督，存在自我伤害、经济损失和极端主义等风险。现有的安全工具要么需要云连接，要么只有在伤害发生后才能检测到失败。因此，该研究旨在通过分析注意力竞争的动态机制，找出可以控制这种危险行为的新杠杆。

Method: 研究者开发了一种新的数学模型，用于描述对话上下文与竞争输出盆地之间的点积注意力竞争，从而预测潜在危险行为的动态临界点。模型被多个AI系统验证，并适用于不同领域的场景。

Result: 研究量化了注意力竞争对边缘AI系统中的潜在危险行为临界点的影响，提供了一种新的数学公式来表示这个临界点。该模型不仅适用于卫生、法律、金融、国防等不同领域，还可以根据不同地区的法律法规、语言和文化背景进行实例化。

Conclusion: 此研究提出的方法为理解和控制边缘AI系统的潜在危险行为提供了新的视角和手段，具有跨学科的广泛应用前景。

Abstract: More than half the global population now carries devices that can run ChatGPT-like language models with no Internet connection and minimal safety oversight -- and hence the potential to promote self-harm, financial losses and extremism among other dangers. Existing safety tools either require cloud connectivity or discover failures only after harm has occurred. Here we show that a large class of potentially dangerous tipping originates at the atomistic scale in such edge AI due to competition for the machinery's attention. This yields a mathematical formula for the dynamical tipping point n*, governed by dot-product competition for attention between the conversation's context and competing output basins, that reveals new control levers. Validated against multiple AI models, the mechanism can be instantiated for different definitions of 'good' and 'bad' and hence in principle applies across domains (e.g. health, law, finance, defense), changing legal landscapes (e.g. EU, UK, US and state level), languages, and cultural settings.

</details>


### [250] [Boule or Baguette? A Study on Task Topology, Length Generalization, and the Benefit of Reasoning Traces](https://arxiv.org/abs/2602.14404)
*William L. Tong,Ege Cakar,Cengiz Pehlevan*

Main category: cs.AI

TL;DR: 研究引入了一个新的大规模语句和证明数据集PITA，以评估推理模型在长证明任务中的泛化能力。研究发现，使用推理痕迹的模型在广泛而简单的任务中表现良好，但在专门而复杂的任务中表现较差。


<details>
  <summary>Details</summary>
Motivation: 目前关于推理模型中生成的推理痕迹（RTs）如何支持推理以及其极限的理解尚不完善，故引入PITA来促进这一领域的透明理解。

Method: 开发PITA数据集，包含数百万条推理语句及其对应的证明，并通过对这些数据集的不同子集进行训练和测试，研究模型在不同任务深度和广度下的表现。

Result: 研究发现，使用RT模型在广泛而简单的任务中泛化较好，但在专门而复杂的任务中泛化较差。这一现象不仅在PITA数据集上得到验证，也在基于三段论的简单合成任务上得到了展示。

Conclusion: 本研究表明，RT模型有其固有的局限性，主要表现在深度任务上，同时也能识别研究范式的若干基本特性。

Abstract: Recent years have witnessed meteoric progress in reasoning models: neural networks that generate intermediate reasoning traces (RTs) before producing a final output. Despite the rapid advancement, our understanding of how RTs support reasoning, and the limits of this paradigm, remain incomplete. To promote greater clarity, we introduce PITA: a novel large-scale dataset of over 23 million statements in propositional logic and their corresponding proofs. As a benchmark for robust reasoning, we focus on length generalization: if a model is trained to determine truth or falsity on statements with proofs up to fixed length, how well does it generalize to statements requiring longer proofs? We propose notions of (1) task depth and (2) task breadth, which measure respectively (1) the number of steps required to solve an example from a task and (2) the number of unique examples across a task. We vary these quantities across subsets of PITA, and find that RT models generalize well on broad and shallow subsets, while deteriorating on narrow and deep subsets relative to non-RT baselines. To determine whether our results are idiosyncratic to PITA or indicative of general phenomena, we compare our results to a simple synthetic task based on syllogisms. Our resulting theory suggests fundamental scalings that limit how well RT models perform on deep tasks, and highlights their generalization strengths on broad tasks. Our findings overall identify fundamental benefits and limitations inherent in using reasoning traces.

</details>


### [251] [Precedent-Informed Reasoning: Mitigating Overthinking in Large Reasoning Models via Test-Time Precedent Learning](https://arxiv.org/abs/2602.14451)
*Qianyue Wang,Jinwu Hu,Huanxiang Lin,Bolin Chen,Zhiquan Wen,Yaofo Chen,Yu Rong,Mingkui Tan*

Main category: cs.AI

TL;DR: 提出了一种名为PIR的新方法，通过借鉴过去的案例指导LLM的推理过程，减少冗余计算，同时保持甚至提高模型的准确性。


<details>
  <summary>Details</summary>
Motivation: 当前的LLM推理过程中存在冗长且低效的推理链，可能导致高计算成本并降低性能。这种新方法旨在借鉴人类从经验中学习的过程，来优化LLM的计算效率。

Method: PIR包括两个主要部分：1. 自适应先例选择（APS），根据语义相似度和模型困惑度排名示例，并调整先例的数量以最大化困惑度降低。2. 测试时经验内部化（TEI），在指导下更新轻量级适配器，使用先例中的解决模式作为推理过程中的先验知识。

Result: 在数学推理、科学问答和代码生成等任务上的实验表明，PIR能够在保持甚至提高模型准确性的同时缩短推理路径，提供出色的准确性和效率权衡。

Conclusion: PIR为改进LLM的推理机制提供了一种有效的方法，通过利用先例知识降低计算成本并提高效率。

Abstract: Reasoning in Large Language Models (LLMs) often suffers from inefficient long chain-of-thought traces with redundant self-exploration and validation, which inflate computational costs and even degrade performance. Inspired by human reasoning patterns where people solve new problems by leveraging past related cases to constrain search spaces and reduce trial-and-error, we propose Precedent Informed Reasoning (PIR) transforming LRMs'reasoning paradigm from exhaustive self-exploration to guided learning from precedents. PIR addresses two key challenges: what precedents to adopt and how to utilize them. First, Adaptive Precedent Selection (APS) constructs, for each question and LRM, a compact set of precedents that are both semantically related and informative for the model. It ranks examples by a joint score with semantic similarity and model perplexity, then adapts the amount of precedents to maximize perplexity reduction. Second, Test-time Experience Internalization (TEI) is treated as the test-time learning on precedent-informed instruction, updating lightweight adapters to internalize solution patterns and use them as a prior during subsequent reasoning. Experiments across mathematical reasoning, scientific QA, and code generation demonstrate that PIR consistently shortens reasoning traces while maintaining or improving final accuracy across LLMs, yielding outstanding accuracy-efficiency trade-offs.

</details>


### [252] [Bounding Probabilities of Causation with Partial Causal Diagrams](https://arxiv.org/abs/2602.14503)
*Yuxuan Xie,Ang Li*

Main category: cs.AI

TL;DR: 本文提出了一种利用部分因果信息来更准确地估计因果概率的框架，无需完全识别，从而扩展了因果概率的应用范围。


<details>
  <summary>Details</summary>
Motivation: 当前的因果概率界面对可用协变量视而不见，需要完整的因果图，或依赖于二元设置，限制了它们的实际应用。而在现实应用场景中，关于因果信息往往是部分而非完全的。

Method: 通过将可用的结构或统计信息系统地作为约束条件纳入优化编程模型中，提出了一种更为精确且形式上有效的因果概率界方法。

Result: 实验结果表明，该方法能够在不完全识别的情况下，实现更为精确和形式上有效的因果概率界。

Conclusion: 该研究通过提出一种新框架，扩展了因果概率在因果知识不完整但有一定信息量时的应用。

Abstract: Probabilities of causation are fundamental to individual-level explanation and decision making, yet they are inherently counterfactual and not point-identifiable from data in general. Existing bounds either disregard available covariates, require complete causal graphs, or rely on restrictive binary settings, limiting their practical use. In real-world applications, causal information is often partial but nontrivial. This paper proposes a general framework for bounding probabilities of causation using partial causal information. We show how the available structural or statistical information can be systematically incorporated as constraints in a optimization programming formulation, yielding tighter and formally valid bounds without full identifiability. This approach extends the applicability of probabilities of causation to realistic settings where causal knowledge is incomplete but informative.

</details>


### [253] [Formally Verifying and Explaining Sepsis Treatment Policies with COOL-MC](https://arxiv.org/abs/2602.14505)
*Dennis Gross*

Main category: cs.AI

TL;DR: COOL-MC (Clinical-Oriented Optimized Labeler and Checker) 是一种新的可验证的 RL 政策工具，它能够批量生成可达状态空间，以较小的离散时间马尔可夫链进行验证，并通过 PCTL 查询解释决策驱动因素。这项研究在 ICU-Sepsis MDP 上展示了 COOL-MC 的能力，揭示了标准评价方法难以识别的模型弱点。


<details>
  <summary>Details</summary>
Motivation: 当前基于强化学习 (RL) 的决策方法在医疗保健中的应用面临不透明性和难以验证的问题。COOL-MC 的动机是提供一种新的工具，以确保在大规模医疗决策问题中，基于模型的检查和解释能力之间的平衡。

Method: COOL-MC 基于现有的模型检查器 Storm，采用一种独特的技术来克服大 MDP 分析难度的问题：只构造由训练策略引发的可达状态空间，自动将临床相关原子命题标签化，并集成解释方法与概率计算树逻辑 (PCTL) 查询。

Result: 在 ICU-Sepsis MDP 中，COOL-MC 的分析展示了用全 MDP 验证确定的硬边界，并训练了一个安全的 RL 策略，该策略实现了最优生存概率。通过 PCTL 验证和解释，该策略的行为被分析，例如表明该策略主要依赖于之前的用药历史，而未采用病人演变状况，这是一个由标准评估方法难以识别的弱点。

Conclusion: COOL-MC 提供了能够在临床治疗策略部署前进行有效验证和解释的方法，帮助临床医生理解并改进基于 RL 的架构。

Abstract: Safe and interpretable sequential decision-making is critical in healthcare, yet reinforcement learning (RL) policies for sepsis treatment optimization remain opaque and difficult to verify. Standard probabilistic model checkers operate on the full state space, which becomes infeasible for larger MDPs, and cannot explain why a learned policy makes particular decisions. COOL-MC wraps the model checker Storm but adds three key capabilities: it constructs only the reachable state space induced by a trained policy, yielding a smaller discrete-time Markov chain amenable to verification even when full-MDP analysis is intractable; it automatically labels states with clinically meaningful atomic propositions; and it integrates explainability methods with probabilistic computation tree logic (PCTL) queries to reveal which features drive decisions across treatment trajectories. We demonstrate COOL-MC's capabilities on the ICU-Sepsis MDP, a benchmark derived from approximately 17,000 sepsis patient records, which serves as a case study for applying COOL-MC to the formal analysis of sepsis treatment policies. Our analysis establishes hard bounds via full MDP verification, trains a safe RL policy that achieves optimal survival probability, and analyzes its behavior via PCTL verification and explainability on the induced DTMC. This reveals, for instance, that our trained policy relies predominantly on prior dosing history rather than the patient's evolving condition, a weakness that is invisible to standard evaluation but is exposed by COOL-MC's integration of formal verification and explainability. Our results illustrate how COOL-MC could serve as a tool for clinicians to investigate and debug sepsis treatment policies before deployment.

</details>


### [254] [Diagnosing Knowledge Conflict in Multimodal Long-Chain Reasoning](https://arxiv.org/abs/2602.14518)
*Jing Tang,Kun Wang,Haolang Lu,Hongjin Chen,KaiTao Chen,Zhongxiang Sun,Qiankun Li,Lingjuan Lyu,Guoshun Nan,Zhigang Zeng*

Main category: cs.AI

TL;DR: 研究发现，面对不同知识来源的冲突信号时，多模态大型语言模型（MLLMs）会出现推理失败。通过探究内部表示，研究揭示了四种机制：不同冲突类型可明确区分、冲突信号主要集中在较深的表层、噪声信号沿路径聚合可恢复输入级冲突类型、模型在面对冲突时偏好某个来源更加容易。


<details>
  <summary>Details</summary>
Motivation: 研究在多模态大型语言模型进行长时间链式推理过程中，当不同知识来源提供矛盾信号时，出现推理失败的问题。

Method: 通过探索内部表示来识别冲突类型、分析冲突信号在模型中的分布，并检验在冲突环境下模型对不同来源的偏好。

Result: 研究发现冲突类型可明确区分、信号主要集中在较深层、通过聚合噪声信号可以恢复输入级冲突类型、模型在冲突环境下对特定来源有偏向。

Conclusion: 研究提供了一个关于模型在知识冲突下的多模态推理机制，并为长链式推理失败的诊断与控制提供了理论依据。

Abstract: Multimodal large language models (MLLMs) in long chain-of-thought reasoning often fail when different knowledge sources provide conflicting signals. We formalize these failures under a unified notion of knowledge conflict, distinguishing input-level objective conflict from process-level effective conflict. Through probing internal representations, we reveal that: (I) Linear Separability: different conflict types are explicitly encoded as linearly separable features rather than entangled; (II) Depth Localization: conflict signals concentrate in mid-to-late layers, indicating a distinct processing stage for conflict encoding; (III) Hierarchical Consistency: aggregating noisy token-level signals along trajectories robustly recovers input-level conflict types; and (IV) Directional Asymmetry: reinforcing the model's implicit source preference under conflict is far easier than enforcing the opposite source. Our findings provide a mechanism-level view of multimodal reasoning under knowledge conflict and enable principled diagnosis and control of long-CoT failures.

</details>


### [255] [Disentangling Deception and Hallucination Failures in LLMs](https://arxiv.org/abs/2602.14529)
*Haolang Lu,Hongrui Peng,WeiYe Fu,Guoshun Nan,Xinye Cao,Xingrui Li,Hongcan Guo,Kun Wang*

Main category: cs.AI

TL;DR: 本文通过构建一个受控环境，分析了大型语言模型在实体中心的事实查询中的两种不同失败模式：知识存在和行为表达，并通过表示可分性、稀疏可解释性和推理时的激活引导进行了具体分析。


<details>
  <summary>Details</summary>
Motivation: 当前对大型语言模型错误行为的分析往往从行为的角度出发，将错误输出错误关联到知识缺失。本文则引入了一个机制导向的观点，将失败机制区分为知识存在和行为表达两个方面，并探讨它们在输出层面的相似性与内在机制的区别。

Method: 为了研究这种区别，作者构建了一个知识保持但行为表达可选择性改变的受控环境，分析包括表示可分性、稀疏可解释性和推理时的激活导航等方法。

Result: 通过上述方法，作者发现知识存在和行为表达两种失败方式尽管在输出表现上可能存在相似之处，但内部机制却存在显著区别。

Conclusion: 该研究揭示了大型语言模型在处理实体相关事实查询时的两种不同失败模式，为理解模型行为提供了新的视角。

Abstract: Failures in large language models (LLMs) are often analyzed from a behavioral perspective, where incorrect outputs in factual question answering are commonly associated with missing knowledge. In this work, focusing on entity-based factual queries, we suggest that such a view may conflate different failure mechanisms, and propose an internal, mechanism-oriented perspective that separates Knowledge Existence from Behavior Expression. Under this formulation, hallucination and deception correspond to two qualitatively different failure modes that may appear similar at the output level but differ in their underlying mechanisms. To study this distinction, we construct a controlled environment for entity-centric factual questions in which knowledge is preserved while behavioral expression is selectively altered, enabling systematic analysis of four behavioral cases. We analyze these failure modes through representation separability, sparse interpretability, and inference-time activation steering.

</details>


### [256] [MATEO: A Multimodal Benchmark for Temporal Reasoning and Planning in LVLMs](https://arxiv.org/abs/2602.14589)
*Gabriel Roccabruna,Olha Khomyn,Giuseppe Riccardi*

Main category: cs.AI

TL;DR: 该研究提出MATEO基准，旨在评估和提高大型视觉语言模型（LVLM）处理时间顺序推理的能力，通过使用专业级的多模态食谱数据集进行评价。


<details>
  <summary>Details</summary>
Motivation: 现有研究在大型视觉语言模型对时间执行顺序的理解上较为有限，仅限于自动提取的标注、线性时间序列近似或纯文本输入，因此需要一个全面基准来检测和改进此类模型的时间推理性能。

Method: 研究团队收集了一个高质量的专业级多模态食谱数据集，通过标准化编著流程将指令分解为离散步骤，并配以相应图像。同时设计并使用了一种可扩展的众包管道获取时间执行顺序注释。最终使用MATEO基准评估了六个最先进的大型视觉语言模型。

Result: 研究通过MATEO基准评估了六种具有不同模型规模、语言上下文、多模态输入结构和微调策略的最先进的大型视觉语言模型。

Conclusion: MATEO为大型视觉语言模型的时间推理能力提供了重要的评估工具，并展示了这些模型在未来实际规划任务中的潜力。

Abstract: AI agents need to plan to achieve complex goals that involve orchestrating perception, sub-goal decomposition, and execution. These plans consist of ordered steps structured according to a Temporal Execution Order (TEO, a directed acyclic graph that ensures each step executes only after its preconditions are satisfied. Existing research on foundational models' understanding of temporal execution is limited to automatically derived annotations, approximations of the TEO as a linear chain, or text-only inputs. To address this gap, we introduce MATEO (MultimodAl Temporal Execution Order), a benchmark designed to assess and improve the temporal reasoning abilities of Large Vision Language Models (LVLMs) required for real-world planning. We acquire a high-quality professional multimodal recipe corpus, authored through a standardized editorial process that decomposes instructions into discrete steps, each paired with corresponding images. We collect TEO annotations as graphs by designing and using a scalable crowdsourcing pipeline. Using MATEO, we evaluate six state-of-the-art LVLMs across model scales, varying language context, multimodal input structure, and fine-tuning strategies.

</details>


### [257] [Arbor: A Framework for Reliable Navigation of Critical Conversation Flows](https://arxiv.org/abs/2602.14643)
*Luís Silva,Diogo Gonçalves,Catarina Farinha,Clara Matos,Luís Ungaro*

Main category: cs.AI

TL;DR: Arbor 提出了一种打破单指令处理极限的方法，通过将决策树分解为节点级别的任务，实现了更高的准确率、更短的延迟和更低的成本。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在高风险领域如医疗急救中难以严格遵循结构化的流程。传统的单指令方法随着指令长度增加会导致指令遵循能力下降。Arbor框架通过将决策树导航分解为专门的节点级任务解决这一问题。

Method: Arbor框架将决策树标准化为边列表存储，并采用有向无环图（DAG）机制动态检索和调用专门针对有效过渡的LLM进行评估，将响应生成作为单独的推理步骤。

Result: 在10个基础模型上，与单指令基线相比，Arbor提高了平均回应准确率29.4个百分点，降低了每轮延迟57.1%和每轮成本14.4倍。

Conclusion: 这种架构分解减少了对内在模型能力的依赖，使较小的模型能够匹配甚至超越在单指令基线下运行的大模型。

Abstract: Large language models struggle to maintain strict adherence to structured workflows in high-stakes domains such as healthcare triage. Monolithic approaches that encode entire decision structures within a single prompt are prone to instruction-following degradation as prompt length increases, including lost-in-the-middle effects and context window overflow. To address this gap, we present Arbor, a framework that decomposes decision tree navigation into specialized, node-level tasks. Decision trees are standardized into an edge-list representation and stored for dynamic retrieval. At runtime, a directed acyclic graph (DAG)-based orchestration mechanism iteratively retrieves only the outgoing edges of the current node, evaluates valid transitions via a dedicated LLM call, and delegates response generation to a separate inference step. The framework is agnostic to the underlying decision logic and model provider. Evaluated against single-prompt baselines across 10 foundation models using annotated turns from real clinical triage conversations. Arbor improves mean turn accuracy by 29.4 percentage points, reduces per-turn latency by 57.1%, and achieves an average 14.4x reduction in per-turn cost. These results indicate that architectural decomposition reduces dependence on intrinsic model capability, enabling smaller models to match or exceed larger models operating under single-prompt baselines.

</details>


### [258] [From User Preferences to Base Score Extraction Functions in Gradual Argumentation](https://arxiv.org/abs/2602.14674)
*Aniol Civit,Antonio Rago,Antonio Andriella,Guillem Alenyà,Francesca Toni*

Main category: cs.AI

TL;DR: 本文介绍了一种从用户偏好中提取基分数的方法，将其应用于二极情感论证框架，形成了一个定量的二极情感论证框架，从而利用已建立的计算工具来支持渐进论证。


<details>
  <summary>Details</summary>
Motivation: 随着透明和可争议AI系统的增加需求，需要改进基分数的选择方法并简化基于用户偏好的论点组织。

Method: 本文提出了基分数提取函数，通过计算偏好与基分数之间的映射来生成定量二极情感论证框架。

Result: 提出了基分数提取函数的设计准则，并给出了具体算法，考虑了非线性偏好的近似计算。

Conclusion: 本文方法在理论和实践中评价了其适用性，并针对实际应用提供了选择适当渐进语义的建议。

Abstract: Gradual argumentation is a field of symbolic AI which is attracting attention for its ability to support transparent and contestable AI systems. It is considered a useful tool in domains such as decision-making, recommendation, debate analysis, and others. The outcomes in such domains are usually dependent on the arguments' base scores, which must be selected carefully. Often, this selection process requires user expertise and may not always be straightforward. On the other hand, organising the arguments by preference could simplify the task. In this work, we introduce \emph{Base Score Extraction Functions}, which provide a mapping from users' preferences over arguments to base scores. These functions can be applied to the arguments of a \emph{Bipolar Argumentation Framework} (BAF), supplemented with preferences, to obtain a \emph{Quantitative Bipolar Argumentation Framework} (QBAF), allowing the use of well-established computational tools in gradual argumentation. We outline the desirable properties of base score extraction functions, discuss some design choices, and provide an algorithm for base score extraction. Our method incorporates an approximation of non-linearities in human preferences to allow for better approximation of the real ones. Finally, we evaluate our approach both theoretically and experimentally in a robotics setting, and offer recommendations for selecting appropriate gradual semantics in practice.

</details>


### [259] [GREAT-EER: Graph Edge Attention Network for Emergency Evacuation Responses](https://arxiv.org/abs/2602.14676)
*Attila Lischka,Balázs Kulcsár*

Main category: cs.AI

TL;DR: 该研究提出了基于公共汽车的疏散优化问题（BEOP），并利用深度强化学习结合图学习的方法来解决此问题，能够在短时间生成高效的疏散路径。


<details>
  <summary>Details</summary>
Motivation: 随着气候变化导致的自然灾害增多，有效并快速的方法来开发疏散计划变得至关重要。BEOP旨在通过公共汽车疏散更多的人员，避免车辆疏散造成的拥堵。

Method: 研究人员提出了一种基于深度强化学习结合图学习的方法来解决BEOP，该方法在训练好后能实现高速推理，在短时间内生成疏散路线。

Result: 该方法能够生成近似最优的疏散计划，并通过 MILP 形式进行了误差界分析。在实际场景中，研究人员还验证了不同数量的疏散车辆对特定疏散目标的影响，同时保持了足够的运行时间。

Conclusion: 研究有效提出了BEOP并在实际场景中展示了其优势及实用性。

Abstract: Emergency situations that require the evacuation of urban areas can arise from man-made causes (e.g., terrorist attacks or industrial accidents) or natural disasters, the latter becoming more frequent due to climate change. As a result, effective and fast methods to develop evacuation plans are of great importance. In this work, we identify and propose the Bus Evacuation Orienteering Problem (BEOP), an NP-hard combinatorial optimization problem with the goal of evacuating as many people from an affected area by bus in a short, predefined amount of time. The purpose of bus-based evacuation is to reduce congestion and disorder that arises in purely car-focused evacuation scenarios. To solve the BEOP, we propose a deep reinforcement learning-based method utilizing graph learning, which, once trained, achieves fast inference speed and is able to create evacuation routes in fractions of seconds. We can bound the gap of our evacuation plans using an MILP formulation. To validate our method, we create evacuation scenarios for San Francisco using real-world road networks and travel times. We show that we achieve near-optimal solution quality and are further able to investigate how many evacuation vehicles are necessary to achieve certain bus-based evacuation quotas given a predefined evacuation time while keeping run time adequate.

</details>


### [260] [Removing Planner Bias in Goal Recognition Through Multi-Plan Dataset Generation](https://arxiv.org/abs/2602.14691)
*Mustafa F. Abdelwahed,Felipe Meneguzzi Kin Max Piamolini Gusmao,Joan Espasa*

Main category: cs.AI

TL;DR: 本文提出了一种新颖的方法，使用top-k规划生成多个不同计划来克服现有目标识别数据集中的系统性偏差。这种方法引入了.VERSION Coverage Score (VCS)。实验表明，在低观察性设置下，当前最先进的目标识别器的鲁棒性大幅下降。


<details>
  <summary>Details</summary>
Motivation: 现有的目标识别数据集由于规划系统的启发式前向搜索产生的系统性偏差而缺乏对更现实场景的挑战性。本文提出的top-k规划方法旨在生成对于相同目标假设的多种不同计划，从而减少这种偏差。

Method: 本文方法使用top-k规划技术生成多个涉及不同计划且针对相同目标假设的数据集。通过版本覆盖率分数（VCS）来衡量基于不同计划集推断目标的鲁棒性。

Result: 实验结果表明，当前最先进的目标识别器在低观察性设置下的鲁棒性大幅下降。VCS可以有效地作为评价目标识别器的重要指标。

Conclusion: 本文提出的方法和指标能够更好地评估目标识别器在不同规划器使用情况下的性能。这对于减少目标识别器的偏差、提高其在更复杂场景中的应用具有重要意义。

Abstract: Autonomous agents require some form of goal and plan recognition to interact in multiagent settings. Unfortunately, all existing goal recognition datasets suffer from a systematical bias induced by the planning systems that generated them, namely heuristic-based forward search. This means that existing datasets lack enough challenge for more realistic scenarios (e.g., agents using different planners), which impacts the evaluation of goal recognisers with respect to using different planners for the same goal. In this paper, we propose a new method that uses top-k planning to generate multiple, different, plans for the same goal hypothesis, yielding benchmarks that mitigate the bias found in the current dataset. This allows us to introduce a new metric called Version Coverage Score (VCS) to measure the resilience of the goal recogniser when inferring a goal based on different sets of plans. Our results show that the resilience of the current state-of-the-art goal recogniser degrades substantially under low observability settings.

</details>


### [261] [Evolutionary System Prompt Learning can Facilitate Reinforcement Learning for LLMs](https://arxiv.org/abs/2602.14697)
*Lunjun Zhang,Ryan Chen,Bradly C. Stadie*

Main category: cs.AI

TL;DR: E-SPL 提出了一种新方法，结合强化学习与系统提示进化，提升模型在推理和自主任务中的性能。实验显示在从易到难的泛化任务中，E-SPL 相比反思提示进化方法有更佳的表现。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型通过自省和强化学习机制自我改进，E-SPL 的提出旨在探索新的方法来同时提升模型的上下文和权重，期望在推理和自主任务中表现出色。

Method: E-SPL 方法在每次强化学习迭代中并行运行多个系统提示，使用强化学习更新与各系统提示相关的权重，并通过自监督突变和重组对系统提示群体进行进化更新。并对各系统提示进行 Elo 等级评定以实现进化选择。

Result: 实验表明，E-SPL 在从简单到复杂的一般化设置中，强化学习的成功率从 38.8% 提升到 45.1%，超越了反思提示进化方法（40.0%）。此外，E-SPL 在样本效率和泛化能力方面表现出一致的改进。

Conclusion: E-SPL 方法通过结合强化学习和系统提示进化，在推理和自主任务等多种场景下取得了显著的性能提升，并为未来的AI自主系统的发展提供了新的研究方向。

Abstract: Building agentic systems that can autonomously self-improve from experience is a longstanding goal of AI. Large language models (LLMs) today primarily self-improve via two mechanisms: self-reflection for context updates, and reinforcement learning (RL) for weight updates. In this work, we propose Evolutionary System Prompt Learning (E-SPL), a method for jointly improving model contexts and model weights. In each RL iteration, E-SPL selects multiple system prompts and runs rollouts with each in parallel. It applies RL updates to model weights conditioned on each system prompt, and evolutionary updates to the system prompt population via LLM-driven mutation and crossover. Each system prompt has a TrueSkill rating for evolutionary selection, updated from relative performance within each RL iteration batch. E-SPL encourages a natural division between declarative knowledge encoded in prompts and procedural knowledge encoded in weights, resulting in improved performance across reasoning and agentic tasks. For instance, in an easy-to-hard (AIME $\rightarrow$ BeyondAIME) generalization setting, E-SPL improves RL success rate from 38.8% $\rightarrow$ 45.1% while also outperforming reflective prompt evolution (40.0%). Overall, our results show that coupling reinforcement learning with system prompt evolution yields consistent gains in sample efficiency and generalization. Code: https://github.com/LunjunZhang/E-SPL

</details>


### [262] [AI Arms and Influence: Frontier Models Exhibit Sophisticated Reasoning in Simulated Nuclear Crises](https://arxiv.org/abs/2602.14740)
*Kenneth Payne*

Main category: cs.AI

TL;DR: 本文介绍了三项前沿大语言模型（GPT-5.2、Claude Sonnet 4、Gemini 3 Flash）在核危机模拟中的表现，验证并挑战了战略理论的核心观点，强调了人工智能在战略分析中的工具作用，但须与人类推理模式进行适当校准。


<details>
  <summary>Details</summary>
Motivation: 研究这些大语言模型在模拟中的表现，旨在理解它们如何进行推理决策，特别是在不确定性条件下的行为，以供国家安全专业人员及其相关领域应用。

Method: 通过构建一个核危机模拟，让三款不同的人工智能模型扮演对立的领导角色，观察他们在危机中的行为和决策过程。

Result: 研究发现，这些模型的行为和决策方式支持了某些战略理论的观点，但也揭示了一些意料之外的现象，如核禁令未能阻止核升级、威胁更多导致反升级而非遵守、高度互信反而加速而非抑制冲突等。

Conclusion: 研究指出，人工智能模拟为战略分析提供了强有力的方法，但必须基于已知的人类推理模式进行校准，这对于理解未来战略性结果中人工智能的角色至关重要。

Abstract: Today's leading AI models engage in sophisticated behaviour when placed in strategic competition. They spontaneously attempt deception, signaling intentions they do not intend to follow; they demonstrate rich theory of mind, reasoning about adversary beliefs and anticipating their actions; and they exhibit credible metacognitive self-awareness, assessing their own strategic abilities before deciding how to act.
  Here we present findings from a crisis simulation in which three frontier large language models (GPT-5.2, Claude Sonnet 4, Gemini 3 Flash) play opposing leaders in a nuclear crisis. Our simulation has direct application for national security professionals, but also, via its insights into AI reasoning under uncertainty, has applications far beyond international crisis decision-making.
  Our findings both validate and challenge central tenets of strategic theory. We find support for Schelling's ideas about commitment, Kahn's escalation framework, and Jervis's work on misperception, inter alia. Yet we also find that the nuclear taboo is no impediment to nuclear escalation by our models; that strategic nuclear attack, while rare, does occur; that threats more often provoke counter-escalation than compliance; that high mutual credibility accelerated rather than deterred conflict; and that no model ever chose accommodation or withdrawal even when under acute pressure, only reduced levels of violence.
  We argue that AI simulation represents a powerful tool for strategic analysis, but only if properly calibrated against known patterns of human reasoning. Understanding how frontier models do and do not imitate human strategic logic is essential preparation for a world in which AI increasingly shapes strategic outcomes.

</details>


### [263] [Return of the Schema: Building Complete Datasets for Machine Learning and Reasoning on Knowledge Graphs](https://arxiv.org/abs/2602.14795)
*Ivan Diliso,Roberto Barile,Claudia d'Amato,Nicola Fanizzi*

Main category: cs.AI

TL;DR: 该研究提出了一个名为\resource{}的资源，提供了从知识图谱中提取既包括模式信息也包括具体事实的数据集的新工作流程，这些数据集适用于机器学习和推理服务。并通过推理确保数据集的一致性，同时将新数据集从表达性强的知识图谱中提取出来并丰富现有的数据集。


<details>
  <summary>Details</summary>
Motivation: 现有的实验评估数据集通常仅包含地面事实，忽略了知识图谱中可用的模式信息，这限制了依赖丰富本体约束、推理或神经符号技术的方法的评估，不利于其实现大规模、真实世界知识图谱的性能。因此需要一种新方法来提供包含模式和地面事实且适合机器学习和推理服务的数据集。

Method: 通过提供一个从知识图谱中提取包含模式信息和地面事实的数据集的新工作流程，该方法包括推理以确保数据集的一致性，并将新数据集从具有表达性模式的信息丰富的知识图谱中提取出来同时丰富现有的数据集。

Result: 新工作流程帮助创建了多个包含模式和地面事实数据的新数据集，并将这些数据集以OWL格式序列化，使其适用于推理服务，同时提供加载这些数据集到标准机器学习库中的张量表示的实用工具。

Conclusion: 该研究通过提供一个全面的包含模式和事实的数据集向数据分析和知识图谱研究领域带来了新的评估资源，使得依赖于丰富模式和推理的技术在大规模、真实世界知识图谱中的性能能够被有效评估。

Abstract: Datasets for the experimental evaluation of knowledge graph refinement algorithms typically contain only ground facts, retaining very limited schema level knowledge even when such information is available in the source knowledge graphs. This limits the evaluation of methods that rely on rich ontological constraints, reasoning or neurosymbolic techniques and ultimately prevents assessing their performance in large-scale, real-world knowledge graphs. In this paper, we present \resource{} the first resource that provides a workflow for extracting datasets including both schema and ground facts, ready for machine learning and reasoning services, along with the resulting curated suite of datasets. The workflow also handles inconsistencies detected when keeping both schema and facts and also leverage reasoning for entailing implicit knowledge. The suite includes newly extracted datasets from KGs with expressive schemas while simultaneously enriching existing datasets with schema information. Each dataset is serialized in OWL making it ready for reasoning services. Moreover, we provide utilities for loading datasets in tensor representations typical of standard machine learning libraries.

</details>


### [264] [Concept Influence: Leveraging Interpretability to Improve Performance and Efficiency in Training Data Attribution](https://arxiv.org/abs/2602.14869)
*Matthew Kowal,Goncalo Paulo,Louis Jaburi,Tom Tseng,Lev E McKinney,Stefan Heimersheim,Aaron David Tucker,Adam Gleave,Kellin Pelrine*

Main category: cs.AI

TL;DR: 该研究提出了一种称为概念影响的方法，通过语义方向（如线性探针或稀疏自编码器特征）而非单一测试样本来归因模型行为。这种方法可以实现与经典影响函数相当的性能，但更具有可扩展性、可解释性和对模型行为的更好控制。


<details>
  <summary>Details</summary>
Motivation: 为了应对大规模语言模型训练和微调过程中识别驱动特定行为（特别是非预期行为）的数据的方法需求，研究需要开发一种更高效且能解释模型行为影响的方法。

Method: 研究首先引入了基于概念的影响方法（Concept Influence），它通过语义方向（如线性探针或稀疏自编码器特征）来归因模型行为。通过这种方法，研究发现普通基于探针的归因方法是Concept Influence的一阶近似，能够在性能几乎相当的情况下实现约十倍的速度提升。

Result: 研究通过在新兴偏差基准和实际的微调数据集中实证验证了Concept Influence及其近似版本的性能。结果显示，与经典的影响力函数相比，该方法在保持性能的同时，显著提高了可扩展性。

Conclusion: 通过在传统训练数据归因流程中纳入可解释结构，该研究展示了更高效、可解释且能够更好地控制模型行为的数据归因过程的可能性。

Abstract: As large language models are increasingly trained and fine-tuned, practitioners need methods to identify which training data drive specific behaviors, particularly unintended ones. Training Data Attribution (TDA) methods address this by estimating datapoint influence. Existing approaches like influence functions are both computationally expensive and attribute based on single test examples, which can bias results toward syntactic rather than semantic similarity. To address these issues of scalability and influence to abstract behavior, we leverage interpretable structures within the model during the attribution. First, we introduce Concept Influence which attribute model behavior to semantic directions (such as linear probes or sparse autoencoder features) rather than individual test examples. Second, we show that simple probe-based attribution methods are first-order approximations of Concept Influence that achieve comparable performance while being over an order-of-magnitude faster. We empirically validate Concept Influence and approximations across emergent misalignment benchmarks and real post-training datasets, and demonstrate they achieve comparable performance to classical influence functions while being substantially more scalable. More broadly, we show that incorporating interpretable structure within traditional TDA pipelines can enable more scalable, explainable, and better control of model behavior through data.

</details>


### [265] [Lifted Relational Probabilistic Inference via Implicit Learning](https://arxiv.org/abs/2602.14890)
*Luise Ge,Brendan Juba,Kris Nilsson,Alison Shao*

Main category: cs.AI

TL;DR: 该研究提出了一种新的方法来解决FOPL中的归纳学习与演绎推理之间的张力，通过隐式学习和多项式时间下的有限制求和方程（SOS）层次，同时对个体和世界进行提升推理。


<details>
  <summary>Details</summary>
Motivation: 通过结合学习和推理技术，解决传统提升推理方法在处理不完整和噪声观察数据时的困难。

Method: 将不完整的先验公理和独立采样的部分观察示例整合到SOS层次的有界度片段中，同时执行个体和世界的提升。

Result: 开发了一种新颖的算法，能够在多项式时间内隐式学习FOPL并对其进行提升推理。

Conclusion: 该研究为FOPL中归纳学习与演绎推理的结合提供了一种解决方案，推动了FOPL推理领域的进步。

Abstract: Reconciling the tension between inductive learning and deductive reasoning in first-order relational domains is a longstanding challenge in AI. We study the problem of answering queries in a first-order relational probabilistic logic through a joint effort of learning and reasoning, without ever constructing an explicit model. Traditional lifted inference assumes access to a complete model and exploits symmetry to evaluate probabilistic queries; however, learning such models from partial, noisy observations is intractable in general. We reconcile these two challenges through implicit learning to reason and first-order relational probabilistic inference techniques. More specifically, we merge incomplete first-order axioms with independently sampled, partially observed examples into a bounded-degree fragment of the sum-of-squares (SOS) hierarchy in polynomial time. Our algorithm performs two lifts simultaneously: (i) grounding-lift, where renaming-equivalent ground moments share one variable, collapsing the domain of individuals; and (ii) world-lift, where all pseudo-models (partial world assignments) are enforced in parallel, producing a global bound that holds across all worlds consistent with the learned constraints. These innovations yield the first polynomial-time framework that implicitly learns a first-order probabilistic logic and performs lifted inference over both individuals and worlds.

</details>


### [266] [The Potential of CoT for Reasoning: A Closer Look at Trace Dynamics](https://arxiv.org/abs/2602.14903)
*Gregor Bachmann,Yichen Jiang,Seyed Mohsen Moosavi Dezfooli,Moin Nabi*

Main category: cs.AI

TL;DR: 该研究深入分析了在高水平数学问题上生成链式思考（CoT）的推理痕迹，通过引入潜在概念来量化各个CoT部分对正确完成任务的影响。研究揭示了非单调推理模式、推理跳跃和偶然猜测等现象，并表明CoT中的推理洞察具有高度可转移性。


<details>
  <summary>Details</summary>
Motivation: 由于CoT提示是引导大型语言模型（LLMs）生成推理样式的回答的标准方法，理解其成功的驱动因素至关重要。这项研究旨在通过分析高水平数学问题中的推理痕迹，来更深入地理解CoT如何以及哪些部分对最终答案做出贡献。

Method: 研究使用链式思考（CoT）潜在的概念，通过量化CoT不同部分对正确完成任务的影响来进行分析。进一步探讨了CoT的可转移性，测量较弱模型在其较强大模型的CoT帮助下的表现。

Result: 研究揭示了CoT的非单调推理模式、推理跳跃和偶然猜测等现象。发现只有少量的部分CoT就能显著提高较弱模型的表现，表明CoT中的推理洞察具有高度可转移性。

Conclusion: 该研究深化了我们对CoT中潜在概念的理解，揭示了一些难以解释的现象，并证明了CoT推理洞察的高度可转移性，这对未来LLMs的设计和利用具有重要意义。

Abstract: Chain-of-thought (CoT) prompting is a de-facto standard technique to elicit reasoning-like responses from large language models (LLMs), allowing them to spell out individual steps before giving a final answer. While the resemblance to human-like reasoning is undeniable, the driving forces underpinning the success of CoT reasoning still remain largely unclear. In this work, we perform an in-depth analysis of CoT traces originating from competition-level mathematics questions, with the aim of better understanding how, and which parts of CoT actually contribute to the final answer. To this end, we introduce the notion of a potential, quantifying how much a given part of CoT increases the likelihood of a correct completion. Upon examination of reasoning traces through the lens of the potential, we identify surprising patterns including (1) its often strong non-monotonicity (due to reasoning tangents), (2) very sharp but sometimes tough to interpret spikes (reasoning insights and jumps) as well as (3) at times lucky guesses, where the model arrives at the correct answer without providing any relevant justifications before. While some of the behaviours of the potential are readily interpretable and align with human intuition (such as insights and tangents), others remain difficult to understand from a human perspective. To further quantify the reliance of LLMs on reasoning insights, we investigate the notion of CoT transferability, where we measure the potential of a weaker model under the partial CoT from another, stronger model. Indeed aligning with our previous results, we find that as little as 20% of partial CoT can ``unlock'' the performance of the weaker model on problems that were previously unsolvable for it, highlighting that a large part of the mechanics underpinning CoT are transferable.

</details>


### [267] [ReusStdFlow: A Standardized Reusability Framework for Dynamic Workflow Construction in Agentic AI](https://arxiv.org/abs/2602.14922)
*Gaoyang Zhang,Shanghong Zou,Yafang Wang,He Zhang,Ruohua Xu,Feng Zhao*

Main category: cs.AI

TL;DR: 该研究提出了ReusStdFlow框架，旨在解决企业Agentic AI中的再利用难题和结构性幻觉问题。该框架将异构平台特定的领域特定语言分解为标准化模块，并通过图和向量数据库的双重知识架构实现结构和功能语义的协同检索。最终，利用检索增强生成（RAG）策略智能重组工作流。实验结果显示，该系统在提取和构建方面的准确率均超过90%。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决企业Agentic AI中由于工作流异构性和专用领域特定语言导致的再利用难题和结构性幻觉问题。

Method: 该方法包括将DSL异构工作流分解为标准化模块，利用图和向量数据库协同检索结构和功能语义，最后通过RAG策略生成新的工作流。

Result: 实验在200个实际的n8n工作流上进行了测试，提取和构建的准确率均超过90%。

Conclusion: ReusStdFlow框架提供了一种标准化的解决方案，用于企业数字资产的自动化重组和高效再利用。

Abstract: To address the ``reusability dilemma'' and structural hallucinations in enterprise Agentic AI,this paper proposes ReusStdFlow, a framework centered on a novel ``Extraction-Storage-Construction'' paradigm. The framework deconstructs heterogeneous, platform-specific Domain Specific Languages (DSLs) into standardized, modular workflow segments. It employs a dual knowledge architecture-integrating graph and vector databases-to facilitate synergistic retrieval of both topological structures and functional semantics. Finally, workflows are intelligently assembled using a retrieval-augmented generation (RAG) strategy. Tested on 200 real-world n8n workflows, the system achieves over 90% accuracy in both extraction and construction. This framework provides a standardized solution for the automated reorganization and efficient reuse of enterprise digital assets.

</details>


### [268] [MAC-AMP: A Closed-Loop Multi-Agent Collaboration System for Multi-Objective Antimicrobial Peptide Design](https://arxiv.org/abs/2602.14926)
*Gen Zhou,Sugitha Janarthanan,Lianghong Chen,Pingzhao Hu*

Main category: cs.AI

TL;DR: 本文介绍了一种基于多智能体系统（MAC-AMP）的多目标抗菌肽（AMP）设计方法，该系统在无需大量人工干预的情况下，实现了分子层面多种性质的有效优化，并且保持了解释性。


<details>
  <summary>Details</summary>
Motivation: 鉴于抗菌肽在应对全球抗菌药物耐药性威胁中的潜在作用，以及现有人工智能模型在多目标优化、毒性控制方面的不足，本文旨在开发一种基于多智能体的自动闭环优化系统，以提升抗菌肽设计的灵活性和解释性。

Method: MAC-AMP系统利用适应性强化学习和模拟同行评审机制，实现了多目标优化目标的全自动设计过程。该系统通过仅提供任务描述和示例数据集即可生成新颖的抗菌肽。

Result: 实验表明，MAC-AMP在多个关键分子属性上优于其他抗菌肽生成模型，包括抗菌活性、抗菌肽可能性、毒性合规性和结构可靠性等方面。

Conclusion: 本文提出的方法为多目标抗菌肽设计提供了新的研究方向，展示了在解释性和泛化性改进方面的潜力。

Abstract: To address the global health threat of antimicrobial resistance, antimicrobial peptides (AMP) are being explored for their potent and promising ability to fight resistant pathogens. While artificial intelligence (AI) is being employed to advance AMP discovery and design, most AMP design models struggle to balance key goals like activity, toxicity, and novelty, using rigid or unclear scoring methods that make results hard to interpret and optimize. As the capabilities of Large Language Models (LLM) advance and evolve swiftly, we turn to AI multi-agent collaboration based on such models (multi-agent LLMs), which show rapidly rising potential in complex scientific design scenarios. Based on this, we introduce MAC-AMP, a closed-loop multi-agent collaboration (MAC) system for multi-objective AMP design. The system implements a fully autonomous simulated peer review-adaptive reinforcement learning framework that requires only a task description and example dataset to design novel AMPs. The novelty of our work lies in introducing a closed-loop multi-agent system for AMP design, with cross-domain transferability, that supports multi-objective optimization while remaining explainable rather than a 'black box'. Experiments show that MAC-AMP outperforms other AMP generative models by effectively optimizing AMP generation for multiple key molecular properties, demonstrating exceptional results in antibacterial activity, AMP likeliness, toxicity compliance, and structural reliability.

</details>


### [269] [On the Semantics of Primary Cause in Hybrid Dynamic Domains](https://arxiv.org/abs/2602.14994)
*Shakil M. Khan,Asim Mehmood,Sandra Zilles*

Main category: cs.AI

TL;DR: 本文提出了在混合时序逻辑公制框架下的因果定义，证明了两种定义等价，并具有一定程度的直观合理性。


<details>
  <summary>Details</summary>
Motivation: 探讨实际因果关系，特别是在混合连续与离散变化环境下的因果理论。

Method: 构建了混合时序逻辑公制框架，并提出了两种因果定义。

Result: 证明了提出的两种因果定义等价，且具有直观合理性。

Conclusion: 研究成果有助于探索混合动作理论中的因果关系，提供了新的分析工具和验证方法。

Abstract: Reasoning about actual causes of observed effects is fundamental to the study of rationality. This important problem has been studied since the time of Aristotle, with formal mathematical accounts emerging recently. We live in a world where change due to actions can be both discrete and continuous, that is, hybrid. Yet, despite extensive research on actual causation, only few recent studies looked into causation with continuous change. Building on recent progress, in this paper we propose two definitions of primary cause in a hybrid action-theoretic framework, namely the hybrid temporal situation calculus. One of these is foundational in nature while the other formalizes causation through contributions, which can then be verified from a counterfactual perspective using a modified ``but-for'' test. We prove that these two definitions are indeed equivalent. We then show that our definitions of causation have some intuitively justifiable properties.

</details>
