<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 90]
- [cs.CL](#cs.CL) [Total: 15]
- [cs.AI](#cs.AI) [Total: 19]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.PF](#cs.PF) [Total: 1]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Near-real time fires detection using satellite imagery in Sudan conflict](https://arxiv.org/abs/2512.07925)
*Kuldip Singh Atwal,Dieter Pfoser,Daniel Rothbart*

Main category: cs.CV

TL;DR: 通过使用Planet Labs的4波段图像和深度学习模型，该研究展示了在苏丹武装冲突中监控火灾损害的即时性，与基线方法相比，自动方法能够更准确地捕捉到活跃的火灾和烧伤区域。


<details>
  <summary>Details</summary>
Motivation: 当前苏丹冲突的持续性突显了对此类冲突进行快速监测和分析的需求。研究利用遥感图像和先进的深度学习技术，以实现近乎实时的冲突监测。

Method: 研究采用了Planet Labs的4波段卫星遥感图像，并应用了基于深度学习的方法来检测火灾损害。它通过五个案例研究展示了该方法的有效性。

Result: 研究结果表明，本自动化方法在捕捉活跃火灾及烧伤区域方面表现优于基线方法。然而，使用8波段图像或此类图像的时间序列不会带来显著改进。

Conclusion: 研究结论指出使用多波段遥感图像或其时间序列进行火灾损害监测存在局限性，进一步改善需要探索新的技术和数据集。

Abstract: The challenges of ongoing war in Sudan highlight the need for rapid moni- toring and analysis of such conflicts. Advances in deep learning and readily available satellite remote sensing imagery allow for near real-time monitor- ing. This paper uses 4-band imagery from Planet Labs with a deep learning model to show that fire damage in armed conflicts can be monitored with minimal delay. We demonstrate the effectiveness of our approach using five case studies in Sudan. We show that, compared to a baseline, the automated method captures the active fires and charred areas more accurately. Our re- sults indicate that using 8-band imagery or time series of such imagery only result in marginal gains.

</details>


### [2] [Preserving Source Video Realism: High-Fidelity Face Swapping for Cinematic Quality](https://arxiv.org/abs/2512.07951)
*Zekai Luo,Zongze Du,Zhouhang Zhu,Hao Zhong,Muzhi Zhu,Wen Wang,Yuling Xi,Chenchen Jing,Hao Chen,Chunhua Shen*

Main category: cs.CV

TL;DR: LivingSwap 是一种基于视频参考引导的面部替换模型，通过使用关键帧作为条件信号，实现了高质量且时间一致的面部替换效果。


<details>
  <summary>Details</summary>
Motivation: 为了在复杂的视频序列中实现高保真度和时间一致性，在电影和娱乐生产中进行视频面部替换仍然是一个重大挑战。本文探讨并利用参考视频中的丰富视觉属性，以增强面部替换的保真度和时间连贯性。

Method: LivingSwap 使用关键帧作为条件信号来注入目标身份，并结合视频参考引导进行时间缝合，确保在长时间视频序列中稳定的身份保持和高保真度重建。

Result: LivingSwap 在大量实验中表现出最优的结果，能够在保留目标身份的表情、光线和动作的同时，无缝地与源视频画面相结合，大幅减少生产流程中的手工操作。

Conclusion: 通过构造配对的面部替换数据集 Face2Face，并进一步反转数据对以确保可靠的真实监督，本文展示了 LivingSwap 在参考引导训练中的优越性能。

Abstract: Video face swapping is crucial in film and entertainment production, where achieving high fidelity and temporal consistency over long and complex video sequences remains a significant challenge. Inspired by recent advances in reference-guided image editing, we explore whether rich visual attributes from source videos can be similarly leveraged to enhance both fidelity and temporal coherence in video face swapping. Building on this insight, this work presents LivingSwap, the first video reference guided face swapping model. Our approach employs keyframes as conditioning signals to inject the target identity, enabling flexible and controllable editing. By combining keyframe conditioning with video reference guidance, the model performs temporal stitching to ensure stable identity preservation and high-fidelity reconstruction across long video sequences. To address the scarcity of data for reference-guided training, we construct a paired face-swapping dataset, Face2Face, and further reverse the data pairs to ensure reliable ground-truth supervision. Extensive experiments demonstrate that our method achieves state-of-the-art results, seamlessly integrating the target identity with the source video's expressions, lighting, and motion, while significantly reducing manual effort in production workflows. Project webpage: https://aim-uofa.github.io/LivingSwap

</details>


### [3] [FRIEDA: Benchmarking Multi-Step Cartographic Reasoning in Vision-Language Models](https://arxiv.org/abs/2512.08016)
*Jiyoon Pyo,Yuankun Jiao,Dongwon Jung,Zekun Li,Leeje Jang,Sofia Kirsanova,Jina Kim,Yijun Lin,Qin Liu,Junyi Xie,Hadi Askari,Nan Xu,Muhao Chen,Yao-Yi Chiang*

Main category: cs.CV

TL;DR: FRIEDA提出了一个基准测试，以评价大型视觉语言模型在复杂、开放性地理推理任务上的能力，尤其关注跨地图的多步骤推理，结果显示现有模型在这个领域的表现尚不理想。


<details>
  <summary>Details</summary>
Motivation: 当前大视觉语言模型对地理信息的理解存在不足，尤其是在处理跨图层、跨地图的多步推理问题时。FRIEDA旨在弥补这一空白。

Method: FRIEDA通过从实际文档和报告中提取地图图像，并根据地理信息系统（GIS）文献中的分类，设计了一个包含拓扑、度量和定向三种类型的空间关系的数据集。

Result: 在FRIEDA上，最先进的模型如Gemini-2.5-Pro和GPT-5-Think的表现仅分别达到了38.20%和37.20%的准确率，远低于人类表现（84.87%）。

Conclusion: 研究揭示了多步骤地理推理的持续性差距，表明有必要进一步推动大型视觉语言模型在空间智能方面的进步。

Abstract: Cartographic reasoning is the skill of interpreting geographic relationships by aligning legends, map scales, compass directions, map texts, and geometries across one or more map images. Although essential as a concrete cognitive capability and for critical tasks such as disaster response and urban planning, it remains largely unevaluated. Building on progress in chart and infographic understanding, recent large vision language model studies on map visual question-answering often treat maps as a special case of charts. In contrast, map VQA demands comprehension of layered symbology (e.g., symbols, geometries, and text labels) as well as spatial relations tied to orientation and distance that often span multiple maps and are not captured by chart-style evaluations. To address this gap, we introduce FRIEDA, a benchmark for testing complex open-ended cartographic reasoning in LVLMs. FRIEDA sources real map images from documents and reports in various domains and geographical areas. Following classifications in Geographic Information System (GIS) literature, FRIEDA targets all three categories of spatial relations: topological (border, equal, intersect, within), metric (distance), and directional (orientation). All questions require multi-step inference, and many require cross-map grounding and reasoning. We evaluate eleven state-of-the-art LVLMs under two settings: (1) the direct setting, where we provide the maps relevant to the question, and (2) the contextual setting, where the model may have to identify the maps relevant to the question before reasoning. Even the strongest models, Gemini-2.5-Pro and GPT-5-Think, achieve only 38.20% and 37.20% accuracy, respectively, far below human performance of 84.87%. These results reveal a persistent gap in multi-step cartographic reasoning, positioning FRIEDA as a rigorous benchmark to drive progress on spatial intelligence in LVLMs.

</details>


### [4] [SSplain: Sparse and Smooth Explainer for Retinopathy of Prematurity Classification](https://arxiv.org/abs/2512.08038)
*Elifnur Sunger,Tales Imbiriba,Peter Campbell,Deniz Erdogmus,Stratis Ioannidis,Jennifer Dy*

Main category: cs.CV

TL;DR: 该论文提出了一种名为SSplain的新解释方法，用于从眼底图像中分类早产儿视网膜病变（ROP）。SSplain通过优化问题和交替方向乘子法（ADMM），在保持图像结构平滑性和稀疏性的同时生成像素级解释，提高了模型的可解释性和准确性。


<details>
  <summary>Details</summary>
Motivation: 当前的模型解释方法难以同时满足保持输入图像结构和输出解释质量的需求，尤其是对于医疗诊断等需要强解释性的场景。

Method: SSplain采用了一种新的优化方法，在保持图像平滑性和稀疏性的前提下生成像素级别的解释。这种方法在算法层面上通过定义具有组合约束的优化问题，并利用交替方向乘子法（ADMM）来实现。

Result: 实验结果显示，SSplain在后处理准确性和平滑性分析方面优于常用的解释方法，并能够识别出与临床理解一致的区分性特征。

Conclusion: SSplain方法具有良好的泛化性，并且在多个公开数据集上进行了验证，能够提供有助于临床决策的高质量解释。

Abstract: Neural networks are frequently used in medical diagnosis. However, due to their black-box nature, model explainers are used to help clinicians understand better and trust model outputs. This paper introduces an explainer method for classifying Retinopathy of Prematurity (ROP) from fundus images. Previous methods fail to generate explanations that preserve input image structures such as smoothness and sparsity. We introduce Sparse and Smooth Explainer (SSplain), a method that generates pixel-wise explanations while preserving image structures by enforcing smoothness and sparsity. This results in realistic explanations to enhance the understanding of the given black-box model. To achieve this goal, we define an optimization problem with combinatorial constraints and solve it using the Alternating Direction Method of Multipliers (ADMM). Experimental results show that SSplain outperforms commonly used explainers in terms of both post-hoc accuracy and smoothness analyses. Additionally, SSplain identifies features that are consistent with domain-understandable features that clinicians consider as discriminative factors for ROP. We also show SSplain's generalization by applying it to additional publicly available datasets. Code is available at https://github.com/neu-spiral/SSplain.

</details>


### [5] [Towards Sustainable Universal Deepfake Detection with Frequency-Domain Masking](https://arxiv.org/abs/2512.08042)
*Chandler Timm C. Doloriel,Habib Ullah,Kristian Hovde Liland,Fadi Al Machot,Ngai-Man Cheung*

Main category: cs.CV

TL;DR: 本文提出了一种基于频域遮罩的训练策略，用于训练深度伪造检测器。这种方法通过引入随机遮罩和几何变换，特别利用频域遮罩的泛化性能，以提高不同生成器的检测精度，并在模型修剪下保持性能。


<details>
  <summary>Details</summary>
Motivation: 随着深度伪造技术的快速发展，检测新生成的深度伪造成为了一个关键问题。现有方法依赖于空间特征或大规模预训练模型，而本文通过频域遮罩方法，提供了一种减少计算开销并适应不同生成器的可持续深度伪造检测方案。

Method: 本文提出了基于频域遮罩的训练策略。通过引入随机遮罩和几何变换，特别利用频域遮罩的泛化优势，增强了检测器的适应性和鲁棒性。在大规模模型剪枝的情况下，依然能保持检测性能。

Result: 该方法在判别器和扩散生成的图像数据集上达到了最先进的泛化性能，并且在结构化剪枝下保持了一致的鲁棒性。

Conclusion: 基于频域遮罩的训练策略为深度伪造检测提供了一种可持续和可泛化的解决方案，具有广泛的应用前景。

Abstract: Universal deepfake detection aims to identify AI-generated images across a broad range of generative models, including unseen ones. This requires robust generalization to new and unseen deepfakes, which emerge frequently, while minimizing computational overhead to enable large-scale deepfake screening, a critical objective in the era of Green AI. In this work, we explore frequency-domain masking as a training strategy for deepfake detectors. Unlike traditional methods that rely heavily on spatial features or large-scale pretrained models, our approach introduces random masking and geometric transformations, with a focus on frequency masking due to its superior generalization properties. We demonstrate that frequency masking not only enhances detection accuracy across diverse generators but also maintains performance under significant model pruning, offering a scalable and resource-conscious solution. Our method achieves state-of-the-art generalization on GAN- and diffusion-generated image datasets and exhibits consistent robustness under structured pruning. These results highlight the potential of frequency-based masking as a practical step toward sustainable and generalizable deepfake detection. Code and models are available at: [https://github.com/chandlerbing65nm/FakeImageDetection](https://github.com/chandlerbing65nm/FakeImageDetection).

</details>


### [6] [Mask to Adapt: Simple Random Masking Enables Robust Continual Test-Time Learning](https://arxiv.org/abs/2512.08048)
*Chandler Timm C. Doloriel*

Main category: cs.CV

TL;DR: 介绍了Mask to Adapt (M2A)方法，通过生成一系列被遮罩的视图（空间或频域）并采用一致性损失和熵最小化损失来实现有效的测试时适应，而不需要依赖于校准的不确定性或注意力分数。


<details>
  <summary>Details</summary>
Motivation: 近期的持续测试时适应（CTTA）方法依赖于校准不确定性和稳定的注意力评分，这可能增加复杂性。因此，本文旨在探究是否可以仅使用简单的随机遮罩策略实现有效的测试时适应。

Method: M2A 方法生成了一系列被遮罩的视图（空间或频域），并通过一致性损失和熵最小化损失来适应模型，进一步研究了空间遮罩和频率遮罩的不同子类型。

Result: 在 CIFAR10C、CIFAR100C 和 ImageNetC 数据集上进行测试，M2A（空间）方法取得了较高的性能，优于或匹配了强大的 CTTA 基线，而 M2A（频率）方法表现较弱。消融研究进一步表明，简单的随机遮罩策略是有效的和鲁棒的。

Conclusion: 研究表明，简单的随机遮罩策略与一致性损失和熵最小化损失相结合可以实现有效的测试时适应，无需依赖不确定性或注意力信号。

Abstract: Distribution shifts at test time degrade image classifiers. Recent continual test-time adaptation (CTTA) methods use masking to regulate learning, but often depend on calibrated uncertainty or stable attention scores and introduce added complexity. We ask: do we need custom-made masking designs, or can a simple random masking schedule suffice under strong corruption? We introduce Mask to Adapt (M2A), a simple CTTA approach that generates a short sequence of masked views (spatial or frequency) and adapts with two objectives: a mask consistency loss that aligns predictions across different views and an entropy minimization loss that encourages confident outputs. Motivated by masked image modeling, we study two common masking families -- spatial masking and frequency masking -- and further compare subtypes within each (spatial: patch vs.\ pixel; frequency: all vs.\ low vs.\ high). On CIFAR10C/CIFAR100C/ImageNetC (severity~5), M2A (Spatial) attains 8.3\%/19.8\%/39.2\% mean error, outperforming or matching strong CTTA baselines, while M2A (Frequency) lags behind. Ablations further show that simple random masking is effective and robust. These results indicate that a simple random masking schedule, coupled with consistency and entropy objectives, is sufficient to drive effective test-time adaptation without relying on uncertainty or attention signals.

</details>


### [7] [Identification of Deforestation Areas in the Amazon Rainforest Using Change Detection Models](https://arxiv.org/abs/2512.08075)
*Christian Massao Konishi,Helio Pedrini*

Main category: cs.CV

TL;DR: 本文研究了利用PRODES数据开发基于卫星图像变化检测的机器学习模型，评估了多种模型的有效性，并通过预处理技术和模型融合策略提升了整体性能。


<details>
  <summary>Details</summary>
Motivation: 为了更好地保护亚马逊雨林，应对雨林退化的挑战。尽管现有的方法存在不足，该文章旨在利用更先进的模型和技术提供有效的监测手段。

Method: 研究通过评估包含完整卷积模型和基于Transformer的自注意力机制的多种模型，使用统一的数据集，进行预处理测试，如基于连通区域大小筛选预测的退化区域，纹理替换和影像增强，以及模型融合策略。

Result: 研究发现预处理和模型融合策略显著提升了模型的检测效果，F1分数达到了80.41%，达到了之前研究中的较高水平。

Conclusion: 本文的研究成果表明，在亚马逊雨林变化检测领域，通过改进现有技术可以更有效地监测和控制退化的现象。

Abstract: The preservation of the Amazon Rainforest is one of the global priorities in combating climate change, protecting biodiversity, and safeguarding indigenous cultures. The Satellite-based Monitoring Project of Deforestation in the Brazilian Legal Amazon (PRODES), a project of the National Institute for Space Research (INPE), stands out as a fundamental initiative in this effort, annually monitoring deforested areas not only in the Amazon but also in other Brazilian biomes. Recently, machine learning models have been developed using PRODES data to support this effort through the comparative analysis of multitemporal satellite images, treating deforestation detection as a change detection problem. However, existing approaches present significant limitations: models evaluated in the literature still show unsatisfactory effectiveness, many do not incorporate modern architectures, such as those based on self-attention mechanisms, and there is a lack of methodological standardization that allows direct comparisons between different studies. In this work, we address these gaps by evaluating various change detection models in a unified dataset, including fully convolutional models and networks incorporating self-attention mechanisms based on Transformers. We investigate the impact of different pre- and post-processing techniques, such as filtering deforested areas predicted by the models based on the size of connected components, texture replacement, and image enhancements; we demonstrate that such approaches can significantly improve individual model effectiveness. Additionally, we test different strategies for combining the evaluated models to achieve results superior to those obtained individually, reaching an F1-score of 80.41%, a value comparable to other recent works in the literature.

</details>


### [8] [CVP: Central-Peripheral Vision-Inspired Multimodal Model for Spatial Reasoning](https://arxiv.org/abs/2512.08135)
*Zeyuan Chen,Xiang Zhang,Haiyang Xu,Jianwen Xie,Zhuowen Tu*

Main category: cs.CV

TL;DR: 提出了一种新颖的中央-外围视觉启发式框架（CVP），它引入了两个互补组件来改善大型多模态模型的空间推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于点云、体素或补丁特征等无结构表示，并通过坐标嵌入隐式地注入场景语境，但这种方式通常会导致有限的空间推理能力，因为缺乏明确的高层结构理解。

Method: 在大型多模态模型的基础上引入了目标亲和标记和全观坐标网格两个组件，分别模拟中央视觉和外周视觉，以实现结构化、情境感知的复杂3D环境理解。

Result: CVP在一系列3D场景理解基准测试中取得了领先性能。

Conclusion: 此框架展示了利用人类视觉系统原理进行空间推理的新可能性，为未来的多模态3D场景理解提供了有价值的工具。

Abstract: We present a central-peripheral vision-inspired framework (CVP), a simple yet effective multimodal model for spatial reasoning that draws inspiration from the two types of human visual fields -- central vision and peripheral vision. Existing approaches primarily rely on unstructured representations, such as point clouds, voxels, or patch features, and inject scene context implicitly via coordinate embeddings. However, this often results in limited spatial reasoning capabilities due to the lack of explicit, high-level structural understanding. To address this limitation, we introduce two complementary components into a Large Multimodal Model-based architecture: target-affinity token, analogous to central vision, that guides the model's attention toward query-relevant objects; and allocentric grid, akin to peripheral vision, that captures global scene context and spatial arrangements. These components work in tandem to enable structured, context-aware understanding of complex 3D environments. Experiments show that CVP achieves state-of-the-art performance across a range of 3D scene understanding benchmarks.

</details>


### [9] [Fourier-RWKV: A Multi-State Perception Network for Efficient Image Dehazing](https://arxiv.org/abs/2512.08161)
*Lirong Zheng,Yanshan Li,Rui Yu,Kaihao Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种名为Fourier Receptance Weighted Key Value (Fourier-RWKV)的新框架，用于解决现实环境中非均匀遮蔽条件下的去雾问题。该框架通过结合空间形式感知、频域感知和语义关系感知，实现了线性复杂度的全面遮蔽退化建模。实验结果显示，Fourier-RWKV 在多种基准测试上表现出优越性能，同时显著降低了计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有的去雾方法虽然在某些方面表现出色，但仍然难以应对现实世界中非均匀遮蔽条件下的去雾问题。鉴于此，本文提出了Fourier-RWKV框架，旨在同时提高去雾的质量和降低计算复杂度。

Method: Fourier-RWKV框架包含三种感知组件：空间形式感知 (通过DQ-Shift操作实现)，频域感知 (在Fourier Mix块中实现)，以及语义关系感知 (通过SBM和DSK-Fusion实现)。这些组件共同作用，使模型能够有效地建模复杂遮蔽现象，同时保持线性推理复杂度。

Result: 实验结果表明，Fourier-RWKV在多个基准平台上均取得了最先进的性能，同时大大降低了计算复杂度，实现了高质量恢复与实用效率之间的良好平衡。

Conclusion: 本文提出的 Fourier-RWKV 是一个有效的实时去雾解决方案，将在推广真实环境中应用于视觉感知任务。

Abstract: Image dehazing is crucial for reliable visual perception, yet it remains highly challenging under real-world non-uniform haze conditions. Although Transformer-based methods excel at capturing global context, their quadratic computational complexity hinders real-time deployment. To address this, we propose Fourier Receptance Weighted Key Value (Fourier-RWKV), a novel dehazing framework based on a Multi-State Perception paradigm. The model achieves comprehensive haze degradation modeling with linear complexity by synergistically integrating three distinct perceptual states: (1) Spatial-form Perception, realized through the Deformable Quad-directional Token Shift (DQ-Shift) operation, which dynamically adjusts receptive fields to accommodate local haze variations; (2) Frequency-domain Perception, implemented within the Fourier Mix block, which extends the core WKV attention mechanism of RWKV from the spatial domain to the Fourier domain, preserving the long-range dependencies essential for global haze estimation while mitigating spatial attenuation; (3) Semantic-relation Perception, facilitated by the Semantic Bridge Module (SBM), which utilizes Dynamic Semantic Kernel Fusion (DSK-Fusion) to precisely align encoder-decoder features and suppress artifacts. Extensive experiments on multiple benchmarks demonstrate that Fourier-RWKV delivers state-of-the-art performance across diverse haze scenarios while significantly reducing computational overhead, establishing a favorable trade-off between restoration quality and practical efficiency. Code is available at: https://github.com/Dilizlr/Fourier-RWKV.

</details>


### [10] [Accuracy Does Not Guarantee Human-Likeness in Monocular Depth Estimators](https://arxiv.org/abs/2512.08163)
*Yuki Kubota,Taiki Fukiage*

Main category: cs.CV

TL;DR: 本研究通过分析69种单目深度估计模型在KITTI数据集上的表现，揭示了模型准确性和人类感知之间的关系。尽管模型和人类在某些方面的估计偏差相似，但模型准确性和人类感知之间的权衡关系有所不同，表明提高模型准确度并不一定能够实现更类似人类的行为。因此，需要发展多元化的、以人为本的评估方法。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在探讨深度估计模型的准确性与人类感知之间的权衡关系，以及如何使模型的结果更接近人类的感知。

Method: 研究者通过系统地分析69种单目深度估计模型在KITTI数据集上的表现，并应用形变换拟合方法将预测误差分解成可解释的组件来进行研究。

Result: 研究表明，模型和人类在某些方面的估计偏差相似，但模型准确性和人类感知之间的权衡关系有所不同。形变换拟合显示，提高模型准确度并不一定能实现更类似人类的行为。

Conclusion: 该研究强调了除了传统准确性评估外，发展多元化、以人为本评估方法的重要性，以帮助模型结果更接近人类感知。

Abstract: Monocular depth estimation is a fundamental capability for real-world applications such as autonomous driving and robotics. Although deep neural networks (DNNs) have achieved superhuman accuracy on physical-based benchmarks, a key challenge remains: aligning model representations with human perception, a promising strategy for enhancing model robustness and interpretability. Research in object recognition has revealed a complex trade-off between model accuracy and human-like behavior, raising a question whether a similar divergence exist in depth estimation, particularly for natural outdoor scenes where benchmarks rely on sensor-based ground truth rather than human perceptual estimates. In this study, we systematically investigated the relationship between model accuracy and human similarity across 69 monocular depth estimators using the KITTI dataset. To dissect the structure of error patterns on a factor-by-factor basis, we applied affine fitting to decompose prediction errors into interpretable components. Intriguingly, our results reveal while humans and DNNs share certain estimation biases (positive error correlations), we observed distinct trade-off relationships between model accuracy and human similarity. This finding indicates that improving accuracy does not necessarily lead to more human-like behavior, underscoring the necessity of developing multifaceted, human-centric evaluations beyond traditional accuracy.

</details>


### [11] [GeoLoom: High-quality Geometric Diagram Generation from Textual Input](https://arxiv.org/abs/2512.08180)
*Xiaojing Wei,Ting Zhang,Wei He,Jingdong Wang,Hua Huang*

Main category: cs.CV

TL;DR: GeoLoom 提出了一种用于几何域的文本到图形生成的新框架，通过将自然语言转化为特定形式的几何语言 GeoLingua，并利用高效蒙特卡洛优化将形式约束映射到精确坐标，GeoLoom 显著提高了结构保真度，超越了现有基线。


<details>
  <summary>Details</summary>
Motivation: 几何图形的高质量生成既是挑战也是机会，它需要严格的空间准确性但同时提供明确定义的约束来引导生成。GeoLoom 旨在实现这一目标，通过结合形式语言和符号求解器的技术，增强生成的正确性和可解释性。

Method: GeoLoom 包含两个核心组件：自动格式化模块将自然语言翻译为专为生成设计的正式语言 GeoLingua，以及坐标求解器将形式约束映射到精确坐标。此外，还引入了 GeoNF 数据集，其中包含了自然语言几何描述与正式的 GeoLingua 描述的对照。

Result: 实验结果表明，GeoLoom 在结构保真度上显著优于最新基线，为理解性和可扩展的图形生成提供了坚实的理论基础。

Conclusion: 因此，GeoLoom 代表了一种创新的方法，用于几何域中的图形生成，通过数学验证的监督机制，支持迭代优化过程，从而实现高质量的生成结果。

Abstract: High-quality geometric diagram generation presents both a challenge and an opportunity: it demands strict spatial accuracy while offering well-defined constraints to guide generation. Inspired by recent advances in geometry problem solving that employ formal languages and symbolic solvers for enhanced correctness and interpretability, we propose GeoLoom, a novel framework for text-to-diagram generation in geometric domains. GeoLoom comprises two core components: an autoformalization module that translates natural language into a specifically designed generation-oriented formal language GeoLingua, and a coordinate solver that maps formal constraints to precise coordinates using the efficient Monte Carlo optimization. To support this framework, we introduce GeoNF, a dataset aligning natural language geometric descriptions with formal GeoLingua descriptions. We further propose a constraint-based evaluation metric that quantifies structural deviation, offering mathematically grounded supervision for iterative refinement. Empirical results demonstrate that GeoLoom significantly outperforms state-of-the-art baselines in structural fidelity, providing a principled foundation for interpretable and scalable diagram generation.

</details>


### [12] [VisKnow: Constructing Visual Knowledge Base for Object Understanding](https://arxiv.org/abs/2512.08221)
*Ziwei Yao,Qiyang Wan,Ruiping Wang,Xilin Chen*

Main category: cs.CV

TL;DR: 该研究提出了一种视觉知识库，通过构建图结构化的多模态对象知识，并提出VisKnow框架提取对象级别的多模态知识，以增强对象理解和视觉任务，特别是零样本识别和细粒度VQA。


<details>
  <summary>Details</summary>
Motivation: 当前对象理解数据往往是面向特定任务的，缺乏系统组织，不足以实现对对象类别的深入理解。因此，需要一个可以结构化多模态对象知识并支持高级任务（如推理和问答）的框架。

Method: VisKnow框架结合专家设计和大规模模型应用，从文本和图像源中提取多层次（对象和部件）的多模态知识，并将其结构化为图。

Result: 该研究构建了AnimalKB，一个涵盖406个动物类别的结构化动物知识库，包含22K文本知识三元组和420K图像及注释。实验证明AnimalKB能够提升零样本识别和细粒度VQA效果，且为知识图谱补全和部件分割的任务提供了具有挑战性的基准。

Conclusion: 研究展示了自动构建视觉知识库的潜力，可以促进视觉理解和其实际应用的发展。

Abstract: Understanding objects is fundamental to computer vision. Beyond object recognition that provides only a category label as typical output, in-depth object understanding represents a comprehensive perception of an object category, involving its components, appearance characteristics, inter-category relationships, contextual background knowledge, etc. Developing such capability requires sufficient multi-modal data, including visual annotations such as parts, attributes, and co-occurrences for specific tasks, as well as textual knowledge to support high-level tasks like reasoning and question answering. However, these data are generally task-oriented and not systematically organized enough to achieve the expected understanding of object categories. In response, we propose the Visual Knowledge Base that structures multi-modal object knowledge as graphs, and present a construction framework named VisKnow that extracts multi-modal, object-level knowledge for object understanding. This framework integrates enriched aligned text and image-source knowledge with region annotations at both object and part levels through a combination of expert design and large-scale model application. As a specific case study, we construct AnimalKB, a structured animal knowledge base covering 406 animal categories, which contains 22K textual knowledge triplets extracted from encyclopedic documents, 420K images, and corresponding region annotations. A series of experiments showcase how AnimalKB enhances object-level visual tasks such as zero-shot recognition and fine-grained VQA, and serves as challenging benchmarks for knowledge graph completion and part segmentation. Our findings highlight the potential of automatically constructing visual knowledge bases to advance visual understanding and its practical applications. The project page is available at https://vipl-vsu.github.io/VisKnow.

</details>


### [13] [SOP^2: Transfer Learning with Scene-Oriented Prompt Pool on 3D Object Detection](https://arxiv.org/abs/2512.08223)
*Ching-Hung Cheng,Hsiu-Fu Wu,Bing-Chen Wu,Khanh-Phong Bui,Van-Tin Luu,Ching-Chun Huang*

Main category: cs.CV

TL;DR: 本文探讨了通用提示调优方法在3D物体检测中的效果，提出了场景导向的提示池（SOP²），展示了提示池在3D物体检测中的有效性。


<details>
  <summary>Details</summary>
Motivation: 随着大规模语言模型（如GPT-3）的兴起，这些模型具有强大的泛化能力。通过微调等迁移学习技术，它们能够通过很少的参数调整适应各种下游任务，特别是自然语言处理（NLP）领域。本文目的是研究这些提示调优方法在3D物体检测中的应用潜力。

Method: 本文使用了从大规模Waymo数据集训练的模型作为基础模型，并研究了提示令牌和提示生成器的作用，进一步提出了场景导向的提示池（SOP²）。实验旨在探索提示池在3D物体检测中的效果。

Result: 研究表明，提示池在3D物体检测中表现出显著效果，且提出了一个具有潜力的新方法SOP²。

Conclusion: 提示池可以有效提高3D物体检测的性能，展示了其在3D领域中的应用前景，将激励未来的研究深入发掘提示的潜力。

Abstract: With the rise of Large Language Models (LLMs) such as GPT-3, these models exhibit strong generalization capabilities. Through transfer learning techniques such as fine-tuning and prompt tuning, they can be adapted to various downstream tasks with minimal parameter adjustments. This approach is particularly common in the field of Natural Language Processing (NLP). This paper aims to explore the effectiveness of common prompt tuning methods in 3D object detection. We investigate whether a model trained on the large-scale Waymo dataset can serve as a foundation model and adapt to other scenarios within the 3D object detection field. This paper sequentially examines the impact of prompt tokens and prompt generators, and further proposes a Scene-Oriented Prompt Pool (\textbf{SOP$^2$}). We demonstrate the effectiveness of prompt pools in 3D object detection, with the goal of inspiring future researchers to delve deeper into the potential of prompts in the 3D field.

</details>


### [14] [MM-CoT:A Benchmark for Probing Visual Chain-of-Thought Reasoning in Multimodal Models](https://arxiv.org/abs/2512.08228)
*Jusheng Zhang,Kaitong Cai,Xiaoyang Guo,Sidi Liu,Qinhan Lv,Ruiqi Chen,Jing Yang,Yijia Fan,Xiaofei Sun,Jian Wang,Ziliang Chen,Liang Lin,Keze Wang*

Main category: cs.CV

TL;DR: MM-CoT 是一个针对多模态模型（MMs）的诊断基准，专门用于检验其在视觉推理中的可观察一致性和逻辑连贯性。虽然最新的视觉-语言模型在生成流畅的解释方面表现出色，但在确保可观察一致性和逻辑有效性方面却表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要侧重于生成，忽视了验证，即评估推理链条是否在视觉上一致且逻辑上有效。MM-CoT 旨在填补这一空白，通过设计要满足视觉一致性和逻辑连贯性的模型，检测它们的视觉推理能力。

Method: MM-CoT 是通过一种区分性的设计来实现的。在选择事件链时必须同时满足两个正交约束条件：视觉一致性和逻辑连贯性。引入了对抗性干扰项来违反其中一个约束，从而揭示模型的推理失败。

Result: 现有的最先进的视觉-语言模型在 MM-CoT 上表现出色，表明它们在生成流畅的解释方面具有很强的能力。然而，在确保可观察一致性和逻辑有效性方面存在明显的缺口，这说明现有的评估标准需要补充，包括对视觉推理和逻辑推理的评估。

Conclusion: MM-CoT 提供了一个新的基准，用于更准确地评估模型的视觉推理能力。结论表明，未来的模型应该不仅在视觉推理上可信和连贯，而且还要与视觉现实相吻合。

Abstract: The ability to perform Chain-of-Thought (CoT) reasoning marks a major milestone for multimodal models (MMs), enabling them to solve complex visual reasoning problems. Yet a critical question remains: is such reasoning genuinely grounded in visual evidence and logically coherent? Existing benchmarks emphasize generation but neglect verification, i.e., the capacity to assess whether a reasoning chain is both visually consistent and logically valid. To fill this gap, we introduce MM-CoT, a diagnostic benchmark specifically designed to probe the visual grounding and logical coherence of CoT reasoning in MMs. Instead of generating free-form explanations, models must select the sole event chain that satisfies two orthogonal constraints: (i) visual consistency, ensuring all steps are anchored in observable evidence, and (ii) logical coherence, ensuring causal and commonsense validity. Adversarial distractors are engineered to violate one of these constraints, exposing distinct reasoning failures. We evaluate leading vision-language models on MM-CoT and find that even the most advanced systems struggle, revealing a sharp discrepancy between generative fluency and true reasoning fidelity. MM-CoT shows low correlation with existing benchmarks, confirming that it measures a unique combination of visual grounding and logical reasoning. This benchmark provides a foundation for developing future models that reason not just plausibly, but faithfully and coherently within the visual world.

</details>


### [15] [Geometry-Aware Sparse Depth Sampling for High-Fidelity RGB-D Depth Completion in Robotic Systems](https://arxiv.org/abs/2512.08229)
*Tony Salloom,Dandi Zhou,Xinhai Sun*

Main category: cs.CV

TL;DR: 本文提出了一种基于法线引导的稀疏深度采样策略，通过利用RGB-D点云上的PCA法线估计来计算每个像素的深度可靠性度量，并据此分布进行稀疏深度采样。这种方法与Marigold-DC扩散型深度完成模型结合，实验表明这种几何感知的稀疏深度能提高准确性，减少边缘和不连续性附近的伪影，并更好地反映真实传感器的行为。


<details>
  <summary>Details</summary>
Motivation: 当前深度完成管道的一个主要限制是生成不现实的稀疏深度：稀疏像素通常从稠密的真实深度图中随机选择，忽略了传感器具有几何依赖性和空间非均匀可靠性的事实。本文旨在解决这一问题。

Method: 提出了一种基于法线引导的稀疏深度采样策略，通过计算每个像素的深度可靠性度量，并利用PCA法线估计在RGB-D点云上将其分布到稀疏深度采样中。该方法与Marigold-DC扩散型深度完成模型结合。

Result: 该方法在NYU Depth v2上的实验表明，几何感知的稀疏深度提高了准确性，减少了边缘和不连续性附近的伪影，并产生了更接近真实传感器行为的训练条件。

Conclusion: 本文提出了一种新的稀疏深度采样方法，该方法通过考虑深度可靠性的几何依赖性提高了深度完成的性能。

Abstract: Accurate three-dimensional perception is essential for modern industrial robotic systems that perform manipulation, inspection, and navigation tasks. RGB-D and stereo vision sensors are widely used for this purpose, but the depth maps they produce are often noisy, incomplete, or biased due to sensor limitations and environmental conditions. Depth completion methods aim to generate dense, reliable depth maps from RGB images and sparse depth input. However, a key limitation in current depth completion pipelines is the unrealistic generation of sparse depth: sparse pixels are typically selected uniformly at random from dense ground-truth depth, ignoring the fact that real sensors exhibit geometry-dependent and spatially nonuniform reliability. In this work, we propose a normal-guided sparse depth sampling strategy that leverages PCA-based surface normal estimation on the RGB-D point cloud to compute a per-pixel depth reliability measure. The sparse depth samples are then drawn according to this reliability distribution. We integrate this sampling method with the Marigold-DC diffusion-based depth completion model and evaluate it on NYU Depth v2 using the standard metrics. Experiments show that our geometry-aware sparse depth improves accuracy, reduces artifacts near edges and discontinuities, and produces more realistic training conditions that better reflect real sensor behavior.

</details>


### [16] [FastBEV++: Fast by Algorithm, Deployable by Design](https://arxiv.org/abs/2512.08237)
*Yuanpeng Chen,Hui Song,Wei Tao,ShanHui Mo,Shuang Zhang,Xiao Hua,TianKun Zhao*

Main category: cs.CV

TL;DR: FastBEV++提出了一种框架，通过算法层面和设计层面的优化，实现了高性能且易于部署的BEV感知。它通过将投影分解为标准的索引-聚集-重塑管道，并采用端到端的深度感知融合机制，显著提升了BEV表示的几何精度。


<details>
  <summary>Details</summary>
Motivation: 当前，由于计算成本高昂的视图变换和平台特定的内核依赖，相机仅的Bird's Eye View(BEV)感知技术面临着一个根本性的瓶颈，即高性能和车载部署效率之间的矛盾。FastBEV++旨在通过算法和设计双重优化，解决这一问题，以平衡高性能和高效部署。

Method: FastBEV++提出了一种独特的视图变换方法，将投影解构成索引-聚集-重塑标准管道，并利用确定性的预排序策略，确保操作原语（例如Gather, 矩阵乘法）的完全执行。同时，该框架采用端到端深度感知融合机制，包括联合学习的深度调制、时间聚合和强数据增强等特性，提高了BEV表示的几何精度。

Result: FastBEV++在nuScenes基准上建立了新的最先进的0.359 NDS分数，同时保持了优秀的实时性能，超过134 FPS在汽车级硬件（如Tesla T4）上运行。该方法不仅免于使用专门为CUDA设计的内核，还提供了成熟且可扩展的设计策略，适用于生产自动驾驶系统。

Conclusion: FastBEV++提供了一种平衡高效和高准确性的方法，具有广泛应用潜力，尤其适合作为用于生产级自主驾驶系统的框架模板。

Abstract: The advancement of camera-only Bird's-Eye-View(BEV) perception is currently impeded by a fundamental tension between state-of-the-art performance and on-vehicle deployment tractability. This bottleneck stems from a deep-rooted dependency on computationally prohibitive view transformations and bespoke, platform-specific kernels. This paper introduces FastBEV++, a framework engineered to reconcile this tension, demonstrating that high performance and deployment efficiency can be achieved in unison via two guiding principles: Fast by Algorithm and Deployable by Design. We realize the "Deployable by Design" principle through a novel view transformation paradigm that decomposes the monolithic projection into a standard Index-Gather-Reshape pipeline. Enabled by a deterministic pre-sorting strategy, this transformation is executed entirely with elementary, operator native primitives (e.g Gather, Matrix Multiplication), which eliminates the need for specialized CUDA kernels and ensures fully TensorRT-native portability. Concurrently, our framework is "Fast by Algorithm", leveraging this decomposed structure to seamlessly integrate an end-to-end, depth-aware fusion mechanism. This jointly learned depth modulation, further bolstered by temporal aggregation and robust data augmentation, significantly enhances the geometric fidelity of the BEV representation.Empirical validation on the nuScenes benchmark corroborates the efficacy of our approach. FastBEV++ establishes a new state-of-the-art 0.359 NDS while maintaining exceptional real-time performance, exceeding 134 FPS on automotive-grade hardware (e.g Tesla T4). By offering a solution that is free of custom plugins yet highly accurate, FastBEV++ presents a mature and scalable design philosophy for production autonomous systems. The code is released at: https://github.com/ymlab/advanced-fastbev

</details>


### [17] [Residual-SwinCA-Net: A Channel-Aware Integrated Residual CNN-Swin Transformer for Malignant Lesion Segmentation in BUSI](https://arxiv.org/abs/2512.08243)
*Saeeda Naz,Saddam Hussain Khan*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的深度Hybrid Residual-SwinCA-Net分割框架，通过自定义Swin Transformer块，增强局部和全局特征，并结合了多种技术提高图像分割效果。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决现有的图像分割方法在提取局部相关性和鲁棒特征方面的一些挑战，特别是在学习全局依赖性、抑制噪声和强化结构过渡等方面。

Method: 该方法采用了自定义的Swin Transformer块，结合局部操作和多尺度通道注意力机制，增强了模型对细微结构的捕捉能力。此外，还通过逐步减少特征图来捕获尺度不变性，并通过像素注意力模块增强了恶性病变的区分度。

Result: 实验结果表明，Residual-SwinCA-Net在具有挑战性的BUSI数据集上实现了99.29%的平均准确率，98.74%的IoU和0.9041的Dice系数，显著优于现有技术。

Conclusion: 该研究提出了Residual-SwinCA-Net框架，以提高乳腺病变分割的性能，该框架改善了BUSI病变诊断性能，并有助于及时的临床决策。

Abstract: A novel deep hybrid Residual-SwinCA-Net segmentation framework is proposed in the study for addressing such challenges by extracting locally correlated and robust features, incorporating residual CNN modules. Furthermore, for learning global dependencies, Swin Transformer blocks are customized using internal residual pathways, which reinforce gradient stability, refine local patterns, and facilitate global feature fusion. Formerly, for enhancing tissue continuity, ultrasound noise suppressions, and accentuating fine structural transitions Laplacian-of-Gaussian regional operator is applied, and for maintaining the morphological integrity of malignant lesion contours, a boundary-oriented operator has been incorporated. Subsequently, a contraction strategy was applied stage-wise by progressively reducing features-map progressively for capturing scale invariance and enhancing the robustness of structural variability. In addition, each decoder level prior augmentation integrates a new Multi-Scale Channel Attention and Squeezing (MSCAS) module. The MSCAS selectively emphasizes encoder salient maps, retains discriminative global context, and complementary local structures with minimal computational cost while suppressing redundant activations. Finally, the Pixel-Attention module encodes class-relevant spatial cues by adaptively weighing malignant lesion pixels while suppressing background interference. The Residual-SwinCA-Net and existing CNNs/ViTs techniques have been implemented on the publicly available BUSI dataset. The proposed Residual-SwinCA-Net framework outperformed and achieved 99.29% mean accuracy, 98.74% IoU, and 0.9041 Dice for breast lesion segmentation. The proposed Residual-SwinCA-Net framework improves the BUSI lesion diagnostic performance and strengthens timely clinical decision-making.

</details>


### [18] [Distilling Future Temporal Knowledge with Masked Feature Reconstruction for 3D Object Detection](https://arxiv.org/abs/2512.08247)
*Haowen Zheng,Hu Zhu,Lu Deng,Weihao Gu,Yang Yang,Yanyan Liang*

Main category: cs.CV

TL;DR: Future Temporal Knowledge Distillation (FTKD) proposes a sparse query-based approach to transfer future frame knowledge from offline teachers to online students, improving 3D object detection accuracy by 1.3 mAP and 1.3 NDS on the nuScenes dataset.


<details>
  <summary>Details</summary>
Motivation: 为了解决现有知识蒸馏方法忽略了未来帧的问题，FTKD旨在弥合在线模型与离线模型之间的时间差距，提高在自动驾驶领域3D物体检测的实时性和准确性。

Method: FTKD方法采用了未来感知的特征重构策略，鼓励学生模型在没有严格帧对齐的情况下捕获未来的特征，并引入了未来导向的logit蒸馏策略，充分利用教师模型稳定的目标前景和背景环境。

Result: FTKD在nuScenes数据集上的表现显著提升，实现了高达1.3 mAP和1.3 NDS的增益，同时保持了不需要增加推理成本的优势。

Conclusion: FTKD为在线模型提供了未来帧的知识蒸馏解决方案，实现在不增加计算成本的情况下提高3D物体检测的精度。

Abstract: Camera-based temporal 3D object detection has shown impressive results in autonomous driving, with offline models improving accuracy by using future frames. Knowledge distillation (KD) can be an appealing framework for transferring rich information from offline models to online models. However, existing KD methods overlook future frames, as they mainly focus on spatial feature distillation under strict frame alignment or on temporal relational distillation, thereby making it challenging for online models to effectively learn future knowledge. To this end, we propose a sparse query-based approach, Future Temporal Knowledge Distillation (FTKD), which effectively transfers future frame knowledge from an offline teacher model to an online student model. Specifically, we present a future-aware feature reconstruction strategy to encourage the student model to capture future features without strict frame alignment. In addition, we further introduce future-guided logit distillation to leverage the teacher's stable foreground and background context. FTKD is applied to two high-performing 3D object detection baselines, achieving up to 1.3 mAP and 1.3 NDS gains on the nuScenes dataset, as well as the most accurate velocity estimation, without increasing inference cost.

</details>


### [19] [Query-aware Hub Prototype Learning for Few-Shot 3D Point Cloud Semantic Segmentation](https://arxiv.org/abs/2512.08253)
*YiLin Zhou,Lili Wei,Zheming Xu,Ziyi Chen,Congyan Lang*

Main category: cs.CV

TL;DR: 该研究提出了一个名为Query-aware Hub Prototype (QHP)的新颖学习方法，通过Query和Support点之间的语义关联，生成与查询集相关的原型。并引入了Prototype Distribution Optimization (PDO)模块以优化原型分布，从而改善了Few-shot 3D点云语义分割的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于元标记学习的方法仅从支持集生成原型，容易产生原型偏差，导致无法有效泛化到查询分布，特别是面对分布偏移时性能显著下降。因此，需要设计一种新的方法来缓解这一问题。

Method: QHP方法包括两个模块：Hub Prototype Generation (HPG)模块和Prototype Distribution Optimization (PDO)模块。HPG模块构建查询和支持点之间的二分图，识别常见链接的支持中心，并生成更加平滑和相关的原型。PDO模块通过纯度加权的对比损失来优化原型表示，将偏离的原型拉近类别中心。

Result: 在S3DIS和ScanNet数据集上的实验表明，QHP方法在Few-shot 3D点云语义分割中实现了显著的性能提升。

Conclusion: QHP方法通过模型化支持和查询集合之间的语义关联来生成查询相关原型，并通过优化原型分布以提升Few-shot 3D点云语义分割的性能。

Abstract: Few-shot 3D point cloud semantic segmentation (FS-3DSeg) aims to segment novel classes with only a few labeled samples. However, existing metric-based prototype learning methods generate prototypes solely from the support set, without considering their relevance to query data. This often results in prototype bias, where prototypes overfit support-specific characteristics and fail to generalize to the query distribution, especially in the presence of distribution shifts, which leads to degraded segmentation performance. To address this issue, we propose a novel Query-aware Hub Prototype (QHP) learning method that explicitly models semantic correlations between support and query sets. Specifically, we propose a Hub Prototype Generation (HPG) module that constructs a bipartite graph connecting query and support points, identifies frequently linked support hubs, and generates query-relevant prototypes that better capture cross-set semantics. To further mitigate the influence of bad hubs and ambiguous prototypes near class boundaries, we introduce a Prototype Distribution Optimization (PDO) module, which employs a purity-reweighted contrastive loss to refine prototype representations by pulling bad hubs and outlier prototypes closer to their corresponding class centers. Extensive experiments on S3DIS and ScanNet demonstrate that QHP achieves substantial performance gains over state-of-the-art methods, effectively narrowing the semantic gap between prototypes and query sets in FS-3DSeg.

</details>


### [20] [RLCNet: An end-to-end deep learning framework for simultaneous online calibration of LiDAR, RADAR, and Camera](https://arxiv.org/abs/2512.08262)
*Hafeez Husain Cholakkal,Stefano Arrigoni,Francesco Braghin*

Main category: cs.CV

TL;DR: RLCNet 是一个新型的端到端可训练的深度学习框架，用于同时在线校准 LiDAR、RADAR 和相机传感器，适用于动态环境，并且具有抗漂移能力和实时操作性。


<details>
  <summary>Details</summary>
Motivation: 由于机械振动和累积传感器漂移的影响，LiDAR、RADAR 和相机传感器的外在校准在动态环境中仍然具有挑战性。RLCNet 的提出旨在提供一种有效且鲁棒的解决方案。

Method: RLCNet 使用一种结合了加权移动平均和离群值拒绝的在线校准框架，该框架能够动态调整校准参数，减少预测噪声并提高对漂移的鲁棒性。

Result: 通过在现实世界的数据集上的验证，RLCNet 展示了在各种条件下的稳健性能，并通过消融研究和与现有方法的比较证明了其更高的准确性和鲁棒性。

Conclusion: RLCNet 引入了一种适应动态环境的在线校准机制，并通过实验证明了其在现实驾驶场景中的有效性。

Abstract: Accurate extrinsic calibration of LiDAR, RADAR, and camera sensors is essential for reliable perception in autonomous vehicles. Still, it remains challenging due to factors such as mechanical vibrations and cumulative sensor drift in dynamic environments. This paper presents RLCNet, a novel end-to-end trainable deep learning framework for the simultaneous online calibration of these multimodal sensors. Validated on real-world datasets, RLCNet is designed for practical deployment and demonstrates robust performance under diverse conditions. To support real-time operation, an online calibration framework is introduced that incorporates a weighted moving average and outlier rejection, enabling dynamic adjustment of calibration parameters with reduced prediction noise and improved resilience to drift. An ablation study highlights the significance of architectural choices, while comparisons with existing methods demonstrate the superior accuracy and robustness of the proposed approach.

</details>


### [21] [EgoX: Egocentric Video Generation from a Single Exocentric Video](https://arxiv.org/abs/2512.08269)
*Taewoong Kang,Kinam Kim,Dohyeon Kim,Minho Park,Junha Hyung,Jaegul Choo*

Main category: cs.CV

TL;DR: EgoX 是一种新的框架，能够利用单个外部视频生成第一人称视频，通过轻量级的 LoRA 调整和统一条件策略实现几何一致性的合成，同时保持视觉真实性。


<details>
  <summary>Details</summary>
Motivation: 通过将第三人称视频转化为第一人称视频，提供更沉浸式的理解体验，解决由于极端相机姿态变化和极小视角重叠带来的挑战。

Method: EgoX 通过轻量级 LoRA 调整大型时空视频扩散模型，结合外部和第一人称先验信息，使用宽度和通道级连接，并引入几何引导的自注意力机制。

Result: EgoX 实现了连贯且具现实感的第一人称视频生成，并且在未见和野生视频上表现出了强大的可扩展性和鲁棒性。

Conclusion: EgoX 为第一人称视频的生成提供了一种新的解决方案，通过利用扩散模型的知识和改进的条件策略，实现了更高效、更真实的转换。

Abstract: Egocentric perception enables humans to experience and understand the world directly from their own point of view. Translating exocentric (third-person) videos into egocentric (first-person) videos opens up new possibilities for immersive understanding but remains highly challenging due to extreme camera pose variations and minimal view overlap. This task requires faithfully preserving visible content while synthesizing unseen regions in a geometrically consistent manner. To achieve this, we present EgoX, a novel framework for generating egocentric videos from a single exocentric input. EgoX leverages the pretrained spatio temporal knowledge of large-scale video diffusion models through lightweight LoRA adaptation and introduces a unified conditioning strategy that combines exocentric and egocentric priors via width and channel wise concatenation. Additionally, a geometry-guided self-attention mechanism selectively attends to spatially relevant regions, ensuring geometric coherence and high visual fidelity. Our approach achieves coherent and realistic egocentric video generation while demonstrating strong scalability and robustness across unseen and in-the-wild videos.

</details>


### [22] [PAVAS: Physics-Aware Video-to-Audio Synthesis](https://arxiv.org/abs/2512.08282)
*Oh Hyun-Bin,Yuhta Takida,Toshimitsu Uesaka,Tae-Hyun Oh,Yuki Mitsufuji*

Main category: cs.CV

TL;DR: PAVAS 通过引入物理驱动的音频适配器来生成视频到音频，并通过物理参数估计和动态重建模块捕捉物理因素，从而合成符合物理真实性的音频。该方法在定量和定性评估中均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前的 V2A 模型主要基于视觉特征，未能充分考虑影响真实世界声音的物理因素。PAVAS 通过物理驱动的音频适配器提升了这一限制。

Method: PAVAS 方法首先使用物理参数估计器（PPE）通过视觉语言模型估计物体的质量以及分割驱动的 3D 动态重建模块恢复其运动轨迹来计算速度。这些物理线索通过物理驱动音频适配器输入到 V2A 模型中，以生成符合物理真实性的音频。

Result: 通过对新建立的 VGG-Impact 基准测试，PAVAS 证明了其在物理真实性和听觉属性一致性方面的能力。实验结果显示该方法在定量和定性评估上均优于现有 V2A 模型。

Conclusion: PAVAS 在视频到音频生成中的研究引入了物理驱动的音频适配器，不仅提升了合成音频的物理真实性，还展示了其对 V2A 任务的巨大潜力。

Abstract: Recent advances in Video-to-Audio (V2A) generation have achieved impressive perceptual quality and temporal synchronization, yet most models remain appearance-driven, capturing visual-acoustic correlations without considering the physical factors that shape real-world sounds. We present Physics-Aware Video-to-Audio Synthesis (PAVAS), a method that incorporates physical reasoning into a latent diffusion-based V2A generation through the Physics-Driven Audio Adapter (Phy-Adapter). The adapter receives object-level physical parameters estimated by the Physical Parameter Estimator (PPE), which uses a Vision-Language Model (VLM) to infer the moving-object mass and a segmentation-based dynamic 3D reconstruction module to recover its motion trajectory for velocity computation. These physical cues enable the model to synthesize sounds that reflect underlying physical factors. To assess physical realism, we curate VGG-Impact, a benchmark focusing on object-object interactions, and introduce Audio-Physics Correlation Coefficient (APCC), an evaluation metric that measures consistency between physical and auditory attributes. Comprehensive experiments show that PAVAS produces physically plausible and perceptually coherent audio, outperforming existing V2A models in both quantitative and qualitative evaluations. Visit https://physics-aware-video-to-audio-synthesis.github.io for demo videos.

</details>


### [23] [OpenSubject: Leveraging Video-Derived Identity and Diversity Priors for Subject-driven Image Generation and Manipulation](https://arxiv.org/abs/2512.08294)
*Yexin Liu,Manyuan Zhang,Yueze Wang,Hongyu Li,Dian Zheng,Weiming Zhang,Changsheng Lu,Xunliang Cai,Yan Feng,Peng Pei,Harry Yang*

Main category: cs.CV

TL;DR: OpenSubject是一个包含250万样本和435万图像的视频衍生大规模数据集，旨在为单主体生成和操作提供高质量参考图像，通过改进的图像合成和配对过程，特别是在复杂场景中提高生成和操作性能。


<details>
  <summary>Details</summary>
Motivation: 当前的图像生成模型在处理基于多个主体的复杂场景时，往往难以保持参考身份的一致性，因此，OpenSubject旨在解决这一问题，通过引入高质量的图像生成和配对流程，提高在复杂场景下的生成和操作效果。

Method: OpenSubject的生成过程分为四个阶段，包括视频筛选、跨帧主体挖掘及配对、身份保留的参考图像合成以及验证和描述。每个阶段通过特定的技术（如分辨率和美学过滤、基于视觉语言模型（VLM）的类别共识、局部定位、多样性配对等）实现具体目标。

Result: 通过使用OpenSubject，实验结果表明在复杂场景中生成和操作的性能得到了显著提升，尤其是在身份保真、指令遵从、操作一致性以及背景一致性方面。

Conclusion: 研究表明，采用OpenSubject作为训练数据可以有效提高图像生成和操纵的效果，特别是在处理复杂场景时更显著，为后续研究提供了高质量的数据支持和验证基准。

Abstract: Despite the promising progress in subject-driven image generation, current models often deviate from the reference identities and struggle in complex scenes with multiple subjects. To address this challenge, we introduce OpenSubject, a video-derived large-scale corpus with 2.5M samples and 4.35M images for subject-driven generation and manipulation. The dataset is built with a four-stage pipeline that exploits cross-frame identity priors. (i) Video Curation. We apply resolution and aesthetic filtering to obtain high-quality clips. (ii) Cross-Frame Subject Mining and Pairing. We utilize vision-language model (VLM)-based category consensus, local grounding, and diversity-aware pairing to select image pairs. (iii) Identity-Preserving Reference Image Synthesis. We introduce segmentation map-guided outpainting to synthesize the input images for subject-driven generation and box-guided inpainting to generate input images for subject-driven manipulation, together with geometry-aware augmentations and irregular boundary erosion. (iv) Verification and Captioning. We utilize a VLM to validate synthesized samples, re-synthesize failed samples based on stage (iii), and then construct short and long captions. In addition, we introduce a benchmark covering subject-driven generation and manipulation, and then evaluate identity fidelity, prompt adherence, manipulation consistency, and background consistency with a VLM judge. Extensive experiments show that training with OpenSubject improves generation and manipulation performance, particularly in complex scenes.

</details>


### [24] [Terrain Diffusion: A Diffusion-Based Successor to Perlin Noise in Infinite, Real-Time Terrain Generation](https://arxiv.org/abs/2512.08309)
*Alexander Goslin*

Main category: cs.CV

TL;DR: 该研究提出了Terrain Diffusion，一种结合了无限生成能力和细节控制能力的新方法，用以替代传统过程噪声函数，适用于构建无缝、实时、大规模的虚拟世界。


<details>
  <summary>Details</summary>
Motivation: 传统的过程噪声函数如Perlin噪声在真实性和全局一致性方面存在局限，而Terrain Diffusion旨在解决这些问题，提供一种更高效的、具有无限生成能力的新方法，以满足现代AI应用的需求。

Method: Terrain Diffusion的核心是一个名为InfiniteDiffusion的新型算法，用于无限生成，同时采用层级堆叠的扩散模型实现行星背景与局部细节的结合，以及紧凑的Laplacian编码稳定输出。此外，还提供了一个无限张量框架，支持无需额外内存即可操作无界张量，并通过几步一致性蒸馏提高生成效率。

Result: 通过Terrain Diffusion，研究人员能够实时、无缝地合成无尽的景观，涵盖了从行星尺度到局部细节的广泛动态范围，并成功地将扩散模型应用于程序化世界生成中。

Conclusion: 这种新的方法展示了扩散模型在程序化世界生成中的巨大潜力，能够生成整个行星甚至更大规模的可控、一致和无边界内容。

Abstract: For decades, procedural worlds have been built on procedural noise functions such as Perlin noise, which are fast and infinite, yet fundamentally limited in realism and large-scale coherence. We introduce Terrain Diffusion, an AI-era successor to Perlin noise that bridges the fidelity of diffusion models with the properties that made procedural noise indispensable: seamless infinite extent, seed-consistency, and constant-time random access. At its core is InfiniteDiffusion, a novel algorithm for infinite generation, enabling seamless, real-time synthesis of boundless landscapes. A hierarchical stack of diffusion models couples planetary context with local detail, while a compact Laplacian encoding stabilizes outputs across Earth-scale dynamic ranges. An open-source infinite-tensor framework supports constant-memory manipulation of unbounded tensors, and few-step consistency distillation enables efficient generation. Together, these components establish diffusion models as a practical foundation for procedural world generation, capable of synthesizing entire planets coherently, controllably, and without limits.

</details>


### [25] [GeoDM: Geometry-aware Distribution Matching for Dataset Distillation](https://arxiv.org/abs/2512.08317)
*Xuhui Li,Zhengquan Luo,Zihui Cui,Zhiqiang Xu*

Main category: cs.CV

TL;DR: 该研究提出了一种新框架GeoDM，旨在合成一个紧凑的数据子集，以保持与原始大数据集相当的性能。该框架能在欧几里得、双曲和球面流形的笛卡尔积上操作，能够捕捉不同层次的数据几何结构。通过引入变化曲率和权重参数，GeoDM能够更好地匹配数据分布。


<details>
  <summary>Details</summary>
Motivation: 现有的分布匹配方法仅限于欧几里得空间，无法充分捕捉高维数据的真实几何结构，如曲率。因此，需要一种新的框架以更好地捕捉数据的非线性特征。

Method: GeoDM框架通过在欧几里得、双曲和球面流形的笛卡尔积上操作，利用了三种类型的几何结构。通过引入可学习的曲率和权重参数，以及优化传输损失，增强数据分布的保真度。

Result: 理论分析表明，在产品空间中具有几何感知的分布匹配有更小的泛化误差界。实验结果表明，该方法在标准基准数据上的表现优于最先进的数据蒸馏方法，且在不同的几何匹配策略下仍保持有效性。

Conclusion: GeoDM框架通过几何感知的分布匹配，在保持数据集紧凑性的前提下，达到了与原始大数据集相当的性能，且具有更优的泛化能力。

Abstract: Dataset distillation aims to synthesize a compact subset of the original data, enabling models trained on it to achieve performance comparable to those trained on the original large dataset. Existing distribution-matching methods are confined to Euclidean spaces, making them only capture linear structures and overlook the intrinsic geometry of real data, e.g., curvature. However, high-dimensional data often lie on low-dimensional manifolds, suggesting that dataset distillation should have the distilled data manifold aligned with the original data manifold. In this work, we propose a geometry-aware distribution-matching framework, called \textbf{GeoDM}, which operates in the Cartesian product of Euclidean, hyperbolic, and spherical manifolds, with flat, hierarchical, and cyclical structures all captured by a unified representation. To adapt to the underlying data geometry, we introduce learnable curvature and weight parameters for three kinds of geometries. At the same time, we design an optimal transport loss to enhance the distribution fidelity. Our theoretical analysis shows that the geometry-aware distribution matching in a product space yields a smaller generalization error bound than the Euclidean counterparts. Extensive experiments conducted on standard benchmarks demonstrate that our algorithm outperforms state-of-the-art data distillation methods and remains effective across various distribution-matching strategies for the single geometries.

</details>


### [26] [Detecting Dental Landmarks from Intraoral 3D Scans: the 3DTeethLand challenge](https://arxiv.org/abs/2512.08323)
*Achraf Ben-Hamadou,Nour Neifar,Ahmed Rekik,Oussama Smaoui,Firas Bouzguenda,Sergi Pujades,Niels van Nistelrooij,Shankeeth Vinayahalingam,Kaibo Shi,Hairong Jin,Youyi Zheng,Tibor Kubík,Oldřich Kodym,Petr Šilling,Kateřina Trávníčková,Tomáš Mojžiš,Jan Matula,Jeffry Hartanto,Xiaoying Zhu,Kim-Ngan Nguyen,Tudor Dascalu,Huikai Wu,and Weijie Liu,Shaojie Zhuang,Guangshun Wei,Yuanfeng Zhou*

Main category: cs.CV

TL;DR: 该研究旨在通过3D牙齿标识挑战解决临床正畸中的牙齿地标定位问题，该挑战首次提供了一个公共数据集，旨在推动深度学习算法在这一任务中的进步。


<details>
  <summary>Details</summary>
Motivation: 牙齿地标检测在现代临床正畸中至关重要，能够支持精准诊断、个性化治疗策略及治疗进展的有效监控。但个体牙齿的复杂几何结构和个体间显著的差异性带来了挑战，因此需开发先进的技术，尤其是通过深度学习的应用，以实现精确可靠地检测3D牙齿地标。

Method: 为应对这些复杂性，作者提出了一项名为3DTeethLand的挑战，与医学图像计算和计算机辅助介入国际会议（MICCAI）合作举办，并提供了首个用于3D牙齿地标检测的公共数据集。参与者需开发特定于口腔3D扫描的牙齿地标检测算法。

Result: 该公共数据集为评估最新方法和鼓励研究社区提供了一种有价值的方式来解决具有临床意义的问题。然而，具体结果未在摘要中提及。

Conclusion: 该挑战的成功举办推动了3D口腔数据领域的新研究，并为临床正畸中的牙齿检测算法评估提供了重要资源。

Abstract: Teeth landmark detection is a critical task in modern clinical orthodontics. Their precise identification enables advanced diagnostics, facilitates personalized treatment strategies, and supports more effective monitoring of treatment progress in clinical dentistry. However, several significant challenges may arise due to the intricate geometry of individual teeth and the substantial variations observed across different individuals. To address these complexities, the development of advanced techniques, especially through the application of deep learning, is essential for the precise and reliable detection of 3D tooth landmarks. In this context, the 3DTeethLand challenge was held in collaboration with the International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI) in 2024, calling for algorithms focused on teeth landmark detection from intraoral 3D scans. This challenge introduced the first publicly available dataset for 3D teeth landmark detection, offering a valuable resource to assess the state-of-the-art methods in this task and encourage the community to provide methodological contributions towards the resolution of their problem with significant clinical implications.

</details>


### [27] [GeoDiffMM: Geometry-Guided Conditional Diffusion for Motion Magnification](https://arxiv.org/abs/2512.08325)
*Xuedeng Liu,Jiabao Guo,Zheng Zhang,Fei Wang,Zhi Liu,Dan Guo*

Main category: cs.CV

TL;DR: GeoDiffMM 提出了一种新的扩散基于拉格朗日 Video Motion Magnification（VMM）框架，通过光学流动作为几何线索进行条件化，实现了结构一致的运动放大。该框架包括无噪声光学流动增强策略和扩散运动放大器，能够在保持图像领域高保真度的同时，放大与场景语义和结构一致的运动成分，同时抑制无关内容的扰动。


<details>
  <summary>Details</summary>
Motivation: 现有的基于欧拉方法在放大运动时难以区分光子噪声和真实的微小运动，GeoDiffMM旨在解决这个问题。

Method: GeoDiffMM的方法包括：1. 无噪声光学流动增强策略，利用光学流动生成非刚性运动场；2. 扩散运动放大器，基于光学流动作为几何先验和可学习的放大因子进行条件化。

Result: 实验结果显示，GeoDiffMM在真实和合成数据集上的表现优于现有的先进方法，显著提高了运动放大效果。

Conclusion: GeoDiffMM 提供了一种结构化运动放大的新方法，通过光学流动增强和扩散运动放大器的结合，能够在保持图像保真度的同时放大与场景相关的运动。

Abstract: Video Motion Magnification (VMM) amplifies subtle macroscopic motions to a perceptible level. Recently, existing mainstream Eulerian approaches address amplification-induced noise via decoupling representation learning such as texture, shape and frequancey schemes, but they still struggle to separate photon noise from true micro-motion when motion displacements are very small. We propose GeoDiffMM, a novel diffusion-based Lagrangian VMM framework conditioned on optical flow as a geometric cue, enabling structurally consistent motion magnification. Specifically, we design a Noise-free Optical Flow Augmentation strategy that synthesizes diverse nonrigid motion fields without photon noise as supervision, helping the model learn more accurate geometry-aware optial flow and generalize better. Next, we develop a Diffusion Motion Magnifier that conditions the denoising process on (i) optical flow as a geometry prior and (ii) a learnable magnification factor controlling magnitude, thereby selectively amplifying motion components consistent with scene semantics and structure while suppressing content-irrelevant perturbations. Finally, we perform Flow-based Video Synthesis to map the amplified motion back to the image domain with high fidelity. Extensive experiments on real and synthetic datasets show that GeoDiffMM outperforms state-of-the-art methods and significantly improves motion magnification.

</details>


### [28] [Low Rank Support Quaternion Matrix Machine](https://arxiv.org/abs/2512.08327)
*Wang Chen,Ziyan Luo,Shuangyue Wang*

Main category: cs.CV

TL;DR: 本文提出了一种新的面向颜色图像分类的方法，称为低秩支持四元数矩阵机( LSQMM)，通过将RGB通道视为纯四元数来有效保留通道之间的内在耦合关系。通过添加基于四元数核范数正则化的核损失函数，增强强相关颜色通道中的低秩结构。实验结果显示，该方法在多个颜色图像分类数据集上具有更高的分类精度、鲁棒性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 颜色图像分类任务通常使用实数域中的向量、矩阵或三阶张量来表示输入特征。为了提高分类性能并更好地利用颜色通道之间的相关性，本文采用四元数数据建模颜色图像，提出了 LSQMM 方法。

Method: 该方法利用四元数代数将RGB通道视为纯四元数，以保留颜色通道之间的内在耦合关系。同时，通过引入基于四元数核范数的正则化项，增强数据中的低秩结构，进而提高了分类器的泛化能力。为了优化该模型，设计了一种基于ADMM的迭代算法。

Result: 实验结果显示，与支持向量机、支持矩阵机和支持张量机等最新方法相比，LSQMM 方法在多个颜色图像分类数据集上具有更高的分类精度、鲁棒性和计算效率。

Conclusion: 本文提出的 LSQMM 方法为颜色图像分类提供了一种有效的解决方案，可以更好地保留颜色通道之间的耦合关系，并通过噪声降低和特征选择增强了模型的低秩结构。

Abstract: Input features are conventionally represented as vectors, matrices, or third order tensors in the real field, for color image classification. Inspired by the success of quaternion data modeling for color images in image recovery and denoising tasks, we propose a novel classification method for color image classification, named as the Low-rank Support Quaternion Matrix Machine (LSQMM), in which the RGB channels are treated as pure quaternions to effectively preserve the intrinsic coupling relationships among channels via the quaternion algebra. For the purpose of promoting low-rank structures resulting from strongly correlated color channels, a quaternion nuclear norm regularization term, serving as a natural extension of the conventional matrix nuclear norm to the quaternion domain, is added to the hinge loss in our LSQMM model. An Alternating Direction Method of Multipliers (ADMM)-based iterative algorithm is designed to effectively resolve the proposed quaternion optimization model. Experimental results on multiple color image classification datasets demonstrate that our proposed classification approach exhibits advantages in classification accuracy, robustness and computational efficiency, compared to several state-of-the-art methods using support vector machines, support matrix machines, and support tensor machines.

</details>


### [29] [Interpreting Structured Perturbations in Image Protection Methods for Diffusion Models](https://arxiv.org/abs/2512.08329)
*Michael R. Martin,Garrick Chan,Kwan-Liu Ma*

Main category: cs.CV

TL;DR: 该研究通过统一框架对非可见篡改机制如Glaze和Nightshade进行了深入分析，展示了它们如何对图像内容产生结构化低熵扰动，而非全局语义位移。频率域分析表明，这些机制在保留内容驱动特征组织的同时，将能量沿主图像对齐的频率轴重分布，而非引入无目标的噪声。


<details>
  <summary>Details</summary>
Motivation: 当前的图像保护机制虽然在实验效果上取得了成功，但其内部结构、可检测性和表征行为仍不明确。本文旨在通过结合白盒特征空间检验和黑盒信号级探测的统一框架，从表征域、空间域和频谱域多维度提供解释性AI分析。

Method: 通过潜在空间聚类、特征通道激活分析、遮挡基空间敏感映射和频域特征分析，系统研究了保护机制在表征、空间和频谱域上的表现。

Result: 研究发现，保护机制通过结构化的低熵扰动紧耦合于基底图像内容，在保持内容相关特征组织的同时不具备全球表征偏移。遮挡感知的空间敏感映射揭示扰动熵、空间部署和频谱对齐的交互作用决定了可感知结构的性质，并且保护机制的连续叠加会增加而非抑制可感知的结构。

Conclusion: 频域分析进一步表明，Glaze和Nightshade重分布能量沿主图像对齐的频谱轴，而非引入无目标的噪声。这些发现阐明了当前图像保护机制通过特征级有结构的扭曲而非语义类移实现其功能，解释了为何保护信号虽然视觉上微弱却能始终被检测到。本研究提高了对抗式图像保护的可解释性，并为生成式AI系统的防御和检测策略设计提供了指导。

Abstract: Recent image protection mechanisms such as Glaze and Nightshade introduce imperceptible, adversarially designed perturbations intended to disrupt downstream text-to-image generative models. While their empirical effectiveness is known, the internal structure, detectability, and representational behavior of these perturbations remain poorly understood. This study provides a systematic, explainable AI analysis using a unified framework that integrates white-box feature-space inspection and black-box signal-level probing. Through latent-space clustering, feature-channel activation analysis, occlusion-based spatial sensitivity mapping, and frequency-domain characterization, we show that protection mechanisms operate as structured, low-entropy perturbations tightly coupled to underlying image content across representational, spatial, and spectral domains. Protected images preserve content-driven feature organization with protection-specific substructure rather than inducing global representational drift. Detectability is governed by interacting effects of perturbation entropy, spatial deployment, and frequency alignment, with sequential protection amplifying detectable structure rather than suppressing it. Frequency-domain analysis shows that Glaze and Nightshade redistribute energy along dominant image-aligned frequency axes rather than introducing diffuse noise. These findings indicate that contemporary image protection operates through structured feature-level deformation rather than semantic dislocation, explaining why protection signals remain visually subtle yet consistently detectable. This work advances the interpretability of adversarial image protection and informs the design of future defenses and detection strategies for generative AI systems.

</details>


### [30] [PointDico: Contrastive 3D Representation Learning Guided by Diffusion Models](https://arxiv.org/abs/2512.08330)
*Pengbo Li,Yiding Sun,Haozhe Cheng*

Main category: cs.CV

TL;DR: PointDico结合了扩散模型和对比学习的优势，通过知识蒸馏实现非结构化点云的3D表示学习，获得最佳性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理3D数据时存在缺陷，如对比模型容易过拟合，而3D Mask Autoencoders难以处理无序点云。

Method: 提出PointDico模型，通过知识蒸馏结合降噪生成建模和跨模态对比学习，采用分层金字塔有条件生成器进行多尺度几何特征提取，使用双通道设计有效整合局部和全局上下文信息。

Result: PointDico在3D表示学习上取得了最佳性能，例如在ScanObjectNN数据集上的准确率为94.32%，ShapeNetPart数据集上的实例mIoU为86.5%。

Conclusion: 该研究提出了一个新颖的3D表示学习模型PointDico，显著提高了3D数据的表示能力。

Abstract: Self-supervised representation learning has shown significant improvement in Natural Language Processing and 2D Computer Vision. However, existing methods face difficulties in representing 3D data because of its unordered and uneven density. Through an in-depth analysis of mainstream contrastive and generative approaches, we find that contrastive models tend to suffer from overfitting, while 3D Mask Autoencoders struggle to handle unordered point clouds. This motivates us to learn 3D representations by sharing the merits of diffusion and contrast models, which is non-trivial due to the pattern difference between the two paradigms. In this paper, we propose \textit{PointDico}, a novel model that seamlessly integrates these methods. \textit{PointDico} learns from both denoising generative modeling and cross-modal contrastive learning through knowledge distillation, where the diffusion model serves as a guide for the contrastive model. We introduce a hierarchical pyramid conditional generator for multi-scale geometric feature extraction and employ a dual-channel design to effectively integrate local and global contextual information. \textit{PointDico} achieves a new state-of-the-art in 3D representation learning, \textit{e.g.}, \textbf{94.32\%} accuracy on ScanObjectNN, \textbf{86.5\%} Inst. mIoU on ShapeNetPart.

</details>


### [31] [Bi^2MAC: Bimodal Bi-Adaptive Mask-Aware Convolution for Remote Sensing Pansharpening](https://arxiv.org/abs/2512.08331)
*Xianghong Xiao,Zeyu Xia,Zhou Fei,Jinliang Xiao,Haorui Chen,Liangjian Deng*

Main category: cs.CV

TL;DR: 论文提出了一种名为Bimodal Bi-Adaptive Mask-Aware Convolution（Bi^2MAC）的新方法，旨在有效融合高分辨率多光谱图像与低分辨率多光谱图像，以生成高分辨率多光谱图像。Bi^2MAC方法通过生成软硬两种掩码，将输入特征适当调节，并引导不同类型区域进入独立处理分支，从而减少冗余特征的计算成本，同时保证了对异质区域的详细建模。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以有效处理遥感图像中的区域异质性，容易导致计算成本过高和对异质区域捕捉不足。本文提出的方法旨在改进在特征表示中适应区域异质性的能力。

Method: 提出的Bi^2MAC方法包含一个轻量级模块，用于生成软硬两种掩码。软掩码用于初步调节输入特征，硬掩码用于指导不同类型的区域进入不同的处理分支。对于冗余特征，它们被导向一个紧凑分支进行低成本的全局处理。对于异质特征，它们被导向一个聚焦分支，该分支将投入更多计算资源进行更精细的建模。

Result: 在多个基准数据集上的实验表明，与传统方法相比，Bi^2MAC方法在保持最高性能的同时，训练时间和参数数量显著减少，并且计算成本更低。

Conclusion: 本文提出的方法通过更有效地利用来自不同类型的区域的信息并智能地分配计算资源，显著改善了特征表示中的区域异质性适应能力。

Abstract: Pansharpening aims to fuse a high-resolution panchromatic (PAN) image with a low-resolution multispectral (LRMS) image to generate a high-resolution multispectral image (HRMS). Conventional deep learning-based methods are inherently limited in their ability to adapt to regional heterogeneity within feature representations. Although various adaptive convolution methods have been proposed to address this limitation, they often suffer from excessive computational costs and a limited ability to capture heterogeneous regions in remote sensing images effectively. To overcome these challenges, we propose Bimodal Bi-Adaptive Mask-Aware Convolution (Bi^2MAC), which effectively exploits information from different types of regions while intelligently allocating computational resources. Specifically, we design a lightweight module to generate both soft and hard masks, which are used to modulate the input features preliminarily and to guide different types of regions into separate processing branches, respectively. Redundant features are directed to a compact branch for low-cost global processing. In contrast, heterogeneous features are routed to a focused branch that invests more computational resources for fine-grained modeling. Extensive experiments on multiple benchmark datasets demonstrate that Bi^2MAC achieves state-of-the-art (SOTA) performance while requiring substantially lower training time and parameter counts, and the minimal computational cost among adaptive convolution models.

</details>


### [32] [HybridSplat: Fast Reflection-baked Gaussian Tracing using Hybrid Splatting](https://arxiv.org/abs/2512.08334)
*Chang Liu,Hongliang Yuan,Lianghao Zhang,Sichao Wang,Jianwei Guo,Shi-Sheng Huang*

Main category: cs.CV

TL;DR: HybridSplat 提出了一种新的混合散斑机制，通过视点依赖的反射烘焙和统一的混合散斑框架加速了复杂反射场景的渲染速度，降低了内存存储需求，且保持了反射渲染质量。


<details>
  <summary>Details</summary>
Motivation: 为了解决使用 3D 高斯散斑渲染现实世界场景中存在的渲染速度慢和内存存储限制问题。

Method: 通过引入视点依赖的反射烘焙和统一的混合散斑框架来实现反射高保真场景重建，同时设计了在管道级别加速混合散斑的方法和反射敏感的高斯模型修剪策略。

Result: HybridSplat 在复杂反射场景中的渲染速度提高了约 7 倍，使用了比传统基于光线追踪的高斯散斑基线少 4 倍的高斯模型，并成为复杂反射场景下的最新基准方法。

Conclusion: HybridSplat 提供了一种有效的方法来实现复杂反射场景的实时光照渲染，降低内存需求，同时保持高质量的视觉效果。

Abstract: Rendering complex reflection of real-world scenes using 3D Gaussian splatting has been a quite promising solution for photorealistic novel view synthesis, but still faces bottlenecks especially in rendering speed and memory storage. This paper proposes a new Hybrid Splatting(HybridSplat) mechanism for Gaussian primitives. Our key idea is a new reflection-baked Gaussian tracing, which bakes the view-dependent reflection within each Gaussian primitive while rendering the reflection using tile-based Gaussian splatting. Then we integrate the reflective Gaussian primitives with base Gaussian primitives using a unified hybrid splatting framework for high-fidelity scene reconstruction. Moreover, we further introduce a pipeline-level acceleration for the hybrid splatting, and reflection-sensitive Gaussian pruning to reduce the model size, thus achieving much faster rendering speed and lower memory storage while preserving the reflection rendering quality. By extensive evaluation, our HybridSplat accelerates about 7x rendering speed across complex reflective scenes from Ref-NeRF, NeRF-Casting with 4x fewer Gaussian primitives than similar ray-tracing based Gaussian splatting baselines, serving as a new state-of-the-art method especially for complex reflective scenes.

</details>


### [33] [MELLA: Bridging Linguistic Capability and Cultural Groundedness for Low-Resource Language MLLMs](https://arxiv.org/abs/2508.05502)
*Yufei Gao,Jiaying Fei,Nuo Chen,Ruirui Chen,Guohang Yan,Yunshi Lan,Botian Shi*

Main category: cs.CV

TL;DR: 研究提出了一个用于低资源语言的双源策略，通过MELLA数据集的双模态、多语言信息增强，改善了MLLM在语言能力和文化背景方面的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管大型多模态语言模型在高资源语言中表现出色，但在低资源语言中却效果不佳。当前方法主要集中在文本领域或依赖机器翻译，忽视了多模态信息和文化根基的重要性。

Method: 研究提出了一个双源策略，从原生网页替代文本收集文化信息，从MLLM生成的字幕收集语言能力。构建了一个多模态、多语言数据集MELLA，并在不同低资源语言上进行了微调实验。

Result: 微调后，多种MLLM基础模型在八种不同语言上的表现普遍有所提高，模型生成的描述更加丰富。

Conclusion: 此研究增强了MLLM在语言能力与文化背景方面的表现，数据集可以在https://opendatalab.com/applyMultilingualCorpus找到。

Abstract: Multimodal Large Language Models (MLLMs) have shown remarkable performance in high-resource languages. However, their effectiveness diminishes significantly in the contexts of low-resource languages. Current multilingual enhancement methods are often limited to text modality or rely solely on machine translation. While such approaches help models acquire basic linguistic capabilities and produce "thin descriptions", they neglect the importance of multimodal informativeness and cultural groundedness, both of which are crucial for serving low-resource language users effectively. To bridge this gap, in this study, we identify two significant objectives for a truly effective MLLM in low-resource language settings, namely 1) linguistic capability and 2) cultural groundedness, placing special emphasis on cultural awareness. To achieve these dual objectives, we propose a dual-source strategy that guides the collection of data tailored to each goal, sourcing native web alt-text for culture and MLLM-generated captions for linguistics. As a concrete implementation, we introduce MELLA, a multimodal, multilingual dataset. Experiment results show that after fine-tuning on MELLA, there is a general performance improvement for the eight languages on various MLLM backbones, with models producing "thick descriptions". We verify that the performance gains are from both cultural knowledge enhancement and linguistic capability enhancement. Our dataset can be found at https://opendatalab.com/applyMultilingualCorpus.

</details>


### [34] [TrackingWorld: World-centric Monocular 3D Tracking of Almost All Pixels](https://arxiv.org/abs/2512.08358)
*Jiahao Lu,Weitao Xiong,Jiacheng Deng,Peng Li,Tianyu Huang,Zhiyang Dou,Cheng Lin,Sai-Kit Yeung,Yuan Liu*

Main category: cs.CV

TL;DR: 提出了一种名为TrackingWorld的新颖管道，用于在世界中心的3D坐标系中进行密集的3D跟踪，以解决现有单目3D跟踪方法在分离摄像机运动和前景动态运动以及稀疏跟踪新出现动态主体方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的单目3D跟踪方法在分离摄像机运动和前景动态运动方面存在不足，无法密集跟踪视频中新出现的动态主体。

Method: 提出了一种管道，包括跟踪上采样器、重复减少和基于优化的框架，最终将密集的2D轨迹逆投影到世界中心的3D轨迹。

Result: 在合成和真实世界数据集上进行了广泛的评估，表明系统在世界中心坐标系中实现了准确的密集3D跟踪。

Conclusion: 该方法通过提出一种新的管道，解决了现有单目3D跟踪的两个限制，实现了准确且密集的世界中心坐标系3D跟踪。

Abstract: Monocular 3D tracking aims to capture the long-term motion of pixels in 3D space from a single monocular video and has witnessed rapid progress in recent years. However, we argue that the existing monocular 3D tracking methods still fall short in separating the camera motion from foreground dynamic motion and cannot densely track newly emerging dynamic subjects in the videos. To address these two limitations, we propose TrackingWorld, a novel pipeline for dense 3D tracking of almost all pixels within a world-centric 3D coordinate system. First, we introduce a tracking upsampler that efficiently lifts the arbitrary sparse 2D tracks into dense 2D tracks. Then, to generalize the current tracking methods to newly emerging objects, we apply the upsampler to all frames and reduce the redundancy of 2D tracks by eliminating the tracks in overlapped regions. Finally, we present an efficient optimization-based framework to back-project dense 2D tracks into world-centric 3D trajectories by estimating the camera poses and the 3D coordinates of these 2D tracks. Extensive evaluations on both synthetic and real-world datasets demonstrate that our system achieves accurate and dense 3D tracking in a world-centric coordinate frame.

</details>


### [35] [SCU-CGAN: Enhancing Fire Detection through Synthetic Fire Image Generation and Dataset Augmentation](https://arxiv.org/abs/2512.08362)
*Ju-Young Kim,Ji-Hong Park,Gun-Woo Kim*

Main category: cs.CV

TL;DR: 本文提出了一种名为SCU-CGAN的模型，该模型通过将U-Net、CBAM与额外的判别器结合，从非火灾图像生成逼真的火灾图像，显著提升了火灾检测模型的性能。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏充足的火灾数据集限制了火灾检测模型的性能，SCU-CGAN旨在解决这一问题，通过生成逼真的火灾图像来改善火灾检测系统的性能。

Method: SCU-CGAN结合了U-Net、CBAM（卷积块注意力模块）和一个额外的判别器，以生成高保真的火灾图像。

Result: SCU-CGAN在生成的火灾图像质量上表现出色，KID评分比CycleGAN提高了41.5%，特别是YOLOv5 nano模型在mAP@0.5:0.95指标上提高了56.5%。

Conclusion: 实验表明，增强的数据集可以显著提高火灾检测模型的准确性，而不会改变模型结构，SCU-CGAN展示了其在火灾检测领域的有效性和优越性。

Abstract: Fire has long been linked to human life, causing severe disasters and losses. Early detection is crucial, and with the rise of home IoT technologies, household fire detection systems have emerged. However, the lack of sufficient fire datasets limits the performance of detection models. We propose the SCU-CGAN model, which integrates U-Net, CBAM, and an additional discriminator to generate realistic fire images from nonfire images. We evaluate the image quality and confirm that SCU-CGAN outperforms existing models. Specifically, SCU-CGAN achieved a 41.5% improvement in KID score compared to CycleGAN, demonstrating the superior quality of the generated fire images. Furthermore, experiments demonstrate that the augmented dataset significantly improves the accuracy of fire detection models without altering their structure. For the YOLOv5 nano model, the most notable improvement was observed in the mAP@0.5:0.95 metric, which increased by 56.5%, highlighting the effectiveness of the proposed approach.

</details>


### [36] [The Unseen Bias: How Norm Discrepancy in Pre-Norm MLLMs Leads to Visual Information Loss](https://arxiv.org/abs/2512.08374)
*Bozhou Li,Xinda Xue,Sihan Yang,Yang Shi,Xinlong Chen,Yushuo Guan,Yuanxing Zhang,Wentao Zhang*

Main category: cs.CV

TL;DR: 研究指出MLLMs由于预训练中的预归一化架构导致视觉和文本归一化差异，提出在视觉投影后插入一个初始化好的LayerNorm层以解决这一问题，从而提高模型性能。


<details>
  <summary>Details</summary>
Motivation: 探讨MLLMs中存在的归一化差异问题，旨在改进这些模型的跨模态特征融合效果，并提出一个简单有效的解决方案。

Method: 通过理论分析论证归一化差异问题的影响，并通过实验证明问题的普遍存在性，最终提出插入LayerNorm层的方法来解决此问题。

Result: 提出的解决方案在多种主流MLLMs架构上得到验证，不仅提升了跨模态基准测试的表现，还在纯文本评估中也表现出色。

Conclusion: 归一化差异问题在MLLMs中普遍存在，通过在视觉投影后插入LayerNorm层，能够有效解决这一问题，提高模型的整体性能。

Abstract: Multimodal Large Language Models (MLLMs), which couple pre-trained vision encoders and language models, have shown remarkable capabilities. However, their reliance on the ubiquitous Pre-Norm architecture introduces a subtle yet critical flaw: a severe norm disparity between the high-norm visual tokens and the low-norm text tokens. In this work, we present a formal theoretical analysis demonstrating that this imbalance is not a static issue. Instead, it induces an ``asymmetric update dynamic,'' where high-norm visual tokens exhibit a ``representational inertia,'' causing them to transform semantically much slower than their textual counterparts. This fundamentally impairs effective cross-modal feature fusion. Our empirical validation across a range of mainstream MLLMs confirms that this theoretical dynamic -- the persistence of norm disparity and the resulting asymmetric update rates -- is a prevalent phenomenon. Based on this insight, we propose a remarkably simple yet effective solution: inserting a single, carefully initialized LayerNorm layer after the visual projector to enforce norm alignment. Experiments conducted on the LLaVA-1.5 architecture show that this intervention yields significant performance gains not only on a wide suite of multimodal benchmarks but also, notably, on text-only evaluations such as MMLU, suggesting that resolving the architectural imbalance leads to a more holistically capable model.

</details>


### [37] [Simultaneous Enhancement and Noise Suppression under Complex Illumination Conditions](https://arxiv.org/abs/2512.08378)
*Jing Tao,You Li,Banglei Guan,Yang Shang,Qifeng Yu*

Main category: cs.CV

TL;DR: 提出了一种在复杂光照条件下同时增强图像质量和抑制噪声的新框架。


<details>
  <summary>Details</summary>
Motivation: 针对复杂光照条件导致的图像质量问题，现有的图像增强方法要么增加噪声要么只在特定光照条件下有效。

Method: 新框架使用了梯度域加权指导滤波（GDWGIF）估计光照并改善图像质量，接着通过去 Fleshed out the Retinex 模型将图像分解为照明层和反射层，并对这两个层进行平行处理，修正照明层以优化光照条件，增强反射层以提升图像质量，最后通过多曝光融合和线性拉伸策略优化图像动态范围。

Result: 在实际应用中获得的数据集上进行了实验，结果显示，新方法在对比度增强和噪声抑制方面优于现有方法。

Conclusion: 新框架为复杂光照条件下同时提升图像质量和抑制噪声提供了一种有效的方法。

Abstract: Under challenging light conditions, captured images often suffer from various degradations, leading to a decline in the performance of vision-based applications. Although numerous methods have been proposed to enhance image quality, they either significantly amplify inherent noise or are only effective under specific illumination conditions. To address these issues, we propose a novel framework for simultaneous enhancement and noise suppression under complex illumination conditions. Firstly, a gradient-domain weighted guided filter (GDWGIF) is employed to accurately estimate illumination and improve image quality. Next, the Retinex model is applied to decompose the captured image into separate illumination and reflection layers. These layers undergo parallel processing, with the illumination layer being corrected to optimize lighting conditions and the reflection layer enhanced to improve image quality. Finally, the dynamic range of the image is optimized through multi-exposure fusion and a linear stretching strategy. The proposed method is evaluated on real-world datasets obtained from practical applications. Experimental results demonstrate that our proposed method achieves better performance compared to state-of-the-art methods in both contrast enhancement and noise suppression.

</details>


### [38] [Detection of Digital Facial Retouching utilizing Face Beauty Information](https://arxiv.org/abs/2512.08397)
*Philipp Srock,Juan E. Tapia,Christoph Busch*

Main category: cs.CV

TL;DR: 本文研究了面部修复图像对美容评估算法的影响，并基于人工智能的不同特征提取方法提高了面部修复检测率，尤其在未知修复算法的情况下，实现了单张图像检测1.1%的D-EER。


<details>
  <summary>Details</summary>
Motivation: 考虑到面部修复在社交媒体、广告和专业摄影中的广泛应用，以及这种做法可能在使用面部修复图像作为生物特征样本并注册到生物识别系统时带来的问题，本文旨在研究修复图像对美容评估的影响，以及是否可以利用面部美丽来提高检测率。

Method: 本文采用了基于人工智能的不同特征提取方法来识别面部修复，并评估了这些方法的效果。

Result: 本文提出了一种方法，在未知修复算法的情况下，实现了单张图像检测1.1%的D-EER。

Conclusion: 本文的研究表明，面部修复检测可以通过优化特征提取方法来改进，并且面部美丽可以作为一种辅助手段来提高检测率。

Abstract: Facial retouching to beautify images is widely spread in social media, advertisements, and it is even applied in professional photo studios to let individuals appear younger, remove wrinkles and skin impurities. Generally speaking, this is done to enhance beauty. This is not a problem itself, but when retouched images are used as biometric samples and enrolled in a biometric system, it is one. Since previous work has proven facial retouching to be a challenge for face recognition systems,the detection of facial retouching becomes increasingly necessary. This work proposes to study and analyze changes in beauty assessment algorithms of retouched images, assesses different feature extraction methods based on artificial intelligence in order to improve retouching detection, and evaluates whether face beauty can be exploited to enhance the detection rate. In a scenario where the attacking retouching algorithm is unknown, this work achieved 1.1% D-EER on single image detection.

</details>


### [39] [Towards Visual Re-Identification of Fish using Fine-Grained Classification for Electronic Monitoring in Fisheries](https://arxiv.org/abs/2512.08400)
*Samitha Nuwan Thilakarathna,Ercan Avsar,Martin Mathias Nielsen,Malte Pedersen*

Main category: cs.CV

TL;DR: 该研究利用AutoFish数据集和优化的深度学习管道，通过自动鱼类再识别提高渔业数据准确性，采用Hard Triplet mining和特定数据集的图像变换策略，Swin-T模型在mAP@k和Rank-1准确性上优于ResNet-50。


<details>
  <summary>Details</summary>
Motivation: 由于电子监测系统（EM）收集的视频数据量巨大，手动审查变得不切实际，因此开发自动化鱼类再识别技术变得至关重要，以提高渔业资源管理的效率和可持续性。

Method: 研究通过使用AutoFish数据集构建优化的深度学习管道，结合Hard Triplet mining技术和自定义图像变换策略，如特定的图像归一化方法，来提高鱼类再识别的准确性。

Result: 实验结果显示，Swin-T模型在mAP@k值达到41.65%，Rank-1准确率达到90.43%，在所有测试条件下均优于ResNet-50模型。

Conclusion: 研究证明，通过优化的深度学习方法可以显著提高鱼类再识别的准确性，这对于改进渔业数据管理和推动可持续海洋资源管理具有重要意义。

Abstract: Accurate fisheries data are crucial for effective and sustainable marine resource management. With the recent adoption of Electronic Monitoring (EM) systems, more video data is now being collected than can be feasibly reviewed manually. This paper addresses this challenge by developing an optimized deep learning pipeline for automated fish re-identification (Re-ID) using the novel AutoFish dataset, which simulates EM systems with conveyor belts with six similarly looking fish species. We demonstrate that key Re-ID metrics (R1 and mAP@k) are substantially improved by using hard triplet mining in conjunction with a custom image transformation pipeline that includes dataset-specific normalization. By employing these strategies, we demonstrate that the Vision Transformer-based Swin-T architecture consistently outperforms the Convolutional Neural Network-based ResNet-50, achieving peak performance of 41.65% mAP@k and 90.43% Rank-1 accuracy. An in-depth analysis reveals that the primary challenge is distinguishing visually similar individuals of the same species (Intra-species errors), where viewpoint inconsistency proves significantly more detrimental than partial occlusion. The source code and documentation are available at: https://github.com/msamdk/Fish_Re_Identification.git

</details>


### [40] [SAM-Body4D: Training-Free 4D Human Body Mesh Recovery from Videos](https://arxiv.org/abs/2512.08406)
*Mingqi Gao,Yunqi Miao,Jungong Han*

Main category: cs.CV

TL;DR: 本研究提出SAM-Body4D，一种无需额外训练即可实现视频中3D人体网格恢复的一体化框架，有效解决了传统方法的时序不一致和遮挡问题。


<details>
  <summary>Details</summary>
Motivation: 现有的基于图像的人体网格恢复方法虽然在野外图像中表现出色，但在视频处理时会因为帧间推理导致时序不一致和性能下降，特别是在遮挡情况下。

Method: 提出了利用基于视频的人体连续性的方法。首先使用可提示的视频分割模型生成身份一致的掩码，随后通过遮挡感知模块进行优化以恢复缺失区域，最后利用掩码引导3D人体模型生成连贯的身体网格轨迹。此外，该方法还采用基于填充的并行策略实现多人的高效推理。

Result: 实验结果表明，该方法在挑战性的野外视频中能显著提高时序稳定性与鲁棒性，无需额外训练。

Conclusion: 本研究提出的方法为未来视频中的人体网格恢复提供了新的思路，展示了其在实际场景中的应用潜力。

Abstract: Human Mesh Recovery (HMR) aims to reconstruct 3D human pose and shape from 2D observations and is fundamental to human-centric understanding in real-world scenarios. While recent image-based HMR methods such as SAM 3D Body achieve strong robustness on in-the-wild images, they rely on per-frame inference when applied to videos, leading to temporal inconsistency and degraded performance under occlusions. We address these issues without extra training by leveraging the inherent human continuity in videos. We propose SAM-Body4D, a training-free framework for temporally consistent and occlusion-robust HMR from videos. We first generate identity-consistent masklets using a promptable video segmentation model, then refine them with an Occlusion-Aware module to recover missing regions. The refined masklets guide SAM 3D Body to produce consistent full-body mesh trajectories, while a padding-based parallel strategy enables efficient multi-human inference. Experimental results demonstrate that SAM-Body4D achieves improved temporal stability and robustness in challenging in-the-wild videos, without any retraining. Our code and demo are available at: https://github.com/gaomingqi/sam-body4d.

</details>


### [41] [Towards Effective and Efficient Long Video Understanding of Multimodal Large Language Models via One-shot Clip Retrieval](https://arxiv.org/abs/2512.08410)
*Tao Chen,Shaobo Ju,Qiong Wu,Chenxin Fang,Kun Zhang,Jun Peng,Hui Li,Yiyi Zhou,Rongrong Ji*

Main category: cs.CV

TL;DR: 提出了一种名为OneClip-RAG的方法，它通过利用视频片段增强视频理解，并且具备查询引导的视频切块算法，从而提高指令遵循能力。该方法在长时间视频基准上的实验中展示了显著的性能提升和高效率。


<details>
  <summary>Details</summary>
Motivation: 大多数多模态大型语言模型由于内存开销过大，只能处理有限帧的视频，因此提出了OneClip-RAG方法来解决这一问题。

Method: OneClip-RAG方法包括一个查询引导的视频切块算法，它可以统一视频切块和跨模态检索，从而避免冗余计算。此外，还设计了一个名为SynLongVideo的新数据集，并提出了一个渐进式训练方案。

Result: OneClip-RAG方法在将自身融合到多个最近的多模态大型语言模型后，在一组长时间视频基准上的实验表明，该方法在性能上获得了显著提升，同时在处理长时间视频的效率上也具有优势。

Conclusion: 研究者们认为，OneClip-RAG方法对于改善当前多模态语言模型在处理长时间视频方面的能力具有重要意义。

Abstract: Due to excessive memory overhead, most Multimodal Large Language Models (MLLMs) can only process videos of limited frames. In this paper, we propose an effective and efficient paradigm to remedy this shortcoming, termed One-shot video-Clip based Retrieval AuGmentation (OneClip-RAG). Compared with existing video RAG methods, OneClip-RAG makes full use of the merits of video clips for augmented video understanding in terms of both knowledge integrity and semantic coherence. Besides, it is also equipped with a novel query-guided video chunking algorithm that can unify clip chunking and cross-modal retrieval in one processing step, avoiding redundant computations. To improve instruction following, we further propose a new dataset called SynLongVideo and design a progressive training regime for OneClip-RAG. OneClip-RAG is plugged into five recent MLLMs and validated on a set of long-video benchmarks. Experimental results not only show the obvious performance gains by OneClip-RAG over MLLMs, e.g., boosting InternLV2 8B and Qwen2-VL 7B to the level of GPT-4o on MLVU, but also show its superior efficiency in handling long videos. e.g., enabling LLaVA-Video understand up to an hour of videos in less than 2.2 minutes on a single 4090 GPU.

</details>


### [42] [SDT-6D: Fully Sparse Depth-Transformer for Staged End-to-End 6D Pose Estimation in Industrial Multi-View Bin Picking](https://arxiv.org/abs/2512.08430)
*Nico Leuze,Maximilian Hoh,Samed Doğan,Nicolas R. -Peña,Alfred Schoettl*

Main category: cs.CV

TL;DR: 本文提出了一种基于单目深度图的6D姿态估计方法，用于密集包装的工业拣选环境。该方法通过多视角深度图生成精细的3D点云或稀疏截断的-signed距离场(TSDF)，并在不同分辨率下提供场景自适应注意力先验，有效处理遮挡和纹理缺失等问题。


<details>
  <summary>Details</summary>
Motivation: 在密集包装的工业环境中准确恢复6D姿态是一项重大挑战，因为存在遮挡、反射和无纹理的问题。本文提出的方法旨在解决这些问题，特别是在近距离机器人应用中的潜在优势。

Method: 该方法融合了多视角深度图，生成精细的3D点云或稀疏截断的-signed距离场(TSDF)。通过引入分阶段的热图机制和密度感知的稀疏变压器块，增强了对自遮挡和3D数据非均匀分布的关注。

Result: 在IPD和MV-YCB多视角数据集上验证了该方法的有效性，展示了在高度拥挤的工业和家庭拣选场景中的竞争性能。

Conclusion: 该框架以完全稀疏的方式运行，能够捕获关键的几何细节，实现准确的6D姿态估计。

Abstract: Accurately recovering 6D poses in densely packed industrial bin-picking environments remain a serious challenge, owing to occlusions, reflections, and textureless parts. We introduce a holistic depth-only 6D pose estimation approach that fuses multi-view depth maps into either a fine-grained 3D point cloud in its vanilla version, or a sparse Truncated Signed Distance Field (TSDF). At the core of our framework lies a staged heatmap mechanism that yields scene-adaptive attention priors across different resolutions, steering computation toward foreground regions, thus keeping memory requirements at high resolutions feasible. Along, we propose a density-aware sparse transformer block that dynamically attends to (self-) occlusions and the non-uniform distribution of 3D data. While sparse 3D approaches has proven effective for long-range perception, its potential in close-range robotic applications remains underexplored. Our framework operates fully sparse, enabling high-resolution volumetric representations to capture fine geometric details crucial for accurate pose estimation in clutter. Our method processes the entire scene integrally, predicting the 6D pose via a novel per-voxel voting strategy, allowing simultaneous pose predictions for an arbitrary number of target objects. We validate our method on the recently published IPD and MV-YCB multi-view datasets, demonstrating competitive performance in heavily cluttered industrial and household bin picking scenarios.

</details>


### [43] [LapFM: A Laparoscopic Segmentation Foundation Model via Hierarchical Concept Evolving Pre-training](https://arxiv.org/abs/2512.08439)
*Qing Xu,Kun Yuan,Yuxiang Luo,Yuhao Zhai,Wenting Duan,Nassir Navab,Zhen Chen*

Main category: cs.CV

TL;DR: LapFM，一种用于腹腔镜手术场景理解的预训练模型，通过大规模无标签手术图像和独特的预训练策略，在分割任务上显著优于现有方法，提出了LCH和CDEL方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖有限监督进行微调，导致泛化能力有限；提出LapFM以解决手术分割中的标注稀缺和语义不一致问题，通过预训练获得良好的分割能力。

Method: LapFM使用一种双阶段预训练策略：首先通过层次概念进化预训练建立层次概念体系（LCH），然后通过置信驱动的进化标签生成（CDEL）迭代生成和过滤伪标签，逐步引入可靠标注。

Result: LapFM在多个数据集上优于现有技术，建立了新的通用腹腔镜分割标准。构建了包含114K图像-掩码对的LapBench基准数据集。

Conclusion: LapFM通过提出的预训练方法在无监督学习条件下显著提高了普适腹腔镜分割任务的性能。

Abstract: Surgical segmentation is pivotal for scene understanding yet remains hindered by annotation scarcity and semantic inconsistency across diverse procedures. Existing approaches typically fine-tune natural foundation models (e.g., SAM) with limited supervision, functioning merely as domain adapters rather than surgical foundation models. Consequently, they struggle to generalize across the vast variability of surgical targets. To bridge this gap, we present LapFM, a foundation model designed to evolve robust segmentation capabilities from massive unlabeled surgical images. Distinct from medical foundation models relying on inefficient self-supervised proxy tasks, LapFM leverages a Hierarchical Concept Evolving Pre-training paradigm. First, we establish a Laparoscopic Concept Hierarchy (LCH) via a hierarchical mask decoder with parent-child query embeddings, unifying diverse entities (i.e., Anatomy, Tissue, and Instrument) into a scalable knowledge structure with cross-granularity semantic consistency. Second, we propose a Confidence-driven Evolving Labeling that iteratively generates and filters pseudo-labels based on hierarchical consistency, progressively incorporating reliable samples from unlabeled images into training. This process yields LapBench-114K, a large-scale benchmark comprising 114K image-mask pairs. Extensive experiments demonstrate that LapFM significantly outperforms state-of-the-art methods, establishing new standards for granularity-adaptive generalization in universal laparoscopic segmentation. The source code is available at https://github.com/xq141839/LapFM.

</details>


### [44] [Leveraging Multispectral Sensors for Color Correction in Mobile Cameras](https://arxiv.org/abs/2512.08441)
*Luca Cogo,Marco Buzzelli,Simone Bianco,Javier Vazquez-Corral,Raimondo Schettini*

Main category: cs.CV

TL;DR: 本研究提出了一种端到端的学习框架，可以联合利用高分辨率RGB传感器和辅助低分辨率多光谱（MS）传感器的数据，进行色彩校正，整体上提高了色彩准确性并减少了误差。


<details>
  <summary>Details</summary>
Motivation: 现有的色彩校正方法通常将过程分为多个阶段，且常常早期丢弃MS数据。而本研究旨在解决这一问题，通过统一学习框架，提高色彩校正的准确性和稳定性。

Method: 研究者将端到端的色彩校正过程集成在一个单一的模型中，并重构了两种先进图像到图像的架构。同时，他们构建了一个专用的数据集，结合和重新利用了多个公开的光谱数据集，为不同RGB相机敏感度下的渲染提供支持。

Result: 与仅使用RGB和MS驱动的基线方法相比，该方法在色彩准确性上显著提升，降低错误率高达50%。

Conclusion: 该研究提出的方法能够在统一模型中联合使用RGB和低分辨率MS数据进行色彩校正，显著提高了色彩准确性和稳定性。所提出的框架具有灵活性和普适性。

Abstract: Recent advances in snapshot multispectral (MS) imaging have enabled compact, low-cost spectral sensors for consumer and mobile devices. By capturing richer spectral information than conventional RGB sensors, these systems can enhance key imaging tasks, including color correction. However, most existing methods treat the color correction pipeline in separate stages, often discarding MS data early in the process. We propose a unified, learning-based framework that (i) performs end-to-end color correction and (ii) jointly leverages data from a high-resolution RGB sensor and an auxiliary low-resolution MS sensor. Our approach integrates the full pipeline within a single model, producing coherent and color-accurate outputs. We demonstrate the flexibility and generality of our framework by refactoring two different state-of-the-art image-to-image architectures. To support training and evaluation, we construct a dedicated dataset by aggregating and repurposing publicly available spectral datasets, rendering under multiple RGB camera sensitivities. Extensive experiments show that our approach improves color accuracy and stability, reducing error by up to 50% compared to RGB-only and MS-driven baselines. Datasets, code, and models will be made available upon acceptance.

</details>


### [45] [Uncertainty-Aware Subset Selection for Robust Visual Explainability under Distribution Shifts](https://arxiv.org/abs/2512.08445)
*Madhav Gupta,Vishak Prasad C,Ganesh Ramakrishnan*

Main category: cs.CV

TL;DR: 本文通过在多种ID-OOD数据集上的实验，发现现有的基于子集的选择方法在OOD条件下可靠性降低，提出了结合子模性子集选择和逐层梯度不确定性估计的框架，以提高鲁棒性和精确性，同时避免额外训练或辅助模型。实验表明，该框架不仅在OOD场景中改善了现有方法的缺陷，也在ID场景中有所改进。这揭示了当前基于子集的方法的局限性，并展示了不确定性驱动的优化如何增强归因和对象级可解释性，为实际视觉应用中的更透明和可信赖的AI铺平了道路。


<details>
  <summary>Details</summary>
Motivation: 现有的基于子集的选择方法在ID设置下表现良好，但在OOD条件下其行为仍然不太被理解。本文旨在解决现有方法在OOD条件下的鲁棒性和精确性不足的问题。

Method: 该研究提出了一种结合子模性子集选择和逐层梯度不确定性估计的框架。通过适应性权重扰动估计不确定性，并使用这些估计值引导子模性优化，以确保多样性和信息丰富的子集选择。

Result: 实验结果表明，该框架不仅在OOD场景中改善了现有方法的缺陷，也在ID场景中有所改进。这表明，基于不确定性驱动的方法可以提高归因和对象级可解释性。

Conclusion: 该研究揭示了当前基于子集的方法的局限性，并展示了如何利用不确定性驱动的优化来增强AI的透明度和可信度，特别是在实际视觉应用中。

Abstract: Subset selection-based methods are widely used to explain deep vision models: they attribute predictions by highlighting the most influential image regions and support object-level explanations. While these methods perform well in in-distribution (ID) settings, their behavior under out-of-distribution (OOD) conditions remains poorly understood. Through extensive experiments across multiple ID-OOD sets, we find that reliability of the existing subset based methods degrades markedly, yielding redundant, unstable, and uncertainty-sensitive explanations. To address these shortcomings, we introduce a framework that combines submodular subset selection with layer-wise, gradient-based uncertainty estimation to improve robustness and fidelity without requiring additional training or auxiliary models. Our approach estimates uncertainty via adaptive weight perturbations and uses these estimates to guide submodular optimization, ensuring diverse and informative subset selection. Empirical evaluations show that, beyond mitigating the weaknesses of existing methods under OOD scenarios, our framework also yields improvements in ID settings. These findings highlight limitations of current subset-based approaches and demonstrate how uncertainty-driven optimization can enhance attribution and object-level interpretability, paving the way for more transparent and trustworthy AI in real-world vision applications.

</details>


### [46] [Team-Aware Football Player Tracking with SAM: An Appearance-Based Approach to Occlusion Recovery](https://arxiv.org/abs/2512.08467)
*Chamath Ranasinghe,Uthayasanker Thayasivam*

Main category: cs.CV

TL;DR: 本文提出了一种结合SAM和CSRT追踪器以及基于球衣颜色的外观模型的轻量级跟踪方法，用于足球场景中的目标追踪。这种方法通过SAM进行精确初始化，使用HSV直方图实现再识别，以改善遮挡恢复。实验表明，在轻度遮挡下可达100%的追踪成功率，在密集人群中的遮挡恢复达到50%。


<details>
  <summary>Details</summary>
Motivation: 为了解决足球场景中追踪面临的多目标拥挤遮挡、相似外观和快速运动的挑战，提升追踪系统的鲁棒性和准确性。

Method: 方法结合了Segment Anything Model (SAM) 和经典的CSRT追踪器，同时利用基于球衣颜色的外观模型辅助处理。具体步骤包括使用SAM进行精确的初始化，利用HSV直方图进行再识别。

Result: 实验结果显示，该方法可实现7.6-7.7 FPS的处理速度并维持稳定内存使用（~1880 MB）。在轻度遮挡下可达100%的追踪成功率，在密集的罚球区中（5人及以上）成功率为90%。基于外观的再识别方式能恢复50%的严重遮挡情况。

Conclusion: 研究揭示了经典追踪器方法在持续视线跟踪下的稳健性，但对于长时间遮挡的情况效果较差，需要增强的再识别机制以应对长期的缺席。

Abstract: Football player tracking is challenged by frequent occlusions, similar appearances, and rapid motion in crowded scenes. This paper presents a lightweight SAM-based tracking method combining the Segment Anything Model (SAM) with CSRT trackers and jersey color-based appearance models. We propose a team-aware tracking system that uses SAM for precise initialization and HSV histogram-based re-identification to improve occlusion recovery. Our evaluation measures three dimensions: processing speed (FPS and memory), tracking accuracy (success rate and box stability), and robustness (occlusion recovery and identity consistency). Experiments on football video sequences show that the approach achieves 7.6-7.7 FPS with stable memory usage (~1880 MB), maintaining 100 percent tracking success in light occlusions and 90 percent in crowded penalty-box scenarios with 5 or more players. Appearance-based re-identification recovers 50 percent of heavy occlusions, demonstrating the value of domain-specific cues. Analysis reveals key trade-offs: the SAM + CSRT combination provides consistent performance across crowd densities but struggles with long-term occlusions where players leave the frame, achieving only 8.66 percent re-acquisition success. These results offer practical guidelines for deploying football tracking systems under resource constraints, showing that classical tracker-based methods work well with continuous visibility but require stronger re-identification mechanisms for extended absences.

</details>


### [47] [ContextDrag: Precise Drag-Based Image Editing via Context-Preserving Token Injection and Position-Consistent Attention](https://arxiv.org/abs/2512.08477)
*Huiguo He,Pengyu Yan,Ziqi Yi,Weizhi Zhong,Zheng Liu,Yejun Tang,Huan Yang,Kun Gai,Guanbin Li,Lianwen Jin*

Main category: cs.CV

TL;DR: ContextDrag 通过引入 Context-preserving Token Injection 和 Position-Consistent Attention，有效提高了基于拖拽的图像编辑的上下文信息利用和编辑质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法在处理图像编辑时未能充分利用参考图像中的上下文信息，导致编辑效果不一致且细节不足，因此提出了.ContextDrag 算法来改进。

Method: ContextDrag 采用了一种新颖的 Context-preserving Token Injection (CTI) 方法，通过 Latent-space Reverse Mapping (LRM) 算法将无噪点的参考特征注入到正确的目标位置，同时引入了 Position-Consistent Attention (PCA) 对参考标记进行位置重编码，并应用重叠感知掩码。

Result: 在DragBench-SR 和 DragBench-DR 数据集上进行的广泛实验表明，ContextDrag 方法优于所有现有的 SOTA 方法。

Conclusion: ContextDrag 成功解决了基于拖拽的图像编辑中存在的问题，提供了更高质量和更一致的编辑结果。

Abstract: Drag-based image editing aims to modify visual content followed by user-specified drag operations. Despite existing methods having made notable progress, they still fail to fully exploit the contextual information in the reference image, including fine-grained texture details, leading to edits with limited coherence and fidelity. To address this challenge, we introduce ContextDrag, a new paradigm for drag-based editing that leverages the strong contextual modeling capability of editing models, such as FLUX-Kontext. By incorporating VAE-encoded features from the reference image, ContextDrag can leverage rich contextual cues and preserve fine-grained details, without the need for finetuning or inversion. Specifically, ContextDrag introduced a novel Context-preserving Token Injection (CTI) that injects noise-free reference features into their correct destination locations via a Latent-space Reverse Mapping (LRM) algorithm. This strategy enables precise drag control while preserving consistency in both semantics and texture details. Second, ContextDrag adopts a novel Position-Consistent Attention (PCA), which positional re-encodes the reference tokens and applies overlap-aware masking to eliminate interference from irrelevant reference features. Extensive experiments on DragBench-SR and DragBench-DR demonstrate that our approach surpasses all existing SOTA methods. Code will be publicly available.

</details>


### [48] [Temporal Concept Dynamics in Diffusion Models via Prompt-Conditioned Interventions](https://arxiv.org/abs/2512.08486)
*Ada Gorgun,Fawaz Sammani,Nikos Deligiannis,Bernt Schiele,Jonas Fischer*

Main category: cs.CV

TL;DR: 该研究提出了一种名为PCI（Prompt-Conditioned Intervention）的框架，通过分析概念插入成功率（CIS）来探究概念动态，揭示不同扩散模型在特定时间阶段更有利于某些概念的动态行为，这一发现为文本驱动的图像编辑提供了实用指导。


<details>
  <summary>Details</summary>
Motivation: 当前的扩散模型主要关注最终输出的质量，但忽视了生成过程中的动态变化，研究如何在生成过程中控制和理解概念的形成对于提高模型的可控性和可靠性至关重要。

Method: 研究提出了一个名为PCI的框架，该框架在扩散时间上分析概念插入的成功率（CIS），以此来表征概念形成的时间动态特性。

Result: 该研究应用于多个先进的文本到图像的扩散模型和多个概念类别，发现扩散模型在特定生成阶段对某些概念有利，并且提供的干预措施有助于实现更准确的语义和内容保留的编辑。

Conclusion: 该研究成果不仅揭示了扩散模型在不同时间阶段对概念形成的不同影响，还为文本驱动的图像编辑提供了有实际操作意义的建议，无需访问模型内部或重新训练。

Abstract: Diffusion models are usually evaluated by their final outputs, gradually denoising random noise into meaningful images. Yet, generation unfolds along a trajectory, and analyzing this dynamic process is crucial for understanding how controllable, reliable, and predictable these models are in terms of their success/failure modes. In this work, we ask the question: when does noise turn into a specific concept (e.g., age) and lock in the denoising trajectory? We propose PCI (Prompt-Conditioned Intervention) to study this question. PCI is a training-free and model-agnostic framework for analyzing concept dynamics through diffusion time. The central idea is the analysis of Concept Insertion Success (CIS), defined as the probability that a concept inserted at a given timestep is preserved and reflected in the final image, offering a way to characterize the temporal dynamics of concept formation. Applied to several state-of-the-art text-to-image diffusion models and a broad taxonomy of concepts, PCI reveals diverse temporal behaviors across diffusion models, in which certain phases of the trajectory are more favorable to specific concepts even within the same concept type. These findings also provide actionable insights for text-driven image editing, highlighting when interventions are most effective without requiring access to model internals or training, and yielding quantitatively stronger edits that achieve a balance of semantic accuracy and content preservation than strong baselines. Code is available at: https://github.com/adagorgun/PCI-Prompt-Controlled-Interventions

</details>


### [49] [Disrupting Hierarchical Reasoning: Adversarial Protection for Geographic Privacy in Multimodal Reasoning Models](https://arxiv.org/abs/2512.08503)
*Jiaming Zhang,Che Wang,Yang Cao,Longtao Huang,Wei Yang Bryan Lim*

Main category: cs.CV

TL;DR: 该研究提出了一种名为ReasonBreak的新颖对抗框架，通过概念相关的扰动破坏MLRM中的层次推理，从而保护个人隐私。实验结果显示，ReasonBreak显著提高了对地理位置推理的防护效果。


<details>
  <summary>Details</summary>
Motivation: 目前的隐私保护技术主要针对基于感知的模型，难以有效应对MLRM复杂的多层次推理过程。因此，研究者设计了ReasonBreak，以提供一种新的对抗层次推理的隐私保护方法。

Method: 研究者构建了ReasonBreak框架，通过分析关键的概念依赖关系，生成针对性的扰动。同时，他们还提供了一个包含6,341张高分辨率图像（≥2K）和层次概念注释的GeoPrivacy-6K数据集，以支持其方法的有效性验证。

Result: 在对七个最先进的MLRM模型（包括GPT-o3、GPT-5、Gemini 2.5 Pro）的广泛评估中，ReasonBreak表现出色，提高了地理推理保护效果，取得了14.4%的提升（从19.4%提高到33.8%），并且几乎将区块级防护提高了近一倍（从16.8%提高到33.5%）。

Conclusion: 研究提出了理由破坏（ReasonBreak），作为一种对抗MLRM推理的新隐私保护方法，并展示了其在保护地理隐私方面的显著效果。

Abstract: Multi-modal large reasoning models (MLRMs) pose significant privacy risks by inferring precise geographic locations from personal images through hierarchical chain-of-thought reasoning. Existing privacy protection techniques, primarily designed for perception-based models, prove ineffective against MLRMs' sophisticated multi-step reasoning processes that analyze environmental cues. We introduce \textbf{ReasonBreak}, a novel adversarial framework specifically designed to disrupt hierarchical reasoning in MLRMs through concept-aware perturbations. Our approach is founded on the key insight that effective disruption of geographic reasoning requires perturbations aligned with conceptual hierarchies rather than uniform noise. ReasonBreak strategically targets critical conceptual dependencies within reasoning chains, generating perturbations that invalidate specific inference steps and cascade through subsequent reasoning stages. To facilitate this approach, we contribute \textbf{GeoPrivacy-6K}, a comprehensive dataset comprising 6,341 ultra-high-resolution images ($\geq$2K) with hierarchical concept annotations. Extensive evaluation across seven state-of-the-art MLRMs (including GPT-o3, GPT-5, Gemini 2.5 Pro) demonstrates ReasonBreak's superior effectiveness, achieving a 14.4\% improvement in tract-level protection (33.8\% vs 19.4\%) and nearly doubling block-level protection (33.5\% vs 16.8\%). This work establishes a new paradigm for privacy protection against reasoning-based threats.

</details>


### [50] [On-the-fly Large-scale 3D Reconstruction from Multi-Camera Rigs](https://arxiv.org/abs/2512.08498)
*Yijia Guo,Tong Hu,Zhiwei Li,Liwen Hu,Keming Qian,Xitong Lin,Shengbo Chen,Tiejun Huang,Lei Ma*

Main category: cs.CV

TL;DR: 该研究提出了首个用于多相机阵列的即插即用3D重建框架，通过稠密RGB流的增量融合，实现了无漂移轨迹估计和高效实时重建。引入了无冗余的高斯采样策略和频率感知优化调度，以减少所需的高斯原语数量和优化迭代次数，同时保持效率和重建保真度。


<details>
  <summary>Details</summary>
Motivation: 近年来，三维高斯点云（3DGS）的发展使自由视角渲染和摄影测量恢复成为可能。然而，当前的单目RGB流实时重建方法由于视野有限，难以实现全面的3D覆盖。本研究旨在通过利用多相机阵列，克服视野限制，实现高效的即插即用3D重建。

Method: 该方法采用无冗余的高斯采样策略和频率感知优化调度，通过多相机流的增量融合，实现无漂移的轨迹估计和实时重建。具体过程包括无校准的多相机初始化，轻量级的多相机束调整，以及最终的高频次优化。

Result: 实验结果表明，该方法能够在仅使用多相机原生视频流的情况下，于2分钟内重建数百米的3D场景，显示出前所未有的速度、鲁棒性和重建保真度。

Conclusion: 该研究展示了多相机阵列即时3D重建框架的有效性，该框架能够高效地实时生成高质量的3D模型，即使在视野受限的情况下也能实现全面的3D覆盖。

Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have enabled efficient free-viewpoint rendering and photorealistic scene reconstruction. While on-the-fly extensions of 3DGS have shown promise for real-time reconstruction from monocular RGB streams, they often fail to achieve complete 3D coverage due to the limited field of view (FOV). Employing a multi-camera rig fundamentally addresses this limitation. In this paper, we present the first on-the-fly 3D reconstruction framework for multi-camera rigs. Our method incrementally fuses dense RGB streams from multiple overlapping cameras into a unified Gaussian representation, achieving drift-free trajectory estimation and efficient online reconstruction. We propose a hierarchical camera initialization scheme that enables coarse inter-camera alignment without calibration, followed by a lightweight multi-camera bundle adjustment that stabilizes trajectories while maintaining real-time performance. Furthermore, we introduce a redundancy-free Gaussian sampling strategy and a frequency-aware optimization scheduler to reduce the number of Gaussian primitives and the required optimization iterations, thereby maintaining both efficiency and reconstruction fidelity. Our method reconstructs hundreds of meters of 3D scenes within just 2 minutes using only raw multi-camera video streams, demonstrating unprecedented speed, robustness, and Fidelity for on-the-fly 3D scene reconstruction.

</details>


### [51] [Beyond the Noise: Aligning Prompts with Latent Representations in Diffusion Models](https://arxiv.org/abs/2512.08505)
*Vasco Ramos,Regev Cohen,Idan Szpektor,Joao Magalhaes*

Main category: cs.CV

TL;DR: 该研究提出了一种名为NoisyCLIP的方法，在图像生成的去噪过程中早期检测文本/图像不对齐，无需等待生成完成即可进行实时对齐评估，有效降低了计算成本并保持了语义精度。


<details>
  <summary>Details</summary>
Motivation: 当前条件扩散模型虽然能生成语义准确的输出，但仍存在对齐不准和幻觉的问题。传统的对齐检测方法在生成完成后才进行评估，效率较低。因此，本文提出了一种新的方法NoisyCLIP，可以在生成过程中的去噪阶段检测文本/图像的不对齐问题。

Method: NoisyCLIP通过在逆向扩散过程中使用双编码器，在嘈杂的潜在空间中测量语义对齐。这种方法首次在图像生成过程中探索和验证了从提示到潜在表示的对齐检测。

Result: 实验结果表明，NoisyCLIP可以在保持与CLIP对齐性能98%的情况下，使BoN设置中的计算成本降低50%。

Conclusion: 该方法能够在不等待完整生成的情况下，在图像生成过程中进行实时对齐评估，有效降低了成本并在不对齐的问题上避免了幻觉，提高了生成的效率和质量。

Abstract: Conditional diffusion models rely on language-to-image alignment methods to steer the generation towards semantically accurate outputs. Despite the success of this architecture, misalignment and hallucinations remain common issues and require automatic misalignment detection tools to improve quality, for example by applying them in a Best-of-N (BoN) post-generation setting. Unfortunately, measuring the alignment after the generation is an expensive step since we need to wait for the overall generation to finish to determine prompt adherence. In contrast, this work hypothesizes that text/image misalignments can be detected early in the denoising process, enabling real-time alignment assessment without waiting for the complete generation. In particular, we propose NoisyCLIP a method that measures semantic alignment in the noisy latent space. This work is the first to explore and benchmark prompt-to-latent misalignment detection during image generation using dual encoders in the reverse diffusion process. We evaluate NoisyCLIP qualitatively and quantitatively and find it reduces computational cost by 50% while achieving 98% of CLIP alignment performance in BoN settings. This approach enables real-time alignment assessment during generation, reducing costs without sacrificing semantic fidelity.

</details>


### [52] [Disturbance-Free Surgical Video Generation from Multi-Camera Shadowless Lamps for Open Surgery](https://arxiv.org/abs/2512.08577)
*Yuna Kato,Shohei Mori,Hideo Saito,Yoshifumi Takatsume,Hiroki Kajita,Mariko Isogawa*

Main category: cs.CV

TL;DR: 本文提出一种方法，自动识别照明系统移动的帧，并重新对齐它们。该方法选择较少遮挡的摄像头生成视频，从而保持手术区域的固定视角。实验证明，该方法生成的视频在确认手术区域的便利性和观看舒适度上优于传统方法，且在视频质量上有所提升。


<details>
  <summary>Details</summary>
Motivation: 传统的多摄像头设置虽然可以避免手术区域的遮挡，但仍需要手动对齐摄像头获取的图像，此过程繁琐且增加了额外的工作量。本文旨在通过自动对齐因照明变化而移动的帧来减少此过程的工作量并改进视频质量。

Method: 本文提出了一种方法，首先识别由于照明系统移动导致的帧变化，然后重新对齐这些帧。方法选择那些遮挡最少的摄像头来生成最终的视频。此外，该方法还提供了不同的合成选项供用户选择。

Result: 实验结果表明，使用本文提出的方法生成的视频在确认手术区域的便利性和观看舒适度上优于传统的多摄像头方法，并且在视频质量上有所改进。用户研究也显示，不同的合成选项受到用户的不同偏好。

Conclusion: 本文提出的方法能够完全自动化处理多摄像头设置中因照明变化导致的帧对齐问题，生成的视频具有更好的视觉效果和用户友好性。

Abstract: Video recordings of open surgeries are greatly required for education and research purposes. However, capturing unobstructed videos is challenging since surgeons frequently block the camera field of view. To avoid occlusion, the positions and angles of the camera must be frequently adjusted, which is highly labor-intensive. Prior work has addressed this issue by installing multiple cameras on a shadowless lamp and arranging them to fully surround the surgical area. This setup increases the chances of some cameras capturing an unobstructed view. However, manual image alignment is needed in post-processing since camera configurations change every time surgeons move the lamp for optimal lighting. This paper aims to fully automate this alignment task. The proposed method identifies frames in which the lighting system moves, realigns them, and selects the camera with the least occlusion to generate a video that consistently presents the surgical field from a fixed perspective. A user study involving surgeons demonstrated that videos generated by our method were superior to those produced by conventional methods in terms of the ease of confirming the surgical area and the comfort during video viewing. Additionally, our approach showed improvements in video quality over existing techniques. Furthermore, we implemented several synthesis options for the proposed view-synthesis method and conducted a user study to assess surgeons' preferences for each option.

</details>


### [53] [OCCDiff: Occupancy Diffusion Model for High-Fidelity 3D Building Reconstruction from Noisy Point Clouds](https://arxiv.org/abs/2512.08506)
*Jialu Sui,Rui Liu,Hongsheng Zhang*

Main category: cs.CV

TL;DR: 该研究提出了一种名为OCCDiff的方法，通过在占用函数空间中应用隐式扩散，灵活地从LiDAR点云中重建高保真的3D建筑表面。该方法结合了隐式扩散过程和函数自编码器架构，通过点编码器提供条件特征，增强模型的鲁棒性和多样性。


<details>
  <summary>Details</summary>
Motivation: 面对来自LiDAR点云建筑表面在不同点密度和噪声干扰下的重建挑战，为了获得高质量的3D建筑轮廓，研究开发了一种新的方法OCCDiff。

Method: OCCDiff方法将隐式扩散过程与函数自编码器架构相结合，生成可在任意位置评估的连续占用函数。同时，提出了一种点编码器，提供条件特征以控制占用解码器的最终占用预测，并为隐编码器中的潜在生成引入多模式特征。

Result: 实验结果表明，OCCDiff方法能够生成与目标分布高度一致的物理一致样本，并对噪声数据表现出良好的鲁棒性。

Conclusion: OCCDiff方法通过多任务训练策略确保模型学习到多样且鲁棒的特征表示，为从LiDAR点云中重建高保真的3D建筑表面提供了有力支持。

Abstract: A major challenge in reconstructing buildings from LiDAR point clouds lies in accurately capturing building surfaces under varying point densities and noise interference. To flexibly gather high-quality 3D profiles of the building in diverse resolution, we propose OCCDiff applying latent diffusion in the occupancy function space. Our OCCDiff combines a latent diffusion process with a function autoencoder architecture to generate continuous occupancy functions evaluable at arbitrary locations. Moreover, a point encoder is proposed to provide condition features to diffusion learning, constraint the final occupancy prediction for occupancy decoder, and insert multi-modal features for latent generation to latent encoder. To further enhance the model performance, a multi-task training strategy is employed, ensuring that the point encoder learns diverse and robust feature representations. Empirical results show that our method generates physically consistent samples with high fidelity to the target distribution and exhibits robustness to noisy data.

</details>


### [54] [Decoupling Template Bias in CLIP: Harnessing Empty Prompts for Enhanced Few-Shot Learning](https://arxiv.org/abs/2512.08606)
*Zhenyu Zhang,Guangyao Chen,Yixiong Zou,Zhimeng Huang,Yuhua Li*

Main category: cs.CV

TL;DR: 通过引入无类别提示，该研究减少CLIP模型中模板相似性引致的偏差，提高了分类准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: CLIP模型因模板-样本相似性(TSS)导致对视觉与文本真正对齐的依赖不足，此研究旨在消除这一偏差，提升模型性能。

Method: 该研究提出一种框架，使用无类别提示作为文本输入，该提示不带类别信息但传达“空”的概念。在预训练阶段，无类别提示揭示并减少了模板诱导的偏差；在少样本微调阶段，通过偏差校正损失确保图像与其类别间的正确对齐。

Result: 实验结果显示，该模板纠正方法显著减少了由TSS引起的表现波动，提高了分类准确性和鲁棒性。

Conclusion: 该研究提出的方法能够有效消除CLIP模型中的模板偏差，提高模型的性能和稳定性。

Abstract: The Contrastive Language-Image Pre-Training (CLIP) model excels in few-shot learning by aligning visual and textual representations. Our study shows that template-sample similarity (TSS), defined as the resemblance between a text template and an image sample, introduces bias. This bias leads the model to rely on template proximity rather than true sample-to-category alignment, reducing both accuracy and robustness in classification. We present a framework that uses empty prompts, textual inputs that convey the idea of "emptiness" without category information. These prompts capture unbiased template features and offset TSS bias. The framework employs two stages. During pre-training, empty prompts reveal and reduce template-induced bias within the CLIP encoder. During few-shot fine-tuning, a bias calibration loss enforces correct alignment between images and their categories, ensuring the model focuses on relevant visual cues. Experiments across multiple benchmarks demonstrate that our template correction method significantly reduces performance fluctuations caused by TSS, yielding higher classification accuracy and stronger robustness. The repository of this project is available at https://github.com/zhenyuZ-HUST/Decoupling-Template-Bias-in-CLIP.

</details>


### [55] [Aerial Vision-Language Navigation with a Unified Framework for Spatial, Temporal and Embodied Reasoning](https://arxiv.org/abs/2512.08639)
*Huilin Xu,Zhuoyang Liu,Yixiang Luomei,Feng Xu*

Main category: cs.CV

TL;DR: 该研究提出了一种基于单目RGB视觉和自然语言指令的统一航空VLN框架，能够完成场景理解、路径规划和动作预测，相比现有方法在单目RGB场景下表现更佳。


<details>
  <summary>Details</summary>
Motivation: 当前的Aerial VLN方法依赖全景图像、深度信息或航迹推算，增加了系统的成本和集成复杂度，阻碍了轻型无人机的实用部署。因此，本研究旨在开发一种基于单目RGB视觉的航空VLN方法，以简化系统实现并提高实际应用中的性能。

Method: 研究主要通过提示引导的多任务学习优化空间感知、路径推理和动作预测，采用关键帧选择策略减少视觉冗余，并提出了一种行为合并和标签重加权机制来缓解监督不平衡。

Result: 该方法在Aerial VLN基准数据集上进行了广泛实验，单目RGB设置下表现强劲，优于现有RGB基线，并减少了与全景RGB-D模型的性能差距。

Conclusion: 研究证明了该方法的有效性，并通过消融实验展示了任务设计和架构选择的贡献。这种方法提高了轻型无人机在复杂城市环境中的导航能力。

Abstract: Aerial Vision-and-Language Navigation (VLN) aims to enable unmanned aerial vehicles (UAVs) to interpret natural language instructions and navigate complex urban environments using onboard visual observation. This task holds promise for real-world applications such as low-altitude inspection, search-and-rescue, and autonomous aerial delivery. Existing methods often rely on panoramic images, depth inputs, or odometry to support spatial reasoning and action planning. These requirements increase system cost and integration complexity, thus hindering practical deployment for lightweight UAVs. We present a unified aerial VLN framework that operates solely on egocentric monocular RGB observations and natural language instructions. The model formulates navigation as a next-token prediction problem, jointly optimizing spatial perception, trajectory reasoning, and action prediction through prompt-guided multi-task learning. Moreover, we propose a keyframe selection strategy to reduce visual redundancy by retaining semantically informative frames, along with an action merging and label reweighting mechanism that mitigates long-tailed supervision imbalance and facilitates stable multi-task co-training. Extensive experiments on the Aerial VLN benchmark validate the effectiveness of our method. Under the challenging monocular RGB-only setting, our model achieves strong results across both seen and unseen environments. It significantly outperforms existing RGB-only baselines and narrows the performance gap with state-of-the-art panoramic RGB-D counterparts. Comprehensive ablation studies further demonstrate the contribution of our task design and architectural choices.

</details>


### [56] [PaintFlow: A Unified Framework for Interactive Oil Paintings Editing and Generation](https://arxiv.org/abs/2512.08534)
*Zhangli Hu,Ye Chen,Jiajun Yao,Bingbing Ni*

Main category: cs.CV

TL;DR: 本文提出了一种统一的多模态框架，用于油画生成和编辑。该框架允许用户通过参考图像、手绘草图和自然语言提示实现精确的语义控制和空间结构对齐，同时保持一致的绘画风格。通过引入空间对齐和语义增强的训练策略、基于笔画渲染的自我监督风格迁移管道和适配IN操作符进行风格一致性整合，该方法实现了交互式的油画创作。


<details>
  <summary>Details</summary>
Motivation: 本文针对现有技术和数据的限制，旨在提供一种更灵活和强大的油画生成和编辑方法，以更好地保留艺术质量并实现高度细腻的编辑。

Method: 本文方法包括三个关键技术创新：1）在训练阶段引入空间对齐和语义增强的策略，将遮罩和草图映射为空间约束，并从参考图像和文本中编码上下文嵌入为特征约束，以实现对象级别的语义对齐。2）提出一种基于笔画渲染的自我监督风格迁移管道，模拟油画修复的 inpainting 动力学，将真实图像转化为保留笔触纹理的风格化的油画图像，构建大规模配对训练数据集。3）在推理阶段使用适配IN操作符整合特征，以确保风格一致性。

Result: 实验结果表明，本文方法能够在保持油画艺术质量的同时实现精细的编辑，并在风格化油画生成和编辑中达到了前所未有的想象实现水平。

Conclusion: 本文提出的方法提供了一种新的多模态框架，能够灵活地生成和编辑油画，同时保留其艺术特性。

Abstract: Oil painting, as a high-level medium that blends human abstract thinking with artistic expression, poses substantial challenges for digital generation and editing due to its intricate brushstroke dynamics and stylized characteristics. Existing generation and editing techniques are often constrained by the distribution of training data and primarily focus on modifying real photographs. In this work, we introduce a unified multimodal framework for oil painting generation and editing. The proposed system allows users to incorporate reference images for precise semantic control, hand-drawn sketches for spatial structure alignment, and natural language prompts for high-level semantic guidance, while consistently maintaining a unified painting style across all outputs. Our method achieves interactive oil painting creation through three crucial technical advancements. First, we enhance the training stage with spatial alignment and semantic enhancement conditioning strategy, which map masks and sketches into spatial constraints, and encode contextual embedding from reference images and text into feature constraints, enabling object-level semantic alignment. Second, to overcome data scarcity, we propose a self-supervised style transfer pipeline based on Stroke-Based Rendering (SBR), which simulates the inpainting dynamics of oil painting restoration, converting real images into stylized oil paintings with preserved brushstroke textures to construct a large-scale paired training dataset. Finally, during inference, we integrate features using the AdaIN operator to ensure stylistic consistency. Extensive experiments demonstrate that our interactive system enables fine-grained editing while preserving the artistic qualities of oil paintings, achieving an unprecedented level of imagination realization in stylized oil paintings generation and editing.

</details>


### [57] [Mitigating Individual Skin Tone Bias in Skin Lesion Classification through Distribution-Aware Reweighting](https://arxiv.org/abs/2512.08733)
*Kuniko Paxton,Zeinab Dehghani,Koorosh Aslansefat,Dhavalkumar Thakker,Yiannis Papadopoulos*

Main category: cs.CV

TL;DR: 研究提出了一种基于分布的方法，用于评估和减轻皮肤病变分类中的个体公平性问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究往往使用粗略的群体分类，忽视了个体差异，可能导致对小众群体的偏差被掩盖。

Method: 该研究采用了基于分布的框架，将肤色视为连续属性，采用核密度估计（KDE）建模。同时比较了12种统计距离度量来量化肤色分布间的差异，并提出了一种基于距离的重新加权（DRW）损失函数来纠正少数肤色的代表性不足。

Result: 研究发现，基于分布的重新加权方法在多个模型中优于基于类别的重新加权方法，特别是通过Fidelity Similarity (FS)，Wasserstein Distance (WD)，Hellinger Metric (HM)和Harmonic Mean Similarity (HS)取得了更好的性能。

Conclusion: 研究提出的方法对于在皮肤科AI系统中推进个体级别的公平性有着坚实的基础，并强调了敏感连续属性在医学图像分析中的广泛意义。

Abstract: Skin color has historically been a focal point of discrimination, yet fairness research in machine learning for medical imaging often relies on coarse subgroup categories, overlooking individual-level variations. Such group-based approaches risk obscuring biases faced by outliers within subgroups. This study introduces a distribution-based framework for evaluating and mitigating individual fairness in skin lesion classification. We treat skin tone as a continuous attribute rather than a categorical label, and employ kernel density estimation (KDE) to model its distribution. We further compare twelve statistical distance metrics to quantify disparities between skin tone distributions and propose a distance-based reweighting (DRW) loss function to correct underrepresentation in minority tones. Experiments across CNN and Transformer models demonstrate: (i) the limitations of categorical reweighting in capturing individual-level disparities, and (ii) the superior performance of distribution-based reweighting, particularly with Fidelity Similarity (FS), Wasserstein Distance (WD), Hellinger Metric (HM), and Harmonic Mean Similarity (HS). These findings establish a robust methodology for advancing fairness at individual level in dermatological AI systems, and highlight broader implications for sensitive continuous attributes in medical image analysis.

</details>


### [58] [Photo3D: Advancing Photorealistic 3D Generation through Structure-Aligned Detail Enhancement](https://arxiv.org/abs/2512.08535)
*Xinyue Liang,Zhinyuan Ma,Lingchen Sun,Yanjun Guo,Lei Zhang*

Main category: cs.CV

TL;DR: Photo3D框架通过使用GPT-4o-Image生成的图像数据，设计了结构对齐的多视图合成管道和细节增强的数据集，提出了利用感知特征适应和语义结构匹配的现实细节增强方案，以提升3D生成的外观和结构一致性，从而实现了跨多种3D生成模型的高性能 photorealistic 3D生成。


<details>
  <summary>Details</summary>
Motivation: 现有的3D生成器在实现逼真的外观方面仍然存在不足，主要由于缺乏多样性和高质量的真实3D资产，这些资产具有丰富的纹理细节。Photo3D框架旨在通过利用高分辨率图像数据来推动光照真实渲染的3D生成。

Method: 提出了一种结构对齐的多视图合成管道，并构建了一个与3D几何相配套的细节增强多视图数据集。该方案利用感知特征适应和语义结构匹配来增强3D生成的外观一致性，同时保持结构一致性。为了优化3D几何和纹理的耦合与分离生成模式，提出了专门的训练策略。

Result: 实验表明，Photo3D框架在跨多种3D生成模型中表现出良好的泛化性能，并实现了最先进的光照真实3D生成效果。

Conclusion: Photo3D框架通过创新的多视图合成和细节增强策略，显著提升了3D生成的逼真度，为未来的3D重建和生成研究提供了有力支持。

Abstract: Although recent 3D-native generators have made great progress in synthesizing reliable geometry, they still fall short in achieving realistic appearances. A key obstacle lies in the lack of diverse and high-quality real-world 3D assets with rich texture details, since capturing such data is intrinsically difficult due to the diverse scales of scenes, non-rigid motions of objects, and the limited precision of 3D scanners. We introduce Photo3D, a framework for advancing photorealistic 3D generation, which is driven by the image data generated by the GPT-4o-Image model. Considering that the generated images can distort 3D structures due to their lack of multi-view consistency, we design a structure-aligned multi-view synthesis pipeline and construct a detail-enhanced multi-view dataset paired with 3D geometry. Building on it, we present a realistic detail enhancement scheme that leverages perceptual feature adaptation and semantic structure matching to enforce appearance consistency with realistic details while preserving the structural consistency with the 3D-native geometry. Our scheme is general to different 3D-native generators, and we present dedicated training strategies to facilitate the optimization of geometry-texture coupled and decoupled 3D-native generation paradigms. Experiments demonstrate that Photo3D generalizes well across diverse 3D-native generation paradigms and achieves state-of-the-art photorealistic 3D generation performance.

</details>


### [59] [Refining Visual Artifacts in Diffusion Models via Explainable AI-based Flaw Activation Maps](https://arxiv.org/abs/2512.08774)
*Seoyeon Lee,Gwangyeol Yu,Chaewon Kim,Jonghyuk Park*

Main category: cs.CV

TL;DR: 本文提出了一种增强自修复扩散模型，通过检测图像中的缺陷和不现实区域来提升图像生成质量。这种方法利用可解释的人工智能技术生成缺陷激活图，从而改善图像重建质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像生成方面取得了显著成果，但去除图像中的缺陷和不现实区域仍是关键挑战。为此，本文提出了一种可解释的人工智能缺陷检测框架，能够显著提高各种扩散模型的图像生成质量。

Method: 该框架包括生成缺陷激活图的功能组件，该组件使用可解释的人工智能技术来识别图像中的缺陷和不现实区域。在生成图像时，算法会放大这些区域的噪声；在生成过程中，算法会将注意力集中在这些区域上。

Result: 实验结果表明，该方法可以将Frechet Inception Distance提高27.3%，并且这项技术在不同的任务中表现出色，包括图像生成、文本到图像生成和修复。

Conclusion: 研究表明，解释型人工智能技术不仅可以用于可解释性，还能积极地提高图像的质量，该方法适用于多个扩散模型和任务，显著推动了图像合成领域的研究。

Abstract: Diffusion models have achieved remarkable success in image synthesis. However, addressing artifacts and unrealistic regions remains a critical challenge. We propose self-refining diffusion, a novel framework that enhances image generation quality by detecting these flaws. The framework employs an explainable artificial intelligence (XAI)-based flaw highlighter to produce flaw activation maps (FAMs) that identify artifacts and unrealistic regions. These FAMs improve reconstruction quality by amplifying noise in flawed regions during the forward process and by focusing on these regions during the reverse process. The proposed approach achieves up to a 27.3% improvement in Fréchet inception distance across various diffusion-based models, demonstrating consistently strong performance on diverse datasets. It also shows robust effectiveness across different tasks, including image generation, text-to-image generation, and inpainting. These results demonstrate that explainable AI techniques can extend beyond interpretability to actively contribute to image refinement. The proposed framework offers a versatile and effective approach applicable to various diffusion models and tasks, significantly advancing the field of image synthesis.

</details>


### [60] [Fast-ARDiff: An Entropy-informed Acceleration Framework for Continuous Space Autoregressive Generation](https://arxiv.org/abs/2512.08537)
*Zhen Zou,Xiaoxiao Ma,Jie Huang,Zichao Yu,Feng Zhao*

Main category: cs.CV

TL;DR: Fast-ARDiff 提出了一种新的统一框架，结合自动回归和扩散模型的优势，通过优化熵告知的前瞻性策略和端到端的动态调度方法，显著加快了这两种模型的合成速度，实现在不同模型上的加速效果。


<details>
  <summary>Details</summary>
Motivation: 现有的自动回归-扩散混合模型虽然具有良好的结构化建模和照片级真实合成能力，但由于顺序生成和迭代去噪等原因，存在较高的延迟问题，该工作旨在解决这一瓶颈。

Method: Fast-ARDiff框架采用了熵指导的前瞻性策略，在生成模型阶段鼓励生成更高熵的表示；同时将扩散编码部分与自动回归模型整合到相同的端到端框架中，通过动态调度策略优化扩散步骤。该框架还使用了联合蒸馏框架来确保训练的稳定性和高质量的合成效果。在推理阶段，使用浅层特征熵来预筛选低熵草稿样本，避免重复计算，进一步提升延时。

Result: 实验表明，Fast-ARDiff 在多种模型上实现了显著的加速效果。例如，在 ImageNet 256x256 数据集上，TransDiff 达到了 4.3 倍的无损加速，而在条件文本生成中，NextStep-1 达到了 3 倍的加速。

Conclusion: Fast-ARDiff 通过优化前瞻性和端到端的联合编码策略，提供了快速而高质量的图像和文本合成方法，具有广泛的适用性和显著的速度提升。

Abstract: Autoregressive(AR)-diffusion hybrid paradigms combine AR's structured modeling with diffusion's photorealistic synthesis, yet suffer from high latency due to sequential AR generation and iterative denoising. In this work, we tackle this bottleneck and propose a unified AR-diffusion framework Fast-ARDiff that jointly optimizes both components, accelerating AR speculative decoding while simultaneously facilitating faster diffusion decoding. Specifically: (1) The entropy-informed speculative strategy encourages draft model to produce higher-entropy representations aligned with target model's entropy characteristics, mitigating entropy mismatch and high rejection rates caused by draft overconfidence. (2) For diffusion decoding, rather than treating it as an independent module, we integrate it into the same end-to-end framework using a dynamic scheduler that prioritizes AR optimization to guide the diffusion part in further steps. The diffusion part is optimized through a joint distillation framework combining trajectory and distribution matching, ensuring stable training and high-quality synthesis with extremely few steps. During inference, shallow feature entropy from AR module is used to pre-filter low-entropy drafts, avoiding redundant computation and improving latency. Fast-ARDiff achieves state-of-the-art acceleration across diverse models: on ImageNet 256$\times$256, TransDiff attains 4.3$\times$ lossless speedup, and NextStep-1 achieves 3$\times$ acceleration on text-conditioned generation. Code will be available at https://github.com/aSleepyTree/Fast-ARDiff.

</details>


### [61] [An Iteration-Free Fixed-Point Estimator for Diffusion Inversion](https://arxiv.org/abs/2512.08547)
*Yifei Chen,Kaiyu Song,Yan Pan,Jianxing Yu,Jian Yin,Hanjiang Lai*

Main category: cs.CV

TL;DR: 该研究提出了一种迭代自由的固定点估算方法用于图像去噪反演，通过理论分析和实验验证，该方法在重建性能上优于其他基于固定点迭代的方法。


<details>
  <summary>Details</summary>
Motivation: 固定点迭代方法在图像去噪反演中广泛应用，但其计算成本高且对超参数的选择复杂。为解决这些问题，研究者提出了一种迭代自由的固定点估算方法。

Method: 研究者首先从理想的反演步骤中推导出固定点的显式表达式，由于该表达式包含未知的数据预测误差，从而引入误差近似，利用前一步的可计算误差来估计当前步的未知误差，得到一个可计算且无偏差的固定点近似表达式。

Result: 该方法在NOCAPS和MS-COCO两个文本-图像数据集上的重建性能上表现优于DDIM反演及其他基于固定点迭代的方法，无需额外迭代或训练。

Conclusion: 该研究提出了一种有效且计算效率更高的图像去噪反演方法。

Abstract: Diffusion inversion aims to recover the initial noise corresponding to a given image such that this noise can reconstruct the original image through the denoising diffusion process. The key component of diffusion inversion is to minimize errors at each inversion step, thereby mitigating cumulative inaccuracies. Recently, fixed-point iteration has emerged as a widely adopted approach to minimize reconstruction errors at each inversion step. However, it suffers from high computational costs due to its iterative nature and the complexity of hyperparameter selection. To address these issues, we propose an iteration-free fixed-point estimator for diffusion inversion. First, we derive an explicit expression of the fixed point from an ideal inversion step. Unfortunately, it inherently contains an unknown data prediction error. Building upon this, we introduce the error approximation, which uses the calculable error from the previous inversion step to approximate the unknown error at the current inversion step. This yields a calculable, approximate expression for the fixed point, which is an unbiased estimator characterized by low variance, as shown by our theoretical analysis. We evaluate reconstruction performance on two text-image datasets, NOCAPS and MS-COCO. Compared to DDIM inversion and other inversion methods based on the fixed-point iteration, our method achieves consistent and superior performance in reconstruction tasks without additional iterations or training.

</details>


### [62] [MatteViT: High-Frequency-Aware Document Shadow Removal with Shadow Matte Guidance](https://arxiv.org/abs/2512.08789)
*Chaewon Kim,Seoyeon Lee,Jonghyuk Park*

Main category: cs.CV

TL;DR: 该论文提出了一种新颖的文档阴影去除框架——MatteViT，该框架通过空间和频率域信息有效去除阴影同时保留细粒度结构细节。通过引入轻量级高频放大模块和连续亮度基于的遮罩，MatteViT 在公共基准测试中达到了最先进的性能，特别是在文本级细节保留方面优于先前的方法。


<details>
  <summary>Details</summary>
Motivation: 提出该模型的目的是为了增强数字化文档的清晰度，解决阴影造成的文本边缘和细线模糊问题，保留重要结构细节，以便于后续如光学字符识别等任务。

Method: MatteViT框架结合空间和频率域信息，通过轻量级高频放大模块（HFAM）和连续亮度基于的遮罩生成，实现阴影去除。HFAM用于分解并适配放大高频成分，遮罩生成部分则通过自定义遮罩数据集和生成器提供精确的空间指导。

Result: 实验结果表明，MatteViT在公共基准测试中表现出色，实现了最先进的性能，并且在光学字符识别等下游任务中保持了更好的文本级细节处理。

Conclusion: MatteViT提供了一个稳健且实用的文档阴影去除解决方案，不仅能准确识别细粒度区域，还能恢复高度忠实的文档细节，对于实际情况具有良好的应用潜力。

Abstract: Document shadow removal is essential for enhancing the clarity of digitized documents. Preserving high-frequency details (e.g., text edges and lines) is critical in this process because shadows often obscure or distort fine structures. This paper proposes a matte vision transformer (MatteViT), a novel shadow removal framework that applies spatial and frequency-domain information to eliminate shadows while preserving fine-grained structural details. To effectively retain these details, we employ two preservation strategies. First, our method introduces a lightweight high-frequency amplification module (HFAM) that decomposes and adaptively amplifies high-frequency components. Second, we present a continuous luminance-based shadow matte, generated using a custom-built matte dataset and shadow matte generator, which provides precise spatial guidance from the earliest processing stage. These strategies enable the model to accurately identify fine-grained regions and restore them with high fidelity. Extensive experiments on public benchmarks (RDD and Kligler) demonstrate that MatteViT achieves state-of-the-art performance, providing a robust and practical solution for real-world document shadow removal. Furthermore, the proposed method better preserves text-level details in downstream tasks, such as optical character recognition, improving recognition performance over prior methods.

</details>


### [63] [SSCATeR: Sparse Scatter-Based Convolution Algorithm with Temporal Data Recycling for Real-Time 3D Object Detection in LiDAR Point Clouds](https://arxiv.org/abs/2512.08557)
*Alexander Dow,Manduhu Manduhu,Matheus Santos,Ben Bartlett,Gerard Dooly,James Riordan*

Main category: cs.CV

TL;DR: 通过利用LiDAR扫描的连续运动，集中检测努力于点云变化区域，提出Sparse Scatter-Based Convolution Algorithm with Temporal Data Recycling (SSCATeR)算法，实现了6.61倍的处理时间减少。


<details>
  <summary>Details</summary>
Motivation: 减少LiDAR数据处理中的卷积计算量，提高检测效率，同时保持检测精度。

Method: 采用滑动时间窗和短步长，存储卷积结果并在多次通过时使用，将变化区域与不变区域分开处理，提出Sparse Scatter-Based Convolution Algorithm with Temporal Data Recycling (SSCATeR)。

Result: 与传统方法相比，SSCATeR在保持相同检测结果的情况下，将处理时间减少了最高6.61倍，显著提高了计算效率。

Conclusion: SSCATeR算法通过在LiDAR数据中识别变化区域并仅对其执行卷积操作，成功降低了处理负担，提高了系统的运行效率。

Abstract: This work leverages the continuous sweeping motion of LiDAR scanning to concentrate object detection efforts on specific regions that receive a change in point data from one frame to another. We achieve this by using a sliding time window with short strides and consider the temporal dimension by storing convolution results between passes. This allows us to ignore unchanged regions, significantly reducing the number of convolution operations per forward pass without sacrificing accuracy. This data reuse scheme introduces extreme sparsity to detection data. To exploit this sparsity, we extend our previous work on scatter-based convolutions to allow for data reuse, and as such propose Sparse Scatter-Based Convolution Algorithm with Temporal Data Recycling (SSCATeR). This operation treats incoming LiDAR data as a continuous stream and acts only on the changing parts of the point cloud. By doing so, we achieve the same results with as much as a 6.61-fold reduction in processing time. Our test results show that the feature maps output by our method are identical to those produced by traditional sparse convolution techniques, whilst greatly increasing the computational efficiency of the network.

</details>


### [64] [BrainExplore: Large-Scale Discovery of Interpretable Visual Representations in the Human Brain](https://arxiv.org/abs/2512.08560)
*Navve Wasserman,Matias Cosarinsky,Yuval Golbari,Aude Oliva,Antonio Torralba,Tamar Rott Shaham,Michal Irani*

Main category: cs.CV

TL;DR: 该研究提出了一种自动化框架，用于发现并解释人类神经皮层中的视觉表示，揭示了数千个可解释的模式，包括以前未报道的细粒度表示。


<details>
  <summary>Details</summary>
Motivation: 传统的神经科学研究局限于小规模样本和有限的区域，缺乏系统验证，无法全面理解视觉概念在大脑中的编码。

Method: 研究采用了一种包含两阶段的过程：首先，利用无监督的数据驱动分解方法发现候选可解释模式；其次，通过识别能强烈激活这些模式的自然图像并生成描述其共同视觉意义的自然语言来解释这些模式。为了提高效率，引入了自动化流水线测试多个候选解释、分配定量可靠性评分并为每个体素模式选择最一致的描述。

Result: 研究揭示了跨越许多不同视觉概念的数千个可解释模式，包括了以前未被报告的细粒度表示。

Conclusion: 该研究提出了一个新的自动化框架以更全面地揭示大脑如何编码视觉概念，该框架有助于神经科学领域的发展。

Abstract: Understanding how the human brain represents visual concepts, and in which brain regions these representations are encoded, remains a long-standing challenge. Decades of work have advanced our understanding of visual representations, yet brain signals remain large and complex, and the space of possible visual concepts is vast. As a result, most studies remain small-scale, rely on manual inspection, focus on specific regions and properties, and rarely include systematic validation. We present a large-scale, automated framework for discovering and explaining visual representations across the human cortex. Our method comprises two main stages. First, we discover candidate interpretable patterns in fMRI activity through unsupervised, data-driven decomposition methods. Next, we explain each pattern by identifying the set of natural images that most strongly elicit it and generating a natural-language description of their shared visual meaning. To scale this process, we introduce an automated pipeline that tests multiple candidate explanations, assigns quantitative reliability scores, and selects the most consistent description for each voxel pattern. Our framework reveals thousands of interpretable patterns spanning many distinct visual concepts, including fine-grained representations previously unreported.

</details>


### [65] [Training-Free Dual Hyperbolic Adapters for Better Cross-Modal Reasoning](https://arxiv.org/abs/2512.08820)
*Yi Zhang,Chun-Wun Cheng,Junyi He,Ke Yu,Yushun Tang,Carola-Bibiane Schönlieb,Zhihai He,Angelica I. Aviles-Rivero*

Main category: cs.CV

TL;DR: T-DHA方法通过在双曲空间中嵌入视觉-语言概念的层级关系，实现了跨模态推断能力的提升，在少数图像识别和领域泛化任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有视觉-语言模型在面对领域变化时性能下降，或者需要大量计算资源进行微调。T-DHA旨在解决这一问题。

Method: T-DHA在双曲空间中建模视觉-语言概念之间的层级关系，利用Poincaré球模型嵌入这种结构，结合负学习提高分类准确性。

Result: 在多种数据集上的实验结果表明，T-DHA方法明显优于现有最先进的方法，特别是在少数图像识别和领域泛化任务中。

Conclusion: T-DHA通过新颖的方法提升了视觉-语言模型的适应性和泛化能力。

Abstract: Recent research in Vision-Language Models (VLMs) has significantly advanced our capabilities in cross-modal reasoning. However, existing methods suffer from performance degradation with domain changes or require substantial computational resources for fine-tuning in new domains. To address this issue, we develop a new adaptation method for large vision-language models, called \textit{Training-free Dual Hyperbolic Adapters} (T-DHA). We characterize the vision-language relationship between semantic concepts, which typically has a hierarchical tree structure, in the hyperbolic space instead of the traditional Euclidean space. Hyperbolic spaces exhibit exponential volume growth with radius, unlike the polynomial growth in Euclidean space. We find that this unique property is particularly effective for embedding hierarchical data structures using the Poincaré ball model, achieving significantly improved representation and discrimination power. Coupled with negative learning, it provides more accurate and robust classifications with fewer feature dimensions. Our extensive experimental results on various datasets demonstrate that the T-DHA method significantly outperforms existing state-of-the-art methods in few-shot image recognition and domain generalization tasks.

</details>


### [66] [Modular Neural Image Signal Processing](https://arxiv.org/abs/2512.08564)
*Mahmoud Afifi,Zhongling Wang,Ran Zhang,Michael S. Brown*

Main category: cs.CV

TL;DR: 本文提出了一种模块化的神经图像信号处理（ISP）框架，能够对原始输入进行处理，生成高质量的显示相关图像，该框架具有高模块化程度，能够灵活渲染并支持用户预设风格的图像编辑，且模型大小适中，性能表现良好。


<details>
  <summary>Details</summary>
Motivation: 当前的神经ISP设计缺乏模块性，难以灵活匹配不同用户的偏好和适应未见过的相机。本文旨在通过引入模块化设计，提高ISP的可扩展性、可调试性、跨相机泛化能力和灵活性。

Method: 本文提出的方法是一个完全基于学习的框架，包括多个模块的处理流程。每个模块都可以独立训练和调整，从而提供了对多个渲染阶段的完全控制。

Result: 该模块化设计不仅实现了高渲染准确性，还在多个测试集中提供了具有竞争力的定性和定量结果。此外，还开发了一个用户交互的相册编辑工具，利用神经ISP支持各种编辑操作和图片风格。

Conclusion: 模块化神经ISP框架提供了可控的多阶段渲染过程，提高了ISP的性能和应用灵活性。

Abstract: This paper presents a modular neural image signal processing (ISP) framework that processes raw inputs and renders high-quality display-referred images. Unlike prior neural ISP designs, our method introduces a high degree of modularity, providing full control over multiple intermediate stages of the rendering process.~This modular design not only achieves high rendering accuracy but also improves scalability, debuggability, generalization to unseen cameras, and flexibility to match different user-preference styles. To demonstrate the advantages of this design, we built a user-interactive photo-editing tool that leverages our neural ISP to support diverse editing operations and picture styles. The tool is carefully engineered to take advantage of the high-quality rendering of our neural ISP and to enable unlimited post-editable re-rendering. Our method is a fully learning-based framework with variants of different capacities, all of moderate size (ranging from ~0.5 M to ~3.9 M parameters for the entire pipeline), and consistently delivers competitive qualitative and quantitative results across multiple test sets. Watch the supplemental video at: https://youtu.be/ByhQjQSjxVM

</details>


### [67] [InfiniteVL: Synergizing Linear and Sparse Attention for Highly-Efficient, Unlimited-Input Vision-Language Models](https://arxiv.org/abs/2512.08829)
*Hongyuan Tao,Bencheng Liao,Shaoyu Chen,Haoran Yin,Qian Zhang,Wenyu Liu,Xinggang Wang*

Main category: cs.CV

TL;DR: InfiniteVL是一种线性复杂度的Vision-Language模型架构，结合了滑动窗口注意机制和Gated DeltaNet，提出了三种训练策略，并且在资源受限条件下达到了可竞争的表现。


<details>
  <summary>Details</summary>
Motivation: 当前的Vision-Language模型中，窗式注意和线性注意各有不足，InfiniteVL旨在克服这些限制，解决长序列长度导致的性能下降问题以及信息密集任务中的表现不佳。

Method: InfiniteVL采用滑动窗口注意机制与Gated DeltaNet的组合，并设计了包含蒸馏预训练、指令微调和长序列SFT的三阶段训练策略。

Result: InfiniteVL在少于2%的训练数据下不仅超越了之前的线性复杂度模型，还与领先的Transformer基线模型表现相当，同时表现出有效的长期记忆保留能力。此外，模型的速度明显提升，具有更稳定的实时预填充速度。

Conclusion: InfiniteVL通过创新的架构和训练策略，在有限资源条件下取得了优异的性能，有望推动Vision-Language模型的发展。

Abstract: Window attention and linear attention represent two principal strategies for mitigating the quadratic complexity and ever-growing KV cache in Vision-Language Models (VLMs). However, we observe that window-based VLMs suffer performance degradation when sequence length exceeds the window size, while linear attention underperforms on information-intensive tasks such as OCR and document understanding. To overcome these limitations, we propose InfiniteVL, a linear-complexity VLM architecture that synergizes sliding window attention (SWA) with Gated DeltaNet. For achieving competitive multimodal performance under constrained resources, we design a three-stage training strategy comprising distillation pretraining, instruction tuning, and long-sequence SFT. Remarkably, using less than 2\% of the training data required by leading VLMs, InfiniteVL not only substantially outperforms previous linear-complexity VLMs but also matches the performance of leading Transformer-based VLMs, while demonstrating effective long-term memory retention. Compared to similar-sized Transformer-based VLMs accelerated by FlashAttention-2, InfiniteVL achieves over 3.6\times inference speedup while maintaining constant latency and memory footprint. In streaming video understanding scenarios, it sustains a stable 24 FPS real-time prefill speed while preserving long-term memory cache. Code and models are available at https://github.com/hustvl/InfiniteVL.

</details>


### [68] [Instance-Aware Test-Time Segmentation for Continual Domain Shifts](https://arxiv.org/abs/2512.08569)
*Seunghwan Lee,Inyoung Jung,Hojoon Lee,Eunil Park,Sungeun Hong*

Main category: cs.CV

TL;DR: 该研究提出了一种新颖的方法，能够在持续测试时自适应调整伪标签，以反映每张图像内的置信度分布，并动态平衡对受影响最严重的类别的学习，从而减少错误积累并提高在持续变化条件下的语义分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有的持续测试时适应方法通常使用固定或批处理级别的阈值，这不能适应不同类别和实例变化的难度。在语义分割任务中，这一限制尤为严重，因为每个图像需要密集的多类预测。

Method: 该方法通过计算每张图像内部的置信度分布，并根据这个分布自适应调整伪标签。同时，它还动态平衡了对受影响最严重的类别的学习。

Result: 通过在八个CTTA和TTA场景下的广泛实验，包括合成到现实和长期的转变，该方法在所有场景中均表现优于最先进的技术，特别是在持续变化条件下语义分割性能方面表现出色。

Conclusion: 该研究提出的方法在持续测试时适应语义分割任务中的应用证明了其有效性和优越性，为处理语义分割在动态变化环境下的挑战提供了新的标准。

Abstract: Continual Test-Time Adaptation (CTTA) enables pre-trained models to adapt to continuously evolving domains. Existing methods have improved robustness but typically rely on fixed or batch-level thresholds, which cannot account for varying difficulty across classes and instances. This limitation is especially problematic in semantic segmentation, where each image requires dense, multi-class predictions. We propose an approach that adaptively adjusts pseudo labels to reflect the confidence distribution within each image and dynamically balances learning toward classes most affected by domain shifts. This fine-grained, class- and instance-aware adaptation produces more reliable supervision and mitigates error accumulation throughout continual adaptation. Extensive experiments across eight CTTA and TTA scenarios, including synthetic-to-real and long-term shifts, show that our method consistently outperforms state-of-the-art techniques, setting a new standard for semantic segmentation under evolving conditions.

</details>


### [69] [From Cells to Survival: Hierarchical Analysis of Cell Inter-Relations in Multiplex Microscopy for Lung Cancer Prognosis](https://arxiv.org/abs/2512.08572)
*Olle Edgren Schüllerqvist,Jens Baumann,Joakim Lindblad,Love Nordling,Artur Mezheyeuski,Patrick Micke,Nataša Sladoje*

Main category: cs.CV

TL;DR: HiGINE 是一种基于图的层次结构方法，用于从多重免疫荧光图像中预测肺癌患者生存情况并增强风险分层。


<details>
  <summary>Details</summary>
Motivation: 肿瘤微环境 (TME) 已成为预测生物标志物的有希望来源。为了充分利用其潜力，分析方法必须捕捉不同细胞类型之间的复杂相互作用。

Method: HiGINE 模型结合了局部和全局细胞簇之间的关系，并整合了有关细胞类型和形态的信息。此外，它通过将癌症阶段与多重免疫荧光 (mIF) 提取的特征进行多模态融合来进一步提升性能。

Result: HiGINE 在两个公开数据集上的验证显示，它可以更有效地进行风险分层、提高稳健性和泛化能力。

Conclusion: HiGINE 为从 TME 特征中预测病人生存并改善肺癌风险分层提供了有效的解决方案。

Abstract: The tumor microenvironment (TME) has emerged as a promising source of prognostic biomarkers. To fully leverage its potential, analysis methods must capture complex interactions between different cell types. We propose HiGINE -- a hierarchical graph-based approach to predict patient survival (short vs. long) from TME characterization in multiplex immunofluorescence (mIF) images and enhance risk stratification in lung cancer. Our model encodes both local and global inter-relations in cell neighborhoods, incorporating information about cell types and morphology. Multimodal fusion, aggregating cancer stage with mIF-derived features, further boosts performance. We validate HiGINE on two public datasets, demonstrating improved risk stratification, robustness, and generalizability.

</details>


### [70] [No Labels, No Problem: Training Visual Reasoners with Multimodal Verifiers](https://arxiv.org/abs/2512.08889)
*Damiano Marsili,Georgia Gkioxari*

Main category: cs.CV

TL;DR: 文章提出了一种无需注释的训练框架，结合了LLM和VLM的优势，改进了视觉推理和视觉定位，超越了现有模型。


<details>
  <summary>Details</summary>
Motivation: 视觉推理具有挑战性，需要精确的对象定位和理解复杂的空间关系。现有方法要么依赖于大规模的（图像，查询，答案）监督，要么使用预训练模型而不进行训练，但存在逻辑错误和错误的定位问题。

Method: 该框架采用了AI验证器：通过强化学习改进LLM推理，并通过自动化硬负样本挖掘增强视觉定位，无需真实标签。该设计结合了现代AI系统的优点：先进的仅语言推理模型用于将空间查询分解为更简单的子任务，以及通过高效的VLM批评家改进的强大视觉专家模型。

Result: 该方法在多项空间推理任务上进行了评估，证明了其在视觉推理上的改进并且超过了开源和专有的模型，同时改进的视觉定位模型也超越了最近的纯文本视觉推理方法。

Conclusion: 文章提出的方法成功改进了视觉推理和视觉定位，并展现了其在多种任务上的优越性。

Abstract: Visual reasoning is challenging, requiring both precise object grounding and understanding complex spatial relationships. Existing methods fall into two camps: language-only chain-of-thought approaches, which demand large-scale (image, query, answer) supervision, and program-synthesis approaches which use pre-trained models and avoid training, but suffer from flawed logic and erroneous grounding. We propose an annotation-free training framework that improves both reasoning and grounding. Our framework uses AI-powered verifiers: an LLM verifier refines LLM reasoning via reinforcement learning, while a VLM verifier strengthens visual grounding through automated hard-negative mining, eliminating the need for ground truth labels. This design combines the strengths of modern AI systems: advanced language-only reasoning models for decomposing spatial queries into simpler subtasks, and strong vision specialist models improved via performant VLM critics. We evaluate our approach across diverse spatial reasoning tasks, and show that our method improves visual reasoning and surpasses open-source and proprietary models, while with our improved visual grounding model we further outperform recent text-only visual reasoning methods. Project webpage: https://glab-caltech.github.io/valor/

</details>


### [71] [Automated Pollen Recognition in Optical and Holographic Microscopy Images](https://arxiv.org/abs/2512.08589)
*Swarn Singh Warshaneyan,Maksims Ivanovs,Blaž Cugmas,Inese Bērziņa,Laura Goldberga,Mindaugas Tamosiunas,Roberts Kadiķis*

Main category: cs.CV

TL;DR: 该研究使用YOLOv8s进行目标检测，MobileNetV3L进行分类，针对光学和全息显微镜图像，特别是在兽医细胞学应用中，提高了检测和分类性能。通过数据集扩展和数据增强，实现了在光学图像上的优异结果，并在全息图像上显著改善了性能。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索深度学习在兽医细胞学中自动化的花粉粒检测和分类中的应用，以提高准确性和效率。

Method: 研究采用了YOLOv8s进行对象检测，MobileNetV3L进行分类任务，并针对不同的成像模态评估了它们的表现。

Result: 在光学图像上，模型实现了91.3%的mAP50检测精度和97%的整体分类准确率；在灰度全息图像上的初始性能较低。通过数据集扩展和数据增强技术，光学图像检测性能从2.49%提高到13.3%，分类精度从42%提高到54%。

Conclusion: 研究结果表明，至少对于图像分类任务，深度学习技术可以与低成本的无透镜数字全息显微镜设备相结合。

Abstract: This study explores the application of deep learning to improve and automate pollen grain detection and classification in both optical and holographic microscopy images, with a particular focus on veterinary cytology use cases. We used YOLOv8s for object detection and MobileNetV3L for the classification task, evaluating their performance across imaging modalities. The models achieved 91.3% mAP50 for detection and 97% overall accuracy for classification on optical images, whereas the initial performance on greyscale holographic images was substantially lower. We addressed the performance gap issue through dataset expansion using automated labeling and bounding box area enlargement. These techniques, applied to holographic images, improved detection performance from 2.49% to 13.3% mAP50 and classification performance from 42% to 54%. Our work demonstrates that, at least for image classification tasks, it is possible to pair deep learning techniques with cost-effective lensless digital holographic microscopy devices.

</details>


### [72] [Astra: General Interactive World Model with Autoregressive Denoising](https://arxiv.org/abs/2512.08931)
*Yixuan Zhu,Jiaqi Feng,Wenzhao Zheng,Yuan Gao,Xin Tao,Pengfei Wan,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: Astra通过自回归去噪架构和面向动作的适配器等技术，实现基于交互的泛化长远视频预测，支持多种互动形式，并在多个数据集上优于现有最先进的世界模型。


<details>
  <summary>Details</summary>
Motivation: 当前扩散变换器使视频生成模型从文本或图像生成高质量视频片段，但预测长期未来的能力不足。Astra旨在解决这一问题，特别是在通用场景和不同动作形式的预测方面。

Method: Astra采用自回归去噪架构，使用时序因果注意力来汇总过去观察，并支持流式输出。它采用噪声增强的过去记忆来避免过多依赖过去帧，平衡响应性和时间连贯性。Astra还引入了一个面向动作的适配器，直接将动作信号注入去噪过程。此外，Astra开发了一个动作专家混合体，动态路由异质动作模态，从而在各种现实任务中增强其灵活性。

Result: Astra实现了交互式、一致性和通用性的长时预测，并在多种数据集上提高了精度、长范围预测和动作对齐。

Conclusion: Astra为通用场景中的长远视频预测提供了新的方法，能够处理复杂的交互动作，具有广泛的应用前景。

Abstract: Recent advances in diffusion transformers have empowered video generation models to generate high-quality video clips from texts or images. However, world models with the ability to predict long-horizon futures from past observations and actions remain underexplored, especially for general-purpose scenarios and various forms of actions. To bridge this gap, we introduce Astra, an interactive general world model that generates real-world futures for diverse scenarios (e.g., autonomous driving, robot grasping) with precise action interactions (e.g., camera motion, robot action). We propose an autoregressive denoising architecture and use temporal causal attention to aggregate past observations and support streaming outputs. We use a noise-augmented history memory to avoid over-reliance on past frames to balance responsiveness with temporal coherence. For precise action control, we introduce an action-aware adapter that directly injects action signals into the denoising process. We further develop a mixture of action experts that dynamically route heterogeneous action modalities, enhancing versatility across diverse real-world tasks such as exploration, manipulation, and camera control. Astra achieves interactive, consistent, and general long-term video prediction and supports various forms of interactions. Experiments across multiple datasets demonstrate the improvements of Astra in fidelity, long-range prediction, and action alignment over existing state-of-the-art world models.

</details>


### [73] [OpenMonoGS-SLAM: Monocular Gaussian Splatting SLAM with Open-set Semantics](https://arxiv.org/abs/2512.08625)
*Jisang Yoo,Gyeongjin Kang,Hyun-kyu Ko,Hyeonwoo Yu,Eunbyung Park*

Main category: cs.CV

TL;DR: 本文提出了OpenMonoGS-SLAM，这是一种新颖的单目SLAM框架，结合了3D高斯点合并与开放领域语义理解。该方法利用最近在视觉基础模型方面的进展，包括MASt3R、SAM和CLIP，避免了深度传感器和闭合集语义模型的依赖，实现了在开放环境中的鲁棒语义理解。


<details>
  <summary>Details</summary>
Motivation: 随着空间AI的兴起，将SLAM与语义理解结合变得愈发重要。然而，现有方法往往依赖深度传感器或闭集语义模型，这限制了它们在开放世界环境中的适应性和扩展性。因此，本文旨在提出一种新的单目SLAM框架，能够同时进行3D建图和开放领域语义理解。

Method: 本文采用视觉基础模型（VFMs），如MASt3R、SAM和CLIP，进行视觉几何和开放词汇语义理解。该方法通过自监督学习目标进行训练，并引入特定的记忆机制来管理和构建高维语义特征图，从而提高整体性能。

Result: 实验结果显示，在封闭集和开放集语义分割任务中，本文提出的方法达到了或超过了现有基线的性能，无需依赖补充传感器如深度图或语义注释。

Conclusion: OpenMonoGS-SLAM为在单目SLAM中结合3D建图和开放领域语义理解提供了一种创新方法，展示了其在开放世界环境中的强大鲁棒性和适应性。

Abstract: Simultaneous Localization and Mapping (SLAM) is a foundational component in robotics, AR/VR, and autonomous systems. With the rising focus on spatial AI in recent years, combining SLAM with semantic understanding has become increasingly important for enabling intelligent perception and interaction. Recent efforts have explored this integration, but they often rely on depth sensors or closed-set semantic models, limiting their scalability and adaptability in open-world environments. In this work, we present OpenMonoGS-SLAM, the first monocular SLAM framework that unifies 3D Gaussian Splatting (3DGS) with open-set semantic understanding. To achieve our goal, we leverage recent advances in Visual Foundation Models (VFMs), including MASt3R for visual geometry and SAM and CLIP for open-vocabulary semantics. These models provide robust generalization across diverse tasks, enabling accurate monocular camera tracking and mapping, as well as a rich understanding of semantics in open-world environments. Our method operates without any depth input or 3D semantic ground truth, relying solely on self-supervised learning objectives. Furthermore, we propose a memory mechanism specifically designed to manage high-dimensional semantic features, which effectively constructs Gaussian semantic feature maps, leading to strong overall performance. Experimental results demonstrate that our approach achieves performance comparable to or surpassing existing baselines in both closed-set and open-set segmentation tasks, all without relying on supplementary sensors such as depth maps or semantic annotations.

</details>


### [74] [Trajectory Densification and Depth from Perspective-based Blur](https://arxiv.org/abs/2512.08627)
*Tianchen Qiu,Qirun Zhang,Jiajian He,Zhengyue Zhuge,Jiahui Xu,Yueting Chen*

Main category: cs.CV

TL;DR: 本文提出了一种通过分析视频流和密集轨迹中的模糊模式来估计米制深度的新方法。该方法利用光学设计算法，并结合预训练的视觉编码器和点追踪器提取视频信息，通过窗口嵌入和多窗口聚合估计深度图，并使用视觉-语言模型稀疏轨迹进行密集重建。


<details>
  <summary>Details</summary>
Motivation: 为了克服手持拍摄过程中相机不可避免的旋转动态带来的基于视角的模糊，特别是长时间曝光场景中，本文通过优化方法从视频流和密集轨迹中估计距离变化的模糊模式，以实现对物体深度的精确估计。

Method: 本文采用了一种结合视觉编码器和点追踪器提取视频信息的方法，并通过窗口嵌入和多窗口聚合估计深度图。此外，通过视觉-语言模型进一步稀疏轨迹进行密集重建。

Result: 通过在多个深度数据集上的评估，本文的方法在较宽的深度范围内展示了强大的性能，保持了良好的泛化能力。同时，与手持拍摄中的真实轨迹相比，本文的光学算法具有更高的精确度，并且密集重建也保持了很高的准确性。

Conclusion: 本文提出的新方法能够有效地估计米制深度，具有广泛的应用前景。

Abstract: In the absence of a mechanical stabilizer, the camera undergoes inevitable rotational dynamics during capturing, which induces perspective-based blur especially under long-exposure scenarios. From an optical standpoint, perspective-based blur is depth-position-dependent: objects residing at distinct spatial locations incur different blur levels even under the same imaging settings. Inspired by this, we propose a novel method that estimate metric depth by examining the blur pattern of a video stream and dense trajectory via joint optical design algorithm. Specifically, we employ off-the-shelf vision encoder and point tracker to extract video information. Then, we estimate depth map via windowed embedding and multi-window aggregation, and densify the sparse trajectory from the optical algorithm using a vision-language model. Evaluations on multiple depth datasets demonstrate that our method attains strong performance over large depth range, while maintaining favorable generalization. Relative to the real trajectory in handheld shooting settings, our optical algorithm achieves superior precision and the dense reconstruction maintains strong accuracy.

</details>


### [75] [Chain-of-Image Generation: Toward Monitorable and Controllable Image Generation](https://arxiv.org/abs/2512.08645)
*Young Kyung Kim,Oded Schlesinger,Yuzhou Zhao,J. Matias Di Martino,Guillermo Sapiro*

Main category: cs.CV

TL;DR: CoIG 提出了一种新的图像生成框架，通过将复杂提示分解为一系列简单的步骤，使图像生成过程具有可监测性并能精确控制，同时保持生成结果的质量。


<details>
  <summary>Details</summary>
Motivation: 当前最新的图像生成模型虽然视觉效果出色，但其内部生成过程仍不透明，影响了人类的观察和干预，也限制了模型的可靠性、安全性和控制性。

Method: CoIG 引入了一种将图像生成过程视作顺序语义级流程的方法，利用一个大语言模型（LLM）将复杂提示分解为一系列简单步骤，图像生成模型则按步骤生成和编辑图像。

Result: 通过 CoIG 的实施，研究实现了更高的定量监测能力，同时在组成鲁棒性方面与现有的基线模型竞争。虽然确保了生成过程的可观察性，但图像生成的质量也得到了保持。

Conclusion: CoIG 提供了一种提高图像生成模型可观察性、可控制性的同时，保持生成结果质量的新方法，该方法可用于任何图像生成模型。

Abstract: While state-of-the-art image generation models achieve remarkable visual quality, their internal generative processes remain a "black box." This opacity limits human observation and intervention, and poses a barrier to ensuring model reliability, safety, and control. Furthermore, their non-human-like workflows make them difficult for human observers to interpret. To address this, we introduce the Chain-of-Image Generation (CoIG) framework, which reframes image generation as a sequential, semantic process analogous to how humans create art. Similar to the advantages in monitorability and performance that Chain-of-Thought (CoT) brought to large language models (LLMs), CoIG can produce equivalent benefits in text-to-image generation. CoIG utilizes an LLM to decompose a complex prompt into a sequence of simple, step-by-step instructions. The image generation model then executes this plan by progressively generating and editing the image. Each step focuses on a single semantic entity, enabling direct monitoring. We formally assess this property using two novel metrics: CoIG Readability, which evaluates the clarity of each intermediate step via its corresponding output; and Causal Relevance, which quantifies the impact of each procedural step on the final generated image. We further show that our framework mitigates entity collapse by decomposing the complex generation task into simple subproblems, analogous to the procedural reasoning employed by CoT. Our experimental results indicate that CoIG substantially enhances quantitative monitorability while achieving competitive compositional robustness compared to established baseline models. The framework is model-agnostic and can be integrated with any image generation model.

</details>


### [76] [C-DIRA: Computationally Efficient Dynamic ROI Routing and Domain-Invariant Adversarial Learning for Lightweight Driver Behavior Recognition](https://arxiv.org/abs/2512.08647)
*Keito Inoshita*

Main category: cs.CV

TL;DR: C-DIRA 合并了基于显著性的Top-K ROI池化和融合分类，通过动态 ROI 选择和对抗学习提升了局部特征的提取和集成，从而在轻量级驾驶员行为识别中实现了高效性、紧凑性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前轻量模型难以捕捉细粒度的行为线索，而基于区域的兴趣方法也增加了计算成本，C-DIRA旨在解决这些问题，提高在边缘设备上的实时推理性能。

Method: C-DIRA框架结合了基于显著性选择的Top-K ROI池化和融合分类，用于局部特征的提取和集成。此外，通过伪领域标签和对抗学习来训练不变特征，以应对驾驶员和背景变化。

Result: C-DIRA在State Farm Distracted Driver Detection数据集上，具有较低的FLOPs和延迟，保持了高准确率。同时表现出对视觉退化的鲁棒性和跨未知域的稳定性能。

Conclusion: C-DIRA通过上述方法提高了轻量级驾驶员行为识别的效率、紧凑性和泛化能力，为实际应用中的实时推理提供了有效的解决方案。

Abstract: Driver distraction behavior recognition using in-vehicle cameras demands real-time inference on edge devices. However, lightweight models often fail to capture fine-grained behavioral cues, resulting in reduced performance on unseen drivers or under varying conditions. ROI-based methods also increase computational cost, making it difficult to balance efficiency and accuracy. This work addresses the need for a lightweight architecture that overcomes these constraints. We propose Computationally efficient Dynamic region of Interest Routing and domain-invariant Adversarial learning for lightweight driver behavior recognition (C-DIRA). The framework combines saliency-driven Top-K ROI pooling and fused classification for local feature extraction and integration. Dynamic ROI routing enables selective computation by applying ROI inference only to high difficulty data samples. Moreover, pseudo-domain labeling and adversarial learning are used to learn domain-invariant features robust to driver and background variation. Experiments on the State Farm Distracted Driver Detection Dataset show that C-DIRA maintains high accuracy with significantly fewer FLOPs and lower latency than prior lightweight models. It also demonstrates robustness under visual degradation such as blur and low-light, and stable performance across unseen domains. These results confirm C-DIRA's effectiveness in achieving compactness, efficiency, and generalization.

</details>


### [77] [Repulsor: Accelerating Generative Modeling with a Contrastive Memory Bank](https://arxiv.org/abs/2512.08648)
*Shaofeng Zhang,Xuanqi Chen,Ning Liao,Haoxiang Zhao,Xiaoxing Wang,Haoru Tan,Sitong Wu,Xiaosong Jia,Qi Fan,Junchi Yan*

Main category: cs.CV

TL;DR: 该研究提出了一种插件式的训练框架mNAME，通过引入动态更新的负样本库，无需额外依赖预训练模型或增加计算成本，实现了生成模型的高效训练，特别是在FID指标上达到了SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 当前生成模型如去噪生成模型在视觉合成中占据主导地位，但由于训练成本高和表示学习效率低而受到限制。研究引入了能够增强批次内部潜在表示间分离的策略，以减少对外部预训练编码器的依赖。为评估负样本数量在生成建模中的作用，提出mNAME框架来克服已有方法的缺点。

Method: mNAME框架包括建立动态更新的负样本库，使用低维度投影头降低内存和带宽开销。这种方法不依赖预训练视觉基础模型，并且在推理阶段不增加额外参数和计算成本。

Result: 实验表明，mNAME在ImageNet-256数据集上实现了400k步骤内的最佳FID得分2.40，显示出比同类方法更优的生成质量。

Conclusion: mNAME框架展示了生成模型无需外部预训练编码器的高效训练方式，并取得显著的SOTA生成效果。

Abstract: The dominance of denoising generative models (e.g., diffusion, flow-matching) in visual synthesis is tempered by their substantial training costs and inefficiencies in representation learning. While injecting discriminative representations via auxiliary alignment has proven effective, this approach still faces key limitations: the reliance on external, pre-trained encoders introduces overhead and domain shift. A dispersed-based strategy that encourages strong separation among in-batch latent representations alleviates this specific dependency. To assess the effect of the number of negative samples in generative modeling, we propose {\mname}, a plug-and-play training framework that requires no external encoders. Our method integrates a memory bank mechanism that maintains a large, dynamically updated queue of negative samples across training iterations. This decouples the number of negatives from the mini-batch size, providing abundant and high-quality negatives for a contrastive objective without a multiplicative increase in computational cost. A low-dimensional projection head is used to further minimize memory and bandwidth overhead. {\mname} offers three principal advantages: (1) it is self-contained, eliminating dependency on pretrained vision foundation models and their associated forward-pass overhead; (2) it introduces no additional parameters or computational cost during inference; and (3) it enables substantially faster convergence, achieving superior generative quality more efficiently. On ImageNet-256, {\mname} achieves a state-of-the-art FID of \textbf{2.40} within 400k steps, significantly outperforming comparable methods.

</details>


### [78] [What really matters for person re-identification? A Mixture-of-Experts Framework for Semantic Attribute Importance](https://arxiv.org/abs/2512.08697)
*Athena Psalta,Vasileios Tsironis,Konstantinos Karantzalos*

Main category: cs.CV

TL;DR: MoSAIC-ReID 是一个 Mixture-of-Experts 框架，通过 LoRA 基础专家和一个或acles 评级器，系统性地量化了行人属性在重新识别中的重要性。该研究不仅在 Market-1501 和 DukeMTMC 上达到了有竞争力的性能，更重要的是提供了一个大规模的定量研究，揭示了哪些属性如服饰颜色和固有特征对重新识别最具有贡献性，而稀有线索如配饰的影响较小。


<details>
  <summary>Details</summary>
Motivation: 现有的一流行人再识别方法具有出色的准确度但仍然不太透明，无法确定模型实际依赖哪种高级语义属性。MoSAIC-ReID 的目标是填补这一知识空白，通过一种新的 Mixture-of-Experts 框架，系统地量化不同行人属性对于行人再识别的重要性。

Method: MoSAIC-ReID 使用了 LoRA 基础专家机制，每个专家仅关联单一的属性，并通过或acles 评级器进行控制，利用正则化线性模型、统计测试和特征重要性分析等工具提供了一种系统化的可解释再识别方法。

Result: 在 Market-1501 和 DukeMTMC 数据集上的实验验证了 MoSAIC-ReID 的性能竞争力。更显著的是，该研究通过大规模分析揭示了哪些语义属性对再识别贡献最大，为实现实现可解释的行人再识别提供了依据。

Conclusion: MoSAIC-ReID 的贡献在于提供了一种全新的解析行人再识别的方法，理论上通过对不同属性重要性的研究，提出了整合明确语义知识的必要性，为未来改进和解释再识别技术奠定了基础。

Abstract: State-of-the-art person re-identification methods achieve impressive accuracy but remain largely opaque, leaving open the question: which high-level semantic attributes do these models actually rely on? We propose MoSAIC-ReID, a Mixture-of-Experts framework that systematically quantifies the importance of pedestrian attributes for re-identification. Our approach uses LoRA-based experts, each linked to a single attribute, and an oracle router that enables controlled attribution analysis. While MoSAIC-ReID achieves competitive performance on Market-1501 and DukeMTMC under the assumption that attribute annotations are available at test time, its primary value lies in providing a large-scale, quantitative study of attribute importance across intrinsic and extrinsic cues. Using generalized linear models, statistical tests, and feature-importance analyses, we reveal which attributes, such as clothing colors and intrinsic characteristics, contribute most strongly, while infrequent cues (e.g. accessories) have limited effect. This work offers a principled framework for interpretable ReID and highlights the requirements for integrating explicit semantic knowledge in practice. Code is available at https://github.com/psaltaath/MoSAIC-ReID

</details>


### [79] [SegEarth-OV3: Exploring SAM 3 for Open-Vocabulary Semantic Segmentation in Remote Sensing Images](https://arxiv.org/abs/2512.08730)
*Kaiyu Li,Shengqi Zhang,Yupeng Deng,Zhi Wang,Deyu Meng,Xiangyong Cao*

Main category: cs.CV

TL;DR: 本文通过利用SAM 3模型和融合策略，对遥感开放词汇语义分割任务进行了初步探索，展示了SAM 3在该任务上的潜力。


<details>
  <summary>Details</summary>
Motivation: 针对现有的基于CLIP的开放词汇语义分割方法在精确定位和复杂管线上的不足，本文提出了利用SAM 3模型在遥感场景下的开放词汇语义分割的新方法。

Method: 本文的方法包括实现一种掩码融合策略，将SAM 3的语义分割头和实例头的输出结合，利用存在分数过滤掉场景中不存在的类别。

Result: 实验表明，本文提出的方法在多个遥感数据集上取得了令人满意的效果，展示了SAM 3在开放词汇语义分割任务上的潜力。

Conclusion: 本文初步探索了SAM 3在遥感卵形容词语义分割任务中的应用，并提供了该代码的公开版本。

Abstract: Most existing methods for training-free Open-Vocabulary Semantic Segmentation (OVSS) are based on CLIP. While these approaches have made progress, they often face challenges in precise localization or require complex pipelines to combine separate modules, especially in remote sensing scenarios where numerous dense and small targets are present. Recently, Segment Anything Model 3 (SAM 3) was proposed, unifying segmentation and recognition in a promptable framework. In this paper, we present a preliminary exploration of applying SAM 3 to the remote sensing OVSS task without any training. First, we implement a mask fusion strategy that combines the outputs from SAM 3's semantic segmentation head and the Transformer decoder (instance head). This allows us to leverage the strengths of both heads for better land coverage. Second, we utilize the presence score from the presence head to filter out categories that do not exist in the scene, reducing false positives caused by the vast vocabulary sizes and patch-level processing in geospatial scenes. We evaluate our method on extensive remote sensing datasets. Experiments show that this simple adaptation achieves promising performance, demonstrating the potential of SAM 3 for remote sensing OVSS. Our code is released at https://github.com/earth-insights/SegEarth-OV-3.

</details>


### [80] [A Scalable Pipeline Combining Procedural 3D Graphics and Guided Diffusion for Photorealistic Synthetic Training Data Generation in White Button Mushroom Segmentation](https://arxiv.org/abs/2512.08747)
*Artúr I. Károly,Péter Galambos*

Main category: cs.CV

TL;DR: 本文提出了一种使用Blender 3D渲染与约束扩散模型的全新工作流程，以自动生成高质量、注释完整的蘑菇合成数据集，这些数据集适用于农业蘑菇培养，并展示了在使用合成数据训练的Mask R-CNN模型上的出色分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有的蘑菇监测和自动化收割依赖于计算机视觉，但准确的检测和分割模型的开发需要大量精确标注的数据集，这成本高昂。合成数据虽可提供规模化的解决方案，但往往缺乏真实性，无法很好地推广至真实场景。

Method: 该方法结合Blender中的3D渲染与约束扩散模型，自动生成高清、注释准确的合成蘑菇图像，同时保持3D场景配置的完全控制，无需专业计算机图形学知识，便能达到视觉现实。

Result: 研究发布了两个包含6000张图像（每个数据集中超过25万个蘑菇实例）的合成数据集，并在使用合成数据训练的Mask R-CNN模型上进行了评估。在零样本设置中，该方法在M18K基准测试上的分割性能达到了0.859的F1分数。

Conclusion: 该方法展示了使用合成数据训练的模型在真实世界数据上的优异表现，显示出合成数据在农业蘑菇领域监测和收获中的潜力，同时提出的方法流程可以应用于其他蘑菇种类或农业领域，如水果和叶片检测。

Abstract: Industrial mushroom cultivation increasingly relies on computer vision for monitoring and automated harvesting. However, developing accurate detection and segmentation models requires large, precisely annotated datasets that are costly to produce. Synthetic data provides a scalable alternative, yet often lacks sufficient realism to generalize to real-world scenarios. This paper presents a novel workflow that integrates 3D rendering in Blender with a constrained diffusion model to automatically generate high-quality annotated, photorealistic synthetic images of Agaricus Bisporus mushrooms. This approach preserves full control over 3D scene configuration and annotations while achieving photorealism without the need for specialized computer graphics expertise. We release two synthetic datasets (each containing 6,000 images depicting over 250k mushroom instances) and evaluate Mask R-CNN models trained on them in a zero-shot setting. When tested on two independent real-world datasets (including a newly collected benchmark), our method achieves state-of-the-art segmentation performance (F1 = 0.859 on M18K), despite using only synthetic training data. Although the approach is demonstrated on Agaricus Bisporus mushrooms, the proposed pipeline can be readily adapted to other mushroom species or to other agricultural domains, such as fruit and leaf detection.

</details>


### [81] [Skewness-Guided Pruning of Multimodal Swin Transformers for Federated Skin Lesion Classification on Edge Devices](https://arxiv.org/abs/2512.08751)
*Kuniko Paxton,Koorosh Aslansefat,Dhavalkumar Thakker,Yiannis Papadopoulos*

Main category: cs.CV

TL;DR: 该研究提出了一种基于统计偏度的剪枝方法，以减少多模态Swin Transformer模型的复杂性，并在联邦学习环境中进行了验证，表明可以在不影响准确性的基础上减小模型大小约36%。


<details>
  <summary>Details</summary>
Motivation: 随着高性能计算机视觉模型在医疗影像中取得显著成功，尤其是皮肤病变分类系统超过了皮肤科专家的诊断准确率。但由于计算密集型和大数据模型不适用于边缘设备部署，以及严格的隐私约束阻碍了集中式数据管理，因此促使研究者采用联邦学习（FL）来解决这些挑战。

Method: 该研究提出了一种基于统计偏度指导的剪枝方法，选择性地对多头自我注意层和多层感知器层进行剪枝，以保持模型的性能同时显著减少模型复杂性。

Result: 在水平联邦学习环境中验证了该方法的有效性，并通过紧凑型Swin Transformer实验展示了在不损失准确性的前提下，模型大小减少了约36%。

Conclusion: 该研究结果表明了实现高效模型压缩和隐私保护分布式学习在边缘设备上进行多模态医疗人工智能的可行性。

Abstract: In recent years, high-performance computer vision models have achieved remarkable success in medical imaging, with some skin lesion classification systems even surpassing dermatology specialists in diagnostic accuracy. However, such models are computationally intensive and large in size, making them unsuitable for deployment on edge devices. In addition, strict privacy constraints hinder centralized data management, motivating the adoption of Federated Learning (FL). To address these challenges, this study proposes a skewness-guided pruning method that selectively prunes the Multi-Head Self-Attention and Multi-Layer Perceptron layers of a multimodal Swin Transformer based on the statistical skewness of their output distributions. The proposed method was validated in a horizontal FL environment and shown to maintain performance while substantially reducing model complexity. Experiments on the compact Swin Transformer demonstrate approximately 36\% model size reduction with no loss in accuracy. These findings highlight the feasibility of achieving efficient model compression and privacy-preserving distributed learning for multimodal medical AI on edge devices.

</details>


### [82] [Wan-Move: Motion-controllable Video Generation via Latent Trajectory Guidance](https://arxiv.org/abs/2512.08765)
*Ruihang Chu,Yefei He,Zhekai Chen,Shiwei Zhang,Xiaogang Xu,Bin Xia,Dingdong Wang,Hongwei Yi,Xihui Liu,Hengshuang Zhao,Yu Liu,Yingya Zhang,Yujiu Yang*

Main category: cs.CV

TL;DR: Wan-Move 提出了一种简单可扩展的框架，实现了视频生成模型中的精细且高质量的运动控制，无需额外的结构修改，通过大规模训练生成 5 秒的 480p 视频，其运动控制效果与商业产品相当。此外，还设计了 MoveBench 作为模型评估基准。


<details>
  <summary>Details</summary>
Motivation: 现有方法在运动控制粒度和可扩展性上存在不足，导致生成视频的质量和实用性受限，Wan-Move 旨在解决这些局限性，实现精确和高质量的运动控制。

Method: Wan-Move 的核心思路是使原始条件特征具备运动感知能力，首先通过稠密点轨迹表示对象运动，实现细粒度的场景控制；然后将这些轨迹投影到潜在空间，并沿着每个轨迹传播初始帧的特征，生成对场景元素运动的对齐时空特征地图。此特征地图用作更新后的潜在条件，自然地集成到现成的图像到视频模型中，无需任何结构修改。

Result: Wan-Move 通过大规模训练生成了 5 秒的 480p 视频，其运动控制效果与商业产品 Kling 1.5 Pro 的 Motion Brush 相当；设计了 MoveBench 基准，提供多种内容类别和高质量的运动注释。

Conclusion: Wan-Move 突破了现有运动控制方法的限制，实现了高质量和细粒度的运动控制，且具有较高的可扩展性，展示了其在多种场景中的优越性。

Abstract: We present Wan-Move, a simple and scalable framework that brings motion control to video generative models. Existing motion-controllable methods typically suffer from coarse control granularity and limited scalability, leaving their outputs insufficient for practical use. We narrow this gap by achieving precise and high-quality motion control. Our core idea is to directly make the original condition features motion-aware for guiding video synthesis. To this end, we first represent object motions with dense point trajectories, allowing fine-grained control over the scene. We then project these trajectories into latent space and propagate the first frame's features along each trajectory, producing an aligned spatiotemporal feature map that tells how each scene element should move. This feature map serves as the updated latent condition, which is naturally integrated into the off-the-shelf image-to-video model, e.g., Wan-I2V-14B, as motion guidance without any architecture change. It removes the need for auxiliary motion encoders and makes fine-tuning base models easily scalable. Through scaled training, Wan-Move generates 5-second, 480p videos whose motion controllability rivals Kling 1.5 Pro's commercial Motion Brush, as indicated by user studies. To support comprehensive evaluation, we further design MoveBench, a rigorously curated benchmark featuring diverse content categories and hybrid-verified annotations. It is distinguished by larger data volume, longer video durations, and high-quality motion annotations. Extensive experiments on MoveBench and the public dataset consistently show Wan-Move's superior motion quality. Code, models, and benchmark data are made publicly available.

</details>


### [83] [Generation is Required for Data-Efficient Perception](https://arxiv.org/abs/2512.08854)
*Jack Brady,Bernhard Schölkopf,Thomas Kipf,Simon Buchholz,Wieland Brendel*

Main category: cs.CV

TL;DR: 该研究探讨了生成模型与非生成模型在实现人类级视觉感知中的对比。研究证明，生成模型可以通过约束解码器及其逆过程实现较好的组合泛化能力，而非生成模型即使经过大规模预训练或附加监督也难以实现良好的组合泛化。


<details>
  <summary>Details</summary>
Motivation: 文章旨在探讨生成模型与非生成模型在组合泛化能力上的差异，并指出生成模型通过内在生成机制能够更有效地实现这一目标。

Method: 研究通过理论分析证明编码器和解码器的方法在实现组合泛化所需的归纳偏置上的可行性差异，并通过实验验证不同方法在特定数据集上的表现。

Result: 实验结果表明，非生成方法即使进行大规模预训练或附加监督也只能在有限范围内实现有效组合泛化，而生成方法在遵循适当归纳偏置的情况下，无论是否有额外监督，都能表现出显著的组合泛化能力。

Conclusion: 研究结论强调了建立适当的生成机制对实现有效组合泛化的关键作用，并强调了解码器逆过程对于生成模型优越组合泛化能力的关键在于实现这一逆过程的效率和可行性。

Abstract: It has been hypothesized that human-level visual perception requires a generative approach in which internal representations result from inverting a decoder. Yet today's most successful vision models are non-generative, relying on an encoder that maps images to representations without decoder inversion. This raises the question of whether generation is, in fact, necessary for machines to achieve human-level visual perception. To address this, we study whether generative and non-generative methods can achieve compositional generalization, a hallmark of human perception. Under a compositional data generating process, we formalize the inductive biases required to guarantee compositional generalization in decoder-based (generative) and encoder-based (non-generative) methods. We then show theoretically that enforcing these inductive biases on encoders is generally infeasible using regularization or architectural constraints. In contrast, for generative methods, the inductive biases can be enforced straightforwardly, thereby enabling compositional generalization by constraining a decoder and inverting it. We highlight how this inversion can be performed efficiently, either online through gradient-based search or offline through generative replay. We examine the empirical implications of our theory by training a range of generative and non-generative methods on photorealistic image datasets. We find that, without the necessary inductive biases, non-generative methods often fail to generalize compositionally and require large-scale pretraining or added supervision to improve generalization. By comparison, generative methods yield significant improvements in compositional generalization, without requiring additional data, by leveraging suitable inductive biases on a decoder along with search and replay.

</details>


### [84] [Tri-Bench: Stress-Testing VLM Reliability on Spatial Reasoning under Camera Tilt and Object Interference](https://arxiv.org/abs/2512.08860)
*Amit Bendkhale*

Main category: cs.CV

TL;DR: Tri-Bench 是一个专注于测试视觉语言模型在平面三角问题中几何推理能力的基准，发现模型在处理立体几何信息和干扰物体方面表现不佳。


<details>
  <summary>Details</summary>
Motivation: 研究团队认为当前的视觉语言模型在面对现实场景变化时表现不佳，因此提出 Tri-Bench 来检验模型的几何推理能力。

Method: 设计了一个包含平面三角几何题的基准测试集，评估了四个近期的视觉语言模型。

Result: 实验结果显示，这些模型在三维几何推理和识别小众三角形类别上表现较差，且准确性会因相机倾斜角度增加而下降。

Conclusion: 研究证明当前的视觉语言模型难以充分利用提示中的参考框架信息，更倾向于依赖二维图像平面线索。

Abstract: Verifiable geometric reasoning is a critical component for trustworthy and controllable agentic AI. Despite impressive capabilities, Vision-Language Models (VLMs) often fail under realistic scene changes. We present Tri-Bench, a compact benchmark of planar triangle problems that isolates relative geometric reasoning while stressing two deployment-critical factors: camera pose (planar vs. tilted) and scene context via object interference (10 everyday objects). To test verifiability and control, we evaluate four recent VLMs using a single, fixed prompt whose guardrail explicitly describes a surrounding square border, enabling correct answers via homography. We evaluate six simple tasks over binary and continuous targets, and observe that the overall accuracy with respect to 3D ground truth is modest, ~69% on average (best ~75%, worst ~64%). The same responses align even more closely with 2D projections in the image plane, where mean accuracy is ~72%. All four VLMs consistently fail, with accuracy falling to ~0%, on recognizing minority shape classes (equilateral, isosceles, right-angled triangles). Additionally, overall VLM accuracy degrades by ~4.1% under camera tilt. This demonstrates that models fail to correctly utilize the explicit frame-of-reference hint provided in the prompt and default to 2D image plane cues. Finally, we find that object interference has no significant effect on VLM accuracy.

</details>


### [85] [SATGround: A Spatially-Aware Approach for Visual Grounding in Remote Sensing](https://arxiv.org/abs/2512.08881)
*Aysim Toker,Andreea-Maria Oncescu,Roy Miles,Ismail Elezi,Jiankang Deng*

Main category: cs.CV

TL;DR: 本文通过提出一种新颖的结构化定位机制，对预训练的视觉语言模型进行微调以处理多样化的指令遵循任务，并通过专门的控制令牌接口专用的地面模块，显著提高了卫星遥感图像中物体精确定位的能力。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型（VLMs）已经被证明是强大的通用工具，能够跨多个任务整合信息，并通过聊天界面实现灵活的指令驱动交互。本文旨在通过结构化定位机制增强VLM在卫星图像中的视觉定位能力。

Method: 本文通过在多样化的指令遵循任务上微调预训练的VLM，并引入专用的定位模块，使用特殊的控制标记与VLM交互，实现语言与空间信息的联合推理。

Result: 该框架在多种遥感基准测试中均优于现有方法，提高了24.8%的视觉定位性能。

Conclusion: 研究结果表明，在视觉语言模型中集成结构化的空间推理对实际卫星数据的可靠分析尤为重要。

Abstract: Vision-language models (VLMs) are emerging as powerful generalist tools for remote sensing, capable of integrating information across diverse tasks and enabling flexible, instruction-based interactions via a chat interface. In this work, we enhance VLM-based visual grounding in satellite imagery by proposing a novel structured localization mechanism. Our approach involves finetuning a pretrained VLM on a diverse set of instruction-following tasks, while interfacing a dedicated grounding module through specialized control tokens for localization. This method facilitates joint reasoning over both language and spatial information, significantly enhancing the model's ability to precisely localize objects in complex satellite scenes. We evaluate our framework on several remote sensing benchmarks, consistently improving the state-of-the-art, including a 24.8% relative improvement over previous methods on visual grounding. Our results highlight the benefits of integrating structured spatial reasoning into VLMs, paving the way for more reliable real-world satellite data analysis.

</details>


### [86] [UniLayDiff: A Unified Diffusion Transformer for Content-Aware Layout Generation](https://arxiv.org/abs/2512.08897)
*Zeyang Liu,Le Wang,Sanping Zhou,Yuxuan Wu,Xiaolong Sun,Gang Hua,Haoxiang Li*

Main category: cs.CV

TL;DR: 提出了一种名为UniLayDiff的统一扩散变换器，能够以端到端的方式处理多种内容感知布局生成任务，通过多模态扩散变换器框架捕捉背景图像、布局元素及其各类型约束之间的复杂相互作用，同时利用LoRA微调来整合关系约束，从而实现统一的条件生成并增强整体布局质量。


<details>
  <summary>Details</summary>
Motivation: 现实世界中多种图形设计任务的多样化需求促使寻求一个单一模型能够同时处理不同类型、大小或关系等内容感知布局生成任务的方法。

Method: 通过将布局约束作为一种独特的模态，并利用多模态扩散变换器框架来建模背景图像、布局元素及不同约束之间的复杂交互作用，通过基于其他任务预训练后再使用LoRA进行微调来整合关系约束。

Result: 实验证明，UniLayDiff在无条件生成及各种条件生成任务中达到了最先进的性能，并且是首个能够统一所有内容感知布局生成任务的模型。

Conclusion: UniLayDiff为多样化的图形设计自动化任务提供了一个强大的解决方案，展示了多模态扩散变换器在复杂布局生成任务中的潜力，并通过LoRA方式提升了模型泛化能力。

Abstract: Content-aware layout generation is a critical task in graphic design automation, focused on creating visually appealing arrangements of elements that seamlessly blend with a given background image. The variety of real-world applications makes it highly challenging to develop a single model capable of unifying the diverse range of input-constrained generation sub-tasks, such as those conditioned by element types, sizes, or their relationships. Current methods either address only a subset of these tasks or necessitate separate model parameters for different conditions, failing to offer a truly unified solution. In this paper, we propose UniLayDiff: a Unified Diffusion Transformer, that for the first time, addresses various content-aware layout generation tasks with a single, end-to-end trainable model. Specifically, we treat layout constraints as a distinct modality and employ Multi-Modal Diffusion Transformer framework to capture the complex interplay between the background image, layout elements, and diverse constraints. Moreover, we integrate relation constraints through fine-tuning the model with LoRA after pretraining the model on other tasks. Such a schema not only achieves unified conditional generation but also enhances overall layout quality. Extensive experiments demonstrate that UniLayDiff achieves state-of-the-art performance across from unconditional to various conditional generation tasks and, to the best of our knowledge, is the first model to unify the full range of content-aware layout generation tasks.

</details>


### [87] [Self-Evolving 3D Scene Generation from a Single Image](https://arxiv.org/abs/2512.08905)
*Kaizhi Zheng,Yue Fan,Jing Gu,Zishuo Xu,Xuehai He,Xin Eric Wang*

Main category: cs.CV

TL;DR: EvoScene 是一个自进化、无需训练的框架，可以逐步从单张图像中重建完整的3D场景。它通过结合3D生成模型的几何推理能力和视频生成模型的视觉知识，在2D和3D领域之间交替工作，提升结构和外观。


<details>
  <summary>Details</summary>
Motivation: 现有的单图到3D生成器能够恢复合理的几何结构，但它们的对象中心化训练限制了它们在复杂大规模场景中的泛化能力。因此，为了生成高质量、具有结构和纹理一致性的3D场景，提出了EvoScene框架。

Method: EvoScene框架分为三个迭代阶段：空间先验初始化、基于视觉的3D场景网格生成和基于空间的新型视角生成。在每个阶段，模型在2D和3D领域之间交替工作，逐步提升3D场景的结构和外观。

Result: EvoScene在多种场景上的实验表明，它能够实现比强基线更好的几何稳定性、视图一致的纹理以及未见区域的补全，生成可以直接应用于实际场景的3D网格。

Conclusion: EvoScene框架展示了一种新的思路，即通过结合不同模型的优势，实现从单图到复杂3D场景的高质量重建。

Abstract: Generating high-quality, textured 3D scenes from a single image remains a fundamental challenge in vision and graphics. Recent image-to-3D generators recover reasonable geometry from single views, but their object-centric training limits generalization to complex, large-scale scenes with faithful structure and texture. We present EvoScene, a self-evolving, training-free framework that progressively reconstructs complete 3D scenes from single images. The key idea is combining the complementary strengths of existing models: geometric reasoning from 3D generation models and visual knowledge from video generation models. Through three iterative stages--Spatial Prior Initialization, Visual-guided 3D Scene Mesh Generation, and Spatial-guided Novel View Generation--EvoScene alternates between 2D and 3D domains, gradually improving both structure and appearance. Experiments on diverse scenes demonstrate that EvoScene achieves superior geometric stability, view-consistent textures, and unseen-region completion compared to strong baselines, producing ready-to-use 3D meshes for practical applications.

</details>


### [88] [LiDAS: Lighting-driven Dynamic Active Sensing for Nighttime Perception](https://arxiv.org/abs/2512.08912)
*Simon de Moreau,Andrei Bursuc,Hafid El-Idrissi,Fabien Moutarde*

Main category: cs.CV

TL;DR: LiDAS 是一种闭合环路的主动照明系统，通过动态预测最优照明场来优化下游感知性能，并在保持性能的同时减少能源消耗。


<details>
  <summary>Details</summary>
Motivation: 现有的夜间环境下的相机感知方法依赖于场景照明，夜间场景的低照度成为感知性能优化的一大障碍。LiDAS 通过动态照明提高了夜间场景下的感知能力，使得白天训练的模型能够零样本泛化到夜间场景。

Method: LiDAS 结合了现成的视觉感知模型和高分辨率前大灯。系统能够根据感知需求动态调整照明强度，将空白区域的光线重新分配给目标区域，从而增强感知效果。

Result: LiDAS 在合成数据集上训练，并在真实世界的闭合环路驾驶场景中实现零样本泛化。与标准低光束相比，在相同功率条件下，LiDAS 提高了18.7% 的mAP50 和5.0% 的mIoU，同时能源使用降低了40%。

Conclusion: LiDAS 提供了一个经济高效的解决方案，充分利用现有的前大灯，转化为主动视觉执行器来提升夜间感知能力。该系统增强了模型的鲁棒性，无需重新训练即可适应不同领域。

Abstract: Nighttime environments pose significant challenges for camera-based perception, as existing methods passively rely on the scene lighting. We introduce Lighting-driven Dynamic Active Sensing (LiDAS), a closed-loop active illumination system that combines off-the-shelf visual perception models with high-definition headlights. Rather than uniformly brightening the scene, LiDAS dynamically predicts an optimal illumination field that maximizes downstream perception performance, i.e., decreasing light on empty areas to reallocate it on object regions. LiDAS enables zero-shot nighttime generalization of daytime-trained models through adaptive illumination control. Trained on synthetic data and deployed zero-shot in real-world closed-loop driving scenarios, LiDAS enables +18.7% mAP50 and +5.0% mIoU over standard low-beam at equal power. It maintains performances while reducing energy use by 40%. LiDAS complements domain-generalization methods, further strengthening robustness without retraining. By turning readily available headlights into active vision actuators, LiDAS offers a cost-effective solution to robust nighttime perception.

</details>


### [89] [Efficiently Reconstructing Dynamic Scenes One D4RT at a Time](https://arxiv.org/abs/2512.08924)
*Chuhan Zhang,Guillaume Le Moing,Skanda Koppula,Ignacio Rocco,Liliane Momeni,Junyu Xie,Shuyang Sun,Rahul Sukthankar,Joëlle K Barral,Raia Hadsell,Zoubin Ghahramani,Andrew Zisserman,Junlin Zhang,Mehdi SM Sajjadi*

Main category: cs.CV

TL;DR: 本文提出了一种名为D4RT的简单而强大的前馈模型，该模型利用统一的变压器架构从单个视频中联合推断深度、时空对应关系和全部相机参数，显著提高4D重建任务的效率，并达到了新的顶级水平。


<details>
  <summary>Details</summary>
Motivation: 针对动态场景的几何形状和运动理解与重建的挑战，本研究旨在提出一种高效且灵活的方法，应用于4D重建任务。

Method: D4RT采用了一种创新的查询机制，可以独立且灵活地探测时空中的任何点的三维位置，避免了密集且每帧的解码计算，以及多任务特定解码器的管理复杂性。

Result: 通过使用D4RT模型，研究结果显示在多种4D重建任务上实现了新的顶级性能。

Conclusion: 与现有的方法相比，D4RT提供了一种轻量级且高度可扩展的解决方案，具有高效训练和推理能力，显著减少了计算复杂性，适用于动态场景的重建挑战。

Abstract: Understanding and reconstructing the complex geometry and motion of dynamic scenes from video remains a formidable challenge in computer vision. This paper introduces D4RT, a simple yet powerful feedforward model designed to efficiently solve this task. D4RT utilizes a unified transformer architecture to jointly infer depth, spatio-temporal correspondence, and full camera parameters from a single video. Its core innovation is a novel querying mechanism that sidesteps the heavy computation of dense, per-frame decoding and the complexity of managing multiple, task-specific decoders. Our decoding interface allows the model to independently and flexibly probe the 3D position of any point in space and time. The result is a lightweight and highly scalable method that enables remarkably efficient training and inference. We demonstrate that our approach sets a new state of the art, outperforming previous methods across a wide spectrum of 4D reconstruction tasks. We refer to the project webpage for animated results: https://d4rt-paper.github.io/.

</details>


### [90] [Selfi: Self Improving Reconstruction Engine via 3D Geometric Feature Alignment](https://arxiv.org/abs/2512.08930)
*Youming Deng,Songyou Peng,Junyi Zhang,Kathryn Heal,Tiancheng Sun,John Flynn,Steve Marschner,Lucy Chai*

Main category: cs.CV

TL;DR: Selfi 是一种自我改进的 3D 重建流水线，通过特征对齐将 VGGT 的特征空间转换为几何对齐的特征空间，从而提高多视图几何一致性，进而提升 NVS 和相机姿态估计任务的表现。


<details>
  <summary>Details</summary>
Motivation: 传统 NVS 模型依赖于带有显式 3D 诱导偏置的模型和已知的摄像机参数。VGGT 则通过训练数据和损失目标隐式地获得 3D 知识，使得可以从前置的一组未经校准的图像中直接预测相机参数和 3D 表征。然而，VGGT 特征缺乏显式的多视图几何一致性。

Method: Selfi 通过引入一种利用特征对齐机制的轻量级特征适配器进行训练，该适配器使用基于再投影的一致性损失来将 VGGT 输出提炼到一个几何对齐的特征空间，该空间能够捕捉 3D 空间中的邻近性。

Result: Selfi 在 NVS 和相机姿态估计任务上都达到了最先进的表现。

Conclusion: 特征对齐是用于下游 3D 理解的一个非常有益的步骤。

Abstract: Novel View Synthesis (NVS) has traditionally relied on models with explicit 3D inductive biases combined with known camera parameters from Structure-from-Motion (SfM) beforehand. Recent vision foundation models like VGGT take an orthogonal approach -- 3D knowledge is gained implicitly through training data and loss objectives, enabling feed-forward prediction of both camera parameters and 3D representations directly from a set of uncalibrated images. While flexible, VGGT features lack explicit multi-view geometric consistency, and we find that improving such 3D feature consistency benefits both NVS and pose estimation tasks. We introduce Selfi, a self-improving 3D reconstruction pipeline via feature alignment, transforming a VGGT backbone into a high-fidelity 3D reconstruction engine by leveraging its own outputs as pseudo-ground-truth. Specifically, we train a lightweight feature adapter using a reprojection-based consistency loss, which distills VGGT outputs into a new geometrically-aligned feature space that captures spatial proximity in 3D. This enables state-of-the-art performance in both NVS and camera pose estimation, demonstrating that feature alignment is a highly beneficial step for downstream 3D reasoning.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [91] [Segment, Embed, and Align: A Universal Recipe for Aligning Subtitles to Signing](https://arxiv.org/abs/2512.08094)
*Zifan Jiang,Youngjoon Jang,Liliane Momeni,Gül Varol,Sarah Ebling,Andrew Zisserman*

Main category: cs.CL

TL;DR: 该工作提出了一种通用的方法（名为Segment, Embed, and Align，简称SEA），用于将字幕与连续的手语视频对齐，适用于多种手语语言和场景。


<details>
  <summary>Details</summary>
Motivation: 传统的端到端训练方法通常依赖于特定的手语语言或数据集，这限制了其通用性。SEA方法提供了一个跨多种手语语言和领域的单一框架。

Method: SEA方法分为三个步骤：首先将视频帧序列分割成单个手语动作；其次，将每个手语片段嵌入到共享的潜在空间中，与文本对齐；最后使用轻量级的动态规划程序进行对齐。

Result: 在四种手语数据集上的实验显示SEA方法达到了行业领先的效果，证明了它可以生成高质量的平行数据，促进手语处理进步。

Conclusion: SEA方法不仅在性能上有所提升，而且其代码和模型也是开源的，便于其他研究者进一步开发和应用。

Abstract: The goal of this work is to develop a universal approach for aligning subtitles (i.e., spoken language text with corresponding timestamps) to continuous sign language videos. Prior approaches typically rely on end-to-end training tied to a specific language or dataset, which limits their generality. In contrast, our method Segment, Embed, and Align (SEA) provides a single framework that works across multiple languages and domains. SEA leverages two pretrained models: the first to segment a video frame sequence into individual signs and the second to embed the video clip of each sign into a shared latent space with text. Alignment is subsequently performed with a lightweight dynamic programming procedure that runs efficiently on CPUs within a minute, even for hour-long episodes. SEA is flexible and can adapt to a wide range of scenarios, utilizing resources from small lexicons to large continuous corpora. Experiments on four sign language datasets demonstrate state-of-the-art alignment performance, highlighting the potential of SEA to generate high-quality parallel data for advancing sign language processing. SEA's code and models are openly available.

</details>


### [92] [Universal Adversarial Suffixes Using Calibrated Gumbel-Softmax Relaxation](https://arxiv.org/abs/2512.08123)
*Sampriti Soor,Suklav Ghosh,Arijit Sur*

Main category: cs.CL

TL;DR: 该研究探索了通用对抗后缀，这是一种能够在多种任务和模型上减少准确性和校准置信度的短词序列。通过采用Gumbel-Softmax松弛学习可微的后缀形式，并在掩码黄金标记的情况下最大化校准交叉熵损失，实现在不同模型间有效的转移。


<details>
  <summary>Details</summary>
Motivation: 现有的语言模型在零样本或少样本分类中使用时容易受到对抗提示的影响，且此前的工作通常针对特定任务或模型进行优化，这使得结果难以比较并限制了转移性。这项研究旨在开发一种通用的对抗后缀，可以在不同任务和模型间进行有效转移，从而提高模型的鲁棒性。

Method: 研究采用Gumbel-Softmax松弛技术以学习可微的后缀形式，通过最大化标签区域的校准交叉熵损失并在掩码黄金标记的情况下进行训练。此外还使用了熵正则化以避免后缀退化为单个标记。

Result: 研究发现，一个在单一模型上训练的后缀可以在其他模型上有效转移，持续降低准确性和校准置信度，实验涵盖了情感分析、自然语言推理、同义判断、常识问答以及物理推理等多个任务和不同模型系列。

Conclusion: 该研究揭示了通用对抗后缀的存在和应用，为提升模型鲁棒性提供了新的方法。

Abstract: Language models (LMs) are often used as zero-shot or few-shot classifiers by scoring label words, but they remain fragile to adversarial prompts. Prior work typically optimizes task- or model-specific triggers, making results difficult to compare and limiting transferability. We study universal adversarial suffixes: short token sequences (4-10 tokens) that, when appended to any input, broadly reduce accuracy across tasks and models. Our approach learns the suffix in a differentiable "soft" form using Gumbel-Softmax relaxation and then discretizes it for inference. Training maximizes calibrated cross-entropy on the label region while masking gold tokens to prevent trivial leakage, with entropy regularization to avoid collapse. A single suffix trained on one model transfers effectively to others, consistently lowering both accuracy and calibrated confidence. Experiments on sentiment analysis, natural language inference, paraphrase detection, commonsense QA, and physical reasoning with Qwen2-1.5B, Phi-1.5, and TinyLlama-1.1B demonstrate consistent attack effectiveness and transfer across tasks and model families.

</details>


### [93] [Universal Adversarial Suffixes for Language Models Using Reinforcement Learning with Calibrated Reward](https://arxiv.org/abs/2512.08131)
*Sampriti Soor,Suklav Ghosh,Arijit Sur*

Main category: cs.CL

TL;DR: 本文提出了一种使用强化学习框架来发现语言模型的对抗后缀的方法，这种方法能够有效泛化到不同的任务和模型，并且相较于之前的对抗触发器具有更显著的效果。


<details>
  <summary>Details</summary>
Motivation: 现有的方法虽然能够发现对抗后缀，但往往脆弱且仅针对单一任务或模型有效，本文旨在提出一种更鲁棒、更具泛化能力的方法。

Method: 本文采用强化学习框架，将对抗后缀视为策略并使用Proximal Policy Optimization进行训练，以冻结的模型作为奖励预言机。奖励通过校准的交叉熵来塑造，以减少标签偏差并提高泛化能力。

Result: 该方法在五个不同NLP基准数据集上进行了评估，涵盖了情感分析、自然语言推理、同义替换和常识推理，并使用了三种不同的语言模型：Qwen2-1.5B Instruct、TinyLlama-1.1B Chat和Phi-1.5。实验结果表明，与相似类型的先前对抗触发器相比，通过强化学习训练的后缀在各方面都表现出更具优势的效果。

Conclusion: 本文提出的方法能够更有效地发现并利用对抗后缀来削弱语言模型的预测，具备更强的转移学习能力，为提升模型安全性和鲁棒性提供了新的思路。

Abstract: Language models are vulnerable to short adversarial suffixes that can reliably alter predictions. Previous works usually find such suffixes with gradient search or rule-based methods, but these are brittle and often tied to a single task or model. In this paper, a reinforcement learning framework is used where the suffix is treated as a policy and trained with Proximal Policy Optimization against a frozen model as a reward oracle. Rewards are shaped using calibrated cross-entropy, removing label bias and aggregating across surface forms to improve transferability. The proposed method is evaluated on five diverse NLP benchmark datasets, covering sentiment, natural language inference, paraphrase, and commonsense reasoning, using three distinct language models: Qwen2-1.5B Instruct, TinyLlama-1.1B Chat, and Phi-1.5. Results show that RL-trained suffixes consistently degrade accuracy and transfer more effectively across tasks and models than previous adversarial triggers of similar genres.

</details>


### [94] [ClinicalTrialsHub: Bridging Registries and Literature for Comprehensive Clinical Trial Access](https://arxiv.org/abs/2512.08193)
*Jiwoo Park,Ruoqi Liu,Avani Jagdale,Andrew Srisuwananukorn,Jing Zhao,Lang Li,Ping Zhang,Sachin Kumar*

Main category: cs.CL

TL;DR: ClinicalTrialsHub 提供了一个综合 ClinicalTrials.gov 和 PubMed 数据的交互式平台，通过自动抽取和结构化临床试验相关信息，提升了 83.8% 的数据访问量，使用了大语言模型提升可访问性。


<details>
  <summary>Details</summary>
Motivation: 为了简化临床试验数据的访问，使患者、医生、研究人员和政策制定者更容易获得信息，从而推动循证医学的发展。

Method: 利用大型语言模型（如 GPT-5.1 和 Gemini-3-Pro）增强平台的可访问性，自动解析全文研究文章以提取结构化的试验信息，并将用户查询转化为结构化的数据库搜索。

Result: 与仅依赖 ClinicalTrials.gov 相比，ClinicalTrialsHub 提升了 83.8% 的数据访问量，通过用户研究和系统自动评估证实了其信息抽取和问答功能的有效性。

Conclusion: ClinicalTrialsHub 是一个能够大幅提高临床试验数据访问效率的平台，具有广泛的应用前景。

Abstract: We present ClinicalTrialsHub, an interactive search-focused platform that consolidates all data from ClinicalTrials.gov and augments it by automatically extracting and structuring trial-relevant information from PubMed research articles. Our system effectively increases access to structured clinical trial data by 83.8% compared to relying on ClinicalTrials.gov alone, with potential to make access easier for patients, clinicians, researchers, and policymakers, advancing evidence-based medicine. ClinicalTrialsHub uses large language models such as GPT-5.1 and Gemini-3-Pro to enhance accessibility. The platform automatically parses full-text research articles to extract structured trial information, translates user queries into structured database searches, and provides an attributed question-answering system that generates evidence-grounded answers linked to specific source sentences. We demonstrate its utility through a user study involving clinicians, clinical researchers, and PhD students of pharmaceutical sciences and nursing, and a systematic automatic evaluation of its information extraction and question answering capabilities.

</details>


### [95] [Are generative AI text annotations systematically biased?](https://arxiv.org/abs/2512.08404)
*Sjoerd B. Stolwijk,Mark Boukes,Damian Trilling*

Main category: cs.CL

TL;DR: 本研究通过使用多种生成型语言模型（GLLM）和不同概念的提示，复制了手动注释，发现了GLLM在F1分数上表现良好，但在其他方面与手动注释存在显著不同，显示出系统性偏见。


<details>
  <summary>Details</summary>
Motivation: 鉴于生成型语言模型（GLLM）在自然语言处理中的广泛应用，本文试图评估GLLM注释的准确性及其偏见，以改进其应用和理解其局限性。

Method: 使用多种GLLM（Llama3.1:8b, Llama3.3:70b, GPT4o, Qwen2.5:72b）和五个概念（政治内容、互动性、理性、不文明和意识形态）的不同提示进行注释比较。

Result: GLLM在F1分数上表现良好，但在预估值、下游结果和系统性偏见方面与手动注释有显著差异。

Conclusion: GLLM的偏见不能仅通过F1分数来解释，这些发现强调了使用的挑战，并为改善模型和提高其准确性提供了指导。

Abstract: This paper investigates bias in GLLM annotations by conceptually replicating manual annotations of Boukes (2024). Using various GLLMs (Llama3.1:8b, Llama3.3:70b, GPT4o, Qwen2.5:72b) in combination with five different prompts for five concepts (political content, interactivity, rationality, incivility, and ideology). We find GLLMs perform adequate in terms of F1 scores, but differ from manual annotations in terms of prevalence, yield substantively different downstream results, and display systematic bias in that they overlap more with each other than with manual annotations. Differences in F1 scores fail to account for the degree of bias.

</details>


### [96] [What Triggers my Model? Contrastive Explanations Inform Gender Choices by Translation Models](https://arxiv.org/abs/2512.08440)
*Janiça Hackenbuchner,Arda Tezcan,Joke Daems*

Main category: cs.CL

TL;DR: 本文旨在探索用于机器翻译的模型中性别偏见的根源，通过对比解释和计算注意力归因来分析源句子中哪些上下文（输入词）影响目标语言中的性别词形变化选择。


<details>
  <summary>Details</summary>
Motivation: 目前关于这方面的研究相对有限，特别是在解决这些问题时，简单地衡量偏见已不够。本文研究者旨在从根源探索这些模型中的性别偏见。

Method: 研究者使用了对比解释和计算注意力归因的方法，并重点研究了来源单词的不同层次对模型性别决定的影响程度。此外，还对显着词进行了语言学分析。

Result: 研究结果表明，模型的注意力归因与人类对性别的感知之间存在明显的重叠。显着的单词在语言层面也进行了分析。

Conclusion: 研究结果表明理解模型翻译决策中的性别至关重要，这应与其他人类决策进行比较，并利用这些信息来缓解性别偏见。

Abstract: Interpretability can be implemented as a means to understand decisions taken by (black box) models, such as machine translation (MT) or large language models (LLMs). Yet, research in this area has been limited in relation to a manifested problem in these models: gender bias. With this research, we aim to move away from simply measuring bias to exploring its origins. Working with gender-ambiguous natural source data, this study examines which context, in the form of input tokens in the source sentence, influences (or triggers) the translation model choice of a certain gender inflection in the target language. To analyse this, we use contrastive explanations and compute saliency attribution. We first address the challenge of a lacking scoring threshold and specifically examine different attribution levels of source words on the model gender decisions in the translation. We compare salient source words with human perceptions of gender and demonstrate a noticeable overlap between human perceptions and model attribution. Additionally, we provide a linguistic analysis of salient words. Our work showcases the relevance of understanding model translation decisions in terms of gender, how this compares to human decisions and that this information should be leveraged to mitigate gender bias.

</details>


### [97] [Soft Inductive Bias Approach via Explicit Reasoning Perspectives in Inappropriate Utterance Detection Using Large Language Models](https://arxiv.org/abs/2512.08480)
*Ju-Young Kim,Ji-Hong Park,Se-Yeon Lee,Sujin Park,Gun-Woo Kim*

Main category: cs.CL

TL;DR: 本研究提出了一种软诱导偏置方法，通过明确定义推理视角来指导推理过程，从而促进理性决策并防止推理过程中可能出现的错误。通过这种方法对Kanana-1.5模型进行微调，实验结果表明其在不适当言论检测中的准确率为87.0046%，相较于标准监督学习提升了约3.89%。


<details>
  <summary>Details</summary>
Motivation: 随着在线游戏中匿名环境下不当言论的频繁升级，以及由此引发的严重社会问题，迫切需要研究能够检测对话文本中不当言论的技术，以建立更加安全的交流环境。

Method: 该研究提出了一种软诱导偏置的方法，它明确定义了推理视角来引导推理过程，旨在促进理性决策并防止推理错误。作者通过定义和指导模型的推理视角，使其能够更精准和一致地进行判断，从而提高模型在检测不适当言论方面的性能。

Result: 最终，通过该方法对Kanana-1.5模型进行微调后，实验结果显示其准确率为87.0046%，比标准监督学习提高了约3.89%。

Conclusion: 研究发现，所提出的方法不仅超越了大语言模型的简单知识模仿，还能通过受限的推理视角实现更精确和一致的判断，适用于不适当言论检测。

Abstract: Recent incidents in certain online games and communities, where anonymity is guaranteed, show that unchecked inappropriate remarks frequently escalate into verbal abuse and even criminal behavior, raising significant social concerns. Consequently, there is a growing need for research on techniques that can detect inappropriate utterances within conversational texts to help build a safer communication environment. Although large-scale language models trained on Korean corpora and chain-of-thought reasoning have recently gained attention, research applying these approaches to inappropriate utterance detection remains limited. In this study, we propose a soft inductive bias approach that explicitly defines reasoning perspectives to guide the inference process, thereby promoting rational decision-making and preventing errors that may arise during reasoning. We fine-tune a Korean large language model using the proposed method and conduct both quantitative performance comparisons and qualitative evaluations across different training strategies. Experimental results show that the Kanana-1.5 model achieves an average accuracy of 87.0046, improving by approximately 3.89 percent over standard supervised learning. These findings indicate that the proposed method goes beyond simple knowledge imitation by large language models and enables more precise and consistent judgments through constrained reasoning perspectives, demonstrating its effectiveness for inappropriate utterance detection.

</details>


### [98] [Curriculum Guided Massive Multi Agent System Solving For Robust Long Horizon Tasks](https://arxiv.org/abs/2512.08545)
*Indrajit Kar,Kalathur Chenchu Kishore Kumar*

Main category: cs.CL

TL;DR: 本文提出了一种分层多agent架构，将推理分布在64*64网格的小型agents上，由选择性仲裁器支持。通过时空课程逐步扩展操作区域，确保agents从易到难学习。引入负对数似然度作为信心度量，优先处理准确且校准良好的区域。Thompson Sampling课程经理根据能力与NLL驱动的奖励信号动态选择训练区域。通过对空间接地的汉诺塔基准测试评估，展示了分布式agents合作后稳定性提升、oracle使用量减少以及更强的远期推理能力。


<details>
  <summary>Details</summary>
Motivation: 针对大型语言模型和多agent系统在长时任务推理和计算成本上的不足，本文提出了一种新的分层多agent架构，旨在提高解决复杂任务的效率与可靠性。

Method: 该方法采用了分布式多agent架构，其中64*64网格的轻量级agents由一个选择性仲裁器支持。通过时空课程逐渐扩展agent的区域，同时使用负对数似然度作为信心度量，并结合Thompson Sampling课程经理进行动态训练区域选择。

Result: 在空间接地的汉诺塔基准测试中，该方法展示了分布式-agent合作带来的稳定性提升、oracle用量减少及强化的远期推理能力。

Conclusion: 该研究提出的方法为解决复杂的任务提供了新的思路，通过改进agent间的协作机制，有效降低了长时任务推理的难度和计算开销。

Abstract: Large Language Models and multi-agent systems have shown promise in decomposing complex tasks, yet they struggle with long-horizon reasoning tasks and escalating computation cost. This work introduces a hierarchical multi-agent architecture that distributes reasoning across a 64*64 grid of lightweight agents, supported by a selective oracle. A spatial curriculum progressively expands the operational region of the grid, ensuring that agents master easier central tasks before tackling harder peripheral ones. To improve reliability, the system integrates Negative Log-Likelihood as a measure of confidence, allowing the curriculum to prioritize regions where agents are both accurate and well calibrated. A Thompson Sampling curriculum manager adaptively chooses training zones based on competence and NLL-driven reward signals. We evaluate the approach on a spatially grounded Tower of Hanoi benchmark, which mirrors the long-horizon structure of many robotic manipulation and planning tasks. Results demonstrate improved stability, reduced oracle usage, and stronger long-range reasoning from distributed agent cooperation.

</details>


### [99] [HealthcareNLP: where are we and what is next?](https://arxiv.org/abs/2512.08617)
*Lifeng Han,Paul Rayson,Suzan Verberne,Andrew Moore,Goran Nenadic*

Main category: cs.CL

TL;DR: 本文档旨在为自然语言处理（NLP）从业者、研究者、医疗研究者及NLP领域的学生提供关于医疗健康领域应用的入门级教程，涵盖数据伦理、NLP评估任务及患者层面上的应用，并包含实践环节。


<details>
  <summary>Details</summary>
Motivation: 现有文献忽视了关键任务和技术，如合成数据生成、解释型临床NLP、涉及到的信息检索增强生成方法及神经符号整合方法，故本教程旨在填补这些空白，为未来研究提供指导。

Method: 本教程分为三个层次：数据/资源层、NLP-Eval层及患者层。具体涵盖数据标注指南、伦理审批、治理、合成数据、NLP评估任务如NER、RE、情感分析及链接/编码，以及患者公共参与和健康素养等主题。

Result: 将提供实际操作环节，帮助学员了解医疗健康领域的NLP应用。为NLP在医疗领域的应用提供一个全面的入门了解。

Conclusion: 此教程将全面覆盖医疗健康领域NLP应用的核心方面，并提供实际操作机会，是适合初学者及非专业人士的进阶性培训课程。

Abstract: This proposed tutorial focuses on Healthcare Domain Applications of NLP, what we have achieved around HealthcareNLP, and the challenges that lie ahead for the future. Existing reviews in this domain either overlook some important tasks, such as synthetic data generation for addressing privacy concerns, or explainable clinical NLP for improved integration and implementation, or fail to mention important methodologies, including retrieval augmented generation and the neural symbolic integration of LLMs and KGs. In light of this, the goal of this tutorial is to provide an introductory overview of the most important sub-areas of a patient- and resource-oriented HealthcareNLP, with three layers of hierarchy: data/resource layer: annotation guidelines, ethical approvals, governance, synthetic data; NLP-Eval layer: NLP tasks such as NER, RE, sentiment analysis, and linking/coding with categorised methods, leading to explainable HealthAI; patients layer: Patient Public Involvement and Engagement (PPIE), health literacy, translation, simplification, and summarisation (also NLP tasks), and shared decision-making support. A hands-on session will be included in the tutorial for the audience to use HealthcareNLP applications. The target audience includes NLP practitioners in the healthcare application domain, NLP researchers who are interested in domain applications, healthcare researchers, and students from NLP fields. The type of tutorial is "Introductory to CL/NLP topics (HealthcareNLP)" and the audience does not need prior knowledge to attend this. Tutorial materials: https://github.com/4dpicture/HealthNLP

</details>


### [100] [QSTN: A Modular Framework for Robust Questionnaire Inference with Large Language Models](https://arxiv.org/abs/2512.08646)
*Maximilian Kreutner,Jens Rupprecht,Georg Ahnert,Ahmed Salem,Markus Strohmaier*

Main category: cs.CL

TL;DR: QSTN 是一个开源框架，可以系统地从问卷风格的提示生成响应，支持使用大规模语言模型（LLMs）进行虚拟调查和注释任务。通过广泛的评估，发现问卷结构和响应生成方法对生成的调查响应与人类答案的对齐有显著影响，且无需大量计算成本。QSTN 还提供了无需编程知识即可设置强大实验的无代码用户界面。


<details>
  <summary>Details</summary>
Motivation: 论文旨在提供一个工具来评估问卷呈现、提示变化和响应生成方法，支持LLMs在虚拟调查和注释任务中的应用。

Method: QSTN框架通过系统的提示生成方法实现对问卷的数据支持，利用大规模语言模型进行生成。

Result: 通过广泛的实验评估显示，问卷结构和响应生成方法对生成的调查响应与人类答案的对齐有显著影响。此外，无需大量计算资源即可获得生成结果。提供了一个无需编程知识即可操作的无代码用户界面。

Conclusion: QSTN为基于LLMs的研究提供了支持，提高实验的可重复性和可靠性。

Abstract: We introduce QSTN, an open-source Python framework for systematically generating responses from questionnaire-style prompts to support in-silico surveys and annotation tasks with large language models (LLMs). QSTN enables robust evaluation of questionnaire presentation, prompt perturbations, and response generation methods. Our extensive evaluation ($>40 $ million survey responses) shows that question structure and response generation methods have a significant impact on the alignment of generated survey responses with human answers, and can be obtained for a fraction of the compute cost. In addition, we offer a no-code user interface that allows researchers to set up robust experiments with LLMs without coding knowledge. We hope that QSTN will support the reproducibility and reliability of LLM-based research in the future.

</details>


### [101] [Automatic Essay Scoring and Feedback Generation in Basque Language Learning](https://arxiv.org/abs/2512.08713)
*Ekhi Azurmendi,Xabier Arregi,Oier Lopez de Lacalle*

Main category: cs.CL

TL;DR: 该论文介绍了一个新的公开可用数据集，用于巴斯克语的自动文章评分和反馈生成，该数据集包含3200篇论文，每篇都由专家评分并提供详细的反馈和错误示例。使用开源模型RoBERTa-EusCrawl和Latxa进行微调，结果显示，微调后的Latxa模型在评分一致性及反馈质量上显著超越了最先进的商业系统。


<details>
  <summary>Details</summary>
Motivation: 开发用户可访问的数据集和提高自动化评估系统的性能，以便于低资源语言的学习和教学。

Method: 通过将商业和开源语言模型（RoBERTa-EusCrawl, Latxa 8B/70B）进行微调，结合自动一致度评估和专家验证来生成反馈，以提高自动评分系统的准确性和反馈质量。

Result: 证明了微调后的模型在评分一致性及反馈质量上显著超越了最先进的商业系统，并提出了一种新的评估反馈生成方法。

Conclusion: 微调后的模型能够生成目标导向且教育价值丰富的反馈，并识别出比专有模型更多的错误类型，这为低资源语言如巴斯克语的自然语言处理研究提供了基础。

Abstract: This paper introduces the first publicly available dataset for Automatic Essay Scoring (AES) and feedback generation in Basque, targeting the CEFR C1 proficiency level. The dataset comprises 3,200 essays from HABE, each annotated by expert evaluators with criterion specific scores covering correctness, richness, coherence, cohesion, and task alignment enriched with detailed feedback and error examples. We fine-tune open-source models, including RoBERTa-EusCrawl and Latxa 8B/70B, for both scoring and explanation generation. Our experiments show that encoder models remain highly reliable for AES, while supervised fine-tuning (SFT) of Latxa significantly enhances performance, surpassing state-of-the-art (SoTA) closed-source systems such as GPT-5 and Claude Sonnet 4.5 in scoring consistency and feedback quality. We also propose a novel evaluation methodology for assessing feedback generation, combining automatic consistency metrics with expert-based validation of extracted learner errors. Results demonstrate that the fine-tuned Latxa model produces criterion-aligned, pedagogically meaningful feedback and identifies a wider range of error types than proprietary models. This resource and benchmark establish a foundation for transparent, reproducible, and educationally grounded NLP research in low-resource languages such as Basque.

</details>


### [102] [Fluent Alignment with Disfluent Judges: Post-training for Lower-resource Languages](https://arxiv.org/abs/2512.08777)
*David Samuel,Lilja Øvrelid,Erik Velldal,Andrey Kutuzov*

Main category: cs.CL

TL;DR: 本文提出了一种用于低资源语言的后训练方法，该方法通过使用偏好优化技术，即使使用错误奖励模型也能保持语言模型的流畅性。该研究对比了上策训练法与监督微调和多语言微调两种常见方法，并以挪威语作为案例研究，结果显示上策训练方法在不依赖于任何难以获取的数据的情况下表现出色。


<details>
  <summary>Details</summary>
Motivation: 低资源语言在市场上很少受到关注，这主要是因为缺乏高质量的训练数据和能够生成流畅合成数据的语言模型。此外，尽管偏好优化是一个非常有吸引力的研究领域，但对于低资源语言的关注较少，该研究旨在弥合这一差距。

Method: 本文提出了一种基于上策训练的后训练方法，该方法使用偏好优化来生成流畅的语言模型。该方法与监督微调和多语言微调进行了比较，并在挪威语上进行了实验。

Result: 实验结果显示，上策训练方法即使在使用错误奖励模型的情况下也能生成流畅的语言模型，并且在不依赖于任何难以获取的数据的情况下，其性能优于其他方法。

Conclusion: 该研究证明了上策训练方法的有效性，该方法可以在没有目标语言指令调优数据的情况下生成流畅的语言模型。这种方法为低资源语言的自然语言处理任务提供了一种新的可能性。

Abstract: We propose a post-training method for lower-resource languages that preserves fluency of language models even when aligned by disfluent reward models. Preference-optimization is now a well-researched topic, but previous work has mostly addressed models for English and Chinese. Lower-resource languages lack both datasets written by native speakers and language models capable of generating fluent synthetic data. Thus, in this work, we focus on developing a fluent preference-aligned language model without any instruction-tuning data in the target language. Our approach uses an on-policy training method, which we compare with two common approaches: supervised finetuning on machine-translated data and multilingual finetuning. We conduct a case study on Norwegian Bokmål and evaluate fluency through native-speaker assessments. The results show that the on-policy aspect is crucial and outperforms the alternatives without relying on any hard-to-obtain data.

</details>


### [103] [A Systematic Evaluation of Preference Aggregation in Federated RLHF for Pluralistic Alignment of LLMs](https://arxiv.org/abs/2512.08786)
*Mahmoud Srewa,Tianyu Zhao,Salma Elmalaki*

Main category: cs.CL

TL;DR: 该论文提出了一种全面的评估框架，用于在联邦学习环境中评估不同类型的人类偏好聚合策略，特别是在大规模语言模型与多样化的人类偏好对齐方面的公平性与质量之间的权衡。该工作展示了如何动态调整偏好权重，以实现更好的公平性，同时保持与标准方法相近的对齐性能。


<details>
  <summary>Details</summary>
Motivation: 论文的动机在于解决联邦学习环境中大规模语言模型对齐多样化人类偏好时面临的挑战，弥补了传统方法在代表性上的不足。

Method: 论文提出了一个综合评估体系，包含标准奖励聚合技术（最小、最大和平均）的评估，以及一种基于群体历史对齐性能的自适应调整偏好权重的新方案。

Result: 在问答任务上，基于PPO的RLHF管道实验表明，自适应方案在保证公平性的前提下，能够实现更优的对齐效果。

Conclusion: 这项工作提供了一种评估LLM行为多样群体的方法，并提出了一种实用工具来开发真正的多元且公平对齐的语言模型。

Abstract: This paper addresses the challenge of aligning large language models (LLMs) with diverse human preferences within federated learning (FL) environments, where standard methods often fail to adequately represent diverse viewpoints. We introduce a comprehensive evaluation framework that systematically assesses the trade-off between alignment quality and fairness when using different aggregation strategies for human preferences. In our federated setting, each group locally evaluates rollouts and produces reward signals, and the server aggregates these group-level rewards without accessing any raw data. Specifically, we evaluate standard reward aggregation techniques (min, max, and average) and introduce a novel adaptive scheme that dynamically adjusts preference weights based on a group's historical alignment performance. Our experiments on question-answering (Q/A) tasks using a PPO-based RLHF pipeline demonstrate that our adaptive approach consistently achieves superior fairness while maintaining competitive alignment scores. This work offers a robust methodology for evaluating LLM behavior across diverse populations and provides a practical solution for developing truly pluralistic and fairly aligned models.

</details>


### [104] [Do Depth-Grown Models Overcome the Curse of Depth? An In-Depth Analysis](https://arxiv.org/abs/2512.08819)
*Ferdinand Kapl,Emmanouil Angelis,Tobias Höppe,Kaitlin Maile,Johannes von Oswald,Nino Scherrer,Stefan Bauer*

Main category: cs.CL

TL;DR: 本文提出了一种渐进增长Transformer模型深度的方法，该方法不仅能降低训练成本，还能提高推理性能，同时废弃了一种名为‘深度诅咒’的现象。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏对逐步增加Transformer深度后性能提升的机理理解。本文旨在通过揭示非增长的预层规范化Transformer在前半部分的层相较于后半部分的层对最终输出贡献较少的现象（即‘深度诅咒’），来加深对这一问题的理解。

Method: 本文利用深度分析，结合逐步中间堆叠技术，展示了如何更有效地利用模型深度，改变残差流结构，并促进可互换计算块的形成。此外，还提出了一种轻量级的MIDAS改进方法，旨在进一步提高下游推理基准。

Result: 结果显示，通过逐步中间堆叠增长模型深度，能够形成独特的计算电路，克服了标准未增长模型中的深度利用限制。

Conclusion: 本文强调，逐步增长模型深度对于提高推理性能和改善模型利用率的重要性，揭示了如何通过渐进式深度增长来优化Transformer模型的表现。

Abstract: Gradually growing the depth of Transformers during training can not only reduce training cost but also lead to improved reasoning performance, as shown by MIDAS (Saunshi et al., 2024). Thus far, however, a mechanistic understanding of these gains has been missing. In this work, we establish a connection to recent work showing that layers in the second half of non-grown, pre-layernorm Transformers contribute much less to the final output distribution than those in the first half - also known as the Curse of Depth (Sun et al., 2025, Csordás et al., 2025). Using depth-wise analyses, we demonstrate that growth via gradual middle stacking yields more effective utilization of model depth, alters the residual stream structure, and facilitates the formation of permutable computational blocks. In addition, we propose a lightweight modification of MIDAS that yields further improvements in downstream reasoning benchmarks. Overall, this work highlights how the gradual growth of model depth can lead to the formation of distinct computational circuits and overcome the limited depth utilization seen in standard non-grown models.

</details>


### [105] [Toward Faithful Retrieval-Augmented Generation with Sparse Autoencoders](https://arxiv.org/abs/2512.08892)
*Guangzhi Xiong,Zhenghao He,Bohan Liu,Sanchit Sinha,Aidong Zhang*

Main category: cs.CL

TL;DR: RAGLens 使用稀疏自编码器（SAEs）准确地识别 RAG 生成过程中的错误信息，并提供可解释的决策理由，从而有效缓解 RAG 的不忠实问题。


<details>
  <summary>Details</summary>
Motivation: 为了应对 RAG 生成过程中存在的信仰不一致问题，传统方法要么需要大量的标注数据进行检测器训练，要么需要查询外部 LLM 判官进行推理，这两种方法都存在较大问题。因此，本文试图通过运用机制解释的最新进展，使用稀疏自编码器（SAEs）来分离内部激活，从而准确识别 RAG 的幻觉特征，提出 RAGLens 作为轻量级的幻觉检测器。

Method: 本文采用基于信息的特征选择和加性特征建模的系统化流程，并利用稀疏自编码器（SAEs）分离内部分布特征，以识别 RAG 环境中的幻觉信号，以驱动 RAGLens 的设计。

Result: RAGLens 相比于现有方法，在幻觉检测方面表现更优，并提供了可解释的决策理由，表明其在判断 RAG 输出的真实性上有一定的可靠性和解释力。

Conclusion: RAGLens 能够有效检测和解释 RAG 的幻觉现象，提供了一种轻量级且具有解释性的解决方案，对未来研究和实际应用均有重要意义。

Abstract: Retrieval-Augmented Generation (RAG) improves the factuality of large language models (LLMs) by grounding outputs in retrieved evidence, but faithfulness failures, where generations contradict or extend beyond the provided sources, remain a critical challenge. Existing hallucination detection methods for RAG often rely either on large-scale detector training, which requires substantial annotated data, or on querying external LLM judges, which leads to high inference costs. Although some approaches attempt to leverage internal representations of LLMs for hallucination detection, their accuracy remains limited. Motivated by recent advances in mechanistic interpretability, we employ sparse autoencoders (SAEs) to disentangle internal activations, successfully identifying features that are specifically triggered during RAG hallucinations. Building on a systematic pipeline of information-based feature selection and additive feature modeling, we introduce RAGLens, a lightweight hallucination detector that accurately flags unfaithful RAG outputs using LLM internal representations. RAGLens not only achieves superior detection performance compared to existing methods, but also provides interpretable rationales for its decisions, enabling effective post-hoc mitigation of unfaithful RAG. Finally, we justify our design choices and reveal new insights into the distribution of hallucination-related signals within LLMs. The code is available at https://github.com/Teddy-XiongGZ/RAGLens.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [106] [Impact of Data-Oriented and Object-Oriented Design on Performance and Cache Utilization with Artificial Intelligence Algorithms in Multi-Threaded CPUs](https://arxiv.org/abs/2512.07841)
*Gabriel M. Arantes,Richard F. Pinto,Bruno L. Dalmazo,Eduardo N. Borges,Giancarlo Lucca,Viviane L. D. de Mattos,Fabian C. Cardoso,Rafael A. Berri*

Main category: cs.AI

TL;DR: 本文对比了面向数据设计（DOD）和面向对象设计（OOD）在A*搜索算法中的表现，特别是在多线程环境下的缓存利用率和效率。


<details>
  <summary>Details</summary>
Motivation: 鉴于多核CPU与主存性能差距的扩大，研究旨在通过分析和对比两种不同的设计方法，探索提高硬件效率的有效途径。

Method: 开发了四种不同版本的A*搜索算法，包括单线程OOD、单线程DOD、多线程OOD和多线程DOD，通过执行时间、内存使用量和CPU缓存缺失等指标进行评估。

Result: 多线程DOD版本在执行时间和缓存缺失方面表现更好，具有显著的性能提升，而OOD在某些情况下显示出内存使用量的优势。此外，研究发现细粒度任务中线程管理的开销使得单线程版本总体上优于多线程版本。

Conclusion: 研究结论表明，DOD在关键性能指标上的持续优势表明其在复杂大型AI和并行计算任务中具有更好的架构优越性，是一种更有效的方法，用以最大化硬件效率。

Abstract: The growing performance gap between multi-core CPUs and main memory necessitates hardware-aware software design paradigms. This study provides a comprehensive performance analysis of Data Oriented Design (DOD) versus the traditional Object-Oriented Design (OOD), focusing on cache utilization and efficiency in multi-threaded environments. We developed and compared four distinct versions of the A* search algorithm: single-threaded OOD (ST-OOD), single-threaded DOD (ST-DOD), multi-threaded OOD (MT-OOD), and multi-threaded DOD (MT-DOD). The evaluation was based on metrics including execution time, memory usage, and CPU cache misses. In multi-threaded tests, the DOD implementation demonstrated considerable performance gains, with faster execution times and a lower number of raw system calls and cache misses. While OOD occasionally showed marginal advantages in memory usage or percentage-based cache miss rates, DOD's efficiency in data-intensive operations was more evident. Furthermore, our findings reveal that for a fine-grained task like the A* algorithm, the overhead associated with thread management led to single-threaded versions significantly outperforming their multi-threaded counterparts in both paradigms. We conclude that even when performance differences appear subtle in simple algorithms, the consistent advantages of DOD in critical metrics highlight its foundational architectural superiority, suggesting it is a more effective approach for maximizing hardware efficiency in complex, large-scale AI and parallel computing tasks.

</details>


### [107] [SkipKV: Selective Skipping of KV Generation and Storage for Efficient Inference with Large Reasoning Models](https://arxiv.org/abs/2512.07993)
*Jiayi Tian,Seyedarmin Azizi,Yequan Zhao,Erfan Baghaei Potraghloo,Sean McPherson,Sharath Nittur Sridhar,Zhengyang Wang,Zheng Zhang,Massoud Pedram,Souvik Kundu*

Main category: cs.AI

TL;DR: SkipKV是一种无需训练的KV压缩方法，它通过句子级别的序列删除来实现选择性的KV删除和生成，从而提高大型推理模型在多批推理中的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的KV缓存淘汰方法在处理长推理链时无法维持准确性和效率，导致内存和吞吐量瓶颈，SkipKV旨在解决这一问题。

Method: SkipKV方法通过引入句子评分指标识别并移除高度相似的句子，保持语义连贯性。同时，它通过动态调整引导向量更新推理阶段的隐藏激活状态，抑制冗余生成。

Result: 在多个推理基准上的测评表明，与替代方案相比，SkipKV在保持相似压缩预算的同时，能够提升高达26.7%的准确率。此外，与最先进的技术相比，SkipKV可将生成长度减少1.6倍，并提高1.7倍的吞吐量。

Conclusion: SkipKV为处理长推理链中的KV缓存问题提供了一个有效的解决方案，既保持了模型的准确性和生成效率，又减少了冗余和内存消耗。

Abstract: Large reasoning models (LRMs) often cost significant key-value (KV) cache overhead, due to their linear growth with the verbose chain-of-thought (CoT) reasoning process. This costs both memory and throughput bottleneck limiting their efficient deployment. Towards reducing KV cache size during inference, we first investigate the effectiveness of existing KV cache eviction methods for CoT reasoning. Interestingly, we find that due to unstable token-wise scoring and the reduced effective KV budget caused by padding tokens, state-of-the-art (SoTA) eviction methods fail to maintain accuracy in the multi-batch setting. Additionally, these methods often generate longer sequences than the original model, as semantic-unaware token-wise eviction leads to repeated revalidation during reasoning. To address these issues, we present \textbf{SkipKV}, a \textbf{\textit{training-free}} KV compression method for selective \textit{eviction} and \textit{generation} operating at a coarse-grained sentence-level sequence removal for efficient CoT reasoning. In specific, it introduces a \textit{sentence-scoring metric} to identify and remove highly similar sentences while maintaining semantic coherence. To suppress redundant generation, SkipKV dynamically adjusts a steering vector to update the hidden activation states during inference enforcing the LRM to generate concise response. Extensive evaluations on multiple reasoning benchmarks demonstrate the effectiveness of SkipKV in maintaining up to $\mathbf{26.7}\%$ improved accuracy compared to the alternatives, at a similar compression budget. Additionally, compared to SoTA, SkipKV yields up to $\mathbf{1.6}\times$ fewer generation length while improving throughput up to $\mathbf{1.7}\times$.

</details>


### [108] [Toward an AI Reasoning-Enabled System for Patient-Clinical Trial Matching](https://arxiv.org/abs/2512.08026)
*Caroline N. Leach,Mitchell A. Klusty,Samuel E. Armstrong,Justine C. Pickarski,Kristen L. Hankins,Emily B. Collier,Maya Shah,Aaron D. Mullen,V. K. Cody Bumgardner*

Main category: cs.AI

TL;DR: 该系统通过集成异构电子健康记录数据、支持专家审查及保持严格的安全标准，利用开放源代码的推理驱动大型语言模型实现人工智能辅助的患者-试验匹配，不仅进行二元分类，还生成包含可解释推理链的概率评估，代表了患者资格状态的动态表现，识别潜在匹配，并为患者提供未来变合格的行动建议。


<details>
  <summary>Details</summary>
Motivation: 目前临床试验患者的筛查流程耗时且资源密集，该研究提出了一种安全可扩展的AI辅助患者-试验匹配概念验证系统，旨在减轻研究协调员的负担，更智能地扩大考虑每位患者的试验范围，并确保所有AI生成结果的全面审计。

Method: 该系统基于开放源代码的推理驱动大型语言模型，跨异构电子健康记录数据进行集成，并生成包含可解释推理链的结构化资格评估，支持专家审查，同时将患者的资格状态视为动态状态，提供未来可能变合格的建议。

Result: 该系统能够识别当下的匹配并为患者提供将来变合格的行动建议，智能扩展考虑的试验范围，减少研究协调员的工作负担，并确保全面的AI输出审计。

Conclusion: 通过上述方法，该研究提供了一种有效的解决方案，旨在提高临床试验患者筛选的效率和准确性，同时维持严格的数据安全标准。

Abstract: Screening patients for clinical trial eligibility remains a manual, time-consuming, and resource-intensive process. We present a secure, scalable proof-of-concept system for Artificial Intelligence (AI)-augmented patient-trial matching that addresses key implementation challenges: integrating heterogeneous electronic health record (EHR) data, facilitating expert review, and maintaining rigorous security standards. Leveraging open-source, reasoning-enabled large language models (LLMs), the system moves beyond binary classification to generate structured eligibility assessments with interpretable reasoning chains that support human-in-the-loop review. This decision support tool represents eligibility as a dynamic state rather than a fixed determination, identifying matches when available and offering actionable recommendations that could render a patient eligible in the future. The system aims to reduce coordinator burden, intelligently broaden the set of trials considered for each patient and guarantee comprehensive auditability of all AI-generated outputs.

</details>


### [109] [Large Language Models for Education and Research: An Empirical and User Survey-based Analysis](https://arxiv.org/abs/2512.08057)
*Md Mostafizer Rahman,Ariful Islam Shiplu,Md Faizul Ibne Amin,Yutaka Watanobe,Lu Peng*

Main category: cs.AI

TL;DR: 本研究针对ChatGPT和DeepSeek两大预训练大型语言模型，在教育和科研领域的文本生成、编程及专业知识解决方面进行了全面评估，定量和定性结果显示了各自的优势与局限。


<details>
  <summary>Details</summary>
Motivation: 研究希望通过全面评估预训练大型语言模型在教育和科研中的应用，为教育技术的发展提供数据支持，同时为用户选择合适的语言模型提供参考。

Method: 通过背景技术分析、实验测试和实地用户调研三种方法对ChatGPT和DeepSeek进行综合评估。

Result: 实验结果显示，ChatGPT在通用语言理解和文本生成方面表现优异，而DeepSeek在编程任务方面因其效率性设计表现出色。两者都能提供医学准确的诊断输出和有效解决复杂数学问题。

Conclusion: 综合评估结果和实地用户调研表明，ChatGPT适用于一般语言理解和文本生成场景，DeepSeek适用于高效编程场景，两者在教育和科研领域都具有显著的应用价值和局限性。

Abstract: Pretrained Large Language Models (LLMs) have achieved remarkable success across diverse domains, with education and research emerging as particularly impactful areas. Among current state-of-the-art LLMs, ChatGPT and DeepSeek exhibit strong capabilities in mathematics, science, medicine, literature, and programming. In this study, we present a comprehensive evaluation of these two LLMs through background technology analysis, empirical experiments, and a real-world user survey. The evaluation explores trade-offs among model accuracy, computational efficiency, and user experience in educational and research affairs. We benchmarked these LLMs performance in text generation, programming, and specialized problem-solving. Experimental results show that ChatGPT excels in general language understanding and text generation, while DeepSeek demonstrates superior performance in programming tasks due to its efficiency- focused design. Moreover, both models deliver medically accurate diagnostic outputs and effectively solve complex mathematical problems. Complementing these quantitative findings, a survey of students, educators, and researchers highlights the practical benefits and limitations of these models, offering deeper insights into their role in advancing education and research.

</details>


### [110] [Scalable Back-End for an AI-Based Diabetes Prediction Application](https://arxiv.org/abs/2512.08147)
*Henry Anand Septian Radityo,Bernardus Willson,Reynard Tanadi,Latifa Dwiyanti,Saiful Akbar*

Main category: cs.AI

TL;DR: 该研究开发并评估了一种面向移动糖尿病预测应用的可扩展后端系统，主要目标是保持低于5%的故障率和平均延迟低于1000毫秒。该架构采用水平扩展、数据库分片和使用RabbitMQ的消息队列异步通信实现可扩展性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 随着全球糖尿病发病率的上升，早期检测变得尤为迫切。AI预测应用虽潜力巨大，但仍需具备响应快速和可扩展的后台架构以满足大量用户需求。该研究旨在满足高效能要求，设计并评估能满足这些性能标准的可扩展后端系统。

Method: 研究采用了水平扩展、数据库分片等技术，结合异步通信机制以提高系统性能和可靠性。特别通过RabbitMQ来管理复杂计算需求，确保在高负载情况下不会丢失数据并对请求进行排队。

Result: 系统中有83%的功能（20项中的24项）达到了预期性能要求，关键功能如用户资料管理、活动跟踪以及读操作的预测任务均满足了预期目标。同时，系统经受住了高达1万个并发用户的考验，显示了其优秀的可扩展性。

Conclusion: 研究成功的开发并验证了一个面向移动糖尿病预测应用的具有高可靠性的可扩展后端系统，该系统能够满足高性能需求，验证了异步通信机制对于系统稳定性的重要性。

Abstract: The rising global prevalence of diabetes necessitates early detection to prevent severe complications. While AI-powered prediction applications offer a promising solution, they require a responsive and scalable back-end architecture to serve a large user base effectively. This paper details the development and evaluation of a scalable back-end system designed for a mobile diabetes prediction application. The primary objective was to maintain a failure rate below 5% and an average latency of under 1000 ms. The architecture leverages horizontal scaling, database sharding, and asynchronous communication via a message queue. Performance evaluation showed that 83% of the system's features (20 out of 24) met the specified performance targets. Key functionalities such as user profile management, activity tracking, and read-intensive prediction operations successfully achieved the desired performance. The system demonstrated the ability to handle up to 10,000 concurrent users without issues, validating its scalability. The implementation of asynchronous communication using RabbitMQ proved crucial in minimizing the error rate for computationally intensive prediction requests, ensuring system reliability by queuing requests and preventing data loss under heavy load.

</details>


### [111] [Empowerment Gain and Causal Model Construction: Children and adults are sensitive to controllability and variability in their causal interventions](https://arxiv.org/abs/2512.08230)
*Eunice Yiu,Kelsey Allen,Shiry Ginosar,Alison Gopnik*

Main category: cs.AI

TL;DR: Summary generation failed


<details>
  <summary>Details</summary>
Motivation: Motivation analysis unavailable

Method: Method extraction failed

Result: Result analysis unavailable

Conclusion: Conclusion extraction failed

Abstract: Learning about the causal structure of the world is a fundamental problem for human cognition. Causal models and especially causal learning have proved to be difficult for large pretrained models using standard techniques of deep learning. In contrast, cognitive scientists have applied advances in our formal understanding of causation in computer science, particularly within the Causal Bayes Net formalism, to understand human causal learning. In the very different tradition of reinforcement learning, researchers have described an intrinsic reward signal called "empowerment" which maximizes mutual information between actions and their outcomes. "Empowerment" may be an important bridge between classical Bayesian causal learning and reinforcement learning and may help to characterize causal learning in humans and enable it in machines. If an agent learns an accurate causal world model, they will necessarily increase their empowerment, and increasing empowerment will lead to a more accurate causal world model. Empowerment may also explain distinctive features of childrens causal learning, as well as providing a more tractable computational account of how that learning is possible. In an empirical study, we systematically test how children and adults use cues to empowerment to infer causal relations, and design effective causal interventions.

</details>


### [112] [Beyond Traditional Diagnostics: Transforming Patient-Side Information into Predictive Insights with Knowledge Graphs and Prototypes](https://arxiv.org/abs/2512.08261)
*Yibowen Zhao,Yinan Zhang,Zhixiang Su,Lizhen Cui,Chunyan Miao*

Main category: cs.AI

TL;DR: 提出了一个名为KPI的框架，通过集成医学知识、构建疾病原型以及使用对比学习提高预测准确性，同时利用大型语言模型生成解释，以提高解释性。


<details>
  <summary>Details</summary>
Motivation: 由于从患者侧信息预测疾病可以增强患者意识、促进早期健康护理参与并提高健康系统效率，因此备受研究关注。然而，现有的方法面临着不平衡的疾病分布和缺乏可解释性的问题，导致了有偏或不可靠的预测。为了克服这些挑战，提出了KPI框架。

Method: KPI框架系统地将结构化的医学知识整合到统一的疾病知识图中，构建具有临床意义的疾病原型，并使用对比学习来提高预测准确性，特别是在长尾疾病方面。KPI还利用大型语言模型生成患者特定且相关医学解释，以提高可解释性和可靠性。

Result: 在现实世界数据集上的广泛实验表明，KPI在预测准确性上优于最先进的方法，并提供了与患者叙述紧密一致的临床有效解释，突显了其在以患者为中心的健康护理交付中的实用价值。

Conclusion: KPI框架提供了一种新颖的方法来克服疾病预测中的挑战，特别适用于长尾疾病，并具有较高的预测准确性和临床相关性。

Abstract: Predicting diseases solely from patient-side information, such as demographics and self-reported symptoms, has attracted significant research attention due to its potential to enhance patient awareness, facilitate early healthcare engagement, and improve healthcare system efficiency. However, existing approaches encounter critical challenges, including imbalanced disease distributions and a lack of interpretability, resulting in biased or unreliable predictions. To address these issues, we propose the Knowledge graph-enhanced, Prototype-aware, and Interpretable (KPI) framework. KPI systematically integrates structured and trusted medical knowledge into a unified disease knowledge graph, constructs clinically meaningful disease prototypes, and employs contrastive learning to enhance predictive accuracy, which is particularly important for long-tailed diseases. Additionally, KPI utilizes large language models (LLMs) to generate patient-specific, medically relevant explanations, thereby improving interpretability and reliability. Extensive experiments on real-world datasets demonstrate that KPI outperforms state-of-the-art methods in predictive accuracy and provides clinically valid explanations that closely align with patient narratives, highlighting its practical value for patient-centered healthcare delivery.

</details>


### [113] [Reasoning Models Ace the CFA Exams](https://arxiv.org/abs/2512.08270)
*Jaisal Patel,Yunzhe Chen,Kaiwen He,Keyi Wang,David Li,Kairong Xiao,Xiao-Yang Liu*

Main category: cs.AI

TL;DR: 研究发现最先进的推理模型在模拟CFA考试中表现出色，所有通过的模型在三级考试中均达到良好的成绩。


<details>
  <summary>Details</summary>
Motivation: 先前的研究表明，大规模语言模型在CFA考试中的表现不佳，但近期的研究表明，推理模型在研究生水平的学术和专业考试中表现出色。本文旨在评估这些模型在CFA考试中的表现。

Method: 研究人员使用与先前研究相同的通过标准，评估了最先进的推理模型在980道模拟CFA考试中的表现。

Result: 大多数模型通过了所有三级考试。表现最好的模型依次为Gemini 3.0 Pro、Gemini 2.5 Pro、GPT-5、Grok 4、Claude Opus 4.1和DeepSeek-V3.1。具体而言，Gemini 3.0 Pro在一级考试中取得了97.6%的成绩，GPT-5在二级考试中的成绩为94.3%，而Gemini 2.5 Pro在三级考试中获得了最高的成绩，分别为86.4%的选择题和92.0%的构建响应题。

Conclusion: 研究结论是，最先进的推理模型在模拟CFA考试中表现优异，特别是在三级考试中的所有模型均表现出良好的成绩。

Abstract: Previous research has reported that large language models (LLMs) demonstrate poor performance on the Chartered Financial Analyst (CFA) exams. However, recent reasoning models have achieved strong results on graduate-level academic and professional examinations across various disciplines. In this paper, we evaluate state-of-the-art reasoning models on a set of mock CFA exams consisting of 980 questions across three Level I exams, two Level II exams, and three Level III exams. Using the same pass/fail criteria from prior studies, we find that most models clear all three levels. The models that pass, ordered by overall performance, are Gemini 3.0 Pro, Gemini 2.5 Pro, GPT-5, Grok 4, Claude Opus 4.1, and DeepSeek-V3.1. Specifically, Gemini 3.0 Pro achieves a record score of 97.6% on Level I. Performance is also strong on Level II, led by GPT-5 at 94.3%. On Level III, Gemini 2.5 Pro attains the highest score with 86.4% on multiple-choice questions while Gemini 3.0 Pro achieves 92.0% on constructed-response questions.

</details>


### [114] [Predicting California Bearing Ratio with Ensemble and Neural Network Models: A Case Study from Türkiye](https://arxiv.org/abs/2512.08340)
*Abdullah Hulusi Kökçam,Uğur Dağdeviren,Talas Fikret Kurnaz,Alparslan Serhat Demir,Caner Erden*

Main category: cs.AI

TL;DR: 本文介绍了一种基于随机森林回归器的人工智能框架，用于预测土壤承载力指标——加州承载比（CBR），并且通过多种机器学习算法的对比试验，最终确定了随机森林方法的优越性。


<details>
  <summary>Details</summary>
Motivation: 现有的CBR确定方法效率低下且成本高昂，特别是在大范围或多样土壤分布的情况下，因此本文旨在利用机器学习技术提高CBR预测的效率和准确性。

Method: 本文使用了来自土耳其不同地理气候区的382个土壤样本数据集，包含了与承载能力相关的物理和化学土壤特性。研究通过12种机器学习算法（决策树、随机森林、极端随机树、梯度提升、xgboost、K近邻、支持向量回归、多层感知机、AdaBoost、袋装、投票和堆叠回归器）进行模型训练、验证和评估。

Result: 随机森林回归器在模型训练、验证和测试阶段分别获得了0.95、0.76和0.83的R2得分，显示出强大的非线性映射能力。

Conclusion: 研究支持将智能数据驱动模型融入土木工程中的概念，提出这种方法可能成为传统方法的有效替代，并促进基础设施分析和设计中的数字化转型。

Abstract: The California Bearing Ratio (CBR) is a key geotechnical indicator used to assess the load-bearing capacity of subgrade soils, especially in transportation infrastructure and foundation design. Traditional CBR determination relies on laboratory penetration tests. Despite their accuracy, these tests are often time-consuming, costly, and can be impractical, particularly for large-scale or diverse soil profiles. Recent progress in artificial intelligence, especially machine learning (ML), has enabled data-driven approaches for modeling complex soil behavior with greater speed and precision. This study introduces a comprehensive ML framework for CBR prediction using a dataset of 382 soil samples collected from various geoclimatic regions in Türkiye. The dataset includes physicochemical soil properties relevant to bearing capacity, allowing multidimensional feature representation in a supervised learning context. Twelve ML algorithms were tested, including decision tree, random forest, extra trees, gradient boosting, xgboost, k-nearest neighbors, support vector regression, multi-layer perceptron, adaboost, bagging, voting, and stacking regressors. Each model was trained, validated, and evaluated to assess its generalization and robustness. Among them, the random forest regressor performed the best, achieving strong R2 scores of 0.95 (training), 0.76 (validation), and 0.83 (test). These outcomes highlight the model's powerful nonlinear mapping ability, making it a promising tool for predictive geotechnical tasks. The study supports the integration of intelligent, data-centric models in geotechnical engineering, offering an effective alternative to traditional methods and promoting digital transformation in infrastructure analysis and design.

</details>


### [115] [Soil Compaction Parameters Prediction Based on Automated Machine Learning Approach](https://arxiv.org/abs/2512.08343)
*Caner Erden,Alparslan Serhat Demir,Abdullah Hulusi Kokcam,Talas Fikret Kurnaz,Ugur Dagdeviren*

Main category: cs.AI

TL;DR: 该研究提出了一种自动化机器学习（AutoML）方法来预测土壤最佳含水量（OMC）和最大干密度（MDD），并通过实验表明XGBoost算法在预测这些参数方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统方法在确定土壤最佳含水量和最大干密度时存在劳动密集和准确性不足的问题。因此，研究旨在探索自动化机器学习在预测土壤压缩参数方面的潜力。

Method: 研究采用了自动化机器学习方法，自动选择算法并实现超参数优化。通过广泛的实验，研究比较了多种算法在预测MDD和OMC上的表现。

Result: 研究发现XGBoost算法在这两方面的表现最佳，MDD的决定性系数（R-squared）达到80.4%，OMC达到89.1%。使用异质数据集，研究强调了模型泛化能力和预测性能的重要性。

Conclusion: 研究为更高效和可靠的道路工程提供了贡献，通过改进土壤压缩参数的预测。

Abstract: Soil compaction is critical in construction engineering to ensure the stability of structures like road embankments and earth dams. Traditional methods for determining optimum moisture content (OMC) and maximum dry density (MDD) involve labor-intensive laboratory experiments, and empirical regression models have limited applicability and accuracy across diverse soil types. In recent years, artificial intelligence (AI) and machine learning (ML) techniques have emerged as alternatives for predicting these compaction parameters. However, ML models often struggle with prediction accuracy and generalizability, particularly with heterogeneous datasets representing various soil types. This study proposes an automated machine learning (AutoML) approach to predict OMC and MDD. AutoML automates algorithm selection and hyperparameter optimization, potentially improving accuracy and scalability. Through extensive experimentation, the study found that the Extreme Gradient Boosting (XGBoost) algorithm provided the best performance, achieving R-squared values of 80.4% for MDD and 89.1% for OMC on a separate dataset. These results demonstrate the effectiveness of AutoML in predicting compaction parameters across different soil types. The study also highlights the importance of heterogeneous datasets in improving the generalization and performance of ML models. Ultimately, this research contributes to more efficient and reliable construction practices by enhancing the prediction of soil compaction parameters.

</details>


### [116] [Reflecting with Two Voices: A Co-Adaptive Dual-Strategy Framework for LLM-Based Agent Decision Making](https://arxiv.org/abs/2512.08366)
*Wentao Zhang,Qunbo Wang,Tao Zhang,Junsheng Wu,Hongping Gan,Yang Liu,Ling Dai,Shizhuang Deng,Shuntong Sun*

Main category: cs.AI

TL;DR: DuSAR 是一种无需演示的框架，它利用两个互补策略进行多步推理：宏观全局计划和微观局部执行策略。通过轻量级的反思机制，该模型能够在遇到困难时调整规划，并在取得进步时细化规划。在 ALFWorld 和 Mind2Web 上，DuSAR 超过了以前的最佳成绩。


<details>
  <summary>Details</summary>
Motivation: 目前 LLM 收集外部示例或检索增强规划的依赖性导致了脆弱性、泛化能力差和高计算成本。因此，引入 DuSAR 的目的是在无需外部经典知识输入的情况下，让单个冻结的 LLM 能够进行协同适应推理。

Method: DuSAR 采用了一种双策略的方法：左右两种不同的策略相互协作。一种是宏观的、全面的整体规划，另一种是微观的、基于上下文的策略执行。通过一种轻量级的反思机制，在遇到瓶颈时调整整体规划，在取得进展时细化规划，从而模拟人类元认知行为。

Result: DuSAR 在 ALFWorld 和 Mind2Web 上取得的性能超越了之前的最佳结果。在 ALFWorld 上，使用开源 LLM (7B-70B)，DuSAR 达到了 37.1% 的成功率，超过了之前最好的成绩 13.0% 的两倍多。在 Mind2Web 上，DuSAR 达到了 4.02% 的成功率，也超过了最强基线结果的两倍多。此外，DuSAR 还通过减少每次步骤的令牌消耗，表现出对资源利用的优势。

Conclusion: DuSAR 作为无需外部演示的单个冻结 LLM 解决框架，通过引入双策略协同机制和轻量级反馈机制，成功实现了性能的突破，同时提高了效率。同时，通过与专家演示的可选融合，进一步证实了其在外部知识整合和应用方面的灵活性和兼容性。

Abstract: Large language model (LLM) agents often rely on external demonstrations or retrieval-augmented planning, leading to brittleness, poor generalization, and high computational overhead. Inspired by human problem-solving, we propose DuSAR (Dual-Strategy Agent with Reflecting) - a demonstration-free framework that enables a single frozen LLM to perform co-adaptive reasoning via two complementary strategies: a high-level holistic plan and a context-grounded local policy. These strategies interact through a lightweight reflection mechanism, where the agent continuously assesses progress via a Strategy Fitness Score and dynamically revises its global plan when stuck or refines it upon meaningful advancement, mimicking human metacognitive behavior. On ALFWorld and Mind2Web, DuSAR achieves state-of-the-art performance with open-source LLMs (7B-70B), reaching 37.1% success on ALFWorld (Llama3.1-70B) - more than doubling the best prior result (13.0%) - and 4.02% on Mind2Web, also more than doubling the strongest baseline. Remarkably, it reduces per-step token consumption by 3-9X while maintaining strong performance. Ablation studies confirm the necessity of dual-strategy coordination. Moreover, optional integration of expert demonstrations further boosts results, highlighting DuSAR's flexibility and compatibility with external knowledge.

</details>


### [117] [DeepFeature: Iterative Context-aware Feature Generation for Wearable Biosignals](https://arxiv.org/abs/2512.08379)
*Kaiwei Liu,Yuting He,Bufang Yang,Mu Yuan,Chun Man Victor Wong,Ho Pong Andrew Sze,Zhenyu Yan,Hongkai Chen*

Main category: cs.AI

TL;DR: 该研究提出了一种名为DeepFeature的框架，用于穿戴生物信号的特征生成，结合专家知识和任务设置，通过迭代特征优化和多层过滤验证，提高了多种生物信号处理任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有特征提取方法缺乏任务特异性上下文知识，在高维特征空间中难以确定最优设置，并且容易出现代码生成和自动化错误。

Method: DeepFeature框架采用多源特征生成机制，结合专家知识与任务设置，并通过特征评估反馈进行特征重选。框架还利用多层过滤和验证策略，确保特征到代码的有效翻译。

Result: 实验结果显示，DeepFeature在八个不同任务中平均提高了4.21-9.67%的AUROC性能，优于当前最先进的方法。

Conclusion: 该研究为穿戴生物信号处理提供了新的方法，通过综合专家知识和自动化调优，显著提升了性能。

Abstract: Biosignals collected from wearable devices are widely utilized in healthcare applications. Machine learning models used in these applications often rely on features extracted from biosignals due to their effectiveness, lower data dimensionality, and wide compatibility across various model architectures. However, existing feature extraction methods often lack task-specific contextual knowledge, struggle to identify optimal feature extraction settings in high-dimensional feature space, and are prone to code generation and automation errors. In this paper, we propose DeepFeature, the first LLM-empowered, context-aware feature generation framework for wearable biosignals. DeepFeature introduces a multi-source feature generation mechanism that integrates expert knowledge with task settings. It also employs an iterative feature refinement process that uses feature assessment-based feedback for feature re-selection. Additionally, DeepFeature utilizes a robust multi-layer filtering and verification approach for robust feature-to-code translation to ensure that the extraction functions run without crashing. Experimental evaluation results show that DeepFeature achieves an average AUROC improvement of 4.21-9.67% across eight diverse tasks compared to baseline methods. It outperforms state-of-the-art approaches on five tasks while maintaining comparable performance on the remaining tasks.

</details>


### [118] [Principles2Plan: LLM-Guided System for Operationalising Ethical Principles into Plans](https://arxiv.org/abs/2512.08536)
*Tammy Zhong,Yang Song,Maurice Pagnucco*

Main category: cs.AI

TL;DR: 该研究提出了一种名为Principles2Plan的原型系统，旨在通过人与大规模语言模型的合作，为经典规划场景生成基于原则的具体道德规则，从而指导自动规划并生成富有伦理意识的计划。


<details>
  <summary>Details</summary>
Motivation: 当前的自动化规划工具在人类环境中的机器人操作中缺乏伦理意识支持，且手动规定伦理规则效率低下且高度具体化。因此，需要一种解决方案来提高自动化规划的道德实践性。

Method: 研究人员开发了一种名为Principles2Plan的协作式原型系统，其中包括一个领域专家、人类和一个大规模语言模型。领域专家提供规划领域、问题细节和相关的高级原则，系统基于这些原则生成可操作的道德规则，由用户审核、优先排序，并传递给规划器以生成有道德依据的计划。

Result: Principles2Plan展示了人与大规模语言模型合作在生成适用于经典规划场景的基于原则的具体道德规则方面的潜力。

Conclusion: Principles2Plan证明了人类-大规模语言模型协作在使道德自动规划更加实用和可行方面的潜力。

Abstract: Ethical awareness is critical for robots operating in human environments, yet existing automated planning tools provide little support. Manually specifying ethical rules is labour-intensive and highly context-specific. We present Principles2Plan, an interactive research prototype demonstrating how a human and a Large Language Model (LLM) can collaborate to produce context-sensitive ethical rules and guide automated planning. A domain expert provides the planning domain, problem details, and relevant high-level principles such as beneficence and privacy. The system generates operationalisable ethical rules consistent with these principles, which the user can review, prioritise, and supply to a planner to produce ethically-informed plans. To our knowledge, no prior system supports users in generating principle-grounded rules for classical planning contexts. Principles2Plan showcases the potential of human-LLM collaboration for making ethical automated planning more practical and feasible.

</details>


### [119] [CogMCTS: A Novel Cognitive-Guided Monte Carlo Tree Search Framework for Iterative Heuristic Evolution with Large Language Models](https://arxiv.org/abs/2512.08609)
*Hui Wang,Yang Liu,Xiaoyu Zhang,Chaoxu Mu*

Main category: cs.AI

TL;DR: 本文提出了一种名为CogMCTS的新颖认知引导的MCTS框架，该框架通过多轮认知反馈和元启发式管理，提高了解的多样性以及整体优化性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于大型语言模型的进化方法易陷入局部最优，且认知集成的多轮次不足，搜索多样性受限，因此提出了这种新的认知引导的MCTS框架来克服这些限制。

Method: CogMCTS框架将大型语言模型的认知指导机制与MCTS紧密结合，并通过多轮次的认知反馈机制结合历史经验和节点信息，动态提高启发式生成的效率与质量。此外，通过精英启发式管理与策略性变异，该框架平衡了启发式多样性和高质量经验的利用。

Result: 实验结果表明，CogMCTS在稳定性、效率和解的质量方面优于现有基于大型语言模型的自动启发式设计方法。

Conclusion: CogMCTS框架能够有效提高启发式的自动生成效率与质量，提供更具竞争力的整体优化性能。

Abstract: Automatic Heuristic Design (AHD) is an effective1 framework for solving complex optimization prob-2 lems. The development of large language mod-3 els (LLMs) enables the automated generation of4 heuristics. Existing LLM-based evolutionary meth-5 ods rely on population strategies and are prone6 to local optima. Integrating LLMs with Monte7 Carlo Tree Search (MCTS) improves the trade-off8 between exploration and exploitation, but multi-9 round cognitive integration remains limited and10 search diversity is constrained. To overcome these11 limitations, this paper proposes a novel cognitive-12 guided MCTS framework (CogMCTS). CogMCTS13 tightly integrates the cognitive guidance mecha-14 nism of LLMs with MCTS to achieve efficient au-15 tomated heuristic optimization. The framework16 employs multi-round cognitive feedback to incor-17 porate historical experience, node information, and18 negative outcomes, dynamically improving heuris-19 tic generation. Dual-track node expansion com-20 bined with elite heuristic management balances the21 exploration of diverse heuristics and the exploita-22 tion of high-quality experience. In addition, strate-23 gic mutation modifies the heuristic forms and pa-24 rameters to further enhance the diversity of the so-25 lution and the overall optimization performance.26 The experimental results indicate that CogMCTS27 outperforms existing LLM-based AHD methods in28 stability, efficiency, and solution quality.

</details>


### [120] [Protein Secondary Structure Prediction Using Transformers](https://arxiv.org/abs/2512.08613)
*Manzi Kevin Maxime*

Main category: cs.AI

TL;DR: 本文提出了一种基于Transformer的模型，通过注意力机制预测蛋白质二级结构。采用滑动窗口数据增强技术增加了训练样本，模型表现出在不同长度序列上良好的泛化能力和捕捉局部与远程残基交互的良好能力。


<details>
  <summary>Details</summary>
Motivation: 蛋白质功能的理解依赖于对其二级结构（如α螺旋、β片层和无规则卷曲）的预测，这对于生物学研究至关重要。

Method: 该研究利用Transformer模型和注意力机制处理蛋白质序列数据，同时采用滑动窗口技术增强数据集。

Result: 模型显示了在各种长度序列上出色的泛化能力和捕捉局部及远程残基相互作用的能力。

Conclusion: 该工作提出的方法为蛋白质二级结构预测提供了有效的新方法。

Abstract: Predicting protein secondary structures such as alpha helices, beta sheets, and coils from amino acid sequences is essential for understanding protein function. This work presents a transformer-based model that applies attention mechanisms to protein sequence data to predict structural motifs. A sliding-window data augmentation technique is used on the CB513 dataset to expand the training samples. The transformer shows strong ability to generalize across variable-length sequences while effectively capturing both local and long-range residue interactions.

</details>


### [121] [Towards Foundation Models with Native Multi-Agent Intelligence](https://arxiv.org/abs/2512.08743)
*Shuyue Hu,Haoyang Yan,Yiqun Zhang,Yang Chen,Dongzhan Zhou,Lei Bai*

Main category: cs.AI

TL;DR: 文章强调在基础模型（FMs）中增强多智能体智能的重要性，并指出单任务性能不足以保证多智能体环境中的智能表现，提出了构建具有多智能体能力的基础模型的关键研究方向。


<details>
  <summary>Details</summary>
Motivation: 现有的基础模型虽然在单个智能体任务上取得了显著进步，但目前缺乏在多智能体环境中的表现。文章旨在填补这一研究空白。

Method: 文章通过对41个大型语言模型进行了全面的实验研究，证明了单智能体性能的强并不能自动转化为多智能体环境中的能力，进而指出了构建基础模型的关键研究方向。

Result: 研究结果显示单智能体模型的压力测试并不能代表多智能体环境中的多样性挑战，指出在构建FMs时需要考虑新的多智能体环境的具体因素。

Conclusion: 文章建议未来研究应在数据集构建、评估方法、训练策略和安全考虑等方面做出改进，以支持基础模型在多智能体环境中的应用。

Abstract: Foundation models (FMs) are increasingly assuming the role of the "brain" of AI agents. While recent efforts have begun to equip FMs with native single-agent abilities -- such as GUI interaction or integrated tool use -- we argue that the next frontier is endowing FMs with native multi-agent intelligence. We identify four core capabilities of FMs in multi-agent contexts: understanding, planning, efficient communication, and adaptation. Contrary to assumptions about the spontaneous emergence of such abilities, we provide extensive empirical evidence across 41 large language models showing that strong single-agent performance alone does not automatically yield robust multi-agent intelligence. To address this gap, we outline key research directions -- spanning dataset construction, evaluation, training paradigms, and safety considerations -- for building FMs with native multi-agent intelligence.

</details>


### [122] [CARLoS: Retrieval via Concise Assessment Representation of LoRAs at Scale](https://arxiv.org/abs/2512.08826)
*Shahar Sarfaty,Adi Haviv,Uri Hacohen,Niva Elkin-Koren,Roi Livni,Amit H. Bermano*

Main category: cs.AI

TL;DR: CARLoS 提出了一种大规模的框架，用于不依赖额外元数据来表征 LoRA。通过夹角和基模型生成结果的差异，定义了方向、强度和一致性三个维度来表征 LoRA。在此基础上，CARLoS 建立了有效的检索框架，能够在自动化和人工评价中超越纯文本基线。同时，这种表征也有助于版权分析。


<details>
  <summary>Details</summary>
Motivation: 现有的 LoRA 发现方法依赖于不可靠的用户描述或有偏见的流行度指标，这限制了 LoRA 的可实用性。CARLoS 指出，必须开发一种新的方法来更好地理解 LoRA，提高其易用性。

Method: CARLoS 采用了一个多阶段的方法。首先，对超过 650 个 LoRA 进行广泛分析，并将它们应用于各种提示和种子进行图像生成。其次，使用 CLIP 向量的夹角和与基模型生成结果的对比，定义了方向、强度和一致性三个部分来描述 LoRA 的行为。最后，CARLoS 依据这些表征开发了一个高效的检索框架，用于文本查询的语义匹配，并能够过滤掉强度过大或不稳定的 LoRA。

Result: 通过 CARLoS，研究人员能够以有效和可靠的方式表征 LoRA。在自动化和人工评估中，新的检索框架的表现优于传统的基于文本的方法。此外，CARLoS 还提供了一种将 LoRA 的效果与法律中的实质性以及意图概念联系起来的方法，这增强了该方法在版权法等相关领域的应用潜力。

Conclusion: CARLoS 为未受多余元数据约束的 LoRA 表征提供了一个强大的框架，标志着在比较和检索 LoRA 方面取得了重要进展，并且为版权法等领域提供了实用的支持。

Abstract: The rapid proliferation of generative components, such as LoRAs, has created a vast but unstructured ecosystem. Existing discovery methods depend on unreliable user descriptions or biased popularity metrics, hindering usability. We present CARLoS, a large-scale framework for characterizing LoRAs without requiring additional metadata. Analyzing over 650 LoRAs, we employ them in image generation over a variety of prompts and seeds, as a credible way to assess their behavior. Using CLIP embeddings and their difference to a base-model generation, we concisely define a three-part representation: Directions, defining semantic shift; Strength, quantifying the significance of the effect; and Consistency, quantifying how stable the effect is. Using these representations, we develop an efficient retrieval framework that semantically matches textual queries to relevant LoRAs while filtering overly strong or unstable ones, outperforming textual baselines in automated and human evaluations. While retrieval is our primary focus, the same representation also supports analyses linking Strength and Consistency to legal notions of substantiality and volition, key considerations in copyright, positioning CARLoS as a practical system with broader relevance for LoRA analysis.

</details>


### [123] [Interpolation in Knowledge Representation](https://arxiv.org/abs/2512.08833)
*Jean Christoph Jung,Patrick Koopmann,Matthias Knorr*

Main category: cs.AI

TL;DR: 本文探讨了描述逻辑和逻辑程序两种知识表示法的Craig插值和均匀插值问题，理论分析了不存在的情况，并提供了计算插值的实践方法。


<details>
  <summary>Details</summary>
Motivation: 解释、遗忘、模块化和重用、甚至学习等知识表示的应用促使作者更深入地研究Craig插值和均匀插值。

Method: 作者通过对描述逻辑和逻辑程序这两大知识表示正式系统的理论分析和实践方法探索来研究插值问题。

Result: 作者研究了描述逻辑和逻辑程序是否具有Craig或均匀插值，并提出了一些计算插值的实践方法。

Conclusion: 尽管许多知识表示形式一般不具有Craig或均匀插值，但作者的工作提供了方法来计算这些形式的插值。

Abstract: Craig interpolation and uniform interpolation have many applications in knowledge representation, including explainability, forgetting, modularization and reuse, and even learning. At the same time, many relevant knowledge representation formalisms do in general not have Craig or uniform interpolation, and computing interpolants in practice is challenging. We have a closer look at two prominent knowledge representation formalisms, description logics and logic programming, and discuss theoretical results and practical methods for computing interpolants.

</details>


### [124] [Same Content, Different Answers: Cross-Modal Inconsistency in MLLMs](https://arxiv.org/abs/2512.08923)
*Angela van Sprang,Laurens Samson,Ana Lucic,Erman Acar,Sennay Ghebreab,Yuki M. Asano*

Main category: cs.AI

TL;DR: 该研究引入了两个新的基准测试REST和REST+，以系统地评估多模态大型语言模型（MLLMs）中的跨模态不一致性。发现尽管OCR准确，视觉特征和视图词元数量也会影响模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究中的多模态大型语言模型（MLLMs）在不同模态中无法执行相同的任务，导致跨模态不一致性问题。该研究旨在开发新的基准测试以更好地评估和理解这种不一致性。

Method: 研究团队设计了REST和REST+两个新的基准测试，包含了三种模态（图像、文本、混合）下具有相同语义信息的样本。通过评估15个最新的MLLMs，研究团队量化了不同模态间的不一致性程度，并研究了视觉特征和视图词元数量对模型性能的影响。

Result: 研究发现，最先进的MLLMs在不同模态之间的不一致性问题依然显著，即使考虑OCR的问题，视觉特征（如文本的颜色和分辨率，但不是字体）和视图词元数量也对模型性能有影响。

Conclusion: 研究结论认为，MLLMs的跨模态不一致性评分与文本和图像之间的模态差距有关，这为理解跨模态不一致的机制提供了机械性解释。

Abstract: We introduce two new benchmarks REST and REST+(Render-Equivalence Stress Tests) to enable systematic evaluation of cross-modal inconsistency in multimodal large language models (MLLMs). MLLMs are trained to represent vision and language in the same embedding space, yet they cannot perform the same tasks in both modalities. Our benchmarks contain samples with the same semantic information in three modalities (image, text, mixed) and we show that state-of-the-art MLLMs cannot consistently reason over these different modalities. We evaluate 15 MLLMs and find that the degree of modality inconsistency varies substantially, even when accounting for problems with text recognition (OCR). Neither rendering text as image nor rendering an image as text solves the inconsistency. Even if OCR is correct, we find that visual characteristics (text colour and resolution, but not font) and the number of vision tokens have an impact on model performance. Finally, we find that our consistency score correlates with the modality gap between text and images, highlighting a mechanistic interpretation of cross-modal inconsistent MLLMs.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [125] [NysX: An Accurate and Energy-Efficient FPGA Accelerator for Hyperdimensional Graph Classification at the Edge](https://arxiv.org/abs/2512.08089)
*Jebacyril Arockiaraj,Dhruv Parikh,Viktor Prasanna*

Main category: cs.AR

TL;DR: NysX 是一个针对Nyström 基础的Hyperdimensional Computing (HDC) 图分类的FPGA加速器，通过综合优化，实现资源受限平台下的实时、高效推理。


<details>
  <summary>Details</summary>
Motivation: 为了在边缘设备上进行实时、能源高效的图分类推理，而传统的Nyström方法面临多种挑战，如样本冗余、存储限制、昂贵的查找时间和负载不平衡。

Method: NysX 提出了四个关键优化技术：结合均匀抽样和确定性点过程（DPP）的混合地标样本选择策略，从而减少冗余并提高准确性；流式架构以最大化外部内存带宽利用；使用最小完美散列查找引擎实现O(1)时间复杂度的关键字到索引映射，同时保留低片上内存开销；以及针对稀疏矩阵乘法优化的负载均衡算法。

Result: NysX 在AMD Zynq UltraScale+ FPGA 上实施，相比优化的CPU和GPU 主线程实现了6.85 倍（4.32 倍）的加速，以及169 倍（314 倍）的能效提升，并且在TUDataset 基准测试中，平均提高了3.4% 的分类准确性。

Conclusion: NysX 通过多方面的优化，成功地实现了在资源受限的边缘平台上进行实时高效的图分类推理，并且在性能和准确性上都有显著的提升。

Abstract: Real-time, energy-efficient inference on edge devices is essential for graph classification across a range of applications. Hyperdimensional Computing (HDC) is a brain-inspired computing paradigm that encodes input features into low-precision, high-dimensional vectors with simple element-wise operations, making it well-suited for resource-constrained edge platforms. Recent work enhances HDC accuracy for graph classification via Nyström kernel approximations. Edge acceleration of such methods faces several challenges: (i) redundancy among (landmark) samples selected via uniform sampling, (ii) storing the Nyström projection matrix under limited on-chip memory, (iii) expensive, contention-prone codebook lookups, and (iv) load imbalance due to irregular sparsity in SpMV. To address these challenges, we propose NysX, the first end-to-end FPGA accelerator for Nyström-based HDC graph classification at the edge. NysX integrates four key optimizations: (i) a hybrid landmark selection strategy combining uniform sampling with determinantal point processes (DPPs) to reduce redundancy while improving accuracy; (ii) a streaming architecture for Nyström projection matrix maximizing external memory bandwidth utilization; (iii) a minimal-perfect-hash lookup engine enabling $O(1)$ key-to-index mapping with low on-chip memory overhead; and (iv) sparsity-aware SpMV engines with static load balancing. Together, these innovations enable real-time, energy-efficient inference on resource-constrained platforms. Implemented on an AMD Zynq UltraScale+ (ZCU104) FPGA, NysX achieves $6.85\times$ ($4.32\times$) speedup and $169\times$ ($314\times$) energy efficiency gains over optimized CPU (GPU) baselines, while improving classification accuracy by $3.4\%$ on average across TUDataset benchmarks, a widely used standard for graph classification.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [126] [Multi-domain performance analysis with scores tailored to user preferences](https://arxiv.org/abs/2512.08715)
*Sébastien Piérard,Adrien Deliège,Marc Van Droogenbroeck*

Main category: cs.PF

TL;DR: 本文通过概率框架将性能视作概率度量，并研究加权平均过程，提出了四种基于用户偏好定义的领域（最容易、最难、优势和瓶颈领域），并为二分类任务开发了新的可视化工具。


<details>
  <summary>Details</summary>
Motivation: 作者试图揭示算法性能对不同应用领域分布的敏感性，并通过特定的概率框架来理解和表征这种现象。

Method: 采用概率框架表示性能，引入概率措施来定义性能。通过计算加权均值来总结性能，并基于此定义了不同类型的领域。

Result: 作者定义了四种类别的领域，并提出了适用于二分类任务的新可视化工具。

Conclusion: 本文的工作提供了对性能分布敏感性的深入见解，并为复杂任务中的算法分析提供了新的方法和工具

Abstract: The performance of algorithms, methods, and models tends to depend heavily on the distribution of cases on which they are applied, this distribution being specific to the applicative domain. After performing an evaluation in several domains, it is highly informative to compute a (weighted) mean performance and, as shown in this paper, to scrutinize what happens during this averaging. To achieve this goal, we adopt a probabilistic framework and consider a performance as a probability measure (e.g., a normalized confusion matrix for a classification task). It appears that the corresponding weighted mean is known to be the summarization, and that only some remarkable scores assign to the summarized performance a value equal to a weighted arithmetic mean of the values assigned to the domain-specific performances. These scores include the family of ranking scores, a continuum parameterized by user preferences, and that the weights to consider in the arithmetic mean depend on the user preferences. Based on this, we rigorously define four domains, named easiest, most difficult, preponderant, and bottleneck domains, as functions of user preferences. After establishing the theory in a general setting, regardless of the task, we develop new visual tools for two-class classification.

</details>
