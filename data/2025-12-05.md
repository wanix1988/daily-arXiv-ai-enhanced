<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 92]
- [cs.CL](#cs.CL) [Total: 24]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.AI](#cs.AI) [Total: 31]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [OnSight Pathology: A real-time platform-agnostic computational pathology companion for histopathology](https://arxiv.org/abs/2512.04187)
*Jinzhen Hu,Kevin Faust,Parsa Babaei Zadeh,Adrienn Bourkas,Shane Eaton,Andrew Young,Anzar Alvi,Dimitrios George Oreopoulos,Ameesha Paliwal,Assem Saleh Alrumeh,Evelyn Rose Kamski-Hennekam,Phedias Diamandis*

Main category: cs.CV

TL;DR: OnSight Pathology是一种平台无关的计算机视觉软件，通过连续自定义屏幕捕获，提供实时AI推理，解决了数字病理学中的壁垒，能够在各种研究和临床工作流程中实现成本效益和安全部署。


<details>
  <summary>Details</summary>
Motivation: 现有数字病理学解决方案限制了AI在宏微观病理分析中的实际应用，OnSight Pathology旨在解决这些挑战，提供便捷、安全且成本效益的AI辅助工具。

Method: OnSight Pathology使用连续的、定制的屏幕捕获技术，实现本地实时AI推理，适用于各种等级的个人电脑，无需复杂的软件集成。并且内置了多模式聊天助手，提供可验证的图像描述，规避了严格的类别标签。

Result: 该软件在不同的显微镜查看器和临床数字病理设置中的超过2500个全切片图像上展示了实用性，能够完成包括脑肿瘤类别分类、有丝分裂检测和免疫组化染色量化的常规组病理任务。此外，该软件还与实时显微镜摄像头连接兼容，支持从个人智能手机获取实时样本。

Conclusion: OnSight Pathology能够跨越广泛的病理管道，实时提供AI推理，有助于降低AI工具在组织病理学中的采用门槛，提高诊断准确性和工作流程效率。

Abstract: The microscopic examination of surgical tissue remains a cornerstone of disease classification but relies on subjective interpretations and access to highly specialized experts, which can compromise accuracy and clinical care. While emerging breakthroughs in artificial intelligence (AI) offer promise for automated histological analysis, the growing number of proprietary digital pathology solutions has created barriers to real-world deployment. To address these challenges, we introduce OnSight Pathology, a platform-agnostic computer vision software that uses continuous custom screen captures to provide real-time AI inferences to users as they review digital slide images. Accessible as a single, self-contained executable file (https://onsightpathology.github.io/ ), OnSight Pathology operates locally on consumer-grade personal computers without complex software integration, enabling cost-effective and secure deployment in research and clinical workflows. Here we demonstrate the utility of OnSight Pathology using over 2,500 publicly available whole slide images across different slide viewers, as well as cases from our clinical digital pathology setup. The software's robustness is highlighted across routine histopathological tasks, including the classification of common brain tumor types, mitosis detection, and the quantification of immunohistochemical stains. A built-in multi-modal chat assistant provides verifiable descriptions of images, free of rigid class labels, for added quality control. Lastly, we show compatibility with live microscope camera feeds, including from personal smartphones, offering potential for deployment in more analog, inter-operative, and telepathology settings. Together, we highlight how OnSight Pathology can deliver real-time AI inferences across a broad range of pathology pipelines, removing key barriers to the adoption of AI tools in histopathology.

</details>


### [2] [Look Around and Pay Attention: Multi-camera Point Tracking Reimagined with Transformers](https://arxiv.org/abs/2512.04213)
*Bishoy Galoaa,Xiangyu Bai,Shayda Moezzi,Utsav Nandi,Sai Siddhartha Vivek Dhir Rangoju,Somaieh Amraee,Sarah Ostadabbas*

Main category: cs.CV

TL;DR: LAPA 是一种新颖的端到端的基于Transformer的多摄像机点跟踪架构，它结合了基于外观的匹配和几何约束，通过跨视图与时间的联合推理和自注意力机制建立软对应关系，有效提升在复杂场景下的跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 传统的多摄像机点跟踪管道会将检测、关联和跟踪分离开处理，导致错误传播和时间上的不一致性，特别是在具有挑战性的场景中。LAPA 通过联合视觉和时间的自注意力机制来解决这些问题，从而减少这些限制。

Method: LAPA 使用自注意力机制来跨视图和时间共同推理，通过几何先验增强的跨视图注意力机制建立软对应关系，采用注意力加权聚合来构建3D点表示，从而内在地处理不确定性与部分观察，确保跨事件时间一致性。

Result: LAPA 在具有挑战性的数据集上表现出色，包括新创建的多摄像机（MC）版本的 TAPVid-3D panoptic 和 PointOdyssey，取得了显著的性能提升。实验证明，LAPA 在 TAPVid-3D-MC 获得了 37.5% 的 APD，在 PointOdyssey-MC 获得了 90.3% 的 APD，特别是在复杂运动和遮挡的场景下。

Conclusion: LAPA 的统一方法显著提高了多摄像机点跟踪的性能与准确性，为多摄像机的点跟踪提供了新的解决方案。

Abstract: This paper presents LAPA (Look Around and Pay Attention), a novel end-to-end transformer-based architecture for multi-camera point tracking that integrates appearance-based matching with geometric constraints. Traditional pipelines decouple detection, association, and tracking, leading to error propagation and temporal inconsistency in challenging scenarios. LAPA addresses these limitations by leveraging attention mechanisms to jointly reason across views and time, establishing soft correspondences through a cross-view attention mechanism enhanced with geometric priors. Instead of relying on classical triangulation, we construct 3D point representations via attention-weighted aggregation, inherently accommodating uncertainty and partial observations. Temporal consistency is further maintained through a transformer decoder that models long-range dependencies, preserving identities through extended occlusions. Extensive experiments on challenging datasets, including our newly created multi-camera (MC) versions of TAPVid-3D panoptic and PointOdyssey, demonstrate that our unified approach significantly outperforms existing methods, achieving 37.5% APD on TAPVid-3D-MC and 90.3% APD on PointOdyssey-MC, particularly excelling in scenarios with complex motions and occlusions. Code is available at https://github.com/ostadabbas/Look-Around-and-Pay-Attention-LAPA-

</details>


### [3] [ReasonX: MLLM-Guided Intrinsic Image Decomposition](https://arxiv.org/abs/2512.04222)
*Alara Dirik,Tuanfeng Wang,Duygu Ceylan,Stefanos Zafeiriou,Anna Frühstück*

Main category: cs.CV

TL;DR: ReasonX提出了一种创新框架，利用多模态大语言模型作为感知裁判，提供相对内在成分的比较，并将这些比较作为强化奖励，用于在未标记、真实世界的图像上微调内在分解模型，从而显著提升了多种基架构下的不同内在预测器性能。


<details>
  <summary>Details</summary>
Motivation: 传统的成对监督难以适应多样化的现实场景，尤其是对于无监督或低标记度的数据。为了提高模型在真实环境中的泛化能力，本文提出了ReasonX框架。

Method: ReasonX采用了一种基于模型的强化学习方法，使用多模态大语言模型（MLLM）作为评判者，对图像的内在成分进行相对比较，并通过这种比较为模型的训练提供反馈。评判者的反馈基于模型输出的分析关系，指引模型学习更多一致性的内在特性。

Result: 在多种基础模型和模态中，ReasonX框架展示了显著的提升效果，包括IIW数据集上曝辐的9-25% WHDR降低和ETH3D数据集上深度的最高46%准确率提升。

Conclusion: 此项研究证明了基于大语言模型的比较监督对于连接低级和高级视觉推理的潜力，对于提高模型在真实世界应用中的表现具有重要意义。

Abstract: Intrinsic image decomposition aims to separate images into physical components such as albedo, depth, normals, and illumination. While recent diffusion- and transformer-based models benefit from paired supervision from synthetic datasets, their generalization to diverse, real-world scenarios remains challenging. We propose ReasonX, a novel framework that leverages a multimodal large language model (MLLM) as a perceptual judge providing relative intrinsic comparisons, and uses these comparisons as GRPO rewards for fine-tuning intrinsic decomposition models on unlabeled, in-the-wild images. Unlike RL methods for generative models, our framework aligns conditional intrinsic predictors by rewarding agreement between the judge's relational assessments and analytically derived relations from the model's outputs. ReasonX is model-agnostic and can be applied to different intrinsic predictors. Across multiple base architectures and modalities, ReasonX yields significant improvements, including 9-25% WHDR reduction on IIW albedo and up to 46% depth accuracy gains on ETH3D, highlighting the promise of MLLM-guided comparative supervision to bridge low- and high-level vision reasoning.

</details>


### [4] [6 Fingers, 1 Kidney: Natural Adversarial Medical Images Reveal Critical Weaknesses of Vision-Language Models](https://arxiv.org/abs/2512.04238)
*Leon Mayer,Piotr Kalinowski,Caroline Ebersbach,Marcel Knopp,Tim Rädsch,Evangelia Christodoulou,Annika Reinke,Fiona R. Kolbinger,Lena Maier-Hein*

Main category: cs.CV

TL;DR: 该基准引入了自然发生的稀有解剖变异，涵盖了多种影像学模态和解剖区域，评估了22种最先进的视觉-语言模型在稀有解剖学表现上的性能。研究发现，针对基本医学感知任务，模型的准确性从典型解剖结构的74%下降到稀有解剖结构的29%。模型误差与预期的解剖学偏差相符，且未发现任何模型规模增加或干预措施能解决此问题。这突显出当前视觉-语言模型在通用化稀有解剖呈现方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试主要评估常见的解剖学表现，未能捕捉到稀有变异带来的挑战。因此，研究团队希望开发一个新的基准测试，以填补这一空白。

Method: 研究团队引入了AdversarialAnatomyBench，其中包含来自不同影像学模态和解剖区域的自然发生的稀有解剖变异。使用这个基准测试了22种最先进的视觉-语言模型。

Result: 研究结果显示，视觉-语言模型在典型和稀有解剖结构上的表现差异很大。当面对基本医学感知任务时，模型的准确率从74%下降到29%。即便是最先进的模型也显示出41-51%的性能下降。模型的误差模式与预期的解剖学偏差相符，并且无论是模型规模的增加还是干预措施，如有意识的提示和测试时的推理都无法解决这个问题。

Conclusion: 当前的视觉-语言模型在通用化稀有解剖呈现方面存在重要局限性，这需要系统的衡量和缓解。此外，该基准为评估和减轻多模态医学AI系统中的解剖学偏见提供了基础。

Abstract: Vision-language models are increasingly integrated into clinical workflows. However, existing benchmarks primarily assess performance on common anatomical presentations and fail to capture the challenges posed by rare variants. To address this gap, we introduce AdversarialAnatomyBench, the first benchmark comprising naturally occurring rare anatomical variants across diverse imaging modalities and anatomical regions. We call such variants that violate learned priors about "typical" human anatomy natural adversarial anatomy. Benchmarking 22 state-of-the-art VLMs with AdversarialAnatomyBench yielded three key insights. First, when queried with basic medical perception tasks, mean accuracy dropped from 74% on typical to 29% on atypical anatomy. Even the best-performing models, GPT-5, Gemini 2.5 Pro, and Llama 4 Maverick, showed performance drops of 41-51%. Second, model errors closely mirrored expected anatomical biases. Third, neither model scaling nor interventions, including bias-aware prompting and test-time reasoning, resolved these issues. These findings highlight a critical and previously unquantified limitation in current VLM: their poor generalization to rare anatomical presentations. AdversarialAnatomyBench provides a foundation for systematically measuring and mitigating anatomical bias in multimodal medical AI systems.

</details>


### [5] [MVRoom: Controllable 3D Indoor Scene Generation with Multi-View Diffusion Models](https://arxiv.org/abs/2512.04248)
*Shaoheng Fang,Chaohui Yu,Fan Wang,Qixing Huang*

Main category: cs.CV

TL;DR: MVRoom引入了一种新的NVS管道，使用多视角扩散并结合粗糙的3D布局来合成室内场景，通过两个阶段设计确保多视角一致性。


<details>
  <summary>Details</summary>
Motivation: 当前的NVS方法在合成室内场景时缺乏语义连贯性，MVRoom旨在提供高保真度且可控的3D场景生成，以超越现有技术。

Method: MVRoom采用了两阶段设计。第一阶段使用新颖的表示来连接3D布局和一致的图像条件信号以进行多视角生成。第二阶段通过布局感知的视差注意机制进行条件下的多视角生成，以在生成过程中增强多视角一致性。

Result: 实验结果表明，MVRoom在定量和定性方面均优于最先进的基线方法，实现了高保真度和可控的3D场景生成。此外，通过迭代框架支持从文本生成具有可变对象数量和场景复杂性的3D场景。

Conclusion: MVRoom提供了一种新的方法来解决NVS中的多视角一致性问题，通过引入多视角生成、布局感知视差注意力机制以及迭代框架来确保合成场景的语义连贯性和多样性。

Abstract: We introduce MVRoom, a controllable novel view synthesis (NVS) pipeline for 3D indoor scenes that uses multi-view diffusion conditioned on a coarse 3D layout. MVRoom employs a two-stage design in which the 3D layout is used throughout to enforce multi-view consistency. The first stage employs novel representations to effectively bridge the 3D layout and consistent image-based condition signals for multi-view generation. The second stage performs image-conditioned multi-view generation, incorporating a layout-aware epipolar attention mechanism to enhance multi-view consistency during the diffusion process. Additionally, we introduce an iterative framework that generates 3D scenes with varying numbers of objects and scene complexities by recursively performing multi-view generation (MVRoom), supporting text-to-scene generation. Experimental results demonstrate that our approach achieves high-fidelity and controllable 3D scene generation for NVS, outperforming state-of-the-art baseline methods both quantitatively and qualitatively. Ablation studies further validate the effectiveness of key components within our generation pipeline.

</details>


### [6] [UniLight: A Unified Representation for Lighting](https://arxiv.org/abs/2512.04267)
*Zitian Zhang,Iliyan Georgiev,Michael Fischer,Yannick Hold-Geoffroy,Jean-François Lalonde,Valentin Deschaintre*

Main category: cs.CV

TL;DR: 本文提出了一种名为UniLight的统一光照表示方法，使用联合潜空间统一多种模态，通过对比训练使不同模态的表示对齐，并结合方向性理解的辅助任务进行优化。实现大规模跨任务训练和评估，展示其在光照检索、环境图生成和基于扩散的图像合成等任务中捕获一致且可转移的光照特征。


<details>
  <summary>Details</summary>
Motivation: 当前各种光照表示方法虽然丰富，但各不兼容，影响跨模态的任务转移。为了克服这一问题并提供一种统一的光照表示方法，文章提出了UniLight。

Method: UniLight通过四个模态特定编码器针对文本、图像、辐照度和环境图进行对比训练，以对齐它们的表示。同时，通过一个辅助的关于球谐函数预测的任务来增强方向性的理解。该方法建立了多模态数据管道，支持大规模的训练和评估。

Result: 实验结果显示，UniLight捕获了可以跨模态且一致的光照特征，并展示了它能够灵活地应用于光照检索、环境图生成和基于扩散的图像合成等多种任务。

Conclusion: 本文提出了一种用于统一多个模态的光照表示方法，并通过实验证明其有效性和广泛适用性。

Abstract: Lighting has a strong influence on visual appearance, yet understanding and representing lighting in images remains notoriously difficult. Various lighting representations exist, such as environment maps, irradiance, spherical harmonics, or text, but they are incompatible, which limits cross-modal transfer. We thus propose UniLight, a joint latent space as lighting representation, that unifies multiple modalities within a shared embedding. Modality-specific encoders for text, images, irradiance, and environment maps are trained contrastively to align their representations, with an auxiliary spherical-harmonics prediction task reinforcing directional understanding. Our multi-modal data pipeline enables large-scale training and evaluation across three tasks: lighting-based retrieval, environment-map generation, and lighting control in diffusion-based image synthesis. Experiments show that our representation captures consistent and transferable lighting features, enabling flexible manipulation across modalities.

</details>


### [7] [Inference-time Stochastic Refinement of GRU-Normalizing Flow for Real-time Video Motion Transfer](https://arxiv.org/abs/2512.04282)
*Tasmiah Haque,Srinjoy Das*

Main category: cs.CV

TL;DR: 论文提出了一种新的GRU-STP（Gated Recurrent Unit-Stochastic Normalizing Flows）方法，通过在推理时引入MCMC步骤，增强了GRU-NF（Gated Recurrent Unit-Normalizing Flows）模型的表达能力，从而使未来预测更加多样化。


<details>
  <summary>Details</summary>
Motivation: 现有序列预测方法在多样性的提升上有局限性，尤其是在实时视频运动转移应用中需要准确且多样的未来预测来支持真实合成和不确定条件下的稳健决策。

Method: 论文提出将门控循环单元（GRU）与正则化流（NF）相结合，同时引入蒙特卡洛模拟（MCMC）步骤来增强模型在推理时的表达能力。该方法能够捕捉多模态分布，并通过探索更丰富的输出空间来更好地逼近真实的数据分布。

Result: 实验结果显示，与GRU-NF相比，GRU-STP在生成多样化输出方面表现更优，即使在较长的预测时序上也是如此，且不会牺牲准确性。这意味着该模型能够更有效地捕捉多模态行为。

Conclusion: 该研究为基于流的序列生成模型和含有随机动力学的生成时间序列预测提供了潜在的新方向，展示了GRU-STP在实时视频运动转移中的优势。

Abstract: Real-time video motion transfer applications such as immersive gaming and vision-based anomaly detection require accurate yet diverse future predictions to support realistic synthesis and robust downstream decision making under uncertainty. To improve the diversity of such sequential forecasts we propose a novel inference-time refinement technique that combines Gated Recurrent Unit-Normalizing Flows (GRU-NF) with stochastic sampling methods. While GRU-NF can capture multimodal distributions through its integration of normalizing flows within a temporal forecasting framework, its deterministic transformation structure can limit expressivity. To address this, inspired by Stochastic Normalizing Flows (SNF), we introduce Markov Chain Monte Carlo (MCMC) steps during GRU-NF inference, enabling the model to explore a richer output space and better approximate the true data distribution without retraining. We validate our approach in a keypoint-based video motion transfer pipeline, where capturing temporally coherent and perceptually diverse future trajectories is essential for realistic samples and low bandwidth communication. Experiments show that our inference framework, Gated Recurrent Unit- Stochastic Normalizing Flows (GRU-SNF) outperforms GRU-NF in generating diverse outputs without sacrificing accuracy, even under longer prediction horizons. By injecting stochasticity during inference, our approach captures multimodal behavior more effectively. These results highlight the potential of integrating stochastic dynamics with flow-based sequence models for generative time series forecasting.

</details>


### [8] [Learning Single-Image Super-Resolution in the JPEG Compressed Domain](https://arxiv.org/abs/2512.04284)
*Sruthi Srinivasan,Elham Shakibapour,Rajy Rawther,Mehdi Saeedi*

Main category: cs.CV

TL;DR: 该研究提出直接在JPEG特征上训练模型以减少数据加载开销，提高数据加载和训练速度，同时保持与标准single-image超级分辨率方法相似的视觉质量。


<details>
  <summary>Details</summary>
Motivation: 由于深度学习模型变得越来越复杂，输入数据的大小也随之增加，尽管专门的深度学习硬件取得了很大进展，但数据加载仍是训练和推断速度的瓶颈。研究旨在减轻这一限制，提出了一种新的方法，即将模型直接训练在JPEG编码特征上，减少全JPEG解码的计算开销，提高数据加载效率。

Method: 研究提出了一种新的轻量级单图像超分辨率（SISR）管道，该管道在JPEG离散余弦变换（DCT）系数的频域上操作。将模型直接训练在JPEG编码特征上，而跳过了完整的JPEG解码过程。

Result: 研究的成果包括实现了2.6倍的数据加载速度提升和2.5倍的训练速度提升，同时保持了与标准SISR方法相似的视觉质量。

Conclusion: 该研究提出了一种新颖的方法，直接在JPEG编码特征上训练模型，以优化数据加载和训练速度，适用于单一图像超分辨率任务。

Abstract: Deep learning models have grown increasingly complex, with input data sizes scaling accordingly. Despite substantial advances in specialized deep learning hardware, data loading continues to be a major bottleneck that limits training and inference speed. To address this challenge, we propose training models directly on encoded JPEG features, reducing the computational overhead associated with full JPEG decoding and significantly improving data loading efficiency. While prior works have focused on recognition tasks, we investigate the effectiveness of this approach for the restoration task of single-image super-resolution (SISR). We present a lightweight super-resolution pipeline that operates on JPEG discrete cosine transform (DCT) coefficients in the frequency domain. Our pipeline achieves a 2.6x speedup in data loading and a 2.5x speedup in training, while preserving visual quality comparable to standard SISR approaches.

</details>


### [9] [Gamma-from-Mono: Road-Relative, Metric, Self-Supervised Monocular Geometry for Vehicular Applications](https://arxiv.org/abs/2512.04303)
*Gasser Elazab,Maximilian Jansen,Michael Unterreiner,Olaf Hellwich*

Main category: cs.CV

TL;DR: GfM是基于单目相机估计车辆周围3D几何结构的方法，通过预测主导道路平面和由Gamma表示的残余变化，实现了高度精确的深度和Gamma估计，特别适合自监督学习，无需大量标注数据，并在多种场景下具有鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决传统单目深度估计方法对微距特征的过度平滑问题，改善车辆控制的安全性和舒适性。

Method: 通过提出GfM方法，该方法利用单目相机预测主导的道路平面，结合无量纲的Gamma值来表示相对于平面的高度波动，不依赖于完整的外在校准。

Result: 在KITTI和Road Surface Reconstruction Dataset上的评估表明，GfM在接近场深度和Gamma估计方面达到了最先进的精度，同时保持了全球深度性能。该模型具有8.88M参数，能够适应多种相机设置，是首个在RSRD上评估的自监督单目方法。

Conclusion: GfM为单目几何估计提供了一种新颖的解决方案，对自我监督学习具有重要意义，无需大量标注数据，并展示了在不同场景中的鲁棒性能。

Abstract: Accurate perception of the vehicle's 3D surroundings, including fine-scale road geometry, such as bumps, slopes, and surface irregularities, is essential for safe and comfortable vehicle control. However, conventional monocular depth estimation often oversmooths these features, losing critical information for motion planning and stability. To address this, we introduce Gamma-from-Mono (GfM), a lightweight monocular geometry estimation method that resolves the projective ambiguity in single-camera reconstruction by decoupling global and local structure. GfM predicts a dominant road surface plane together with residual variations expressed by gamma, a dimensionless measure of vertical deviation from the plane, defined as the ratio of a point's height above it to its depth from the camera, and grounded in established planar parallax geometry. With only the camera's height above ground, this representation deterministically recovers metric depth via a closed form, avoiding full extrinsic calibration and naturally prioritizing near-road detail. Its physically interpretable formulation makes it well suited for self-supervised learning, eliminating the need for large annotated datasets. Evaluated on KITTI and the Road Surface Reconstruction Dataset (RSRD), GfM achieves state-of-the-art near-field accuracy in both depth and gamma estimation while maintaining competitive global depth performance. Our lightweight 8.88M-parameter model adapts robustly across diverse camera setups and, to our knowledge, is the first self-supervised monocular approach evaluated on RSRD.

</details>


### [10] [How (Mis)calibrated is Your Federated CLIP and What To Do About It?](https://arxiv.org/abs/2512.04305)
*Mainak Singha,Masih Aminbeidokhti,Paolo Casari,Elisa Ricci,Subhankar Roy*

Main category: cs.CV

TL;DR: 该研究探讨了联邦学习（FL）对CLIP模型校准的影响，并提出了一种名为FL²oRA的新方法来改进FL设置中的可靠性。


<details>
  <summary>Details</summary>
Motivation: 尽管CLIP等视觉-语言模型已经得到了广泛的研究，但它们的校准问题，即确保模型预测可靠性的能力，却受到了较少的关注。论文针对这一问题，在联邦学习设置下研究了CLIP模型的校准情况。

Method: 研究首先分析了文本提示调优（Textual Prompt Tuning） approaches，并发现它们在联邦学习环境下会降低校准指标。然后评估了现有的训练中校准技术在四种全局聚合方法下的表现，发现结果有限。基于此发现，提出了一个基于LoRA（低秩自适应）的简单方法FL²oRA，并分析了其有效性的原因。

Result: 在多个基准测试上的实验表明，FL²oRA能够持续产生校准良好的模型，减少了明确的校准过程的需求。

Conclusion: FL²oRA在联邦学习路径上提供了简单且有效的校准改进，对于提高模型的可靠性具有重要意义。

Abstract: While vision-language models like CLIP have been extensively studied, their calibration, crucial for reliable predictions, has received limited attention. Although a few prior works have examined CLIP calibration in offline settings, the impact of fine-tuning CLIP in a federated learning (FL) setup remains unexplored. In this work, we investigate how FL affects CLIP calibration and propose strategies to improve reliability in this distributed setting. We first analyze Textual Prompt Tuning approaches and show that they degrade calibration metrics when operating under FL. We also evaluate existing in-training calibration techniques across four global aggregation methods, finding that they provide limited improvements. Our results suggest that the key challenge lies not only in how we aggregate or calibrate, but in which components we choose to fine-tune. Motivated by this insight, we propose $\text{FL}^2\text{oRA}$, a straightforward LoRA-based approach that naturally improves calibration in FL, and we analyze the factors behind its effectiveness. Experiments on multiple benchmarks demonstrate that $\text{FL}^2\text{oRA}$ consistently produces well-calibrated models, reducing the need for explicit calibration procedures. Codes are available at https://github.com/mainaksingha01/FL2oRA.

</details>


### [11] [Text-Only Training for Image Captioning with Retrieval Augmentation and Modality Gap Correction](https://arxiv.org/abs/2512.04309)
*Rui Fonseca,Bruno Martins,Gil Rocha*

Main category: cs.CV

TL;DR: 文章提出了一种名为TOMCap的方法，通过利用预训练语言模型和CLIP表示来生成图像描述，这种方法无需对齐的图像-描述对，并且在实验中超过了其他无需训练和仅文本的方法。


<details>
  <summary>Details</summary>
Motivation: 为了减少对人为标注图像-文本对的依赖，提高无监督或弱监督图像描述方法的性能。

Method: TOMCap方法基于预训练的语言模型解码器，并通过减少模态差距和检索扩增的方法来生成图像描述。

Result: TOMCap在实验中表现出色，超过了其他无需训练和仅文本的方法。

Conclusion: 通过实验证明，TOMCap方法在图像描述任务上具有一定的优势。

Abstract: Image captioning has drawn considerable attention from the natural language processing and computer vision fields. Aiming to reduce the reliance on curated data, several studies have explored image captioning without any humanly-annotated image-text pairs for training, although existing methods are still outperformed by fully supervised approaches. This paper proposes TOMCap, i.e., an improved text-only training method that performs captioning without the need for aligned image-caption pairs. The method is based on prompting a pre-trained language model decoder with information derived from a CLIP representation, after undergoing a process to reduce the modality gap. We specifically tested the combined use of retrieved examples of captions, and latent vector representations, to guide the generation process. Through extensive experiments, we show that TOMCap outperforms other training-free and text-only methods. We also analyze the impact of different choices regarding the configuration of the retrieval-augmentation and modality gap reduction components.

</details>


### [12] [Real-time Cricket Sorting By Sex](https://arxiv.org/abs/2512.04311)
*Juan Manuel Cantarero Angulo,Matthew Smith*

Main category: cs.CV

TL;DR: 该研究开发了一种低成本的实时系统，用于自动按性别分类Aketa domesticus，增加筛选准确性，推动可持续昆虫养殖。


<details>
  <summary>Details</summary>
Motivation: 全球对可持续蛋白来源的需求激增，导致对可食用昆虫的兴趣增加。研究希望通过自动性别分类提高家蟋蟀养殖的效率和可持续性。

Method: 研究结合计算机视觉和物理执行器，使用Raspberry Pi 5、官方Raspberry AI摄像头和自定义YOLOv8 nano目标检测模型开发了一套装置。还集成了一个伺服驱动的分拣臂。

Result: 测试中，模型在IoU 0.5下的平均平均精确度（mAP@0.5）达到了0.977。实际实验中，装有蟋蟀的群体被分类的准确率为86.8%。

Conclusion: 该研究表明，可以在资源受限的设备上部署轻量级深度学习模型，为昆虫养殖应用提供了一种提高效率和可持续性的实际解决方案。

Abstract: The global demand for sustainable protein sources is driving increasing interest in edible insects, with Acheta domesticus (house cricket) identified as one of the most suitable species for industrial production. Current farming practices typically rear crickets in mixed-sex populations without automated sex sorting, despite potential benefits such as selective breeding, optimized reproduction ratios, and nutritional differentiation. This work presents a low-cost, real-time system for automated sex-based sorting of Acheta domesticus, combining computer vision and physical actuation. The device integrates a Raspberry Pi 5 with the official Raspberry AI Camera and a custom YOLOv8 nano object detection model, together with a servo-actuated sorting arm. The model reached a mean Average Precision at IoU 0.5 (mAP@0.5) of 0.977 during testing, and real-world experiments with groups of crickets achieved an overall sorting accuracy of 86.8%. These results demonstrate the feasibility of deploying lightweight deep learning models on resource-constrained devices for insect farming applications, offering a practical solution to improve efficiency and sustainability in cricket production.

</details>


### [13] [Mind-to-Face: Neural-Driven Photorealistic Avatar Synthesis via EEG Decoding](https://arxiv.org/abs/2512.04313)
*Haolin Xiong,Tianwen Fu,Pratusha Bhuvana Prasad,Yunxuan Cai,Haiwei Chen,Wenbin Teng,Hanyuan Xiao,Yajie Zhao*

Main category: cs.CV

TL;DR: Mind-to-Face 引入了一种新的框架，能够通过解码无创脑电图（EEG）信号直接生成高保真面部表情，不受面部遮挡或内部情绪的影响。


<details>
  <summary>Details</summary>
Motivation: 目前的表达性角色系统主要依赖于视觉线索，但在面部被遮挡或情绪保持内部化的情况下失效。Mind-to-Face 提出了一种新技术，通过解码无创脑电图（EEG）信号，直接生成逼真的面部表情。

Method: 该方法采用双模态记录设置来收集同步的 EEG 和多视图面部视频，在引发情绪刺激的过程期间。使用 CNN-Transformer 编码器将 EEG 信号映射到密集的 3D 位置图中，该模型可以采样超过 65,000 个顶点，捕获细微的几何形状和微妙的情绪动态，并通过修改的 3D 高斯斑点渲染管线生成逼真的、视图一致的结果。

Result: 实验结果显示，单独的 EEG 能够可靠地预测动态、个体特定的面部表情，包括微妙的情绪响应，表明神经信号的丰富性远超以前的认识。

Conclusion: Mind-to-Face 确立了一个新的神经驱动角色模式，能够实现个性化、情绪感知的远程存在和认知互动，在沉浸式环境中具有重要作用。

Abstract: Current expressive avatar systems rely heavily on visual cues, failing when faces are occluded or when emotions remain internal. We present Mind-to-Face, the first framework that decodes non-invasive electroencephalogram (EEG) signals directly into high-fidelity facial expressions. We build a dual-modality recording setup to obtain synchronized EEG and multi-view facial video during emotion-eliciting stimuli, enabling precise supervision for neural-to-visual learning. Our model uses a CNN-Transformer encoder to map EEG signals into dense 3D position maps, capable of sampling over 65k vertices, capturing fine-scale geometry and subtle emotional dynamics, and renders them through a modified 3D Gaussian Splatting pipeline for photorealistic, view-consistent results. Through extensive evaluation, we show that EEG alone can reliably predict dynamic, subject-specific facial expressions, including subtle emotional responses, demonstrating that neural signals contain far richer affective and geometric information than previously assumed. Mind-to-Face establishes a new paradigm for neural-driven avatars, enabling personalized, emotion-aware telepresence and cognitive interaction in immersive environments.

</details>


### [14] [DisentangleFormer: Spatial-Channel Decoupling for Multi-Channel Vision](https://arxiv.org/abs/2512.04314)
*Jiashu Liao,Pietro Liò,Marc de Kamps,Duygu Sarikaya*

Main category: cs.CV

TL;DR: DisentangleFormer 提出了一个通过空间-通道分解的多层次非纠缠表示学习架构，该架构在多个遥感和医学影像数据集上取得了SOTA性能，同时降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: Vision Transformers 中的标准自注意力机制同时处理空域和特征维度，导致结构和语义依赖的纠缠表示，限制了模型对不同信息的独立建模能力。特别是在高光谱成像领域，其中通道捕获不同的生物物理或生化线索，这一问题尤为显著。

Method: DisentangleFormer 采用了分布式设计，包括平行解纠缠模块、挤压令牌增强器和多尺度 FFN 三个核心组件，以实现空域和特征域的独立建模，同时尽可能减少两者的冗余。

Result: 在多种高光谱基准数据集上，DisentangleFormer 达到了最先进的性能，特别是在印度pine、帕维亚大学、休斯顿、大型遥感数据集 BigEarthNet 以及红外病理数据集上。此外，相比现有模型，DisentangleFormer 在 ImageNet 上保持了竞争力的同时，计算成本降低了 17.8%。

Conclusion: DisentangleFormer 架构通过空间-通道分解，在高光谱成像和遥远感知任务中展示了强大的表示学习能力，有望在未来推进相关领域的研究和发展。

Abstract: Vision Transformers face a fundamental limitation: standard self-attention jointly processes spatial and channel dimensions, leading to entangled representations that prevent independent modeling of structural and semantic dependencies. This problem is especially pronounced in hyperspectral imaging, from satellite hyperspectral remote sensing to infrared pathology imaging, where channels capture distinct biophysical or biochemical cues. We propose DisentangleFormer, an architecture that achieves robust multi-channel vision representation through principled spatial-channel decoupling. Motivated by information-theoretic principles of decorrelated representation learning, our parallel design enables independent modeling of structural and semantic cues while minimizing redundancy between spatial and channel streams. Our design integrates three core components: (1) Parallel Disentanglement: Independently processes spatial-token and channel-token streams, enabling decorrelated feature learning across spatial and spectral dimensions, (2) Squeezed Token Enhancer: An adaptive calibration module that dynamically fuses spatial and channel streams, and (3) Multi-Scale FFN: complementing global attention with multi-scale local context to capture fine-grained structural and semantic dependencies. Extensive experiments on hyperspectral benchmarks demonstrate that DisentangleFormer achieves state-of-the-art performance, consistently outperforming existing models on Indian Pine, Pavia University, and Houston, the large-scale BigEarthNet remote sensing dataset, as well as an infrared pathology dataset. Moreover, it retains competitive accuracy on ImageNet while reducing computational cost by 17.8% in FLOPs. The code will be made publicly available upon acceptance.

</details>


### [15] [SyncTrack4D: Cross-Video Motion Alignment and Video Synchronization for Multi-Video 4D Gaussian Splatting](https://arxiv.org/abs/2512.04315)
*Yonghan Lee,Tsung-Wei Huang,Shiv Gehlot,Jaehoon Choi,Guan-Ming Su,Dinesh Manocha*

Main category: cs.CV

TL;DR: 该论文提出了一种新颖的多视频4D高斯点云（4DGS）方法，通过融合高斯空间特征跟踪和最优运输算法，实现无同步视频集的4D重建，最终输出是同步的4DGS表示。


<details>
  <summary>Details</summary>
Motivation: 提出了同步跟踪4D（SyncTrack4D）方法，旨在解决多视图同步和高维动态场景重建的问题，特别是在处理无同步视频集时带来了新的挑战。

Method: 首先通过融合高斯- Wasserstein最优传输方法计算密集的视图间4D特征跟踪和对应关系；然后进行全局帧级时间对齐，以最大化匹配4D跟踪的运动叠加；最后通过基于运动样条骨架表示的多视频4D高斯点云方法实现亚帧同步。

Result: 在Panoptic Studio和SyncNeRF Blender上进行评估，结果表明亚帧同步的平均时间误差低于0.26帧，4D重建达到了26.3 PSNR分数。

Conclusion: 这是首个无预设场景对象或先验模型的通用4D高斯点云方法，具有重要的理论和实践意义。

Abstract: Modeling dynamic 3D scenes is challenging due to their high-dimensional nature, which requires aggregating information from multiple views to reconstruct time-evolving 3D geometry and motion. We present a novel multi-video 4D Gaussian Splatting (4DGS) approach designed to handle real-world, unsynchronized video sets. Our approach, SyncTrack4D, directly leverages dense 4D track representation of dynamic scene parts as cues for simultaneous cross-video synchronization and 4DGS reconstruction. We first compute dense per-video 4D feature tracks and cross-video track correspondences by Fused Gromov-Wasserstein optimal transport approach. Next, we perform global frame-level temporal alignment to maximize overlapping motion of matched 4D tracks. Finally, we achieve sub-frame synchronization through our multi-video 4D Gaussian splatting built upon a motion-spline scaffold representation. The final output is a synchronized 4DGS representation with dense, explicit 3D trajectories, and temporal offsets for each video. We evaluate our approach on the Panoptic Studio and SyncNeRF Blender, demonstrating sub-frame synchronization accuracy with an average temporal error below 0.26 frames, and high-fidelity 4D reconstruction reaching 26.3 PSNR scores on the Panoptic Studio dataset. To the best of our knowledge, our work is the first general 4D Gaussian Splatting approach for unsynchronized video sets, without assuming the existence of predefined scene objects or prior models.

</details>


### [16] [Mitigating Object and Action Hallucinations in Multimodal LLMs via Self-Augmented Contrastive Alignment](https://arxiv.org/abs/2512.04356)
*Kai-Po Chang,Wei-Yuan Cheng,Chi-Pin Huang,Fu-En Yang,Yu-Chiang Frank Wang*

Main category: cs.CV

TL;DR: 提出一种称为 SANTA 的框架来缓解 MLLMs 生成动态视频时出现的视觉对象和时序动作 Hallucination 问题。


<details>
  <summary>Details</summary>
Motivation: 当前的 MLLMs 在生成视频描述时容易出现事实不准确的问题，特别是对于动态视频中的视觉对象和时序动作存在幻觉现象。

Method: SANTA 框架通过自增强对比对齐来剔除无关的关联，强调视觉事实，通过视觉事实对比对齐匹配区域物体和关系指导的动作。

Result: SANTA 在缓解视觉对象和时序动作幻觉方面优于现有方法，在幻觉检查基准测试中表现更优。

Conclusion: SANTA 框架为缓解 MLLMs 生成视频时的幻觉问题提供了一种有效的解决方案。

Abstract: Recent advancement in multimodal LLMs (MLLMs) has demonstrated their remarkable capability to generate descriptive captions for input videos. However, these models suffer from factual inaccuracies in the generated descriptions, causing severe hallucination issues. While prior works have explored alleviating hallucinations for static images, jointly mitigating visual object and temporal action hallucinations for dynamic videos remains a challenging and unsolved task. To tackle this challenge, we propose a Self-Augmented Contrastive Alignment (SANTA) framework for enabling object and action faithfulness by exempting the spurious correlations and enforcing the emphasis on visual facts. SANTA employs a hallucinative self-augmentation scheme to identify the potential hallucinations that lie in the MLLM and transform the original captions to the contrasted negatives. Furthermore, we develop a tracklet-phrase contrastive alignment to match the regional objects and relation-guided actions with their corresponding visual and temporal phrases. Extensive experiments demonstrate that SANTA outperforms existing methods in alleviating object and action hallucinations, yielding superior performance on the hallucination examination benchmarks.

</details>


### [17] [FMA-Net++: Motion- and Exposure-Aware Real-World Joint Video Super-Resolution and Deblurring](https://arxiv.org/abs/2512.04390)
*Geunhyuk Youk,Jihyong Oh,Munchurl Kim*

Main category: cs.CV

TL;DR: FMA-Net++ 是一种新的框架，能够处理视频中的复杂退化，尤其针对运动和曝光变化的影响，并且在多个基准数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的视频恢复技术忽视了由运动和动态变化的曝光共同造成的复杂退化问题，FMA-Net++旨在解决这一缺陷，主要针对自动曝光或低光拍摄产生的常见伪影。

Method: FMA-Net++ 采用了一种基于层次精炼的双向传播块序列级架构，每个块中包含一个感知曝光时间的调制层，该层根据每帧的曝光信息调整特征，进而驱动运动和曝光感知的流引导动态滤波模块来推断运动和曝光相关的降级核。该框架将降级学习和恢复过程解耦，通过预测曝光和运动相关先验来指导恢复过程。

Result: FMA-Net++ 在新的 REDS-ME 和 REDS-RE 基准数据集以及 GoPro 数据集上实现了最先进的准确性和时间一致性，证明了其在合成数据上的训练效果。

Conclusion: FMA-Net++ 显示出在复杂退化处理方面优于现有方法的效果，在恢复质量和推理速度方面表现出优异的表现，适用于具有挑战性的实际视频。

Abstract: Real-world video restoration is plagued by complex degradations from motion coupled with dynamically varying exposure - a key challenge largely overlooked by prior works and a common artifact of auto-exposure or low-light capture. We present FMA-Net++, a framework for joint video super-resolution and deblurring that explicitly models this coupled effect of motion and dynamically varying exposure. FMA-Net++ adopts a sequence-level architecture built from Hierarchical Refinement with Bidirectional Propagation blocks, enabling parallel, long-range temporal modeling. Within each block, an Exposure Time-aware Modulation layer conditions features on per-frame exposure, which in turn drives an exposure-aware Flow-Guided Dynamic Filtering module to infer motion- and exposure-aware degradation kernels. FMA-Net++ decouples degradation learning from restoration: the former predicts exposure- and motion-aware priors to guide the latter, improving both accuracy and efficiency. To evaluate under realistic capture conditions, we introduce REDS-ME (multi-exposure) and REDS-RE (random-exposure) benchmarks. Trained solely on synthetic data, FMA-Net++ achieves state-of-the-art accuracy and temporal consistency on our new benchmarks and GoPro, outperforming recent methods in both restoration quality and inference speed, and generalizes well to challenging real-world videos.

</details>


### [18] [Fourier-Attentive Representation Learning: A Fourier-Guided Framework for Few-Shot Generalization in Vision-Language Models](https://arxiv.org/abs/2512.04395)
*Hieu Dinh Trung Pham,Huy Minh Nhat Nguyen,Cuong Tuan Nguyen*

Main category: cs.CV

TL;DR: 本文提出了一种新的Fourier-注意表示学习框架FARL，通过傅里叶分析显式分离视觉特征，提升模型的泛化能力，实验结果表明该方法在15个数据集上具有有效性。


<details>
  <summary>Details</summary>
Motivation: 当前的方法通常学习的是包含不变结构和特定风格的综合表示，这限制了模型的泛化能力。为此，本文试图通过FARL框架实现视觉特征的拆分，以增强模型的泛化性能。

Method: FARL使用傅里叶分析分离视觉结构特征和风格特征，通过双重交叉注意力机制分别查询图像的相位谱中的结构性特征和幅度谱中的风格特征。然后将拆分后的特征注入到Vision-Language模型的编码器中，以指导模型的适应过程。

Result: 在15个不同数据集上的实验表明，FARL方法能有效提高模型的泛化能力。

Conclusion: 本文提出的FARL框架通过拆分视觉特征，展示了改进Vision-Language模型泛化能力的潜力。

Abstract: Large-scale pre-trained Vision-Language Models (VLMs) have demonstrated strong few-shot learning capabilities. However, these methods typically learn holistic representations where an image's domain-invariant structure is implicitly entangled with its domain-specific style. This presents an opportunity to further enhance generalization by disentangling these visual cues. In this paper, we propose Fourier-Attentive Representation Learning (FARL), a novel framework that addresses this by explicitly disentangling visual representations using Fourier analysis. The core of our method is a dual cross-attention mechanism, where learnable representation tokens separately query an image's structural features (from the phase spectrum) and stylistic features (from the amplitude spectrum). This process yields enriched, disentangled tokens that are then injected deep into the VLM encoders to guide adaptation. Our design, which includes an asymmetric injection strategy, forces the model to learn a more robust vision-language alignment. Extensive experiments on 15 datasets demonstrate the effectiveness of our approach.

</details>


### [19] [Dual-Stream Spectral Decoupling Distillation for Remote Sensing Object Detection](https://arxiv.org/abs/2512.04413)
*Xiangyi Gao,Danpei Zhao,Bo Yuan,Wentao Li*

Main category: cs.CV

TL;DR: 提出了一个名为DS2D2的自适应特征解耦蒸馏方法，用于解决遥感图像中密集小目标检测的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有蒸馏方法在处理遥感图像时遇到特征混杂的问题，未能充分捕捉到细微的特征差异，导致知识混淆。为解决这些问题，本文提出了一种适用于通用遥感目标检测任务的方法。

Method: DS2D2采用了频域分解来提取关键空间特性，利用一阶小波变换进行频域分解并设计了密度无关尺度权重，此外，通过全频和高频放大器提取隐含在学生-教师特征差异中的知识。

Result: 在DIOR和DOTA数据集上进行了大量的实验验证，表明DS2D2的有效性。特别是在DIOR数据集上，对于RetinaNet和Faster R-CNN，DS2D2分别提高了4.2%和3.8%的AP50指标。

Conclusion: DS2D2显著提升了密集和小目标检测的性能，对于远程感应光学目标检测具有重要意义，是现有蒸馏方法的有效补充。

Abstract: Knowledge distillation is an effective and hardware-friendly method, which plays a key role in lightweighting remote sensing object detection. However, existing distillation methods often encounter the issue of mixed features in remote sensing images (RSIs), and neglect the discrepancies caused by subtle feature variations, leading to entangled knowledge confusion. To address these challenges, we propose an architecture-agnostic distillation method named Dual-Stream Spectral Decoupling Distillation (DS2D2) for universal remote sensing object detection tasks. Specifically, DS2D2 integrates explicit and implicit distillation grounded in spectral decomposition. Firstly, the first-order wavelet transform is applied for spectral decomposition to preserve the critical spatial characteristics of RSIs. Leveraging this spatial preservation, a Density-Independent Scale Weight (DISW) is designed to address the challenges of dense and small object detection common in RSIs. Secondly, we show implicit knowledge hidden in subtle student-teacher feature discrepancies, which significantly influence predictions when activated by detection heads. This implicit knowledge is extracted via full-frequency and high-frequency amplifiers, which map feature differences to prediction deviations. Extensive experiments on DIOR and DOTA datasets validate the effectiveness of the proposed method. Specifically, on DIOR dataset, DS2D2 achieves improvements of 4.2% in AP50 for RetinaNet and 3.8% in AP50 for Faster R-CNN, outperforming existing distillation approaches. The source code will be available at https://github.com/PolarAid/DS2D2.

</details>


### [20] [Explainable Parkinsons Disease Gait Recognition Using Multimodal RGB-D Fusion and Large Language Models](https://arxiv.org/abs/2512.04425)
*Manar Alnaasan,Md Selim Sarowar,Sungho Kim*

Main category: cs.CV

TL;DR: 该研究提出了一种基于RGB和深度数据融合的可解释多模态框架，通过YOLOv11和MLGE模块提升时空特征表示能力，结合LLM进行视觉和结构化元数据的解释，实现了对帕金森病步态的高识别准确性和环境变化鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖单一模态输入，鲁棒性较差，缺乏临床透明度，因此需要提出一种能融合RGB和深度数据的可解释多模态框架，以提高帕金森病步态的识别精度和解释能力。

Method: 研究采用了YOLOv11进行模态特定特征提取，通过MLGE模块和跨空间颈部融合机制增强时空表现力，结合LLM将视觉嵌入和结构化元数据转化为临床有意义的文本解释。

Result: 实验证明，该RGB-D融合框架在识别精度上优于单一输入基线，具有更好的环境变化鲁棒性，并能清晰地进行视觉和语言推理。

Conclusion: 本研究填补了视觉识别与临床理解之间的差距，为帕金森病步态分析提供了一种可靠的、可解释的新视角，结合多模态特征学习和语言解释能力。

Abstract: Accurate and interpretable gait analysis plays a crucial role in the early detection of Parkinsons disease (PD),yet most existing approaches remain limited by single-modality inputs, low robustness, and a lack of clinical transparency. This paper presents an explainable multimodal framework that integrates RGB and Depth (RGB-D) data to recognize Parkinsonian gait patterns under realistic conditions. The proposed system employs dual YOLOv11-based encoders for modality-specific feature extraction, followed by a Multi-Scale Local-Global Extraction (MLGE) module and a Cross-Spatial Neck Fusion mechanism to enhance spatial-temporal representation. This design captures both fine-grained limb motion (e.g., reduced arm swing) and overall gait dynamics (e.g., short stride or turning difficulty), even in challenging scenarios such as low lighting or occlusion caused by clothing. To ensure interpretability, a frozen Large Language Model (LLM) is incorporated to translate fused visual embeddings and structured metadata into clinically meaningful textual explanations. Experimental evaluations on multimodal gait datasets demonstrate that the proposed RGB-D fusion framework achieves higher recognition accuracy, improved robustness to environmental variations, and clear visual-linguistic reasoning compared with single-input baselines. By combining multimodal feature learning with language-based interpretability, this study bridges the gap between visual recognition and clinical understanding, offering a novel vision-language paradigm for reliable and explainable Parkinsons disease gait analysis. Code:https://github.com/manaralnaasan/RGB-D_parkinson-LLM

</details>


### [21] [MindDrive: An All-in-One Framework Bridging World Models and Vision-Language Model for End-to-End Autonomous Driving](https://arxiv.org/abs/2512.04441)
*Bin Suna,Yaoguang Caob,Yan Wanga,Rui Wanga,Jiachen Shanga,Xiejie Fenga,Jiayi Lu,Jia Shi,Shichun Yang,Xiaoyu Yane,Ziying Song*

Main category: cs.CV

TL;DR: 研究提出了一种名为MindDrive的框架，融合了高质量的轨迹生成和全面的决策推理能力，构建了一个结构化的推理框架，通过未来感知轨迹生成器（FaTG）和视觉语言模型导向的评估器（VLoE）来预测未来场景并进行多目标评估。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要集中在两种方向上：轨迹生成导向，专注于生成高质量轨迹但决策机制简单；轨迹选择导向，进行多维度评估但缺乏生成能力。为解决这些问题，该研究提出了一种名为MindDrive的新框架。

Method: MindDrive框架整合了未来感知轨迹生成器（FaTG）和视觉语言模型导向的评估器（VLoE）。FaTG基于世界行为模型（WaM）进行情景模拟和轨迹生成，VLoE则运用大型视觉语言模型进行多目标评估。

Result: 在NAVSIM-v1和NAVSIM-v2基准测试上的实验证明，MindDrive在多个驾驶度量指标上达到了业界领先的表现，显著改善了安全性和适用性。

Conclusion: 该研究为可解释和认知引导的自动驾驶提供了一条有希望的道路。

Abstract: End-to-End autonomous driving (E2E-AD) has emerged as a new paradigm, where trajectory planning plays a crucial role. Existing studies mainly follow two directions: trajectory generation oriented, which focuses on producing high-quality trajectories with simple decision mechanisms, and trajectory selection oriented, which performs multi-dimensional evaluation to select the best trajectory yet lacks sufficient generative capability. In this work, we propose MindDrive, a harmonized framework that integrates high-quality trajectory generation with comprehensive decision reasoning. It establishes a structured reasoning paradigm of "context simulation - candidate generation - multi-objective trade-off". In particular, the proposed Future-aware Trajectory Generator (FaTG), based on a World Action Model (WaM), performs ego-conditioned "what-if" simulations to predict potential future scenes and generate foresighted trajectory candidates. Building upon this, the VLM-oriented Evaluator (VLoE) leverages the reasoning capability of a large vision-language model to conduct multi-objective evaluations across safety, comfort, and efficiency dimensions, leading to reasoned and human-aligned decision making. Extensive experiments on the NAVSIM-v1 and NAVSIM-v2 benchmarks demonstrate that MindDrive achieves state-of-the-art performance across multi-dimensional driving metrics, significantly enhancing safety, compliance, and generalization. This work provides a promising path toward interpretable and cognitively guided autonomous driving.

</details>


### [22] [StreamEQA: Towards Streaming Video Understanding for Embodied Scenarios](https://arxiv.org/abs/2512.04451)
*Yifei Wang,Zhenkai Li,Tianwen Qian,Huanran Zheng,Zheng Wang,Yuqian Fu,Xiaoling Wang*

Main category: cs.CV

TL;DR: StreamEQA 是首个用于体感场景下流式视频问答的基准测试，它通过评估模型在感知、交互和规划三个级别的能力，以及回溯、实时和前瞻推理三个时间维度上的表现，识别现有的视频理解模型在采用流式视频进行体感应用时的不足，并期望推动该领域的研究。


<details>
  <summary>Details</summary>
Motivation: 随着嵌入式智能迈向实际部署，连续感知和推理流式视觉输入变得至关重要。现有的模型在体感应用中的流式视频理解能力不足。

Method: StreamEQA 构建在 156 个独立的长视频上，定义了 42 个任务并生成了约 21,000 个带精确时间戳的问题-答案对，通过结合自动生成和人工校正的混合流程实现。

Result: 对 13 个最先进的视频-LLM 进行的评估显示，尽管这些模型在传统基准测试中表现出色，但在体感场景中的流式视频理解能力仍然存在不足。

Conclusion: StreamEQA 旨在促进在体感应用场景中的流式视频理解研究，希望通过提供新的评估视角来推动这一领域的进步。

Abstract: As embodied intelligence advances toward real-world deployment, the ability to continuously perceive and reason over streaming visual inputs becomes essential. In such settings, an agent must maintain situational awareness of its environment, comprehend the interactions with surrounding entities, and dynamically plan actions informed by past observations, current contexts, and anticipated future events. To facilitate progress in this direction, we introduce StreamEQA, the first benchmark designed for streaming video question answering in embodied scenarios. StreamEQA evaluates existing MLLMs along two orthogonal dimensions: Embodied and Streaming. Along the embodied dimension, we categorize the questions into three levels: perception, interaction, and planning, which progressively assess a model's ability to recognize fine-grained visual details, reason about agent-object interactions, and perform high-level goal-directed reasoning. For the streaming dimension, questions are divided into backward, real-time, and forward reasoning, with each mode relying on a distinct temporal context. Built upon 156 independent long videos, StreamEQA defines 42 tasks and generates approximately 21K question-answer pairs with precise timestamps through a hybrid pipeline combining automated generation and human refinement. Evaluations of 13 state-of-the-art video-LLMs reveal that, despite strong performance on conventional benchmarks, these models still struggle with streaming video understanding in embodied scenarios. We hope StreamEQA will catalyze research on streaming video understanding for embodied applications.

</details>


### [23] [GuidNoise: Single-Pair Guided Diffusion for Generalized Noise Synthesis](https://arxiv.org/abs/2512.04456)
*Changjin Kim,HyeokJun Lee,YoungJoon Yoo*

Main category: cs.CV

TL;DR: 论文提出了一种使用单组噪声/清晰图像对的单对引导扩散方法，以指导噪声生成，这种方法无需相机元数据即可在不同噪声环境下生成高质量的噪声图像，此外，它还能够在推理阶段生成噪声/清晰图像对，增强了训练数据，并提高了轻量级模型在有限训练数据情况下的去噪性能。


<details>
  <summary>Details</summary>
Motivation: 现有的图像去噪方法依赖于昂贵的获取真实噪声数据，且大多数生成模型需要相机元数据和目标特定的噪声/清晰图像对，表现出在不同设置中的有限泛化能力。

Method: 该论文提出了一种名为GuidNoise的方法，该方法使用单组噪声/清晰图像对作为指导，在训练过程中引入了指导意识的仿射特征修改（GAFM）和噪声感知细化损失，以利用扩散模型的内在潜力。这种方法允许在不同噪声环境下生成高质量的噪声图像，同时在推理过程中也能自动生成噪声/清晰图像对。

Result: GuidNoise能够在无需额外元数据的情况下，在不同的噪声环境中生成高质量的噪声图像。此外，这种方法还使噪声/清晰图像对的合成在推理阶段变得更为高效，为训练数据扩充提供了方便，特别是对于轻量级模型在有限训练数据 scenario 下的去噪性能有显著改进。

Conclusion: GuidNoise 方法通过减少对外部数据需求和提高模型的泛化能力，在实际应用场景中展示了优秀的性能，对于提高图像去噪任务中的轻量级模型性能具有重要意义。

Abstract: Recent image denoising methods have leveraged generative modeling for real noise synthesis to address the costly acquisition of real-world noisy data. However, these generative models typically require camera metadata and extensive target-specific noisy-clean image pairs, often showing limited generalization between settings. In this paper, to mitigate the prerequisites, we propose a Single-Pair Guided Diffusion for generalized noise synthesis GuidNoise, which uses a single noisy/clean pair as the guidance, often easily obtained by itself within a training set. To train GuidNoise, which generates synthetic noisy images from the guidance, we introduce a guidance-aware affine feature modification (GAFM) and a noise-aware refine loss to leverage the inherent potential of diffusion models. This loss function refines the diffusion model's backward process, making the model more adept at generating realistic noise distributions. The GuidNoise synthesizes high-quality noisy images under diverse noise environments without additional metadata during both training and inference. Additionally, GuidNoise enables the efficient generation of noisy-clean image pairs at inference time, making synthetic noise readily applicable for augmenting training data. This self-augmentation significantly improves denoising performance, especially in practical scenarios with lightweight models and limited training data. The code is available at https://github.com/chjinny/GuidNoise.

</details>


### [24] [dVLM-AD: Enhance Diffusion Vision-Language-Model for Driving via Controllable Reasoning](https://arxiv.org/abs/2512.04459)
*Yingzi Ma,Yulong Cao,Wenhao Ding,Shuibai Zhang,Yan Wang,Boris Ivanovic,Ming Jiang,Marco Pavone,Chaowei Xiao*

Main category: cs.CV

TL;DR: 本文介绍了一种基于扩散模型的视觉语言模型dVLM-AD，该模型结合了感知、结构化推理和低级规划，以实现端到端的自动驾驶，其在nuScenes和WOD-E2E数据集上的表现优于基于自回归的基线模型。


<details>
  <summary>Details</summary>
Motivation: 现有的基于自回归模型的视觉语言模型在处理复杂的出行场景时，缺乏上下文理解和推理的一致性，而基于离散扩散模型的视觉语言模型通过双向注意力机制和迭代去噪方法，能够更可靠地执行端到端的自动驾驶任务。

Method: dVLM-AD模型通过结合扩散模型的优势，不仅能够处理复杂的上下文信息，还能够保证在低级任务执行过程中的可控制性和可靠性。具体而言，该模型在感知、结构性推理和低级规划方面进行了统一设计，以实现更高效的端到端自动驾驶。

Result: dVLM-AD模型在nuScenes和WOD-E2E数据集上的表现优于基于自回归模型的基线模型。特别是在长尾场景下，dVLM-AD的轨迹一致性提高了9%，RFS指标提高了6%。

Conclusion: 基于扩散模型的dVLM-AD为实现可控制和可靠的端到端自动驾驶提供了一条途径。未来的研究可以进一步探索如何在更复杂的场景下优化模型性能。

Abstract: The autonomous driving community is increasingly focused on addressing the challenges posed by out-of-distribution (OOD) driving scenarios. A dominant research trend seeks to enhance end-to-end (E2E) driving systems by integrating vision-language models (VLMs), leveraging their rich world knowledge and reasoning abilities to improve generalization across diverse environments. However, most existing VLMs or vision-language agents (VLAs) are built upon autoregressive (AR) models. In this paper, we observe that existing AR-based VLMs -- limited by causal attention and sequential token generation -- often fail to maintain consistency and controllability between high-level reasoning and low-level planning. In contrast, recent discrete diffusion VLMs equipped with bidirectional attention exhibit superior controllability and reliability through iterative denoising. Building on these observations, we introduce dVLM-AD, a diffusion-based vision-language model that unifies perception, structured reasoning, and low-level planning for end-to-end driving. Evaluated on nuScenes and WOD-E2E, dVLM-AD yields more consistent reasoning-action pairs and achieves planning performance comparable to existing driving VLM/VLA systems despite a modest backbone, outperforming AR-based baselines with a 9 percent improvement in behavior-trajectory consistency and a 6 percent increase in RFS on long-tail WOD-E2E scenarios. These results suggest a controllable and reliable pathway for scalable end-to-end driving.

</details>


### [25] [SEASON: Mitigating Temporal Hallucination in Video Large Language Models via Self-Diagnostic Contrastive Decoding](https://arxiv.org/abs/2512.04643)
*Chang-Hsun Wu,Kai-Po Chang,Yu-Yang Sheng,Hung-Kai Chung,Kuei-Chun Wang,Yu-Chiang Frank Wang*

Main category: cs.CV

TL;DR: SEASON是一种无需训练的方法，通过动态诊断每个输出token的幻觉倾向并应用自适应对比解码，增强每个token的时空一致性，从而提高视频理解过程中事件描述的时空连贯性和因果合理性。


<details>
  <summary>Details</summary>
Motivation: 当前 VideoLLMs 在处理视频中的时间信息时仍存在显著的幻觉问题，尤其是在事件描述的一致性和因果关系方面。尽管已有研究主要关注空间幻觉（如物体匹配错误），但时序推理在视频理解方面的潜力尚未得到充分探索。

Method: SEASON 通过动态诊断每个 token 的幻觉倾向，并基于对应的时间和空间负例应用自适应对比解码，以增强时空一致性。

Result: 在三个幻觉检测基准测试中，SEASON 出色地超越了所有现有的训练免费幻觉缓解方法，并进一步提升了四种通用视频理解基准测试的结果。

Conclusion: SEASON 提供了一种无需训练的方法来改善 VideoLLMs 的时空一致性，解决了幻觉问题。

Abstract: Video Large Language Models (VideoLLMs) have shown remarkable progress in video understanding. However, these models still struggle to effectively perceive and exploit rich temporal information in videos when responding to user queries. Therefore, they often generate descriptions of events that are temporal inconsistent or causally implausible, causing severe hallucination issues. While most prior studies have focused on spatial hallucinations (e.g. object mismatches), temporal reasoning in video understanding remains relatively underexplored. To address this issue, we propose Self-Diagnostic Contrastive Decoding (SEASON), a training-free method that adaptively enhances temporal and spatial faithfulness for each output token. It achieves this by dynamically diagnosing each token's hallucination tendency and applying adaptive contrastive decoding against its corresponding temporal and spatial negatives. Extensive experiments demonstrate that SEASON outperforms all existing training-free hallucination mitigation approaches on three hallucination examination benchmarks, while further improves VideoLLMs across four general video understanding benchmarks. The code will be released upon acceptance.

</details>


### [26] [UniTS: Unified Time Series Generative Model for Remote Sensing](https://arxiv.org/abs/2512.04461)
*Yuxiang Zhang,Shunlin Liang,Wenyuan Li,Han Ma,Jianglei Xu,Yichuan Ma,Jiangwei Xie,Wei Li,Mengmeng Zhang,Ran Tao,Xiang-Gen Xia*

Main category: cs.CV

TL;DR: UniTS 是一种统一的时间序列生成模型，能够处理多种时间序列任务，如时间序列重建、去云、语义变化检测和预测未来地面演变。它通过流匹配生成范式，结合空间-时间感知模块等设计，提供统一的空间-时间表征。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常需要针对特定任务进行定制，这导致了缺乏跨时间和空间任务的统一建模。UniTS 解决了这一问题，旨在提供一个通用框架来全面覆盖这些任务，解决具有挑战性的时间序列问题，如严重的云污染、模态缺失和预测植物学变化。

Method: UniTS 采用了流匹配生成方法，其架构包括一个带有空间-时间块的扩散变换器，还设计了自适应条件注入器 (ACor) 以增强模态输入的多条件感知，并改进了空间-时间块以更好地捕捉复杂的空间-时间依赖性。

Result: 实验证明，UniTS 在低级和高级时间序列任务中展现了出色的生成和认知能力，特别是在面对严重云污染、模态缺失和预测植物学变化等挑战时优于现有方法。

Conclusion: UniTS 提供了一种有效的解决方案，能够在各种时空任务中统一建模时空特征，这将有助于解决目前时间序列相关领域的挑战，并推动相关领域的进一步发展。

Abstract: One of the primary objectives of satellite remote sensing is to capture the complex dynamics of the Earth environment, which encompasses tasks such as reconstructing continuous cloud-free time series images, detecting land cover changes, and forecasting future surface evolution. However, existing methods typically require specialized models tailored to different tasks, lacking unified modeling of spatiotemporal features across multiple time series tasks. In this paper, we propose a Unified Time Series Generative Model (UniTS), a general framework applicable to various time series tasks, including time series reconstruction, time series cloud removal, time series semantic change detection, and time series forecasting. Based on the flow matching generative paradigm, UniTS constructs a deterministic evolution path from noise to targets under the guidance of task-specific conditions, achieving unified modeling of spatiotemporal representations for multiple tasks. The UniTS architecture consists of a diffusion transformer with spatio-temporal blocks, where we design an Adaptive Condition Injector (ACor) to enhance the model's conditional perception of multimodal inputs, enabling high-quality controllable generation. Additionally, we design a Spatiotemporal-aware Modulator (STM) to improve the ability of spatio-temporal blocks to capture complex spatiotemporal dependencies. Furthermore, we construct two high-quality multimodal time series datasets, TS-S12 and TS-S12CR, filling the gap of benchmark datasets for time series cloud removal and forecasting tasks. Extensive experiments demonstrate that UniTS exhibits exceptional generative and cognitive capabilities in both low-level and high-level time series tasks. It significantly outperforms existing methods, particularly when facing challenges such as severe cloud contamination, modality absence, and forecasting phenological variations.

</details>


### [27] [DeRA: Decoupled Representation Alignment for Video Tokenization](https://arxiv.org/abs/2512.04483)
*Pengbo Guo,Junke Wang,Zhen Xing,Chengxu Liu,Daoguo Dong,Xueming Qian,Zuxuan Wu*

Main category: cs.CV

TL;DR: DeRA是一种新颖的一维视频分词器，通过分离时空表征学习，实现了更好的训练效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频分词方法可能在效率和性能上存在局限，DeRA的提出旨在解决这些问题。

Method: DeRA通过保持紧凑的一维潜在空间，并将视频编码分解为外观和运动流，分别使用预训练的视觉基础模型捕捉空间语义和时间动态。同时，它引入了Symmetric Alignment-Conflict Projection (SACP) 模块来解决异质监督引入的梯度冲突。

Result: 实验结果表明，DeRA在UCF-101数据集上的rFVD性能比之前的SOTA方法LARP提高25%。此外，使用DeRA进行自回归视频生成时，我们在UCF-101类别条件生成和K600帧预测方面也获得了新的SOTA结果。

Conclusion: DeRA提升了一维视频分词的效率和性能，并在多个评估指标上取得了突破性进展。

Abstract: This paper presents DeRA, a novel 1D video tokenizer that decouples the spatial-temporal representation learning in video tokenization to achieve better training efficiency and performance. Specifically, DeRA maintains a compact 1D latent space while factorizing video encoding into appearance and motion streams, which are aligned with pretrained vision foundation models to capture the spatial semantics and temporal dynamics in videos separately. To address the gradient conflicts introduced by the heterogeneous supervision, we further propose the Symmetric Alignment-Conflict Projection (SACP) module that proactively reformulates gradients by suppressing the components along conflicting directions. Extensive experiments demonstrate that DeRA outperforms LARP, the previous state-of-the-art video tokenizer by 25% on UCF-101 in terms of rFVD. Moreover, using DeRA for autoregressive video generation, we also achieve new state-of-the-art results on both UCF-101 class-conditional generation and K600 frame prediction.

</details>


### [28] [Controllable Long-term Motion Generation with Extended Joint Targets](https://arxiv.org/abs/2512.04487)
*Eunjong Lee,Eunhee Kim,Sanghoon Hong,Eunho Jung,Jihoon Kim*

Main category: cs.CV

TL;DR: COMET 是一个基于自回归框架的实时高效Transformer条件VAE，通过引入参考引导反馈机制确保长时间稳定性，实现了精细可控的角色动画生成，在复杂运动控制任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的角色动画生成方法要么缺乏细粒度的控制，要么在长时间序列中出现运动退化，限制了它们在交互式应用中的使用。

Method: COMET 使用一个高效的基于Transformer的条件VAE和参考引导反馈机制。这种机制允许在单一模型中精细且交互地控制任意用户指定的关节，同时确保长时间的时空稳定性。

Result: 实验评估显示，COMET 能以实时速度生成高质量的运动，相比现有的最先进的方法在复杂的运动控制任务中表现更优。

Conclusion: COMET 为复杂的交互式应用提供了可靠和高效的实时角色动画生成解决方案，推测其在未来可能会有广泛的应用。

Abstract: Generating stable and controllable character motion in real-time is a key challenge in computer animation. Existing methods often fail to provide fine-grained control or suffer from motion degradation over long sequences, limiting their use in interactive applications. We propose COMET, an autoregressive framework that runs in real time, enabling versatile character control and robust long-horizon synthesis. Our efficient Transformer-based conditional VAE allows for precise, interactive control over arbitrary user-specified joints for tasks like goal-reaching and in-betweening from a single model. To ensure long-term temporal stability, we introduce a novel reference-guided feedback mechanism that prevents error accumulation. This mechanism also serves as a plug-and-play stylization module, enabling real-time style transfer. Extensive evaluations demonstrate that COMET robustly generates high-quality motion at real-time speeds, significantly outperforming state-of-the-art approaches in complex motion control tasks and confirming its readiness for demanding interactive applications.

</details>


### [29] [Shift-Window Meets Dual Attention: A Multi-Model Architecture for Specular Highlight Removal](https://arxiv.org/abs/2512.04496)
*Tianci Huo,Lingfeng Qi,Yuhan Chen,Qihong Xue,Jinyuan Shao,Hai Yu,Jie Li,Zhanhua Zhang,Guofa Li*

Main category: cs.CV

TL;DR: 本文提出了一种多模型架构（MM-SHR），用于消除所有尺度下的镜面高光。它结合了局部和全局信息，通过Attention机制和特定设计的模块提高了高光区域的细节捕捉和长依赖关系建模能力。


<details>
  <summary>Details</summary>
Motivation: 实际环境中不可避免的镜面高光会严重损害视觉表现，影响任务效率。现有方法大多针对局部信息或全局信息，单一模型难以兼顾细部和长范围依赖关系，导致去除不同规模高光的效果不佳。

Method: 本文提出了一种多模型架构（MM-SHR），在浅层通过卷积操作提取局部细节，在深层通过Attention机制收集全局特征。还引入了Omni-Directional Attention Integration Block 和 Adaptive Region-Aware Hybrid-Domain Dual Attention Convolutional Network，实现了全方向的像素位移和窗口划分，为高光去除提供高效的长依赖关系建模。

Result: 在三个基准任务和六种表面材质上的实验表明，MM-SHR在高光去除的准确性和效率上均优于现有方法。

Conclusion: 本文提出的MM-SHR架构为高光去除提供了一种有效的解决方案，能够在多种情况下实现高精度和高性能的高光识别与去除。

Abstract: Inevitable specular highlights in practical environments severely impair the visual performance, thus degrading the task effectiveness and efficiency. Although there exist considerable methods that focus on local information from convolutional neural network models or global information from transformer models, the single-type model falls into a modeling dilemma between local fine-grained details and global long-range dependencies, thus deteriorating for specular highlights with different scales. Therefore, to accommodate specular highlights of all scales, we propose a multi-model architecture for specular highlight removal (MM-SHR) that effectively captures fine-grained features in highlight regions and models long-range dependencies between highlight and highlight-free areas. Specifically, we employ convolution operations to extract local details in the shallow layers of MM-SHR, and utilize the attention mechanism to capture global features in the deep layers, ensuring both operation efficiency and removal accuracy. To model long-range dependencies without compromising computational complexity, we utilize a coarse-to-fine manner and propose Omni-Directional Attention Integration Block(OAIBlock) and Adaptive Region-Aware Hybrid-Domain Dual Attention Convolutional Network(HDDAConv) , which leverage omni-directiona pixel-shifting and window-dividing operations at the raw features to achieve specular highlight removal. Extensive experimental results on three benchmark tasks and six types of surface materials demonstrate that MM-SHR outperforms state-of-the-art methods in both accuracy and efficiency for specular highlight removal. The implementation will be made publicly available at https://github.com/Htcicv/MM-SHR.

</details>


### [30] [DraCo: Draft as CoT for Text-to-Image Preview and Rare Concept Generation](https://arxiv.org/abs/2512.05112)
*Dongzhi Jiang,Renrui Zhang,Haodong Li,Zhuofan Zong,Ziyu Guo,Jun He,Claire Guo,Junyan Ye,Rongyao Fang,Weijia Li,Rui Liu,Hongsheng Li*

Main category: cs.CV

TL;DR: 本文提出了DraCo，一种新颖的交替推理范式，旨在更好地利用文本和视觉内容的结合，提高规划和验证，并提出了DraCo-240K数据集和DraCo-CFG策略来支持该方法的训练。DraCo在多个基准测试上表现出色，显著超越直接生成和其他依赖于CoT的生成方法。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态语言模型在文本到图像生成中主要依赖抽象文本规划或仅作为独立生成器，而在规划和验证过程中存在局限性。DraCo旨在通过交替推理方式结合文本和视觉内容，解决粗粒度文本规划和生成稀有属性组合困难的问题。

Method: DraCo方法首先生成低分辨率草图图像，提供更具体和结构化的视觉规划和指导。然后利用模型的内在理解能力验证草图与输入提示之间的潜在语义对齐问题，并通过选择性修正进行超分辨率细化。为了支持训练，还开发了DraCo-240K数据集，以增强一般修正、实例操作和布局重组这三种基本能力。通过DraCo-CFG策略，改进了交错推理过程中的无分类器引导。

Result: DraCo在GenEval (+8%)、Imagine-Bench (+0.91) 和 GenEval++ (+3%) 等多个基准测试中取得显著提升。相较于直接生成及其他依赖于CoT的生成方法，DraCo表现出了显著的优越性。

Conclusion: 本研究通过提出DraCo，展示了交替推理范式在多模态生成任务中的有效性，验证了这种方法可以更好地解决现有模型的局限性，并在多项指标上实现了性能突破。

Abstract: Recent unified multimodal large language models (MLLMs) have shown impressive capabilities, incorporating chain-of-thought (CoT) reasoning for enhanced text-to-image generation. However, existing approaches remain limited, either treating the model merely as a standalone generator or relying on abstract textual planning. To this end, we propose Draft-as-CoT (DraCo), a novel interleaved reasoning paradigm that fully leverages both textual and visual contents in CoT for better planning and verification. Our method first generates a low-resolution draft image as preview, providing more concrete and structural visual planning and guidance. Then, we employ the model's inherent understanding capability to verify potential semantic misalignments between the draft and input prompt, and performs refinement through selective corrections with super-resolution. In this way, our approach addresses two fundamental challenges: the coarse-grained nature of textual planning and the difficulty in generating rare attribute combinations. To support training, we curate DraCo-240K, aiming to enhance three atomic capabilities spanning general correction, instance manipulation, and layout reorganization. Supported by DraCo-CFG, a specialized classifier-free guidance (CFG) strategy for interleaved reasoning, DraCo achieves a tremendous increase on GenEval (+8%), Imagine-Bench (+0.91), and GenEval++ (+3%), significantly outperforming direct generation and other generation methods empowered by CoT.

</details>


### [31] [UltraImage: Rethinking Resolution Extrapolation in Image Diffusion Transformers](https://arxiv.org/abs/2512.04504)
*Min Zhao,Bokai Yan,Xue Yang,Hongzhou Zhu,Jintao Zhang,Shilong Liu,Chongxuan Li,Jun Zhu*

Main category: cs.CV

TL;DR: UltraImage 提出了一种解决大尺度图像生成中内容重复和质量下降问题的框架，通过频率分析和修正主导频率周期，以及引入自适应注意力机制提高图像的视觉保真度。


<details>
  <summary>Details</summary>
Motivation: 现有的图像扩散变压器在高保真度图像生成上表现良好，但难以生成更大尺度的高质量图像。UltraImage 提出了解决这一瓶颈的方法。

Method: UltraImage 通过分析位置嵌入的频率特征，识别出重复的原因在于主导频率的周期性，并引入了一种递归的主导频率修正方法以限制其在单个周期内。此外，为了解决质量下降的问题，UltraImage 提出了一种基于熵的自适应注意力集中机制，以提高局部关注并保持结构一致性。

Result: 实验表明，UltraImage 在 Qwen-Image 和 Flux（约 4K）数据集上的三个生成场景中始终优于先前的方法，减少了重复并提高了视觉保真度。此外，UltraImage 能够从训练分辨率为 1328p 的图像生成高达 6K*6K 的图像，展示了其极强的外推能力。

Conclusion: 总之，UltraImage 提出的框架显著提高了大尺度图像生成的质量，并展示了其在外推能力上的优势。

Abstract: Recent image diffusion transformers achieve high-fidelity generation, but struggle to generate images beyond these scales, suffering from content repetition and quality degradation. In this work, we present UltraImage, a principled framework that addresses both issues. Through frequency-wise analysis of positional embeddings, we identify that repetition arises from the periodicity of the dominant frequency, whose period aligns with the training resolution. We introduce a recursive dominant frequency correction to constrain it within a single period after extrapolation. Furthermore, we find that quality degradation stems from diluted attention and thus propose entropy-guided adaptive attention concentration, which assigns higher focus factors to sharpen local attention for fine detail and lower ones to global attention patterns to preserve structural consistency. Experiments show that UltraImage consistently outperforms prior methods on Qwen-Image and Flux (around 4K) across three generation scenarios, reducing repetition and improving visual fidelity. Moreover, UltraImage can generate images up to 6K*6K without low-resolution guidance from a training resolution of 1328p, demonstrating its extreme extrapolation capability. Project page is available at \href{https://thu-ml.github.io/ultraimage.github.io/}{https://thu-ml.github.io/ultraimage.github.io/}.

</details>


### [32] [DuGI-MAE: Improving Infrared Mask Autoencoders via Dual-Domain Guidance](https://arxiv.org/abs/2512.04511)
*Yinghui Xing,Xiaoting Su,Shizhou Zhang,Donghao Chu,Di Xu*

Main category: cs.CV

TL;DR: 提出了一种名为DuGI-MAE的双域引导红外基础模型，通过确定性掩码策略保留高熵标记并引入双域导向模块来改进全局关系捕捉和背景噪声过滤，从而优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的基础模型如Masked Autoencoder（MAE）在处理红外图像时表现不佳，因为它们未专门针对红外图像进行训练。为了改进这一点，该研究开发了DuGI-MAE模型。

Method: DuGI-MAE采用了确定性掩码策略，通过保留高熵标记来增强信息量，并引入了双域导向模块，该模块能够同时捕捉全局标记关系并对红外图像中常见的非均匀背景噪声进行自适应过滤。

Result: DuGI-MAE在多种下游任务中表现出强大的泛化能力，包括红外物体检测、语义分割和小型目标检测。实验结果证明该方法优于监督和自监督的对比方法。

Conclusion: 研究通过DuGI-MAE展示了在红外图像处理中的显著改进，并验证了其优越性。

Abstract: Infrared imaging plays a critical role in low-light and adverse weather conditions. However, due to the distinct characteristics of infrared images, existing foundation models such as Masked Autoencoder (MAE) trained on visible data perform suboptimal in infrared image interpretation tasks. To bridge this gap, an infrared foundation model known as InfMAE was developed and pre-trained on large-scale infrared datasets. Despite its effectiveness, InfMAE still faces several limitations, including the omission of informative tokens, insufficient modeling of global associations, and neglect of non-uniform noise. In this paper, we propose a Dual-domain Guided Infrared foundation model based on MAE (DuGI-MAE). First, we design a deterministic masking strategy based on token entropy, preserving only high-entropy tokens for reconstruction to enhance informativeness. Next, we introduce a Dual-Domain Guidance (DDG) module, which simultaneously captures global token relationships and adaptively filters non-uniform background noise commonly present in infrared imagery. To facilitate large-scale pretraining, we construct Inf-590K, a comprehensive infrared image dataset encompassing diverse scenes, various target types, and multiple spatial resolutions. Pretrained on Inf-590K, DuGI-MAE demonstrates strong generalization capabilities across various downstream tasks, including infrared object detection, semantic segmentation, and small target detection. Experimental results validate the superiority of the proposed method over both supervised and self-supervised comparison methods. Our code is available in the supplementary material.

</details>


### [33] [Physics-Informed Deformable Gaussian Splatting: Towards Unified Constitutive Laws for Time-Evolving Material Field](https://arxiv.org/abs/2511.06299)
*Haoqin Hong,Ding Fan,Fubin Dou,Zhi-Li Zhou,Haoran Sun,Congcong Zhu,Jingrun Chen*

Main category: cs.CV

TL;DR: 本文提出了一种新的物理约束下的可变形高斯洒点法PIDG，通过将每个高斯粒子视为具有时变本构参数的拉格朗日质点，结合时空编码高效重建几何和运动，并通过匹配拉格朗日粒子流动和摄像机补偿的光学流动来加速收敛和提升泛化能力，最终在物理一致性和单目动态重建质量方面取得显著提高。


<details>
  <summary>Details</summary>
Motivation: 现有的基于纯数据驱动的3D高斯洒点技术在捕捉动态场景中复杂的物理驱动运动模式方面存在局限性，因此需要一种能够克服这些局限的技术。

Method: PIDG方法将每个高斯粒子视作具有时变本构参数的拉格朗日质点，结合静态动态解耦的四维分解哈希编码高效重建几何和运动，通过Cauchy动量残差作为物理约束来预测每个粒子的独立运动和本构应力，最终通过匹配拉格朗日粒子流动和摄像机补偿的光学流动来加速收敛和提高泛化能力。

Result: 在自定义的物理驱动数据集，以及标准的合成和现实世界数据集上进行的实验表明，该方法在物理一致性和单目动态重建质量方面取得了显著的改进。

Conclusion: PIDG方法有望提供更接近真实结果的动态图像合成，特别是在现实环境中，使其成为现有技术的一个有价值的补充。

Abstract: Recently, 3D Gaussian Splatting (3DGS), an explicit scene representation technique, has shown significant promise for dynamic novel-view synthesis from monocular video input. However, purely data-driven 3DGS often struggles to capture the diverse physics-driven motion patterns in dynamic scenes. To fill this gap, we propose Physics-Informed Deformable Gaussian Splatting (PIDG), which treats each Gaussian particle as a Lagrangian material point with time-varying constitutive parameters and is supervised by 2D optical flow via motion projection. Specifically, we adopt static-dynamic decoupled 4D decomposed hash encoding to reconstruct geometry and motion efficiently. Subsequently, we impose the Cauchy momentum residual as a physics constraint, enabling independent prediction of each particle's velocity and constitutive stress via a time-evolving material field. Finally, we further supervise data fitting by matching Lagrangian particle flow to camera-compensated optical flow, which accelerates convergence and improves generalization. Experiments on a custom physics-driven dataset as well as on standard synthetic and real-world datasets demonstrate significant gains in physical consistency and monocular dynamic reconstruction quality.

</details>


### [34] [EgoLCD: Egocentric Video Generation with Long Context Diffusion](https://arxiv.org/abs/2512.04515)
*Liuzhou Zhang,Jiarui Ye,Yuanlei Wang,Ming Zhong,Mingju Cao,Wanke Xia,Bowen Zeng,Zeyu Zhang,Hao Tang*

Main category: cs.CV

TL;DR: EgoLCD 是一种用于自监督长期上下文视频生成的端到端框架，通过结合 Long-Term Sparse KV 缓存和基于注意力的短时记忆（扩展自 LoRA 用于局部适应），并使用 Memory Regulation Loss 和结构化叙述提示提供的一致性记忆使用，有效解决了长视频生成中的记忆管理问题。


<details>
  <summary>Details</summary>
Motivation: 现有的自回归模型在长视频生成中存在内容漂移的问题，导致对象身份和场景语义随着时间的推移而退化。EgoLCD 设计的目的是解决这一挑战，通过有效的长期记忆管理来生成长期且连续的个人视角视频。

Method: EgoLCD 采用一种长时稀疏 KV 缓存结构来维持稳定的全局上下文，结合基于注意力的短时记忆并扩展了 LoRA 以实现局部适应。同时引入 Memory Regulation Loss 来确保一致的记忆使用，并通过结构化叙述引导实现在时间上的明确指导。

Result: EgoLCD 在 EgoVid-5M 挑战基准上的实验结果表明，该框架能实现感知质量和时间一致性方面的领先表现，显著改善了生成的记忆问题。

Conclusion: EgoLCD 是迈向构建具有情景感知能力的潜在世界模型的重要一步，为增强人工智能体的能力奠定了基础。

Abstract: Generating long, coherent egocentric videos is difficult, as hand-object interactions and procedural tasks require reliable long-term memory. Existing autoregressive models suffer from content drift, where object identity and scene semantics degrade over time. To address this challenge, we introduce EgoLCD, an end-to-end framework for egocentric long-context video generation that treats long video synthesis as a problem of efficient and stable memory management. EgoLCD combines a Long-Term Sparse KV Cache for stable global context with an attention-based short-term memory, extended by LoRA for local adaptation. A Memory Regulation Loss enforces consistent memory usage, and Structured Narrative Prompting provides explicit temporal guidance. Extensive experiments on the EgoVid-5M benchmark demonstrate that EgoLCD achieves state-of-the-art performance in both perceptual quality and temporal consistency, effectively mitigating generative forgetting and representing a significant step toward building scalable world models for embodied AI. Code: https://github.com/AIGeeksGroup/EgoLCD. Website: https://aigeeksgroup.github.io/EgoLCD.

</details>


### [35] [Auto3R: Automated 3D Reconstruction and Scanning via Data-driven Uncertainty Quantification](https://arxiv.org/abs/2512.04528)
*Chentao Shen,Sizhe Zheng,Bingqian Wu,Yaohua Feng,Yuanchen Fei,Mingyu Mei,Hanwen Jiang,Xiangru Huang*

Main category: cs.CV

TL;DR: Auto3R 是一种数据驱动的不确定性量化模型，能够在无人干预的情况下自动化 3D 扫描和重建，包括非朗伯性和镜面材料的物体。


<details>
  <summary>Details</summary>
Motivation: 随着无人机和机器人的快速发展，需要无手动规划地进行准确的 3D 扫描和重建。

Method: Auto3R 模型通过迭代的 3D 重建和扫描过程，预测潜在扫描视角的不确定性分布，无需知道真实几何和外观。

Result: Auto3R 在广泛实验中表现出色，显著优于现有最先进的方法，能够用于现实世界的 3D 对象数字化，提供具有photorealistic效果的数字资产。

Conclusion: Auto3R 为自动化 3D 扫描和重建提供了新的解决方案，突破了传统依赖大量人工规划的限制。

Abstract: Traditional high-quality 3D scanning and reconstruction typically relies on human labor to plan the scanning procedure. With the rapid development of embodied systems such as drones and robots, there is a growing demand of performing accurate 3D scanning and reconstruction in an fully automated manner. We introduce Auto3R, a data-driven uncertainty quantification model that is designed to automate the 3D scanning and reconstruction of scenes and objects, including objects with non-lambertian and specular materials. Specifically, in a process of iterative 3D reconstruction and scanning, Auto3R can make efficient and accurate prediction of uncertainty distribution over potential scanning viewpoints, without knowing the ground truth geometry and appearance. Through extensive experiments, Auto3R achieves superior performance that outperforms the state-of-the-art methods by a large margin. We also deploy Auto3R on a robot arm equipped with a camera and demonstrate that Auto3R can be used to effectively digitize real-world 3D objects and delivers ready-to-use and photorealistic digital assets. Our homepage: https://tomatoma00.github.io/auto3r.github.io .

</details>


### [36] [PhyVLLM: Physics-Guided Video Language Model with Motion-Appearance Disentanglement](https://arxiv.org/abs/2512.04532)
*Yu-Wei Zhan,Xin Wang,Hong Chen,Tongtong Feng,Wei Feng,Ren Wang,Guangyao Li,Qing Li,Wenwu Zhu*

Main category: cs.CV

TL;DR: PhyVLLM 是一种物理指导的视频语言框架，通过显式引入物理运动，解决了现有视频大语言模型在物理理解上的不足，显著提升了在物理推理和一般视频理解任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有视频大语言模型依赖于外观匹配，对于需要深入理解物理动力学的场景表现不佳，PhyVLLM 是为解决这一问题而提出的。

Method: PhyVLLM 通过一个双分支编码器分离视觉外观和物体运动，并引入了神经常微分方程模块（Neural ODE）来建模时间上的物理动态。它还能通过自我监督的方式捕捉物体运动的连续演化，无需明确标注物理属性。

Result: PhyVLLM 在物理推理和通用视频理解任务上均表现出色，大幅超越了现有最先进的视频大语言模型。

Conclusion: PhyVLLM 的提出表明，引入显式物理建模对于提升视频大语言模型的性能至关重要。

Abstract: Video Large Language Models (Video LLMs) have shown impressive performance across a wide range of video-language tasks. However, they often fail in scenarios requiring a deeper understanding of physical dynamics. This limitation primarily arises from their reliance on appearance-based matching. Incorporating physical motion modeling is crucial for deeper video understanding, but presents three key challenges: (1) motion signals are often entangled with appearance variations, making it difficult to extract clean physical cues; (2) effective motion modeling requires not only continuous-time motion representations but also capturing physical dynamics; and (3) collecting accurate annotations for physical attributes is costly and often impractical. To address these issues, we propose PhyVLLM, a physical-guided video-language framework that explicitly incorporates physical motion into Video LLMs. Specifically, PhyVLLM disentangles visual appearance and object motion through a dual-branch encoder. To model physical dynamics over time, we incorporate a Neural Ordinary Differential Equation (Neural ODE) module, which generates differentiable physical dynamic representations. The resulting motion-aware representations are projected into the token space of a pretrained LLM, enabling physics reasoning without compromising the model's original multimodal capabilities. To circumvent the need for explicit physical labels, PhyVLLM employs a self-supervised manner to model the continuous evolution of object motion. Experimental results demonstrate that PhyVLLM significantly outperforms state-of-the-art Video LLMs on both physical reasoning and general video understanding tasks, highlighting the advantages of incorporating explicit physical modeling.

</details>


### [37] [Refaçade: Editing Object with Given Reference Texture](https://arxiv.org/abs/2512.04534)
*Youze Huang,Penghui Ruan,Bojia Zi,Xianbiao Qi,Jianan Wang,Rong Xiao*

Main category: cs.CV

TL;DR: 提出了一个新颖的任务——物体重涂，该任务旨在将在参考对象上的局部纹理转移到目标对象上；为了解决从根本上受到参考图像结构信息干扰以及无法分离纹理与结构信息的问题，该方法引入了Refaçade，实现了图像和视频中的精确和可控纹理转移。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型在图像和视频编辑方面取得了显著进展，但某些任务尚未得到充分探索。为了解决这一问题，作者提出了一个新的任务——物体重涂。

Method: 作者首先利用在配对的有纹理和无纹理3D网格渲染上训练的纹理去除器去除外观信息以保留来源的几何和运动。其次，用拼图排列干扰参考全局布局，促使模型关注局部纹理统计而非对象的全局布局。

Result: 大量的实验表明，与强基线相比，该方法在视觉质量、精准编辑和可控性方面都表现更优，且相关代码已开源。

Conclusion: 该工作提出了一种新的方法Refaçade，为物体重涂任务提供了精确且可控的纹理转移解决方案，证明了其在图像和视频编辑中的优越性。

Abstract: Recent advances in diffusion models have brought remarkable progress in image and video editing, yet some tasks remain underexplored. In this paper, we introduce a new task, Object Retexture, which transfers local textures from a reference object to a target object in images or videos. To perform this task, a straightforward solution is to use ControlNet conditioned on the source structure and the reference texture. However, this approach suffers from limited controllability for two reasons: conditioning on the raw reference image introduces unwanted structural information, and it fails to disentangle the visual texture and structure information of the source. To address this problem, we propose Refaçade, a method that consists of two key designs to achieve precise and controllable texture transfer in both images and videos. First, we employ a texture remover trained on paired textured/untextured 3D mesh renderings to remove appearance information while preserving the geometry and motion of source videos. Second, we disrupt the reference global layout using a jigsaw permutation, encouraging the model to focus on local texture statistics rather than the global layout of the object. Extensive experiments demonstrate superior visual quality, precise editing, and controllability, outperforming strong baselines in both quantitative and human evaluations. Code is available at https://github.com/fishZe233/Refacade.

</details>


### [38] [Detection of Intoxicated Individuals from Facial Video Sequences via a Recurrent Fusion Model](https://arxiv.org/abs/2512.04536)
*Bita Baroutian,Atefe Aghaei,Mohsen Ebrahimi Moghaddam*

Main category: cs.CV

TL;DR: 本研究提出了一种基于视频的面部序列分析方法，用于酒精中毒检测，使用Graph Attention Network (GAT)进行面部关键点分析，并结合3D ResNet提取时空视觉特征，实现了95.82%的高准确率，展示了其在公共安全系统的非侵入性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 酒精消费是公共卫生领域的重大问题，本研究旨在开发一种准确、非侵入性的酒精中毒检测方法，以减少由酒精引起的事故和死亡事件。

Method: 该方法结合了Graph Attention Network (GAT)用于面部关键点分析以及3D ResNet提取的时空视觉特征，动态融合这些特征并通过自适应优先级增强分类性能。

Result: 实验结果显示，该模型的准确率为95.82%，精确度为0.977，召回率为0.97，优于现有的基线方法。

Conclusion: 研究结果表明，该模型具有在公共安全系统中非侵入性地准确检测酒精中毒的潜力。

Abstract: Alcohol consumption is a significant public health concern and a major cause of accidents and fatalities worldwide. This study introduces a novel video-based facial sequence analysis approach dedicated to the detection of alcohol intoxication. The method integrates facial landmark analysis via a Graph Attention Network (GAT) with spatiotemporal visual features extracted using a 3D ResNet. These features are dynamically fused with adaptive prioritization to enhance classification performance. Additionally, we introduce a curated dataset comprising 3,542 video segments derived from 202 individuals to support training and evaluation. Our model is compared against two baselines: a custom 3D-CNN and a VGGFace+LSTM architecture. Experimental results show that our approach achieves 95.82% accuracy, 0.977 precision, and 0.97 recall, outperforming prior methods. The findings demonstrate the model's potential for practical deployment in public safety systems for non-invasive, reliable alcohol intoxication detection.

</details>


### [39] [X-Humanoid: Robotize Human Videos to Generate Humanoid Videos at Scale](https://arxiv.org/abs/2512.04537)
*Pei Yang,Hai Ci,Yiren Song,Mike Zheng Shou*

Main category: cs.CV

TL;DR: 介绍了X-Humanoid，一种生成性视频编辑方法，通过将Wan 2.2模型适应并微调为视频到视频结构，实现了将人类视频转化为类人机器人的大规模数据集生成。该方法利用 Unreal Engine 创建配对合成视频，并应用于 Ego-Exo4D 视频，生成超过 360 万帧的机器人化类人视频。


<details>
  <summary>Details</summary>
Motivation: 现有机器人化方法仅能处理部分场景，无法处理复杂的全身动作和第三人称视频中的遮挡问题。因此，需要开发一种新的方法来解决这些限制。

Method: 通过将 Wan 2.2 模型调整为视频到视频结构，并设计一个可扩展的数据生成管道，将社区资源转化为配对合成视频，利用训练后的模型在 Ego-Exo4D 数据集上生成并发布了一个包含超过 360 万帧的“机器人化”类人视频的大规模数据集。

Result: 生成了一个包含超过 360 万帧的“机器人化”类人视频的大规模数据集。定量分析和用户研究证实该方法在动作一致性方面优于现有基线方法（69%的用户认为最佳），并在身体正确的表现方面也具有明显优势（62.1%的用户认为最佳）。

Conclusion: X-Humanoid 方法为解决大规模、多样化的训练数据稀缺问题提供了一种有效的解决方案，并展示了其在人类到类人机器人转化方面的可靠性和有效性，为智能类人机器人的研究开辟了新的可能性。

Abstract: The advancement of embodied AI has unlocked significant potential for intelligent humanoid robots. However, progress in both Vision-Language-Action (VLA) models and world models is severely hampered by the scarcity of large-scale, diverse training data. A promising solution is to "robotize" web-scale human videos, which has been proven effective for policy training. However, these solutions mainly "overlay" robot arms to egocentric videos, which cannot handle complex full-body motions and scene occlusions in third-person videos, making them unsuitable for robotizing humans. To bridge this gap, we introduce X-Humanoid, a generative video editing approach that adapts the powerful Wan 2.2 model into a video-to-video structure and finetunes it for the human-to-humanoid translation task. This finetuning requires paired human-humanoid videos, so we designed a scalable data creation pipeline, turning community assets into 17+ hours of paired synthetic videos using Unreal Engine. We then apply our trained model to 60 hours of the Ego-Exo4D videos, generating and releasing a new large-scale dataset of over 3.6 million "robotized" humanoid video frames. Quantitative analysis and user studies confirm our method's superiority over existing baselines: 69% of users rated it best for motion consistency, and 62.1% for embodiment correctness.

</details>


### [40] [VideoMem: Enhancing Ultra-Long Video Understanding via Adaptive Memory Management](https://arxiv.org/abs/2512.04540)
*Hongbo Jin,Qingyuan Wang,Wenhao Zhang,Yang Liu,Sijie Cheng*

Main category: cs.CV

TL;DR: VideoMem 提出了一种新的框架，通过自适应的记忆管理技术，使视觉语言模型能够更好地理解超长视频。该方法利用 PRPO 算法中的 PSP 模块和 TCR 模块，动态更新全局记忆缓冲区，以保留关键信息并丢弃冗余内容。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉语言模型在处理超长视频内容时存在局限性，如上下文长度有限和长期记忆保留效率低。VideoMem 旨在解决这一问题。

Method: VideoMem 引入了一种基于自适应记忆管理的顺序生成任务方法。它通过动态更新全局记忆缓冲区来工作，该缓冲区能选择性地保留关键信息并丢弃冗余内容。此外，该框架引入了 PRPO 算法，其中包括 Progressive State Propagation (PSP) 和 Temporal Cascading Reward (TCR) 两个关键模块，前者适应性保留有效状态并逐步缩小模型的探索范围，后者通过缓解奖励稀疏性来提高样本利用效率和加速收敛。

Result: 实验结果显示，VideoMem 在多种超长视频理解任务基准上显著优于现有的开源模型。

Conclusion: VideoMem 为超长视频的理解提供了有效的解决方案，证明了其在视觉语言模型长时任务上的优越性。

Abstract: Ultra long video understanding remains an open challenge, as existing vision language models (VLMs) falter on such content due to limited context length and inefficient long term memory retention. To address this, recent works have attempted to construct external knowledge bases and corresponding retrieval agumented generation (RAG) systems, yet these incur enormous storage and computational overhead. In this paper, we propose VideoMem, a novel framework that pioneers models long video understanding as a sequential generation task via adaptive memory management. Specifically, VideoMem dynamically updates a global memory buffer, which adaptively retains critical information while discarding redundant content across the video timeline. To efficiently train VLMs for such long-term tasks, VideoMem integrates the Progressive Grouped Relative Policy Optimization (PRPO) algorithm, equipped with two core modules: Progressive State Propagation (PSP) adaptively retains valid current states, propagates them to the next rollout step, and gradually narrows the model exploration space. Temporal Cascading Reward (TCR) further alleviates reward sparsity, improving sample utilization and accelerating convergence. Extensive experiments demonstrate that VideoMem significantly outperforms existing open-source models across diverse benchmarks for ultra-long video understanding tasks.

</details>


### [41] [Gaussian Entropy Fields: Driving Adaptive Sparsity in 3D Gaussian Optimization](https://arxiv.org/abs/2512.04542)
*Hong Kuang,Jianchen Liu*

Main category: cs.CV

TL;DR: 该研究提出了一种用于新颖视图合成的3D Gaussian Splatting (3DGS)方法，通过熵驱动的表面建模、自适应空间正则化和多尺度几何保持，实现了在DTU和T&T基准上的竞争性几何精确度，同时在Mip-NeRF 360基准上提供了优于现有方法的渲染质量。


<details>
  <summary>Details</summary>
Motivation: 3DGS方法通过熵驱动的表面建模、自适应空间正则化和多尺度几何保持技术，增强了表面重建的准确性，同时保持了光度保真度。

Method: 该方法包含三项技术贡献：熵驱动表面建模，通过熵最小化实现低构型熵的原始分布；自适应空间正则化，使用Surface Neighborhood Redundancy Index (SNRI)和图像熵指导加权；多尺度几何保持，通过竞争性跨尺度熵对齐来实现几何信息的保留。

Result: 通过广泛实验，GEF方法在DTU和T&T基准上的几何精度与现有方法相当，而在Mip-NeRF 360基准上的渲染质量优于现有方法。Chamfer Distance取得了0.64的优异成绩，F1分数为0.44，而在Mip-NeRF 360基准上，SSIM达到0.855，LPIPS为0.136，这是基线中的最佳结果。

Conclusion: 该研究提出的3DGS方法不仅提高了表面重建的准确性，还保持了与现有的良好光度保真度，展示了其在新颖视图合成中的广泛应用潜力。

Abstract: 3D Gaussian Splatting (3DGS) has emerged as a leading technique for novel view synthesis, demonstrating exceptional rendering efficiency. \replaced[]{Well-reconstructed surfaces can be characterized by low configurational entropy, where dominant primitives clearly define surface geometry while redundant components are suppressed.}{The key insight is that well-reconstructed surfaces naturally exhibit low configurational entropy, where dominant primitives clearly define surface geometry while suppressing redundant components.} Three complementary technical contributions are introduced: (1) entropy-driven surface modeling via entropy minimization for low configurational entropy in primitive distributions; (2) adaptive spatial regularization using the Surface Neighborhood Redundancy Index (SNRI) and image entropy-guided weighting; (3) multi-scale geometric preservation through competitive cross-scale entropy alignment. Extensive experiments demonstrate that GEF achieves competitive geometric precision on DTU and T\&T benchmarks, while delivering superior rendering quality compared to existing methods on Mip-NeRF 360. Notably, superior Chamfer Distance (0.64) on DTU and F1 score (0.44) on T\&T are obtained, alongside the best SSIM (0.855) and LPIPS (0.136) among baselines on Mip-NeRF 360, validating the framework's ability to enhance surface reconstruction accuracy without compromising photometric fidelity.

</details>


### [42] [Counterfeit Answers: Adversarial Forgery against OCR-Free Document Visual Question Answering](https://arxiv.org/abs/2512.04554)
*Marco Pintore,Maura Pintor,Dimosthenis Karatzas,Battista Biggio*

Main category: cs.CV

TL;DR: 本文提出了针对文档视觉问答（DocVQA）模型的新型攻击方法，通过微小但具有语义目标的修改，使文档内容引发错误答案。这些攻击算法能够适应不同攻击者的意图，有效对抗当前最先进的模型Pix2Struct和Donut。


<details>
  <summary>Details</summary>
Motivation: 当前的DocVQA模型虽然表现出色，但却容易受到攻击，这表明需要改进现有的防御机制。

Method: 设计并实现了专门的攻击算法，能够在不明显改变文档外观的前提下，植入语义目标的修改，从而误导DocVQA模型生成错误的答案。这些算法能够根据攻击者的不同目标创建针对性的伪造文档。

Result: 证明了所设计的攻击方法能够有效地误导两种最先进的DocVQA模型（Pix2Struct和Donut），并对模型产生误导性的输出起到实质性的效果。

Conclusion: 研究发现当前DocVQA系统存在关键的安全漏洞，强调了需要进一步研究和开发更强的防御措施。

Abstract: Document Visual Question Answering (DocVQA) enables end-to-end reasoning grounded on information present in a document input. While recent models have shown impressive capabilities, they remain vulnerable to adversarial attacks. In this work, we introduce a novel attack scenario that aims to forge document content in a visually imperceptible yet semantically targeted manner, allowing an adversary to induce specific or generally incorrect answers from a DocVQA model. We develop specialized attack algorithms that can produce adversarially forged documents tailored to different attackers' goals, ranging from targeted misinformation to systematic model failure scenarios. We demonstrate the effectiveness of our approach against two end-to-end state-of-the-art models: Pix2Struct, a vision-language transformer that jointly processes image and text through sequence-to-sequence modeling, and Donut, a transformer-based model that directly extracts text and answers questions from document images. Our findings highlight critical vulnerabilities in current DocVQA systems and call for the development of more robust defenses.

</details>


### [43] [COOPER: A Unified Model for Cooperative Perception and Reasoning in Spatial Intelligence](https://arxiv.org/abs/2512.04563)
*Zefeng Zhang,Xiangzhao Hao,Hengzhu Tang,Zhenyu Zhang,Jiawei Sheng,Xiaodong Li,Zhenyang Li,Li Gao,Daiting Shi,Dawei Yin,Tingwen Liu*

Main category: cs.CV

TL;DR: 本文提出了COOPER，一种结合深度和分割作为辅助模态的统一MLLM，通过两阶段训练获得辅助模态生成及自适应交错推理能力。COOPER在空间推理上取得了6.91%的平均改进，即使仅专注于辅助模态生成训练的变体也在距离和大小估计上获得了7.92%的增益。


<details>
  <summary>Details</summary>
Motivation: 当前的MLLMs在3D场景理解上存在局限性，研究者希望通过结合深度和分割信息，以及统一训练方式改进MLLMs的空间感知和推理能力。

Method: 提出了COOPER，结合深度和分割模态，并分两阶段训练，第一阶段训练辅助模态生成，第二阶段结合多模态数据进行交互式推理训练。

Result: COOPER在空间推理任务上取得了6.91%的平均性能提升，即使仅专注于辅助模态生成训练的变体也在距离和大小估计上也实现了显著提升，达到了7.92%的增益。

Conclusion: 研究证明了结合深度和分割信息以及统一两阶段训练方式对提升MLLMs的空间感知和推理能力的有效性，并在未来的研究中有着一定的参考价值。

Abstract: Visual Spatial Reasoning is crucial for enabling Multimodal Large Language Models (MLLMs) to understand object properties and spatial relationships, yet current models still struggle with 3D-aware reasoning. Existing approaches typically enhance either perception, by augmenting RGB inputs with auxiliary modalities such as depth and segmentation, or reasoning, by training on spatial VQA datasets and applying reinforcement learning, and thus treat these two aspects in isolation. In this work, we investigate whether a unified MLLM can develop an intrinsic ability to enhance spatial perception and, through adaptive interleaved reasoning, achieve stronger spatial intelligence. We propose \textbf{COOPER}, a unified MLLM that leverages depth and segmentation as auxiliary modalities and is trained in two stages to acquire auxiliary modality generation and adaptive, interleaved reasoning capabilities. COOPER achieves an average \textbf{6.91\%} improvement in spatial reasoning while maintaining general performance. Moreover, even a variant trained only for auxiliary modality generation attains a \textbf{7.92\%} gain on distance and size estimation, suggesting that learning to generate auxiliary modalities helps internalize spatial knowledge and strengthen spatial understanding.

</details>


### [44] [Dataset creation for supervised deep learning-based analysis of microscopic images - review of important considerations and recommendations](https://arxiv.org/abs/2512.04564)
*Christof A. Bertram,Viktoria Weiss,Jonas Ammeling,F. Maria Schabel,Taryn A. Donovan,Frauke Wilm,Christian Marzahl,Katharina Breininger,Marc Aubreville*

Main category: cs.CV

TL;DR: 本文综述了基于显微镜图像的深度学习模型数据集创建的关键步骤，包括图像获取、标注软件选择和标注创建。强调了数据集质量的关键标准，提供了质量保证的建议，并讨论了标准操作程序(SOP)的重要性，以促进创新和提高研究的可重复性。


<details>
  <summary>Details</summary>
Motivation: 现有文献支持深度学习在分析显微镜图像方面的潜力，但高质量的大型数据集的创建仍然面临诸多挑战。本文旨在提供数据集创建的综合指南，以克服现有障碍并提高数据集质量。

Method: 本文通过回顾数据集创建的每个关键步骤，并提出了质量保证的建议来解决现有挑战。

Result: 本文提供了图像获取、注释软件选择和标注创建的方法指导，并强调了注释质量的三个关键标准：准确、完整和一致。同时，提出了增强注释质量的方法，以减轻单个注释员的局限性。

Conclusion: 本文强调了标准操作程序的重要性，以促进数据集创建的最佳实践，并支持开放数据集的使用，从而推动创新、提高研究的可重复性，促进深度学习模型在病理学中的广泛应用。

Abstract: Supervised deep learning (DL) receives great interest for automated analysis of microscopic images with an increasing body of literature supporting its potential. The development and validation of those DL models relies heavily on the availability of high-quality, large-scale datasets. However, creating such datasets is a complex and resource-intensive process, often hindered by challenges such as time constraints, domain variability, and risks of bias in image collection and label creation. This review provides a comprehensive guide to the critical steps in dataset creation, including: 1) image acquisition, 2) selection of annotation software, and 3) annotation creation. In addition to ensuring a sufficiently large number of images, it is crucial to address sources of image variability (domain shifts) - such as those related to slide preparation and digitization - that could lead to algorithmic errors if not adequately represented in the training data. Key quality criteria for annotations are the three "C"s: correctness, completeness, and consistency. This review explores methods to enhance annotation quality through the use of advanced techniques that mitigate the limitations of single annotators. To support dataset creators, a standard operating procedure (SOP) is provided as supplemental material, outlining best practices for dataset development. Furthermore, the article underscores the importance of open datasets in driving innovation and enhancing reproducibility of DL research. By addressing the challenges and offering practical recommendations, this review aims to advance the creation of and availability to high-quality, large-scale datasets, ultimately contributing to the development of generalizable and robust DL models for pathology applications.

</details>


### [45] [When Robots Should Say "I Don't Know": Benchmarking Abstention in Embodied Question Answering](https://arxiv.org/abs/2512.04597)
*Tao Wu,Chuhao Zhou,Guangyu Zhao,Haozhi Cao,Yewen Pu,Jianfei Yang*

Main category: cs.CV

TL;DR: 本文研究了在无须回答每个问题的任务要求下，使类人智能体知道何时应避免提供回答（即'沉默'）的方法。通过人类查询和错误沟通理论，确定了五种需要避免回答的情况，并创建了一组用于评估的注释数据集。研究发现当前最佳模型的沉默召回率为42.79%，而人类的表现为91.17%，表明沉默是一项基本要求。


<details>
  <summary>Details</summary>
Motivation: 现有的EQA基准假设每个问题都必须被回答，但是类人智能体应该具备判断何时缺乏足够的信息而不回答问题的能力，即所谓的'沉默'。

Method: 作者对初始的500个人类查询进行了研究，并基于这一研究和认知理论中的沟通错误，定义了五个需要避免回答的情况。使用这些定义对OpenEQA进行了注释扩展，形成了AbstainEQA数据集，该数据集用于评估避免回答的能力。

Result: 实验表明，即便是在最先进的模型中，避免回答的召回率也只有42.79%，而人类的表现达到了91.17%。同时，实验还发现缩放、提示和推理方法只带来了微小的改进，并且微调模型容易过度拟合于文本线索。

Conclusion: 研究确定了沉默作为可靠交互的一项基本要求，强调了在可能性和必要性基础上传输的必要性，并指出当前的工作对开发有效的澄清基础打下了必要而坚实的基础。

Abstract: Embodied Question Answering (EQA) requires an agent to interpret language, perceive its environment, and navigate within 3D scenes to produce responses. Existing EQA benchmarks assume that every question must be answered, but embodied agents should know when they do not have sufficient information to answer. In this work, we focus on a minimal requirement for EQA agents, abstention: knowing when to withhold an answer. From an initial study of 500 human queries, we find that 32.4% contain missing or underspecified context. Drawing on this initial study and cognitive theories of human communication errors, we derive five representative categories requiring abstention: actionability limitation, referential underspecification, preference dependence, information unavailability, and false presupposition. We augment OpenEQA by having annotators transform well-posed questions into ambiguous variants outlined by these categories. The resulting dataset, AbstainEQA, comprises 1,636 annotated abstention cases paired with 1,636 original OpenEQA instances for balanced evaluation. Evaluating on AbstainEQA, we find that even the best frontier model only attains 42.79% abstention recall, while humans achieve 91.17%. We also find that scaling, prompting, and reasoning only yield marginal gains, and that fine-tuned models overfit to textual cues. Together, these results position abstention as a fundamental prerequisite for reliable interaction in embodied settings and as a necessary basis for effective clarification.

</details>


### [46] [E3AD: An Emotion-Aware Vision-Language-Action Model for Human-Centric End-to-End Autonomous Driving](https://arxiv.org/abs/2512.04733)
*Yihong Tang,Haicheng Liao,Tong Nie,Junlin He,Ao Qu,Kehua Chen,Wei Ma,Zhenning Li,Lijun Sun,Chengzhong Xu*

Main category: cs.CV

TL;DR: 该研究提出了一种名为E3AD的情感意识VLA框架，通过将情绪理解融入自主驾驶中，提升视觉定位和路径规划，实现情感状态的一致性训练，从而获得更接近人类行为的规划和反馈。


<details>
  <summary>Details</summary>
Motivation: 当前的自主驾驶系统普遍忽视了乘客的情感状态，这影响了驾乘舒适度和自动驾驶技术的接受度。因此，本文通过引入情感意识的VLA框架，旨在提升自动驾驶系统的舒适性和接受度。

Method: E3AD框架结合了连续的心理状态模型（VAD）和双路径空间推理机制，以及一种一致性导向的训练方案，以确保情绪意图与驾驶行为的一致性。

Result: 实验结果表明，E3AD在视觉定位和路径规划上优于现有方法，并实现了情感状态的最新相关性。这表明将情绪纳入VLA风格的驾驶中，可以产生更符合人类行为的定位、规划和人本反馈。

Conclusion: 该研究提出的方法为未来的情感意识自主驾驶系统奠定了基础，有助于提高驾驶体验和接受度。

Abstract: End-to-end autonomous driving (AD) systems increasingly adopt vision-language-action (VLA) models, yet they typically ignore the passenger's emotional state, which is central to comfort and AD acceptance. We introduce Open-Domain End-to-End (OD-E2E) autonomous driving, where an autonomous vehicle (AV) must interpret free-form natural-language commands, infer the emotion, and plan a physically feasible trajectory. We propose E3AD, an emotion-aware VLA framework that augments semantic understanding with two cognitively inspired components: a continuous Valenc-Arousal-Dominance (VAD) emotion model that captures tone and urgency from language, and a dual-pathway spatial reasoning module that fuses egocentric and allocentric views for human-like spatial cognition. A consistency-oriented training scheme, combining modality pretraining with preference-based alignment, further enforces coherence between emotional intent and driving actions. Across real-world datasets, E3AD improves visual grounding and waypoint planning and achieves state-of-the-art (SOTA) VAD correlation for emotion estimation. These results show that injecting emotion into VLA-style driving yields more human-aligned grounding, planning, and human-centric feedback.

</details>


### [47] [Malicious Image Analysis via Vision-Language Segmentation Fusion: Detection, Element, and Location in One-shot](https://arxiv.org/abs/2512.04599)
*Sheng Hang,Chaoxiang He,Hongsheng Hu,Hanqing Hu,Bin Benjamin Zhu,Shi-Feng Sun,Dawu Gu,Shuo Wang*

Main category: cs.CV

TL;DR: 该研究提出了一种零样本管道，能够同时检测图像中的有害内容，识别每个关键元素并精确定位这些元素，实现了85.8%的元素召回率、78.1%的精度和92.1%的分割成功率，相对于直接的零样本VLM定位，其召回率高出27.4%，具有高度的抗攻击性。


<details>
  <summary>Details</summary>
Motivation: 当前的NSFW标志无法提供足够的信息给内容审核者，他们需要了解图像中哪些物体使得该图像违法，以及这些物体的位置。因此，研究引入了一种新的零样本框架，以提供更多的信息帮助审核。

Method: 方法包括首先应用基础分割模型（SAM）以生成候选物体掩码，并对其进行细化为独立的大区域。然后，每个区域通过视觉-语言模型和开放式词汇表提示进行评估，以确定其恶意相关性。最后，这些得分被融合以生成综合恶意物体图。整个过程结合了多个分割器以提高鲁棒性。

Result: 该方法在包含药物、性、暴力和极端主义内容的新标注790图像数据集上进行了测试，实现了85.8%的元素召回率、78.1%的精度和92.1%的分割成功率。相较于直接的零样本视觉-语言模型定位，其召回率高出27.4%。在PGD对抗攻击下，方法的精度和召回率下降不超过10%，显示了高度的抗攻击性。

Conclusion: 该零样本框架是第一个用于细微、可解释的非法图像审核的实际工具，能够快速处理图像并无缝集成到现有的视觉-语言模型工作流程中。

Abstract: Detecting illicit visual content demands more than image-level NSFW flags; moderators must also know what objects make an image illegal and where those objects occur. We introduce a zero-shot pipeline that simultaneously (i) detects if an image contains harmful content, (ii) identifies each critical element involved, and (iii) localizes those elements with pixel-accurate masks - all in one pass. The system first applies foundation segmentation model (SAM) to generate candidate object masks and refines them into larger independent regions. Each region is scored for malicious relevance by a vision-language model using open-vocabulary prompts; these scores weight a fusion step that produces a consolidated malicious object map. An ensemble across multiple segmenters hardens the pipeline against adaptive attacks that target any single segmentation method. Evaluated on a newly-annotated 790-image dataset spanning drug, sexual, violent and extremist content, our method attains 85.8% element-level recall, 78.1% precision and a 92.1% segment-success rate - exceeding direct zero-shot VLM localization by 27.4% recall at comparable precision. Against PGD adversarial perturbations crafted to break SAM and VLM, our method's precision and recall decreased by no more than 10%, demonstrating high robustness against attacks. The full pipeline processes an image in seconds, plugs seamlessly into existing VLM workflows, and constitutes the first practical tool for fine-grained, explainable malicious-image moderation.

</details>


### [48] [Denoise to Track: Harnessing Video Diffusion Priors for Robust Correspondence](https://arxiv.org/abs/2512.04619)
*Tianyu Yuan,Yuanbo Yang,Lin-Zhuo Chen,Yao Yao,Zhuzhong Qian*

Main category: cs.CV

TL;DR: 本文提出了一种名为HeFT的零样本点跟踪框架，该框架利用了预训练视频扩散模型的视觉先验。通过对VDiT的内部表示进行分析，提出了基于头部和频率的特征选择策略，以提高跟踪性能。实验结果显示，该方法可以达到最先进的零样本跟踪效果。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探索如何利用预训练的视频扩散模型进行点跟踪。通过分析VDiT的内部表示，发现注意力头部具有不同的功能特化，提出了基于头部和频率的特征选择策略，希望通过这种方式提升点跟踪技术的效果。

Method: 本文的方法主要包括两部分：首先，通过对VDiT的内部表示进行分析，了解其编码时空信息的方式；其次，基于这些分析结果，提出了一个头-频率感知特征选择策略。具体来说，该策略通过单步去除噪声、特征选择以及软-argmax定位和前向-后向一致性检查来估计对应关系。

Result: 在TAP-Vid基准测试上，HeFT达到了最先进的零样本跟踪性能，接近监督方法的效果，但不需要标注的训练数据。

Conclusion: 本文的研究表明，视频扩散模型可以作为多种下游任务的强大基础模型，有助于实现统一的视觉基础模型。

Abstract: In this work, we introduce HeFT (Head-Frequency Tracker), a zero-shot point tracking framework that leverages the visual priors of pretrained video diffusion models. To better understand how they encode spatiotemporal information, we analyze the internal representations of Video Diffusion Transformer (VDiT). Our analysis reveals that attention heads act as minimal functional units with distinct specializations for matching, semantic understanding, and positional encoding. Additionally, we find that the low-frequency components in VDiT features are crucial for establishing correspondences, whereas the high-frequency components tend to introduce noise. Building on these insights, we propose a head- and frequency-aware feature selection strategy that jointly selects the most informative attention head and low-frequency components to enhance tracking performance. Specifically, our method extracts discriminative features through single-step denoising, applies feature selection, and employs soft-argmax localization with forward-backward consistency checks for correspondence estimation. Extensive experiments on TAP-Vid benchmarks demonstrate that HeFT achieves state-of-the-art zero-shot tracking performance, approaching the accuracy of supervised methods while eliminating the need for annotated training data. Our work further underscores the promise of video diffusion models as powerful foundation models for a wide range of downstream tasks, paving the way toward unified visual foundation models.

</details>


### [49] [I2I-Bench: A Comprehensive Benchmark Suite for Image-to-Image Editing Models](https://arxiv.org/abs/2512.04660)
*Juntong Wang,Jiarui Wang,Huiyu Duan,Jiaxiang Kang,Guangtao Zhai,Xiongkuo Min*

Main category: cs.CV

TL;DR: I2I-Bench 提出了一种全面的图像到图像编辑基准，涵盖了丰富的任务类别、全面的评估维度以及自动化混合评估方法。


<details>
  <summary>Details</summary>
Motivation: 现有的图像编辑基准存在任务范围有限、评估维度不足和依赖手动注释的问题，限制了它们的可扩展性和实用性。

Method: I2I-Bench 提出了一个集成多种方向的基准，通过多样化的任务、多种评估维度和自动化的综合评估方法，注重模型性能的客观性与一致性。

Result: 该基准能够评估多个主流图像编辑模型，并揭示了它们在不同维度上的差距与权衡。

Conclusion: I2I-Bench 的发布为未来的相关研究提供了基础，并提高了此类模型的评估标准。

Abstract: Image editing models are advancing rapidly, yet comprehensive evaluation remains a significant challenge. Existing image editing benchmarks generally suffer from limited task scopes, insufficient evaluation dimensions, and heavy reliance on manual annotations, which significantly constrain their scalability and practical applicability. To address this, we propose \textbf{I2I-Bench}, a comprehensive benchmark for image-to-image editing models, which features (i) diverse tasks, encompassing 10 task categories across both single-image and multi-image editing tasks, (ii) comprehensive evaluation dimensions, including 30 decoupled and fine-grained evaluation dimensions with automated hybrid evaluation methods that integrate specialized tools and large multimodal models (LMMs), and (iii) rigorous alignment validation, justifying the consistency between our benchmark evaluations and human preferences. Using I2I-Bench, we benchmark numerous mainstream image editing models, investigating the gaps and trade-offs between editing models across various dimensions. We will open-source all components of I2I-Bench to facilitate future research.

</details>


### [50] [ReflexFlow: Rethinking Learning Objective for Exposure Bias Alleviation in Flow Matching](https://arxiv.org/abs/2512.04904)
*Guanbo Huang,Jingjia Mao,Fanding Huang,Fengkai Liu,Xiangyang Luo,Yaoyuan Liang,Jiasheng Lu,Xiaoe Wang,Pei Liu,Ruiliu Fu,Shao-Lun Huang*

Main category: cs.CV

TL;DR: 本文提出了一种名为ReflexFlow的方法，以缓解Flow Matching方法中的曝光偏差。该方法通过动态矫正预测目标（Anti-Drift Rectification）和低频成分补偿（Frequency Compensation）两个部分来实现。


<details>
  <summary>Details</summary>
Motivation: 本文的研究动机在于解决Flow Matching方法中的曝光偏差问题，尽管该领域近期有所进展，但仍存在模型对偏差输入缺乏泛化以及早期去噪过程不足以捕捉低频内容等根本原因。

Method: ReflexFlow由两个部分组成：一是设计了一种在训练时使用调度采样的反漂移校正（Anti-Drift Rectification），该方法能够根据偏差输入调整预测目标；二是通过在损失中重新加权来补偿缺失的低频成分（Frequency Compensation），以此来解决模型缺乏泛化能力的问题。

Result: 实验结果显示，ReflexFlow在CelebA-64数据集上的FID得分降低了35.65%，并且能够改进多种数据集上的生成质量。

Conclusion: ReflexFlow方法为Flow Matching领域的曝光偏差问题提供了一种简单而有效的解决方案，这一方法适用于所有Flow Matching框架。

Abstract: Despite tremendous recent progress, Flow Matching methods still suffer from exposure bias due to discrepancies in training and inference. This paper investigates the root causes of exposure bias in Flow Matching, including: (1) the model lacks generalization to biased inputs during training, and (2) insufficient low-frequency content captured during early denoising, leading to accumulated bias. Based on these insights, we propose ReflexFlow, a simple and effective reflexive refinement of the Flow Matching learning objective that dynamically corrects exposure bias. ReflexFlow consists of two components: (1) Anti-Drift Rectification (ADR), which reflexively adjusts prediction targets for biased inputs utilizing a redesigned loss under training-time scheduled sampling; and (2) Frequency Compensation (FC), which reflects on missing low-frequency components and compensates them by reweighting the loss using exposure bias. ReflexFlow is model-agnostic, compatible with all Flow Matching frameworks, and improves generation quality across datasets. Experiments on CIFAR-10, CelebA-64, and ImageNet-256 show that ReflexFlow outperforms prior approaches in mitigating exposure bias, achieving a 35.65% reduction in FID on CelebA-64.

</details>


### [51] [Reward Forcing: Efficient Streaming Video Generation with Rewarded Distribution Matching Distillation](https://arxiv.org/abs/2512.04678)
*Yunhong Lu,Yanhong Zeng,Haobo Li,Hao Ouyang,Qiuyu Wang,Ka Leong Cheng,Jiapeng Zhu,Hengyuan Cao,Zhipeng Zhang,Xing Zhu,Yujun Shen,Min Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的Reward Forcing框架，通过EMA-Sink和Re-DMD技术提高了视频帧的动态捕捉和运动质量，从而提升了流媒体视频的生成效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖静态的初始帧作为注意力机制的中心，这导致了运动动态的损失。为了解决这一问题，本文提出了一种新的Reward Forcing框架，旨在更有效地捕捉视频帧的运动动态，从而提高流媒体视频的质量。

Method: 本文提出了EMA-Sink，它使用初始帧初始化固定大小的令牌，并通过滑动窗口外的令牌进行指数移动平均更新。同时提出了一种新的Re-DMD技术，通过优先处理高奖励的样本来偏向模型输出概率分布，从而提高运动动态的提取。

Result: 实验结果表明，Reward Forcing在标准基准测试中达到了最先进的性能，同时在单块H100 GPU上实现了23.1 FPS的高速流媒体视频生成。

Conclusion: 本文提出的方法显著提升了视频流中的运动质量和长期上下文一致性，为流媒体视频生成提供了新的方法和参考。

Abstract: Efficient streaming video generation is critical for simulating interactive and dynamic worlds. Existing methods distill few-step video diffusion models with sliding window attention, using initial frames as sink tokens to maintain attention performance and reduce error accumulation. However, video frames become overly dependent on these static tokens, resulting in copied initial frames and diminished motion dynamics. To address this, we introduce Reward Forcing, a novel framework with two key designs. First, we propose EMA-Sink, which maintains fixed-size tokens initialized from initial frames and continuously updated by fusing evicted tokens via exponential moving average as they exit the sliding window. Without additional computation cost, EMA-Sink tokens capture both long-term context and recent dynamics, preventing initial frame copying while maintaining long-horizon consistency. Second, to better distill motion dynamics from teacher models, we propose a novel Rewarded Distribution Matching Distillation (Re-DMD). Vanilla distribution matching treats every training sample equally, limiting the model's ability to prioritize dynamic content. Instead, Re-DMD biases the model's output distribution toward high-reward regions by prioritizing samples with greater dynamics rated by a vision-language model. Re-DMD significantly enhances motion quality while preserving data fidelity. We include both quantitative and qualitative experiments to show that Reward Forcing achieves state-of-the-art performance on standard benchmarks while enabling high-quality streaming video generation at 23.1 FPS on a single H100 GPU.

</details>


### [52] [Towards Cross-View Point Correspondence in Vision-Language Models](https://arxiv.org/abs/2512.04686)
*Yipu Wang,Yuheng Ji,Yuyang Liu,Enshen Zhou,Ziqiang Yang,Yuxuan Tian,Ziheng Qin,Yue Liu,Huajie Tan,Cheng Chi,Zhiyuan Ma,Daniel Dajun Zeng,Xiaolong Zheng*

Main category: cs.CV

TL;DR: 本文提出了跨视图点对应任务（CVPC）及其基准CrossPoint-Bench，并构建了涵盖378K问题-答案对的CrossPoint-378K数据集，用于 fine-grained 坐标预测。所提出的CroPond模型在CrossPoint-Bench上取得了优异表现，超越了现有的 Gemini-2.5-Pro 模型。


<details>
  <summary>Details</summary>
Motivation: VLMs 在实现精确的点级对应方面仍存在挑战，这对于实现精确的物质量化交互至关重要。因此，作者引入了CVPC任务和CrossPoint-Bench基准，旨在通过模拟人类认知过程中的感知、推理和对应步骤来构建一个综合基准。

Method: 作者首先介绍了CVPC任务和CrossPoint-Bench基准，它们都是受到人类认知过程启发而设计的。接着，作者构建了一个名为CrossPoint-378K的数据集，用于细粒度坐标预测。然后，作者提出了一个基于CrossPoint-378K数据集训练的模型CroPond。最后，作者评估了CroPond并在CrossPoint-Bench上显示了其性能。

Result: CroPond模型在CrossPoint-Bench测试中获得了显著的高准确率，超过现有的SOTA模型，准确率提高了39.7%。

Conclusion: 本文提出了CVPC任务及基准，构建了大量细粒度对应问题的数据集，并提出了CroPond模型以解决精细化坐标预测问题。

Abstract: Cross-view correspondence is a fundamental capability for spatial understanding and embodied AI. However, it is still far from being realized in Vision-Language Models (VLMs), especially in achieving precise point-level correspondence, which is crucial for precise affordance interaction. So we propose the Cross-View Point Correspondence (CVPC) task and CrossPoint-Bench, a comprehensive benchmark with hierarchical design, inspired by the human cognitive process of "perceive", "reason", and "correspond". Our evaluation shows the state-of-the-art models (e.g., Gemini-2.5-Pro) still fall far behind humans, with a gap of over 54.65% in overall accuracy, exposing a challenge in transitioning from coarse-grained judgement to fine-grained coordinate prediction. To address this problem, we construct CrossPoint-378K, a dataset with 378K question-answering pairs across 900 scenes, focused on actionable affordance regions that better reflect real-world manipulation and interaction scenarios. Furthermore, we propose CroPond that trained on the CrossPoint-378K dataset. Our CroPond achieves state-of-the-art performance on CrossPoint-Bench, surpassing Gemini-2.5-Pro by 39.7% accuracy, which offers a foundation for advancing future work on cross-view correspondence. The benchmark, dataset, and model are publicly available at https://github.com/WangYipu2002/CrossPoint.

</details>


### [53] [OmniScaleSR: Unleashing Scale-Controlled Diffusion Prior for Faithful and Realistic Arbitrary-Scale Image Super-Resolution](https://arxiv.org/abs/2512.04699)
*Xinning Chai,Zhengxue Cheng,Yuhong Zhang,Hengsheng Zhang,Yingsheng Qin,Yucai Yang,Rong Xie,Li Song*

Main category: cs.CV

TL;DR: OmniScaleSR 提出了一种基于扩散的高级任意尺度超分辨率框架，该方法通过引入扩散本源的尺度控制机制和多领域保真度增强设计，实现了高精度和高逼真度，尤其适用于大放大倍数。


<details>
  <summary>Details</summary>
Motivation: 当前的扩散模型虽然能实现高质量的4倍超分辨率，但在任意放大倍数下的表现欠佳，存在过度虚构或模糊等问题，为此提出 OmniScaleSR 来提升其在不同放大倍数下的性能。

Method: OmniScaleSR 引入了扩散本源的尺度控制机制，协同隐式尺度适应，实现对扩散过程的感知尺度调整，同时结合多领域保真度增强设计来优化重建精度。

Result: 在双立方降级基准和真实世界数据集上的实验表明，OmniScaleSR 在准确性和感知逼真度方面超越了现有顶级方法，尤其在大放大倍数下表现突出。

Conclusion: OmniScaleSR 通过引入创新机制显著提升了扩散模型在任意尺度上的超分辨率能力，为超分辨率领域带来了新的解决方案。

Abstract: Arbitrary-scale super-resolution (ASSR) overcomes the limitation of traditional super-resolution (SR) methods that operate only at fixed scales (e.g., 4x), enabling a single model to handle arbitrary magnification. Most existing ASSR approaches rely on implicit neural representation (INR), but its regression-driven feature extraction and aggregation intrinsically limit the ability to synthesize fine details, leading to low realism. Recent diffusion-based realistic image super-resolution (Real-ISR) models leverage powerful pre-trained diffusion priors and show impressive results at the 4x setting. We observe that they can also achieve ASSR because the diffusion prior implicitly adapts to scale by encouraging high-realism generation. However, without explicit scale control, the diffusion process cannot be properly adjusted for different magnification levels, resulting in excessive hallucination or blurry outputs, especially under ultra-high scales. To address these issues, we propose OmniScaleSR, a diffusion-based realistic arbitrary-scale SR framework designed to achieve both high fidelity and high realism. We introduce explicit, diffusion-native scale control mechanisms that work synergistically with implicit scale adaptation, enabling scale-aware and content-aware modulation of the diffusion process. In addition, we incorporate multi-domain fidelity enhancement designs to further improve reconstruction accuracy. Extensive experiments on bicubic degradation benchmarks and real-world datasets show that OmniScaleSR surpasses state-of-the-art methods in both fidelity and perceptual realism, with particularly strong performance at large magnification factors. Code will be released at https://github.com/chaixinning/OmniScaleSR.

</details>


### [54] [GeoPE:A Unified Geometric Positional Embedding for Structured Tensors](https://arxiv.org/abs/2512.04963)
*Yupu Yao,Bowen Yang*

Main category: cs.CV

TL;DR: 文章提出了一种名为Geometric Positional Embedding (GeoPE)的方法，通过将旋转扩展到3D欧几里得空间并使用quaternions解决非交换性问题，GeoPE能够在图像分类、对象检测和3D语义分割任务中表现出色，较现有方法有显著提升。


<details>
  <summary>Details</summary>
Motivation: 传统Vision Transformers将2D图像扁平化为1D序列，破坏了自然的空间拓扑结构。虽然RoPE在1D上表现出色，但它也继承了这一局限性，导致空间上相距甚远的片段被误认为顺序邻居。现有的2D方法通常会独立处理空间轴，无法消除这种虚假的顺序接近性与真实的空间距离之间的耦合。

Method: GeoPE方法通过使用quaternions将旋转扩展到3D欧几里得空间，并通过计算李代数中的几何平均值构建统一的旋转操作符，来克服非交换性并确保对称性。这种方法生成了几何上耦合的嵌入编码，有效地区分了空间维度。

Result: 实验结果表明，GeoPE在图像分类、对象检测和3D语义分割任务上表现出色，显著优于现有的2D RoPE变体，并且能够更好地捕捉真正的几何结构。

Conclusion: 文章结论表明，GeoPE通过重建2D空间流形，提升了模型对图像空间结构的理解和处理效果。

Abstract: Standard Vision Transformers flatten 2D images into 1D sequences, disrupting the natural spatial topology. While Rotary Positional Embedding (RoPE) excels in 1D, it inherits this limitation, often treating spatially distant patches (e.g., at row edges) as sequence neighbors. Existing 2D approaches typically treat spatial axes independently, failing to decouple this false sequential proximity from true spatial distance. To restore the 2D spatial manifold, we introduce Geometric Positional Embedding (GeoPE), a framework that extends rotations to 3D Euclidean space using quaternions. To overcome non-commutativity and ensure symmetry, GeoPE constructs a unified rotational operator by computing the geometric mean in the Lie algebra. This creates a geometrically coupled encoding that effectively separates spatial dimensions. Extensive experiments on image classification, object detection, and 3D semantic segmentation demonstrate that GeoPE consistently outperforms existing 2D RoPE variants and significantly enhances shape bias, confirming its ability to capture true geometric structure.

</details>


### [55] [Balanced Few-Shot Episodic Learning for Accurate Retinal Disease Diagnosis](https://arxiv.org/abs/2512.04967)
*Jasmaine Khale,Ravi Prakash Srivastava*

Main category: cs.CV

TL;DR: 本研究提出了一种针对 Retinal Fundus Multi-Disease Image Dataset 的平衡少量样本 episodic 学习框架，通过平衡的采样、针对性的数据增强和预训练的 ResNet-50 编码器，提高了稀有疾病的准确度并减少了对常见疾病的偏见。


<details>
  <summary>Details</summary>
Motivation: 由于糖尿病视网膜病变和黄斑变性等疾病的高发病率，自动化视网膜疾病诊断非常重要。然而，传统的深度学习方法依赖于大量标注数据，这些数据成本高昂且类别间往往不均衡，限制了它们在实际应用中的可靠性。少量样本学习可以解决这个问题，使模型能够在从每个类少量标签样本中泛化。

Method: 该方法提出了一种平衡的少量样本 episodic 学习框架，针对 Retinal Fundus Multi-Disease Image Dataset 设计。该框架包括三个关键组件：平衡的 episodic 抽样以确保每个 5-way 5-shot episode 中所有类别的平等参与；针对性的数据增强，包括CLAHE和颜色/几何变换，以提高少数类的多样性；以及使用在 ImageNet 上预训练的 ResNet-50 编码器，因为它具有更好的捕捉细粒度视网膜特征的能力。原型在嵌入空间中进行计算，并使用余弦相似性进行分类以提高稳定性。

Result: 该框架通过 100 次训练 episode 和 1,000 次测试 episode 的评估，实现了显著的准确度提升，并减少了对常见疾病的偏见，尤其是对稀有疾病有显著改进。

Conclusion: 此研究表明，与数据受限条件下结合了少量样本 learning 和平衡采样的数据处理方法相结合，可以提供更稳健和临床公平的视网膜疾病诊断。通过针对性的数据增强和平衡采样，可以缓解数据类别不平衡的问题，提供更可靠的诊断结果。

Abstract: Automated retinal disease diagnosis is vital given the rising prevalence of conditions such as diabetic retinopathy and macular degeneration. Conventional deep learning approaches require large annotated datasets, which are costly and often imbalanced across disease categories, limiting their reliability in practice. Few-shot learning (FSL) addresses this challenge by enabling models to generalize from only a few labeled samples per class. In this study,we propose a balanced few-shot episodic learning framework tailored to the Retinal Fundus Multi-Disease Image Dataset (RFMiD). Focusing on the ten most represented classes, which still show substantial imbalance between majority diseases (e.g., Diabetic Retinopathy, Macular Hole) and minority ones (e.g., Optic Disc Edema, Branch Retinal Vein Occlusion), our method integrates three key components: (i) balanced episodic sampling, ensuring equal participation of all classes in each 5-way 5-shot episode; (ii) targeted augmentation, including Contrast Limited Adaptive Histogram Equalization (CLAHE) and color/geometry transformations, to improve minority-class di- versity; and (iii) a ResNet-50 encoder pretrained on ImageNet, selected for its superior ability to capture fine-grained retinal features. Prototypes are computed in the embedding space and classification is performed with cosine similarity for improved stability. Trained on 100 episodes and evaluated on 1,000 test episodes, our framework achieves substantial accuracy gains and reduces bias toward majority classes, with notable improvements for underrepresented diseases. These results demonstrate that dataset-aware few-shot pipelines, combined with balanced sampling and CLAHE-enhanced preprocessing, can deliver more robust and clinically fair retinal disease diagnosis under data-constrained conditions.

</details>


### [56] [Reflection Removal through Efficient Adaptation of Diffusion Transformers](https://arxiv.org/abs/2512.05000)
*Daniyar Zakarin,Thiemo Wandel,Anton Obukhov,Dengxin Dai*

Main category: cs.CV

TL;DR: 该论文提出了一种利用预训练的扩散变换器（DiT）框架来进行单张图像反光去除的方法。它通过物理基础渲染（PBR）管道生成合成数据，并结合高效的低秩适配技术，取得了业内领先的效果。


<details>
  <summary>Details</summary>
Motivation: 随着图像处理技术的发展，反射去除成为一个重要研究方向。传统的任务特定架构存在灵活性不足的问题，因此论文提出了一种基于扩散变换器的通用框架来提高反射去除的性能。

Method: 论文利用预训练的DiT模型，并通过条件化输入的方式，使其适应反射污染的图像。此外，还开发了基于Blender的物理基础渲染（PBR）管道，通过合成真实玻璃材质与反射效果来丰富训练数据集。

Result: 实验表明，该方法在室内和零样本基准测试中都取得了业内领先的效果。与传统方法相比，该方法在处理复杂反射场景时具有更好的稳定性和精度。

Conclusion: 该研究证明了预训练的扩散变换器与物理上真实的合成数据相结合，能够为反射去除提供高性价比的解决方案，具有很好的通用性和鲁棒性。

Abstract: We introduce a diffusion-transformer (DiT) framework for single-image reflection removal that leverages the generalization strengths of foundation diffusion models in the restoration setting. Rather than relying on task-specific architectures, we repurpose a pre-trained DiT-based foundation model by conditioning it on reflection-contaminated inputs and guiding it toward clean transmission layers. We systematically analyze existing reflection removal data sources for diversity, scalability, and photorealism. To address the shortage of suitable data, we construct a physically based rendering (PBR) pipeline in Blender, built around the Principled BSDF, to synthesize realistic glass materials and reflection effects. Efficient LoRA-based adaptation of the foundation model, combined with the proposed synthetic data, achieves state-of-the-art performance on in-domain and zero-shot benchmarks. These results demonstrate that pretrained diffusion transformers, when paired with physically grounded data synthesis and efficient adaptation, offer a scalable and high-fidelity solution for reflection removal. Project page: https://hf.co/spaces/huawei-bayerlab/windowseat-reflection-removal-web

</details>


### [57] [Order Matters: 3D Shape Generation from Sequential VR Sketches](https://arxiv.org/abs/2512.04761)
*Yizi Chen,Sidi Wu,Tianyi Xiao,Nina Wiedemann,Loic Landrieu*

Main category: cs.CV

TL;DR: 研究提出了一种名为VRSketch2Shape的新框架和多类别数据集，用于生成3D形状。该方法能够从任意形状自动生成顺序VR素描，并结合差分基于的3D生成器实现对顺序素描的理解。


<details>
  <summary>Details</summary>
Motivation: 现有技术忽略了素描顺序，VRSketch2Shape旨在填补这一空白。

Method: VRSketch2Shape具有一个自动化的生成顺序VR素描的管道，一个多类别数据集包含超过2万个合成和900个手绘素描-形状配对，以及一个意识顺序的素描编码器和基于扩散的3D生成器。

Result: 实验结果表明，此方法在几何保真度方面优于之前的方法，能够在最少的监督下从合成素描到真实素描进行有效泛化，甚至在部分素描上也能表现良好。

Conclusion: VRSketch2Shape框架展示了其在生成3D模型方面的高效性和实用价值，所有数据和模型将开源。

Abstract: VR sketching lets users explore and iterate on ideas directly in 3D, offering a faster and more intuitive alternative to conventional CAD tools. However, existing sketch-to-shape models ignore the temporal ordering of strokes, discarding crucial cues about structure and design intent. We introduce VRSketch2Shape, the first framework and multi-category dataset for generating 3D shapes from sequential VR sketches. Our contributions are threefold: (i) an automated pipeline that generates sequential VR sketches from arbitrary shapes, (ii) a dataset of over 20k synthetic and 900 hand-drawn sketch-shape pairs across four categories, and (iii) an order-aware sketch encoder coupled with a diffusion-based 3D generator. Our approach yields higher geometric fidelity than prior work, generalizes effectively from synthetic to real sketches with minimal supervision, and performs well even on partial sketches. All data and models will be released open-source at https://chenyizi086.github.io/VRSketch2Shape_website.

</details>


### [58] [SA-IQA: Redefining Image Quality Assessment for Spatial Aesthetics with Multi-Dimensional Rewards](https://arxiv.org/abs/2512.05098)
*Yuan Gao,Jin Song*

Main category: cs.CV

TL;DR: 本文介绍了一种称为Spatial Aesthetics的新方法，用于评估AI生成图像中的室内场景美学质量，并构建了首个Spatial Aesthetics基准SA-BENCH。通过SA-BENCH，作者评测了当前的图像质量评估方法并开发了SA-IQA，这是一种结合MLLM微调和多维度融合的方法。SA-IQA被应用于两个下游任务：用于GRPO强化学习以优化AIGC生成管道；和Best-of-N选择以筛选高质量图像。实验表明，SA-IQA在SA-BENCH上领先于现有方法，为室内场景美学评估设定新标准。


<details>
  <summary>Details</summary>
Motivation: 当前的图像质量评估方法主要针对肖像和艺术图像，缺乏对室内场景的系统性评价。因此，作者旨在开发一种新的方法来评估AI生成图像中的室内场景美学质量。

Method: 作者首先构建了SA-BENCH，包含18,000张图像及其50,000个精确注释。然后，利用SA-BENCH评估了现有的图像质量评估（IQA）方法，并开发了SA-IQA，通过MLLM微调和多维度融合，构建了一个全面的奖励框架。

Result: 通过SA-IQA，作者成功地优化了AI生成图像中室内场景的美学质量，并将其应用于两个下游任务，显示出SA-IQA比现有方法更优越。

Conclusion: 文章证明了SA-IQA方法的有效性，并为室内场景美学评估设定了新的标准，相关代码和数据集将对外开放，以促进该领域的研究和应用。

Abstract: In recent years, Image Quality Assessment (IQA) for AI-generated images (AIGI) has advanced rapidly; however, existing methods primarily target portraits and artistic images, lacking a systematic evaluation of interior scenes. We introduce Spatial Aesthetics, a paradigm that assesses the aesthetic quality of interior images along four dimensions: layout, harmony, lighting, and distortion. We construct SA-BENCH, the first benchmark for spatial aesthetics, comprising 18,000 images and 50,000 precise annotations. Employing SA-BENCH, we systematically evaluate current IQA methodologies and develop SA-IQA, through MLLM fine-tuning and a multidimensional fusion approach, as a comprehensive reward framework for assessing spatial aesthetics. We apply SA-IQA to two downstream tasks: (1) serving as a reward signal integrated with GRPO reinforcement learning to optimize the AIGC generation pipeline, and (2) Best-of-N selection to filter high-quality images and improve generation quality. Experiments indicate that SA-IQA significantly outperforms existing methods on SA-BENCH, setting a new standard for spatial aesthetics evaluation. Code and dataset will be open-sourced to advance research and applications in this domain.

</details>


### [59] [PaCo-RL: Advancing Reinforcement Learning for Consistent Image Generation with Pairwise Reward Modeling](https://arxiv.org/abs/2512.04784)
*Bowen Ping,Chengyou Jia,Minnan Luo,Changliang Xia,Xin Shen,Zhuohang Dang,Hangwei Qian*

Main category: cs.CV

TL;DR: 该研究提出了一种名为PaCo-RL的框架，通过结合专门的一致性奖励模型和高效的强化学习算法，实现了更加一致且直观的图像生成。


<details>
  <summary>Details</summary>
Motivation: 现有的监督训练方法在构建大规模一致性数据集和模拟人类感知偏好方面存在困难，因此需要一种新的方法来学习复杂的和主观的视觉标准。

Method: PaCo-RL框架通过引入PaCo-Reward(一种经过大量数据训练的两两一致性评估器)和PaCo-GRPO(一种新型分辨率解耦优化策略)，结合任务感知指令和CoT推理，以及多奖励聚合机制，实现了一种数据驱动的方式学习视觉一致性。

Result: 实验表明，PaCo-Reward显著提高了与人类对视觉一致性的感知的一致性，而PaCo-GRPO则在一致性性能上达到了最先进的水平，且具有更高的训练效率和稳定性。

Conclusion: PaCo-RL的这些结果表明，强化学习可以作为一种实际且可扩展的方案来解决一致图像生成的问题。

Abstract: Consistent image generation requires faithfully preserving identities, styles, and logical coherence across multiple images, which is essential for applications such as storytelling and character design. Supervised training approaches struggle with this task due to the lack of large-scale datasets capturing visual consistency and the complexity of modeling human perceptual preferences. In this paper, we argue that reinforcement learning (RL) offers a promising alternative by enabling models to learn complex and subjective visual criteria in a data-free manner. To achieve this, we introduce PaCo-RL, a comprehensive framework that combines a specialized consistency reward model with an efficient RL algorithm. The first component, PaCo-Reward, is a pairwise consistency evaluator trained on a large-scale dataset constructed via automated sub-figure pairing. It evaluates consistency through a generative, autoregressive scoring mechanism enhanced by task-aware instructions and CoT reasons. The second component, PaCo-GRPO, leverages a novel resolution-decoupled optimization strategy to substantially reduce RL cost, alongside a log-tamed multi-reward aggregation mechanism that ensures balanced and stable reward optimization. Extensive experiments across the two representative subtasks show that PaCo-Reward significantly improves alignment with human perceptions of visual consistency, and PaCo-GRPO achieves state-of-the-art consistency performance with improved training efficiency and stability. Together, these results highlight the promise of PaCo-RL as a practical and scalable solution for consistent image generation. The project page is available at https://x-gengroup.github.io/HomePage_PaCo-RL/.

</details>


### [60] [LaFiTe: A Generative Latent Field for 3D Native Texturing](https://arxiv.org/abs/2512.04786)
*Chia-Hao Chen,Zi-Xin Zou,Yan-Pei Cao,Ze Yuan,Guan Luo,Xiaojuan Qi,Ding Liang,Song-Hai Zhang,Yuan-Chen Guo*

Main category: cs.CV

TL;DR: 本文介绍了一种名为LaFiTe的框架，通过学习生成3D稀疏隐色空间来填补纹理生成中的代表空缺，从而实现3D表面高保真无缝纹理的直接生成，该方法在重建中超越了现有最先进的方法。


<details>
  <summary>Details</summary>
Motivation: 现有的3D纹理化方法要么受限于UV映射、多视角投影的局限，要么缺乏强大的、多功能的潜在表示，这限制了纹理生成的质量和普遍性。

Method: LaFiTe框架采用变分自编码器（VAE）将复杂的表观信息编码为稀疏结构的潜在空间，然后解码为连续的色场。通过这种方法，实现了前所未有的高保真度，并且通过有效地解纠缠纹理外观与网格拓扑和UV参数化。

Result: 实验表明，LaFiTe不仅在3D纹理生成方面树立了新的基准，还开启了材料合成和纹理超分辨率等下游应用的灵活性，为下一代3D内容创作流程铺平了道路。

Conclusion: LaFiTe框架通过创新的潜在表示和合成模型，推动了3D纹理生成领域的发展，展示了其在复杂几何和多样风格下的高质并连贯纹理生成能力。

Abstract: Generating high-fidelity, seamless textures directly on 3D surfaces, what we term 3D-native texturing, remains a fundamental open challenge, with the potential to overcome long-standing limitations of UV-based and multi-view projection methods. However, existing native approaches are constrained by the absence of a powerful and versatile latent representation, which severely limits the fidelity and generality of their generated textures. We identify this representation gap as the principal barrier to further progress. We introduce LaFiTe, a framework that addresses this challenge by learning to generate textures as a 3D generative sparse latent color field. At its core, LaFiTe employs a variational autoencoder (VAE) to encode complex surface appearance into a sparse, structured latent space, which is subsequently decoded into a continuous color field. This representation achieves unprecedented fidelity, exceeding state-of-the-art methods by >10 dB PSNR in reconstruction, by effectively disentangling texture appearance from mesh topology and UV parameterization. Building upon this strong representation, a conditional rectified-flow model synthesizes high-quality, coherent textures across diverse styles and geometries. Extensive experiments demonstrate that LaFiTe not only sets a new benchmark for 3D-native texturing but also enables flexible downstream applications such as material synthesis and texture super-resolution, paving the way for the next generation of 3D content creation workflows.

</details>


### [61] [ShadowDraw: From Any Object to Shadow-Drawing Compositional Art](https://arxiv.org/abs/2512.05110)
*Rundong Luo,Noah Snavely,Wei-Chiu Ma*

Main category: cs.CV

TL;DR: ShadowDraw 是一个框架，能将普通3D物体转化为阴影绘画组成艺术。该系统预测场景参数并生成部分线稿，让投影的阴影完成绘画，形成可识别的图像。该方法适用于多种3D模型，支持多对象场景、动画及物理部署，并推动计算视觉艺术的发展。


<details>
  <summary>Details</summary>
Motivation: 为了弥合算法设计与艺术叙事之间的差距，创造可拓展的、高视觉质量的阴影绘画艺术作品。

Method: 优化场景配置以揭示有意义的阴影；用阴影笔触指导线稿生成；自动评估以确保阴影绘画的连贯性和视觉质量。

Result: ShadowDraw 能够跨多种输入生成引人入胜的结果，包括真实世界的扫描、定制数据集和生成的资产。此外，它还可应用于多对象场景、动画和实际部署。

Conclusion: 该工作提供了一个实用的管道，可用于创建阴影绘画艺术，扩展了计算视觉艺术的设计空间。

Abstract: We introduce ShadowDraw, a framework that transforms ordinary 3D objects into shadow-drawing compositional art. Given a 3D object, our system predicts scene parameters, including object pose and lighting, together with a partial line drawing, such that the cast shadow completes the drawing into a recognizable image. To this end, we optimize scene configurations to reveal meaningful shadows, employ shadow strokes to guide line drawing generation, and adopt automatic evaluation to enforce shadow-drawing coherence and visual quality. Experiments show that ShadowDraw produces compelling results across diverse inputs, from real-world scans and curated datasets to generative assets, and naturally extends to multi-object scenes, animations, and physical deployments. Our work provides a practical pipeline for creating shadow-drawing art and broadens the design space of computational visual art, bridging the gap between algorithmic design and artistic storytelling. Check out our project page https://red-fairy.github.io/ShadowDraw/ for more results and an end-to-end real-world demonstration of our pipeline!

</details>


### [62] [RobustSplat++: Decoupling Densification, Dynamics, and Illumination for In-the-Wild 3DGS](https://arxiv.org/abs/2512.04815)
*Chuanyu Fu,Guanying Chen,Yuqi Zhang,Kunbin Yao,Yuan Xiong,Chuan Huang,Shuguang Cui,Yasuyuki Matsushita,Xiaochun Cao*

Main category: cs.CV

TL;DR: RobustSplat++通过引入延迟高斯生长策略和尺度递进掩码启动方法，解决了3DGS方法在处理动态物体和光照变化时产生的渲染缺陷，从而在真实场景建模中取得显著效果。


<details>
  <summary>Details</summary>
Motivation: 当前的3DGS方法在处理动态物体和光照变化时容易产生渲染缺陷，RobustSplat++旨在解决这一问题，提供更加稳健和有效的解决方案。

Method: RobustSplat++方法包括延迟高斯生长策略和尺度递进掩码启动方法，结合外观建模来处理动态物体和光照变化。

Result: RobustSplat++在多个具有挑战性的数据集上表现出色，相较于现有方法具有更高的稳健性和有效性。

Conclusion: RobustSplat++通过优化静态场景结构，并结合掩码启动方法和外观建模，显著提高了3DGS方法在实时、高保真渲染中的性能。

Abstract: 3D Gaussian Splatting (3DGS) has gained significant attention for its real-time, photo-realistic rendering in novel-view synthesis and 3D modeling. However, existing methods struggle with accurately modeling in-the-wild scenes affected by transient objects and illuminations, leading to artifacts in the rendered images. We identify that the Gaussian densification process, while enhancing scene detail capture, unintentionally contributes to these artifacts by growing additional Gaussians that model transient disturbances and illumination variations. To address this, we propose RobustSplat++, a robust solution based on several critical designs. First, we introduce a delayed Gaussian growth strategy that prioritizes optimizing static scene structure before allowing Gaussian splitting/cloning, mitigating overfitting to transient objects in early optimization. Second, we design a scale-cascaded mask bootstrapping approach that first leverages lower-resolution feature similarity supervision for reliable initial transient mask estimation, taking advantage of its stronger semantic consistency and robustness to noise, and then progresses to high-resolution supervision to achieve more precise mask prediction. Third, we incorporate the delayed Gaussian growth strategy and mask bootstrapping with appearance modeling to handling in-the-wild scenes including transients and illuminations. Extensive experiments on multiple challenging datasets show that our method outperforms existing methods, clearly demonstrating the robustness and effectiveness of our method.

</details>


### [63] [LatentFM: A Latent Flow Matching Approach for Generative Medical Image Segmentation](https://arxiv.org/abs/2512.04821)
*Huynh Trinh Ngoc,Hoang Anh Nguyen Kim,Toan Nguyen Hai,Long Tran Quoc*

Main category: cs.CV

TL;DR: 本文提出了一种基于流的医学图像分割方法LatentFM，使用两个变分自编码器将图像和掩码编码到低维的潜在空间，并通过条件速度场生成多样的分割输出，同时生成置信图以量化模型的确定性。


<details>
  <summary>Details</summary>
Motivation: 提出了基于流匹配（FM）的LatentFM方法，以解决医学图像分割中数据分布建模的挑战，通过潜在空间学习数据密度。

Method: 首先设计两个VAE分别编码图像和掩码到低维空间；然后估计条件速度场来指导流；最后通过采样潜在表示生成多样化的分割输出。

Result: 在ISIC-2018和CVC-Clinic数据集上，LatentFM方法在定量和定性评估中均表现出色，实现了高精度和不确定性感知的分割。

Conclusion: 通过在潜在空间建模，证明了LatentFM方法在医学图像分割中的优越性和高效性。

Abstract: Generative models have achieved remarkable progress with the emergence of flow matching (FM). It has demonstrated strong generative capabilities and attracted significant attention as a simulation-free flow-based framework capable of learning exact data densities. Motivated by these advances, we propose LatentFM, a flow-based model operating in the latent space for medical image segmentation. To model the data distribution, we first design two variational autoencoders (VAEs) to encode both medical images and their corresponding masks into a lower-dimensional latent space. We then estimate a conditional velocity field that guides the flow based on the input image. By sampling multiple latent representations, our method synthesizes diverse segmentation outputs whose pixel-wise variance reliably captures the underlying data distribution, enabling both highly accurate and uncertainty-aware predictions. Furthermore, we generate confidence maps that quantify the model certainty, providing clinicians with richer information for deeper analysis. We conduct experiments on two datasets, ISIC-2018 and CVC-Clinic, and compare our method with several prior baselines, including both deterministic and generative approach models. Through comprehensive evaluations, both qualitative and quantitative results show that our approach achieves superior segmentation accuracy while remaining highly efficient in the latent space.

</details>


### [64] [FreeGen: Feed-Forward Reconstruction-Generation Co-Training for Free-Viewpoint Driving Scene Synthesis](https://arxiv.org/abs/2512.04830)
*Shijie Chen,Peixi Peng*

Main category: cs.CV

TL;DR: FreeGen 提出了一种新颖的前馈重建-生成联合训练框架，用于自由视角驾驶场景合成。通过共享几何表示，该模型实现了在未见过视角下的几何感知增强，并且增强了生成模型的结构引导，从而在自由视角驾驶场景合成上达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的自动驾驶场景生成方法难以同时实现插值一致性与未见过视角的逼真度。FreeGen 模型通过联合训练重建和生成过程来解决这一问题。

Method: FreeGen 使用一种前馈的重建-生成联合训练框架来合成自由视角驾驶场景。重建模型提供稳定的几何表征以确保插值一致性，生成模型进行几何感知增强以提高未见过视角下的视觉真实性。

Result: 实验结果显示，FreeGen 在自由视角驾驶场景合成上达到了最先进的性能。

Conclusion: FreeGen 模型通过联合训练机制，有效提高了生成模型的几何一致性与视觉真实性，为自动驾驶领域的自由视角场景合成提供了新的解决方案。

Abstract: Closed-loop simulation and scalable pre-training for autonomous driving require synthesizing free-viewpoint driving scenes. However, existing datasets and generative pipelines rarely provide consistent off-trajectory observations, limiting large-scale evaluation and training. While recent generative models demonstrate strong visual realism, they struggle to jointly achieve interpolation consistency and extrapolation realism without per-scene optimization. To address this, we propose FreeGen, a feed-forward reconstruction-generation co-training framework for free-viewpoint driving scene synthesis. The reconstruction model provides stable geometric representations to ensure interpolation consistency, while the generation model performs geometry-aware enhancement to improve realism at unseen viewpoints. Through co-training, generative priors are distilled into the reconstruction model to improve off-trajectory rendering, and the refined geometry in turn offers stronger structural guidance for generation. Experiments demonstrate that FreeGen achieves state-of-the-art performance for free-viewpoint driving scene synthesis.

</details>


### [65] [Tokenizing Buildings: A Transformer for Layout Synthesis](https://arxiv.org/abs/2512.04832)
*Manuel Ladron de Guevara,Jinmo Rhee,Ardavan Bidgoli,Vaidas Razgaitis,Michael Bergin*

Main category: cs.CV

TL;DR: SBM是一种基于Transformer的架构，用于BIM场景中的布局合成。它通过将异构的建筑元素特征统一封装成序列，同时保持组合结构，学习房间的联合表示。在两个不同的训练模式下，SBM可以生成高质量的房间嵌入或自回归预测房间实体。


<details>
  <summary>Details</summary>
Motivation: 在BIM场景中，如何有效地对建筑物进行分词，并在保持组成结构的同时将异构特征集统一表示为顺序。

Method: SBM采用了一种新颖的分词方法，定义了一个统一的嵌入模块来学习类别和相关连续特征组的联合表示。通过将基于Transformer的架构应用于编码器和编码器-解码器管道，从多维特征数据集中构建了具有高保真的房间嵌入。

Result: 实验表明，SBM可以学习到紧凑的房间嵌入，它们可靠地按照类型和拓扑结构聚类，从而能在检索和生成布局合成中提供强大的语义检索能力。在数据驱动实体预测模式下，会生成功能合理的布局，具有较少的碰撞和边界违反，以及改进的导航性。

Conclusion: SBM展示了在BIM场景中有效进行布局合成的潜力，并证明了其在检索和生成布局合成中的实用性。

Abstract: We introduce Small Building Model (SBM), a Transformer-based architecture for layout synthesis in Building Information Modeling (BIM) scenes. We address the question of how to tokenize buildings by unifying heterogeneous feature sets of architectural elements into sequences while preserving compositional structure. Such feature sets are represented as a sparse attribute-feature matrix that captures room properties. We then design a unified embedding module that learns joint representations of categorical and possibly correlated continuous feature groups. Lastly, we train a single Transformer backbone in two modes: an encoder-only pathway that yields high-fidelity room embeddings, and an encoder-decoder pipeline for autoregressive prediction of room entities, referred to as Data-Driven Entity Prediction (DDEP). Experiments across retrieval and generative layout synthesis show that SBM learns compact room embeddings that reliably cluster by type and topology, enabling strong semantic retrieval. In DDEP mode, SBM produces functionally sound layouts, with fewer collisions and boundary violations and improved navigability.

</details>


### [66] [A Sanity Check for Multi-In-Domain Face Forgery Detection in the Real World](https://arxiv.org/abs/2512.04837)
*Jikang Cheng,Renye Yan,Zhiyuan Yan,Yaozhong Gan,Xueyi Zhang,Zhongyuan Wang,Wei Peng,Ling Liang*

Main category: cs.CV

TL;DR: 本文提出了一种新的多域面部伪造检测（MID-FFD）研究范式，旨在解决深度伪造检测中不同域之间的特征空间问题。通过Proposed框架DevDet，增强了真实与伪造之间的差异，实现了对未见数据的良好泛化。


<details>
  <summary>Details</summary>
Motivation: 现有的深度伪造检测方法旨在开发通用检测器，但面对未见的多样性变化时，实际应用中的泛化能力受限。因此，提出MID-FFD范式，旨在为检测器提供足够多样化的训练数据，以应对多域场景下的单一图像真实/伪造判定难题。

Method: 该方法首先定义了MID-FFD范式，包括足够多样化的真实与伪造域数据进行训练，然后提出了一个模型无关的框架DevDet，包含Face Forgery Developer（FFDev）和Dose-Adaptive detector Fine-Tuning strategy（DAFT）两个子模块。

Result: 实验结果表明，新方法在MID-FFD场景下，对真实与伪造的预测效果更佳，同时保留了对未见数据的泛化能力。

Conclusion: DevDet框架的成功展示了改善深度伪造检测中多域差异问题的路径，并为实际应用提供了更有效的解决方案。

Abstract: Existing methods for deepfake detection aim to develop generalizable detectors. Although "generalizable" is the ultimate target once and for all, with limited training forgeries and domains, it appears idealistic to expect generalization that covers entirely unseen variations, especially given the diversity of real-world deepfakes. Therefore, introducing large-scale multi-domain data for training can be feasible and important for real-world applications. However, within such a multi-domain scenario, the differences between multiple domains, rather than the subtle real/fake distinctions, dominate the feature space. As a result, despite detectors being able to relatively separate real and fake within each domain (i.e., high AUC), they struggle with single-image real/fake judgments in domain-unspecified conditions (i.e., low ACC). In this paper, we first define a new research paradigm named Multi-In-Domain Face Forgery Detection (MID-FFD), which includes sufficient volumes of real-fake domains for training. Then, the detector should provide definitive real-fake judgments to the domain-unspecified inputs, which simulate the frame-by-frame independent detection scenario in the real world. Meanwhile, to address the domain-dominant issue, we propose a model-agnostic framework termed DevDet (Developer for Detector) to amplify real/fake differences and make them dominant in the feature space. DevDet consists of a Face Forgery Developer (FFDev) and a Dose-Adaptive detector Fine-Tuning strategy (DAFT). Experiments demonstrate our superiority in predicting real-fake under the MID-FFD scenario while maintaining original generalization ability to unseen data.

</details>


### [67] [Autoregressive Image Generation Needs Only a Few Lines of Cached Tokens](https://arxiv.org/abs/2512.04857)
*Ziran Qin,Youru Lv,Mingbao Lin,Zeren Zhang,Chanfan Gan,Tieyuan Chen,Weiyao Lin*

Main category: cs.CV

TL;DR: LineAR是一种无需训练的新颖方法，通过按行管理缓存，减少视觉注意中的无用信息，实现高效的自回归图像生成，同时节省内存并提高吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有的自回归图像生成方法因其需要存储所有先前生成的视觉标记而导致内存瓶颈，从而限制了其存储需求和处理速度。LineAR提出了一个新鲜的方法，通过巧妙地利用视觉注意力的内在特征，有效地解决了这一问题。

Method: LineAR通过利用视觉注意力的内在特性，在二维视图的基础上，按行级别管理缓存。它通过行间注意力引导，逐行移除对于后续行生成无害的信息量较少的视觉标记，从而实现高效的自回归图像生成，同时减少了所需缓存的大小。

Result: 在六种不同类型的自回归图像生成模型上，LineAR取得了显著的效果，包括类别条件和文本图像生成。例如，它在LlamaGen-XL和Janus-Pro-1B上分别将ImageNet FID从2.77降至2.68，COCO FID从23.85降至22.86，同时只保留了tokenId的1/6；在LlamaGen-XL上实现了多达67.61%的内存减少和7.57倍的吞吐量提升，而在Janus-Pro-7B上则实现了39.66%的内存减少和5.62倍的吞吐量提升。

Conclusion: LineAR通过按行高效管理缓存，显著降低了内存占用并大幅提升了性能，在自回归图像生成领域展现出了新的潜力。

Abstract: Autoregressive (AR) visual generation has emerged as a powerful paradigm for image and multimodal synthesis, owing to its scalability and generality. However, existing AR image generation suffers from severe memory bottlenecks due to the need to cache all previously generated visual tokens during decoding, leading to both high storage requirements and low throughput. In this paper, we introduce \textbf{LineAR}, a novel, training-free progressive key-value (KV) cache compression pipeline for autoregressive image generation. By fully exploiting the intrinsic characteristics of visual attention, LineAR manages the cache at the line level using a 2D view, preserving the visual dependency regions while progressively evicting less-informative tokens that are harmless for subsequent line generation, guided by inter-line attention. LineAR enables efficient autoregressive (AR) image generation by utilizing only a few lines of cache, achieving both memory savings and throughput speedup, while maintaining or even improving generation quality. Extensive experiments across six autoregressive image generation models, including class-conditional and text-to-image generation, validate its effectiveness and generality. LineAR improves ImageNet FID from 2.77 to 2.68 and COCO FID from 23.85 to 22.86 on LlamaGen-XL and Janus-Pro-1B, while retaining only 1/6 KV cache. It also improves DPG on Lumina-mGPT-768 with just 1/8 KV cache. Additionally, LineAR achieves significant memory and throughput gains, including up to 67.61% memory reduction and 7.57x speedup on LlamaGen-XL, and 39.66% memory reduction and 5.62x speedup on Janus-Pro-7B.

</details>


### [68] [Contact-Aware Refinement of Human Pose Pseudo-Ground Truth via Bioimpedance Sensing](https://arxiv.org/abs/2512.04862)
*Maria-Paola Forte,Nikos Athanasiou,Giulia Ballardini,Jan Ulrich Bartels,Katherine J. Kuchenbecker,Michael J. Black*

Main category: cs.CV

TL;DR: 本文提出了一种结合视觉姿态估计和电导率传感的方法，以捕捉考虑自我接触的人的3D姿态，该方法在三种输入姿态估计器的测试中平均提高了11.7%的重建准确性。


<details>
  <summary>Details</summary>
Motivation: 当前的基于视频的姿态估计方法在实际场景中往往会因自我接触情况失效，如手触碰脸部。使用穿戴式生物阻抗传感器可以无痕地测量皮肤接触的真实数据，本文提出的方法通过结合视觉姿态估计与生物阻抗传感，能够更准确地估计在自我接触情况下的3D姿态。

Method: 该方法首先使用现成的姿势估计器初始化姿势，然后在检测到自我接触时进行接触感知的姿势优化。优化过程同时最小化重建误差和与初始姿势估计的偏差，同时保持顶点邻近约束。

Result: 使用新的同步RGB视频、生物阻抗测量和3D动作捕捉数据集验证了该方法。通过三种输入姿势估计器测试，方法在重建准确度上平均提高了11.7%。

Conclusion: 本文还介绍了高效的穿戴式生物阻抗传感器，使其能够在大规模收集带自接触感知的训练数据时改善姿态估计和生成。用户可访问 biotuch.is.tue.mpg.de 获取代码和数据。

Abstract: Capturing accurate 3D human pose in the wild would provide valuable data for training pose estimation and motion generation methods. While video-based estimation approaches have become increasingly accurate, they often fail in common scenarios involving self-contact, such as a hand touching the face. In contrast, wearable bioimpedance sensing can cheaply and unobtrusively measure ground-truth skin-to-skin contact. Consequently, we propose a novel framework that combines visual pose estimators with bioimpedance sensing to capture the 3D pose of people by taking self-contact into account. Our method, BioTUCH, initializes the pose using an off-the-shelf estimator and introduces contact-aware pose optimization during measured self-contact: reprojection error and deviations from the input estimate are minimized while enforcing vertex proximity constraints. We validate our approach using a new dataset of synchronized RGB video, bioimpedance measurements, and 3D motion capture. Testing with three input pose estimators, we demonstrate an average of 11.7% improvement in reconstruction accuracy. We also present a miniature wearable bioimpedance sensor that enables efficient large-scale collection of contact-aware training data for improving pose estimation and generation using BioTUCH. Code and data are available at biotuch.is.tue.mpg.de

</details>


### [69] [SP-Det: Self-Prompted Dual-Text Fusion for Generalized Multi-Label Lesion Detection](https://arxiv.org/abs/2512.04875)
*Qing Xu,Yanqian Wang,Xiangjian Hea,Yue Li,Yixuan Zhang,Rong Qu,Wenting Duan,Zhen Chen*

Main category: cs.CV

TL;DR: 提出了一种全新的自提示检测框架SP-Det，能够自动生成丰富文本上下文引导多标记病灶检测，无需专家注释，实验结果表明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的提示检测框架依赖于手动注释作为提示，这在临床应用中是劳动密集且不实际的。

Method: SP-Det包含一个专家自由的双文本提示生成器（DTPG）用于生成两个互补的文本模态：语义上下文提示和疾病信标提示，以及一个双向特征增强器（BFE）来综合全面的诊断上下文与疾病特定嵌入，以显著提高特征表示和检测准确性。

Result: 在两个具有不同胸部疾病类别的胸片数据集上的实验结果表明，SP-Det框架在不依赖专家注释提示的情况下性能优于现有方法。

Conclusion: SP-Det框架提出了一个有效的自动提示生成方法和双向特征提升技术，用于胸片上的多标记病灶检测，克服了现有方法的不足，展示了其在实际临床应用中的潜力。

Abstract: Automated lesion detection in chest X-rays has demonstrated significant potential for improving clinical diagnosis by precisely localizing pathological abnormalities. While recent promptable detection frameworks have achieved remarkable accuracy in target localization, existing methods typically rely on manual annotations as prompts, which are labor-intensive and impractical for clinical applications. To address this limitation, we propose SP-Det, a novel self-prompted detection framework that automatically generates rich textual context to guide multi-label lesion detection without requiring expert annotations. Specifically, we introduce an expert-free dual-text prompt generator (DTPG) that leverages two complementary textual modalities: semantic context prompts that capture global pathological patterns and disease beacon prompts that focus on disease-specific manifestations. Moreover, we devise a bidirectional feature enhancer (BFE) that synergistically integrates comprehensive diagnostic context with disease-specific embeddings to significantly improve feature representation and detection accuracy. Extensive experiments on two chest X-ray datasets with diverse thoracic disease categories demonstrate that our SP-Det framework outperforms state-of-the-art detection methods while completely eliminating the dependency on expert-annotated prompts compared to existing promptable architectures.

</details>


### [70] [SDG-Track: A Heterogeneous Observer-Follower Framework for High-Resolution UAV Tracking on Embedded Platforms](https://arxiv.org/abs/2512.04883)
*Jiawen Wen,Yu Hu,Suixuan Qiu,Jinshan Huang,Xiaowen Chu*

Main category: cs.CV

TL;DR: SDG-Track 提出了一种 Sparse Detection-Guided Tracker，采用 Observer-Follower 架构解决小 UAV 实时追踪的分辨率和速度冲突。Observer 使用高容量探测器以低频提供准确的位置锚点，Follower 则通过 ROI 限制稀疏光流进行高频轨迹插值。引入的 Dual-Space Recovery 机制在遮挡或模型漂移时提供无训练的重新获取机制，结合颜色直方图匹配和几何一致性约束。实验展示了 SDG-Track 达到 35.1 FPS，保持了 97.2% 的检测精度，并成功追踪了敏捷的 FPV 无人机。


<details>
  <summary>Details</summary>
Motivation: 现有的实时追踪解决方案面临分辨率和速度之间的固有冲突，高分辨率图像无法被标准检测器处理，而直接在资源受限的平台上处理原生高分辨率帧不能满足实时性需求。SDG-Track 通过 Observer-Follower 架构提出了一种解决方案，以在低分辨率下保持准确位置检测，并通过高频轨迹插值实现平滑控制。

Method: SDG-Track 采用 Observer-Follower 架构，Observer 部分利用 GPU 运行高容量检测器，以较低频率处理 1920x1080 帧；Follower 则利用 CPU 进行高频率的轨迹插值，通过 ROI 栏约束的稀疏光流。此外，引入了 Dual-Space Recovery 机制来应对遮挡或模型漂移，结合颜色直方图匹配和几何一致性约束进行无训练的重新获取。

Result: 实验结果表明，SDG-Track 达到了每秒 35.1 帧 (FPS) 的系统吞吐量，同时保持了 97.2% 的帧间检测精度。该系统在真实世界操作条件下成功追踪了敏捷的 FPV 无人机，验证了其有效性和实用价值。

Conclusion: SDG-Track 通过 Observer-Follower 架构和 Dual-Space Recovery 机制解决了资源受限平台上的小 UAV 实时追踪问题，证明了其在实际应用场景中的适用性和优越性。

Abstract: Real-time tracking of small unmanned aerial vehicles (UAVs) on edge devices faces a fundamental resolution-speed conflict. Downsampling high-resolution imagery to standard detector input sizes causes small target features to collapse below detectable thresholds. Yet processing native 1080p frames on resource-constrained platforms yields insufficient throughput for smooth gimbal control. We propose SDG-Track, a Sparse Detection-Guided Tracker that adopts an Observer-Follower architecture to reconcile this conflict. The Observer stream runs a high-capacity detector at low frequency on the GPU to provide accurate position anchors from 1920x1080 frames. The Follower stream performs high-frequency trajectory interpolation via ROI-constrained sparse optical flow on the CPU. To handle tracking failures from occlusion or model drift caused by spectrally similar distractors, we introduce Dual-Space Recovery, a training-free re-acquisition mechanism combining color histogram matching with geometric consistency constraints. Experiments on a ground-to-air tracking station demonstrate that SDG-Track achieves 35.1 FPS system throughput while retaining 97.2\% of the frame-by-frame detection precision. The system successfully tracks agile FPV drones under real-world operational conditions on an NVIDIA Jetson Orin Nano. Our paper code is publicly available at https://github.com/Jeffry-wen/SDG-Track

</details>


### [71] [You Only Train Once (YOTO): A Retraining-Free Object Detection Framework](https://arxiv.org/abs/2512.04888)
*Priyanto Hidayatullah,Nurjannah Syakrani,Yudi Widhiyasana,Muhammad Rizqi Sholahuddin,Refdinal Tubagus,Zahri Al Adzani Hidayat,Hanri Fajar Ramadhan,Dafa Alfarizki Pratama,Farhan Muhammad Yasin*

Main category: cs.CV

TL;DR: 本研究提出了一套名为You Only Train Once (YOTO) 的方法，旨在解决物体检测中的灾难性遗忘问题。通过结合YOLO11n、DeiT和Proxy Anchor Loss等技术，YOTO框架可以有效检测新旧产品，同时显著提高训练效率和推断速度。


<details>
  <summary>Details</summary>
Motivation: 现有的物体检测方法存在灾难性遗忘的问题，每当有新产品引入时需要重新训练模型，这将增加高昂的训练成本和大量时间消耗。针对这一问题，本文提出了You Only Train Once (YOTO) 方法。

Method: YOTO方法采用了YOLO11n进行物体定位、DeIT进行特征提取和Proxy Anchor Loss进行度量学习。分类过程则通过计算目标产品和Qdrant向量数据库中特征的余弦相似度来实现。

Result: 通过在140产品的零售店案例研究中应用YOTO方法，展示了在检测新旧产品时均能达到不错的精度。相较于传统的物体检测方法，我们的方法几乎提高了3倍的训练时间效率，并且在边缘设备上的平均推断时间为每张包含多个物体的图片580ms，验证了方法的实用性和可行性。

Conclusion: 总之，YOTO方法有效解决了物体检测中的灾难性遗忘问题，并且展示了良好的实际应用价值。

Abstract: Object detection constitutes the primary task within the domain of computer vision. It is utilized in numerous domains. Nonetheless, object detection continues to encounter the issue of catastrophic forgetting. The model must be retrained whenever new products are introduced, utilizing not only the new products dataset but also the entirety of the previous dataset. The outcome is obvious: increasing model training expenses and significant time consumption. In numerous sectors, particularly retail checkout, the frequent introduction of new products presents a great challenge. This study introduces You Only Train Once (YOTO), a methodology designed to address the issue of catastrophic forgetting by integrating YOLO11n for object localization with DeIT and Proxy Anchor Loss for feature extraction and metric learning. For classification, we utilize cosine similarity between the embedding features of the target product and those in the Qdrant vector database. In a case study conducted in a retail store with 140 products, the experimental results demonstrate that our proposed framework achieves encouraging accuracy, whether for detecting new or existing products. Furthermore, without retraining, the training duration difference is significant. We achieve almost 3 times the training time efficiency compared to classical object detection approaches. This efficiency escalates as additional new products are added to the product database. The average inference time is 580 ms per image containing multiple products, on an edge device, validating the proposed framework's feasibility for practical use.

</details>


### [72] [Equivariant Symmetry-Aware Head Pose Estimation for Fetal MRI](https://arxiv.org/abs/2512.04890)
*Ramya Muthukrishnan,Borjan Gagoski,Aryn Lee,P. Ellen Grant,Elfar Adalsteinsson,Polina Golland,Benjamin Billot*

Main category: cs.CV

TL;DR: E(3)-Pose是一种新颖的快速姿态估计方法，联合且明确地建模了旋转等变性和物体对称性。它通过捕捉解剖学对称性和刚性姿态等变性，实现了在临床MRI图像上的优越鲁棒性和泛化性。


<details>
  <summary>Details</summary>
Motivation: 由于胎儿头部在诊断MRI扫描期间的运动，实现自动适应性2D诊断MRI切片以及6-DoF头部姿态估计，同时支持快速获取的3D MRI体层，是本研究的动机。

Method: E(3)-Pose通过建模旋转等变性和物体对称性来估计胎儿头部的姿态。

Result: 实验结果表明，E(3)-Pose在公开的和代表性的临床胎儿MRI数据集上表现出优于现有方法的鲁棒性和泛化性。

Conclusion: E(3)-Pose在临床MRI图像上达到了最先进的精度，为该技术的临床应用铺平了道路。

Abstract: We present E(3)-Pose, a novel fast pose estimation method that jointly and explicitly models rotation equivariance and object symmetry. Our work is motivated by the challenging problem of accounting for fetal head motion during a diagnostic MRI scan. We aim to enable automatic adaptive prescription of 2D diagnostic MRI slices with 6-DoF head pose estimation, supported by 3D MRI volumes rapidly acquired before each 2D slice. Existing methods struggle to generalize to clinical volumes, due to pose ambiguities induced by inherent anatomical symmetries, as well as low resolution, noise, and artifacts. In contrast, E(3)-Pose captures anatomical symmetries and rigid pose equivariance by construction, and yields robust estimates of the fetal head pose. Our experiments on publicly available and representative clinical fetal MRI datasets demonstrate the superior robustness and generalization of our method across domains. Crucially, E(3)-Pose achieves state-of-the-art accuracy on clinical MRI volumes, paving the way for clinical translation. Our implementation is available at github.com/ramyamut/E3-Pose.

</details>


### [73] [Semantics Lead the Way: Harmonizing Semantic and Texture Modeling with Asynchronous Latent Diffusion](https://arxiv.org/abs/2512.04926)
*Yueming Pan,Ruoyu Feng,Qi Dai,Yuqi Wang,Wenfeng Lin,Mingyu Guo,Chong Luo,Nanning Zheng*

Main category: cs.CV

TL;DR: 提出了一种名为Semantic-First Diffusion (SFD) 的方法，通过在Latent Diffusion Models中优先生成语义信息并异步去噪语义和纹理latent，实现了更快的收敛速度和更好的生成效果。


<details>
  <summary>Details</summary>
Motivation: 在当前的Latent Diffusion Models (LDMs) 中，虽然已经使用预训练的视觉编码器引入语义先验信息，但它们仍然同步去噪语义和纹理latent，忽略了语义先行的潜在优势。

Method: SFD通过引入一个从预训练视觉编码器提取的紧凑语义latent，结合专用的语义VAE，以及不同的去噪噪声时间表，分别预先去噪语义latent，随后去噪纹理latent。这种方法实现了语义引导的先序生成。

Result: SFD在ImageNet 256x256数据集上使用指导时，相对于LightningDiT-XL和LightningDiT-XXL，其FID分别为1.06和1.04，相比原始DiT的收敛速度提高了100倍。同时，SFD在现有的方法如ReDi和VA-VAE上也表现出改进的效果。

Conclusion: SFD通过异步去噪和语义优先策略，显著提升了生成模型的性能和训练效率，证明了这种新的扩散范式的有效性。

Abstract: Latent Diffusion Models (LDMs) inherently follow a coarse-to-fine generation process, where high-level semantic structure is generated slightly earlier than fine-grained texture. This indicates the preceding semantics potentially benefit texture generation by providing a semantic anchor. Recent advances have integrated semantic priors from pretrained visual encoders to further enhance LDMs, yet they still denoise semantic and VAE-encoded texture synchronously, neglecting such ordering. Observing these, we propose Semantic-First Diffusion (SFD), a latent diffusion paradigm that explicitly prioritizes semantic formation. SFD first constructs composite latents by combining a compact semantic latent, which is extracted from a pretrained visual encoder via a dedicated Semantic VAE, with the texture latent. The core of SFD is to denoise the semantic and texture latents asynchronously using separate noise schedules: semantics precede textures by a temporal offset, providing clearer high-level guidance for texture refinement and enabling natural coarse-to-fine generation. On ImageNet 256x256 with guidance, SFD achieves FID 1.06 (LightningDiT-XL) and FID 1.04 (1.0B LightningDiT-XXL), while achieving up to 100x faster convergence than the original DiT. SFD also improves existing methods like ReDi and VA-VAE, demonstrating the effectiveness of asynchronous, semantics-led modeling. Project page and code: https://yuemingpan.github.io/SFD.github.io/.

</details>


### [74] [LiteVGGT: Boosting Vanilla VGGT via Geometry-aware Cached Token Merging](https://arxiv.org/abs/2512.04939)
*Zhijian Shu,Cheng Lin,Tao Xie,Wei Yin,Ben Li,Zhiyuan Pu,Weize Li,Yao Yao,Xun Cao,Xiaoyang Guo,Xiao-Xiao Long*

Main category: cs.CV

TL;DR: LiteVGGT在保持高性能的同时，实现了Visual Geometry Grounded Transformer (VGGT)的10倍加速和显著的内存减少，使得可以高效处理1000帧的场景。


<details>
  <summary>Details</summary>
Motivation: 3D视觉基础模型如VGGT在几何感知上取得了显著进步，但在处理长序列时速度慢且耗内存，限制了在大型场景中的应用。

Method: 从局部图像区域的几何相关性出发，提出了基于几何感知的缓存令牌合并策略，包括分析每个令牌的几何重要性以优化锚点选择，以及跨层缓存和重用合并索引。

Result: 通过LiteVGGT实现了10倍的速度提升和大量的内存减少，证实了其在稳定性和鲁棒性方面的有效性。

Conclusion: LiteVGGT基于几何感知的策略成功地实现了性能的保持和效率的提升，为后续的工作提供了新的优化方向。

Abstract: 3D vision foundation models like Visual Geometry Grounded Transformer (VGGT) have advanced greatly in geometric perception. However, it is time-consuming and memory-intensive for long sequences, limiting application to large-scale scenes beyond hundreds of images. To address this, we propose LiteVGGT, achieving up to 10x speedup and substantial memory reduction, enabling efficient processing of 1000-image scenes. We derive two key insights for 3D reconstruction: (1) tokens from local image regions have inherent geometric correlations, leading to high similarity and computational redundancy; (2) token similarity across adjacent network layers remains stable, allowing for reusable merge decisions. Guided by these, we design a simple yet efficient strategy, dubbed geometry-aware cached token merging. We analyze each token's geometric importance, optimizing anchor token selection to better preserve key information for reconstruction. We also cache and reuse merge indices across layers, substantially reducing latency with minimal accuracy impact. This strategy retains VGGT's core performance, enabling efficient fine-tuning and FP8 quantization for further gains. Extensive experiments validate LiteVGGT's effectiveness, scalability, and robustness. Project page: https://garlicba.github.io/LiteVGGT/

</details>


### [75] [FASTer: Toward Efficient Autoregressive Vision Language Action Modeling via neural Action Tokenization](https://arxiv.org/abs/2512.04952)
*Yicheng Liu,Shiduo Zhang,Zibin Dong,Baijun Ye,Tianyuan Yuan,Xiaopeng Yu,Linqi Yin,Chenhao Lu,Junhao Shi,Luca Jiang-Tao Yu,Liangtao Zheng,Tao Jiang,Jingjing Gong,Xipeng Qiu,Hang Zhao*

Main category: cs.CV

TL;DR: 文章介绍了一个名为FASTer的统一框架，融合了可学习的分词器和基于它的自回归策略，适用于机器人学习。FASTerVQ通过单通道图像编码动作片段，保持高压缩比的同时捕捉全局时空依赖性，而FASTerVLA进一步使用块级自回归解码和轻量级动作专家，提升了推理速度和任务性能。


<details>
  <summary>Details</summary>
Motivation: 在现有的视觉-语言-动作（VLA）模型中，动作分词过程通常需要在重建保真度和推理效率之间做出权衡。鉴于此，本文提出FASTer框架以提高推理效率和任务性能。

Method: FASTer框架包含两个主要模块：FASTerVQ 和 FASTerVLA。FASTerVQ通过将动作片段编码为单通道图像来实现动作表示，而在FASTerVLA中，引入了块级自回归解码和轻量级动作专家，提高了模型的执行性能。

Result: 实验结果表明，FASTerVQ在重建质量、令牌利用率和跨任务、跨载体泛化方面表现出色，而FASTerVLA则进一步改善了整体能力，超越了现有最先进的VLA模型。

Conclusion: FASTer框架通过有效的动作分词和高效的解码机制，在机器人学习任务中展现出较高的性能和广泛的适用性。

Abstract: Autoregressive vision-language-action (VLA) models have recently demonstrated strong capabilities in robotic manipulation. However, their core process of action tokenization often involves a trade-off between reconstruction fidelity and inference efficiency. We introduce FASTer, a unified framework for efficient and generalizable robot learning that integrates a learnable tokenizer with an autoregressive policy built upon it. FASTerVQ encodes action chunks as single-channel images, capturing global spatio-temporal dependencies while maintaining a high compression ratio. FASTerVLA builds on this tokenizer with block-wise autoregressive decoding and a lightweight action expert, achieving both faster inference and higher task performance. Extensive experiments across simulated and real-world benchmarks show that FASTerVQ delivers superior reconstruction quality, high token utilization, and strong cross-task and cross-embodiment generalization, while FASTerVLA further improves overall capability, surpassing previous state-of-the-art VLA models in both inference speed and task performance.

</details>


### [76] [Rethinking the Use of Vision Transformers for AI-Generated Image Detection](https://arxiv.org/abs/2512.04969)
*NaHyeon Park,Kunhee Kim,Junsuk Choe,Hyunjung Shim*

Main category: cs.CV

TL;DR: 本文研究了CLIP-ViT各层特征对AI生成图像检测任务的贡献，并提出了一种新的自适应方法MoLD，该方法利用门控机制动态集成多层ViT特征，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法大多只利用CLIP-ViT最后一层的特征进行图像检测，但本文研究发现早期层提供更局部和通用的特征，并在检测任务中表现出更优异的性能。

Method: 本文分析了不同层的特征对任务的贡献，并基于此提出了一个自适应方法MoLD，该方法通过门控机制动态整合来自多个ViT层的特征。

Result: 在生成图像上的广泛实验表明，MoLD显著提高了检测性能，增强了对多种生成模型的一般化能力，并在实际场景中表现出稳健性。

Conclusion: 本文的研究发现揭示了CLIP-ViT不同层的特征对检测任务的独特贡献，证明了MoLD方法的有效性和普适性。

Abstract: Rich feature representations derived from CLIP-ViT have been widely utilized in AI-generated image detection. While most existing methods primarily leverage features from the final layer, we systematically analyze the contributions of layer-wise features to this task. Our study reveals that earlier layers provide more localized and generalizable features, often surpassing the performance of final-layer features in detection tasks. Moreover, we find that different layers capture distinct aspects of the data, each contributing uniquely to AI-generated image detection. Motivated by these findings, we introduce a novel adaptive method, termed MoLD, which dynamically integrates features from multiple ViT layers using a gating-based mechanism. Extensive experiments on both GAN- and diffusion-generated images demonstrate that MoLD significantly improves detection performance, enhances generalization across diverse generative models, and exhibits robustness in real-world scenarios. Finally, we illustrate the scalability and versatility of our approach by successfully applying it to other pre-trained ViTs, such as DINOv2.

</details>


### [77] [Stable Single-Pixel Contrastive Learning for Semantic and Geometric Tasks](https://arxiv.org/abs/2512.04970)
*Leonid Pogorelyuk,Niels Bracher,Aaron Verkleeren,Lars Kühmichel,Stefan T. Radev*

Main category: cs.CV

TL;DR: 本文提出了一种新的稳定对比损失方法，用于学习同时捕捉语义和几何信息的像素级表示。


<details>
  <summary>Details</summary>
Motivation: 当前的像素级表示方法往往在语义和几何信息的捕捉上存在不足或需要复杂的训练方式，本文旨在提供一种改进的方法。

Method: 通过将每张图像中的每个像素映射到一个包括语义和几何信息的冗余描述符，以实现视角不变性。提出的方法无需依赖动量教师-学生训练。

Result: 在合成的2D和3D环境中进行的两个实验表明，提出的损失和超出描述符能够准确地跨图像匹配像素。

Conclusion: 该研究通过稳定对比损失方法在像素级表示上取得了显著成果，展示了其在处理语义和几何信息方面的优势。

Abstract: We pilot a family of stable contrastive losses for learning pixel-level representations that jointly capture semantic and geometric information. Our approach maps each pixel of an image to an overcomplete descriptor that is both view-invariant and semantically meaningful. It enables precise point-correspondence across images without requiring momentum-based teacher-student training. Two experiments in synthetic 2D and 3D environments demonstrate the properties of our loss and the resulting overcomplete representations.

</details>


### [78] [Aligned but Stereotypical? The Hidden Influence of System Prompts on Social Bias in LVLM-Based Text-to-Image Models](https://arxiv.org/abs/2512.04981)
*NaHyeon Park,Namin An,Kunhee Kim,Soyeon Yoon,Jiahao Huo,Hyunjung Shim*

Main category: cs.CV

TL;DR: 本研究揭示了基于大视觉语言模型的文本到图像生成系统比非此类模型更具有社会偏见。通过引入一个包含四个语言复杂度级别的基准测试，研究发现系统提示是造成偏见的主要原因，并提出了一个无需训练的元提示框架FairPro，帮助模型生成更公平的图像。


<details>
  <summary>Details</summary>
Motivation: 由于大规模视觉语言模型（LVLM）驱动的文本到图像（T2I）系统已成图像生成的主导方式，但其社会偏见问题尚未充分理解，研究旨在找出偏见来源并提出解决方案。

Method: 研究设计了一个1,024个提示的基准，评估了多个属性的社会偏见，并通过解码中间表示、词汇概率诊断和嵌入关联分析揭示系统提示如何将先入为主的观念传播到图像合成中。研究提出了FairPro框架来帮助LVLM生成公平的系统提示。

Result: 实验表明，使用FairPro框架可以显著减少社会偏见，同时保持文本与图像的一致性。该研究证明了系统提示在偏见传播中的核心作用，并提供了一个实用的构建更负责任的T2I系统的方案。

Conclusion: 研究强调了系统提示在偏见传播中的关键作用，并提出了一种无需训练的FairPro框架，帮助生成公平的图像，从而提高了T2I系统的社会责任感。

Abstract: Large vision-language model (LVLM) based text-to-image (T2I) systems have become the dominant paradigm in image generation, yet whether they amplify social biases remains insufficiently understood. In this paper, we show that LVLM-based models produce markedly more socially biased images than non-LVLM-based models. We introduce a 1,024 prompt benchmark spanning four levels of linguistic complexity and evaluate demographic bias across multiple attributes in a systematic manner. Our analysis identifies system prompts, the predefined instructions guiding LVLMs, as a primary driver of biased behavior. Through decoded intermediate representations, token-probability diagnostics, and embedding-association analyses, we reveal how system prompts encode demographic priors that propagate into image synthesis. To this end, we propose FairPro, a training-free meta-prompting framework that enables LVLMs to self-audit and construct fairness-aware system prompts at test time. Experiments on two LVLM-based T2I models, SANA and Qwen-Image, show that FairPro substantially reduces demographic bias while preserving text-image alignment. We believe our findings provide deeper insight into the central role of system prompts in bias propagation and offer a practical, deployable approach for building more socially responsible T2I systems.

</details>


### [79] [A dynamic memory assignment strategy for dilation-based ICP algorithm on embedded GPUs](https://arxiv.org/abs/2512.04996)
*Qiong Chang,Weimin Wang,Junpei Zhong,Jun Miyazaki*

Main category: cs.CV

TL;DR: 本文提出了一种内存优化策略，将点云注册算法VANICP的全局最近邻搜索转换为局部化过程，从而在资源受限的嵌入式GPU上实现轻量执行。该策略使得VANICP在嵌入式系统上的部署成为可能，同时保持了较高的性能。


<details>
  <summary>Details</summary>
Motivation: VANICP算法虽然提高了点云应用的计算效率，但由于其原始实现需要大量的内存资源，限制了其在嵌入式设备上的应用。本文旨在解决这一问题，通过提出一种适用于GPU的动态内存分配策略，优化了VANICP的空间利用率。

Method: 该方法主要通过将全局最近邻搜索转换为局部过程，并利用扩散机制来进行信息传播，同时提出了一种GPU优化的动态内存分配策略，从而在保证性能的前提下显著降低了内存使用。

Result: 实验结果显示，优化后的VANICP框架在保持原有性能的同时，内存消耗减少了超过97%。该优化策略适用于资源受限的嵌入式GPU，使得VANICP在嵌入式系统上具有更高的应用潜力。

Conclusion: 本文提出了一种适用于嵌入式系统的VANICP优化方法，该方法通过改进内存使用策略，有效提高了VANICP在嵌入式设备上的执行效率和适用性。

Abstract: This paper proposes a memory-efficient optimization strategy for the high-performance point cloud registration algorithm VANICP, enabling lightweight execution on embedded GPUs with constrained hardware resources. VANICP is a recently published acceleration framework that significantly improves the computational efficiency of point-cloud-based applications. By transforming the global nearest neighbor search into a localized process through a dilation-based information propagation mechanism, VANICP greatly reduces the computational complexity of the NNS. However, its original implementation demands a considerable amount of memory, which restricts its deployment in resource-constrained environments such as embedded systems. To address this issue, we propose a GPU-oriented dynamic memory assignment strategy that optimizes the memory usage of the dilation operation. Furthermore, based on this strategy, we construct an enhanced version of the VANICP framework that achieves over 97% reduction in memory consumption while preserving the original performance. Source code is published on: https://github.com/changqiong/VANICP4Em.git.

</details>


### [80] [Self-Supervised Learning for Transparent Object Depth Completion Using Depth from Non-Transparent Objects](https://arxiv.org/abs/2512.05006)
*Xianghui Fan,Zhaoyu Chen,Mengyang Pan,Anping Deng,Hang Yang*

Main category: cs.CV

TL;DR: 本文提出了一种新的自监督深度补全方法，通过模拟不透明区域中的透明物体深度缺失，并使用原始深度图作为监督依据，已证明该方法在性能上与监督方法相当，并且在样本量较小时预训练可以提高模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统的深度传感器无法准确感测透明物体的深度，因为光线会在透明物体上发生折射和反射。利用大体量标注数据进行监督的传统方法成本高昂，而自监督方法无需此类数据，因此本文通过提出一种新的自监督深度补全方法来解决这一挑战。

Method: 文章提出的自监督方法通过模拟不透明区域中透明物体的深度缺失，并以原始深度图作为监督依据。这种方法利用了深度传感器对不透明区域的正常采集数据，通过自动生成透明区域的深度信息，间接完成对透明物体的深度补全。

Result: 实验结果表明，本文提出的自监督方法在性能上达到与传统的监督方法相当的水平。特别是在样本量较少的情况下，使用此自监督方法进行预训练，可以显著提升模型的性能。

Conclusion: 综上所述，本文提出的方法为深度传感器识别透明物体的深度提供了一种有效且低成本的解决方案。

Abstract: The perception of transparent objects is one of the well-known challenges in computer vision. Conventional depth sensors have difficulty in sensing the depth of transparent objects due to refraction and reflection of light. Previous research has typically train a neural network to complete the depth acquired by the sensor, and this method can quickly and accurately acquire accurate depth maps of transparent objects. However, previous training relies on a large amount of annotation data for supervision, and the labeling of depth maps is costly. To tackle this challenge, we propose a new self-supervised method for training depth completion networks. Our method simulates the depth deficits of transparent objects within non-transparent regions and utilizes the original depth map as ground truth for supervision. Experiments demonstrate that our method achieves performance comparable to supervised approach, and pre-training with our method can improve the model performance when the training samples are small.

</details>


### [81] [Generative Neural Video Compression via Video Diffusion Prior](https://arxiv.org/abs/2512.05016)
*Qi Mao,Hao Cheng,Tinghan Yang,Libiao Jin,Siwei Ma*

Main category: cs.CV

TL;DR: GNVC-VD 提出了首个基于 DiT 的生成神经视频压缩框架，通过统一的空间-时间潜在压缩和序列级生成精炼模块，解决了现有可感知压缩方法中缺乏时序建模的问题，并通过视频扩散变换器和压缩感知调整，减少了画质感知上的闪烁。


<details>
  <summary>Details</summary>
Motivation: 现有的可感知视频压缩方法主要依赖于预训练的图像生成先验来恢复高频细节，但由于其帧级别的特性缺乏时序建模，导致存在感知闪烁问题。GNVC-VD 创新地引入了整合的空间-时间潜在压缩与序列级生成精炼模块，利用视频扩散变换器进行序列级降噪，从而提供了一种一致的时间-空间细节保持方法。

Method: GNVC-VD 通过一个统一的流匹配潜在精炼模块改进了视频帧内的和帧间潜在特征，并且从解码后的时间-空间潜在特征进行优化，学习出适应压缩破坏后的调整项，从而通过中间的视频扩散变换器层嵌入压缩感知提示，实现有效的抗伪影处理同时保持时间连贯性。

Result: 实验表明，GNVC-VD 在感知品质上超过了传统和学习型编码器，并且显著减少了前代生成方法中存在的闪烁伪影，甚至在极低的比特率下（低于 0.01 bpp）也保持了出色的表现。

Conclusion: 结合视频原生生成先验来实现神经编码器的新方法显示出了在下一代感知视频压缩中的潜力。

Abstract: We present GNVC-VD, the first DiT-based generative neural video compression framework built upon an advanced video generation foundation model, where spatio-temporal latent compression and sequence-level generative refinement are unified within a single codec. Existing perceptual codecs primarily rely on pre-trained image generative priors to restore high-frequency details, but their frame-wise nature lacks temporal modeling and inevitably leads to perceptual flickering. To address this, GNVC-VD introduces a unified flow-matching latent refinement module that leverages a video diffusion transformer to jointly enhance intra- and inter-frame latents through sequence-level denoising, ensuring consistent spatio-temporal details. Instead of denoising from pure Gaussian noise as in video generation, GNVC-VD initializes refinement from decoded spatio-temporal latents and learns a correction term that adapts the diffusion prior to compression-induced degradation. A conditioning adaptor further injects compression-aware cues into intermediate DiT layers, enabling effective artifact removal while maintaining temporal coherence under extreme bitrate constraints. Extensive experiments show that GNVC-VD surpasses both traditional and learned codecs in perceptual quality and significantly reduces the flickering artifacts that persist in prior generative approaches, even below 0.01 bpp, highlighting the promise of integrating video-native generative priors into neural codecs for next-generation perceptual video compression.

</details>


### [82] [RAMEN: Resolution-Adjustable Multimodal Encoder for Earth Observation](https://arxiv.org/abs/2512.05025)
*Nicolas Houdré,Diego Marcos,Hugo Riffaud de Turckheim,Dino Ienco,Laurent Wendling,Camille Kurtz,Sylvain Lobry*

Main category: cs.CV

TL;DR: RAMEN是一种可调整分辨率的多模态编码器，能够在不依赖于具体传感器的情况下，为地球观测数据学习共享视觉表示。它能够控制输出的空间分辨率，实现了多模态一致的隐空间分析，并在PANGAEA基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 最近的多模式基础模型在跨异构地球观测模式上泛化能力有限，RAMEN旨在提供一种传感器无关、可调整分辨率的多模态编码方法，以提高模型的多模态分析能力。

Method: RAMEN通过将模态、空间和时间分辨率作为输入数据的关键特征进行处理，定义了空间分辨率作为可控输出参数，并使用单一统一的变压器编码器进行预训练。

Result: 经过预训练的RAMEN能够有效迁移到已知和未知的传感器配置中，并在PANGAEA基准测试中优于现有的大型模型。

Conclusion: RAMEN为地球观测数据的多模态学习提供了一种新的方法，通过控制输出的空间分辨率，实现了不同的时间和空间分辨率下的多模态一致分析，并展示了其在实际任务中的应用潜力。

Abstract: Earth observation (EO) data spans a wide range of spatial, spectral, and temporal resolutions, from high-resolution optical imagery to low resolution multispectral products or radar time series. While recent foundation models have improved multimodal integration for learning meaningful representations, they often expect fixed input resolutions or are based on sensor-specific encoders limiting generalization across heterogeneous EO modalities. To overcome these limitations we introduce RAMEN, a resolution-adjustable multimodal encoder that learns a shared visual representation across EO data in a fully sensor-agnostic manner. RAMEN treats the modality and spatial and temporal resolutions as key input data features, enabling coherent analysis across modalities within a unified latent space. Its main methodological contribution is to define spatial resolution as a controllable output parameter, giving users direct control over the desired level of detail at inference and allowing explicit trade-offs between spatial precision and computational cost. We train a single, unified transformer encoder reconstructing masked multimodal EO data drawn from diverse sources, ensuring generalization across sensors and resolutions. Once pretrained, RAMEN transfers effectively to both known and unseen sensor configurations and outperforms larger state-of-the-art models on the community-standard PANGAEA benchmark, containing various multi-sensor and multi-resolution downstream tasks. Our code and pretrained model are available at https://github.com/nicolashoudre/RAMEN.

</details>


### [83] [4DLangVGGT: 4D Language-Visual Geometry Grounded Transformer](https://arxiv.org/abs/2512.05060)
*Xianfeng Wu,Yajing Bai,Minghan Li,Xianzu Wu,Xueqi Zhao,Zhongyuan Lai,Wenyu Liu,Xinggang Wang*

Main category: cs.CV

TL;DR: 该研究提出了4DLangVGGT，一种基于Transformer的前向统一框架，用于4D语言场景理解，通过单一架构整合几何感知和语言对齐，不同场景下的联合训练使得部署更加高效，并且在多种数据集测试中表现出优越的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要依赖于场景特定的空间时间几何表征，需要针对每个场景进行优化，难以推广到实际应用中。

Method: 4DLangVGGT框架包含两个关键组件：4D视觉几何变换器（StreamVGGT）用于捕捉动态场景的空间-时间几何表示；语义桥梁解码器（SBD）用于将几何感知特征映射到语义对齐的空间，增强语义可解释性并保持结构保真性。

Result: 4DLangVGGT可以通过联合训练多个动态场景，避免繁琐的单场景优化，从而提高部署效率和泛化能力。实验结果表明，该方法不仅泛化能力强，还达到SOTA性能，多场景训练相比单场景训练分别取得1%和2%的提高。

Conclusion: 4DLangVGGT通过整合几何感知与语言对齐，提供了一种新的方法来实现大规模部署下的开放词汇4D场景理解。

Abstract: Constructing 4D language fields is crucial for embodied AI, augmented/virtual reality, and 4D scene understanding, as they provide enriched semantic representations of dynamic environments and enable open-vocabulary querying in complex scenarios. However, existing approaches to 4D semantic field construction primarily rely on scene-specific Gaussian splatting, which requires per-scene optimization, exhibits limited generalization, and is difficult to scale to real-world applications. To address these limitations, we propose 4DLangVGGT, the first Transformer-based feed-forward unified framework for 4D language grounding, that jointly integrates geometric perception and language alignment within a single architecture. 4DLangVGGT has two key components: the 4D Visual Geometry Transformer, StreamVGGT, which captures spatio-temporal geometric representations of dynamic scenes; and the Semantic Bridging Decoder (SBD), which projects geometry-aware features into a language-aligned semantic space, thereby enhancing semantic interpretability while preserving structural fidelity. Unlike prior methods that depend on costly per-scene optimization, 4DLangVGGT can be jointly trained across multiple dynamic scenes and directly applied during inference, achieving both deployment efficiency and strong generalization. This design significantly improves the practicality of large-scale deployment and establishes a new paradigm for open-vocabulary 4D scene understanding. Experiments on HyperNeRF and Neu3D datasets demonstrate that our approach not only generalizes effectively but also achieves state-of-the-art performance, achieving up to 2% gains under per-scene training and 1% improvements under multi-scene training. Our code released in https://github.com/hustvl/4DLangVGGT

</details>


### [84] [BulletTime: Decoupled Control of Time and Camera Pose for Video Generation](https://arxiv.org/abs/2512.05076)
*Yiming Wang,Qihang Zhang,Shengqu Cai,Tong Wu,Jan Ackermann,Zhengfei Kuang,Yang Zheng,Frano Rajič,Siyu Tang,Gordon Wetzstein*

Main category: cs.CV

TL;DR: 该研究提出了一种将场景动态与摄像机姿态明确解耦的4D可控视频扩散框架，能够实现对场景动态和摄像机视角的细致操控，实验表明该模型在保持高质量生成的同时，超越了现有工作在可控性方面的表现。


<details>
  <summary>Details</summary>
Motivation: 现有的视频扩散模型虽然在视觉保真度上表现出色，但无法提供精确的空间和时间控制，因为它们将场景动态与摄像机运动紧密关联。因此，提出一种将两种因素分解的新框架，以增强视频生成模型的控制能力。

Method: 该方法通过使用连续的世界时间序列和摄像机轨迹作为条件输入，并在此框架下结合4D位置编码和自适应规范化操作，将输入信息注入到注意力层中。同时，团队还建立了含独立参数化的时空与摄像机变异性的独特数据集，以支持模型训练。

Result: 实验结果表明，该模型能够实现对不同时间模式和摄像机轨迹的稳健4D控制，同时保持高质量的生成效果，并且在可操控性方面超越了先前的工作。

Conclusion: 研究团队提出的方法在视频生成的可控性和质量上取得了显著进步，为未来的视频生成模型设计提供了新的思路和方法。

Abstract: Emerging video diffusion models achieve high visual fidelity but fundamentally couple scene dynamics with camera motion, limiting their ability to provide precise spatial and temporal control. We introduce a 4D-controllable video diffusion framework that explicitly decouples scene dynamics from camera pose, enabling fine-grained manipulation of both scene dynamics and camera viewpoint. Our framework takes continuous world-time sequences and camera trajectories as conditioning inputs, injecting them into the video diffusion model through a 4D positional encoding in the attention layer and adaptive normalizations for feature modulation. To train this model, we curate a unique dataset in which temporal and camera variations are independently parameterized; this dataset will be made public. Experiments show that our model achieves robust real-world 4D control across diverse timing patterns and camera trajectories, while preserving high generation quality and outperforming prior work in controllability. See our website for video results: https://19reborn.github.io/Bullet4D/

</details>


### [85] [Object Reconstruction under Occlusion with Generative Priors and Contact-induced Constraints](https://arxiv.org/abs/2512.05079)
*Minghan Zhu,Zhiyi Wang,Qihang Sun,Maani Ghaffari,Michael Posa*

Main category: cs.CV

TL;DR: 本文提出了结合生成模型先验信息与接触信息的3D生成方法，以提高物体几何重建的准确性。


<details>
  <summary>Details</summary>
Motivation: 相机只能获取物体的部分观察信息，尤其是当遮挡发生时，物体重建面临着诸多挑战。因此，本文提出利用生成模型学习常见物体的形状先验和接触信息作为稀疏边界约束，以减少视觉信号的不确定性。

Method: 该方法通过结合生成模型（利用其先验信息）和物理接触信息（作为几何边界的稀疏约束）进行引导式的3D生成。方法受到生成模型中拖拽编辑的启发。

Result: 实验结果表明，本文方法在合成数据和真实世界数据上的表现优于仅依赖3D生成或基于接触优化的方法。

Conclusion: 本文提出的方法为复杂条件下物体几何重建提供了有效途径，改善了重建精度。

Abstract: Object geometry is key information for robot manipulation. Yet, object reconstruction is a challenging task because cameras only capture partial observations of objects, especially when occlusion occurs. In this paper, we leverage two extra sources of information to reduce the ambiguity of vision signals. First, generative models learn priors of the shapes of commonly seen objects, allowing us to make reasonable guesses of the unseen part of geometry. Second, contact information, which can be obtained from videos and physical interactions, provides sparse constraints on the boundary of the geometry. We combine the two sources of information through contact-guided 3D generation. The guidance formulation is inspired by drag-based editing in generative models. Experiments on synthetic and real-world data show that our approach improves the reconstruction compared to pure 3D generation and contact-based optimization.

</details>


### [86] [Deep Forcing: Training-Free Long Video Generation with Deep Sink and Participative Compression](https://arxiv.org/abs/2512.05081)
*Jung Yi,Wooseok Jang,Paul Hyunbin Cho,Jisu Nam,Heeji Yoon,Seungryong Kim*

Main category: cs.CV

TL;DR: 本文提出了一种名为Deep Forcing的方法，它通过两种无需微调的机制解决了视频扩散中存在的时间重复、漂移和运动减速问题，实现了超过12倍的超长视频生成，同时保持了实时生成能力。


<details>
  <summary>Details</summary>
Motivation: 现有的自回归视频扩散方法虽然能够实现实时帧流式传输，但仍存在时间重复、漂移和运动减速的问题。

Method: Deep Forcing 方法包括 Deep Sink 和 Participative Compression 两个机制。Deep Sink 通过固定一半滑动窗口的持续吸引子令牌并重新对齐其时间 RoPE 相位，确保长期构建过程中的全局上下文稳定。Participative Compression 则进行重要性感知的 KV 缓存剪裁，只保留参与近期注意力的令牌，丢弃冗余和退化的历史，以减少错误累积。

Result: 通过这种方法，本文实现了超过12倍的超长视频生成，生成的视频具有更高的成像质量和美学质量，同时几乎保持整体一致性，动态程度有了显著提升，且保持了实时生成能力。

Conclusion: 研究结果表明，无需训练的 KV 缓存管理可以与基于训练的方法媲美，甚至在自回归长视频生成方面表现出色。

Abstract: Recent advances in autoregressive video diffusion have enabled real-time frame streaming, yet existing solutions still suffer from temporal repetition, drift, and motion deceleration. We find that naively applying StreamingLLM-style attention sinks to video diffusion leads to fidelity degradation and motion stagnation. To overcome this, we introduce Deep Forcing, which consists of two training-free mechanisms that address this without any fine-tuning. Specifically, 1) Deep Sink dedicates half of the sliding window to persistent sink tokens and re-aligns their temporal RoPE phase to the current timeline, stabilizing global context during long rollouts. 2) Participative Compression performs importance-aware KV cache pruning that preserves only tokens actively participating in recent attention while safely discarding redundant and degraded history, minimizing error accumulation under out-of-distribution length generation. Together, these components enable over 12x extrapolation (e.g. 5s-trained to 60s+ generation) with better imaging quality than LongLive, better aesthetic quality than RollingForcing, almost maintaining overall consistency, and substantial gains in dynamic degree, all while maintaining real-time generation. Our results demonstrate that training-free KV-cache management can match or exceed training-based approaches for autoregressively streaming long-video generation.

</details>


### [87] [Visual Reasoning Tracer: Object-Level Grounded Reasoning Benchmark](https://arxiv.org/abs/2512.05091)
*Haobo Yuan,Yueyi Sun,Yanwei Li,Tao Zhang,Xueqing Deng,Henghui Ding,Lu Qi,Anran Wang,Xiangtai Li,Ming-Hsuan Yang*

Main category: cs.CV

TL;DR: 该研究引入了Visual Reasoning Tracer (VRT) 任务，旨在提高模型在视觉推理过程的透明度。通过贡献VRT-Bench基准、新计算推理路径质量的指标以及包含80000个样本的大规模训练集，研究旨在提升模型的视觉推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大型语言模型在视觉推理任务上取得了进步，但仍缺乏透明的推理过程。为了弥补这一不足，提升模型的理解与解释能力。

Method: 研究提出了Visual Reasoning Tracer (VRT) 任务，要求模型不仅要定位目标对象，还要预测推理过程中的中间对象。同时，研究通过构建VRT-Bench基准、开发新的评价指标以及收集大规模的训练数据，为评价和训练视觉推理模型提供了新方法。

Result: 实验表明，虽然现有模型能产生正确的最终输出，但在中间推理步骤的支撑上表现较弱。而通过使用VRT-80k训练的模型在追踪推理路径方面取得了显著进展。

Conclusion: 这项工作旨在通过透明和可解释的推理路径来提升多模态模型的视觉推理能力，为未来的相关研究和应用提供了新的基准和训练资源。

Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have significantly improved performance on tasks such as visual grounding and visual question answering. However, the reasoning processes of these models remain largely opaque; they typically output only final predictions without revealing the intermediate steps or fine-grained evidence (e.g., pixels, locations) that lead to the result. This contrasts with human intelligence, which naturally operates through a chain of visual reasoning. To address this limitation, we introduce the Visual Reasoning Tracer (VRT) task, which requires models to not only localize the target object but also explicitly predict the intermediate objects that form the reasoning path. To advance research in this area, we contribute: (1) VRT-Bench, a human-annotated benchmark for evaluating visual reasoning; (2) a new metric for assessing the quality of reasoning traces; and (3) VRT-80k, a large-scale dataset for reasoning model training. Our experiments reveal that while existing models often produce the correct final output, they struggle to ground their intermediate reasoning. In contrast, models trained on VRT-80k achieve substantial improvements in tracing the reasoning path.

</details>


### [88] [EvoIR: Towards All-in-One Image Restoration via Evolutionary Frequency Modulation](https://arxiv.org/abs/2512.05104)
*Jiaqi Ma,Shengkai Hu,Jun Wan,Jiaxing Huang,Lefei Zhang,Salman Khan*

Main category: cs.CV

TL;DR: EvoIR 提出了一种新的用于 All-in-One 图像恢复任务的框架，该框架引入了进化频率调制模块 (FMM) 和进化优化策略 (EOS)，提高了图像结构保真度和细节精度，比单一使用其中任何一种方法效果更好。


<details>
  <summary>Details</summary>
Motivation: 现有的All-in-One图像恢复方法在处理异构降级时通常缺乏显式的频率建模和固定的或启发式的优化策略，这限制了其泛化能力。

Method: EvoIR 引入了进化频率调制模块 (FMM)，通过显式分解特征为高频和低频分支，实现了动态适应的图像恢复。此外，它还采用了一种基于群体的进化优化策略 (EOS)，通过迭代调整频率感知目标，在图像结构准确性和感知保真度之间动态平衡。

Result: EvoIR 在多个基准测试上的实验表明，它在所有-in-one 图像恢复任务中表现优于现有最先进的方法。

Conclusion: EvoIR 是一个有效的框架，可提高图像结构保真度和细节精度，适用于处理All-in-One图像恢复任务中的异构降级，具有重要的实际应用价值。

Abstract: All-in-One Image Restoration (AiOIR) tasks often involve diverse degradation that require robust and versatile strategies. However, most existing approaches typically lack explicit frequency modeling and rely on fixed or heuristic optimization schedules, which limit the generalization across heterogeneous degradation. To address these limitations, we propose EvoIR, an AiOIR-specific framework that introduces evolutionary frequency modulation for dynamic and adaptive image restoration. Specifically, EvoIR employs the Frequency-Modulated Module (FMM) that decomposes features into high- and low-frequency branches in an explicit manner and adaptively modulates them to enhance both structural fidelity and fine-grained details. Central to EvoIR, an Evolutionary Optimization Strategy (EOS) iteratively adjusts frequency-aware objectives through a population-based evolutionary process, dynamically balancing structural accuracy and perceptual fidelity. Its evolutionary guidance further mitigates gradient conflicts across degradation and accelerates convergence. By synergizing FMM and EOS, EvoIR yields greater improvements than using either component alone, underscoring their complementary roles. Extensive experiments on multiple benchmarks demonstrate that EvoIR outperforms state-of-the-art AiOIR methods.

</details>


### [89] [NeuralRemaster: Phase-Preserving Diffusion for Structure-Aligned Generation](https://arxiv.org/abs/2512.05106)
*Yu Zeng,Charles Ochoa,Mingyuan Zhou,Vishal M. Patel,Vitor Guizilini,Rowan McAllister*

Main category: cs.CV

TL;DR: 论文提出了Phase-Preserving Diffusion φ-PD，这是一种能够保持输入相位并随机化幅度的模型通用扩散过程重新制定方法，适用于图像和视频生成任务。通过引入Frequency-Selective Structured (FSS)噪声，该方法提供了对结构刚性连续控制的能力，无需额外架构或参数。实验结果表明，该方法在多种应用中具有显著优势。


<details>
  <summary>Details</summary>
Motivation: 现有的标准扩散过程依赖于添加的高斯噪声来破坏数据，但这种方法会削弱结构的一致性，而Diffusion φ-PD通过保持输入相位来实现结构对齐生成，弥补了这个不足。

Method: 该方法的核心在于保持输入信号的相位同时随机化幅度，进而提出了一种新的噪声，即Frequency-Selective Structured (FSS)噪声，可以通过单一的频域截止参数来调整结果的刚性。此方法兼容于任何图像或视频的Diffusion模型而无需成本。

Result: 实验表明，Diffusion φ-PD在一系列图像和视频生成任务中表现优越，例如真实和风格化的重渲染、模拟到现实场景的提升等。在CARLA模拟器到Waymo规划器的性能测试中，Diffusion φ-PD提高了50%的性能。

Conclusion: Diffusion φ-PD提供了对结构一致性控制的新方法，并且保持了广泛的适用性，是现有条件模型的有效补充。它为图像和视频之间的生成任务提供了一种新的通用框架。

Abstract: Standard diffusion corrupts data using Gaussian noise whose Fourier coefficients have random magnitudes and random phases. While effective for unconditional or text-to-image generation, corrupting phase components destroys spatial structure, making it ill-suited for tasks requiring geometric consistency, such as re-rendering, simulation enhancement, and image-to-image translation. We introduce Phase-Preserving Diffusion φ-PD, a model-agnostic reformulation of the diffusion process that preserves input phase while randomizing magnitude, enabling structure-aligned generation without architectural changes or additional parameters. We further propose Frequency-Selective Structured (FSS) noise, which provides continuous control over structural rigidity via a single frequency-cutoff parameter. φ-PD adds no inference-time cost and is compatible with any diffusion model for images or videos. Across photorealistic and stylized re-rendering, as well as sim-to-real enhancement for driving planners, φ-PD produces controllable, spatially aligned results. When applied to the CARLA simulator, φ-PD improves CARLA-to-Waymo planner performance by 50\%. The method is complementary to existing conditioning approaches and broadly applicable to image-to-image and video-to-video generation. Videos, additional examples, and code are available on our \href{https://yuzeng-at-tri.github.io/ppd-page/}{project page}.

</details>


### [90] [ARM-Thinker: Reinforcing Multimodal Generative Reward Models with Agentic Tool Use and Visual Reasoning](https://arxiv.org/abs/2512.05111)
*Shengyuan Ding,Xinyu Fang,Ziyu Liu,Yuhang Zang,Yuhang Cao,Xiangyu Zhao,Haodong Duan,Xiaoyi Dong,Jianze Liang,Bin Wang,Conghui He,Dahua Lin,Jiaqi Wang*

Main category: cs.CV

TL;DR: ARM-Thinker 是一种自主调用外部工具（如图像裁剪、文档页检索）的多模态奖励模型，能够验证细粒度的视觉细节，跨多页证据交叉引用，验证推理声明，显著提升了奖励模型的准确性和解释性。


<details>
  <summary>Details</summary>
Motivation: 当前的视觉-语言系统在任务执行上存在问题，如幻觉、视觉定位不足以及缺乏工具调用能力，这限制了模型在复杂多模态推理任务中的可靠性。

Method: ARM-Thinker 通过多阶段强化学习训练，联合优化工具调用决策和判断准确性。提出了 ARMBench-VL，包含三个基准测试：细粒度的视觉定位测试、多页文档理解测试和指令遵循测试。

Result: ARM-Thinker 在奖励模型基准测试中平均提高了 16.2%，在工具使用任务中提高了 9.6%，并在多模态数学和逻辑推理基准测试中超越了基线。

Conclusion: 研究成果表明，自主功能显著提高了奖励模型的准确性和解释性。

Abstract: Reward models are critical for aligning vision-language systems with human preferences, yet current approaches suffer from hallucination, weak visual grounding, and an inability to use tools for verification, limiting their reliability on complex multimodal reasoning tasks. We present ARM-Thinker, an A}gentic multimodal Reward Model that autonomously invokes external tools (e.g., image cropping, doc page retrieval) to ground judgments in verifiable evidence, replacing static, non-interactive reward scoring. This enables the model to verify fine-grained visual details, cross-reference multi-page evidence, and validate reasoning claims, which are capabilities absent in existing reward models. We train ARM-Thinker with multi-stage reinforcement learning, jointly optimizing tool-calling decisions and judgment accuracy. To evaluate agentic reward modeling, we introduce ARMBench-VL, comprising three benchmarks that assess fine-grained visual grounding (image-level tools), multi-page document understanding (retrieval tools), and instruction following (text-level verification). ARM-Thinker achieves +16.2% average improvement on reward modeling benchmarks, +9.6% on tool-use tasks, and outperforms baselines on multimodal math and logical reasoning benchmarks. Our results demonstrate that agentic capabilities significantly enhance both accuracy and interpretability of reward models.

</details>


### [91] [Splannequin: Freezing Monocular Mannequin-Challenge Footage with Dual-Detection Splatting](https://arxiv.org/abs/2512.05113)
*Hao-Jen Chien,Yi-Chuan Huang,Chung-Ho Wu,Wei-Lun Chao,Yu-Lun Liu*

Main category: cs.CV

TL;DR: 该研究提出了Splannequin方法，通过动态高斯体绘制技术，实现从单目Mannequin-Challenge视频中合成高保真定格3D场景。方法通过检测和修正高斯原始体的隐藏和损坏状态，解决了单目捕捉条件下时空稀疏监督带来的鬼影和模糊问题，提高了视觉质量并获得用户高度偏好。


<details>
  <summary>Details</summary>
Motivation: 现有的单目动态场景重建方法主要关注动作建模，无法实现用户可控的定格场景。而Mannequin-Challenge视频提供了一个独特的场景，需要在保留微妙动态的同时，将场景定格。

Method: Splannequin方法通过动态高斯体绘制技术，动态建模附近的时间变异，并通过固定模型的时间参数生成静态场景。它还引入了时间锚定机制来修正高斯原始体在时间上未被观察到或被遮挡的情况。

Result: 实验结果证明，Splannequin方法能够有效地解决单目捕捉下的时空稀疏监督问题，提高了视觉质量，用户对其高度偏好，96%的用户更倾向于使用这种方式生成的定格场景。

Conclusion: 该研究的Splannequin技术在不需要修改基础架构的情况下，通过简单的损失项集成到现有的动态高斯系统中，提升了单目Mannequin-Challenge视频生成定格3D场景的视觉效果和用户体验。

Abstract: Synthesizing high-fidelity frozen 3D scenes from monocular Mannequin-Challenge (MC) videos is a unique problem distinct from standard dynamic scene reconstruction. Instead of focusing on modeling motion, our goal is to create a frozen scene while strategically preserving subtle dynamics to enable user-controlled instant selection. To achieve this, we introduce a novel application of dynamic Gaussian splatting: the scene is modeled dynamically, which retains nearby temporal variation, and a static scene is rendered by fixing the model's time parameter. However, under this usage, monocular capture with sparse temporal supervision introduces artifacts like ghosting and blur for Gaussians that become unobserved or occluded at weakly supervised timestamps. We propose Splannequin, an architecture-agnostic regularization that detects two states of Gaussian primitives, hidden and defective, and applies temporal anchoring. Under predominantly forward camera motion, hidden states are anchored to their recent well-observed past states, while defective states are anchored to future states with stronger supervision. Our method integrates into existing dynamic Gaussian pipelines via simple loss terms, requires no architectural changes, and adds zero inference overhead. This results in markedly improved visual quality, enabling high-fidelity, user-selectable frozen-time renderings, validated by a 96% user preference. Project page: https://chien90190.github.io/splannequin/

</details>


### [92] [Light-X: Generative 4D Video Rendering with Camera and Illumination Control](https://arxiv.org/abs/2512.05115)
*Tianqi Liu,Zhaoxi Chen,Zihao Huang,Shaocong Xu,Saining Zhang,Chongjie Ye,Bohan Li,Zhiguo Cao,Wei Li,Hao Zhao,Ziwei Liu*

Main category: cs.CV

TL;DR: 论文提出了Light-X框架，通过解耦几何和照明信号，并利用Light-Syn生成的多视点及多照明对数据集进行训练，提升了摄像机和照明控制的视频生成质量。


<details>
  <summary>Details</summary>
Motivation: 为了更真实地模拟现实场景，论文旨在缓解光照保真度和时间一致性之间的权衡，通过同时控制摄像机轨迹和光照实现了这一目标。

Method: 论文提出了一个解耦几何和照明信号的设计。几何和运动通过沿用户定义的摄像机轨迹投影的动态点云捕获，而照明线索由与同一几何体一致投影的重光照帧提供。此外，为了平衡数据中的多视角和多照明关系，论文还提出一种基于退化处理的生成管道Light-Syn，从野外单目视频中合成训练对。

Result: 实验结果表明，Light-X方法在联合摄像机和光照控制的视频生成中优于基线方法，尤其是在文本和背景条件设置方面。

Conclusion: 该论文通过引入Light-X框架和Light-Syn合成数据集的方法，提出了一种新的摄像机和光照控制的视频生成解决方案。

Abstract: Recent advances in illumination control extend image-based methods to video, yet still facing a trade-off between lighting fidelity and temporal consistency. Moving beyond relighting, a key step toward generative modeling of real-world scenes is the joint control of camera trajectory and illumination, since visual dynamics are inherently shaped by both geometry and lighting. To this end, we present Light-X, a video generation framework that enables controllable rendering from monocular videos with both viewpoint and illumination control. 1) We propose a disentangled design that decouples geometry and lighting signals: geometry and motion are captured via dynamic point clouds projected along user-defined camera trajectories, while illumination cues are provided by a relit frame consistently projected into the same geometry. These explicit, fine-grained cues enable effective disentanglement and guide high-quality illumination. 2) To address the lack of paired multi-view and multi-illumination videos, we introduce Light-Syn, a degradation-based pipeline with inverse-mapping that synthesizes training pairs from in-the-wild monocular footage. This strategy yields a dataset covering static, dynamic, and AI-generated scenes, ensuring robust training. Extensive experiments show that Light-X outperforms baseline methods in joint camera-illumination control and surpasses prior video relighting methods under both text- and background-conditioned settings.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [93] [On GRPO Collapse in Search-R1: The Lazy Likelihood-Displacement Death Spiral](https://arxiv.org/abs/2512.04220)
*Wenlong Deng,Yushu Li,Boying Gong,Yi Ren,Christos Thrampoulidis,Xiaoxiao Li*

Main category: cs.CL

TL;DR: 该研究揭示了在一个特定环境下，搜索集成的强化学习方法（Search-R1）中的Lazy Likelihood Displacement (LLD)现象导致了训练崩溃，并提出了一种轻量级的正则化方法LLDS来解决这一问题。实验表明，该方法能稳定训练，防止梯度爆炸，并大幅提高性能。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决在工具集成强化学习（TIRL）中，尤其是使用GRPO方法时出现的训练崩溃问题，以提高模型性能并实现更大规模的训练。

Method: 研究者首先通过实验确认了LLD现象的存在及其在训练过程中的作用。随后，他们提出了一种名为LLDS的轻量级正则化技术，该技术在Likelihood下降时启动，且仅矫正受影响的令牌，避免过度干涉优化过程。

Result: 实验结果表明，LLDS能够有效防止LLD的发生，稳定训练过程，避免了梯度爆炸，显著提高模型在多个公开领域和多跳问答任务中的性能。具体来说，LLDS在Qwen2.5-3B和Qwen2.5-7B模型上的性能分别提高了37.8%和32.0%。

Conclusion: 该研究确定了LLD是基于GRPO的TIRL的主要瓶颈，并提出了LLDS来克服这一问题，为稳定和扩展工具集成的大型语言模型的训练提供了实际可行的方法。

Abstract: Tool-integrated (TI) reinforcement learning (RL) enables large language models (LLMs) to perform multi-step reasoning by interacting with external tools such as search engines and retrievers. Group Relative Policy Optimization (GRPO), exemplified by the recent Search-R1, offers fast convergence and a value-free formulation that makes it appealing for this setting, yet consistently suffers from training collapse. We identify Lazy Likelihood Displacement (LLD), a systematic reduction or stagnation in the likelihood of both correct and incorrect responses, as the core mechanism driving this failure. LLD emerges early and triggers a self-reinforcing LLD Death Spiral, where declining likelihood leads to low-confidence responses, inflating gradients, and ultimately causing collapse. We empirically characterize this process across models on a Search-R1-style, search-integrated question answering task, revealing a consistent three-phase trajectory: early stagnation, steady decay, and accelerated collapse. To address this, we propose a lightweight likelihood-preserving regularization LLDS for GRPO that activates only when a trajectory's likelihood decreases, and regularizes only the tokens responsible. This fine-grained structure mitigates LLD with minimal interference to optimization. Across seven open-domain and multi-hop QA benchmarks, our method stabilizes training, prevents gradient explosion, and yields substantial performance improvements, including +37.8% gains on Qwen2.5-3B and +32.0% gains on Qwen2.5-7B. Our results establish LLD as a fundamental bottleneck in GRPO-based TIRL and provide a practical path toward stable, scalable training of tool-integrated LLM.

</details>


### [94] [Computational Linguistics Meets Libyan Dialect: A Study on Dialect Identification](https://arxiv.org/abs/2512.04257)
*Mansour Essgaer,Khamis Massud,Rabia Al Mamlook,Najah Ghmaid*

Main category: cs.CL

TL;DR: 本研究利用视听分析识别出与阿拉伯语方言分类无关的特征，使用QADI语料库和不同长度的单词和字符n-gram表示方法对多种分类器进行评估，发现Multinomial Naive Bayes在识别利比亚方言方面具有最高准确性和F1分数。


<details>
  <summary>Details</summary>
Motivation: 由于阿拉伯语言的广泛性和复杂性，需要一种有效的方法来识别不同方言。本研究旨在通过分析利比亚方言的推特数据，确定最佳的特征选择和分类模型。

Method: 对收集的数据集应用了预处理技术，如处理不一致的拼写和语法。使用了多个统计和机器学习方法，包括chi-square特征选择、n-gram表示方法（单词和字符）以及几种分类器（逻辑回归、线性支持向量机、多项式朴素贝叶斯、伯努利朴素贝叶斯）。

Result: 研究结果表明，多项式朴素贝叶斯在使用（1,2）单词n-gram和（1,5）字符n-gram表示时，达到了最高85.89%的准确率和0.85741的F1分数。其他分类器如逻辑回归和线性支持向量机虽然表现稍逊，但也表现良好。

Conclusion: 该研究表明，选择适当的n-gram表示和分类模型对提高识别阿拉伯语方言的准确性至关重要。研究结果为处理阿拉伯方言识别问题提供了基准和启示，有助于未来在此领域的研究。

Abstract: This study investigates logistic regression, linear support vector machine, multinomial Naive Bayes, and Bernoulli Naive Bayes for classifying Libyan dialect utterances gathered from Twitter. The dataset used is the QADI corpus, which consists of 540,000 sentences across 18 Arabic dialects. Preprocessing challenges include handling inconsistent orthographic variations and non-standard spellings typical of the Libyan dialect. The chi-square analysis revealed that certain features, such as email mentions and emotion indicators, were not significantly associated with dialect classification and were thus excluded from further analysis. Two main experiments were conducted: (1) evaluating the significance of meta-features extracted from the corpus using the chi-square test and (2) assessing classifier performance using different word and character n-gram representations. The classification experiments showed that Multinomial Naive Bayes (MNB) achieved the highest accuracy of 85.89% and an F1-score of 0.85741 when using a (1,2) word n-gram and (1,5) character n-gram representation. In contrast, Logistic Regression and Linear SVM exhibited slightly lower performance, with maximum accuracies of 84.41% and 84.73%, respectively. Additional evaluation metrics, including log loss, Cohen kappa, and Matthew correlation coefficient, further supported the effectiveness of MNB in this task. The results indicate that carefully selected n-gram representations and classification models play a crucial role in improving the accuracy of Libyan dialect identification. This study provides empirical benchmarks and insights for future research in Arabic dialect NLP applications.

</details>


### [95] [SQuARE: Structured Query & Adaptive Retrieval Engine For Tabular Formats](https://arxiv.org/abs/2512.04292)
*Chinmay Gondhalekar,Urjitkumar Patel,Fang-Chun Yeh*

Main category: cs.CL

TL;DR: SQuARE 是一种基于表格的混合检索框架，能够有效处理多行表头、合并单元格和单位标注等问题，提供比单一策略基线和 ChatGPT-4o 更高的检索精度和端到端回答准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的方法难以处理具有多行表头、合并单元格和单位标注的电子表格，SQuARE 框架旨在克服这些挑战。

Method: SQuARE 框架结合了结构保存的片段检索和基于 SQL 的关系表示，通过计算基于表头深度和合并密度的连续评分来决定查询的处理方式。

Result: SQuARE 在多项测试数据集上表现优于单一策略基线和 ChatGPT-4o，提高了检索精度和端到端回答准确性。

Conclusion: SQuARE 提供了一种灵活且高效的解决方案，用于理解和检索复杂表格中的信息，为未来的表理解技术提供了实用桥梁。

Abstract: Accurate question answering over real spreadsheets remains difficult due to multirow headers, merged cells, and unit annotations that disrupt naive chunking, while rigid SQL views fail on files lacking consistent schemas. We present SQuARE, a hybrid retrieval framework with sheet-level, complexity-aware routing. It computes a continuous score based on header depth and merge density, then routes queries either through structure-preserving chunk retrieval or SQL over an automatically constructed relational representation. A lightweight agent supervises retrieval, refinement, or combination of results across both paths when confidence is low. This design maintains header hierarchies, time labels, and units, ensuring that returned values are faithful to the original cells and straightforward to verify. Evaluated on multi-header corporate balance sheets, a heavily merged World Bank workbook, and diverse public datasets, SQuARE consistently surpasses single-strategy baselines and ChatGPT-4o on both retrieval precision and end-to-end answer accuracy while keeping latency predictable. By decoupling retrieval from model choice, the system is compatible with emerging tabular foundation models and offers a practical bridge toward a more robust table understanding.

</details>


### [96] [ClusterFusion: Hybrid Clustering with Embedding Guidance and LLM Adaptation](https://arxiv.org/abs/2512.04350)
*Yiming Xu,Yuan Yuan,Vijay Viswanathan,Graham Neubig*

Main category: cs.CL

TL;DR: ClusterFusion 是一种结合轻量级嵌入方法和大型语言模型的混合聚类框架，分为嵌入引导子集划分、LLM 驱动主题总结和 LLM 基础主题分配三个阶段，从而直接整合领域知识和用户偏好，显著提高了聚类性能。


<details>
  <summary>Details</summary>
Motivation: 传统的聚类算法通常需要昂贵的微调才能适应特定领域，而大型语言模型虽然提供了强大的上下文推理能力，但以往工作主要将其用作辅助模块来优化嵌入或调整聚类边界。因此，提出了一种名为 ClusterFusion 的混合框架，通过将大型语言模型作为聚类核心来充分利用其上下文适应性。

Method: ClusterFusion 框架分为三个阶段：嵌入引导子集划分、LLM 驱动主题总结和 LLM 基础主题分配。具体来说，嵌入导向子集划分使用轻量级嵌入方法进行初步划分；LLM 驱动主题总结通过对每个子集生成总结文本来进一步细化主题；LLM 基础主题分配则利用 LLM 来进行最终的主题分配。

Result: 在三项公开基准和两种新领域特定数据集上的实验表明，ClusterFusion 不仅在标准任务上达到了最先进的性能，还在专门领域提供了实质性的改进。

Conclusion: 该研究提出了 ClusterFusion 框架，并通过实验验证其在领域特定数据集上的优越性能。同时，作者还提供了一个新的数据集和所有基准的结果，以支持未来的研究工作。

Abstract: Text clustering is a fundamental task in natural language processing, yet traditional clustering algorithms with pre-trained embeddings often struggle in domain-specific contexts without costly fine-tuning. Large language models (LLMs) provide strong contextual reasoning, yet prior work mainly uses them as auxiliary modules to refine embeddings or adjust cluster boundaries. We propose ClusterFusion, a hybrid framework that instead treats the LLM as the clustering core, guided by lightweight embedding methods. The framework proceeds in three stages: embedding-guided subset partition, LLM-driven topic summarization, and LLM-based topic assignment. This design enables direct incorporation of domain knowledge and user preferences, fully leveraging the contextual adaptability of LLMs. Experiments on three public benchmarks and two new domain-specific datasets demonstrate that ClusterFusion not only achieves state-of-the-art performance on standard tasks but also delivers substantial gains in specialized domains. To support future work, we release our newly constructed dataset and results on all benchmarks.

</details>


### [97] [MASE: Interpretable NLP Models via Model-Agnostic Saliency Estimation](https://arxiv.org/abs/2512.04386)
*Zhou Yang,Shunyan Luo,Jiazhen Zhu,Fang Jin*

Main category: cs.CL

TL;DR: MASE框架通过使用规范化线性高斯扰动来局部解释文本预测模型，无需深入了解模型内部架构，展示出在Δ准确性方面优于其他模型无偏见解释方法的潜力。


<details>
  <summary>Details</summary>
Motivation: 传统方法在解释涉及离散单词数据的NLP模型的复杂决策过程时存在局限性，MASE旨在提供一种对这些模型进行无偏见解释的框架。

Method: MASE框架利用规范化线性高斯扰动（NLGP）作用于嵌入层来估计输入的显著性，而不是直接作用于原始单词输入。

Result: 实验结果表明，MASE在Δ准确性上优于其他模型无偏见解释方法。

Conclusion: MASE框架被视为解释文本基础模型在NLP中的操作有用的工具。

Abstract: Deep neural networks (DNNs) have made significant strides in Natural Language Processing (NLP), yet their interpretability remains elusive, particularly when evaluating their intricate decision-making processes. Traditional methods often rely on post-hoc interpretations, such as saliency maps or feature visualization, which might not be directly applicable to the discrete nature of word data in NLP. Addressing this, we introduce the Model-agnostic Saliency Estimation (MASE) framework. MASE offers local explanations for text-based predictive models without necessitating in-depth knowledge of a model's internal architecture. By leveraging Normalized Linear Gaussian Perturbations (NLGP) on the embedding layer instead of raw word inputs, MASE efficiently estimates input saliency. Our results indicate MASE's superiority over other model-agnostic interpretation methods, especially in terms of Delta Accuracy, positioning it as a promising tool for elucidating the operations of text-based models in NLP.

</details>


### [98] [RapidUn: Influence-Driven Parameter Reweighting for Efficient Large Language Model Unlearning](https://arxiv.org/abs/2512.04457)
*Guoshenghui Zhao,Huawei Lin,Weijie Zhao*

Main category: cs.CL

TL;DR: RapidUn提供了一种高效且参数经济的忘却框架，通过快速估计每个样本的影响并将其转化为自适应更新权重，实现选择性参数更新，从而在保持一般知识的同时忘记有害行为。与全面重训练相比，RapidUn的效率提高了100倍。


<details>
  <summary>Details</summary>
Motivation: 传统的全面重训练方法成本高昂，且现有的近似忘却方法在面对小或不平衡的忘却集时常常不稳定。因此，研究提出了一个基于影响的、参数效率高的快速忘却框架RapidUn。

Method: RapidUn框架首先通过快速估计模块估算每个样本的影响，然后将这些分数映射为自适应更新权重，以引导有选择性的参数更新。

Result: 在Mistral-7B和Llama-3-8B模型上，RapidUn在Dolly-15k和Alpaca-57k数据集上的结果表明，其效率比全面重训练高100倍，且在分布内和分布外记忆丧失方面，RapidUn优于Fisher、GA和LoReUn方法。

Conclusion: 这些结果证明了基于影响的参数重新加权方法对于大型语言模型忘却是一个可扩展且可解释的框架。

Abstract: Removing specific data influence from large language models (LLMs) remains challenging, as retraining is costly and existing approximate unlearning methods are often unstable. The challenge is exacerbated when the forget set is small or imbalanced. We introduce RapidUn, an influence-driven and parameter-efficient unlearning framework. It first estimates per-sample influence through a fast estimation module, then maps these scores into adaptive update weights that guide selective parameter updates -- forgetting harmful behavior while retaining general knowledge. On Mistral-7B and Llama-3-8B across Dolly-15k and Alpaca-57k, RapidUn achieves up to 100 times higher efficiency than full retraining and consistently outperforms Fisher, GA, and LoReUn on both in-distribution and out-of-distribution forgetting. These results establish influence-guided parameter reweighting as a scalable and interpretable paradigm for LLM unlearning.

</details>


### [99] [UW-BioNLP at ChemoTimelines 2025: Thinking, Fine-Tuning, and Dictionary-Enhanced LLM Systems for Chemotherapy Timeline Extraction](https://arxiv.org/abs/2512.04518)
*Tianmai M. Zhang,Zhaoyi Sun,Sihang Zeng,Chenxi Li,Neil F. Abernethy,Barbara D. Lam,Fei Xia,Meliha Yetisgen*

Main category: cs.CL

TL;DR: 该研究描述了从电子健康记录中构建癌症患者化疗时间线的方法，评估了多种策略，并指出Qwen3-14B最优。


<details>
  <summary>Details</summary>
Motivation: 为了通过电子健康记录中的临床笔记构建癌症患者的化疗时间线。

Method: 采用两种步骤的工作流程：首先使用LLM从单个临床笔记中提取化疗事件，然后将这些事件规范化并聚合到患者级别的时间线上。评估了链式思考、监督微调、直接偏好优化和基于词典的查找策略。

Result: 多种方法在测试集排行榜上表现竞相当前，其中微调后的Qwen3-14B获得最佳官方得分0.678。

Conclusion: 结果和分析为未来在此任务上的尝试和类似任务的设计提供了有用的见解。

Abstract: The ChemoTimelines shared task benchmarks methods for constructing timelines of systemic anticancer treatment from electronic health records of cancer patients. This paper describes our methods, results, and findings for subtask 2 -- generating patient chemotherapy timelines from raw clinical notes. We evaluated strategies involving chain-of-thought thinking, supervised fine-tuning, direct preference optimization, and dictionary-based lookup to improve timeline extraction. All of our approaches followed a two-step workflow, wherein an LLM first extracted chemotherapy events from individual clinical notes, and then an algorithm normalized and aggregated events into patient-level timelines. Each specific method differed in how the associated LLM was utilized and trained. Multiple approaches yielded competitive performances on the test set leaderboard, with fine-tuned Qwen3-14B achieving the best official score of 0.678. Our results and analyses could provide useful insights for future attempts on this task as well as the design of similar tasks.

</details>


### [100] [EvoEdit: Lifelong Free-Text Knowledge Editing through Latent Perturbation Augmentation and Knowledge-driven Parameter Fusion](https://arxiv.org/abs/2512.04545)
*Pengfei Cao,Zeao Ji,Daojian Zeng,Jun Zhao,Kang Liu*

Main category: cs.CL

TL;DR: 本文提出了一个新的任务——终身自由文本知识编辑 (LF-Edit)，旨在使模型能够处理自然语言表达的知识更新，并支持持续编辑。为了解决这一挑战，作者构建了一个大型基准数据集MRLF-Bench，开发了一种基于认知的多级评估框架，并提出了一种新颖的方法EvoEdit来改进知识注入和保留先验信息。


<details>
  <summary>Details</summary>
Motivation: 现有的知识编辑方法存在依赖结构化三元组、无法捕捉事实之间的细微关系和仅支持一次性知识更新等问题。因此，需要一个新任务——终身自由文本知识编辑 (LF-Edit)，可以处理自然语言表达的知识更新并支持持续编辑。

Method: 作者设计了一个名为EvoEdit的新型方法，利用潜在扰动增广和知识驱动参数融合来改进知识注入并保留先验信息。同时，他们构建了一个包含16,835个自然语言编辑请求的大型基准数据集MRLF-Bench，并提出了一种基于认知的多级评估框架，涵盖记忆、理解、约束理解和推理四个层次。

Result: 实验结果表明，EvoEdit在提出的LF-Edit任务上的表现显著优于现有的知识编辑方法。

Conclusion: 本文通过构建新的任务、方法以及评估框架，为处理自然语言表达的知识更新并支持持续编辑提供了新的思路，EvoEdit作为一种新的方法，在LF-Edit任务上表现出色。

Abstract: Adjusting the outdated knowledge of large language models (LLMs) after deployment remains a major challenge. This difficulty has spurred the development of knowledge editing, which seeks to accurately and efficiently modify a model's internal (parametric) knowledge without retraining it from scratch. However, existing methods suffer from two limitations. First, they depend on structured triplets that are misaligned with the free-text nature of LLM pretraining and fail to capture the nuanced relationships among facts. Second, they typically support one-time knowledge updates, with relatively limited research on the problem of sequential or lifelong editing. To address these gaps, we propose a new task, Lifelong Free-text Knowledge Editing (LF-Edit), which enables models to incorporate updates expressed in natural language and supports continual editing over time. Despite its promise, LF-Edit faces the dual challenge of integrating new knowledge while mitigating the forgetting of prior information. To foster research on this new task, we construct a large-scale benchmark, Multi-Rank Lifelong Free-text Editing Benchmark (MRLF-Bench), containing 16,835 free-text edit requests. We further design a cognitively inspired multi-rank evaluation framework encompassing four levels: memorization, understanding, constrained comprehension, and reasoning. To tackle the challenges inherent in LF-Edit, we introduce a novel approach named EvoEdit that enhances knowledge injection through Latent Perturbation Augmentation and preserves prior information via Knowledge-driven Parameter Fusion. Experimental results demonstrate that EvoEdit substantially outperforms existing knowledge editing methods on the proposed LF-Edit task.

</details>


### [101] [AdmTree: Compressing Lengthy Context with Adaptive Semantic Trees](https://arxiv.org/abs/2512.04550)
*Yangning Li,Shaoshen Chen,Yinghui Li,Yankai Chen,Hai-Tao Zheng,Hui Wang,Wenhao Jiang,Philip S. Yu*

Main category: cs.CL

TL;DR: AdmTree 提出了一种新颖的自适应层次化上下文压缩框架，它通过信息密度动态分割输入，使用摘要的 gist tokens 和轻量级聚合机制形成一个语义二叉树，从而高效地保留了长上下文的语义信息，同时减少了位置偏差。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型（LLMs）受到自注意力的二次复杂性限制，在处理长上下文时存在计算瓶颈，这对许多高级应用至关重要。现有的上下文压缩方法往往难以同时保持局部细节和长范围语义依赖性。

Method: AdmTree 方法通过根据信息密度动态分割输入，并在语义二叉树的叶节点中使用摘要的 gist tokens 汇总变量长度的片段。同时运用轻量级聚合机制和冻结骨干大型语言模型，以最小化新的可训练参数，实现高效的层次抽象。

Result: AdmTree 能够保留细粒度的细节和全局语义一致性，减轻位置偏差，动态适应内容，从而在保留长上下文语义信息方面表现出高度的鲁棒性。

Conclusion: AdmTree 为长期上下文建模提供了一种有效的解决方案，通过自适应的层次压缩机制提高了处理大规模输入的能力。

Abstract: The quadratic complexity of self-attention constrains Large Language Models (LLMs) in processing long contexts, a capability essential for many advanced applications. Context compression aims to alleviate this computational bottleneck while retaining critical semantic information. However, existing approaches often fall short: explicit methods may compromise local detail, whereas implicit methods can suffer from positional biases, information degradation, or an inability to capture long-range semantic dependencies. We propose AdmTree, a novel framework for adaptive, hierarchical context compression with a central focus on preserving high semantic fidelity while maintaining efficiency. AdmTree dynamically segments input based on information density, utilizing gist tokens to summarize variable-length segments as the leaves of a semantic binary tree. This structure, together with a lightweight aggregation mechanism and a frozen backbone LLM (thereby minimizing new trainable parameters), enables efficient hierarchical abstraction of the context. By preserving fine-grained details alongside global semantic coherence, mitigating positional bias, and dynamically adapting to content, AdmTree robustly retains the semantic information of long contexts.

</details>


### [102] [ADAPT: Learning Task Mixtures for Budget-Constrained Instruction Tuning](https://arxiv.org/abs/2512.04555)
*Pritam Kadasi,Abhishek Upperwal,Mayank SIngh*

Main category: cs.CL

TL;DR: ADAPT 是一个元学习算法，能够在多任务指令调优中学习任务采样比例，适应性地分配预算以提高下游性能。


<details>
  <summary>Details</summary>
Motivation: 目前的方法通常手动固定任务权重，这可能导致资源分配不当。ADAPT通过维护一个连续的任务分布并使用元梯度更新该分布，实现了更好的资源分配。

Method: ADAPT通过维护一个连续的任务分布，并通过计算最坏情况验证目标的光滑元梯度来更新该分布。具体实施时，ADAPT在不同模型上进行训练，同时控制训练所使用的监督样本数量预算。

Result: 在跨领域基准测试中，ADAPT的表现接近或略优于静态混合的最佳基线，同时有效减少了使用的训练令牌数量，并将预算重新分配给更困难、与基准对齐的任务。

Conclusion: ADAPT能够在预算约束下更智能地调整任务分配，从而提高多任务指令调优的效率和效果。

Abstract: We propose ADAPT, a meta-learning algorithm that \emph{learns} task sampling proportions under an explicit token budget for multi-task instruction tuning. Instead of fixing task weights by hand, \adapt{} maintains a continuous distribution over tasks and updates it via meta-gradients of a smooth worst-case validation objective, inducing an adaptive curriculum that allocates more tokens to useful tasks while avoiding collapse. We instantiate ADAPT on three $\sim$1B-parameter open-weight LLMs (Gemma-3-1B, LLaMA-3.2-1B, Qwen-0.6B), training on 20 Natural Instructions task types under budgets of $1\%$, $5\%$, and $10\%$ of the available supervised tokens, and compare against strong supervised fine-tuning baselines with uniform and size-proportional mixing. We conduct evaluations on 11 out-of-domain benchmarks spanning reasoning, reading comprehension, code generation, and instruction following, we find that ADAPT matches or slightly improves average downstream performance relative to the best static mixture, while using fewer effective training tokens and reallocating budget toward harder, benchmark-aligned tasks.

</details>


### [103] [LexGenius: An Expert-Level Benchmark for Large Language Models in Legal General Intelligence](https://arxiv.org/abs/2512.04578)
*Wenjin Liu,Haoran Luo,Xin Feng,Xiang Ji,Lijuan Zhou,Rui Mao,Jiapu Wang,Shirui Pan,Erik Cambria*

Main category: cs.CL

TL;DR: 本文提出了一种名为LexGenius的中文法律基准测试，旨在评估大语言模型的法律智能，涵盖七个维度、十一个任务和二十项能力，通过对大量实际法律案例和考题的分析，发现当前最先进的大语言模型在法律智能方面仍存在差距。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试主要关注结果而忽视了系统地评估大语言模型的法律智能，阻碍了法律通用智能的发展。因此，作者创建了LexGenius基准测试，以期更好地推动法律通用智能的研究和应用。

Method: LexGenius基准测试采用了维度-任务-能力框架，利用手动和大语言模型共同审查的方式生成包含多种选择题的测试题库，并经过多次审核以确保数据准确性和可靠性。通过对比12个最新的大语言模型进行评估。

Result: 研究表明，即便是在法律智能方面表现最好的大语言模型，仍然无法完全媲美人类法律专业人士。LexGenius测试结果表明了各个大语言模型在法律智能方面存在的显著差异。

Conclusion: 作者认为，LexGenius可以更好地评估大语言模型的法律智能，并促进法律通用人工智能的发展。

Abstract: Legal general intelligence (GI) refers to artificial intelligence (AI) that encompasses legal understanding, reasoning, and decision-making, simulating the expertise of legal experts across domains. However, existing benchmarks are result-oriented and fail to systematically evaluate the legal intelligence of large language models (LLMs), hindering the development of legal GI. To address this, we propose LexGenius, an expert-level Chinese legal benchmark for evaluating legal GI in LLMs. It follows a Dimension-Task-Ability framework, covering seven dimensions, eleven tasks, and twenty abilities. We use the recent legal cases and exam questions to create multiple-choice questions with a combination of manual and LLM reviews to reduce data leakage risks, ensuring accuracy and reliability through multiple rounds of checks. We evaluate 12 state-of-the-art LLMs using LexGenius and conduct an in-depth analysis. We find significant disparities across legal intelligence abilities for LLMs, with even the best LLMs lagging behind human legal professionals. We believe LexGenius can assess the legal intelligence abilities of LLMs and enhance legal GI development. Our project is available at https://github.com/QwenQKing/LexGenius.

</details>


### [104] [Geschlechtsübergreifende Maskulina im Sprachgebrauch Eine korpusbasierte Untersuchung zu lexemspezifischen Unterschieden](https://arxiv.org/abs/2512.04683)
*Carolin Mueller-Spitzer,Samira Ochs,Jan Oliver Ruediger,Sascha Wolfer*

Main category: cs.CL

TL;DR: 本研究通过大规模新闻文本语料库分析了通用男性代词在当代德语中的分布和语言特征，揭示了不同类型人名词之间的显著差异，并提供了有关其实际用法的实证见解。


<details>
  <summary>Details</summary>
Motivation: 研究旨在填补关于通用男性代词（GM）实际使用情况的数据库分析空白，解决学术界和公众在通用男性代词的性别中立性上的争议。

Method: 研究通过手动注释21个人名词的全部变位形式，共6,195个标记的单位，来探讨不同类型人名词之间的差异，并分析其在语法层面的使用情况。

Result: 研究结果表明，通用男性代词在语料库中以复数形式和不定名词短语中出现最为频繁，并且其使用方式与以往认为的不完全一致。

Conclusion: 研究结论认为，通用男性代词的使用更加多样化，应更加关注实际用语而非仅依赖于理论假设。

Abstract: This study examines the distribution and linguistic characteristics of generic masculines (GM) in contemporary German press texts. The use of masculine personal nouns to refer to mixed-gender groups or unspecified individuals has been widely debated in academia and the public, with con-flicting perspectives on its gender-neutrality. While psycholinguistic studies suggest that GM is more readily associated with male referents, corpus-based analyses of its actual use remain scarce. We investigate GM in a large corpus of press texts, focusing on lexeme-specific differences across dif-ferent types of personal nouns. We conducted manual annotations of the whole inflectional para-digm of 21 personal nouns, resulting in 6,195 annotated tokens. Our findings reveal considerable differences between lexical items, especially between passive role nouns and prestige-related per-sonal nouns. On a grammatical level, we find that GM occurs predominantly in the plural and in indefinite noun phrases. Furthermore, our data shows that GM is not primarily used to denote entire classes of people, as has been previously claimed. By providing an empirical insight into the use of GM in authentic written language, we contribute to a more nuanced understanding of its forms and manifestations. These findings provide a solid basis for aligning linguistic stimuli in psy-cholinguistic studies more closely with real-world language use.

</details>


### [105] [SignRoundV2: Closing the Performance Gap in Extremely Low-Bit Post-Training Quantization for LLMs](https://arxiv.org/abs/2512.04746)
*Wenhua Cheng,Weiwei Zhang,Heng Guo,Haihao Shen*

Main category: cs.CL

TL;DR: SignRoundV2 是一种后训练量化框架，即使在不使用混合精度的情况下也能有效进行低比特量化，通过引入基于梯度信息和量化偏差的快速灵敏度指标进行逐层位分配，以及轻量级预调搜索来优化量化标尺。


<details>
  <summary>Details</summary>
Motivation: 介绍了 SignRoundV2 量化框架，旨在解决极端低比特量化导致的性能下降问题，特别是在部署大型语言模型时。

Method: SignRoundV2 通过引入快速灵敏度指标结合梯度信息和量化偏差实现逐层位分配，并利用轻量级预调搜索优化量化标尺。

Result: 实验结果表明，SignRoundV2 在保持 LLM 竞争精度的同时，使用 4-5 比特时误差不超过 1%，2 比特时也有较强表现。

Conclusion: SignRoundV2 为低比特量化的高效部署提供了有效解决方案，证明了其在实际生产环境下的可行性和效率。

Abstract: Extreme low-bit quantization is critical for efficiently deploying Large Language Models (LLMs), yet it often leads to severe performance degradation at 2-bits and even 4-bits (e.g., MXFP4). We present SignRoundV2, a post-training quantization framework that is highly effective even without mixed-precision. SignRoundV2 introduces (1) a fast sensitivity metric that combines gradient information with quantization-induced deviations to guide layer-wise bit allocation, and (2) a lightweight pre-tuning search for quantization scales to improve extremely low-bit quantization. These components allow SignRoundV2 to close the gap with full-precision models. Extensive experiments indicate that our method sustains competitive accuracy for LLMs, achieving production-grade performance with about 1 percent variance at 4-5 bits and strong results even at 2 bits. The implementation is available at https://github.com/intel/auto-round.

</details>


### [106] [Model Whisper: Steering Vectors Unlock Large Language Models' Potential in Test-time](https://arxiv.org/abs/2512.04748)
*Xinyue Kang,Diwei Shi,Li Chen*

Main category: cs.CL

TL;DR: 该研究提出了一种轻量级的Test-Time Steering Vectors (TTSV) 方法，可以在不调整模型参数的情况下提升大型语言模型在特定任务上的推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前的测试时适应方法通常需要调整模型参数，这不仅计算成本高，还可能损害模型的预训练能力。因此，研究者提出了TTSV方法来解决这一问题。

Method: TTSV方法在模型输入前增加了一个轻量级向量，通过在测试数据上优化该向量来最小化模型输出的不确定性，从而引导模型进入更自信的状态。

Result: 实验表明，TTSV方法显著提升了模型在数学任务上的表现，如在MATH500任务上，Qwen2.5-Math-7B模型和Qwen3-4B模型分别实现了45.88%和16.22%的相对性能提升。

Conclusion: 该方法不仅轻量有效，还能跨不同任务进行迁移，展示了强大的鲁棒性和通用性。

Abstract: It is a critical challenge to efficiently unlock the powerful reasoning potential of Large Language Models (LLMs) for specific tasks or new distributions. Existing test-time adaptation methods often require tuning model parameters, which is not only computationally expensive but also risks degrading the model's pre-existing abilities.To address this, we introduce a lightweight component, Test-Time Steering Vectors (TTSV), which is prepended to the input while keeping the LLM's parameters entirely frozen. By optimizing the TTSV on test data to minimize the model's output entropy, we steer the model towards an internal state of higher confidence, activating its inherent abilities most relevant to the current task. TTSV is both lightweight and highly efficient to optimize, making it a true plug-and-play enhancement. Extensive experiments validate our approach's effectiveness on both base models and reasoning-enhanced models. For instance, on the MATH500 task, TTSV achieves a 45.88% relative performance gain on the Qwen2.5-Math-7B model and a 16.22% relative gain on the Qwen3-4B model. Furthermore, our approach exhibits robust generalization, with its steering vectors proving highly transferable across diverse tasks.

</details>


### [107] [EtCon: Edit-then-Consolidate for Reliable Knowledge Editing](https://arxiv.org/abs/2512.04753)
*Ruilin Li,Yibin Wang,Wenhong Zhu,Chenglin Li,Jinghao Zhang,Chenliang Li,Junchi Yan,Jiaqi Wang*

Main category: cs.CL

TL;DR: 提出了Edit-then-Consolidate框架，通过目标导向的局部调整和群组相对策略优化，以解决知识编辑中过度拟合和知识整合不足的问题，从而在实际应用中提高编辑的可靠性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有知识编辑方法在理论和实际应用中的表现存在较大差距，主要是由于过度拟合和缺乏知识整合阶段导致的新知识无法充分融入到模型的推理过程中。

Method: 编辑阶段使用目标导向的局部调整（TPSFT），并通过策略约束限制策略漂移；整合阶段采用群组相对策略优化（GRPO），优化轨迹级别行为以与基于CoT的推理策略对齐。

Result: 实验表明，该框架能有效提升知识编辑的可靠性和泛化能力，同时保持局部特性和预训练能力。

Conclusion: 该研究提供了一种新的知识编辑方法，旨在解决现有方法的局限性，对于实际应用中的知识更新具有重要意义。

Abstract: Knowledge editing aims to update specific facts in large language models (LLMs) without full retraining. Prior efforts sought to tune the knowledge layers of LLMs, proving effective for making selective edits. However, a significant gap exists between their performance in controlled, teacher-forcing evaluations and their real-world effectiveness in lifelong learning scenarios, which greatly limits their practical applicability. This work's empirical analysis reveals two recurring issues associated with this gap: (1) Most traditional methods lead the edited model to overfit to the new fact, thereby degrading pre-trained capabilities; (2) There is a critical absence of a knowledge consolidation stage, leaving new facts insufficiently integrated into LLMs' inference-time behavior under autoregressive generation, thereby leading to a mismatch between parametric knowledge and actual generation behavior. To this end, we propose Edit-then-Consolidate, a novel knowledge editing paradigm that aims to bridge the gap between theoretical knowledge editing methods and their real-world applicability. Specifically, (1) our framework mitigates overfitting via Targeted Proximal Supervised Fine-Tuning (TPSFT) that localizes the edit via a trust-region objective to limit policy drift; (2) Then, a consolidation stage using Group Relative Policy Optimization (GRPO) aligns the edited knowledge with CoT-based inference policy by optimizing trajectory-level behavior under comprehensive reward signals. Extensive experiments demonstrate our framework consistently improves editing reliability and generalization under real-world evaluations, while better preserving locality and pre-trained capabilities.

</details>


### [108] [Challenging the Abilities of Large Language Models in Italian: a Community Initiative](https://arxiv.org/abs/2512.04759)
*Malvina Nissim,Danilo Croce,Viviana Patti,Pierpaolo Basile,Giuseppe Attanasio,Elio Musacchio,Matteo Rinaldi,Federico Borazio,Maria Francis,Jacopo Gili,Daniel Scalena,Begoña Altuna,Ekhi Azurmendi,Valerio Basile,Luisa Bentivogli,Arianna Bisazza,Marianna Bolognesi,Dominique Brunato,Tommaso Caselli,Silvia Casola,Maria Cassese,Mauro Cettolo,Claudia Collacciani,Leonardo De Cosmo,Maria Pia Di Buono,Andrea Esuli,Julen Etxaniz,Chiara Ferrando,Alessia Fidelangeli,Simona Frenda,Achille Fusco,Marco Gaido,Andrea Galassi,Federico Galli,Luca Giordano,Mattia Goffetti,Itziar Gonzalez-Dios,Lorenzo Gregori,Giulia Grundler,Sandro Iannaccone,Chunyang Jiang,Moreno La Quatra,Francesca Lagioia,Soda Marem Lo,Marco Madeddu,Bernardo Magnini,Raffaele Manna,Fabio Mercorio,Paola Merlo,Arianna Muti,Vivi Nastase,Matteo Negri,Dario Onorati,Elena Palmieri,Sara Papi,Lucia Passaro,Giulia Pensa,Andrea Piergentili,Daniele Potertì,Giovanni Puccetti,Federico Ranaldi,Leonardo Ranaldi,Andrea Amelio Ravelli,Martina Rosola,Elena Sofia Ruzzetti,Giuseppe Samo,Andrea Santilli,Piera Santin,Gabriele Sarti,Giovanni Sartor,Beatrice Savoldi,Antonio Serino,Andrea Seveso,Lucia Siciliani,Paolo Torroni,Rossella Varvara,Andrea Zaninello,Asya Zanollo,Fabio Massimo Zanzotto,Kamyar Zeinalipour,Andrea Zugarini*

Main category: cs.CL

TL;DR: CALAMITA 是一个大规模的合作基准测试倡议，专注于意大利语，旨在评估大型语言模型的各种能力，包括语言能力、常识推理、事实一致性、公平性、总结、翻译和代码生成。它通过集中评价管道支持多元化的任务和度量标准。


<details>
  <summary>Details</summary>
Motivation: CALAMITA 的动机在于填补大型语言模型系统评价的空白，尤其是针对非英语等语言，旨在提供一个全面且多样的基准测试平台。

Method: CALAMITA 通过召集来自学术界、工业界和公共部门的超过 80 名贡献者设计、记录和评估任务集。这些任务集涵盖了语言能力、常识推理、事实一致性、公平性、总结、翻译和代码生成。在此过程中，构建了一个支持多元数据集和度量标准的集中评价管道。

Result: 通过为四种开源重量的 LLM 提供结果，CALAMITA 报告了跨能力的系统优势和劣势，以及特定任务评估中的挑战。此外，它还揭示了方法论上的成果，例如细粒度的任务代表性有效度量、协调管道的重要性以及广泛社区参与的好处和限制。

Conclusion: CALAMITA 被设想为滚动基准，允许持续集成新的任务和模型。它不仅是一个资源——迄今为止最全面和多样的意大利语基准——还是一种可持续的、以社区为导向的评价框架。这项工作为其他语言和社区寻求包容和严格的 LLM 评价实践提供了蓝图。

Abstract: The rapid progress of Large Language Models (LLMs) has transformed natural language processing and broadened its impact across research and society. Yet, systematic evaluation of these models, especially for languages beyond English, remains limited. "Challenging the Abilities of LAnguage Models in ITAlian" (CALAMITA) is a large-scale collaborative benchmarking initiative for Italian, coordinated under the Italian Association for Computational Linguistics. Unlike existing efforts that focus on leaderboards, CALAMITA foregrounds methodology: it federates more than 80 contributors from academia, industry, and the public sector to design, document, and evaluate a diverse collection of tasks, covering linguistic competence, commonsense reasoning, factual consistency, fairness, summarization, translation, and code generation. Through this process, we not only assembled a benchmark of over 20 tasks and almost 100 subtasks, but also established a centralized evaluation pipeline that supports heterogeneous datasets and metrics. We report results for four open-weight LLMs, highlighting systematic strengths and weaknesses across abilities, as well as challenges in task-specific evaluation. Beyond quantitative results, CALAMITA exposes methodological lessons: the necessity of fine-grained, task-representative metrics, the importance of harmonized pipelines, and the benefits and limitations of broad community engagement. CALAMITA is conceived as a rolling benchmark, enabling continuous integration of new tasks and models. This makes it both a resource -- the most comprehensive and diverse benchmark for Italian to date -- and a framework for sustainable, community-driven evaluation. We argue that this combination offers a blueprint for other languages and communities seeking inclusive and rigorous LLM evaluation practices.

</details>


### [109] [AdiBhashaa: A Community-Curated Benchmark for Machine Translation into Indian Tribal Languages](https://arxiv.org/abs/2512.04765)
*Pooja Singh,Sandeep Kumar*

Main category: cs.CL

TL;DR: AdiBhashaa 是一个社区驱动的项目，旨在构建印度四大部落语言的开放平行语料库和基线机器翻译系统，旨在促进这些语言在大语言模型和多语言机器翻译系统中的可见性。


<details>
  <summary>Details</summary>
Motivation: 许多土著社区的语言在大语言模型和多语言机器翻译系统中被忽视，这加剧了教育资源、治理和数字参与方面的结构不平等。

Method: AdiBhashaa 项目通过参与式数据创建、本地说话者的人工审核和系统评估编码器-解码器机器翻译模型及大规模语言模型来构建这些部落语言资源。

Result: 该项目成功构建了四个主要印度部落语言（Bhili、Mundari、Gondi 和 Santali）的开放平行语料库，并开发了基于这些语料库的基线机器翻译系统。

Conclusion: AdiBhashaa 项目不仅展示了促进更公平的人工智能研究的可能模式，而且还强调了本地专业知识、提升边缘社区早期研究人员的能力以及在语言技术开发中优先考虑人类验证的重要性。

Abstract: Large language models and multilingual machine translation (MT) systems increasingly drive access to information, yet many languages of the tribal communities remain effectively invisible in these technologies. This invisibility exacerbates existing structural inequities in education, governance, and digital participation. We present AdiBhashaa, a community-driven initiative that constructs the first open parallel corpora and baseline MT systems for four major Indian tribal languages-Bhili, Mundari, Gondi, and Santali. This work combines participatory data creation with native speakers, human-in-the-loop validation, and systematic evaluation of both encoder-decoder MT models and large language models. In addition to reporting technical findings, we articulate how AdiBhashaa illustrates a possible model for more equitable AI research: it centers local expertise, builds capacity among early-career researchers from marginalized communities, and foregrounds human validation in the development of language technologies.

</details>


### [110] [DaLA: Danish Linguistic Acceptability Evaluation Guided by Real World Errors](https://arxiv.org/abs/2512.04799)
*Gianluca Barmina,Nathalie Carmen Hau Norman,Peter Schneider-Kamp,Lukas Galke*

Main category: cs.CL

TL;DR: 该研究提出了评价丹麦语语法正确性的增强基准，通过系统性地引入错误生成错误句子，并通过手动和自动评估确保这些错误的有效性。实验结果显示，该基准不仅比现有基准更全面，还具有更高的区分能力。


<details>
  <summary>Details</summary>
Motivation: 当前用于评价大型语言模型语法正确性的基准可能存在局限性，无法覆盖多种类型的语法错误。为了改进这一情况，研究团队开发了一个新的丹麦语语义接受度基准，并引入了十四种不同的错误生成方法。

Method: 研究团队首先分析了丹麦语中最常见的写作错误，然后基于这些错误开发了十四种系统性引入错误的方法。通过手动和自动评估这些错误的有效性，研究团队将这些规则用于生成错误语句，并将其用作大型语言模型准确评价丹麦语语义接受度的基准。

Result: 实验结果显示，新的丹麦语语义接受度基准比现有的基准更加全面和严谨，能够更好地区分性能较好的模型和性能较差的模型。此外，由于包含更多种类的错误类型，大型语言模型在这项任务上的表现不如用现有基准进行的评价。

Conclusion: 研究团队的新方法能够为评价大型语言模型的语法正确性提供一个更全面和严谨的基准。

Abstract: We present an enhanced benchmark for evaluating linguistic acceptability in Danish. We first analyze the most common errors found in written Danish. Based on this analysis, we introduce a set of fourteen corruption functions that generate incorrect sentences by systematically introducing errors into existing correct Danish sentences. To ensure the accuracy of these corruptions, we assess their validity using both manual and automatic methods. The results are then used as a benchmark for evaluating Large Language Models on a linguistic acceptability judgement task. Our findings demonstrate that this extension is both broader and more comprehensive than the current state of the art. By incorporating a greater variety of corruption types, our benchmark provides a more rigorous assessment of linguistic acceptability, increasing task difficulty, as evidenced by the lower performance of LLMs on our benchmark compared to existing ones. Our results also suggest that our benchmark has a higher discriminatory power which allows to better distinguish well-performing models from low-performing ones.

</details>


### [111] [DAMASHA: Detecting AI in Mixed Adversarial Texts via Segmentation with Human-interpretable Attribution](https://arxiv.org/abs/2512.04838)
*L. D. M. S. Sai Teja,N. Siva Gopala Krishna,Ufaq Khan,Muhammad Haris Khan,Partha Pakray,Atul Mishra*

Main category: cs.CL

TL;DR: 本文提出了一种名为Info-Mask的框架，用于检测混合作者文本，并通过一个对抗基准数据集揭示了系统的鲁棒性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 随着先进大型语言模型的发展，人类和AI生成文本的界限变得模糊。本文旨在解决混合作者文本的分割问题，以确保内容的可信度和可靠性。

Method: 该方法结合了语体学线索、困惑度驱动的信号以及结构化边界建模来准确分割人类和AI合作的内容。为了评估系统的鲁棒性，作者创建了一个对抗性基准数据集。

Result: Info-Mask在对抗条件下的跨度级鲁棒性方面表现出显著改善，并建立了新的基准。同时突出了需要解决的挑战。

Conclusion: 该研究突显了对抗性鲁棒性和可解释性混合作者检测的潜力和局限性，对未来的人工智能合作有重要意义。

Abstract: In the age of advanced large language models (LLMs), the boundaries between human and AI-generated text are becoming increasingly blurred. We address the challenge of segmenting mixed-authorship text, that is identifying transition points in text where authorship shifts from human to AI or vice-versa, a problem with critical implications for authenticity, trust, and human oversight. We introduce a novel framework, called Info-Mask for mixed authorship detection that integrates stylometric cues, perplexity-driven signals, and structured boundary modeling to accurately segment collaborative human-AI content. To evaluate the robustness of our system against adversarial perturbations, we construct and release an adversarial benchmark dataset Mixed-text Adversarial setting for Segmentation (MAS), designed to probe the limits of existing detectors. Beyond segmentation accuracy, we introduce Human-Interpretable Attribution (HIA overlays that highlight how stylometric features inform boundary predictions, and we conduct a small-scale human study assessing their usefulness. Across multiple architectures, Info-Mask significantly improves span-level robustness under adversarial conditions, establishing new baselines while revealing remaining challenges. Our findings highlight both the promise and limitations of adversarially robust, interpretable mixed-authorship detection, with implications for trust and oversight in human-AI co-authorship.

</details>


### [112] [Mitigating Catastrophic Forgetting in Target Language Adaptation of LLMs via Source-Shielded Updates](https://arxiv.org/abs/2512.04844)
*Atsuki Yamaguchi,Terufumi Morishita,Aline Villavicencio,Nikolaos Aletras*

Main category: cs.CL

TL;DR: 本文提出了一种称为Source-Shielded Updates (SSU)的方法，该方法通过选择性地更新参数来保护源语言知识，以适应仅使用未标记的目标语言数据的大规模语言模型，从而缓解灾难性遗忘问题，并提升目标语言性能。


<details>
  <summary>Details</summary>
Motivation: 当前，为了在多种语言环境中提高语言模型的性能，需要依赖昂贵的专门标记数据，而且在模型调整过程中还会发生灾难性遗忘现象。SSU方法旨在解决这些限制，在资源有限的情况下，利用少量源语言数据和参数重要性评分法，保护源模型的关键能力，以适应未标记的目标语言数据。

Method: SSU方法是一种选择性参数更新策略，通过识别并保护对维护源语言能力至关重要的参数来防止灾难性遗忘。它通过计算参数重要性，对参数进行冷冻保护，从而在适应目标语言数据时最大程度地保留源语言知识。

Result: 在五种类型学上不同的语言（包括7B和13B规模的模型）上进行的实验表明，SSU成功地缓解了灾难性遗忘现象，减少了单调源任务的性能下降，具体来说，对于7B模型，下降了3.4%，对于13B模型，下降了2.8%，与完全微调相比，明显更低。此外，SSU在目标语言的性能表现也非常出色，对于7B模型，超过所有基准测试，而对于13B模型，超过多数基准测试。

Conclusion: SSU方法为通过未标记的目标语言数据进行大语言模型的适应性调整提供了一种有效的策略，证明了在资源有限的条件下，能够同时保持源语言知识和提升目标语言性能的能力。

Abstract: Expanding the linguistic diversity of instruct large language models (LLMs) is crucial for global accessibility but is often hindered by the reliance on costly specialized target language labeled data and catastrophic forgetting during adaptation. We tackle this challenge under a realistic, low-resource constraint: adapting instruct LLMs using only unlabeled target language data. We introduce Source-Shielded Updates (SSU), a selective parameter update strategy that proactively preserves source knowledge. Using a small set of source data and a parameter importance scoring method, SSU identifies parameters critical to maintaining source abilities. It then applies a column-wise freezing strategy to protect these parameters before adaptation. Experiments across five typologically diverse languages and 7B and 13B models demonstrate that SSU successfully mitigates catastrophic forgetting. It reduces performance degradation on monolingual source tasks to just 3.4% (7B) and 2.8% (13B) on average, a stark contrast to the 20.3% and 22.3% from full fine-tuning. SSU also achieves target-language performance highly competitive with full fine-tuning, outperforming it on all benchmarks for 7B models and the majority for 13B models.

</details>


### [113] [LLMs Know More Than Words: A Genre Study with Syntax, Metaphor & Phonetics](https://arxiv.org/abs/2512.04957)
*Weiye Shi,Zhaowei Zhang,Shaoheng Yan,Yaodong Yang*

Main category: cs.CL

TL;DR: 本文引入了一个新型多语言体裁分类数据集，通过大型语言模型进行分类，评估其对不同语言学特征的掌握能力，发现不同特征在不同任务上贡献不同。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型能否有效学习并应用深层的语言学特性（如句法结构、语音提示和韵律模式），并通过多语言体裁分类数据集进行实证研究。

Method: 构建了一个多样化语言的体裁分类数据集，包含英语、法语、德语、意大利语、西班牙语和葡萄牙语，涵盖了诗歌和小说、戏剧等文体，同时引入了句法树结构、隐喻计数和语音度量等特征，通过大型语言模型进行分类实验。

Result: 实验结果表明，大型语言模型可以从纯文本或明确提供的特征中学习隐含的语言学结构，但不同特征对不同任务的贡献不同。

Conclusion: 这强调了在模型训练过程中应更重视复杂语言信号的重要性。

Abstract: Large language models (LLMs) demonstrate remarkable potential across diverse language related tasks, yet whether they capture deeper linguistic properties, such as syntactic structure, phonetic cues, and metrical patterns from raw text remains unclear. To analysis whether LLMs can learn these features effectively and apply them to important nature language related tasks, we introduce a novel multilingual genre classification dataset derived from Project Gutenberg, a large-scale digital library offering free access to thousands of public domain literary works, comprising thousands of sentences per binary task (poetry vs. novel;drama vs. poetry;drama vs. novel) in six languages (English, French, German, Italian, Spanish, and Portuguese). We augment each with three explicit linguistic feature sets (syntactic tree structures, metaphor counts, and phonetic metrics) to evaluate their impact on classification performance. Experiments demonstrate that although LLM classifiers can learn latent linguistic structures either from raw text or from explicitly provided features, different features contribute unevenly across tasks, which underscores the importance of incorporating more complex linguistic signals during model training.

</details>


### [114] [Factuality and Transparency Are All RAG Needs! Self-Explaining Contrastive Evidence Re-ranking](https://arxiv.org/abs/2512.05012)
*Francielle Vargas,Daniel Pedronette*

Main category: cs.CL

TL;DR: CER是一种通过对比学习微调嵌入并为每个检索段落生成标记级归因 rationale 的新方法，自动选择难样本以拉近事实 rationale，推离主观或误导性解释，从而改进检索准确性并提高可靠性。


<details>
  <summary>Details</summary>
Motivation: 在安全关键领域中，需要可靠的检索方法来避免RAG系统的幻觉问题，并提供透明、基于证据的检索。

Method: CER通过对比学习增强嵌入，并生成每个检索段落的标记级归因 rationale。难样本通过基于主观性的标准自动选择。

Result: 在临床试验报告上的实验结果显示，CER提升了检索准确性，减少了幻觉风险，并提供了更可靠、基于证据的检索。

Conclusion: CER能够有效改进检索，提高安全关键领域中信息检索的可靠性。

Abstract: This extended abstract introduces Self-Explaining Contrastive Evidence Re-Ranking (CER), a novel method that restructures retrieval around factual evidence by fine-tuning embeddings with contrastive learning and generating token-level attribution rationales for each retrieved passage. Hard negatives are automatically selected using a subjectivity-based criterion, forcing the model to pull factual rationales closer while pushing subjective or misleading explanations apart. As a result, the method creates an embedding space explicitly aligned with evidential reasoning. We evaluated our method on clinical trial reports, and initial experimental results show that CER improves retrieval accuracy, mitigates the potential for hallucinations in RAG systems, and provides transparent, evidence-based retrieval that enhances reliability, especially in safety-critical domains.

</details>


### [115] [Arbitrage: Efficient Reasoning via Advantage-Aware Speculation](https://arxiv.org/abs/2512.05033)
*Monishwaran Maheswaran,Rishabh Tiwari,Yuezhou Hu,Kerem Dilmen,Coleman Hooper,Haocheng Xi,Nicholas Lee,Mehrdad Farajtabar,Michael W. Mahoney,Kurt Keutzer,Amir Gholami*

Main category: cs.CL

TL;DR: Arbitrage是一个新颖的基于步骤的投机生成框架，通过动态路由预测和选择目标模型更有优势的推理步骤，优化了推理延迟和准确性之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 现代大型语言模型在推理任务中表现出色，但推理过程中的计算成本很高。投机解码通过快速但不准确的草案模型提出候选步骤，并由更高能力的目标模型进行验证，但传统方法存在由于词汇不匹配导致的错误拒绝，影响了推理效率。

Method: Arbitrage框架采用了一个轻量级的路由器，这个路由器被训练来预测目标模型在哪个时刻更可能生成意义更丰富的步骤。它根据draft和target模型之间的相对优势动态路由生成过程。

Result: 在多个数学推理基准测试中，Arbitrage均优于之前的基于步骤的投机解码基线，将推理延迟降低了约2倍，同时保持了相同的准确率。

Conclusion: Arbitrage通过优化解决传统投机解码机制中的浪费问题，从而提高了推理过程的效率和准确性。

Abstract: Modern Large Language Models achieve impressive reasoning capabilities with long Chain of Thoughts, but they incur substantial computational cost during inference, and this motivates techniques to improve the performance-cost ratio. Among these techniques, Speculative Decoding accelerates inference by employing a fast but inaccurate draft model to autoregressively propose tokens, which are then verified in parallel by a more capable target model. However, due to unnecessary rejections caused by token mismatches in semantically equivalent steps, traditional token-level Speculative Decoding struggles in reasoning tasks. Although recent works have shifted to step-level semantic verification, which improve efficiency by accepting or rejecting entire reasoning steps, existing step-level methods still regenerate many rejected steps with little improvement, wasting valuable target compute. To address this challenge, we propose Arbitrage, a novel step-level speculative generation framework that routes generation dynamically based on the relative advantage between draft and target models. Instead of applying a fixed acceptance threshold, Arbitrage uses a lightweight router trained to predict when the target model is likely to produce a meaningfully better step. This routing approximates an ideal Arbitrage Oracle that always chooses the higher-quality step, achieving near-optimal efficiency-accuracy trade-offs. Across multiple mathematical reasoning benchmarks, Arbitrage consistently surpasses prior step-level Speculative Decoding baselines, reducing inference latency by up to $\sim2\times$ at matched accuracy.

</details>


### [116] [Structured Document Translation via Format Reinforcement Learning](https://arxiv.org/abs/2512.05100)
*Haiyue Song,Johannes Eschbach-Dymanus,Hour Kaing,Sumire Honda,Hideki Tanaka,Bianka Buschbeck,Masao Utiyama*

Main category: cs.CL

TL;DR: 研究提出了FormatRL，一种通过组相对策略优化方法优化结构感知奖励的框架，以改进文本结构和翻译质量。


<details>
  <summary>Details</summary>
Motivation: 当前的结构文本翻译工作主要集中在句子层面，难以处理复杂的XML或HTML文档结构。FormatRL旨在解决这一问题。

Method: FormatRL采用了一种组相对策略优化的方法，结合监督微调模型，直接优化树相似度(TreeSim)和节点-chrF等结构感知奖励，同时引入了StrucAUC进行细粒度评估。

Result: 在SAP软件文档基准测试中，FormatRL在六个评估指标上均有所改进，进一步的分析显示不同奖励函数对结构质量和翻译质量的提升作用。

Conclusion: 实验结果表明，FormatRL框架能够有效地提高结构文本翻译的质量，并能更清晰地识别细微错误和严重结构错误。

Abstract: Recent works on structured text translation remain limited to the sentence level, as they struggle to effectively handle the complex document-level XML or HTML structures. To address this, we propose \textbf{Format Reinforcement Learning (FormatRL)}, which employs Group Relative Policy Optimization on top of a supervised fine-tuning model to directly optimize novel structure-aware rewards: 1) TreeSim, which measures structural similarity between predicted and reference XML trees and 2) Node-chrF, which measures translation quality at the level of XML nodes. Additionally, we apply StrucAUC, a fine-grained metric distinguishing between minor errors and major structural failures. Experiments on the SAP software-documentation benchmark demonstrate improvements across six metrics and an analysis further shows how different reward functions contribute to improvements in both structural and translation quality.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [117] [FLEX: Leveraging FPGA-CPU Synergy for Mixed-Cell-Height Legalization Acceleration](https://arxiv.org/abs/2512.04527)
*Xingyu Liu,Jiawei Liang,Linfeng Du,Yipu Zhang,Chaofang Ma,Hanwei Fan,Jiang Xu,Wei Zhang*

Main category: cs.AR

TL;DR: FLEX是一种基于FPGA-CPU的加速器，主要用于混合单元高度合法化任务，通过优化任务分配、采用多粒度流水线技术以及优化关键的单元位移过程，实现了高达18.3倍和5.4倍的速度提升，且保持或提高了合法化质量。


<details>
  <summary>Details</summary>
Motivation: 随着大规模集成电路（IC）设计变得越来越复杂，合法化任务成为了制约设计流程的关键瓶颈。现有基于CPU或GPU的方法难以满足高性能需求，因此开发一种能够充分发挥FPGA和CPU优势的加速器具有重要价值。

Method: FLEX加速器通过以下方式实现：1) 优化任务分配策略，实现FPGA与CPU之间的高效任务划分；2) 引入多粒度流水线技术加速最耗时的部分——找到最佳放置位置；3) 专门针对FOP阶段中的单元位移过程进行设计优化，以与多粒度流水线框架无缝集成，进一步提高性能。

Result: FLEX在实验中展示了显著的速度和质量优势：相比最先进的CPU-GPU和多线程CPU合法器，速度分别提升了18.3倍和5.4倍；同时，在保持或略微提升合法化质量的同时达到了这一点。

Conclusion: FLEX通过结合FPGA与CPU的优势，提供了一种高效且可扩展的解决方案，成功解决了大规模集成电路设计中的关键合法化问题。

Abstract: In this work, we present FLEX, an FPGA-CPU accelerator for mixed-cell-height legalization tasks. We address challenges from the following perspectives. First, we optimize the task assignment strategy and perform an efficient task partition between FPGA and CPU to exploit their complementary strengths. Second, a multi-granularity pipelining technique is employed to accelerate the most time-consuming step, finding optimal placement position (FOP), in legalization. At last, we particularly target the computationally intensive cell shifting process in FOP, optimizing the design to align it seamlessly with the multi-granularity pipelining framework for further speedup. Experimental results show that FLEX achieves up to 18.3x and 5.4x speedups compared to state-of-the-art CPU-GPU and multi-threaded CPU legalizers with better scalability, while improving legalization quality by 4% and 1%.

</details>


### [118] [Declarative Synthesis and Multi-Objective Optimization of Stripboard Circuit Layouts Using Answer Set Programming](https://arxiv.org/abs/2512.04910)
*Fang Li*

Main category: cs.AR

TL;DR: 本研究提出了一种使用布尔约束编程（ASP）自动设计条形板电路布局的新方法，该方法能有效缩减电路板面积，同时确保电路布局的可行性。


<details>
  <summary>Details</summary>
Motivation: 随着电路复杂性的增加，手动设计条形板布局变得困难而耗时，因此需要一种自动化设计方法来提高效率并满足复杂电路的需求。

Method: 该方法将电路布局问题表述为合成问题和多目标优化任务的组合。通过ASP的声明性特性，该方法能以自然且简洁的方式表达复杂的几何和电气约束条件，并采用两阶段求解方法来确保布局的可行性和优化其质量。

Result: 实验结果表明，该方法能为不同复杂度的电路生成紧凑且可制造的布局。

Conclusion: 本研究代表了条形板布局自动化的重要进展，提供了电子原型制作与教育的实用工具，同时展示了声明性编程在解决复杂设计自动化问题时的优势。

Abstract: This paper presents a novel approach to automated stripboard circuit layout design using Answer Set Programming (ASP). The work formulates the layout problem as both a synthesis and multi-objective optimization task that simultaneously generates viable layouts while minimizing board area and component strip crossing. By leveraging ASP's declarative nature, this work expresses complex geometric and electrical constraints in a natural and concise manner. The two-phase solving methodology first ensures feasibility before optimizing layout quality. Experimental results demonstrate that this approach generates compact, manufacturable layouts for a range of circuit complexities. This work represents a significant advancement in automated stripboard layout, offering a practical tool for electronics prototyping and education while showcasing the power of declarative programming for solving complex design automation problems.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [119] [Solving N-Queen Problem using Las Vegas Algorithm with State Pruning](https://arxiv.org/abs/2512.04139)
*Susmita Sharma,Aayush Shrestha,Sitasma Thapa,Prashant Timalsina,Prakash Poudyal*

Main category: cs.AI

TL;DR: 本文提出了一种改进的拉斯维加斯算法，通过迭代剪枝技术在放置皇后时动态排除无效布局，从而减少搜索空间。这种方法在处理大规模皇后问题时能更快地生成有效的解决方案，特别适合资源受限的计算环境。


<details>
  <summary>Details</summary>
Motivation: 经典的N后问题是一个适合使用约束满足算法解决的经典问题。虽然完整的回溯算法能够确保找到解决方案，但由于其指数级的时间复杂度，它们不适合大规模实例。因此，我们引入了一个基于标准拉斯维加斯框架的混合算法，通过动态排除无效放置来减小搜索空间，从而提供更快的近似解决方案。

Method: 研究基于标准的拉斯维加斯算法，通过迭代剪枝技术在随机分配阶段动态排除无效放置，从而减少搜索空间，并且通过这种方法解决N后问题。

Result: 分析结果表明，传统的回溯算法随着N的增加扩展性较差；相比之下，提出的算法能够在较短时间内生成有效解决方案，且具有较低的计算成本和较高的解决方案保真度。

Conclusion: 尽管在处理大规模N后问题时会受到一些性能波动的影响，但这种算法证明了计算成本和解决方案保真度的有效权衡，特别适用于资源受限的计算环境。

Abstract: The N-Queens problem, placing all N queens in a N x N chessboard where none attack the other, is a classic problem for constraint satisfaction algorithms. While complete methods like backtracking guarantee a solution, their exponential time complexity makes them impractical for large-scale instances thus, stochastic approaches, such as Las Vegas algorithm, are preferred. While it offers faster approximate solutions, it suffers from significant performance variance due to random placement of queens on the board. This research introduces a hybrid algorithm built on top of the standard Las Vegas framework through iterative pruning, dynamically eliminating invalid placements during the random assignment phase, thus this method effectively reduces the search space. The analysis results that traditional backtracking scales poorly with increasing N. In contrast, the proposed technique consistently generates valid solutions more rapidly, establishing it as a superior alternative to use where a single, timely solution is preferred over completeness. Although large N causes some performance variability, the algorithm demonstrates a highly effective trade-off between computational cost and solution fidelity, making it particularly suited for resource-constrained computing environments.

</details>


### [120] [RippleBench: Capturing Ripple Effects Using Existing Knowledge Repositories](https://arxiv.org/abs/2512.04144)
*Roy Rinberg,Usha Bhalla,Igor Shilov,Flavio P. Calmon,Rohit Gandikota*

Main category: cs.AI

TL;DR: 研究人员开发了一个自动工具RippleBench-Maker，用于生成问答数据集以衡量模型编辑任务中的涟漪效应，并构建了RippleBench-Bio基准，评估了多种去学习方法的性能。


<details>
  <summary>Details</summary>
Motivation: 目前的模型干预方法如去学习、去偏见或模型编辑无法有效避免副作用，因此作者开发了RippleBench-Maker来更准确地测量这些方法的涟漪效应。

Method: 通过基于Wikipedia的RAG管道（WikiRAG）生成关于去学习知识的多选题，涵盖不同语义距离的概念，评估模型在远离去学习知识的知识领域中的表现。

Result: 评估了8种最先进的去学习方法，所有方法在距离去学习知识越来越远的主题上表现出明显的准确率下降，且各自具有不同的传播模式。

Conclusion: RippleBench-Bmaker工具及其在RippleBench-Bio上的应用为持续研究提供了支持，展示了衡量模型编辑任务中不同干预方法的具体传播特征的能力。

Abstract: Targeted interventions on language models, such as unlearning, debiasing, or model editing, are a central method for refining model behavior and keeping knowledge up to date. While these interventions aim to modify specific information within models (e.g., removing virology content), their effects often propagate to related but unintended areas (e.g., allergies); these side-effects are commonly referred to as the ripple effect. In this work, we present RippleBench-Maker, an automatic tool for generating Q&A datasets that allow for the measurement of ripple effects in any model-editing task. RippleBench-Maker builds on a Wikipedia-based RAG pipeline (WikiRAG) to generate multiple-choice questions at varying semantic distances from the target concept (e.g., the knowledge being unlearned). Using this framework, we construct RippleBench-Bio, a benchmark derived from the WMDP (Weapons of Mass Destruction Paper) dataset, a common unlearning benchmark. We evaluate eight state-of-the-art unlearning methods and find that all exhibit non-trivial accuracy drops on topics increasingly distant from the unlearned knowledge, each with distinct propagation profiles. To support ongoing research, we release our codebase for on-the-fly ripple evaluation, along with the benchmark, RippleBench-Bio.

</details>


### [121] [Balancing Safety and Helpfulness in Healthcare AI Assistants through Iterative Preference Alignment](https://arxiv.org/abs/2512.04210)
*Huy Nghiem,Swetasudha Panda,Devashish Khatwani,Huy V. Nguyen,Krishnaram Kenthapadi,Hal Daumé*

Main category: cs.AI

TL;DR: 本文提出了一种迭代部署对齐框架，通过KTO和DPO优化方法，在多个循环中评估四个大型语言模型在医疗助手中的部署表现，显著提高了有害查询检测的安全性指标。


<details>
  <summary>Details</summary>
Motivation: 当前使用大型语言模型（LLMs）在医疗保健领域的应用虽然日益增长，但确保它们的安全性和可靠性仍然是一大障碍。本研究旨在通过迭代部署对齐框架，结合KTO和DPO方法，改进模型以满足特定领域安全信号，以增强医疗助手的安全性。

Method: 研究采用了Kahneman-Tversky优化（KTO）和直接偏好优化（DPO）方法对四个大型语言模型（Llama-3B/8B、Meditron-8B、Mistral-7B）进行优化，并使用CARES-18K对抗鲁棒性基准进行评估，评估过程中经历了多个循环。

Result: 研究结果表明，在有害查询检测的安全性相关指标上最高提升了42%，并且在错误拒绝方面找到了一些有趣的权衡。此外，还通过消融研究确定了在自评价可靠性不足时，需要外部或微调过的评估者来最大化性能增益。

Conclusion: 研究结果强调了在设计对话式医疗助手时采用最佳实践的重要性，以平衡患者安全、用户信任和临床用途。

Abstract: Large Language Models (LLMs) are increasingly used in healthcare, yet ensuring their safety and trustworthiness remains a barrier to deployment. Conversational medical assistants must avoid unsafe compliance without over-refusing benign queries. We present an iterative post-deployment alignment framework that applies Kahneman-Tversky Optimization (KTO) and Direct Preference Optimization (DPO) to refine models against domain-specific safety signals. Using the CARES-18K benchmark for adversarial robustness, we evaluate four LLMs (Llama-3B/8B, Meditron-8B, Mistral-7B) across multiple cycles. Our results show up to 42% improvement in safety-related metrics for harmful query detection, alongside interesting trade-offs against erroneous refusals, thereby exposing architecture-dependent calibration biases. We also perform ablation studies to identify when self-evaluation is reliable and when external or finetuned judges are necessary to maximize performance gains. Our findings underscore the importance of adopting best practices that balance patient safety, user trust, and clinical utility in the design of conversational medical assistants.

</details>


### [122] [Educational Cone Model in Embedding Vector Spaces](https://arxiv.org/abs/2512.04227)
*Yo Ehara*

Main category: cs.AI

TL;DR: 该研究提出了一种基于几何框架的教育圆锥模型，通过假设易于理解的文本较少多样性和难以理解的文本更多样性，模型能够有效识别符合难度标注的教育文本的嵌入空间。


<details>
  <summary>Details</summary>
Motivation: 在智能教育系统中，具有明确难度评级的人标注数据集至关重要。尽管嵌入向量空间广泛用于表示语义接近并在分析文本难度方面颇具前景，但嵌入方法的丰富性也带来了选择最合适的嵌入方法的挑战。

Method: 该研究表明，基于几何框架的教育圆锥模型能够通过设计特定的损失函数来避免昂贵的计算，从而高效地识别符合难度标注的教育文本的嵌入空间。

Result: 实验测试表明，该模型在识别与难度标注的教育文本最优对齐的嵌入空间方面既有效又快速。

Conclusion: 教育圆锥模型为通过几何框架识别教育文本的难度提供了新的视角，特别是在高速计算方面表现出色。

Abstract: Human-annotated datasets with explicit difficulty ratings are essential in intelligent educational systems. Although embedding vector spaces are widely used to represent semantic closeness and are promising for analyzing text difficulty, the abundance of embedding methods creates a challenge in selecting the most suitable method. This study proposes the Educational Cone Model, which is a geometric framework based on the assumption that easier texts are less diverse (focusing on fundamental concepts), whereas harder texts are more diverse. This assumption leads to a cone-shaped distribution in the embedding space regardless of the embedding method used. The model frames the evaluation of embeddings as an optimization problem with the aim of detecting structured difficulty-based patterns. By designing specific loss functions, efficient closed-form solutions are derived that avoid costly computation. Empirical tests on real-world datasets validated the model's effectiveness and speed in identifying the embedding spaces that are best aligned with difficulty-annotated educational texts.

</details>


### [123] [Addressing Logical Fallacies In Scientific Reasoning From Large Language Models: Towards a Dual-Inference Training Framework](https://arxiv.org/abs/2512.04228)
*Peter B. Walker,Hannah Davidson,Aiden Foster,Matthew Lienert,Thomas Pardue,Dale Russell*

Main category: cs.AI

TL;DR: 该论文展示了现有大型语言模型在科学领域推理中对否定、反例或错误前提的系统性弱点，并提出了一种结合肯定生成与结构化反事实否认的双重推理训练框架，增强模型的鲁棒性、可解释性和与人类推理的一致性。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型在科学领域推理中存在逻辑谬误、对抗性操纵和因果推理失败等问题，因此需要一种新的训练框架来增强模型的逻辑能力。

Method: 该研究通过对比现有模型在科学领域的推理表现，结合形式逻辑、认知科学和对抗训练，提出了一种新的双重推理训练框架。

Result: 新框架能够使模型在肯定正确推论的同时，也能够拒绝错误的推论，从而提高系统的鲁棒性、可解释性和与人类推理的合理性。

Conclusion: 该研究提出的方法能够有效提升大型语言模型在科学推理中的表现，为对抗逻辑谬误和增强解释能力提供了新的途径。

Abstract: Large Language Models (LLMs) have transformed natural language processing and hold growing promise for advancing science, healthcare, and decision-making. Yet their training paradigms remain dominated by affirmation-based inference, akin to \textit{modus ponens}, where accepted premises yield predicted consequents. While effective for generative fluency, this one-directional approach leaves models vulnerable to logical fallacies, adversarial manipulation, and failures in causal reasoning. This paper makes two contributions. First, it demonstrates how existing LLMs from major platforms exhibit systematic weaknesses when reasoning in scientific domains with negation, counterexamples, or faulty premises \footnote{Code to recreate these experiments are at https://github.com/hannahdavidsoncollege-maker/ScientificReasoningForEnvironment-MedicineWithLLMs. Second, it introduces a dual-reasoning training framework that integrates affirmative generation with structured counterfactual denial. Grounded in formal logic, cognitive science, and adversarial training, this training paradigm formalizes a computational analogue of ``denying the antecedent'' as a mechanism for disconfirmation and robustness. By coupling generative synthesis with explicit negation-aware objectives, the framework enables models that not only affirm valid inferences but also reject invalid ones, yielding systems that are more resilient, interpretable, and aligned with human reasoning.

</details>


### [124] [Toward Virtuous Reinforcement Learning](https://arxiv.org/abs/2512.04246)
*Majid Ghasemi,Mark Crowley*

Main category: cs.AI

TL;DR: 本文批评了强化学习中常见的机器伦理模式，倡导一种基于美德的方法。该方法旨在克服规则基础（义务论）方法在模糊性和非稳定情况下表现不佳的问题，以及基于单一计算奖励值的实际隐患。


<details>
  <summary>Details</summary>
Motivation: 论文旨在改进在强化学习领域中对伦理问题的处理方法，提出了一种以美德为主要导向的政策层面的习惯，试图解决现有的道德规范和实践中的问题。

Method: 提出了四个方面的组合策略：1. 多智能体强化学习中的社会学习，以从不完美但仍具有规范信息的榜样中获得类似美德的行为模式；2. 多目标和受限形式，保留价值冲突并采用风险意识标准来防止伤害；3. 基于亲和力的正则化，有助于在分布变化时保持特质稳定性，同时允许规范发展；4. 将多元道德传统视为实际控制信号，明确伦理强化学习基准中的价值假设。

Result: 通过这些策略，作者期望能够提高伦理强化学习的稳定性和灵活性，更好地处理道德权衡和变化的环境。

Conclusion: 论文倡导一种新的研究方向，即通过美德导向的行为来指导强化学习中的伦理行为，这种方法强调了政策层面的习惯和持久性的培养，而非单纯依赖规则或单一的计算奖励值。

Abstract: This paper critiques common patterns in machine ethics for Reinforcement Learning (RL) and argues for a virtue focused alternative. We highlight two recurring limitations in much of the current literature: (i) rule based (deontological) methods that encode duties as constraints or shields often struggle under ambiguity and nonstationarity and do not cultivate lasting habits, and (ii) many reward based approaches, especially single objective RL, implicitly compress diverse moral considerations into a single scalar signal, which can obscure trade offs and invite proxy gaming in practice. We instead treat ethics as policy level dispositions, that is, relatively stable habits that hold up when incentives, partners, or contexts change. This shifts evaluation beyond rule checks or scalar returns toward trait summaries, durability under interventions, and explicit reporting of moral trade offs. Our roadmap combines four components: (1) social learning in multi agent RL to acquire virtue like patterns from imperfect but normatively informed exemplars; (2) multi objective and constrained formulations that preserve value conflicts and incorporate risk aware criteria to guard against harm; (3) affinity based regularization toward updateable virtue priors that support trait like stability under distribution shift while allowing norms to evolve; and (4) operationalizing diverse ethical traditions as practical control signals, making explicit the value and cultural assumptions that shape ethical RL benchmarks.

</details>


### [125] [Artificial Intelligence Applications in Horizon Scanning for Infectious Diseases](https://arxiv.org/abs/2512.04287)
*Ian Miles,Mayumi Wakimoto,Wagner Meira,Daniela Paula,Daylene Ticiane,Bruno Rosa,Jane Biddulph,Stelios Georgiou,Valdir Ermida*

Main category: cs.AI

TL;DR: 本文回顾了人工智能如何融入地平线扫描技术，重点在于识别与感染疾病相关的新兴威胁和机遇。研究探讨了AI工具在信号检测、数据监控、情景分析和决策支持中的应用，并提出了有效的实施和治理策略。研究结果为公共卫生准备中的AI潜力和限制提供了洞见。


<details>
  <summary>Details</summary>
Motivation: 随着感染性疾病相关的新威胁和机遇频繁出现，如何有效利用人工智能技术进行前瞻性分析和决策支持成为了研究焦点。本文旨在探讨AI在地平线扫描中的应用及其可能带来的挑战，为公共卫生准备提供指导。

Method: 该研究通过系统回顾和分析相关文献，探讨了AI在地平线扫描中的具体应用场景及可能的风险。研究采用了案例分析和专家访谈等方法。

Result: 结果显示，AI技术可以显著提升信号检测、数据监测、情景分析和决策支持的能力。然而，AI的普及和治理也面临一系列挑战，如数据隐私问题、算法偏见等。研究为有效利用AI技术提供了策略建议。

Conclusion: 本文强调了AI在感染性疾病地平线扫描中的潜力，但也指出了其面临的挑战。提出了实施AI技术的有效策略，以期为公共卫生准备提供更准确和支持。”》这段摘要说明了一项关于人工智能应用于感染疾病预测和管理研究的综述性论文的主要发现和建议。

Abstract: This review explores the integration of Artificial Intelligence into Horizon Scanning, focusing on identifying and responding to emerging threats and opportunities linked to Infectious Diseases. We examine how AI tools can enhance signal detection, data monitoring, scenario analysis, and decision support. We also address the risks associated with AI adoption and propose strategies for effective implementation and governance. The findings contribute to the growing body of Foresight literature by demonstrating the potential and limitations of AI in Public Health preparedness.

</details>


### [126] [Towards better dense rewards in Reinforcement Learning Applications](https://arxiv.org/abs/2512.04302)
*Shuyuan Zhang*

Main category: cs.AI

TL;DR: 该摘要探讨了在强化学习中寻找有意义且准确的密集奖励的重要性，特别是在稀疏、延迟或与目标任务不完全对齐的奖励信号中。密集奖励能够通过反馈帮助智能体的行为塑造和加速学习，但设计不当的奖励也可能导致意外行为、奖励作弊或低效探索。此提案旨在解决这些未解决问题，并提高密集奖励在不同RL应用中的有效性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统的强化学习设置中，智能体通过与环境的交互和奖励信号来学习最优策略。然而，当奖励信号稀疏、延迟或与目标任务不准确时，智能体的学习效率会降低。因此，需要一种能够提供即时反馈、指导智能体行为的密集奖励机制。

Method: 研究提出了几种解决密集奖励构建问题的方法，包括逆强化学习、从人类偏好构建奖励模型以及自监督学习来生成内在奖励。

Result: 尽管这些方法展示了潜力，但在通用性、可扩展性和与人类意图的对齐方面仍有权衡取舍。因此，此提案还探索了解决未解决问题的方法，并旨在提高不同强化学习应用中密集奖励的有效性和可靠性。

Conclusion: 该研究通过多项方法探讨了在复杂或高维环境中如何有效构建密集奖励，为未来的强化学习应用提供了指导。

Abstract: Finding meaningful and accurate dense rewards is a fundamental task in the field of reinforcement learning (RL) that enables agents to explore environments more efficiently. In traditional RL settings, agents learn optimal policies through interactions with an environment guided by reward signals. However, when these signals are sparse, delayed, or poorly aligned with the intended task objectives, agents often struggle to learn effectively. Dense reward functions, which provide informative feedback at every step or state transition, offer a potential solution by shaping agent behavior and accelerating learning. Despite their benefits, poorly crafted reward functions can lead to unintended behaviors, reward hacking, or inefficient exploration. This problem is particularly acute in complex or high-dimensional environments where handcrafted rewards are difficult to specify and validate. To address this, recent research has explored a variety of approaches, including inverse reinforcement learning, reward modeling from human preferences, and self-supervised learning of intrinsic rewards. While these methods offer promising directions, they often involve trade-offs between generality, scalability, and alignment with human intent. This proposal explores several approaches to dealing with these unsolved problems and enhancing the effectiveness and reliability of dense reward construction in different RL applications.

</details>


### [127] [Efficient Reinforcement Learning with Semantic and Token Entropy for LLM Reasoning](https://arxiv.org/abs/2512.04359)
*Hongye Cao,Zhixin Bai,Ziyue Peng,Boyan Wang,Tianpei Yang,Jing Huo,Yuyao Zhang,Yang Gao*

Main category: cs.AI

TL;DR: 该研究提出了一种有效的方法，通过在语义和标记级别利用熵信号来提高大型语言模型的推理能力，从而缓解了强化学习中的熵崩溃问题。


<details>
  <summary>Details</summary>
Motivation: 传统的强化学习方法在增强大型语言模型的推理能力时，常常遇到熵崩溃的问题，这会降低策略探索并限制推理能力。为了应对这一挑战，研究提出了一个利用语义和标记级别熵信号的高效强化学习框架。

Method: 该方法从数据和算法两个角度出发，引入了语义熵指导的阶梯式学习，并通过对低熵标记进行KL正则化和对高方差部分施加更强约束的非均匀标记处理方式，共同优化数据组织和算法设计。

Result: 研究通过跨6个基准测试了3种不同参数规模的基础模型，证明了该方法在提高推理能力方面优于其他基于熵的方法。

Conclusion: 该研究提出的高效强化学习框架能够有效缓解熵崩溃问题，提升大型语言模型的推理能力。

Abstract: Reinforcement learning with verifiable rewards (RLVR) has demonstrated superior performance in enhancing the reasoning capability of large language models (LLMs). However, this accuracy-oriented learning paradigm often suffers from entropy collapse, which reduces policy exploration and limits reasoning capabilities. To address this challenge, we propose an efficient reinforcement learning framework that leverages entropy signals at both the semantic and token levels to improve reasoning. From the data perspective, we introduce semantic entropy-guided curriculum learning, organizing training data from low to high semantic entropy to guide progressive optimization from easier to more challenging tasks. For the algorithmic design, we adopt non-uniform token treatment by imposing KL regularization on low-entropy tokens that critically impact policy exploration and applying stronger constraints on high-covariance portions within these tokens. By jointly optimizing data organization and algorithmic design, our method effectively mitigates entropy collapse and enhances LLM reasoning. Experimental results across 6 benchmarks with 3 different parameter-scale base models demonstrate that our method outperforms other entropy-based approaches in improving reasoning.

</details>


### [128] [GovBench: Benchmarking LLM Agents for Real-World Data Governance Workflows](https://arxiv.org/abs/2512.04416)
*Zhou Liu,Zhaoyang Han,Guochen Yan,Hao Liang,Bohan Zeng,Xing Chen,Yuanfeng Song,Wentao Zhang*

Main category: cs.AI

TL;DR: GovBench 是一个基于真实场景的数据治理基准，包含150个任务，用于评估大型语言模型在数据治理中的表现。DataGovAgent 是一个框架，利用规划器-执行器-评估器架构，结合约束规划、检索增强生成和沙箱反馈驱动调试，显著提高了复杂任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的自动化数据科学基准未能充分涵盖数据治理的实际挑战，即确保数据本身的准确性和质量。因此，需要一个能够更准确反映现实数据治理需求的基准和技术。

Method: GovBench 采用了一种新颖的“反向目标”方法来合成现实噪音，并使用严格指标评估端到端流水线的可靠性。DataGovAgent 利用了一个由规划器-执行器-评估器组成的框架，集成了基于约束的规划、检索增强的生成和沙箱反馈驱动的调试。

Result: DataGovAgent 在复杂任务上的平均任务得分（ATS）显著提高，从39.7提高到54.9，调试迭代次数减少了77.9%以上，相对于通用基准表现更好。

Conclusion: GovBench 的引入和 DataGovAgent 的提出为数据治理领域提供了一个新的视角和有效的解决方案。

Abstract: Data governance ensures data quality, security, and compliance through policies and standards, a critical foundation for scaling modern AI development. Recently, large language models (LLMs) have emerged as a promising solution for automating data governance by translating user intent into executable transformation code. However, existing benchmarks for automated data science often emphasize snippet-level coding or high-level analytics, failing to capture the unique challenge of data governance: ensuring the correctness and quality of the data itself. To bridge this gap, we introduce GovBench, a benchmark featuring 150 diverse tasks grounded in real-world scenarios, built on data from actual cases. GovBench employs a novel "reversed-objective" methodology to synthesize realistic noise and utilizes rigorous metrics to assess end-to-end pipeline reliability. Our analysis on GovBench reveals that current models struggle with complex, multi-step workflows and lack robust error-correction mechanisms. Consequently, we propose DataGovAgent, a framework utilizing a Planner-Executor-Evaluator architecture that integrates constraint-based planning, retrieval-augmented generation, and sandboxed feedback-driven debugging. Experimental results show that DataGovAgent significantly boosts the Average Task Score (ATS) on complex tasks from 39.7 to 54.9 and reduces debugging iterations by over 77.9 percent compared to general-purpose baselines.

</details>


### [129] [Solving LLM Repetition Problem in Production: A Comprehensive Study of Multiple Solutions](https://arxiv.org/abs/2512.04419)
*Weiwei Wang,Weijie Zou,Jiyong Min*

Main category: cs.AI

TL;DR: 本文研究了大语言模型在实际代码解释任务中遇到的重复问题，提出了三种重复模式，并通过马尔可夫模型分析其根本原因，提出三种解决方案并进行了实验验证。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在实际部署中会遇到持续生成重复内容的问题，这严重影响了系统的性能和稳定性。作者通过这项研究旨在解决这一挑战，提高模型在生产环境中的表现。

Method: 作者首先通过理论分析识别了三种重复模式，并通过马尔可夫模型揭示了根本原因。然后，提出了三种解决方案：基于Beam Search的early_stopping机制，presence_penalty超参数，以及Direct Preference Optimization（DPO）微调。

Result: 实验结果显示，Beam Search的early_stopping机制能有效解决三种重复模式，presence_penalty参数适用于特定情况，而DPO微调则提供了统一的模型级解决方案。这些方法在实际部署环境中得到了验证。

Conclusion: 这项工作结合了生产经验与实验验证，提出了系统化的重复机制分析，针对实际问题提供了多样的解决方案，并强调了early_stopping参数的重要性。

Abstract: The repetition problem, where Large Language Models (LLMs) continuously generate repetitive content without proper termination, poses a critical challenge in production deployments, causing severe performance degradation and system stalling. This paper presents a comprehensive investigation and multiple practical solutions for the repetition problem encountered in real-world batch code interpretation tasks.
  We identify three distinct repetition patterns: (1) business rule generation repetition, (2) method call relationship analysis repetition, and (3) PlantUML diagram syntax generation repetition. Through rigorous theoretical analysis based on Markov models, we establish that the root cause lies in greedy decoding's inability to escape repetitive loops, exacerbated by self-reinforcement effects.
  Our comprehensive experimental evaluation demonstrates three viable solutions: (1) Beam Search decoding with early_stopping=True serves as a universal post-hoc mechanism that effectively resolves all three repetition patterns; (2) presence_penalty hyperparameter provides an effective solution specifically for BadCase 1; and (3) Direct Preference Optimization (DPO) fine-tuning offers a universal model-level solution for all three BadCases.
  The primary value of this work lies in combining first-hand production experience with extensive experimental validation. Our main contributions include systematic theoretical analysis of repetition mechanisms, comprehensive evaluation of multiple solutions with task-specific applicability mapping, identification of early_stopping as the critical parameter for Beam Search effectiveness, and practical production-ready solutions validated in real deployment environments.

</details>


### [130] [TaskEval: Synthesised Evaluation for Foundation-Model Tasks](https://arxiv.org/abs/2512.04442)
*Dilani Widanapathiranage,Scott Barnett,Stefanus Kurniawan,Wannita Takerngsaksiri*

Main category: cs.AI

TL;DR: 该研究提出了一种综合方法，通过自动生成针对特定任务的评估程序并结合人类反馈，解决了使用基础模型（FMs）时在没有特定度量或数据集的情况下进行应用程序评估的问题。


<details>
  <summary>Details</summary>
Motivation: 当前的评估方法或数据集并不足以帮助软件团队对其特定任务的FM应用程序进行有效评估，尤其是在缺乏相关度量和数据集的情况下。

Method: 研究提出了一种任务通用的元模型来捕捉任何任务的特性，一个高效的交互协议利用人类反馈，并且开发了一个评估程序生成器，能够选择或生成合适的评估方法。

Result: 研究在两个不同的FM任务上实现了该方法：从图表数据提取和文档问题回答，初步评估表明所选的评估结果的准确性分别为93%和90%。

Conclusion: 该研究为FM任务的评估和审查提供了一种新的自动化和人性化的解决方案，解决了工程团队面临的难题。

Abstract: Hallucinations are a key concern when creating applications that rely on Foundation models (FMs). Understanding where and how these subtle failures occur in an application relies on evaluation methods known as \textit{evals}. Prior work focuses on defining new eval methods or benchmark datasets for specific tasks. However, neither helps a software team with a task-specific FM application when there is no metric or dataset. The demand for both automated approaches and deep integration of human insight makes this a challenging problem. We address this gap by proposing an approach to synthesise a FM task-specific evaluator program that provides automation and a custom UI for capturing feedback. The core novelty of our approach lies in: (1) a task-agnostic meta-model that captures properties of any FM task, (2) an interaction protocol for efficient use of human feedback, and (3) an eval synthesiser that selects or generates an appropriate set of evals. We implement our approach in \toolname and demonstrate the concept on two diverse FM tasks: chart data extraction and document question answering. A preliminary evaluation on the quality of our selected evals shows 93\% and 90\% accuracy respectively. Our research tackles a growing problem facing engineering teams, how to evaluate and review outputs from FM tasks.

</details>


### [131] [MARL Warehouse Robots](https://arxiv.org/abs/2512.04463)
*Price Allman,Lian Thang,Dre Simmons,Salmon Riaz*

Main category: cs.AI

TL;DR: 研究比较了QMIX和IPPO在仓储机器人中的表现，发现QMIX在稀疏奖励环境下的表现优于IPPO，但需要大量的超参数调优。


<details>
  <summary>Details</summary>
Motivation: 研究旨在评估多智能体强化学习（MARL）算法在协作仓储机器人中的应用效果，以解决小型部署中的问题，并探索未来大规模应用的可能性。

Method: 通过在RWARE环境和自定义的Unity 3D仿真中评估QMIX和IPPO算法进行对比研究。

Result: 研究结果表明，QMIX在稀疏奖励环境下显示出显著优于独立学习的方法的表现（平均回报为3.25 vs. IPPO的0.38），但需要进行大量的参数调整，特别是在epsilon衰减方面，以解决稀疏奖励环境中的探索问题。此外，研究还展示了在Unity ML-Agents中的成功部署。

Conclusion: 研究结论表明，虽然MARL在小规模部署中显示出潜力，但大规模部署仍然面临挑战。

Abstract: We present a comparative study of multi-agent reinforcement learning (MARL) algorithms for cooperative warehouse robotics. We evaluate QMIX and IPPO on the Robotic Warehouse (RWARE) environment and a custom Unity 3D simulation. Our experiments reveal that QMIX's value decomposition significantly outperforms independent learning approaches (achieving 3.25 mean return vs. 0.38 for advanced IPPO), but requires extensive hyperparameter tuning -- particularly extended epsilon annealing (5M+ steps) for sparse reward discovery. We demonstrate successful deployment in Unity ML-Agents, achieving consistent package delivery after 1M training steps. While MARL shows promise for small-scale deployments (2-4 robots), significant scaling challenges remain. Code and analyses: https://pallman14.github.io/MARL-QMIX-Warehouse-Robots/

</details>


### [132] [AI-Assisted Game Management Decisions: A Fuzzy Logic Approach to Real-Time Substituitions](https://arxiv.org/abs/2512.04480)
*Pedro Passos*

Main category: cs.AI

TL;DR: 本研究提出了一种基于模糊逻辑的决策支持系统（DSS），该系统能够实时、预示性地管理比赛，并通过引入一种调整后的PlayeRank指标和生理及情境变量，提高了决策的精确度和透明度。案例研究显示了系统在识别高风险决策中的优势。


<details>
  <summary>Details</summary>
Motivation: 现有的精英足球中，换人决策主要依赖于直觉或模仿历史趋势的预测模型，而这些模型往往受到历史偏见的限制。因此，本文旨在通过开发一种基于模糊逻辑的DSS，改进换人决策过程，使其能够基于客观规则和动态评估进行实时管理。

Method: 本文采用了模糊逻辑和基于角色的规范化方法来处理PlayerRank指标，以消除累积求和模型中固有的暴露偏差。同时，将生理指标（如疲劳）和情境变量（如战术角色相关的纪律风险）整合进动态换人优先级计算体系中。

Result: 系统在2018年FIFA世界杯巴西与比利时的比赛案例中证明了其有效性，能预测并识别出由人类决策者未考虑的风险。具体表现为，系统预测了‘FAGNER悖论’和‘Lukaku悖论’，在接近决定性事件时提供了关键的参考。

Conclusion: 研究发现，模糊逻辑是一种比传统黑盒模型更加透明和可解释的优化实时战术决策的工具。

Abstract: In elite soccer, substitution decisions entail significant financial and sporting consequences yet remain heavily reliant on intuition or predictive models that merely mimic historical biases. This paper introduces a Fuzzy Logic based Decision Support System (DSS) designed for real time, prescriptive game management. Unlike traditional Machine Learning approaches that encounter a predictive ceiling by attempting to replicate human behavior, our system audits performance through an objective, rule based inference engine. We propose a methodological advancement by reformulating the PlayeRank metric into a Cumulative Mean with Role Aware Normalization, eliminating the play time exposure bias inherent in cumulative sum models to enable accurate intra match comparison. The system integrates this refined metric with physiological proxies (fatigue) and contextual variables (disciplinary risk modulated by tactical role) to calculate a dynamic Substitution Priority (P final). Validation via a case study of the 2018 FIFA World Cup match between Brazil and Belgium demonstrates the system's ecological validity: it not only aligned with expert consensus on executed substitutions (for example Gabriel Jesus) but, crucially, identified high risk scenarios ignored by human decision makers. Specifically, the model flagged the "FAGNER Paradox" - a maximum priority defensive risk - minutes before a critical yellow card, and detected the "Lukaku Paradox", where an isolated assist masked a severe drop in participation. These results confirm that Fuzzy Logic offers a transparent, explainable, and superior alternative to black box models for optimizing real time tactical decisions.

</details>


### [133] [Persona-based Multi-Agent Collaboration for Brainstorming](https://arxiv.org/abs/2512.04488)
*Nate Straub,Saara Khan,Kat Jay,Brian Cabral,Oskar Linde*

Main category: cs.AI

TL;DR: 本文展示了基于人物的多智能体头脑风暴对于多样化主题和创意的想法生成的重要性，提出了一个基于人物的选择框架，通过多个实验设置了不同的角色搭配和合作模式，结果显示不同人物的选择影响了想法的领域，合作模式影响了想法的多样性，并且多智能体驱动的头脑风暴产生了更深层次和跨领域的创意。


<details>
  <summary>Details</summary>
Motivation: 现有的多智能体合作研究未充分考虑人物角色的影响，本文通过引入人物角色，探索了其对头脑风暴成果的影响。

Method: 本文开发了一个基于人物的选择框架，并通过多种实验设置来评估不同人物配对和合作方式对头脑风暴结果的影响。

Result: 研究表明，不同人物选择改变了想法的主题领域；合作模式影响了想法生成的多样性；多智能体驱动的头脑风暴能产生更深入和跨领域的创意。

Conclusion: 本文论证了基础任务具化的重要性，并证明了基于人物的选择可以使多智能体大脑风暴更加高效和有益。

Abstract: We demonstrate the importance of persona-based multi-agents brainstorming for both diverse topics and subject matter ideation. Prior work has shown that generalized multi-agent collaboration often provides better reasoning than a single agent alone. In this paper, we propose and develop a framework for persona-based agent selection, showing how persona domain curation can improve brainstorming outcomes. Using multiple experimental setups, we evaluate brainstorming outputs across different persona pairings (e.g., Doctor vs VR Engineer) and A2A (agent-to-agent) dynamics (separate, together, separate-then-together). Our results show that (1) persona choice shapes idea domains, (2) collaboration mode shifts diversity of idea generation, and (3) multi-agent persona-driven brainstorming produces idea depth and cross-domain coverage.

</details>


### [134] [A Modular Cognitive Architecture for Assisted Reasoning: The Nemosine Framework](https://arxiv.org/abs/2512.04500)
*Edervaldo Melo*

Main category: cs.AI

TL;DR: 本文介绍了Nemosine框架，这是一个模块化认知架构，旨在支持辅助推理、结构化思考和系统分析。该模型通过执行认知功能模块（“人像”）来组织任务，比如计划、评估、交叉检查和叙述综合。


<details>
  <summary>Details</summary>
Motivation: 该论文的动机在于开发一种基于元认知、分布式认知和模块化认知系统的认知架构，以提供辅助问题解决和决策支持的操作性结构，同时为未来计算实现提供清晰的概念基础，并促进符号模块化推理架构的研究。

Method: 该框架通过规范说明、内部一致性标准和可重复结构组件来记录其架构。

Result: 通过应用Nemosine框架，该研究为未来的计算实现提供了一个清晰的概念基础，并为符号模块化推理架构的研究做出了贡献。

Conclusion: 总的来说，该研究提供了一种新的思考和分析方式，能够促进复杂问题的解决和决策过程的优化。

Abstract: This paper presents the Nemosine Framework, a modular cognitive architecture designed to support assisted reasoning, structured thinking, and systematic analysis. The model operates through functional cognitive modules ("personas") that organize tasks such as planning, evaluation, cross-checking, and narrative synthesis. The framework combines principles from metacognition, distributed cognition, and modular cognitive systems to offer an operational structure for assisted problem-solving and decision support. The architecture is documented through formal specification, internal consistency criteria, and reproducible structural components. The goal is to provide a clear conceptual basis for future computational implementations and to contribute to the study of symbolic-modular architectures for reasoning.

</details>


### [135] [BiTAgent: A Task-Aware Modular Framework for Bidirectional Coupling between Multimodal Large Language Models and World Models](https://arxiv.org/abs/2512.04513)
*Yu-Wei Zhan,Xin Wang,Pengzhe Mao,Tongtong Feng,Ren Wang,Wenwu Zhu*

Main category: cs.AI

TL;DR: BiTAgent 提出了一种新的联合框架，通过双向路径增强了语言模型和世界模型之间的紧密耦合，实现了多任务学习和跨环境的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决统一的多模态语义理解和环境动态建模需求，设计一种能够实现双向交互的框架以支持开放型智能体的发展。

Method: 设计了一种名为BiTAgent的任务感知动态联合框架，包含任务感知动态联合学习、任务感知行为学习以及语义-动态联合优化三大模块。

Result: 在多任务和跨环境实验中，BiTAgent 达到了更高的稳定性和泛化能力，超越了现有基线方法。

Conclusion: BiTAgent 框架在开放型智能体的学习方向上迈出了重要一步，有助于构建可靠的多模态智能体。

Abstract: Building generalist embodied agents requires a unified system that can interpret multimodal goals, model environment dynamics, and execute reliable actions across diverse real-world tasks. Multimodal large language models (MLLMs) offer strong semantic priors and cross-modal generalization, while world models (WMs) provide actionable latent dynamics for prediction and control. Their combination holds promise for open-ended embodied intelligence, yet introduces two key challenges: (1) establishing a tight coupling between the semantic intent from MLLMs and the dynamic state representations within the WM's latent space, and (2) achieving task-aware adaptability that supports multi-task learning and cross-environment generalization. To address these limitations, we propose BiTAgent, a task-aware dynamic joint framework that enables bidirectional coupling between MLLMs and WMs. BiTAgent establishes two complementary pathways: a forward path that injects MLLM representations into the WM's latent space for semantically guided imagination, and a backward path where WM-generated feedback refines the MLLM's semantic space via dense text-conditioned rewards. This bidirectional interaction is realized through three synergistic components: Task-Aware Dynamic Joint Learning, Task-Aware Behavior Learning, and MLLM-WM Joint Optimization, which together harmonize semantic reasoning and dynamic prediction. Extensive experiments across multi-task and cross-environment settings demonstrate superior stability and generalization over state-of-the-art baselines, marking a step toward open-ended embodied learning.

</details>


### [136] [SlideGen: Collaborative Multimodal Agents for Scientific Slide Generation](https://arxiv.org/abs/2512.04529)
*Xin Liang,Xiang Zhang,Yiwei Xu,Siqi Sun,Chenyu You*

Main category: cs.AI

TL;DR: SlideGen 是一种新的框架，能够从科学论文生成高质量且具有逻辑流动性和视觉吸引力的幻灯片，通过协调大纲制定、内容映射、布局设计、注释合成和迭代优化，SlideGen 在视觉质量、内容忠实度和可读性方面超越了现有方法，展示了解决复杂多模态推理任务中理解与呈现之间的桥梁如何通过有意识的合作实现的可能性。


<details>
  <summary>Details</summary>
Motivation: 现有方法集中在文本摘要，忽视了幻灯片制作的视觉成分和设计的繁复性。作者旨在开发一种智能的、模块化的框架 SlideGen，该框架能够实现自动化的科学论文到幻灯片的生成，并鼓励协作式的设计，以产生专业级别的幻灯片。

Method: SlideGen 采用了一种多智能体系统，该系统由一组视觉语言智能体组成，能够协同工作以理解文档结构和意义，生成具有逻辑流动性和视觉吸引力的幻灯片。它通过协作的纲要制定、内容映射、布局设计、注释合成和迭代优化等步骤运作。

Result: 在不同的基准测试和强大的基线模型中，SlideGen 在视觉质量、内容忠实度和可读性方面均优于现有的方法，确立了自动幻灯片生成的新标准。它展示了智能协作如何在复杂多模态推理任务中实现理解和呈现的桥梁。

Conclusion: 本研究不仅为设计感知的多模态幻灯片生成奠定了基础，还证明了通过有意识的合作可以解决复杂多模态推理任务中的理解和呈现问题，这为未来的多模态研究和应用开辟了新的可能性。

Abstract: Generating academic slides from scientific papers is a challenging multimodal reasoning task that requires both long context understanding and deliberate visual planning. Existing approaches largely reduce it to text only summarization, overlooking the visual component and design intensive nature of slide creation. In this paper we introduce SlideGen, an agentic, modular, and visual in the loop framework for scientific paper to slide generation. SlideGen orchestrates a group of vision language agents that reason collaboratively over the document structure and semantics, producing editable PPTX slides with logical flow and compelling visual presentation. By integrating coordinated outlining, mapping, arrangement, note synthesis, and iterative refinement, our system consistently delivers slides of expert level quality. Across diverse benchmarks and strong baselines, SlideGen outperforms existing methods in visual quality, content faithfulness, and readability, positioning it as the new state of the art in automated slide generation. Our work establishes a foundation for design aware multimodal slide generation, demonstrating how agentic collaboration can bridge understanding and presentation in complex multimodal reasoning tasks.

</details>


### [137] [The Ethics of Generative AI](https://arxiv.org/abs/2512.04598)
*Michael Klenk*

Main category: cs.AI

TL;DR: 本章探讨生成式AI的伦理问题，首先介绍了生成式AI技术基础，展示了它如何通过模仿人类体验为科技带来人性化的感受，从而成为哲学伦理研究的良好切入点。接着分析生成式AI如何加剧或缓解诸如责任、隐私、偏见与公平性、疏离与剥削等已有的AI伦理问题。最后，探讨生成式AI特有的摹仿生成性带来的伦理挑战，如作者权与认可、与机器之间似是而非的社会关系的形成以及新型影响力、说服力和操纵形式等问题。


<details>
  <summary>Details</summary>
Motivation: 通过对生成式AI技术及其伦理影响的深入探讨，该章节旨在为研究者和实践者提供一个理解和应对未来技术发展带来的伦理挑战的新视角。

Method: 该章节通过技术介绍和伦理分析相结合的方法，首先介绍生成式AI的工作原理和技术基础，然后探讨其带来的伦理问题和挑战。

Result: 本章节为读者提供了生成式AI技术和伦理问题之间关系的理解路径，有助于更好地认识到技术发展与伦理决策之间的紧密联系。

Conclusion: 生成式AI在带来创新的同时也引发了一系列伦理问题，需要通过多学科的共同探索来找到合适的解决方案，以确保技术的健康发展和公正应用。

Abstract: This chapter discusses the ethics of generative AI. It provides a technical primer to show how generative AI affords experiencing technology as if it were human, and this affordance provides a fruitful focus for the philosophical ethics of generative AI. It then shows how generative AI can both aggravate and alleviate familiar ethical concerns in AI ethics, including responsibility, privacy, bias and fairness, and forms of alienation and exploitation. Finally, the chapter examines ethical questions that arise specifically from generative AI's mimetic generativity, such as debates about authorship and credit, the emergence of as-if social relationships with machines, and new forms of influence, persuasion, and manipulation.

</details>


### [138] [Neural Decoding of Overt Speech from ECoG Using Vision Transformers and Contrastive Representation Learning](https://arxiv.org/abs/2512.04618)
*Mohamed Baha Ben Ticha,Xingchen Ran,Guillaume Saldanha,Gaël Le Godais,Philémon Roussel,Marc Aubert,Amina Fontanell,Thomas Costecalde,Lucas Struber,Serpil Karakas,Shaomin Zhang,Philippe Kahane,Guillaume Charvet,Stéphan Chabardès,Blaise Yvert*

Main category: cs.AI

TL;DR: 该研究提出了一种基于编码器-解码器深度神经架构的离线语音解码pipeline，结合Vision Transformers和对比学习技术，直接从ECoG信号中回归语音。该方法在两种数据集上进行了评估，证明了在完全植入的无线皮质脊髓记录系统中解码语音的可能性。


<details>
  <summary>Details</summary>
Motivation: 为了克服当前从表面ECoG记录中重建语音的挑战，研究提出了一种新的方法，以期实现直接从ECoG信号中回归语音，以便在线重建语音。

Method: 该方法基于编码器-解码器深度神经架构，结合Vision Transformers和对比学习技术，用于直接从ECoG信号中回归语音。在两个数据集上进行了评估，一个来自临床癫痫患者的硬脑膜下电极，另一个来自植入式WIMAGINE皮层硬膜外系统的 motor BCI实验参与者。

Result: 研究表明，该方法能够从完全植入的无线皮质脊髓记录系统中解码语音，为长期使用提供了潜力。

Conclusion: 该研究证明了结合Vision Transformers和对比学习直接从ECoG信号中回归语音的有效性，为BCI提供了一种新的解决方案，有望在未来实现从大脑皮层直接重建语音。

Abstract: Speech Brain Computer Interfaces (BCIs) offer promising solutions to people with severe paralysis unable to communicate. A number of recent studies have demonstrated convincing reconstruction of intelligible speech from surface electrocorticographic (ECoG) or intracortical recordings by predicting a series of phonemes or words and using downstream language models to obtain meaningful sentences. A current challenge is to reconstruct speech in a streaming mode by directly regressing cortical signals into acoustic speech. While this has been achieved recently using intracortical data, further work is needed to obtain comparable results with surface ECoG recordings. In particular, optimizing neural decoders becomes critical in this case. Here we present an offline speech decoding pipeline based on an encoder-decoder deep neural architecture, integrating Vision Transformers and contrastive learning to enhance the direct regression of speech from ECoG signals. The approach is evaluated on two datasets, one obtained with clinical subdural electrodes in an epileptic patient, and another obtained with the fully implantable WIMAGINE epidural system in a participant of a motor BCI trial. To our knowledge this presents a first attempt to decode speech from a fully implantable and wireless epidural recording system offering perspectives for long-term use.

</details>


### [139] [Turbo-Muon: Accelerating Orthogonality-Based Optimization with Pre-Conditioning](https://arxiv.org/abs/2512.04632)
*Thibaut Boissin,Thomas Massena,Franck Mamalet,Mathieu Serrurier*

Main category: cs.AI

TL;DR: 本文介绍了一种预条件加速方法，用于加速Newton-Schulz迭代，使其计算成本显著降低，并能在不牺牲精度的情况下减少迭代次数。实验证明，该方法能实现最多2.8倍的加速，并在实际场景中提高端到端训练时间5-10%。


<details>
  <summary>Details</summary>
Motivation: 为了降低Muon等正交性优化器中昂贵的梯度正交化步骤的成本。

Method: 提出了一个预条件加速预处理程序，该预处理程序加速了Newton-Schulz迭代，并在不牺牲精度的情况下减少了迭代次数。

Result: 该预处理程序实现最多2.8倍的加速，并在实际场景中提高端到端训练时间5-10%。在具有挑战性的语言或视觉任务中，验证了该方法能保持或提升模型性能的同时改善运行时间。

Conclusion: 该预处理程序无需调优，并且可以作为一种简单即插即用的替代品。

Abstract: Orthogonality-based optimizers, such as Muon, have recently shown strong performance across large-scale training and community-driven efficiency challenges. However, these methods rely on a costly gradient orthogonalization step. Even efficient iterative approximations such as Newton-Schulz remain expensive, typically requiring dozens of matrix multiplications to converge. We introduce a preconditioning procedure that accelerates Newton-Schulz convergence and reduces its computational cost. We evaluate its impact and show that the overhead of our preconditioning can be made negligible. Furthermore, the faster convergence it enables allows us to remove one iteration out of the usual five without degrading approximation quality. Our publicly available implementation achieves up to a 2.8x speedup in the Newton-Schulz approximation. We also show that this has a direct impact on end-to-end training runtime with 5-10% improvement in realistic training scenarios across two efficiency-focused tasks. On challenging language or vision tasks, we validate that our method maintains equal or superior model performance while improving runtime. Crucially, these improvements require no hyperparameter tuning and can be adopted as a simple drop-in replacement. Our code is publicly available on github.

</details>


### [140] [Towards Ethical Multi-Agent Systems of Large Language Models: A Mechanistic Interpretability Perspective](https://arxiv.org/abs/2512.04691)
*Jae Hee Lee,Anne Lauscher,Stefano V. Albrecht*

Main category: cs.AI

TL;DR: 该研究旨在通过机制可解释性来确保大型语言模型的多agent系统（MALMs）的伦理行为，提出了三大挑战：建立全面的伦理行为评估框架；通过机制可解释性阐明内在机制；以及实施参数高效对齐技术。


<details>
  <summary>Details</summary>
Motivation: 鉴于大型语言模型在各种应用中的广泛应用，特别是在多agent系统中的作用，本文提出了一种机制可解释性的研究议程，以应对伦理挑战。

Method: 本文没有具体介绍方法，而是提出了研究议程中需要解决的三大挑战：评估框架设计、机制可解释性研究以及参数对齐技术的实现。

Result: 本文为确保MALMs的伦理行为提出了研究方向，旨在通过综合评估、机制探索和对齐策略实现道德目标，但未具体展示技术成果。

Conclusion: 通过机制可解释性，本文成功地提出了一个确保大型语言模型多agent系统伦理行为的研究议程，并识别出了需重点关注的研究方向。

Abstract: Large language models (LLMs) have been widely deployed in various applications, often functioning as autonomous agents that interact with each other in multi-agent systems. While these systems have shown promise in enhancing capabilities and enabling complex tasks, they also pose significant ethical challenges. This position paper outlines a research agenda aimed at ensuring the ethical behavior of multi-agent systems of LLMs (MALMs) from the perspective of mechanistic interpretability. We identify three key research challenges: (i) developing comprehensive evaluation frameworks to assess ethical behavior at individual, interactional, and systemic levels; (ii) elucidating the internal mechanisms that give rise to emergent behaviors through mechanistic interpretability; and (iii) implementing targeted parameter-efficient alignment techniques to steer MALMs towards ethical behaviors without compromising their performance.

</details>


### [141] [Playing the Player: A Heuristic Framework for Adaptive Poker AI](https://arxiv.org/abs/2512.04714)
*Andrew Paterson,Carl Sanders*

Main category: cs.AI

TL;DR: 本文挑战了扑克AI领域长期以来关于无利可图、机器完美游戏的概念，提出了一个名为Patrick的新AI，旨在通过最大化利用对手的心理和理性缺陷来取得胜利。该AI通过独特的学习方法和在大规模对局中的表现证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 本文提出挑战传统观念，构建能够理解并攻击人类对手的心理、心理和非理性行为的AI，揭示了探索人类不完美之处比追求无利可图的游戏更具挑战性和意义。

Method: 该文提出了一个名为Patrick的AI，采用了一种新的预测驱动的学习方法，并通过详细的结构设计阐述了其独特之处。此外，作者还通过大规模的实际对局来验证该AI的有效性。

Result: Patrick在64,267局对局中表现出了强大的效果，展示了其创新的方法和独特的设计在面对人类对手时的优越性。

Conclusion: 本文通过对Patrick的详细分析，作者认为对于AI大师而言，真正值得关注的是如何应对和利用人类对手的不完美，而不仅仅是构建完美的、不可被击败的AI。

Abstract: For years, the discourse around poker AI has been dominated by the concept of solvers and the pursuit of unexploitable, machine-perfect play. This paper challenges that orthodoxy. It presents Patrick, an AI built on the contrary philosophy: that the path to victory lies not in being unexploitable, but in being maximally exploitative. Patrick's architecture is a purpose-built engine for understanding and attacking the flawed, psychological, and often irrational nature of human opponents. Through detailed analysis of its design, its novel prediction-anchored learning method, and its profitable performance in a 64,267-hand trial, this paper makes the case that the solved myth is a distraction from the real, far more interesting challenge: creating AI that can master the art of human imperfection.

</details>


### [142] [Human Cognitive Biases in Explanation-Based Interaction: The Case of Within and Between Session Order Effect](https://arxiv.org/abs/2512.04764)
*Dario Pesenti,Alessandro Bogani,Katya Tentori,Stefano Teso*

Main category: cs.AI

TL;DR: 本研究通过大规模用户实验探讨了解释交互（XIL）中的顺序效应问题，发现顺序效应对用户信任模型的行为测量有一定影响，但对反馈质量的影响较小，表明XIL方法的有效性未受显著威胁。


<details>
  <summary>Details</summary>
Motivation: 论文旨在解决解释交互（XIL）方法在调试任务中可能遇到的顺序效应问题，以明确认知偏差如何影响用户对模型的信任和反馈质量。

Method: 通过设计两场大规模用户研究，操控解释的正确性和错误性顺序展示给参与者，评估顺序效应在调试会话内外的影响。

Result: 顺序效应在调试会话内对用户对模型的信任有所影响，但在不同调试会话之间则不显著。用户反馈的质量总体来说是令人满意的，即使在顺序改变的情况下，也不会出现明显的变化。

Conclusion: 研究结果表明，顺序效应并不会对XIL方法的应用构成重大挑战，这为理解人工智能中的人类因素提供了进一步的见解。

Abstract: Explanatory Interactive Learning (XIL) is a powerful interactive learning framework designed to enable users to customize and correct AI models by interacting with their explanations. In a nutshell, XIL algorithms select a number of items on which an AI model made a decision (e.g. images and their tags) and present them to users, together with corresponding explanations (e.g. image regions that drive the model's decision). Then, users supply corrective feedback for the explanations, which the algorithm uses to improve the model. Despite showing promise in debugging tasks, recent studies have raised concerns that explanatory interaction may trigger order effects, a well-known cognitive bias in which the sequence of presented items influences users' trust and, critically, the quality of their feedback. We argue that these studies are not entirely conclusive, as the experimental designs and tasks employed differ substantially from common XIL use cases, complicating interpretation. To clarify the interplay between order effects and explanatory interaction, we ran two larger-scale user studies (n = 713 total) designed to mimic common XIL tasks. Specifically, we assessed order effects both within and between debugging sessions by manipulating the order in which correct and wrong explanations are presented to participants. Order effects had a limited, through significant impact on users' agreement with the model (i.e., a behavioral measure of their trust), and only when examined withing debugging sessions, not between them. The quality of users' feedback was generally satisfactory, with order effects exerting only a small and inconsistent influence in both experiments. Overall, our findings suggest that order effects do not pose a significant issue for the successful employment of XIL approaches. More broadly, our work contributes to the ongoing efforts for understanding human factors in AI.

</details>


### [143] [Enabling Ethical AI: A case study in using Ontological Context for Justified Agentic AI Decisions](https://arxiv.org/abs/2512.04822)
*Liam McGee,James Harvey,Lucy Cull,Andreas Vermeulen,Bart-Floris Visscher,Malvika Sharan*

Main category: cs.AI

TL;DR: 本研究提出了一种协作的人工智能方法，通过构建可检验的语义层来增强Agentic AI的能力。该方法通过AI提出候选知识结构并由领域专家验证、修正和扩展。这种方法能够捕捉隐性机构知识、提高响应质量和效率，并减轻机构遗忘。


<details>
  <summary>Details</summary>
Motivation: 为了解决当前算法解释性不足的问题，研究倡导从事后解释转向具有可解释性的Agentic AI，使得决策基于明确、可检验的证据和透明推理，供专家和非专家理解。

Method: 通过协作过程，AI提出知识结构，专家进行验证和改进。

Result: 该过程成功地捕捉了隐含的机构知识，提高了响应质量和效率，并缓解了机构遗忘问题。

Conclusion: 论文提出了一个从事后解释到具有可解释性Agentic AI的转变方向，强调了决策过程的透明度和可理解性。

Abstract: In this preprint, we present A collaborative human-AI approach to building an inspectable semantic layer for Agentic AI. AI agents first propose candidate knowledge structures from diverse data sources; domain experts then validate, correct, and extend these structures, with their feedback used to improve subsequent models. Authors show how this process captures tacit institutional knowledge, improves response quality and efficiency, and mitigates institutional amnesia. We argue for a shift from post-hoc explanation to justifiable Agentic AI, where decisions are grounded in explicit, inspectable evidence and reasoning accessible to both experts and non-specialists.

</details>


### [144] [Model-Based and Sample-Efficient AI-Assisted Math Discovery in Sphere Packing](https://arxiv.org/abs/2512.04829)
*Rasul Tutunov,Alexandre Maraval,Antoine Grosnit,Xihan Li,Jun Wang,Haitham Bou-Ammar*

Main category: cs.AI

TL;DR: 文章提出了一种新的样本高效模型导向搜索方法，用于构建半定规划（SDP），以求解高维空间中球体的密集排列问题。这种方法结合了贝叶斯优化和蒙特卡洛树搜索，取得在4到16维的新方法最先进上界。


<details>
  <summary>Details</summary>
Motivation: 为了克服半定规划在评估上的局限性，而现有的数据密集型AI方法不可行，该研究旨在探索如何使用样本高效模型导向搜索方法来求解长期存在的几何问题。

Method: 该方法将SDP构建问题转化为一个顺序决策过程，称为SDP游戏。使用了结合贝叶斯优化和蒙特卡洛树搜索的样本高效模型导向框架。

Result: 该方法在4到16维空间中获得了新的最先进的上界，证明了样本高效模型导向搜索在数学上严格的、受评估限制的问题上的有效性。

Conclusion: 该研究指出，样本高效及模型导向的搜索方法是人工智能辅助发现的一个有潜力的方向，而不是单纯依赖大规模的LLM探索。

Abstract: Sphere packing, Hilbert's eighteenth problem, asks for the densest arrangement of congruent spheres in n-dimensional Euclidean space. Although relevant to areas such as cryptography, crystallography, and medical imaging, the problem remains unresolved: beyond a few special dimensions, neither optimal packings nor tight upper bounds are known. Even a major breakthrough in dimension $n=8$, later recognised with a Fields Medal, underscores its difficulty. A leading technique for upper bounds, the three-point method, reduces the problem to solving large, high-precision semidefinite programs (SDPs). Because each candidate SDP may take days to evaluate, standard data-intensive AI approaches are infeasible. We address this challenge by formulating SDP construction as a sequential decision process, the SDP game, in which a policy assembles SDP formulations from a set of admissible components. Using a sample-efficient model-based framework that combines Bayesian optimisation with Monte Carlo Tree Search, we obtain new state-of-the-art upper bounds in dimensions $4-16$, showing that model-based search can advance computational progress in longstanding geometric problems. Together, these results demonstrate that sample-efficient, model-based search can make tangible progress on mathematically rigid, evaluation limited problems, pointing towards a complementary direction for AI-assisted discovery beyond large-scale LLM-driven exploration.

</details>


### [145] [Are LLMs Truly Multilingual? Exploring Zero-Shot Multilingual Capability of LLMs for Information Retrieval: An Italian Healthcare Use Case](https://arxiv.org/abs/2512.04834)
*Vignesh Kumar Kembu,Pierandrea Morandini,Marta Bianca Maria Ranzini,Antonino Nocera*

Main category: cs.AI

TL;DR: 该研究探讨了开源多语言大语言模型在意大利电子健康记录中提取共病信息方面的表现，发现部分模型在零样本和本地环境中的表现不佳，而其他模型则在不同疾病上的泛化能力较差。


<details>
  <summary>Details</summary>
Motivation: 由于传统NLP技术在处理复杂多变的临床语言时存在局限，需要更先进的方法来有效提取电子健康记录中的信息，因此研究开源多语言大语言模型在这一领域的应用是必要的。

Method: 通过 Detailed Experimental Campaign，研究对开源多语言大语言模型在意大利电子健康记录中的共病提取任务进行了实验。

Result: 实验结果显示，一些大语言模型在零样本本地环境中表现不佳，部分模型在不同疾病上的泛化能力较差，性能不稳定。

Conclusion: 本研究表明，虽然大语言模型在电子健康管理中具有潜力，但其表现受多种因素影响，需要进一步优化以提高其性能一致性。

Abstract: Large Language Models (LLMs) have become a key topic in AI and NLP, transforming sectors like healthcare, finance, education, and marketing by improving customer service, automating tasks, providing insights, improving diagnostics, and personalizing learning experiences. Information extraction from clinical records is a crucial task in digital healthcare. Although traditional NLP techniques have been used for this in the past, they often fall short due to the complexity, variability of clinical language, and high inner semantics in the free clinical text. Recently, Large Language Models (LLMs) have become a powerful tool for better understanding and generating human-like text, making them highly effective in this area. In this paper, we explore the ability of open-source multilingual LLMs to understand EHRs (Electronic Health Records) in Italian and help extract information from them in real-time. Our detailed experimental campaign on comorbidity extraction from EHR reveals that some LLMs struggle in zero-shot, on-premises settings, and others show significant variation in performance, struggling to generalize across various diseases when compared to native pattern matching and manual annotations.

</details>


### [146] [From Task Executors to Research Partners: Evaluating AI Co-Pilots Through Workflow Integration in Biomedical Research](https://arxiv.org/abs/2512.04854)
*Lukas Weidener,Marko Brkić,Chiara Bacci,Mihailo Jovanović,Emre Ulgac,Alex Dobrin,Johannes Weniger,Martin Vlas,Ritvik Singh,Aakaash Meduri*

Main category: cs.AI

TL;DR: 这篇文章快速审查了预临床生物医学研究中人工智能系统的基准测试实践，指出当前的基准测试主要评估单个组件能力，而缺乏对对话质量、工作流程协调、会话连续性和研究体验等关键维度的评估。作者提出了一种面向过程的评估框架以全面评估AI系统作为研究伙伴的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前的评估框架在评估人工智能系统在生物医学研究中的有效性时存在不足，特别是在模拟真实研究协作中的情况。本文旨在通过快速审查现有的基准测试实践，指出其中的不足，并提出一种面向过程的评估框架来解决这些问题。

Method: 该文采用文献搜索方法，从2018年1月1日至2025年10月31日，在三个主要数据库和两个预印本服务器中搜索相关基准测试。共有14个基准评估了AI在文献理解、实验设计和假设生成中的能力。文章详细分析了现有基准测试的内容，并与完整的研究协作过程进行对比。

Result: 文章揭示了当前基准测试主要聚焦在单独组件的能力，如数据分析质量、假设有效性、实验设计等。而真正的研究协作则需要跨越多个阶段的整合工作流程，涉及上下文记忆、适应性对话和约束传播。现有的基准测试无法全面衡量AI系统在这方面的表现。

Conclusion: 文章提出了一种新的面向过程的评估框架，以全面评估AI系统在生物医学研究中的合作能力，涵盖了对话质量、工作流程协调、会话连续性和研究体验四个方面。这种评估框架能够更真实地反映AI系统在研究环境中的表现。

Abstract: Artificial intelligence systems are increasingly deployed in biomedical research. However, current evaluation frameworks may inadequately assess their effectiveness as research collaborators. This rapid review examines benchmarking practices for AI systems in preclinical biomedical research. Three major databases and two preprint servers were searched from January 1, 2018 to October 31, 2025, identifying 14 benchmarks that assess AI capabilities in literature understanding, experimental design, and hypothesis generation. The results revealed that all current benchmarks assess isolated component capabilities, including data analysis quality, hypothesis validity, and experimental protocol design. However, authentic research collaboration requires integrated workflows spanning multiple sessions, with contextual memory, adaptive dialogue, and constraint propagation. This gap implies that systems excelling on component benchmarks may fail as practical research co-pilots. A process-oriented evaluation framework is proposed that addresses four critical dimensions absent from current benchmarks: dialogue quality, workflow orchestration, session continuity, and researcher experience. These dimensions are essential for evaluating AI systems as research co-pilots rather than as isolated task executors.

</details>


### [147] [STELLA: Guiding Large Language Models for Time Series Forecasting with Semantic Abstractions](https://arxiv.org/abs/2512.04871)
*Junjie Fan,Hongye Zhao,Linduo Wei,Jiayu Rao,Guijia Li,Jiaxin Yuan,Wenqi Xu,Yong Qi*

Main category: cs.AI

TL;DR: STELLA通过动态语义抽象机制，将输入时间序列分解为趋势、季节性和残差成分，并将其转换为层级语义锚点，增强LLM的时间序列预测能力。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型时间序列预测方法未能充分利用LLM的推理能力，且缺乏有效结合全局和实例特定上下文的信息注入机制。

Method: STELLA框架利用动态语义抽象机制，将时间序列分解为三个组成部分，并通过上下文锚点指导LLM预测内在动力。

Result: STELLA在八个基准数据集上的实验表明，其在长短期预测中表现优于现有最先进的方法，并且在零样本和少量样本设置下具有更强的泛化能力。

Conclusion: STELLA框架验证了动态生成语义锚点的有效性，为大语言模型的时间序列预测问题提供了新的解决方案。

Abstract: Recent adaptations of Large Language Models (LLMs) for time series forecasting often fail to effectively enhance information for raw series, leaving LLM reasoning capabilities underutilized. Existing prompting strategies rely on static correlations rather than generative interpretations of dynamic behavior, lacking critical global and instance-specific context. To address this, we propose STELLA (Semantic-Temporal Alignment with Language Abstractions), a framework that systematically mines and injects structured supplementary and complementary information. STELLA employs a dynamic semantic abstraction mechanism that decouples input series into trend, seasonality, and residual components. It then translates intrinsic behavioral features of these components into Hierarchical Semantic Anchors: a Corpus-level Semantic Prior (CSP) for global context and a Fine-grained Behavioral Prompt (FBP) for instance-level patterns. Using these anchors as prefix-prompts, STELLA guides the LLM to model intrinsic dynamics. Experiments on eight benchmark datasets demonstrate that STELLA outperforms state-of-the-art methods in long- and short-term forecasting, showing superior generalization in zero-shot and few-shot settings. Ablation studies further validate the effectiveness of our dynamically generated semantic anchors.

</details>


### [148] [Algorithmic Thinking Theory](https://arxiv.org/abs/2512.04923)
*MohammadHossein Bateni,Vincent Cohen-Addad,Yuzhou Gu,Silvio Lattanzi,Simon Meierhans,Christopher Mohri*

Main category: cs.AI

TL;DR: 本文提出了一种理论框架，用于分析涉及迭代改进和答案聚合的推理算法。该框架通过实验验证来规范流行技术背后的原理，为设计新一代更强大的推理方法提供了基础。


<details>
  <summary>Details</summary>
Motivation: 由于大规模语言模型（LLMs）能够通过迭代改善现有的解决方案表现得更加出色，本文旨在通过一个理论框架来理解和改进这种基于试错的推理策略。

Method: 本文通过引入一个新的理论框架，正式化了改进和答案聚合技术背后的原理，该框架是基于实验证据的，能够适用于当前和未来的推理框架。

Result: 该框架提供了一个通用的视角，可以帮助设计出更强的推理方法，其核心在于理解并改进基于试错的推理策略。

Conclusion: 新的理论框架能够超越依赖于架构的具体方法，提供了一种更通用的方式来看待推理策略的发展。

Abstract: Large language models (LLMs) have proven to be highly effective for solving complex reasoning tasks. Surprisingly, their capabilities can often be improved by iterating on previously generated solutions. In this context, a reasoning plan for generating and combining a set of solutions can be thought of as an algorithm for reasoning using a probabilistic oracle.
  We introduce a theoretical framework for analyzing such reasoning algorithms. This framework formalizes the principles underlying popular techniques for iterative improvement and answer aggregation, providing a foundation for designing a new generation of more powerful reasoning methods. Unlike approaches for understanding models that rely on architectural specifics, our model is grounded in experimental evidence. As a result, it offers a general perspective that may extend to a wide range of current and future reasoning oracles.

</details>


### [149] [Toward Continuous Neurocognitive Monitoring: Integrating Speech AI with Relational Graph Transformers for Rare Neurological Diseases](https://arxiv.org/abs/2512.04938)
*Raquel Norel,Michele Merler,Pavitra Modi*

Main category: cs.AI

TL;DR: 本研究提出了一种通过智能手机语音分析进行连续神经认知监测的方法，利用Relational Graph Transformer (RELGT)架构，并在苯丙酮尿症(PKU)患者的“言语表达熟练度”与血液本胺酚水平之间建立了关联，期望这种监测方式能够提前数周预警病情恶化并实现多语言部署，但挑战包括跨多种疾病验证、临床工作流程整合以及确保公平性。


<details>
  <summary>Details</summary>
Motivation: 现有的认知症状（“脑雾”）在传统测试中可能难以显现，特别是在罕见神经性疾病患者中。本研究旨在通过智能手机语音分析和Relational Graph Transformer (RELGT)架构，提供一种连续性、个性化的神经认知监测方法，以期在症状恶化前提供预警。

Method: 研究使用智能手机应用对患者进行语音采样，并基于所收集的语音数据，利用Relational Graph Transformer (RELGT)架构评估和量化患者的认知能力，特别是“言语表达熟练度”。并分析这种评估与血液中苯丙氨酸水平及标准认知测试结果的相关性。

Result: 初步研究在苯丙酮尿症(PKU)患者中发现患者“言语表达熟练度”与血液中苯丙氨酸水平有显著相关性（p = -0.50，p < 0.005），但与标准认知测试结果的相关性较弱（|r| < 0.35）。此方法可能克服多种医疗数据中的信息瓶颈，实现基于夯实信息的提前数周的预警机制。

Conclusion: 尽管研究显示该方法具有一定的潜力，但仍需在更多疾病中进行验证、克服临床工作流程整合障碍，并实现多语言部署，才能真正将间歇性的神经学监测转变为面向大量全球患者进行持续个性化的监测。

Abstract: Patients with rare neurological diseases report cognitive symptoms -"brain fog"- invisible to traditional tests. We propose continuous neurocognitive monitoring via smartphone speech analysis integrated with Relational Graph Transformer (RELGT) architectures. Proof-of-concept in phenylketonuria (PKU) shows speech-derived "Proficiency in Verbal Discourse" correlates with blood phenylalanine (p = -0.50, p < 0.005) but not standard cognitive tests (all |r| < 0.35). RELGT could overcome information bottlenecks in heterogeneous medical data (speech, labs, assessments), enabling predictive alerts weeks before decompensation. Key challenges: multi-disease validation, clinical workflow integration, equitable multilingual deployment. Success would transform episodic neurology into continuous personalized monitoring for millions globally.

</details>
