<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 76]
- [cs.CL](#cs.CL) [Total: 51]
- [cs.AI](#cs.AI) [Total: 16]
- [cs.AR](#cs.AR) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Agent Banana: High-Fidelity Image Editing with Agentic Thinking and Tooling](https://arxiv.org/abs/2602.09084)
*Ruijie Ye,Jiayi Zhang,Zhuoxin Liu,Zihao Zhu,Siyuan Yang,Li Li,Tianfu Fu,Franck Dernoncourt,Yue Zhao,Jiacheng Zhu,Ryan Rossi,Wenhao Chai,Zhengzhong Tu*

Main category: cs.CV

TL;DR: 该研究旨在解决图像编辑中的长期挑战，提出了Agent Banana框架，通过Context Folding和Image Layer Decomposition两大机制提升了多轮编辑的准确性和背景保真度。


<details>
  <summary>Details</summary>
Motivation: 研究发现，当前的图像编辑工具和模型存在过度编辑、单轮编辑不足以及高分辨率评估不准确的问题。因此，提出了一个名为Agent Banana的框架来解决这些问题。

Method: 研究引入了Context Folding和Image Layer Decomposition两个关键技术来实现高质量的目标感知编辑。

Result: 基于HDD-Bench基准测试，Agent Banana在多轮一致性（IC 0.871，SSIM-OM 0.84，LPIPS-OM 0.12）和背景保真度方面表现最佳，同时在指令跟随方面保持竞争力，且在标准单轮编辑基准测试中也表现出色。

Conclusion: 研究通过引入Agent Banana框架，克服了以前的图像编辑工具和模型中存在的挑战，有望推动专业级图像编辑的发展及其实际工作流程的集成。

Abstract: We study instruction-based image editing under professional workflows and identify three persistent challenges: (i) editors often over-edit, modifying content beyond the user's intent; (ii) existing models are largely single-turn, while multi-turn edits can alter object faithfulness; and (iii) evaluation at around 1K resolution is misaligned with real workflows that often operate on ultra high-definition images (e.g., 4K). We propose Agent Banana, a hierarchical agentic planner-executor framework for high-fidelity, object-aware, deliberative editing. Agent Banana introduces two key mechanisms: (1) Context Folding, which compresses long interaction histories into structured memory for stable long-horizon control; and (2) Image Layer Decomposition, which performs localized layer-based edits to preserve non-target regions while enabling native-resolution outputs. To support rigorous evaluation, we build HDD-Bench, a high-definition, dialogue-based benchmark featuring verifiable stepwise targets and native 4K images (11.8M pixels) for diagnosing long-horizon failures. On HDD-Bench, Agent Banana achieves the best multi-turn consistency and background fidelity (e.g., IC 0.871, SSIM-OM 0.84, LPIPS-OM 0.12) while remaining competitive on instruction following, and also attains strong performance on standard single-turn editing benchmarks. We hope this work advances reliable, professional-grade agentic image editing and its integration into real workflows.

</details>


### [2] [SemanticMoments: Training-Free Motion Similarity via Third Moment Features](https://arxiv.org/abs/2602.09146)
*Saar Huberman,Kfir Goldberg,Or Patashnik,Sagie Benaim,Ron Mokady*

Main category: cs.CV

TL;DR: 该研究通过引入SimMotion基准，展示了现有模型在捕捉运动和外观差异方面的不足，并提出了一种称为SemanticMoments的方法，在多个基准上展示了优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有的视频表示方法过于依赖静态外观和场景上下文，而忽视了运动动态，这源于其训练数据和目标。为了填补这一空白，该研究引入了一个新的基准SimMotion，并旨在提供一个基于语义特征的时间统计方法来理解和分割运动与外观。

Method: 研究提出了一个称为SemanticMoments的简单方法，不需要额外训练，通过在预训练的语义模型上计算时间统计数据来实现运动和外观的分割。

Result: 在SimMotion基准上，SemanticMoments方法在多个视频理解任务中取得了优于现有RGB、光流和文本监督方法的结果，特别是在捕捉运动和外观差异方面。

Conclusion: 研究证明了在语义特征空间中的时间统计提供了运动为中心的视频理解的可扩展和感知基础。

Abstract: Retrieving videos based on semantic motion is a fundamental, yet unsolved, problem. Existing video representation approaches overly rely on static appearance and scene context rather than motion dynamics, a bias inherited from their training data and objectives. Conversely, traditional motion-centric inputs like optical flow lack the semantic grounding needed to understand high-level motion. To demonstrate this inherent bias, we introduce the SimMotion benchmarks, combining controlled synthetic data with a new human-annotated real-world dataset. We show that existing models perform poorly on these benchmarks, often failing to disentangle motion from appearance. To address this gap, we propose SemanticMoments, a simple, training-free method that computes temporal statistics (specifically, higher-order moments) over features from pre-trained semantic models. Across our benchmarks, SemanticMoments consistently outperforms existing RGB, flow, and text-supervised methods. This demonstrates that temporal statistics in a semantic feature space provide a scalable and perceptually grounded foundation for motion-centric video understanding.

</details>


### [3] [A Hybrid Deterministic Framework for Named Entity Extraction in Broadcast News Video](https://arxiv.org/abs/2602.09154)
*Andrea Filiberto Lucas,Dylan Seychell*

Main category: cs.CV

TL;DR: 本文提出了一种用于自动检测和提取广播和社交媒体原生新闻视频中个人姓名的全面框架。该框架通过一个结构化且平衡的标注帧库来处理多样化的新闻图形，并提出了一种可解释的模块化提取管道，不仅可以保证操作上的稳健性，还能提供全程追踪。实验显示，虽然生成系统在F1分数上略高，但在透明度和数据追溯性方面不具有竞争优势。


<details>
  <summary>Details</summary>
Motivation: 随着基于视频的内容在新闻媒体中的增加，需要一种透明且可靠的自动方法来提取屏幕上显示的信息。手动索引由于图形布局、字体约定和平台特定的设计模式的差异而变得不可行。

Method: 该研究提出了一种框架，采用结构化的标注帧库来处理多样化的新闻图形，同时引入了一种可解释的模块化提取管道，该管道能够在确定性和可审计的条件下运行。此外，研究还对比了生成型多模态方法，进行性能评估。

Result: 该框架在图形元素定位上表现出稳健性，基础检测器的mAP达到95.8%。尽管生成系统在F1分数上稍高，但生成系统缺乏透明的数据血统，在新闻和分析环境中没有优势。该框架的精准度和召回率分别为79.9%和74.4%。用户反馈显示，59%的受访者在快速广播中阅读屏幕名字有困难。

Conclusion: 该研究建立了一个关于现代新闻媒体混合多模态信息提取的方法学严谨且可解释的基础方法。

Abstract: The growing volume of video-based news content has heightened the need for transparent and reliable methods to extract on-screen information. Yet the variability of graphical layouts, typographic conventions, and platform-specific design patterns renders manual indexing impractical. This work presents a comprehensive framework for automatically detecting and extracting personal names from broadcast and social-media-native news videos. It introduces a curated and balanced corpus of annotated frames capturing the diversity of contemporary news graphics and proposes an interpretable, modular extraction pipeline designed to operate under deterministic and auditable conditions.
  The pipeline is evaluated against a contrasting class of generative multimodal methods, revealing a clear trade-off between deterministic auditability and stochastic inference. The underlying detector achieves 95.8% mAP@0.5, demonstrating operationally robust performance for graphical element localisation. While generative systems achieve marginally higher raw accuracy (F1: 84.18% vs 77.08%), they lack the transparent data lineage required for journalistic and analytical contexts. The proposed pipeline delivers balanced precision (79.9%) and recall (74.4%), avoids hallucination, and provides full traceability across each processing stage. Complementary user findings indicate that 59% of respondents report difficulty reading on-screen names in fast-paced broadcasts, underscoring the practical relevance of the task. The results establish a methodologically rigorous and interpretable baseline for hybrid multimodal information extraction in modern news media.

</details>


### [4] [All-in-One Conditioning for Text-to-Image Synthesis](https://arxiv.org/abs/2602.09165)
*Hirunima Jayasekara,Chuong Huynh,Yixuan Ren,Christabel Acquaye,Abhinav Shrivastava*

Main category: cs.CV

TL;DR: 本文提出了一种基于场景图的零样本方法，通过引入一种轻量级的语言模型生成视觉条件，并在推理时进行优化，使得图像合成能在保持文本-图像对齐的同时，提供轻量、连贯以及多样化的图像生成。


<details>
  <summary>Details</summary>
Motivation: 当前的文本到图像生成模型在处理复杂文本输入时难以保持语义准确性和结构连贯性，而传统的图布局方法则限制了构成的灵活性和多样性。

Method: 本文提出了一个基于场景图的零样本条件机制，该机制通过轻量级的语言模型生成视觉条件，并通过推理阶段的优化指导基于扩散的生成过程。

Result: 该方法使得模型在保持文本-图像一致性的同时，能够生成轻量、连贯且多样的图像。

Conclusion: This novel method significantly improves the compositional abilities of text-to-image synthesis, offering a promising direction for future work in this field.

Abstract: Accurate interpretation and visual representation of complex prompts involving multiple objects, attributes, and spatial relationships is a critical challenge in text-to-image synthesis. Despite recent advancements in generating photorealistic outputs, current models often struggle with maintaining semantic fidelity and structural coherence when processing intricate textual inputs. We propose a novel approach that grounds text-to-image synthesis within the framework of scene graph structures, aiming to enhance the compositional abilities of existing models. Eventhough, prior approaches have attempted to address this by using pre-defined layout maps derived from prompts, such rigid constraints often limit compositional flexibility and diversity. In contrast, we introduce a zero-shot, scene graph-based conditioning mechanism that generates soft visual guidance during inference. At the core of our method is the Attribute-Size-Quantity-Location (ASQL) Conditioner, which produces visual conditions via a lightweight language model and guides diffusion-based generation through inference-time optimization. This enables the model to maintain text-image alignment while supporting lightweight, coherent, and diverse image synthesis.

</details>


### [5] [Wearable environmental sensing to forecast how legged systems will interact with upcoming terrain](https://arxiv.org/abs/2602.09209)
*Michael D. Murray,James Tung,Richard W. Nuckols*

Main category: cs.CV

TL;DR: 该研究使用基于CNN-RNN的方法预测在步态过程中足中心压力点（COP）和脚触地时间（TOI），在触地前250毫秒内实现了较好的预测精度，证明了这种方法在轻量化辅助系统中的潜力。


<details>
  <summary>Details</summary>
Motivation: 当前计算机视觉技术在步态分析和辅助系统控制中应用广泛，但对预测脚部在不同环境下的接触点研究较少。本文旨在评估预测足部在平地到楼梯过渡时的前-后方向中心压力点和脚触地时间的可行性。

Method: 研究使用了穿着于右小腿的RGB-D相机和有传感器的鞋垫，以及步态上升到楼梯的任务。通过训练一种基于CNN-RNN的模型，在步足接触前250毫秒内连续预测中心压力点和脚触地时间。

Result: 在150、100和50毫秒的预测窗口内，中心压力点的平均绝对误差分别是29.42毫米、26.82毫米和23.72毫米，脚触地时间的平均绝对误差分别是21.14毫秒、20.08毫秒和17.73毫秒。结果显示，不同时相的脚部动作对预测准确性有影响，而身体运动速度对两种任务的影响不大。此外，模型可以实现在轻薄笔记本或边缘计算设备上以每秒60帧的速度运行。

Conclusion: 该研究证明了使用计算机视觉技术预测步态过程中足部与环境接触的参数是可行的，对于辅助系统的前瞻控制具有重要意义。

Abstract: Computer-vision (CV) has been used for environmental classification during gait and is often used to inform control in assistive systems; however, the ability to predict how the foot will contact a changing environment is underexplored. We evaluated the feasibility of forecasting the anterior-posterior (AP) foot center-of-pressure (COP) and time-of-impact (TOI) prior to foot-strike on a level-ground to stair-ascent transition. Eight subjects wore an RGB-D camera on their right shank and instrumented insoles while performing the task of stepping onto the stairs. We trained a CNN-RNN to forecast the COP and TOI continuously within a 250ms window prior to foot-strike, termed the forecast horizon (FH). The COP mean-absolute-error (MAE) at 150, 100, and 50ms FH was 29.42mm, 26.82, and 23.72mm respectively. The TOI MAE was 21.14, 20.08, and 17.73ms for 150, 100, and 50ms respectively. While torso velocity had no effect on the error in either task, faster toe-swing speeds prior to foot-strike were found to improve the prediction accuracy in the COP case, however, was insignificant in the TOI case. Further, more anterior foot-strikes were found to reduce COP prediction accuracy but did not affect the TOI prediction accuracy. We also found that our lightweight model was capable at running at 60 FPS on either a consumer grade laptop or an edge computing device. This study demonstrates that forecasting COP and TOI from visual data was feasible using a lightweight model, which may have important implications for anticipatory control in assistive systems.

</details>


### [6] [VLM-UQBench: A Benchmark for Modality-Specific and Cross-Modality Uncertainties in Vision Language Models](https://arxiv.org/abs/2602.09214)
*Chenyu Wang,Tianle Chen,H. M. Sabbir Ahmad,Kayhan Batmanghelich,Wenchao Li*

Main category: cs.CV

TL;DR: 该研究提出了VLM-UQBench基准，旨在量化视觉-语言模型(VLMs)在不同模态下的不确定性，并采用简化的度量标准评估多种不确定性量化方法的表现。


<details>
  <summary>Details</summary>
Motivation: 当前的不确定性量化方法在模态特异性方面存在显著限制，且在检测细微的实例级不确定性方面表现不佳，这限制了VLMs的可靠部署。

Method: 该研究设计了VLM-UQBench，包括真实世界的600个样本，以及视觉、文本和跨模态的8种、5种和3种扰动。此外，还提出了两个简化度量标准来量化不确定性得分对这些扰动的敏感性和它们与幻觉之间的相关性，从而评估不同不确定性量化方法。

Result: 研究发现现有的不确定性量化方法具有模态特异性的专门化，并且在模型底层方面有很大的依赖性。模态特异性不确定性经常伴随着幻觉发生，而当前的不确定性评分仅提供了弱而不确定的风险信号。尽管不确定性量化方法在明而明显的组级模糊性推理上可与基于推理的链式思维基准相媲美，但在研究引入的扰动管道中引入的细粒度、实例级模糊性检测方面仍然表现不足。

Conclusion: 研究结果表明，当前的不确定性量化实践与可靠VLM部署所需的精细程度和模态敏感性存在显著差距。

Abstract: Uncertainty quantification (UQ) is vital for ensuring that vision-language models (VLMs) behave safely and reliably. A central challenge is to localize uncertainty to its source, determining whether it arises from the image, the text, or misalignment between the two. We introduce VLM-UQBench, a benchmark for modality-specific and cross-modal data uncertainty in VLMs, It consists of 600 real-world samples drawn from the VizWiz dataset, curated into clean, image-, text-, and cross-modal uncertainty subsets, and a scalable perturbation pipeline with 8 visual, 5 textual, and 3 cross-modal perturbations. We further propose two simple metrics that quantify the sensitivity of UQ scores to these perturbations and their correlation with hallucinations, and use them to evaluate a range of UQ methods across four VLMs and three datasets. Empirically, we find that: (i) existing UQ methods exhibit strong modality-specific specialization and substantial dependence on the underlying VLM, (ii) modality-specific uncertainty frequently co-occurs with hallucinations while current UQ scores provide only weak and inconsistent risk signals, and (iii) although UQ methods can rival reasoning-based chain-of-thought baselines on overt, group-level ambiguity, they largely fail to detect the subtle, instance-level ambiguity introduced by our perturbation pipeline. These results highlight a significant gap between current UQ practices and the fine-grained, modality-aware uncertainty required for reliable VLM deployment.

</details>


### [7] [VLM-Guided Iterative Refinement for Surgical Image Segmentation with Foundation Models](https://arxiv.org/abs/2602.09252)
*Ange Lou,Yamin Li,Qi Chang,Nan Xi,Luyuan Xie,Zichao Li,Tianyu Luan*

Main category: cs.CV

TL;DR: IR-SIS是一种迭代精修系统，能够接受自然语言描述进行手术图像分割，通过自适应选择精修策略并支持临床人员的自然语言反馈，实现了先进的性能，并在多模态数据集上证明了有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的图像分割方法在手术机器人辅助手术和术中指导中存在局限性，如预定义分类、单一预测、缺乏临床互动机制等。

Method: IR-SIS利用一个预训练了的语言适应SAM3进行初始分割，通过视觉-语言模型检测工具并评估分割质量，采用自适应工作流程选择精修策略，并支持临床人员循环交互。

Result: 该系统在多粒度语言注释的数据集上表现优异，并在领域内和跨领域的数据上显示出超前的性能，临床人员的参与也能带来额外的提升。

Conclusion: IR-SIS是第一个具备自适应自我精修能力的语言驱动的手术分割框架。

Abstract: Surgical image segmentation is essential for robot-assisted surgery and intraoperative guidance. However, existing methods are constrained to predefined categories, produce one-shot predictions without adaptive refinement, and lack mechanisms for clinician interaction. We propose IR-SIS, an iterative refinement system for surgical image segmentation that accepts natural language descriptions. IR-SIS leverages a fine-tuned SAM3 for initial segmentation, employs a Vision-Language Model to detect instruments and assess segmentation quality, and applies an agentic workflow that adaptively selects refinement strategies. The system supports clinician-in-the-loop interaction through natural language feedback. We also construct a multi-granularity language-annotated dataset from EndoVis2017 and EndoVis2018 benchmarks. Experiments demonstrate state-of-the-art performance on both in-domain and out-of-distribution data, with clinician interaction providing additional improvements. Our work establishes the first language-based surgical segmentation framework with adaptive self-refinement capabilities.

</details>


### [8] [Rethinking Global Text Conditioning in Diffusion Transformers](https://arxiv.org/abs/2602.09268)
*Nikita Starodubcev,Daniil Pakhomov,Zongze Wu,Ilya Drobyshevskiy,Yuchen Liu,Zhonghao Wang,Yuqian Zhou,Zhe Lin,Dmitry Baranchuk*

Main category: cs.CV

TL;DR: 本研究探讨了调制机制在扩散变换器中的必要性及其性能优势，发现常规使用下混合调制策略对整体性能影响较小，而从新视角使用嵌入池化可以作为指导，实现可控的属性转变，且该方法适用于多种扩散模型。


<details>
  <summary>Details</summary>
Motivation: 探究调制机制在扩散变换器中的必要性及其潜在优势，旨在简化扩散模型中的调制策略，减少训练成本，同时增强模型的可控性和广泛应用性。

Method: 通过对比研究，评估了不同调制策略和单纯使用注意力机制的效果差异。首先，构建了常规和基于新视角的嵌入池化调制策略；其次，将策略应用于多种扩散模型，并测试其在不同任务中的表现。

Result: 研究发现，常规使用调制机制对整体性能影响不大，而从新视角使用嵌入池化能显著增强扩散模型的可控性，并带来任务表现的提升，涵盖文本到图像/视频生成和图像编辑等多种任务。

Conclusion: 研究结论指出，调制机制并非扩散变换器中不可或缺的组成部分，但在特定情况下，从新视角使用嵌入池化可以显著提升模型性能，同时该方法简化了扩散模型设计，减少了训练需求。”}<tool_call>
淙juventry functions are not needed for this task, the analysis is provided directly in the given format. If you need any adjustments or further analysis, please let me know!<tool_call>
[]{

Abstract: Diffusion transformers typically incorporate textual information via attention layers and a modulation mechanism using a pooled text embedding. Nevertheless, recent approaches discard modulation-based text conditioning and rely exclusively on attention. In this paper, we address whether modulation-based text conditioning is necessary and whether it can provide any performance advantage. Our analysis shows that, in its conventional usage, the pooled embedding contributes little to overall performance, suggesting that attention alone is generally sufficient for faithfully propagating prompt information. However, we reveal that the pooled embedding can provide significant gains when used from a different perspective-serving as guidance and enabling controllable shifts toward more desirable properties. This approach is training-free, simple to implement, incurs negligible runtime overhead, and can be applied to various diffusion models, bringing improvements across diverse tasks, including text-to-image/video generation and image editing.

</details>


### [9] [X-Mark: Saliency-Guided Robust Dataset Ownership Verification for Medical Imaging](https://arxiv.org/abs/2602.09284)
*Pranav Kulkarni,Junfeng Guo,Heng Huang*

Main category: cs.CV

TL;DR: 本文提出了一种名为X-Mark的样本特定干净标签水印方法，用于胸部X光片的版权保护，有效地实现了保诊断质量的同时增强了水印的鲁棒性和可检测性。


<details>
  <summary>Details</summary>
Motivation: 医学影像数据对于训练深度学习模型至关重要，但未经授权使用这些数据会引发严重的版权和伦理问题。现有的针对自然图像的知识产权验证方法在此类数据上的应用效果不佳，因此需要一种新的方法来保护这类数据的版权。

Method: X-Mark使用条件U-Net在每个样本的关键区域生成独特的扰动，并通过多组件训练目标确保水印的有效性和鲁棒性，同时保持诊断质量和视觉区分度。此外还通过引入拉普拉斯正则化来实现水印尺度不变。

Result: 在CheXpert上进行的大量实验验证了X-Mark的有效性，实现了100%的水印检测成功率，并在Ind-M场景下将假阳性率降低了12%。同时，X-Mark也展示了对潜在适应性攻击的抵抗性。

Conclusion: X-Mark方法可以作为一种有效的样本自适应干净标签水印方案，有力地解决了高分辨和动态变化医学影像数据的产权验证问题。

Abstract: High-quality medical imaging datasets are essential for training deep learning models, but their unauthorized use raises serious copyright and ethical concerns. Medical imaging presents a unique challenge for existing dataset ownership verification methods designed for natural images, as static watermark patterns generated in fixed-scale images scale poorly dynamic and high-resolution scans with limited visual diversity and subtle anatomical structures, while preserving diagnostic quality. In this paper, we propose X-Mark, a sample-specific clean-label watermarking method for chest x-ray copyright protection. Specifically, X-Mark uses a conditional U-Net to generate unique perturbations within salient regions of each sample. We design a multi-component training objective to ensure watermark efficacy, robustness against dynamic scaling processes while preserving diagnostic quality and visual-distinguishability. We incorporate Laplacian regularization into our training objective to penalize high-frequency perturbations and achieve watermark scale-invariance. Ownership verification is performed in a black-box setting to detect characteristic behaviors in suspicious models. Extensive experiments on CheXpert verify the effectiveness of X-Mark, achieving WSR of 100% and reducing probability of false positives in Ind-M scenario by 12%, while demonstrating resistance to potential adaptive attacks.

</details>


### [10] [A Deep Multi-Modal Method for Patient Wound Healing Assessment](https://arxiv.org/abs/2602.09315)
*Subba Reddy Oota,Vijay Rowtula,Shahid Mohammed,Jeffrey Galitz,Minghsun Liu,Manish Gupta*

Main category: cs.CV

TL;DR: 本研究提出了一种深度多模态方法以预测患者住院风险，并通过结合伤口变量和伤口图像来提高预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的治疗方法主要集中在基于伤口类型的愈合轨迹上，本研究旨在开发一种新的模型，以便早期识别可能影响愈合过程的伤口复杂性，并减少医生诊断伤口所需的时间。

Method: 本研究提出了一个基于迁移学习的伤口评估解决方案，能够从伤口图像中预测伤口变量及其愈合轨迹，从而全面考虑多种因素。

Result: 通过这种方法，可以更早地检测到可能影响伤口愈合过程的复杂情况，有助于提升伤口管理的效率。

Conclusion: 本研究对于早期识别与伤口愈合相关的重要信息以及降低医疗成本具有重要意义。

Abstract: Hospitalization of patients is one of the major factors for high wound care costs. Most patients do not acquire a wound which needs immediate hospitalization. However, due to factors such as delay in treatment, patient's non-compliance or existing co-morbid conditions, an injury can deteriorate and ultimately lead to patient hospitalization. In this paper, we propose a deep multi-modal method to predict the patient's risk of hospitalization. Our goal is to predict the risk confidently by collectively using the wound variables and wound images of the patient. Existing works in this domain have mainly focused on healing trajectories based on distinct wound types. We developed a transfer learning-based wound assessment solution, which can predict both wound variables from wound images and their healing trajectories, which is our primary contribution. We argue that the development of a novel model can help in early detection of the complexities in the wound, which might affect the healing process and also reduce the time spent by a clinician to diagnose the wound.

</details>


### [11] [Deep Modeling and Interpretation for Bladder Cancer Classification](https://arxiv.org/abs/2602.09324)
*Ahmad Chaddad,Yihang Wu,Xianrui Chen*

Main category: cs.CV

TL;DR: 文章对膀胱癌分类任务中的深层模型进行了评估，主要包括标准分类、校准分析和可解释性评估，发现ConvNext模型在泛化能力上表现有限，ViT及其变体在解释非正常分布样本方面表现较好。


<details>
  <summary>Details</summary>
Motivation: 针对医学成像中异常区域占很小比例的问题，研究最新深度模型在膀胱癌分类任务中的性能。

Method: 使用13种不同模型（4个CNN和8个基于变换器的模型）进行标准分类，使用校准分析评估模型的校准效果，并利用GradCAM++评估模型的可解释性。

Result: 实验结果表明，ConvNext系列模型在分类膀胱癌图像方面的泛化能力有限（约60%的准确率）。相比之下，ViT系列模型及其变体在分类非正常分布样本方面效果更好。通过测试时增强，模型的可解释性得以改善，但没有一种模型适用于所有情况。

Conclusion: ConvNext系列模型适合内分布样本，ViT系列及其变体适合解释外分布样本，没有一种模型可以适用于所有情况。

Abstract: Deep models based on vision transformer (ViT) and convolutional neural network (CNN) have demonstrated remarkable performance on natural datasets. However, these models may not be similar in medical imaging, where abnormal regions cover only a small portion of the image. This challenge motivates this study to investigate the latest deep models for bladder cancer classification tasks. We propose the following to evaluate these deep models: 1) standard classification using 13 models (four CNNs and eight transormer-based models), 2) calibration analysis to examine if these models are well calibrated for bladder cancer classification, and 3) we use GradCAM++ to evaluate the interpretability of these models for clinical diagnosis. We simulate $\sim 300$ experiments on a publicly multicenter bladder cancer dataset, and the experimental results demonstrate that the ConvNext series indicate limited generalization ability to classify bladder cancer images (e.g., $\sim 60\%$ accuracy). In addition, ViTs show better calibration effects compared to ConvNext and swin transformer series. We also involve test time augmentation to improve the models interpretability. Finally, no model provides a one-size-fits-all solution for a feasible interpretable model. ConvNext series are suitable for in-distribution samples, while ViT and its variants are suitable for interpreting out-of-distribution samples.

</details>


### [12] [Kyrtos: A methodology for automatic deep analysis of graphic charts with curves in technical documents](https://arxiv.org/abs/2602.09337)
*Michail S. Alexiou,Nikolaos G. Bourbakis*

Main category: cs.CV

TL;DR: 论文提出了一种名为Kyrtos的方法，用于自动识别和技术文档图形图像中图表曲线的分析。


<details>
  <summary>Details</summary>
Motivation: 技术文档中包含大量有价值的知识，理解和解析这些文档中的各种模态（如图表、表格、图示、文本等）及其关联对于浅层理解不够，因此需要一个自动化的方法来更准确地识别和解析图表。

Method: Kyrtos方法采用了簇聚方法识别分割线段中定义的曲线中间点，然后解析提取出的曲线段，提取其行为特征，如方向、趋势等。这些关联性表示为带属性的图，保持曲线的结构特点，并将其关系用自然语言文本句子表示出来，丰富文档内容，并使其能够转换为图表内部功能的随机Petri网模式。

Result: 经广泛评估，Kyrtos的方法在处理含有多个函数的图表时，能够较好地识别和分析图表曲线，相似性较高。

Conclusion: Kyrtos提供了一种有效的方法来自动识别和理解技术文档中的图形图像，并能够将曲线的结构转换为随机Petri网以表示内部功能。

Abstract: Deep Understanding of Technical Documents (DUTD) has become a very attractive field with great potential due to large amounts of accumulated documents and the valuable knowledge contained in them. In addition, the holistic understanding of technical documents depends on the accurate analysis of its particular modalities, such as graphics, tables, diagrams, text, etc. and their associations. In this paper, we introduce the Kyrtos methodology for the automatic recognition and analysis of charts with curves in graphics images of technical documents. The recognition processing part adopts a clustering based approach to recognize middle-points that delimit the line-segments that construct the illustrated curves. The analysis processing part parses the extracted line-segments of curves to capture behavioral features such as direction, trend and etc. These associations assist the conversion of recognized segments' relations into attributed graphs, for the preservation of the curves' structural characteristics. The graph relations are also are expressed into natural language (NL) text sentences, enriching the document's text and facilitating their conversion into Stochastic Petri-net (SPN) graphs, which depict the internal functionality represented in the chart image. Extensive evaluation results demonstrate the accuracy of Kyrtos' recognition and analysis methods by measuring the structural similarity between input chart curves and the approximations generated by Kyrtos for charts with multiple functions.

</details>


### [13] [Fully Differentiable Bidirectional Dual-Task Synergistic Learning for Semi-Supervised 3D Medical Image Segmentation](https://arxiv.org/abs/2602.09378)
*Jun Li*

Main category: cs.CV

TL;DR: 本文提出了一种无缝集成监督学习、一致性正则化、伪监督学习和不确定性估计的全可微双向协同学习（DBiSL）框架，旨在充分利用跨任务协作的潜力。


<details>
  <summary>Details</summary>
Motivation: 由于高注释成本和专业临床知识的需求，高质量标注数据的稀缺性仍然是医学图像分析中的主要挑战。现有的半监督学习方法受限于单向交互机制，未能充分发挥在线双向跨任务协作的潜力。

Method: DBiSL框架通过整合监督学习、一致性正则化、伪监督学习和不确定性估计，实现了一种全可微的双向协同学习。

Result: 在两个基准数据集上的实验结果表明，该方法达到最先进的性能。

Conclusion: 本文为基于双任务驱动的半监督学习提供了新的架构基础，同时也为更广泛的计算机视觉应用提供了通用的多任务学习框架。

Abstract: Semi-supervised learning relaxes the need of large pixel-wise labeled datasets for image segmentation by leveraging unlabeled data. The scarcity of high-quality labeled data remains a major challenge in medical image analysis due to the high annotation costs and the need for specialized clinical expertise. Semi-supervised learning has demonstrated significant potential in addressing this bottleneck, with pseudo-labeling and consistency regularization emerging as two predominant paradigms. Dual-task collaborative learning, an emerging consistency-aware paradigm, seeks to derive supplementary supervision by establishing prediction consistency between related tasks. However, current methodologies are limited to unidirectional interaction mechanisms (typically regression-to-segmentation), as segmentation results can only be transformed into regression outputs in an offline manner, thereby failing to fully exploit the potential benefits of online bidirectional cross-task collaboration. Thus, we propose a fully Differentiable Bidirectional Synergistic Learning (DBiSL) framework, which seamlessly integrates and enhances four critical SSL components: supervised learning, consistency regularization, pseudo-supervised learning, and uncertainty estimation. Experiments on two benchmark datasets demonstrate our method's state-of-the-art performance. Beyond technical contributions, this work provides new insights into unified SSL framework design and establishes a new architectural foundation for dual-task-driven SSL, while offering a generic multitask learning framework applicable to broader computer vision applications. The code will be released on github upon acceptance.

</details>


### [14] [Single-Slice-to-3D Reconstruction in Medical Imaging and Natural Objects: A Comparative Benchmark with SAM 3D](https://arxiv.org/abs/2602.09407)
*Yan Luo,Advaith Ravishankar,Serena Liu,Yutong Yang,Mengyu Wang*

Main category: cs.CV

TL;DR: 本文探讨了基础模型在医学图像到3D重建中的表现，并对比了五种模型在多个医学数据集上的效果，发现单片重建在体积重建方面存在困难，多视角方法可能更可靠。


<details>
  <summary>Details</summary>
Motivation: 为了克服3D医学影像理解在诊断和治疗规划中的瓶颈，即成本高昂且等待时间长的问题，作者通过评估多种模型在现有医学数据集上的性能来探讨利用2D图像生成3D模型的方法是否适用于医学数据。

Method: 作者采用了零样本基准测试的方法，评估了5种最先进的图像到3D模型在涵盖解剖和病理结构的6个医学数据集上的表现。

Result: 研究结果表明，在医学数据集中，所有模型的体素重叠度都有限，且存在深度重建失败的现象。与医学3D数据的拓扑相似性显示，SAM3D表现最佳，而其他模型倾向于过度简化重建结果。

Conclusion: 本研究揭示了单片医学重建的局限性，并指出了由于医学数据的二维性质所引起的深度不确定性，强调了多视角方法在可靠医学3D推断中的潜力。

Abstract: A 3D understanding of anatomy is central to diagnosis and treatment planning, yet volumetric imaging remains costly with long wait times. Image-to-3D foundations models can solve this issue by reconstructing 3D data from 2D modalites. Current foundation models are trained on natural image distributions to reconstruct naturalistic objects from a single image by leveraging geometric priors across pixels. However, it is unclear whether these learned geometric priors transfer to medical data. In this study, we present a controlled zero-shot benchmark of single slice medical image-to-3D reconstruction across five state-of-the-art image-to-3D models: SAM3D, Hunyuan3D-2.1, Direct3D, Hi3DGen, and TripoSG. These are evaluated across six medical datasets spanning anatomical and pathological structures and two natrual datasets, using voxel based metrics and point cloud distance metrics. Across medical datasets, voxel based overlap remains moderate for all models, consistent with a depth reconstruction failure mode when inferring volume from a single slice. In contrast, global distance metrics show more separation between methods: SAM3D achieves the strongest overall topological similarity to ground truth medical 3D data, while alternative models are more prone to over-simplication of reconstruction. Our results quantify the limits of single-slice medical reconstruction and highlight depth ambiguity caused by the planar nature of 2D medical data, motivating multi-view image-to-3D reconstruction to enable reliable medical 3D inference.

</details>


### [15] [K-Sort Eval: Efficient Preference Evaluation for Visual Generation via Corrected VLM-as-a-Judge](https://arxiv.org/abs/2602.09411)
*Zhikai Li,Jiatong Li,Xuewen Liu,Wangbo Zhao,Pan Du,Kaicheng Zhou,Qingyi Gu,Yang You,Zhen Dong,Kurt Keutzer*

Main category: cs.CV

TL;DR: K-Sort Eval 提出了一种基于 VLM 的高效且可靠的评估框架，该框架通过后验纠正方法和动态匹配策略，使得评价结果与人类偏好高度一致。


<details>
  <summary>Details</summary>
Motivation: 视觉生成模型的快速发展催生了更高效且与人类价值对齐的评价需求，现有的众包平台虽然能够提供人类偏好评估，但成本高且耗时。鉴于此，本文提出了 K-Sort Eval，利用 VLM 替代人工判断来解决这一问题，并通过后验纠正和动态匹配策略提高评估的效率和可靠性。

Method: K-Sort Eval 方法包括两个主要部分：（1）基于由数千次人类投票组成的高质量数据集，构建模型输出和排名。对于新模型，将采用（K+1）-wise 比较方法；（2）采用后验纠正方法根据 VLM 预测与人类监督一致性自适应调整后验概率；（3）采用动态匹配策略平衡不确定性和多样性以优化每次比较的期望效益。

Result: 通过大量实验验证，K-Sort Eval 所得的评估结果与 K-Sort Arena 中的人类偏好评价高度一致，并仅需少于 90 次模型运行，展示了其高效性和可靠性。

Conclusion: K-Sort Eval 通过结合后验纠正和动态匹配策略，克服了 VLM 内在幻觉和偏见，实现了高效、可信赖的模型评价。

Abstract: The rapid development of visual generative models raises the need for more scalable and human-aligned evaluation methods. While the crowdsourced Arena platforms offer human preference assessments by collecting human votes, they are costly and time-consuming, inherently limiting their scalability. Leveraging vision-language model (VLMs) as substitutes for manual judgments presents a promising solution. However, the inherent hallucinations and biases of VLMs hinder alignment with human preferences, thus compromising evaluation reliability. Additionally, the static evaluation approach lead to low efficiency. In this paper, we propose K-Sort Eval, a reliable and efficient VLM-based evaluation framework that integrates posterior correction and dynamic matching. Specifically, we curate a high-quality dataset from thousands of human votes in K-Sort Arena, with each instance containing the outputs and rankings of K models. When evaluating a new model, it undergoes (K+1)-wise free-for-all comparisons with existing models, and the VLM provide the rankings. To enhance alignment and reliability, we propose a posterior correction method, which adaptively corrects the posterior probability in Bayesian updating based on the consistency between the VLM prediction and human supervision. Moreover, we propose a dynamic matching strategy, which balances uncertainty and diversity to maximize the expected benefit of each comparison, thus ensuring more efficient evaluation. Extensive experiments show that K-Sort Eval delivers evaluation results consistent with K-Sort Arena, typically requiring fewer than 90 model runs, demonstrating both its efficiency and reliability.

</details>


### [16] [Stability and Concentration in Nonlinear Inverse Problems with Block-Structured Parameters: Lipschitz Geometry, Identifiability, and an Application to Gaussian Splatting](https://arxiv.org/abs/2602.09415)
*Joe-Mei Feng,Hsin-Hsiung Kao*

Main category: cs.CV

TL;DR: 该研究提出了一种基于算子理论的框架，用于解决高维非线性逆问题中的稳定性及统计收敛性问题。框架下，作者统一提出了基于块结构参数、局部可识别性和亚高斯噪声的一系列假设，建立了确定性稳定性不等式，全局Lipschitz界以及非渐近收敛估计。


<details>
  <summary>Details</summary>
Motivation: 研究目的是为了分析高维非线性逆问题中的稳定性及统计收敛性，尤其是在现代成像和可微渲染领域，解决这些问题对于提高图像重建质量和稳定性至关重要。

Method: 作者结合块结构参数、局部可识别性和亚高斯噪声等假设，建立了一种统一的算子理论框架。框架下，通过建立确定性稳定性不等式，全局Lipschitz界和非渐近收敛估计，来提高对逆问题还原结果稳定性和可靠性的理解。

Result: 研究结果表明，框架能够使得基于前向算子的高概率参数误差边界独立于具体的重建算法。具体实例表明了Gaussian Splatting渲染操作符满足所提出假设的条件，并且能够给出模型复杂性和图像分辨率之间的固有稳定性和分辨率折衷。

Conclusion: 该研究为高维非线性逆问题提供了一种理论基础，能够理解图像重建中的稳定性问题，并且针对特定应用场景（如Gaussian Splatting渲染）进行了实例分析，强调了模型复杂性与图像分辨率之间的平衡。

Abstract: We develop an operator-theoretic framework for stability and statistical concentration in nonlinear inverse problems with block-structured parameters. Under a unified set of assumptions combining blockwise Lipschitz geometry, local identifiability, and sub-Gaussian noise, we establish deterministic stability inequalities, global Lipschitz bounds for least-squares misfit functionals, and nonasymptotic concentration estimates. These results yield high-probability parameter error bounds that are intrinsic to the forward operator and independent of any specific reconstruction algorithm. As a concrete instantiation, we verify that the Gaussian Splatting rendering operator satisfies the proposed assumptions and derive explicit constants governing its Lipschitz continuity and resolution-dependent observability. This leads to a fundamental stability--resolution tradeoff, showing that estimation error is inherently constrained by the ratio between image resolution and model complexity. Overall, the analysis characterizes operator-level limits for a broad class of high-dimensional nonlinear inverse problems arising in modern imaging and differentiable rendering.

</details>


### [17] [SceneReVis: A Self-Reflective Vision-Grounded Framework for 3D Indoor Scene Synthesis via Multi-turn RL](https://arxiv.org/abs/2602.09432)
*Yang Zhao,Shizhao Sun,Meisheng Zhang,Yingdong Shi,Xubo Yang,Jiang Bian*

Main category: cs.CV

TL;DR: SceneReVis 提出了一种基于视觉自我反思框架，通过迭代的诊断和行动循环解决多模态反馈中的空间冲突，构建了一个大规模的因果构建轨迹数据集 SceneChain-12k，并采用从监督微调到自主强化学习的两阶段训练方法，实现了高质量生成和目标导向优化的最新性能。


<details>
  <summary>Details</summary>
Motivation: 现有的单次通过3D场景合成方法容易产生空间幻觉，如碰撞，主要是因为缺乏细致的推理。SceneReVis 的目的是通过引入一种基于视觉的自我反思模块来解决这个问题，从而系统地诊断并纠正空间冲突，实现更高质量的3D场景合成。

Method: SceneReVis 采用了迭代的“诊断-行动”循环机制，利用多模态反馈进行空间冲突的显式拦截和解决。通过构建大规模的因果构建轨迹数据集 SceneChain-12k 和采用两阶段训练方法（从监督微调到自主强化学习），逐步提高模型的空间规划能力。

Result: 详细的实验表明，SceneReVis 在高保真生成和目标导向优化方面取得了最新性能，在多样化的长尾领域中表现出强大的泛化能力。

Conclusion: SceneReVis 通过基于视觉的自我反思框架和新型训练方法，在3D场景合成领域取得了重要进展，为未来的相关研究提供了新的思路和技术基础。

Abstract: Current one-pass 3D scene synthesis methods often suffer from spatial hallucinations, such as collisions, due to a lack of deliberative reasoning. To bridge this gap, we introduce SceneReVis, a vision-grounded self-reflection framework that employs an iterative ``diagnose-and-act'' loop to explicitly intercept and resolve spatial conflicts using multi-modal feedback. To support this step-wise paradigm, we construct SceneChain-12k, a large-scale dataset of causal construction trajectories derived through a novel reverse engineering pipeline. We further propose a two-stage training recipe that transitions from Supervised Fine-Tuning to Agentic Reinforcement Learning, evolving the model into an active spatial planner. Extensive experiments demonstrate that SceneReVis achieves state-of-the-art performance in high-fidelity generation and goal-oriented optimization, with robust generalization to long-tail domains.

</details>


### [18] [Fine-T2I: An Open, Large-Scale, and Diverse Dataset for High-Quality T2I Fine-Tuning](https://arxiv.org/abs/2602.09439)
*Xu Ma,Yitian Zhang,Qihua Dong,Yun Fu*

Main category: cs.CV

TL;DR: Fine-T2I 是一个大规模、高质量且完全开源的文本到图像数据集，包含多种任务组合、提示类别、视觉风格和提示模板，涵盖了预训练模型在指令遵循和生成质量方面的进一步提升。


<details>
  <summary>Details</summary>
Motivation: 当前大多数公开的细调数据集存在分辨率低、文本与图像对齐差或多样性和创意性不足的问题，这导致了公开研究模型和企业级别的模型之间存在明显的性能差距。

Method: Fine-T2I 通过将强大的现代模型生成的合成图像与专业摄影师精心挑选的真实图像结合，并进行严格筛选，确保在文本与图像对齐、视觉保真度和提示质量方面达到高标准。

Result: Fine-T2I 提供了超过600万的文字图像配对，其大小接近预训练数据集，但质量接近于细调级别的水平，极大地促进了图像生成模型的指令遵循和产出质量。

Conclusion: 最终，发布 Fine-T2I 允许开放社区更好地解决文本到图像模型的训练数据缺口，促进相关领域的技术进步。

Abstract: High-quality and open datasets remain a major bottleneck for text-to-image (T2I) fine-tuning. Despite rapid progress in model architectures and training pipelines, most publicly available fine-tuning datasets suffer from low resolution, poor text-image alignment, or limited diversity, resulting in a clear performance gap between open research models and enterprise-grade models. In this work, we present Fine-T2I, a large-scale, high-quality, and fully open dataset for T2I fine-tuning. Fine-T2I spans 10 task combinations, 32 prompt categories, 11 visual styles, and 5 prompt templates, and combines synthetic images generated by strong modern models with carefully curated real images from professional photographers. All samples are rigorously filtered for text-image alignment, visual fidelity, and prompt quality, with over 95% of initial candidates removed. The final dataset contains over 6 million text-image pairs, around 2 TB on disk, approaching the scale of pretraining datasets while maintaining fine-tuning-level quality. Across a diverse set of pretrained diffusion and autoregressive models, fine-tuning on Fine-T2I consistently improves both generation quality and instruction adherence, as validated by human evaluation, visual comparison, and automatic metrics. We release Fine-T2I under an open license to help close the data gap in T2I fine-tuning in the open community.

</details>


### [19] [Look-Ahead and Look-Back Flows: Training-Free Image Generation with Trajectory Smoothing](https://arxiv.org/abs/2602.09449)
*Yan Luo,Henry Huang,Todd Y. Zhou,Mengyu Wang*

Main category: cs.CV

TL;DR: 本文提出了一种基于未来和过去速度及潜在轨迹信息的训练-free 潜空间轨迹调整方法，通过在潜在空间直接精细生成路径，从而改进图像生成过程。提出的两种训练-free 轨迹平滑模型 Look-Ahead 和 Look-Back 在多个数据集上优于最先进的模型。


<details>
  <summary>Details</summary>
Motivation: 为了克服流匹配方法通过调整速度场引入的误差累积问题，本文提出基于未来和过去的潜在轨迹信息的训练-free 调整方法，以减少生成路径中的误差累积，从而提高图像生成质量。

Method: 本文首先介绍了流匹配框架下噪声到数据生成过程的原理。然后，提出了一种基于未来速度和潜在轨迹的训练-free 轨迹调整方法（Look-Ahead），以及基于过去信息的平滑方法（Look-Back）。通过组合不同速度场和潜在轨迹信息，进一步优化生成路径。
具体技术包括使用曲率门控权重进行当前和下一帧潜在特征的混合，以及采用指数移动平均进行过去的轨迹平滑。

Result: 通过广泛的实验和全面的评估指标，本文提出的两种训练-free 轨迹平滑模型（Look-Ahead 和 Look-Back）在多个数据集（如 COCO17，CUB-200 和 Flickr30K）上显著优于现有的先进模型。

Conclusion: 本研究提出的方法在减少生成路径中误差累积方面取得了显著成效，未来可以进一步探索如何将这些方法与其他生成模型相结合，以实现更好的图像生成效果。

Abstract: Recent advances have reformulated diffusion models as deterministic ordinary differential equations (ODEs) through the framework of flow matching, providing a unified formulation for the noise-to-data generative process. Various training-free flow matching approaches have been developed to improve image generation through flow velocity field adjustment, eliminating the need for costly retraining. However, Modifying the velocity field $v$ introduces errors that propagate through the full generation path, whereas adjustments to the latent trajectory $z$ are naturally corrected by the pretrained velocity network, reducing error accumulation. In this paper, we propose two complementary training-free latent-trajectory adjustment approaches based on future and past velocity $v$ and latent trajectory $z$ information that refine the generative path directly in latent space. We propose two training-free trajectory smoothing schemes: \emph{Look-Ahead}, which averages the current and next-step latents using a curvature-gated weight, and \emph{Look-Back}, which smoothes latents using an exponential moving average with decay. We demonstrate through extensive experiments and comprehensive evaluation metrics that the proposed training-free trajectory smoothing models substantially outperform various state-of-the-art models across multiple datasets including COCO17, CUB-200, and Flickr30K.

</details>


### [20] [ArtifactLens: Hundreds of Labels Are Enough for Artifact Detection with VLMs](https://arxiv.org/abs/2602.09475)
*James Burgess,Rameen Abdal,Dan Stoddart,Sergey Tulyakov,Serena Yeung-Levy,Kuan-Chieh Jackson Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为ArtifactLens的新系统，该系统使用预训练的VLMs和少量标注数据就能检测图像生成器产生的伪影，无需频繁大规模 fine-tuning，展示了对多种伪影类型的检测能力和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于大规模标注数据来训练图像检测器，而对于新出现的图像生成器和伪影类型，这种方法难以快速适应。因此，需要寻找一种不需要大量标注数据的方法，以提高检测效率和适应性。

Method: 本研究采用了一个多组件的架构，结合了上下文学习和文本指令优化等技术，来引导预训练的视觉语言模型（VLMs）检测图像伪影。通过少量标注数据训练，实现了对多种伪影类型的检测。

Result: 提出的ArtifactLens系统在五个由不同数据集组成的伪影人工检测基准测试中达到了最先进的性能，比传统方法需要的标记数据量大幅减少。

Conclusion: 该研究展示了如何利用预训练的视觉语言模型和少量标注数据快速和有效地检测图像生成器中的伪影，为未来的研究和应用提供了新的思路。

Abstract: Modern image generators produce strikingly realistic images, where only artifacts like distorted hands or warped objects reveal their synthetic origin. Detecting these artifacts is essential: without detection, we cannot benchmark generators or train reward models to improve them. Current detectors fine-tune VLMs on tens of thousands of labeled images, but this is expensive to repeat whenever generators evolve or new artifact types emerge. We show that pretrained VLMs already encode the knowledge needed to detect artifacts - with the right scaffolding, this capability can be unlocked using only a few hundred labeled examples per artifact category. Our system, ArtifactLens, achieves state-of-the-art on five human artifact benchmarks (the first evaluation across multiple datasets) while requiring orders of magnitude less labeled data. The scaffolding consists of a multi-component architecture with in-context learning and text instruction optimization, with novel improvements to each. Our methods generalize to other artifact types - object morphology, animal anatomy, and entity interactions - and to the distinct task of AIGC detection.

</details>


### [21] [FD-DB: Frequency-Decoupled Dual-Branch Network for Unpaired Synthetic-to-Real Domain Translation](https://arxiv.org/abs/2602.09476)
*Chuanhai Zang,Jiabao Hu,XW Song*

Main category: cs.CV

TL;DR: FD-DB 提出了一种新颖的双分支模型，通过低频可解释编辑和高频残差补偿来减少合成数据与真实数据之间的域差异，从而提升图像分割任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在合成数据与真实数据之间存在显著的域差异，FD-DB 被设计用来解决这一问题，通过对合成数据进行低频合理编辑和高频细节补偿，提高实时域的一致性，从而提升下游任务的表现。

Method: FD-DB 采用频率解耦的双分支架构，其中可解释分支用于预测具有物理意义的编辑参数（如白平衡、曝光、对比度、饱和度、模糊和颗粒），以构建稳定的基础低频外观；残差分支通过残差生成补充详细信息；采用门控融合机制，在明确的频率约束下结合两个分支，以限制低频漂移。还采用两阶段训练计划，首先稳定编辑分支，然后释放残差分支，以提高优化稳定性。

Result: 实验表明，FD-DB 在 YCB-V 数据集上提高了真实域外观的一致性，并显著提升了下游语义分割任务的性能，同时保持几何和语义结构。

Conclusion: FD-DB 开创性地提出了频率解耦的双分支模型，通过合理编辑和残差补偿的方法有效解决了合成数据与真实数据之间的域差异问题，为合成数据和真实数据之间的迁移应用提供了新的解决方案。

Abstract: Synthetic data provide low-cost, accurately annotated samples for geometry-sensitive vision tasks, but appearance and imaging differences between synthetic and real domains cause severe domain shift and degrade downstream performance. Unpaired synthetic-to-real translation can reduce this gap without paired supervision, yet existing methods often face a trade-off between photorealism and structural stability: unconstrained generation may introduce deformation or spurious textures, while overly rigid constraints limit adaptation to real-domain statistics. We propose FD-DB, a frequency-decoupled dual-branch model that separates appearance transfer into low-frequency interpretable editing and high-frequency residual compensation. The interpretable branch predicts physically meaningful editing parameters (white balance, exposure, contrast, saturation, blur, and grain) to build a stable low-frequency appearance base with strong content preservation. The free branch complements fine details through residual generation, and a gated fusion mechanism combines the two branches under explicit frequency constraints to limit low-frequency drift. We further adopt a two-stage training schedule that first stabilizes the editing branch and then releases the residual branch to improve optimization stability. Experiments on the YCB-V dataset show that FD-DB improves real-domain appearance consistency and significantly boosts downstream semantic segmentation performance while preserving geometric and semantic structures.

</details>


### [22] [Weakly Supervised Contrastive Learning for Histopathology Patch Embeddings](https://arxiv.org/abs/2602.09477)
*Bodong Zhang,Xiwen Li,Hamid Manoochehri,Xiaoya Tang,Deepika Sirohi,Beatrice S. Knudsen,Tolga Tasdizen*

Main category: cs.CV

TL;DR: 本文提出了一种新的特征表示学习框架WeakSupCon，该框架整合了袋级标签信息，并在三种数据集的弱监督多重实例学习任务中表现优于自监督对比学习方法。


<details>
  <summary>Details</summary>
Motivation: 针对数字病理切片图像分析中缺乏标注数据的问题，本文致力于解决如何利用有限的标注信息来提升特征表示的学习效果。

Method: 提出了一种名为WeakSupCon的新框架，通过在训练过程中引入袋级标签，能够在无需实例级伪标签的情况下将不同类别的补丁分割开来。

Result: 实验结果显示，使用WeakSupCon生成的图像特征相比自监督对比学习方法，在三种数据集上的弱监督多重实例学习任务中表现更优。

Conclusion: 本文提出的WeakSupCon框架能够在无需耗费大量时间进行实例级标注的情况下，提升图像特征的学习质量，为数字病理图像分析提供了一种新的解决方案。

Abstract: Digital histopathology whole slide images (WSIs) provide gigapixel-scale high-resolution images that are highly useful for disease diagnosis. However, digital histopathology image analysis faces significant challenges due to the limited training labels, since manually annotating specific regions or small patches cropped from large WSIs requires substantial time and effort. Weakly supervised multiple instance learning (MIL) offers a practical and efficient solution by requiring only bag-level (slide-level) labels, while each bag typically contains multiple instances (patches). Most MIL methods directly use frozen image patch features generated by various image encoders as inputs and primarily focus on feature aggregation. However, feature representation learning for encoder pretraining in MIL settings has largely been neglected.
  In our work, we propose a novel feature representation learning framework called weakly supervised contrastive learning (WeakSupCon) that incorporates bag-level label information during training. Our method does not rely on instance-level pseudo-labeling, yet it effectively separates patches with different labels in the feature space. Experimental results demonstrate that the image features generated by our WeakSupCon method lead to improved downstream MIL performance compared to self-supervised contrastive learning approaches in three datasets. Our related code is available at github.com/BzhangURU/Paper_WeakSupCon_for_MIL

</details>


### [23] [Beyond Next-Token Alignment: Distilling Multimodal Large Language Models via Token Interactions](https://arxiv.org/abs/2602.09483)
*Lin Chen,Xiaoke Zhao,Kun Ding,Weiwei Feng,Changtao Miao,Zili Wang,Wenxuan Guo,Ying Wang,Kaiyuan Zheng,Bo Zhang,Zhe Li,Shiming Xiang*

Main category: cs.CV

TL;DR: 本文提出了一种新的知识蒸馏框架Align-TI，该框架强调标记交互，通过模仿教师模型的视觉信息提取能力和捕捉生成逻辑，实现了在多模态大语言模型中更有效的知识蒸馏，从而提高了模型效率，实验表明该方法在多个模型上优于传统的知识蒸馏方法。


<details>
  <summary>Details</summary>
Motivation: 传统的知识蒸馏方法主要依赖静态的下一个标记对齐，忽略了标记间的动态交互，这对多模态理解与生成至关重要。

Method: Align-TI框架引入了两个组件：IVA和TPA。IVA使学生模型能够通过与显著的视觉区域对齐来模仿教师模型的指令相关的视觉信息提取能力。TPA通过对序列中的标记间过渡概率进行对齐，捕捉教师模型的动态生成逻辑。

Result: 实验结果显示，Align-TI在多个模型上优于传统的知识蒸馏方法，提高了模型的效率。相比Vanilla KD，Align-TI实现了2.6%的相对改进，缩小了与更大规模模型的差距。

Conclusion: Align-TI代表了一种新的多模态大语言模型知识蒸馏方法，通过考虑标记交互，显著提高了模型训练的参数效率，并在性能上超越了基准模型。

Abstract: Multimodal Large Language Models (MLLMs) demonstrate impressive cross-modal capabilities, yet their substantial size poses significant deployment challenges. Knowledge distillation (KD) is a promising solution for compressing these models, but existing methods primarily rely on static next-token alignment, neglecting the dynamic token interactions, which embed essential capabilities for multimodal understanding and generation. To this end, we introduce Align-TI, a novel KD framework designed from the perspective of Token Interactions. Our approach is motivated by the insight that MLLMs rely on two primary interactions: vision-instruction token interactions to extract relevant visual information, and intra-response token interactions for coherent generation. Accordingly, Align-TI introduces two components: IVA enables the student model to imitate the teacher's instruction-relevant visual information extract capability by aligning on salient visual regions. TPA captures the teacher's dynamic generative logic by aligning the sequential token-to-token transition probabilities. Extensive experiments demonstrate Align-TI's superiority. Notably, our approach achieves $2.6\%$ relative improvement over Vanilla KD, and our distilled Align-TI-2B even outperforms LLaVA-1.5-7B (a much larger MLLM) by $7.0\%$, establishing a new state-of-the-art distillation framework for training parameter-efficient MLLMs. Code is available at https://github.com/lchen1019/Align-TI.

</details>


### [24] [Equilibrium contrastive learning for imbalanced image classification](https://arxiv.org/abs/2602.09506)
*Sumin Roh,Harim Kim,Ho Yun Lee,Il Yong Chun*

Main category: cs.CV

TL;DR: 本文提出了一种名为ECL的监督对比学习框架，旨在解决现有方法在不平衡数据集上的两个限制：代表性几何均衡和分类器-类中心几何均衡。ECL改进了类均值和分类器之间的对齐，并更公平地处理了类平均特征和类原型的贡献。


<details>
  <summary>Details</summary>
Motivation: 现有的监督对比学习方法在处理不平衡数据集时存在两个主要问题：未考虑类均值/原型与分类器之间的对齐，导致较差的泛化性能；以及将原型作为每个类一个额外样本处理，这在不同批次中的贡献不平衡。这些问题限制了现有方法的有效性，不能充分解决不平衡数据下的性能问题。

Method: ECL框架通过两个主要部分来解决上述问题：促进代表性几何均衡（通过使类特征、类平均值和分类器和谐平衡）以及建立分类器-类中心几何均衡（通过对齐分类器权重和类原型）。此外，ECL还通过两种补丁方法增加了一致性对齐效果。

Result: 在三种长尾数据集（CIFAR-10(0)-LT、ImageNet-LT 及 ISIC 2019 和作者构建的 LCCT 数据集）上进行的实验表明，ECL 比当前最优的监督对比学习方法表现更好。

Conclusion: ECL 提出了一种新的监督对比学习框架，既能改进在不平衡数据集上的泛化性能，又能更公平地处理不同类别的贡献。

Abstract: Contrastive learning (CL) is a predominant technique in image classification, but they showed limited performance with an imbalanced dataset. Recently, several supervised CL methods have been proposed to promote an ideal regular simplex geometric configuration in the representation space-characterized by intra-class feature collapse and uniform inter-class mean spacing, especially for imbalanced datasets. In particular, existing prototype-based methods include class prototypes, as additional samples to consider all classes. However, the existing CL methods suffer from two limitations. First, they do not consider the alignment between the class means/prototypes and classifiers, which could lead to poor generalization. Second, existing prototype-based methods treat prototypes as only one additional sample per class, making their influence depend on the number of class instances in a batch and causing unbalanced contributions across classes. To address these limitations, we propose Equilibrium Contrastive Learning (ECL), a supervised CL framework designed to promote geometric equilibrium, where class features, means, and classifiers are harmoniously balanced under data imbalance. The proposed ECL framework uses two main components. First, ECL promotes the representation geometric equilibrium (i.e., a regular simplex geometry characterized by collapsed class samples and uniformly distributed class means), while balancing the contributions of class-average features and class prototypes. Second, ECL establishes a classifier-class center geometric equilibrium by aligning classifier weights and class prototypes. We ran experiments with three long-tailed datasets, the CIFAR-10(0)-LT, ImageNet-LT, and the two imbalanced medical datasets, the ISIC 2019 and our constructed LCCT dataset. Results show that ECL outperforms existing SOTA supervised CL methods designed for imbalanced classification.

</details>


### [25] [Robust Depth Super-Resolution via Adaptive Diffusion Sampling](https://arxiv.org/abs/2602.09510)
*Kun Wang,Yun Zhu,Pan Zhou,Na Zhao*

Main category: cs.CV

TL;DR: AdaDS是一种深度超分辨率框架，通过利用高斯平滑的收缩性质，能够从严重降级的低分辨率输入中稳健地恢复高分辨率深度图，相较于传统方法其表现更优。


<details>
  <summary>Details</summary>
Motivation: 传统的深度超分辨率方法在处理严重或未知的降级时会表现出异常，AdaDS利用高斯平滑累积噪声的特性，通过自适应选择初始时间步并在逆向扩散路径中注入定制噪声，克服了这一问题。

Method: AdaDS的方法基于逆向扩散过程中的自适应估计不确定性选择起始时间步，并在此基础上注入定制噪声，使中间样本位于目标后验分布的概率高区域。

Result: 在现实和合成的基准测试中，AdaDS展现了出色的零样本泛化能力和对各种降级模式的鲁棒性。

Conclusion: AdaDS框架为深度超分辨率提供了一种新的视角，通过自适应选择和注入噪声，增强了模型对降级输入的恢复能力，优于现有的先进方法。

Abstract: We propose AdaDS, a generalizable framework for depth super-resolution that robustly recovers high-resolution depth maps from arbitrarily degraded low-resolution inputs. Unlike conventional approaches that directly regress depth values and often exhibit artifacts under severe or unknown degradation, AdaDS capitalizes on the contraction property of Gaussian smoothing: as noise accumulates in the forward process, distributional discrepancies between degraded inputs and their pristine high-quality counterparts diminish, ultimately converging to isotropic Gaussian prior. Leveraging this, AdaDS adaptively selects a starting timestep in the reverse diffusion trajectory based on estimated refinement uncertainty, and subsequently injects tailored noise to position the intermediate sample within the high-probability region of the target posterior distribution. This strategy ensures inherent robustness, enabling generative prior of a pre-trained diffusion model to dominate recovery even when upstream estimations are imperfect. Extensive experiments on real-world and synthetic benchmarks demonstrate AdaDS's superior zero-shot generalization and resilience to diverse degradation patterns compared to state-of-the-art methods.

</details>


### [26] [Federated Learning for Surgical Vision in Appendicitis Classification: Results of the FedSurg EndoVis 2024 Challenge](https://arxiv.org/abs/2510.04772)
*Max Kirchner,Hanna Hoffmann,Alexander C. Jenke,Oliver L. Saldanha,Kevin Pfeiffer,Weam Kanjo,Julia Alekseenko,Claas de Boer,Santhi Raj Kolamuri,Lorenzo Mazza,Nicolas Padoy,Sophia Bano,Annika Reinke,Lena Maier-Hein,Danail Stoyanov,Jakob N. Kather,Fiona R. Kolbinger,Sebastian Bodenstedt,Stefanie Speidel*

Main category: cs.CV

TL;DR: FedSurg挑战旨在评估目前联邦学习方法在处理手术视频分类时的泛化能力以及在未见中心环境下的适应能力，通过局部微调的方式实现协作模型开发，而无需共享患者数据。ViViT模型在性能方面表现最佳，但仍然存在泛化能力、类不平衡敏感性和分层调参的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着手术视频分类在临床应用中的重要性日益增加，需要一种新的方法来评估得到的模型是否能够在新的未见中心中有效工作并承受临床中心特定问题下的微调挑战。

Method: 参与者根据附录300多中心视频数据集开发了识别阑尾炎炎症阶段的分类方法，并在两个任务中进行了评估：一是泛化到未见的临床中心，二是通过微调实现中心特异性适应。提交的方法包括具有线性探针的基础模型、基于三重损失的元学习模型和不同的联邦学习聚合方案（FedAvg、FedMedian、FedSAM）。

Result: 在泛化任务中，不同临床中心之间的性能差异显著。但通过微调后，所有队伍的性能都有所提升，尽管排名的稳定性较低。ViViT模型展示了最强的整体性能。此外，泛化能力、类不平衡敏感性及分层调参挑战依然突出。

Conclusion: FedSurg挑战为评估手术视频分类的联邦学习策略提供了首个基准。研究结果突显了局部个人化与全局稳健性的权衡，并强调了架构选择、预处理和损失设计的重要性。这一基准为开发面对不平衡、适应性强且稳健的联邦学习方法提供了参考点。

Abstract: Purpose: The FedSurg challenge was designed to benchmark the state of the art in federated learning for surgical video classification. Its goal was to assess how well current methods generalize to unseen clinical centers and adapt through local fine-tuning while enabling collaborative model development without sharing patient data. Methods: Participants developed strategies to classify inflammation stages in appendicitis using a preliminary version of the multi-center Appendix300 video dataset. The challenge evaluated two tasks: generalization to an unseen center and center-specific adaptation after fine-tuning. Submitted approaches included foundation models with linear probing, metric learning with triplet loss, and various FL aggregation schemes (FedAvg, FedMedian, FedSAM). Performance was assessed using F1-score and Expected Cost, with ranking robustness evaluated via bootstrapping and statistical testing. Results: In the generalization task, performance across centers was limited. In the adaptation task, all teams improved after fine-tuning, though ranking stability was low. The ViViT-based submission achieved the strongest overall performance. The challenge highlighted limitations in generalization, sensitivity to class imbalance, and difficulties in hyperparameter tuning in decentralized training, while spatiotemporal modeling and context-aware preprocessing emerged as promising strategies. Conclusion: The FedSurg Challenge establishes the first benchmark for evaluating FL strategies in surgical video classification. Findings highlight the trade-off between local personalization and global robustness, and underscore the importance of architecture choice, preprocessing, and loss design. This benchmarking offers a reference point for future development of imbalance-aware, adaptive, and robust FL methods in clinical surgical AI.

</details>


### [27] [Attention to details, logits to truth: visual-aware attention and logits enhancement to mitigate hallucinations in LVLMs](https://arxiv.org/abs/2602.09521)
*Jingyi Wang,Fei Li,Rujie Liu*

Main category: cs.CV

TL;DR: 提出了一种无需训练的注意力干预算法，旨在增强任务相关视觉文本的注意力，减少大型视觉-语言模型的幻觉现象，同时保持内容生成的准确性和连贯性。


<details>
  <summary>Details</summary>
Motivation: 现有的大型视觉-语言模型（LVLMs）在视觉注意力方面存在不足，导致产生幻觉。为了缓解这一问题，本文通过减少任务无关的视觉注意力来增强任务相关视觉和文本之间的注意力相似性，从而提出了一种新的注意力干预算法，以减少幻觉现象。

Method: 该算法首先提取视觉-文本交叉注意力子矩阵，表示视觉-文本关系，以此来构建再加权矩阵，重新分配注意力。其次，为了增强视觉令牌的贡献，将视觉注意力值注入到束搜索解码中，识别具有更高视觉注意力的解决方案。

Result: 大量的实验表明，这种方法在主流的大型视觉-语言模型中显著减少了幻觉现象，同时保持了生成内容的准确性和连贯性。

Conclusion: 本文提出的方法对现有的大型视觉-语言模型具有改进作用，特别是在处理视觉幻觉方面表现出色。

Abstract: Existing Large Vision-Language Models (LVLMs) exhibit insufficient visual attention, leading to hallucinations. To alleviate this problem, some previous studies adjust and amplify visual attention. These methods present a limitation that boosting attention for all visual tokens inevitably increases attention to task irrelevant tokens. To tackle this challenge, we propose a training free attentional intervention algorithm to enhance the attention of task-relevant tokens based on the argument that task-relevant tokens generally demonstrate high visual-textual similarities. Specifically, the vision-text cross-attention submatrices, which represent visual-textual correlations, are extracted to construct the reweighting matrices to reallocate attention. Besides, to enhance the contribution of visual tokens, we inject visual attention values into the beam search decoding to identify solutions with higher visual attention. Extensive experiments demonstrate that this method significantly reduces hallucinations across mainstream LVLMs, while preserving the accuracy and coherence of generated content.

</details>


### [28] [Singpath-VL Technical Report](https://arxiv.org/abs/2602.09523)
*Zhen Qiu,Kaiwen Xiao,Zhengwei Lu,Xiangyu Liu,Lei Zhao,Hao Zhang*

Main category: cs.CV

TL;DR: 本文介绍了Singpath-VL，这是一种用于宫颈细胞学领域人工智能助手的视觉-语言大型模型。通过开发一个创新的三阶段流水线合成百万级的图像-描述数据集，进一步微调Qwen3-VL-4B模型，旨在专门针对细胞病理学领域。Singpath-VL在细节形态感知和细胞层面诊断分类方面表现出色。同时，部分合成的数据集将开源。


<details>
  <summary>Details</summary>
Motivation: 解决视觉-语言大型语言模型（MLLMs）在宫颈细胞学领域应用不足的问题，尤其是在缺乏大规模高质量标注数据集的情况下。

Method: 开发了一个创新的三阶段流水线来合成一个百万级的图像-描述数据集，使用多种通用MLLMs作为弱标注器，通过共识融合和专家知识注入来提升输出质量。然后，对Qwen3-VL-4B模型进行多阶段微调，创建了专门针对细胞病理学领域的MLLM。

Result: Singpath-VL展示了在细粒度形态感知和细胞层级诊断分类方面的出色性能。

Conclusion: 未来，部分合成数据集将被开源，以推进该领域的研究。

Abstract: We present Singpath-VL, a vision-language large model, to fill the vacancy of AI assistant in cervical cytology. Recent advances in multi-modal large language models (MLLMs) have significantly propelled the field of computational pathology. However, their application in cytopathology, particularly cervical cytology, remains underexplored, primarily due to the scarcity of large-scale, high-quality annotated datasets. To bridge this gap, we first develop a novel three-stage pipeline to synthesize a million-scale image-description dataset. The pipeline leverages multiple general-purpose MLLMs as weak annotators, refines their outputs through consensus fusion and expert knowledge injection, and produces high-fidelity descriptions of cell morphology. Using this dataset, we then fine-tune the Qwen3-VL-4B model via a multi-stage strategy to create a specialized cytopathology MLLM. The resulting model, named Singpath-VL, demonstrates superior performance in fine-grained morphological perception and cell-level diagnostic classification. To advance the field, we will open-source a portion of the synthetic dataset and benchmark.

</details>


### [29] [SchröMind: Mitigating Hallucinations in Multimodal Large Language Models via Solving the Schrödinger Bridge Problem](https://arxiv.org/abs/2602.09528)
*Ziqiang Shi,Rujie Liu,Shanshan Yu,Satoshi Munakata,Koichi Shirahata*

Main category: cs.CV

TL;DR: 提出了一个名为 SchröMind 的新框架，通过解决薛定谔桥问题减少幻觉，能够在保持模型原始能力的前提下，以轻量级训练实现少量计算资源开销并达到最佳性能。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型在面对视觉输入时容易出现幻觉，即生成的文字与视觉输入不符。这些问题在关键领域的应用，如医疗健康，是个重大障碍。

Method: SchröMind 框架通过解决量子物理学中的薛定谔桥问题，建立幻觉和真实激活之间轻量级的令牌级映射，从而减轻模型的幻觉倾向。

Result: 在 POPE 和 MME 两个基准测试上，SchröMind 实现了最先进的性能，同时仅引入了少量的计算开销。

Conclusion: SchröMind 框架为解决 MLLMs 在面对视觉输入时的幻觉问题提供了一种有效的解决方案。

Abstract: Recent advancements in Multimodal Large Language Models (MLLMs) have achieved significant success across various domains. However, their use in high-stakes fields like healthcare remains limited due to persistent hallucinations, where generated text contradicts or ignores visual input. We contend that MLLMs can comprehend images but struggle to produce accurate token sequences. Minor perturbations can shift attention from truthful to untruthful states, and the autoregressive nature of text generation often prevents error correction. To address this, we propose SchröMind-a novel framework reducing hallucinations via solving the Schrödinger bridge problem. It establishes a token-level mapping between hallucinatory and truthful activations with minimal transport cost through lightweight training, while preserving the model's original capabilities. Extensive experiments on the POPE and MME benchmarks demonstrate the superiority of Schrödinger, which achieves state-of-the-art performance while introducing only minimal computational overhead.

</details>


### [30] [SCA-Net: Spatial-Contextual Aggregation Network for Enhanced Small Building and Road Change Detection](https://arxiv.org/abs/2602.09529)
*Emad Gholibeigi,Abbas Koochari,Azadeh ZamaniFar*

Main category: cs.CV

TL;DR: 该论文提出了名为SCA-Net的增强型检测架构，专为双时相影像中的建筑物和道路变化检测设计。该模型通过引入新型的多尺度差分金字塔块、自适应多尺度处理模块、多级注意力机制以及动态组合损失函数和四阶段训练策略，实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 自动化遥感影像变化检测对于城市管理、环境监测和灾害评估至关重要，然而传统的深度学习模型往往难以处理小目标和高计算成本的问题。此外，现有的技术在精准性、鲁棒性和效率方面都存在不足。

Method: SCA-Net首先引入了一个多尺度差分金字塔块来实现多尺度变化分析。其次，该模型结合了形状感知和高分辨率增强模块，提出了自适应多尺度处理模块。此外，SCA-Net还采用了多级注意力机制（PPM 和 CSAGate）来同时处理上下文和细节。最后，通过引入动态组合损失函数和四阶段训练策略来稳定训练过程并加速收敛。

Result: 在LEVIR-CD和LEVIR-MCI数据集上的全面评估表明，SCA-Net在精度和效率上优于现有方法。SCA-Net在LEVIR-MCI数据集上的平均交并比 (mIoU) 提升了2.64%，对于小建筑的交并比 (IoU) 提升了57.9%，同时将训练时间缩短了61%。

Conclusion: SCA-Net为实际变化检测应用提供了一个高效、准确且鲁棒的解决方案。该模型不仅在性能上有所突破，而且在时间和资源消耗方面也实现了优化。

Abstract: Automated change detection in remote sensing imagery is critical for urban management, environmental monitoring, and disaster assessment. While deep learning models have advanced this field, they often struggle with challenges like low sensitivity to small objects and high computational costs. This paper presents SCA-Net, an enhanced architecture built upon the Change-Agent framework for precise building and road change detection in bi-temporal images. Our model incorporates several key innovations: a novel Difference Pyramid Block for multi-scale change analysis, an Adaptive Multi-scale Processing module combining shape-aware and high-resolution enhancement blocks, and multi-level attention mechanisms (PPM and CSAGate) for joint contextual and detail processing. Furthermore, a dynamic composite loss function and a four-phase training strategy are introduced to stabilize training and accelerate convergence. Comprehensive evaluations on the LEVIR-CD and LEVIR-MCI datasets demonstrate SCA-Net's superior performance over Change-Agent and other state-of-the-art methods. Our approach achieves a significant 2.64% improvement in mean Intersection over Union (mIoU) on LEVIR-MCI and a remarkable 57.9% increase in IoU for small buildings, while reducing the training time by 61%. This work provides an efficient, accurate, and robust solution for practical change detection applications.

</details>


### [31] [DR.Experts: Differential Refinement of Distortion-Aware Experts for Blind Image Quality Assessment](https://arxiv.org/abs/2602.09531)
*Bohan Fu,Guanyi Qin,Fazhan Zhang,Zihao Huang,Mingxuan Li,Runze Hu*

Main category: cs.CV

TL;DR: DR.Experts 是一种通过引入新颖的先验驱动的盲图像质量评估框架，能够更准确地捕捉图像中的细微失真线索，从而更好地与人类主观判断相匹配。


<details>
  <summary>Details</summary>
Motivation: 现有模型在挖掘图像失真线索方面的不足导致其与人类感知之间的偏差，DR.Experts通过引入先验信息和专门的设计模块，企图解决这一问题。

Method: DR.Experts 包括两个主要模块：Distortion-Saliency Differential Module 和 Dynamic Distortion Weighting Module。第一部分通过灾变意识的视觉语言模型获取特定于失真的先验信息，并利用区分机制从语义注意力中提炼这些信息。第二部分则通过混合专家风格的模块——Dynamic Distortion Weighting Module，结合语义信息与失真特定的先验信息，为每个特定的失真特征赋予感知影响的权重。

Result: 在五个具有挑战性的盲图像质量评估基准上的实验表明，DR.Experts 在一般化和数据效率方面都优于现有的方法。

Conclusion: DR.Experts 通过引入先验驱动的设计，能够在评估图像失真时更加准确地反映出人类的主观判断，展现出更强的泛化能力和数据效率。

Abstract: Blind Image Quality Assessment, aiming to replicate human perception of visual quality without reference, plays a key role in vision tasks, yet existing models often fail to effectively capture subtle distortion cues, leading to a misalignment with human subjective judgments. We identify that the root cause of this limitation lies in the lack of reliable distortion priors, as methods typically learn shallow relationships between unified image features and quality scores, resulting in their insensitive nature to distortions and thus limiting their performance. To address this, we introduce DR.Experts, a novel prior-driven BIQA framework designed to explicitly incorporate distortion priors, enabling a reliable quality assessment. DR.Experts begins by leveraging a degradation-aware vision-language model to obtain distortion-specific priors, which are further refined and enhanced by the proposed Distortion-Saliency Differential Module through distinguishing them from semantic attentions, thereby ensuring the genuine representations of distortions. The refined priors, along with semantics and bridging representation, are then fused by a proposed mixture-of-experts style module named the Dynamic Distortion Weighting Module. This mechanism weights each distortion-specific feature as per its perceptual impact, ensuring that the final quality prediction aligns with human perception. Extensive experiments conducted on five challenging BIQA benchmarks demonstrate the superiority of DR.Experts over current methods and showcase its excellence in terms of generalization and data efficiency.

</details>


### [32] [AUHead: Realistic Emotional Talking Head Generation via Action Units Control](https://arxiv.org/abs/2602.09534)
*Jiayi Lyu,Leigang Qu,Wenjing Zhang,Hanyu Jiang,Kai Liu,Zhenglin Zhou,Xiaobo Xia,Jian Xue,Tat-Seng Chua*

Main category: cs.CV

TL;DR: 本文提出了一种新颖的两阶段方法（AUHead），通过动作单元（AUs）的细粒度情感控制和音频的解纠缠，实现了可控制的生成，达到了与基准数据集上现有技术相比的竞争性能。


<details>
  <summary>Details</summary>
Motivation: 当前方法在实现细腻的情感表达方面存在局限，因为缺乏对细粒度情感控制的处理。本文旨在解决这一问题，通过引入具有细粒度情感控制和动作单元解纠缠能力的方法来提高生成的现实性和一致性。

Method: 第一阶段通过大型音频-语言模型（ALMs）的空间-时间AU标记化和“情感-然后-AU”的推理机制探索了如何进行AU生成。第二阶段则提出了一个由AU驱动的可控扩散模型，该模型能够基于AU序列合成出带有真实表情的头部视频。为了进一步提高情感表达的灵活性，引入了AU分离引导策略。

Result: 文中提出的方法在情感现实性、口型同步精度和视觉一致性等方面取得了与现有技术竞争的性能，并在基准数据集上显著超越了现有技术。

Conclusion: 这种两阶段方法（AUHead）通过动作单元的细粒度情感控制和音频的解纠缠，实现了可控生成，为虚拟化身、电影制作和交互系统等领域的实时生成提供了新的可能。

Abstract: Realistic talking-head video generation is critical for virtual avatars, film production, and interactive systems. Current methods struggle with nuanced emotional expressions due to the lack of fine-grained emotion control. To address this issue, we introduce a novel two-stage method (AUHead) to disentangle fine-grained emotion control, i.e. , Action Units (AUs), from audio and achieve controllable generation. In the first stage, we explore the AU generation abilities of large audio-language models (ALMs), by spatial-temporal AU tokenization and an "emotion-then-AU" chain-of-thought mechanism. It aims to disentangle AUs from raw speech, effectively capturing subtle emotional cues. In the second stage, we propose an AU-driven controllable diffusion model that synthesizes realistic talking-head videos conditioned on AU sequences. Specifically, we first map the AU sequences into the structured 2D facial representation to enhance spatial fidelity, and then model the AU-vision interaction within cross-attention modules. To achieve flexible AU-quality trade-off control, we introduce an AU disentanglement guidance strategy during inference, further refining the emotional expressiveness and identity consistency of the generated videos. Results on benchmark datasets demonstrate that our approach achieves competitive performance in emotional realism, accurate lip synchronization, and visual coherence, significantly surpassing existing techniques. Our implementation is available at https://github.com/laura990501/AUHead_ICLR

</details>


### [33] [Scalpel: Fine-Grained Alignment of Attention Activation Manifolds via Mixture Gaussian Bridges to Mitigate Multimodal Hallucination](https://arxiv.org/abs/2602.09541)
*Ziqiang Shi,Rujie Liu,Shanshan Yu,Satoshi Munakata,Koichi Shirahata*

Main category: cs.CV

TL;DR: Scalpel 是一种通过调整注意力激活分布来减少LVLMs幻觉的技术。它使用高斯混合模型和熵最优传输来准确映射信任和幻觉激活成分，并动态调整干预强度和方向以减轻幻觉。


<details>
  <summary>Details</summary>
Motivation: 面对大视图语言模型(LVLMs)由于大型语言模型(LLMs)的先验和跨模态的不匹配注意力导致的幻觉问题，本文提出了一种新的方法Scalpel来减少LVLMs的幻觉现象。

Method: Scalpel 使用高斯混合模型捕获信任和幻觉空间中的多峰注意力分布，通过熵最优传输（相当于薛定谔桥问题）精确映射高斯组件，并在缓解幻觉时动态调整干预强度和方向。

Result: 实验结果表明，Scalpel 在多个数据集和基准测试中显著减少了幻觉，超越了先前的方法，并达到了最先进的性能。此外，Scalpel 是模型和数据无关的，只需单一解码步骤。

Conclusion: Scalpel 通过创新的方法有效减少了LVLMs的幻觉，是一种具有广泛应用前景的技术。

Abstract: Rapid progress in large vision-language models (LVLMs) has achieved unprecedented performance in vision-language tasks. However, due to the strong prior of large language models (LLMs) and misaligned attention across modalities, LVLMs often generate outputs inconsistent with visual content - termed hallucination. To address this, we propose \textbf{Scalpel}, a method that reduces hallucination by refining attention activation distributions toward more credible regions. Scalpel predicts trusted attention directions for each head in Transformer layers during inference and adjusts activations accordingly. It employs a Gaussian mixture model to capture multi-peak distributions of attention in trust and hallucination manifolds, and uses entropic optimal transport (equivalent to Schrödinger bridge problem) to map Gaussian components precisely. During mitigation, Scalpel dynamically adjusts intervention strength and direction based on component membership and mapping relationships between hallucination and trust activations. Extensive experiments across multiple datasets and benchmarks demonstrate that Scalpel effectively mitigates hallucinations, outperforming previous methods and achieving state-of-the-art performance. Moreover, Scalpel is model- and data-agnostic, requiring no additional computation, only a single decoding step.

</details>


### [34] [Delving into Spectral Clustering with Vision-Language Representations](https://arxiv.org/abs/2602.09586)
*Bo Peng,Yuanwei Hu,Bo Liu,Ling Chen,Jie Lu,Zhen Fang*

Main category: cs.CV

TL;DR: 本文提出了一种多模态神经切面内核谱聚类方法，通过视语言预训练模型的跨模态对齐，在视觉邻近性和语义重叠的基础上优化了聚类亲和度，实验表明该方法在多种基准数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前谱聚类方法多为单模态，本文旨在探索多模态谱聚类方法，通过结合视语言预训练模型，拓展谱聚类的应用。

Method: 本文提出了多模态神经切面内核谱聚类方法，基于预训练模型的跨模态对齐，通过锚定神经切面核与与图像相关的积极名词来优化聚合性。

Result: 本文的方法在16个基准数据集中（包含经典、大规模、细粒度和域迁移的数据集）展现出优越于当前最佳方法的性能。

Conclusion: 本文探索了多模态谱聚类方法，证明了方法的有效性，并指出其潜力可进一步应用于其他类别数据集。

Abstract: Spectral clustering is known as a powerful technique in unsupervised data analysis. The vast majority of approaches to spectral clustering are driven by a single modality, leaving the rich information in multi-modal representations untapped. Inspired by the recent success of vision-language pre-training, this paper enriches the landscape of spectral clustering from a single-modal to a multi-modal regime. Particularly, we propose Neural Tangent Kernel Spectral Clustering that leverages cross-modal alignment in pre-trained vision-language models. By anchoring the neural tangent kernel with positive nouns, i.e., those semantically close to the images of interest, we arrive at formulating the affinity between images as a coupling of their visual proximity and semantic overlap. We show that this formulation amplifies within-cluster connections while suppressing spurious ones across clusters, hence encouraging block-diagonal structures. In addition, we present a regularized affinity diffusion mechanism that adaptively ensembles affinity matrices induced by different prompts. Extensive experiments on \textbf{16} benchmarks -- including classical, large-scale, fine-grained and domain-shifted datasets -- manifest that our method consistently outperforms the state-of-the-art by a large margin.

</details>


### [35] [MieDB-100k: A Comprehensive Dataset for Medical Image Editing](https://arxiv.org/abs/2602.09587)
*Yongfan Lai,Wen Qian,Bo Liu,Hongyan Li,Hao Luo,Fan Wang,Bohan Zhuang,Shenda Hong*

Main category: cs.CV

TL;DR: MieDB-100k 是一个大型的、高质量且多样的数据集，旨在解决医学图像编辑数据稀缺和质量不足的问题，通过数据策展流程和严格的临床检查构建，支持更高质量的生成模型。


<details>
  <summary>Details</summary>
Motivation: 现有医学图像编辑数据集存在多样性不足、质量有限和无法均衡质量和可扩展性的问题，MieDB-100k 的设计是为了填补这些空白，通过更全面的数据集促进专门的医学图像编辑研究。

Method: MieDB-100k 是通过模态特定专家模型和基于规则的数据合成方法组合建造，并进行严格的临床检查以确保数据质量。编辑任务被分类为感知、修改和转换视角，以平衡生成和理解能力。

Result: 使用 MieDB-100k 训练的模型在多个模型中表现最佳，特别是在开放源码和专有模型上，展示了强大的泛化能力。

Conclusion: MieDB-100k 将成为未来专业医学图像编辑研究的基础，并促进数据驱动的进步。

Abstract: The scarcity of high-quality data remains a primary bottleneck in adapting multimodal generative models for medical image editing. Existing medical image editing datasets often suffer from limited diversity, neglect of medical image understanding and inability to balance quality with scalability. To address these gaps, we propose MieDB-100k, a large-scale, high-quality and diverse dataset for text-guided medical image editing. It categorizes editing tasks into perspectives of Perception, Modification and Transformation, considering both understanding and generation abilities. We construct MieDB-100k via a data curation pipeline leveraging both modality-specific expert models and rule-based data synthetic methods, followed by rigorous manual inspection to ensure clinical fidelity. Extensive experiments demonstrate that model trained with MieDB-100k consistently outperform both open-source and proprietary models while exhibiting strong generalization ability. We anticipate that this dataset will serve as a cornerstone for future advancements in specialized medical image editing.

</details>


### [36] [Tele-Omni: a Unified Multimodal Framework for Video Generation and Editing](https://arxiv.org/abs/2602.09609)
*Jialun Liu,Yukuo Ma,Xiao Cao,Tian Li,Gonghu Shang,Haibin Huang,Chi Zhang,Xuelong Li,Cong Liu,Junqi Liu,Jiakui Hu,Robby T. Tan,Shiwen Zhang,Liying Yang,Xiaoyan Yang,Qizhen Weng,Xiangzhen Chang,Yuanzhi Liang,Yifan Xu,Zhiyong Huang,Zuoxin Li,Xuelong Li*

Main category: cs.CV

TL;DR: Tele-Omni提出了一种统一的多模态框架，利用预训练的多模态大模型解析多模态指令，并通过扩散生成模型实现高质量视频合成，支持多种视频任务并保持良好的时序一致性和视觉一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的视频生成和编辑方法主要基于文本指令，难以处理多模态输入和复杂的视频生成、编辑场景。而精心设计的视频编辑管道限制了可扩展性和组合性。

Method: Tele-Omni框架利用预训练的多模态大语言模型解析多模态指令，通过扩散生成模型基于这些结构化信号进行高质量的视频合成。此外，引入了一个任务感知的数据处理管道，将多模态输入统一到结构化指令格式中，同时保留任务特定的约束。

Result: Tele-Omni在多种任务中展现了竞争力，实现了对多种视频任务的灵活多模态控制，并保持了良好的时序一致性和视觉一致性。

Conclusion: Tele-Omni提供了一种统一且灵活的解决方案，支持广泛的视频生成和编辑任务，展示了在多个任务上的强性能。

Abstract: Recent advances in diffusion-based video generation have substantially improved visual fidelity and temporal coherence. However, most existing approaches remain task-specific and rely primarily on textual instructions, limiting their ability to handle multimodal inputs, contextual references, and diverse video generation and editing scenarios within a unified framework. Moreover, many video editing methods depend on carefully engineered pipelines tailored to individual operations, which hinders scalability and composability. In this paper, we propose Tele-Omni, a unified multimodal framework for video generation and editing that follows multimodal instructions, including text, images, and reference videos, within a single model. Tele-Omni leverages pretrained multimodal large language models to parse heterogeneous instructions and infer structured generation or editing intents, while diffusion-based generators perform high-quality video synthesis conditioned on these structured signals. To enable joint training across heterogeneous video tasks, we introduce a task-aware data processing pipeline that unifies multimodal inputs into a structured instruction format while preserving task-specific constraints. Tele-Omni supports a wide range of video-centric tasks, including text-to-video generation, image-to-video generation, first-last-frame video generation, in-context video generation, and in-context video editing. By decoupling instruction parsing from video synthesis and combining it with task-aware data design, Tele-Omni achieves flexible multimodal control while maintaining strong temporal coherence and visual consistency. Experimental results demonstrate that Tele-Omni achieves competitive performance across multiple tasks.

</details>


### [37] [AGMark: Attention-Guided Dynamic Watermarking for Large Vision-Language Models](https://arxiv.org/abs/2602.09611)
*Yue Li,Xin Yi,Dongsheng Shi,Yongyi Cui,Gerard de Melo,Linlin Wang*

Main category: cs.CV

TL;DR: 本文提出了一种名为AGMark的新框架，该框架能够在保持视觉保真度的同时嵌入可检测信号，通过动态识别语义关键证据和调整词汇分割以适应生成的动态变化，从而提高生成质量和视觉语义保真度。


<details>
  <summary>Details</summary>
Motivation: 现有水印方法可能引入视觉上无关的标记并破坏视觉对齐，影响LVLM的性能。同时，静态水印可能忽视权重重分布密度，未能有效适应生成过程中的视觉依赖性变化。

Method: AGMark框架通过动态基于注意力权重识别语义关键证据，结合背景感知一致性线索，确定语义关键标记的比例，并通过联合考虑不确定性意识和证据校准，实现自适应词汇分割，避免无关标记。

Result: 实验结果表明，与传统方法相比，AGMark提高了生成质量和视觉语义保真度，特别是在生成过程的后期阶段。

AGMark在检测准确性和对抗攻击韧性方面保持了高效，实现了较高的AUC值（至少99.36%和88.61%），未牺牲推理效率。

Conclusion: AGMark框架为多模态水印提供了新的标准，保留了可靠性的同时，提高了视觉语言模型的生成质量。

Abstract: Watermarking has emerged as a pivotal solution for content traceability and intellectual property protection in Large Vision-Language Models (LVLMs). However, vision-agnostic watermarks may introduce visually irrelevant tokens and disrupt visual grounding by enforcing indiscriminate pseudo-random biases. Additionally, current vision-specific watermarks rely on a static, one-time estimation of vision critical weights and ignore the weight distribution density when determining the proportion of protected tokens. This design fails to account for dynamic changes in visual dependence during generation and may introduce low-quality tokens in the long tail. To address these challenges, we propose Attention-Guided Dynamic Watermarking (AGMark), a novel framework that embeds detectable signals while strictly preserving visual fidelity. At each decoding step, AGMark first dynamically identifies semantic-critical evidence based on attention weights for visual relevance, together with context-aware coherence cues, resulting in a more adaptive and well-calibrated evidence-weight distribution. It then determines the proportion of semantic-critical tokens by jointly considering uncertainty awareness (token entropy) and evidence calibration (weight density), thereby enabling adaptive vocabulary partitioning to avoid irrelevant tokens. Empirical results confirm that AGMark outperforms conventional methods, observably improving generation quality and yielding particularly strong gains in visual semantic fidelity in the later stages of generation. The framework maintains highly competitive detection accuracy (at least 99.36\% AUC) and robust attack resilience (at least 88.61\% AUC) without sacrificing inference efficiency, effectively establishing a new standard for reliability-preserving multi-modal watermarking.

</details>


### [38] [Towards Training-free Multimodal Hate Localisation with Large Language Models](https://arxiv.org/abs/2602.09637)
*Yueming Sun,Long Yang,Jianbo Jiao,Zeyu Fu*

Main category: cs.CV

TL;DR: 该研究提出了一种名为LELA的新框架，用于在线视频中的仇恨内容检测与定位。LELA 不依赖于大规模的人工标注，而是利用大型语言模型和模态特定的字幕来检测和定位仇恨内容。


<details>
  <summary>Details</summary>
Motivation: 当前的仇恨内容检测解决方案要么需要大量人工注释，要么缺乏时间精度，LELA旨在填补这一空白。

Method: LELA框架将视频分解为五种模态（图像、语音、OCR、音乐和视频上下文），并使用一个多阶段提示方案计算每个帧的细粒度仇恨评分。同时，引入了模态匹配机制以增强跨模态推理。

Result: 在HateMM和MultiHateClip两个具有挑战性的基准测试上，LELA的表现优于所有现有的非训练型基线。

Conclusion: LELA为可扩展和可解释的仇恨视频定位提供了坚实的基础，提供了广泛的消融测试和定性可视化。

Abstract: The proliferation of hateful content in online videos poses severe threats to individual well-being and societal harmony. However, existing solutions for video hate detection either rely heavily on large-scale human annotations or lack fine-grained temporal precision. In this work, we propose LELA, the first training-free Large Language Model (LLM) based framework for hate video localization. Distinct from state-of-the-art models that depend on supervised pipelines, LELA leverages LLMs and modality-specific captioning to detect and temporally localize hateful content in a training-free manner. Our method decomposes a video into five modalities, including image, speech, OCR, music, and video context, and uses a multi-stage prompting scheme to compute fine-grained hateful scores for each frame. We further introduce a composition matching mechanism to enhance cross-modal reasoning. Experiments on two challenging benchmarks, HateMM and MultiHateClip, demonstrate that LELA outperforms all existing training-free baselines by a large margin. We also provide extensive ablations and qualitative visualizations, establishing LELA as a strong foundation for scalable and interpretable hate video localization.

</details>


### [39] [VideoAfford: Grounding 3D Affordance from Human-Object-Interaction Videos via Multimodal Large Language Model](https://arxiv.org/abs/2602.09638)
*Hanqing Wang,Mingyu Liu,Xiaoyu Chen,Chengwei MA,Yiming Zhong,Wenti Yin,Yuhao Liu,Zhiqing Cui,Jiahao Yuan,Lu Dai,Zhiyuan Ma,Hui Xiong*

Main category: cs.CV

TL;DR: 该研究提出了一个名为 VideoAfford 的强基线，通过一个综合的视频基础3D工具可用性数据集（VIDA）进行学习，该数据集包含了人类-物体交互视频，以及一个空间意识损失函数来提升模型的动态交互先验和泛化性。


<details>
  <summary>Details</summary>
Motivation: 当前大多数研究主要依赖于静态线索（如语言和图像）来学习可用性知识，但在揭示动态交互上下文中的时间性与因果性方面存在局限。本文提出一个新的数据集以及方法以改善现有研究的限制。

Method: 本文通过构建了一个包含38K人类-物体交互视频的综合3D可用性数据集，利用包含额外可用性分割能力的多模态大型语言模型，并通过引入一个‘空间意识’损失函数，来激活动态交互先验和提取全面的3D空间知识。

Result: 实验结果表明，该方法显著优于现有方法，并展示了强大的开放世界泛化能力，能够进行可用性推理。

Conclusion: 本文提出了一个综合性的数据集VIDA，以及一种多模态大型语言模型与空间意识损失函数结合的新方法VideoAfford，用于提升3D物体可用性的理解与应用，方法在实际应用中显示出良好的性能与泛化能力。

Abstract: 3D affordance grounding aims to highlight the actionable regions on 3D objects, which is crucial for robotic manipulation. Previous research primarily focused on learning affordance knowledge from static cues such as language and images, which struggle to provide sufficient dynamic interaction context that can reveal temporal and causal cues. To alleviate this predicament, we collect a comprehensive video-based 3D affordance dataset, \textit{VIDA}, which contains 38K human-object-interaction videos covering 16 affordance types, 38 object categories, and 22K point clouds. Based on \textit{VIDA}, we propose a strong baseline: VideoAfford, which activates multimodal large language models with additional affordance segmentation capabilities, enabling both world knowledge reasoning and fine-grained affordance grounding within a unified framework. To enhance action understanding capability, we leverage a latent action encoder to extract dynamic interaction priors from HOI videos. Moreover, we introduce a \textit{spatial-aware} loss function to enable VideoAfford to obtain comprehensive 3D spatial knowledge. Extensive experimental evaluations demonstrate that our model significantly outperforms well-established methods and exhibits strong open-world generalization with affordance reasoning abilities. All datasets and code will be publicly released to advance research in this area.

</details>


### [40] [Time2General: Learning Spatiotemporal Invariant Representations for Domain-Generalization Video Semantic Segmentation](https://arxiv.org/abs/2602.09648)
*Siyu Chen,Ting Han,Haoling Huang,Chaolei Wang,Chengzheng Fu,Duxin Zhu,Guorong Cai,Jinhe Su*

Main category: cs.CV

TL;DR: Time2General 提出了一种名为 Spatio-Temporal Memory Decoder 的新框架来解决 DGVSS 中的域移和时间采样移的挑战，通过减少帧间闪烁并提高跨域准确性，使得在未标记的驾驶领域中也能保持跨帧的一致性。


<details>
  <summary>Details</summary>
Motivation: 传统的视频语义分割在面对未见过的领域时容易出现域移和时间采样移，导致跨帧闪烁问题严重。因此，提出 Time2General 框架来应对这些挑战。

Method: Time2General 引入了 Spatio-Temporal Memory Decoder，用于在剪辑级汇集多帧上下文并解码跨帧一致的掩码，同时提出了 Masked Temporal Consistency Loss 来缓解由于不同时间步长引起的预测差异，进一步随机化训练时间步长以增加时间间隔的多样性。

Result: 实验结果显示，Time2General 相较于之前的 DGSS 和 VSS 方法，在多个驾驶场景基准测试中取得了显著的跨域准确性和时间稳定性提升，且运行速度可达每秒 18 帧。

Conclusion: Time2General 框架有效应对了视频语义分割中的域移和时间采样移挑战，展现了提高跨域准确性及跨帧一致性的潜力。

Abstract: Domain Generalized Video Semantic Segmentation (DGVSS) is trained on a single labeled driving domain and is directly deployed on unseen domains without target labels and test-time adaptation while maintaining temporally consistent predictions over video streams. In practice, both domain shift and temporal-sampling shift break correspondence-based propagation and fixed-stride temporal aggregation, causing severe frame-to-frame flicker even in label-stable regions. We propose Time2General, a DGVSS framework built on Stability Queries. Time2General introduces a Spatio-Temporal Memory Decoder that aggregates multi-frame context into a clip-level spatio-temporal memory and decodes temporally consistent per-frame masks without explicit correspondence propagation. To further suppress flicker and improve robustness to varying sampling rates, the Masked Temporal Consistency Loss is proposed to regularize temporal prediction discrepancies across different strides, and randomize training strides to expose the model to diverse temporal gaps. Extensive experiments on multiple driving benchmarks show that Time2General achieves a substantial improvement in cross-domain accuracy and temporal stability over prior DGSS and VSS baselines while running at up to 18 FPS. Code will be released after the review process.

</details>


### [41] [TreeCUA: Efficiently Scaling GUI Automation with Tree-Structured Verifiable Evolution](https://arxiv.org/abs/2602.09662)
*Deyang Jiang,Jing Huang,Xuanle Zhao,Lei Chen,Liming Zheng,Fanfan Liu,Haibo Qiu,Peng Shi,Zhixiong Zeng*

Main category: cs.CV

TL;DR: 本文提出了一种名为TreeCUA的多智能体协作框架及其改进版本TreeCUA-DPO，通过树形结构验证可演变式来高效扩展GUI自动化。该方法通过多层次探索、重放冗余节点、调整探索深度和广度、以及利用全球记忆反向追踪来增强计划质量，并通过相邻轨迹的分支信息提升GUI规划能力，实验结果表明其显著提高了自动化效果和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的GUI自动化工作主要关注于GUI对象的定位而不是更好规划的问题。本文通过树形结构对探索过程进行组织，提出了一个高效的多智能体协作框架来改进GUI自动化。

Method: 本文提出了一种多智能体协作框架，探索环境、验证动作、总结路径、评估质量以生成高质量、可扩展的GUI路径。使用树形结构验证可演变式，并设计了一种新颖的基于树的拓扑结构存储和重放冗余节点。同时设计了一种自适应探索算法以平衡路径难度和多样性，并利用世界知识指导和全局记忆反向追踪来避免低质量生成。

Result: 通过这种多智能体框架和基于树的探索方法，实验结果表明TreeCUA及其改进版本TreeCUA-DPO在GUI自动化方面取得了显著的改进。树形结构组织路径节点方法不仅减少数据成本，还优化了探索过程。

Conclusion: 本文提出的TreeCUA和TreeCUA-DPO在提高GUI自动化的质量和效率方面具有显著效果，并展示了较强的泛化能力。所有节点信息和代码将在https://github.com/UITron-hub/TreeCUA开源。

Abstract: Effectively scaling GUI automation is essential for computer-use agents (CUAs); however, existing work primarily focuses on scaling GUI grounding rather than the more crucial GUI planning, which requires more sophisticated data collection. In reality, the exploration process of a CUA across apps/desktops/web pages typically follows a tree structure, with earlier functional entry points often being explored more frequently. Thus, organizing large-scale trajectories into tree structures can reduce data cost and streamline the data scaling of GUI planning. In this work, we propose TreeCUA to efficiently scale GUI automation with tree-structured verifiable evolution. We propose a multi-agent collaborative framework to explore the environment, verify actions, summarize trajectories, and evaluate quality to generate high-quality and scalable GUI trajectories. To improve efficiency, we devise a novel tree-based topology to store and replay duplicate exploration nodes, and design an adaptive exploration algorithm to balance the depth (\emph{i.e.}, trajectory difficulty) and breadth (\emph{i.e.}, trajectory diversity). Moreover, we develop world knowledge guidance and global memory backtracking to avoid low-quality generation. Finally, we naturally extend and propose the TreeCUA-DPO method from abundant tree node information, improving GUI planning capability by referring to the branch information of adjacent trajectories. Experimental results show that TreeCUA and TreeCUA-DPO offer significant improvements, and out-of-domain (OOD) studies further demonstrate strong generalization. All trajectory node information and code will be available at https://github.com/UITron-hub/TreeCUA.

</details>


### [42] [Semi-supervised Liver Segmentation and Patch-based Fibrosis Staging with Registration-aided Multi-parametric MRI](https://arxiv.org/abs/2602.09686)
*Boya Wang,Ruizhe Li,Chao Chen,Xin Chen*

Main category: cs.CV

TL;DR: 本研究提出了一种适用于多参数MRI的多任务深度学习框架，用于肝脏分割和肝纤维化分期，能够在标注数据有限和跨模态差异的挑战下有效处理多模态成像数据。


<details>
  <summary>Details</summary>
Motivation: 现状中肝纤维化对临床实践构成了重大挑战，需要精确的肝脏分割和准确的疾病分期。因此，为了应对标注数据稀缺和多参数MRI的复杂性问题，本研究提出了一种多任务深度学习框架。

Method: 该多任务深度学习框架分别采用了半监督学习模型进行肝脏分割（LiSeg）和基于小块方法的肝纤维化分期方法（LiFS）。模型利用标签和未标签数据来克服领域转移和模态间的变异性。

Result: 该方法在独立测试集中对分布在分布（ID）和非分布在分布（OOD）情况下的三通道MRI（T1、T2、DWI）和七通道MRI（T1、T2、DWI、GED1-GED4）数据进行了测试。

Conclusion: 研究表明，该多任务深度学习框架能够有效处理多模态成像数据和带标签量有限的情况，在肝纤维化临床诊断中具有重要的应用潜力。

Abstract: Liver fibrosis poses a substantial challenge in clinical practice, emphasizing the necessity for precise liver segmentation and accurate disease staging. Based on the CARE Liver 2025 Track 4 Challenge, this study introduces a multi-task deep learning framework developed for liver segmentation (LiSeg) and liver fibrosis staging (LiFS) using multiparametric MRI. The LiSeg phase addresses the challenge of limited annotated images and the complexities of multi-parametric MRI data by employing a semi-supervised learning model that integrates image segmentation and registration. By leveraging both labeled and unlabeled data, the model overcomes the difficulties introduced by domain shifts and variations across modalities. In the LiFS phase, we employed a patchbased method which allows the visualization of liver fibrosis stages based on the classification outputs. Our approach effectively handles multimodality imaging data, limited labels, and domain shifts. The proposed method has been tested by the challenge organizer on an independent test set that includes in-distribution (ID) and out-of-distribution (OOD) cases using three-channel MRIs (T1, T2, DWI) and seven-channel MRIs (T1, T2, DWI, GED1-GED4). The code is freely available. Github link: https://github.com/mileywang3061/Care-Liver

</details>


### [43] [GenSeg-R1: RL-Driven Vision-Language Grounding for Fine-Grained Referring Segmentation](https://arxiv.org/abs/2602.09701)
*Sandesh Hegde,Jaison Saji Chacko,Debarshi Banerjee,Uma Mahesh*

Main category: cs.CV

TL;DR: 提出了GenSeg-R1框架，利用脱耦的推理-分割管道，通过Group Relative Policy Optimization（GRPO）对Qwen3-VL模型进行微调，显著提高了细粒度图像分割的效果，特别是在无目标检测能力的情况下。


<details>
  <summary>Details</summary>
Motivation: 当前的图像分割技术在处理具有自然语言查询的细粒度图像分割任务时，通常需要大量的标注数据来训练模型，这在实际应用中增加了数据收集和标注的难度。本文通过设计一种新的框架GenSeg-R1，利用一种高效的策略优化方法GRPO，无需大量标注数据即能显著提升模型性能。

Method: 本文采用了一种脱耦的推理-分割管道：首先，通过Vision-Language模型接收视图和自然语言查询，进行场景推理并生成结构化的空间提示；然后，这些空间提示被用作冻结的可提示分割器（SAM 2）的输入，将其转换为高质量的遮罩。此外，还通过从GRefCOCO数据集中的指导训练了一个GenSeg-R1-G版本，进一步优化了分割质量。

Result: 在RefCOCOg验证集上，GenSeg-R1-8B模型达到了0.7127 cIoU和0.7382 mIoU，优于Qwen3-VL Instruct baseline版本15.3点和21.9点。在GRefCOCO验证集上，GenSeg-R1-G实现了76.69%的目标mIoU和82.40%的无目标提示准确性，超过了Seg-R1-7B和Seg-Zero-7B。在ReasonSeg测试集上，GenSeg-R1-4B达到了68.40%的mIoU，分别优于Seg-Zero-7B和Seg-R1-7B 7.0点和10.7点。

Conclusion: 本文创新地通过脱耦的推理-分割框架和高效的策略优化方法，成功解决了细粒度图像分割问题，且不需要大量标注数据。实验结果表明，本文的方法优于现有技术，并具有广阔的应用前景。

Abstract: We study fine-grained referring image segmentation via a decoupled reason-then-segment pipeline. A vision-language model (VLM) receives an image and a natural-language query, reasons about the scene, and emits structured spatial prompts: a bounding box plus two interior keypoints for every referred instance. A frozen promptable segmenter (SAM 2) converts these prompts into high-quality masks.
  Within our GenSeg-R1 framework we finetune Qwen3-VL models (4B and 8B parameters) using Group Relative Policy Optimization (GRPO), requiring no supervised reasoning-chain annotations. On RefCOCOg validation our best model (GenSeg-R1-8B) achieves 0.7127 cIoU and 0.7382 mIoU, substantially outperforming the corresponding Qwen3-VL Instruct baselines (+15.3 and +21.9 points, respectively) and surpassing Seg-Zero-7B [3] by +3.3 cIoU under identical evaluation.
  We further introduce GenSeg-R1-G, a variant trained on GRefCOCO [9] with a SAM 2 in-the-loop reward that directly optimizes mask quality. On GRefCOCO validation GenSeg-R1-G achieves 76.69% target mIoU with 82.40% accuracy on negative (no-target) prompts, substantially outperforming Seg-R1-7B and Seg-Zero-7B, which lack no-target detection capability. On ReasonSeg test, GenSeg-R1-4B reaches 68.40% mIoU, surpassing Seg-Zero-7B by +7.0 and Seg-R1-7B by +10.7 points.

</details>


### [44] [Stroke3D: Lifting 2D strokes into rigged 3D model via latent diffusion models](https://arxiv.org/abs/2602.09713)
*Ruisi Zhao,Haoren Zheng,Zongxin Yang,Hehe Fan,Yi Yang*

Main category: cs.CV

TL;DR: Stroke3D 提出了一种创新的框架，能够直接从用户的二维手绘路径和描述性文本生成可调动画的3D模型。


<details>
  <summary>Details</summary>
Motivation: 解决了现有3D生成方法在创造可调动画几何方面的挑战，以及现有的骨架创造技术在精细结构控制上的不足。

Method: Stroke3D 采用了两阶段的工作流，首先通过 Skeletal Graph VAE 和 Skeletal Graph DiT 分别生成可控的骨架结构，然后通过 TextuRig 和 SKA-DPO 提升骨架到网格的生成过程。

Result: 该方法生成了合理的骨骼结构和高质量的网格，验证了其在创建可立即用于动画3D内容上的有效性。

Conclusion: Stroke3D 是首次能够根据用户绘制的2D路径生成条件化的3D骨架网格，实验结果表明其成功实现了这一目标。

Abstract: Rigged 3D assets are fundamental to 3D deformation and animation. However, existing 3D generation methods face challenges in generating animatable geometry, while rigging techniques lack fine-grained structural control over skeleton creation. To address these limitations, we introduce Stroke3D, a novel framework that directly generates rigged meshes from user inputs: 2D drawn strokes and a descriptive text prompt. Our approach pioneers a two-stage pipeline that separates the generation into: 1) Controllable Skeleton Generation, we employ the Skeletal Graph VAE (Sk-VAE) to encode the skeleton's graph structure into a latent space, where the Skeletal Graph DiT (Sk-DiT) generates a skeletal embedding. The generation process is conditioned on both the text for semantics and the 2D strokes for explicit structural control, with the VAE's decoder reconstructing the final high-quality 3D skeleton; and 2) Enhanced Mesh Synthesis via TextuRig and SKA-DPO, where we then synthesize a textured mesh conditioned on the generated skeleton. For this stage, we first enhance an existing skeleton-to-mesh model by augmenting its training data with TextuRig: a dataset of textured and rigged meshes with captions, curated from Objaverse-XL. Additionally, we employ a preference optimization strategy, SKA-DPO, guided by a skeleton-mesh alignment score, to further improve geometric fidelity. Together, our framework enables a more intuitive workflow for creating ready to animate 3D content. To the best of our knowledge, our work is the first to generate rigged 3D meshes conditioned on user-drawn 2D strokes. Extensive experiments demonstrate that Stroke3D produces plausible skeletons and high-quality meshes.

</details>


### [45] [From Lightweight CNNs to SpikeNets: Benchmarking Accuracy-Energy Tradeoffs with Pruned Spiking SqueezeNet](https://arxiv.org/abs/2602.09717)
*Radib Bin Kabir,Tawsif Tashwar Dipto,Mehedi Ahamed,Sabbir Ahmed,Md Hasanul Kabir*

Main category: cs.CV

TL;DR: 本文通过将紧凑的CNN架构转换为Spiking Neural Networks (SNNs)，探索轻量级CNN到SNN的设计和评估，使用Leaky-Integrate-and-Fire (LIF)神经元和反向传播近似梯度来训练SNNs，并评估了四种轻量级SNNs在CIFAR-10, CIFAR-100和TinyImageNet上的性能。


<details>
  <summary>Details</summary>
Motivation: 由于能量效率的需求，研究者们正在探索使用SNNs替代CNNs作为边缘智能的解决方案，尤其是对于轻量级模型的需求，但已有工作大多专注于大规模模型，而轻量级模型的设计与评估相对较少。

Method: 本文采用统一的设置，将轻量级CNN转换为SNNs，使用Leaky-Integrate-and-Fire (LIF) 神经元，并通过反向传播近似梯度进行训练。此外，还应用结构化剪枝策略，移除冗余模块，进一步优化了模型。

Result: SNNs在能耗效率上优于传统的CNNs，最高可达15.7倍。特别地，SqueezeNet的SNN变体表现尤为出色，剪枝后的SNN-SqueezeNet-P在CIFAR-10上的准确率提高了6%，减少了19%的参数量，同时能耗降低了88.1%。

Conclusion: 本文的研究结果表明，轻量级SNNs可以作为边缘部署的可行且低功耗的替代方案。

Abstract: Spiking Neural Networks (SNNs) are increasingly studied as energy-efficient alternatives to Convolutional Neural Networks (CNNs), particularly for edge intelligence. However, prior work has largely emphasized large-scale models, leaving the design and evaluation of lightweight CNN-to-SNN pipelines underexplored. In this paper, we present the first systematic benchmark of lightweight SNNs obtained by converting compact CNN architectures into spiking networks, where activations are modeled with Leaky-Integrate-and-Fire (LIF) neurons and trained using surrogate gradient descent under a unified setup. We construct spiking variants of ShuffleNet, SqueezeNet, MnasNet, and MixNet, and evaluate them on CIFAR-10, CIFAR-100, and TinyImageNet, measuring accuracy, F1-score, parameter count, computational complexity, and energy consumption. Our results show that SNNs can achieve up to 15.7x higher energy efficiency than their CNN counterparts while retaining competitive accuracy. Among these, the SNN variant of SqueezeNet consistently outperforms other lightweight SNNs. To further optimize this model, we apply a structured pruning strategy that removes entire redundant modules, yielding a pruned architecture, SNN-SqueezeNet-P. This pruned model improves CIFAR-10 accuracy by 6% and reduces parameters by 19% compared to the original SNN-SqueezeNet. Crucially, it narrows the gap with CNN-SqueezeNet, achieving nearly the same accuracy (only 1% lower) but with an 88.1% reduction in energy consumption due to sparse spike-driven computations. Together, these findings establish lightweight SNNs as practical, low-power alternatives for edge deployment, highlighting a viable path toward deploying high-performance, low-power intelligence on the edge.

</details>


### [46] [Allure of Craquelure: A Variational-Generative Approach to Crack Detection in Paintings](https://arxiv.org/abs/2602.09730)
*Laura Paul,Holger Rauhut,Martin Burger,Samira Kabri,Tim Roith*

Main category: cs.CV

TL;DR: 提出了结合生成模型和Mumford-Shah范数的混合方法，用于数字绘画中的裂纹检测。


<details>
  <summary>Details</summary>
Motivation: 利用现代技术实现非侵入性地评估艺术品退化并指导修复。

Method: 采用生成模型作为基础艺术品的强大先验，并结合Mumford-Shah型泛函捕捉裂纹结构。

Result: 实现了像素级别的裂纹定位图。

Conclusion: 提出的方法有助于自动化艺术品裂纹检测，支持艺术品的保存与保护。

Abstract: Recent advances in imaging technologies, deep learning and numerical performance have enabled non-invasive detailed analysis of artworks, supporting their documentation and conservation. In particular, automated detection of craquelure in digitized paintings is crucial for assessing degradation and guiding restoration, yet remains challenging due to the possibly complex scenery and the visual similarity between cracks and crack-like artistic features such as brush strokes or hair. We propose a hybrid approach that models crack detection as an inverse problem, decomposing an observed image into a crack-free painting and a crack component. A deep generative model is employed as powerful prior for the underlying artwork, while crack structures are captured using a Mumford--Shah-type variational functional together with a crack prior. Joint optimization yields a pixel-level map of crack localizations in the painting.

</details>


### [47] [Robust Vision Systems for Connected and Autonomous Vehicles: Security Challenges and Attack Vectors](https://arxiv.org/abs/2602.09740)
*Sandeep Gupta,Roberto Passerone*

Main category: cs.CV

TL;DR: 本文探讨了高级自动驾驶所需的稳健视觉系统在连接和自主车辆（CAVs）中的应用，分析了关键的传感器和视觉组件，提出了CAV视觉系统（CAVVS）的参考架构，并详细阐述了针对每个攻击面的攻击向量，评估了它们对机密性、完整性和可用性的影响，为制定稳健的保安措施提供了依据。


<details>
  <summary>Details</summary>
Motivation: 当前高级自动驾驶技术依赖于稳健的视觉系统来确保车辆在复杂的交通环境中的安全和可靠性。此研究旨在通过分析不同视觉组件及其潜在的攻击向量，来增强视觉系统的安全性，并提出相应的防御策略。

Method: 研究通过对现有CAVs的视觉系统进行全面评估，识别出重要传感器和组件，设计参考架构，并基于此架构深入分析不同攻击向量，重点评估它们对视觉系统安全的影响。

Result: 研究结果确立了CAVVs 的关键组件和潜在攻击向量，并评估了它们对视觉系统机密性、完整性和可用性的潜在威胁，这对于未来构建安全的CAVs具有重要指导意义。

Conclusion: 文章强调了视觉系统在高级自动驾驶中的核心作用，并提出了一种新的参考架构，以及针对各种潜在攻击向量的防御策略，为保障CAVs 的安全性做出了贡献。

Abstract: This article investigates the robustness of vision systems in Connected and Autonomous Vehicles (CAVs), which is critical for developing Level-5 autonomous driving capabilities. Safe and reliable CAV navigation undeniably depends on robust vision systems that enable accurate detection of objects, lane markings, and traffic signage. We analyze the key sensors and vision components essential for CAV navigation to derive a reference architecture for CAV vision system (CAVVS). This reference architecture provides a basis for identifying potential attack surfaces of CAVVS. Subsequently, we elaborate on identified attack vectors targeting each attack surface, rigorously evaluating their implications for confidentiality, integrity, and availability (CIA). Our study provides a comprehensive understanding of attack vector dynamics in vision systems, which is crucial for formulating robust security measures that can uphold the principles of the CIA triad.

</details>


### [48] [Where Do Images Come From? Analyzing Captions to Geographically Profile Datasets](https://arxiv.org/abs/2602.09775)
*Abhipsa Basu,Yugam Bahl,Kirti Bhagat,Preethi Seshadri,R. Venkatesh Babu,Danish Pruthi*

Main category: cs.CV

TL;DR: 研究通过地理定位图像-标签对来分析大规模多媒体数据集，发现美国、英国和加拿大占较大比例，而南美和非洲国家严重缺乏代表性。国家GDP与数据中的代表性呈正相关。生成的图像在覆盖范围上远不如真实世界照片。


<details>
  <summary>Details</summary>
Motivation: 由于文本到图像模型生成的图像缺乏地理代表性，本文旨在通过地理定位细化来研究这些模型的训练数据。

Method: 使用LLMs从图像-标签对中提取地理位置信息，并基于这些信息将它们映射到国家，从而研究数据集的地理分布。

Result: 研究表明，美国、英国和加拿大占数据集的48%，而南美和非洲国家仅占1.8%和3.8%。结果显示，国家GDP与在数据中的代表性正相关，非英语数据子集进一步强化了这种偏见。

Conclusion: 文章指出，尽管生成的图像在视觉上看起来真实，但它们的覆盖范围有限，未能充分反映真实世界多样性。

Abstract: Recent studies show that text-to-image models often fail to generate geographically representative images, raising concerns about the representativeness of their training data and motivating the question: which parts of the world do these training examples come from? We geographically profile large-scale multimodal datasets by mapping image-caption pairs to countries based on location information extracted from captions using LLMs. Studying English captions from three widely used datasets (Re-LAION, DataComp1B, and Conceptual Captions) across $20$ common entities (e.g., house, flag), we find that the United States, the United Kingdom, and Canada account for $48.0\%$ of samples, while South American and African countries are severely under-represented with only $1.8\%$ and $3.8\%$ of images, respectively. We observe a strong correlation between a country's GDP and its representation in the data ($ρ= 0.82$). Examining non-English subsets for $4$ languages from the Re-LAION dataset, we find that representation skews heavily toward countries where these languages are predominantly spoken. Additionally, we find that higher representation does not necessarily translate to greater visual or semantic diversity. Finally, analyzing country-specific images generated by Stable Diffusion v1.3 trained on Re-LAION, we show that while generations appear realistic, they are severely limited in their coverage compared to real-world images.

</details>


### [49] [SciFlow-Bench: Evaluating Structure-Aware Scientific Diagram Generation via Inverse Parsing](https://arxiv.org/abs/2602.09809)
*Tong Zhang,Honglin Lin,Zhou Liu,Chong Chen,Wentao Zhang*

Main category: cs.CV

TL;DR: SciFlow-Bench 提供了一种新基准方法，直接从像素级输出评估科学图表生成，重点在于结构恢复而非视觉相似性。


<details>
  <summary>Details</summary>
Motivation: 现有的评估方法依赖于视觉或主观度量标准，无法准确评估结构信息的准确性。SciFlow-Bench 精确评估了图生成模型在结构恢复方面的表现。

Method: SciFlow-Bench 基于实际的科学 PDF 文件，将源框架中的每个图与一个常用的真实图相对照，并使用一个闭环协议，将生成的图表重新解析回结构化的图进行比较。

Result: 实验数据表明，在结构复杂的图生成中保持结构一致性仍然是一个根本性的挑战，因此强调需要有结构意识的评估方法。

Conclusion: SciFlow-Bench 引入了一种新的评估框架，能够更准确地衡量图生成的结构恢复能力，从而促进该领域的进步。

Abstract: Scientific diagrams convey explicit structural information, yet modern text-to-image models often produce visually plausible but structurally incorrect results. Existing benchmarks either rely on image-centric or subjective metrics insensitive to structure, or evaluate intermediate symbolic representations rather than final rendered images, leaving pixel-based diagram generation underexplored. We introduce SciFlow-Bench, a structure-first benchmark for evaluating scientific diagram generation directly from pixel-level outputs. Built from real scientific PDFs, SciFlow-Bench pairs each source framework figure with a canonical ground-truth graph and evaluates models as black-box image generators under a closed-loop, round-trip protocol that inverse-parses generated diagram images back into structured graphs for comparison. This design enforces evaluation by structural recoverability rather than visual similarity alone, and is enabled by a hierarchical multi-agent system that coordinates planning, perception, and structural reasoning. Experiments show that preserving structural correctness remains a fundamental challenge, particularly for diagrams with complex topology, underscoring the need for structure-aware evaluation.

</details>


### [50] [CompSplat: Compression-aware 3D Gaussian Splatting for Real-world Video](https://arxiv.org/abs/2602.09816)
*Hojun Song,Heejung Choi,Aro Kim,Chae-yeong Song,Gahyeon Kim,Soo Ye Kim,Jaehyup Lee,Sang-hyo Park*

Main category: cs.CV

TL;DR: CompSplat 是一种压缩感知训练框架，它明确建模了帧间压缩特征，以减轻帧间不一致性和累积几何误差，特别适用于严重压缩下的鲁棒性和几何一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的 NVS 方法在处理长序列和高压缩条件下几何一致性较差，不能充分探索长视频中多样化的压缩模式。

Method: CompSplat 训练框架结合了压缩感知的帧权重和自适应剪枝策略来增强鲁棒性和几何一致性。

Result: 在 Tanks and Temples、Free 和 Hike 等具有挑战性的基准测试中，CompSplat 在渲染质量和姿势准确性方面都达到了最先进的水平，尤其在严重压缩条件下。

Conclusion: CompSplat 没有专注于特定的视觉失真或场景，而是提供了一个综合的解决方案来处理复杂的压缩挑战。

Abstract: High-quality novel view synthesis (NVS) from real-world videos is crucial for applications such as cultural heritage preservation, digital twins, and immersive media. However, real-world videos typically contain long sequences with irregular camera trajectories and unknown poses, leading to pose drift, feature misalignment, and geometric distortion during reconstruction. Moreover, lossy compression amplifies these issues by introducing inconsistencies that gradually degrade geometry and rendering quality. While recent studies have addressed either long-sequence NVS or unposed reconstruction, compression-aware approaches still focus on specific artifacts or limited scenarios, leaving diverse compression patterns in long videos insufficiently explored. In this paper, we propose CompSplat, a compression-aware training framework that explicitly models frame-wise compression characteristics to mitigate inter-frame inconsistency and accumulated geometric errors. CompSplat incorporates compression-aware frame weighting and an adaptive pruning strategy to enhance robustness and geometric consistency, particularly under heavy compression. Extensive experiments on challenging benchmarks, including Tanks and Temples, Free, and Hike, demonstrate that CompSplat achieves state-of-the-art rendering quality and pose accuracy, significantly surpassing most recent state-of-the-art NVS approaches under severe compression conditions.

</details>


### [51] [SAKED: Mitigating Hallucination in Large Vision-Language Models via Stability-Aware Knowledge Enhanced Decoding](https://arxiv.org/abs/2602.09825)
*Zhaoxu Li,Chenqi Kong,Peijun Bao,Song Xia,Yi Tu,Yi Yu,Xinghao Jiang,Xudong Jiang*

Main category: cs.CV

TL;DR: 本文通过对LVLM内部知识不稳定的广泛实证分析，识别出三种幻觉模式，并提出了一种基于知识稳定性的解码方法SAKED，提高了模型生成可靠token的能力。


<details>
  <summary>Details</summary>
Motivation: 为了应对大型视觉语言模型在实际应用中面临的幻觉安全和可靠性风险，在分析了人类在不确定时更容易出错的现象后，作者们研究了模型内部知识不稳定性如何导致幻觉。

Method: 作者们从注意力头、模型层和解码标记三个角度进行了广泛的实证分析，发现了三种幻觉模式，并开发了一种新的方法SAKED，通过引入分层知识稳定性得分来量化模型中的知识稳定性。

Result: SAKED能够在不同模型、任务和基准上实现最先进的幻觉缓解性能，通过减少噪声并动态利用最可靠的内部知识来促进真实的标记生成。

Conclusion: SAKED提出了一种创新的解决方案，为解决LVLM的幻觉问题提供了新的视角，并展示了其实用价值。

Abstract: Hallucinations in Large Vision-Language Models (LVLMs) pose significant security and reliability risks in real-world applications. Inspired by the observation that humans are more error-prone when uncertain or hesitant, we investigate how instability in a model 's internal knowledge contributes to LVLM hallucinations. We conduct extensive empirical analyses from three perspectives, namely attention heads, model layers, and decoding tokens, and identify three key hallucination patterns: (i) visual activation drift across attention heads, (ii) pronounced knowledge fluctuations across layers, and (iii) visual focus distraction between neighboring output tokens. Building on these findings, we propose Stability-Aware Knowledge-Enhanced Decoding (SAKED), which introduces a layer-wise Knowledge Stability Score (KSS) to quantify knowledge stability throughout the model. By contrasting the most stability-aware and stability-agnostic layers, SAKED suppresses decoding noise and dynamically leverages the most reliable internal knowledge for faithful token generation. Moreover, SAKED is training-free and can be seamlessly integrated into different architectures. Extensive experiments demonstrate that SAKED achieves state-of-the-art performance for hallucination mitigation on various models, tasks, and benchmarks.

</details>


### [52] [ARK: A Dual-Axis Multimodal Retrieval Benchmark along Reasoning and Knowledge](https://arxiv.org/abs/2602.09839)
*Yijie Lin,Guofeng Ding,Haochen Zhou,Haobin Li,Mouxing Yang,Xi Peng*

Main category: cs.CV

TL;DR: ARK benchmarks analyze multimodal retrieval from knowledge domains and reasoning skills perspectives, emphasizing the need for more complex visual and spatial reasoning.


<details>
  <summary>Details</summary>
Motivation: 现有的多模态检索基准主要集中在日常生活图像上的语义匹配，但未能充分诊断专业知识和复杂推理。为了解决这一问题，引入了ARK基准，旨在从知识领域和推理技能两个互补的角度评估多模态检索。

Method: ARK通过同时使用单模态和多模态查询和候选对象，评估16种异构视觉数据类型下的检索。大多数查询配对的有针对的硬Negative，需要多步推理，以避免评估中的快捷匹配。还评估了23个代表性的基于文本和多模态检索器。

Result: 研究发现，知识密集型和推理密集型检索之间存在显著差距，细微的视觉和空间推理成为耐久瓶颈。简单的增强如重新排名和修改能带来一致的改进，但仍有很大的提升空间。

Conclusion: ARK为多模态检索提供了一个新的评估框架，强调了超越简单匹配、利用专业知识和复杂推理的需求。

Abstract: Existing multimodal retrieval benchmarks largely emphasize semantic matching on daily-life images and offer limited diagnostics of professional knowledge and complex reasoning. To address this gap, we introduce ARK, a benchmark designed to analyze multimodal retrieval from two complementary perspectives: (i) knowledge domains (five domains with 17 subtypes), which characterize the content and expertise retrieval relies on, and (ii) reasoning skills (six categories), which characterize the type of inference over multimodal evidence required to identify the correct candidate. Specifically, ARK evaluates retrieval with both unimodal and multimodal queries and candidates, covering 16 heterogeneous visual data types. To avoid shortcut matching during evaluation, most queries are paired with targeted hard negatives that require multi-step reasoning. We evaluate 23 representative text-based and multimodal retrievers on ARK and observe a pronounced gap between knowledge-intensive and reasoning-intensive retrieval, with fine-grained visual and spatial reasoning emerging as persistent bottlenecks. We further show that simple enhancements such as re-ranking and rewriting yield consistent improvements, but substantial headroom remains.

</details>


### [53] [Kelix Technique Report](https://arxiv.org/abs/2602.09843)
*Boyang Ding,Chenglong Chu,Dunju Zang,Han Li,Jiangxia Cao,Kun Gai,Muhao Wei,Ruiming Tang,Shiyao Wang,Siyang Mao,Xinchen Luo,Yahui Liu,Zhixin Ling,Zhuoran Yang,Ziming Li,Chengru Song,Guorui Zhou,Guowang Zhang,Hao Peng,Hao Wang,Jiaxin Deng,Jin Ouyang,Jinghao Zhang,Lejian Ren,Qianqian Wang,Qigen Hu,Tao Wang,Xingmei Wang,Yiping Yang,Zixing Zhang,Ziqi Wang*

Main category: cs.CV

TL;DR: Kelix 是一种完全离散的自回归统一模型，旨在解决现有离散视觉标记方法中存在的信息损失问题，从而弥合离散和连续视觉表示的理解差距。


<details>
  <summary>Details</summary>
Motivation: 目前大多数视觉-语言模型依赖于离散文本标记与连续视觉变换器特征的混合接口，其监督主要来自文本，导致这些模型偏向理解而非充分利用大量非文本数据的自监督学习，特别是在视觉理解方面。因此，探索能够实现统一理解和生成的离散视觉标记方法变得非常重要。

Method: Kelix 通过引入一种新的采样和解码策略，能够更有效地利用离散视觉标记，从而提高视觉理解力。

Result: 实验结果显示，Kelix 在视觉理解任务上的表现优于现有的离散视觉标记模型，并且在某些情况下甚至接近连续特征 VLM 的表现。

Conclusion: Kelix 模型为全面实现离散和连续视觉表示之间的统一理解提供了有力的支持，为未来的研究奠定了基础。

Abstract: Autoregressive large language models (LLMs) scale well by expressing diverse tasks as sequences of discrete natural-language tokens and training with next-token prediction, which unifies comprehension and generation under self-supervision. Extending this paradigm to multimodal data requires a shared, discrete representation across modalities. However, most vision-language models (VLMs) still rely on a hybrid interface: discrete text tokens paired with continuous Vision Transformer (ViT) features. Because supervision is largely text-driven, these models are often biased toward understanding and cannot fully leverage large-scale self-supervised learning on non-text data. Recent work has explored discrete visual tokenization to enable fully autoregressive multimodal modeling, showing promising progress toward unified understanding and generation. Yet existing discrete vision tokens frequently lose information due to limited code capacity, resulting in noticeably weaker understanding than continuous-feature VLMs. We present Kelix, a fully discrete autoregressive unified model that closes the understanding gap between discrete and continuous visual representations.

</details>


### [54] [Reason-IAD: Knowledge-Guided Dynamic Latent Reasoning for Explainable Industrial Anomaly Detection](https://arxiv.org/abs/2602.09850)
*Peng Chen,Chao Huang,Yunkang Cao,Chengliang Liu,Wenqiang Wang,Mingbo Yang,Li Shen,Wenqi Ren,Xiaochun Cao*

Main category: cs.CV

TL;DR: Reason-IAD 提出了一种知识引导的动态潜在推理框架，增强了解释性工业异常检测，能够更准确地捕捉类别特定的异常。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态大型语言模型在处理特定领域的异常检测时，由于缺乏领域特定知识，难以精确识别细粒度的缺陷模式。

Method: Reason-IAD 采用了检索增强的知识模块和熵驱动的潜在推理机制，以及动态视觉注入策略，通过优化潜在思维令牌和选择性地加入最有信息量的图像片段来提升模型性能。

Result: 实验结果表明，Reason-IAD 在多项指标上超过了现有的顶尖方法，展示了其在解释性和检测精度上的优势。

Conclusion: 该研究提出的方法显著提升了工业异常检测的准确性和可解释性，提供了改进现有技术的可能途径，并已在 GitHub 上开源供社区使用。

Abstract: Industrial anomaly detection demands precise reasoning over fine-grained defect patterns. However, existing multimodal large language models (MLLMs), pretrained on general-domain data, often struggle to capture category-specific anomalies, thereby limiting both detection accuracy and interpretability. To address these limitations, we propose Reason-IAD, a knowledge-guided dynamic latent reasoning framework for explainable industrial anomaly detection. Reason-IAD comprises two core components. First, a retrieval-augmented knowledge module incorporates category-specific textual descriptions into the model input, enabling context-aware reasoning over domain-specific defects. Second, an entropy-driven latent reasoning mechanism conducts iterative exploration within a compact latent space using optimizable latent think tokens, guided by an entropy-based reward that encourages confident and stable predictions. Furthermore, a dynamic visual injection strategy selectively incorporates the most informative image patches into the latent sequence, directing the reasoning process toward regions critical for anomaly detection. Extensive experimental results demonstrate that Reason-IAD consistently outperforms state-of-the-art methods. The code will be publicly available at https://github.com/chenpeng052/Reason-IAD.

</details>


### [55] [SARS: A Novel Face and Body Shape and Appearance Aware 3D Reconstruction System extends Morphable Models](https://arxiv.org/abs/2602.09918)
*Gulraiz Khan,Kenneth Y. Wertheim,Kevin Pimbblet,Waqas Ahmed*

Main category: cs.CV

TL;DR: 该研究提出了一种名为SARS的系统，该系统能够从单张2D图像中提取人体和面部信息，利用3D Morphable Model (3DMM) 控制形态和外观的变化，从而重建人体全貌，解决了以往只关注面部全局结构的问题。


<details>
  <summary>Details</summary>
Motivation: 以前的研究大多关注人脸的全局几何结构，但忽略了年龄、性别等高阶特征以及面部的形状、曲线、凹陷和皱纹。引入SARS系统旨在解决这些高阶面部特征变化的重建问题。

Method: 该方法采用3D Morphable Models (3DMM)，结合身份和表情基础模型和面部特征描述符，通过调整参数来实现面部和身体特征的变化控制。系统设计为一个模块化的流程，可以从单一图像中提取面部和身体信息。

Result: SARS系统能够从单张图像中准确地重建3D人体模型，考虑到面部和身体的细节，如年龄、性别特征以及面部的精确形状。

Conclusion: 该系统通过引入3DMM和模块化设计，显著提升了3D人体重建的精度和细节，特别是在面部特征的重建上取得突破。

Abstract: Morphable Models (3DMMs) are a type of morphable model that takes 2D images as inputs and recreates the structure and physical appearance of 3D objects, especially human faces and bodies. 3DMM combines identity and expression blendshapes with a basic face mesh to create a detailed 3D model. The variability in the 3D Morphable models can be controlled by tuning diverse parameters. They are high-level image descriptors, such as shape, texture, illumination, and camera parameters. Previous research in 3D human reconstruction concentrated solely on global face structure or geometry, ignoring face semantic features such as age, gender, and facial landmarks characterizing facial boundaries, curves, dips, and wrinkles. In order to accommodate changes in these high-level facial characteristics, this work introduces a shape and appearance-aware 3D reconstruction system (named SARS by us), a c modular pipeline that extracts body and face information from a single image to properly rebuild the 3D model of the human full body.

</details>


### [56] [Free-GVC: Towards Training-Free Extreme Generative Video Compression with Temporal Coherence](https://arxiv.org/abs/2602.09868)
*Xiaoyue Ling,Chuqin Zhou,Chunyi Li,Yunuo Chen,Yuan Tian,Guo Lu,Wenjun Zhang*

Main category: cs.CV

TL;DR: Free-GVC为一种无需训练的生成视频压缩框架，它通过视频扩散先验引导的潜在轨迹压缩在GOP级别上对视频进行编码，并通过自适应质量控制模块和跨GOP对齐模块优化重建的一致性和视觉质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法在超低比特率下视觉质量和时间连贯性较差。

Method: Free-GVC框架将视频编码重新定义为由视频扩散先验引导的潜在轨迹压缩。它通过将视频段编码到紧凑的潜在空间并通过扩散轨迹逐步压缩来操作。引入了自适应质量控制模块，该模块实时构建速率-感知模型预测每个GOP的最佳扩散步骤。同时，跨GOP对齐模块通过帧重叠和潜在融合来减轻闪烁并增强时间连贯性。

Result: 实验显示，Free-GVC在DISTS上的平均BD-Rate降低达到了93.29%，并且用户研究进一步证实其在超低比特率下具有更好的视觉质量和时间连贯性。

Conclusion: Free-GVC是一种在超低比特率下仍能保持较高视觉质量和时间连贯性的生成视频压缩框架。

Abstract: Building on recent advances in video generation, generative video compression has emerged as a new paradigm for achieving visually pleasing reconstructions. However, existing methods exhibit limited exploitation of temporal correlations, causing noticeable flicker and degraded temporal coherence at ultra-low bitrates. In this paper, we propose Free-GVC, a training-free generative video compression framework that reformulates video coding as latent trajectory compression guided by a video diffusion prior. Our method operates at the group-of-pictures (GOP) level, encoding video segments into a compact latent space and progressively compressing them along the diffusion trajectory. To ensure perceptually consistent reconstruction across GOPs, we introduce an Adaptive Quality Control module that dynamically constructs an online rate-perception surrogate model to predict the optimal diffusion step for each GOP. In addition, an Inter-GOP Alignment module establishes frame overlap and performs latent fusion between adjacent groups, thereby mitigating flicker and enhancing temporal coherence. Experiments show that Free-GVC achieves an average of 93.29% BD-Rate reduction in DISTS over the latest neural codec DCVC-RT, and a user study further confirms its superior perceptual quality and temporal coherence at ultra-low bitrates.

</details>


### [57] [Monocular Normal Estimation via Shading Sequence Estimation](https://arxiv.org/abs/2602.09929)
*Zongrui Li,Xinhua Ma,Minghui Hu,Yunqing Zhao,Yingchen Yu,Qian Zheng,Chang Liu,Xudong Jiang,Song Bai*

Main category: cs.CV

TL;DR: 本文提出了一种新方法RoSE，通过将单目法表面法线估算问题转化为图像到视频的生成模型中预测光照序列问题。RoSE使用合成数据集MultiShade进行训练，表现出在真实世界基准数据集上的优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在单一RGB图像下难以准确重建几何细节，因为模型对于特定几何信息细微的变化并不敏感。RoSE通过将问题转化为预测光照序列的方法，增强了对几何信息的敏感度。

Method: RoSE利用图像到视频的生成模型预测多步光照序列，通过解简单的最小二乘问题将预测结果转化为法线图。

Result: RoSE在多个真实世界基准数据集上展示了优于现有方法的性能。

Conclusion: RoSE为单目三维表面法线估算提供了一种新的视角，通过预测复杂多变的光照情况增强了对几何细节的恢复能力。

Abstract: Monocular normal estimation aims to estimate the normal map from a single RGB image of an object under arbitrary lights. Existing methods rely on deep models to directly predict normal maps. However, they often suffer from 3D misalignment: while the estimated normal maps may appear to have a correct appearance, the reconstructed surfaces often fail to align with the geometric details. We argue that this misalignment stems from the current paradigm: the model struggles to distinguish and reconstruct varying geometry represented in normal maps, as the differences in underlying geometry are reflected only through relatively subtle color variations. To address this issue, we propose a new paradigm that reformulates normal estimation as shading sequence estimation, where shading sequences are more sensitive to various geometric information. Building on this paradigm, we present RoSE, a method that leverages image-to-video generative models to predict shading sequences. The predicted shading sequences are then converted into normal maps by solving a simple ordinary least-squares problem. To enhance robustness and better handle complex objects, RoSE is trained on a synthetic dataset, MultiShade, with diverse shapes, materials, and light conditions. Experiments demonstrate that RoSE achieves state-of-the-art performance on real-world benchmark datasets for object-based monocular normal estimation.

</details>


### [58] [Unbalanced optimal transport for robust longitudinal lesion evolution with registration-aware and appearance-guided priors](https://arxiv.org/abs/2602.09933)
*Melika Qahqaie,Dominik Neumann,Tobias Heimann,Andreas Maier,Veronika A. Zimmer*

Main category: cs.CV

TL;DR: 本文提出了一种基于不平衡最优运输(UOT)的注册感知匹配方法，能够处理肿瘤负荷变化和病变的出现、消失、合并或分裂，从而提高病变状态的召回率和病变图组件的F1得分。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于几何距离匹配难以处理病变的变化情况，本文旨在提出一种新的匹配方法来更好地处理这些变化。

Method: 本文方法采用了不平衡最优运输（UOT）技术，结合病变大小正常化几何、局部变形场雅可比的信任程度以及可选的补丁级外观一致性来构建运输成本，最终生成一个稀疏的运输计划，以实现病变的准确对应。

Result: 与仅基于距离的方法相比，本文方法在纵向CT数据上的边缘检测精度和召回率更高，病变状态的召回率更好，且在病变图组件的F1分数上表现更优。

Conclusion: 本文提出的方法能够更有效地匹配病变，从而有助于更准确地评估肿瘤患者的治疗响应。

Abstract: Evaluating lesion evolution in longitudinal CT scans of can cer patients is essential for assessing treatment response, yet establishing reliable lesion correspondence across time remains challenging. Standard bipartite matchers, which rely on geometric proximity, struggle when lesions appear, disappear, merge, or split. We propose a registration-aware matcher based on unbalanced optimal transport (UOT) that accommodates unequal lesion mass and adapts priors to patient-level tumor-load changes. Our transport cost blends (i) size-normalized geometry, (ii) local registration trust from the deformation-field Jacobian, and (iii) optional patch-level appearance consistency. The resulting transport plan is sparsified by relative pruning, yielding one-to-one matches as well as new, disappearing, merging, and splitting lesions without retraining or heuristic rules. On longitudinal CT data, our approach achieves consistently higher edge-detection precision and recall, improved lesion-state recall, and superior lesion-graph component F1 scores versus distance-only baselines.

</details>


### [59] [MVISTA-4D: View-Consistent 4D World Model with Test-Time Action Inference for Robotic Manipulation](https://arxiv.org/abs/2602.09878)
*Jiaxu Wang,Yicheng Jiang,Tianlun He,Jingkai Sun,Qiang Zhang,Junhao He,Jiahang Cao,Zesen Gan,Mingyuan Sun,Qiming Shao,Xiangyu Yue*

Main category: cs.CV

TL;DR: 本文提出了一种新的端到端4D世界模型，能够通过单一视角的RGBD观测生成多视角的RGBD图像，并结合逆动力学方法优化最终的执行动作。


<details>
  <summary>Details</summary>
Motivation: 现有的基于世界模型的方法大多只能进行图像级预测或部分3D几何推理，难以完整预测4D场景动力学。本文旨在构建一个能够生成完整3D结构的4D世界模型。

Method: 该模型设计了跨视图和跨模态特征融合机制，以确保RGB和深度之间的一致性，并通过从生成模型反向传播的方法推断出与预测未来最匹配的轨迹级潜在变量，从而优化最终的执行动作。

Result: 实验结果表明，该模型在4D场景生成和下游操作任务上表现优秀，并且消融实验提供了有关关键设计选择的实际见解。

Conclusion: 该工作为机器人操作提供了一个新的方法，通过多视角深度信息的生成和逆动力学优化策略实现更准确的动作执行。

Abstract: World-model-based imagine-then-act becomes a promising paradigm for robotic manipulation, yet existing approaches typically support either purely image-based forecasting or reasoning over partial 3D geometry, limiting their ability to predict complete 4D scene dynamics. This work proposes a novel embodied 4D world model that enables geometrically consistent, arbitrary-view RGBD generation: given only a single-view RGBD observation as input, the model imagines the remaining viewpoints, which can then be back-projected and fused to assemble a more complete 3D structure across time. To efficiently learn the multi-view, cross-modality generation, we explicitly design cross-view and cross-modality feature fusion that jointly encourage consistency between RGB and depth and enforce geometric alignment across views. Beyond prediction, converting generated futures into actions is often handled by inverse dynamics, which is ill-posed because multiple actions can explain the same transition. We address this with a test-time action optimization strategy that backpropagates through the generative model to infer a trajectory-level latent best matching the predicted future, and a residual inverse dynamics model that turns this trajectory prior into accurate executable actions. Experiments on three datasets demonstrate strong performance on both 4D scene generation and downstream manipulation, and ablations provide practical insights into the key design choices.

</details>


### [60] [Bladder Vessel Segmentation using a Hybrid Attention-Convolution Framework](https://arxiv.org/abs/2602.09949)
*Franziska Krauß,Matthias Ege,Zoltan Lovasz,Albrecht Bartz-Schmidt,Igor Tsaur,Oliver Sawodny,Carina Veil*

Main category: cs.CV

TL;DR: 该研究提出了一种新的Hybrid Attention-Convolution (HAC)架构，结合了Transformer捕捉血管全局拓扑信息和CNN学习残差细化图来精准恢复细血管细节，通过优化训练数据和物理感知预训练解决领域特定的复杂性，该方法在BlaVeS数据集上取得了高精度和优越的精确度。


<details>
  <summary>Details</summary>
Motivation: 膀胱缺乏稳定的定位特征，而尿路上皮癌随访中的导航依赖于有创的膀胱镜检查，这要求精确分割血管以进行导航，现有的分割方法难以应对膀胱镜图像的不足，特别是动态背景带来的挑战。

Method: 提出了一种Hybrid Attention-Convolution (HAC)架构，利用Transformer捕捉血管的全局拓扑信息，通过优化后处理方法排除短分支和终端分支，使用优化后的高质量标注数据进行训练，并结合自监督策略通过物理感知预训练增加数据量。

Result: 该方法在BlaVeS数据集上的实验结果表明，其能够达到高准确度（0.94）、高精度（0.61）和高clDice（0.66），并且能够有效抑制来自动态出现和消失的黏膜皱褶的假阳性，从而提高了临床导航的可靠性。

Conclusion: Hybrid Attention-Convolution (HAC)架构为尿路上皮癌随访中的自动血管段提供了有力支持，其稳定性提高了临床操作的可靠性，具有重要的临床应用价值。

Abstract: Urinary bladder cancer surveillance requires tracking tumor sites across repeated interventions, yet the deformable and hollow bladder lacks stable landmarks for orientation. While blood vessels visible during endoscopy offer a patient-specific "vascular fingerprint" for navigation, automated segmentation is challenged by imperfect endoscopic data, including sparse labels, artifacts like bubbles or variable lighting, continuous deformation, and mucosal folds that mimic vessels. State-of-the-art vessel segmentation methods often fail to address these domain-specific complexities. We introduce a Hybrid Attention-Convolution (HAC) architecture that combines Transformers to capture global vessel topology prior with a CNN that learns a residual refinement map to precisely recover thin-vessel details. To prioritize structural connectivity, the Transformer is trained on optimized ground truth data that exclude short and terminal branches. Furthermore, to address data scarcity, we employ a physics-aware pretraining, that is a self-supervised strategy using clinically grounded augmentations on unlabeled data. Evaluated on the BlaVeS dataset, consisting of endoscopic video frames, our approach achieves high accuracy (0.94) and superior precision (0.61) and clDice (0.66) compared to state-of-the-art medical segmentation models. Crucially, our method successfully suppresses false positives from mucosal folds that dynamically appear and vanish as the bladder fills and empties during surgery. Hence, HAC provides the reliable structural stability required for clinical navigation.

</details>


### [61] [AdaTSQ: Pushing the Pareto Frontier of Diffusion Transformers via Temporal-Sensitivity Quantization](https://arxiv.org/abs/2602.09883)
*Shaoqiu Zhang,Zizhong Ding,Kaicheng Yang,Junyi Wu,Xianglong Yan,Xi Li,Bingnan Duan,Jianping Fang,Yulun Zhang*

Main category: cs.CV

TL;DR: AdaTSQ 提出了一种新的后训练量化框架，通过利用扩散模型的时间敏感性，实现了效率与质量的平衡。


<details>
  <summary>Details</summary>
Motivation: 扩散变换器 (DiTs) 虽然在图像和视频生成上表现出色，但由于计算和内存的高需求，限制了其在边缘设备上的应用。现有的后训练量化方法在应用于 DiTs 时效果不佳，因此需要创新的方法来优化 DiTs 的效率。

Method: AdaTSQ 提出了两种主要方法：一种是基于 Pareto 的时间动态比特宽度分配策略，通过约束路径搜索和端到端重建误差来动态分配每个时间步的比特宽度；另一种是基于 Fisher 信息的时间校准机制，优先校准具有高敏感性的时间步。

Result: 实验结果表明，AdaTSQ 在 Flux-Dev、Flux-Schnell、Z-Image、Wan2.1 四种高级 DiTs 上的表现优于 SVDQuant 和 ViDiT-Q 等现有方法。

Conclusion: AdaTSQ 为 DiTs 的后训练量化提供了创新的策略，实现了在保证质量的同时提高效率的目标，展示了其在边缘设备上的应用潜力。

Abstract: Diffusion Transformers (DiTs) have emerged as the state-of-the-art backbone for high-fidelity image and video generation. However, their massive computational cost and memory footprint hinder deployment on edge devices. While post-training quantization (PTQ) has proven effective for large language models (LLMs), directly applying existing methods to DiTs yields suboptimal results due to the neglect of the unique temporal dynamics inherent in diffusion processes. In this paper, we propose AdaTSQ, a novel PTQ framework that pushes the Pareto frontier of efficiency and quality by exploiting the temporal sensitivity of DiTs. First, we propose a Pareto-aware timestep-dynamic bit-width allocation strategy. We model the quantization policy search as a constrained pathfinding problem. We utilize a beam search algorithm guided by end-to-end reconstruction error to dynamically assign layer-wise bit-widths across different timesteps. Second, we propose a Fisher-guided temporal calibration mechanism. It leverages temporal Fisher information to prioritize calibration data from highly sensitive timesteps, seamlessly integrating with Hessian-based weight optimization. Extensive experiments on four advanced DiTs (e.g., Flux-Dev, Flux-Schnell, Z-Image, and Wan2.1) demonstrate that AdaTSQ significantly outperforms state-of-the-art methods like SVDQuant and ViDiT-Q. Our code will be released at https://github.com/Qiushao-E/AdaTSQ.

</details>


### [62] [A benchmark for video-based laparoscopic skill analysis and assessment](https://arxiv.org/abs/2602.09927)
*Isabel Funke,Sebastian Bodenstedt,Felix von Bechtolsheim,Florian Oehme,Michael Maruschke,Stefanie Herrlich,Jürgen Weitz,Marius Distler,Sören Torge Mees,Stefanie Speidel*

Main category: cs.CV

TL;DR: 该研究提出了一个包含1270段立体视频的LASANA数据集，用于评估和识别腹腔镜手术技能和错误。此数据集可以促进针对视频技能评估和错误识别方法的基准测试。


<details>
  <summary>Details</summary>
Motivation: 当前许多腹腔镜手术培训视频缺乏详细的结构化技能评分和错误标记，限制了DEE模型的发展和评估。

Method: 研究团队创造了LASANA数据集，包含四个基本手术任务的1270段立体视频。每段视频都经过三位专家的结构化评分，并标记特定任务的错误。研究还提供了基准结果，以便后续实验进行比较。

Result: 研究提供了基于立体视频的手术技能评估和错误识别方法的基准点。

Conclusion: 该数据集能够促进现有和新型评估方法的发展和比较，提高腹腔镜手术培训的效率和质量。

Abstract: Laparoscopic surgery is a complex surgical technique that requires extensive training. Recent advances in deep learning have shown promise in supporting this training by enabling automatic video-based assessment of surgical skills. However, the development and evaluation of deep learning models is currently hindered by the limited size of available annotated datasets. To address this gap, we introduce the Laparoscopic Skill Analysis and Assessment (LASANA) dataset, comprising 1270 stereo video recordings of four basic laparoscopic training tasks. Each recording is annotated with a structured skill rating, aggregated from three independent raters, as well as binary labels indicating the presence or absence of task-specific errors. The majority of recordings originate from a laparoscopic training course, thereby reflecting a natural variation in the skill of participants. To facilitate benchmarking of both existing and novel approaches for video-based skill assessment and error recognition, we provide predefined data splits for each task. Furthermore, we present baseline results from a deep learning model as a reference point for future comparisons.

</details>


### [63] [Fake-HR1: Rethinking reasoning of vision language model for synthetic image detection](https://arxiv.org/abs/2602.10042)
*Changjiang Jiang,Xinkuan Sha,Fengchang Yu,Jingjing Liu,Jian Liu,Mingqi Fang,Chenfeng Zhang,Wei Lu*

Main category: cs.CV

TL;DR: 该研究提出了一种名为Fake-HR1的混合推理模型，能够在保证高效的同时提升合成图像检测能力。


<details>
  <summary>Details</summary>
Motivation: 当前的图像生成检测模型在处理伪造图像时，会因为冗余的推理过程而消耗大量资源，影响检测效率。为解决这一问题，研究旨在开发一种能够根据生成检测任务特性自适应决定是否进行推理的新方法。

Method: 研究设计了一种两阶段训练框架：首先通过混合精细调整(HFT)进行冷启动初始化，然后利用混合推理组策略优化(HGRPO)进行在线强化学习，学习在不同场景下选择合适的推理模式。

Result: 实验结果表明，Fake-HR1能够在不同类型的查询中自适应地执行推理，相较于现有模型在推理能力和生成检测性能上均有提升，并显著提高了响应效率。

Conclusion: 该研究提出的Fake-HR1模型证明了混合推理在图像生成检测任务中的有效性，为提升模型性能和响应效率提供了新思路。

Abstract: Recent studies have demonstrated that incorporating Chain-of-Thought (CoT) reasoning into the detection process can enhance a model's ability to detect synthetic images. However, excessively lengthy reasoning incurs substantial resource overhead, including token consumption and latency, which is particularly redundant when handling obviously generated forgeries. To address this issue, we propose Fake-HR1, a large-scale hybrid-reasoning model that, to the best of our knowledge, is the first to adaptively determine whether reasoning is necessary based on the characteristics of the generative detection task. To achieve this, we design a two-stage training framework: we first perform Hybrid Fine-Tuning (HFT) for cold-start initialization, followed by online reinforcement learning with Hybrid-Reasoning Grouped Policy Optimization (HGRPO) to implicitly learn when to select an appropriate reasoning mode. Experimental results show that Fake-HR1 adaptively performs reasoning across different types of queries, surpassing existing LLMs in both reasoning ability and generative detection performance, while significantly improving response efficiency.

</details>


### [64] [GeoFormer: A Swin Transformer-Based Framework for Scene-Level Building Height and Footprint Estimation from Sentinel Imagery](https://arxiv.org/abs/2602.09932)
*Han Jinzhen,JinByeong Lee,JiSung Kim,MinKyung Cho,DaHee Kim,HongSik Yun*

Main category: cs.CV

TL;DR: GeoFormer 利用了 Sentinel-1/2 图像和公开 DEM 数据，在 100 米网格上同时估计建筑物高度和足迹，实现了优于最强 CNN 基准的精度，特别是在跨大陆迁移中保持了较低的误差。


<details>
  <summary>Details</summary>
Motivation: 现有的三维城市数据不足，依赖于专有传感器或跨城市的有限推广能力，GeoFormer 提出了一个开源的 Swin Transformer 框架来填充这一空白，提高三维建模的准确性。

Method: GeoFormer 使用了一个基于 Swin Transformer 的框架，利用 Sentinel-1/2 图像和公开的 DEM 数据，在 100 米的网格上同时估计建筑物高度和足迹。通过地理块分割策略确保了训练集和测试集之间的严格空间独立性。

Result: 在 54 个不同城市的评估中，GeoFormer 在建筑物高度估计中的 RMSE 为 3.19 米，在建筑物足迹估计中的 RMSE 为 0.05，比最强的 CNN 基准提高了 7.5% 和 15.3%。此外，它在跨大陆迁移中保持了低于 3.5 米的建筑物高度 RMSE。

Conclusion: GeoFormer 通过利用开放数据资源，在不依赖昂贵的专有传感器的情况下提升了三维城市数据的准确性，并展示了其实用性，所有代码、权重和全球产品均已公开发布。

Abstract: Accurate three-dimensional urban data are critical for climate modelling, disaster risk assessment, and urban planning, yet remain scarce due to reliance on proprietary sensors or poor cross-city generalisation. We propose GeoFormer, an open-source Swin Transformer framework that jointly estimates building height (BH) and footprint (BF) on a 100 m grid using only Sentinel-1/2 imagery and open DEM data. A geo-blocked splitting strategy ensures strict spatial independence between training and test sets. Evaluated over 54 diverse cities, GeoFormer achieves a BH RMSE of 3.19 m and a BF RMSE of 0.05, improving 7.5% and 15.3% over the strongest CNN baseline, while maintaining under 3.5 m BH RMSE in cross-continent transfer. Ablation studies confirm that DEM is indispensable for height estimation and that optical reflectance dominates over SAR, though multi-source fusion yields the best overall accuracy. All code, weights, and global products are publicly released.

</details>


### [65] [Causality in Video Diffusers is Separable from Denoising](https://arxiv.org/abs/2602.10095)
*Xingjian Bai,Guande He,Zhengqi Li,Eli Shechtman,Xun Huang,Zongze Wu*

Main category: cs.CV

TL;DR: 该研究提出了一种新的架构Separable Causal Diffusion (SCD)，将时间推理和框架内渲染明确分离，从而提高了生成质量和参数效率。


<details>
  <summary>Details</summary>
Motivation: 当前的因果扩散模型将时间推理与多步去噪过程交织在一起，导致计算冗余。因此，需要设计一种方法，能够清晰地分离视频预测中的时间推理和渲染过程，以提高计算效率。

Method: 通过系统性地研究自回归视频扩散模型，研究人员发现早期层在去噪步骤间产生非常相似的特征，而深层层则表现出稀疏的跨帧注意，主要执行帧内渲染。基于这些发现，提出的SCD架构包括因果变换器编码器进行每帧的时间推理，以及轻量级的扩散解码器进行多步帧内渲染。

Result: 在合成和现实基准上的预训练和后训练任务中，SCD在保持或超越先进因果扩散基线生成质量的同时，显著提高了吞吐量和每帧延迟。

Conclusion: 提出的SCD架构在保持生成质量的同时，提高了时间和空间效率，对时空生成任务具有广泛的应用前景。

Abstract: Causality -- referring to temporal, uni-directional cause-effect relationships between components -- underlies many complex generative processes, including videos, language, and robot trajectories. Current causal diffusion models entangle temporal reasoning with iterative denoising, applying causal attention across all layers, at every denoising step, and over the entire context. In this paper, we show that the causal reasoning in these models is separable from the multi-step denoising process. Through systematic probing of autoregressive video diffusers, we uncover two key regularities: (1) early layers produce highly similar features across denoising steps, indicating redundant computation along the diffusion trajectory; and (2) deeper layers exhibit sparse cross-frame attention and primarily perform intra-frame rendering. Motivated by these findings, we introduce Separable Causal Diffusion (SCD), a new architecture that explicitly decouples once-per-frame temporal reasoning, via a causal transformer encoder, from multi-step frame-wise rendering, via a lightweight diffusion decoder. Extensive experiments on both pretraining and post-training tasks across synthetic and real benchmarks show that SCD significantly improves throughput and per-frame latency while matching or surpassing the generation quality of strong causal diffusion baselines.

</details>


### [66] [Olaf-World: Orienting Latent Actions for Video World Modeling](https://arxiv.org/abs/2602.10104)
*Yuxin Jiang,Yuchao Gu,Ivor W. Tsang,Mike Zheng Shou*

Main category: cs.CV

TL;DR: 该研究提出了一种序列级别的控制效果对齐目标Seq$\Delta$-REPA，并基于此提出了从大规模被动视频中预训练动作条件化视频世界模型的Olaf-World管道，从而学习到一个更有结构的潜在动作空间，实现更强的零样本动作转移和更高效的新控制界面适应。


<details>
  <summary>Details</summary>
Motivation: 当前方法在无标注视频中学习潜动作时容易出现语境间的动作语义混淆，因此需要一种能够跨语境对齐动作语义的方法。

Method: 提出了一种序列级别的控制效果对齐目标Seq$\Delta$-REPA，该目标将潜在动作与冻结的自监督视频编码器获取的时间特征差异进行对齐。同时，构建了从大规模被动视频中预训练动作条件化视频世界模型的Olaf-World管道。

Result: 实验表明，该方法能够学习到一个更结构化的潜在动作空间，从而实现更强的零样本动作转移和更高效的新控制界面适应。

Conclusion: 研究证明了Seq$\Delta$-REPA目标和Olaf-World管道的有效性，为无监督动作学习领域提供了新的解决方案。

Abstract: Scaling action-controllable world models is limited by the scarcity of action labels. While latent action learning promises to extract control interfaces from unlabeled video, learned latents often fail to transfer across contexts: they entangle scene-specific cues and lack a shared coordinate system. This occurs because standard objectives operate only within each clip, providing no mechanism to align action semantics across contexts. Our key insight is that although actions are unobserved, their semantic effects are observable and can serve as a shared reference. We introduce Seq$Δ$-REPA, a sequence-level control-effect alignment objective that anchors integrated latent action to temporal feature differences from a frozen, self-supervised video encoder. Building on this, we present Olaf-World, a pipeline that pretrains action-conditioned video world models from large-scale passive video. Extensive experiments demonstrate that our method learns a more structured latent action space, leading to stronger zero-shot action transfer and more data-efficient adaptation to new control interfaces than state-of-the-art baselines.

</details>


### [67] [Learning to Detect Baked Goods with Limited Supervision](https://arxiv.org/abs/2602.09979)
*Thomas H. Schmitt,Maximilian Bundscherer,Tobias Bocklet*

Main category: cs.CV

TL;DR: 该研究提出了一种通过自动化图像识别来监测剩余烘焙产品的方案，以优化未来生产。面对德国烘焙品种多样性高的问题，采用弱监督训练方法并结合视频帧增强，实现了在非理想部署条件下的高性能模型。


<details>
  <summary>Details</summary>
Motivation: 在德国食品工业，特别是烘焙行业，新鲜烘焙产品具有极短的保质期。通过自动化监测剩余产品，能够降低成本、提高准确性并优化流程。

Method: 研究采用了包含弱监督训练和视频帧增强的两阶段训练流程，首先结合OWLv2和Grounding DINO进行图像级监督训练，再通过Segment Anything 2作为伪标签传播模型增强模型的视点鲁棒性。选择YOLOv11作为检测模型，因其在速度和准确性之间提供了良好的平衡。

Result: 仅依赖图像级监督的训练，在弱条件下实现了91%的平均精度。通过伪标签进一步优化后，模型性能提高了19.3%，即使在非理想部署条件下也超越了全监督模型。

Conclusion: 该研究展示了在数据稀缺和任务专业化的工业环境中有效应用计算机视觉技术的方法，特别是在烘焙行业的对象检测任务中取得了显著成果。

Abstract: Monitoring leftover products provides valuable insights that can be used to optimize future production. This is especially important for German bakeries because freshly baked goods have a very short shelf life. Automating this process can reduce labor costs, improve accuracy, and streamline operations. We propose automating this process using an object detection model to identify baked goods from images. However, the large diversity of German baked goods makes fully supervised training prohibitively expensive and limits scalability. Although open-vocabulary detectors (e.g., OWLv2, Grounding DINO) offer lexibility, we demonstrate that they are insufficient for our task. While motivated by bakeries, our work addresses the broader challenges of deploying computer vision in industries, where tasks are specialized and annotated datasets are scarce. We compile dataset splits with varying supervision levels, covering 19 classes of baked goods. We propose two training workflows to train an object detection model with limited supervision. First, we combine OWLv2 and Grounding DINO localization with image-level supervision to train the model in a weakly supervised manner. Second, we improve viewpoint robustness by fine-tuning on video frames annotated using Segment Anything 2 as a pseudo-label propagation model. Using these workflows, we train YOLOv11 for our detection task due to its favorable speed accuracy tradeoff. Relying solely on image-level supervision, the model achieves a mean Average Precision (mAP) of 0.91. Finetuning with pseudo-labels raises model performance by 19.3% under non-ideal deployment conditions. Combining these workflows trains a model that surpasses our fully-supervised baseline model under non-ideal deployment conditions, despite relying only on image-level supervision.

</details>


### [68] [Efficient Special Stain Classification](https://arxiv.org/abs/2602.09989)
*Oskar Thaeter,Christian Grashei,Anette Haas,Elisa Schmoeckel,Han Li,Peter J. Schüffler*

Main category: cs.CV

TL;DR: 本文展示了使用全视野图像和轻量级缩略图方法来自动分类组织切片染色剂，其中多实例学习在内部测试数据中表现最好，但在外部TCGA数据中，缩略图模型表现更佳且比多实例学习方法提高了两倍的处理速度。


<details>
  <summary>Details</summary>
Motivation: 为了提高病理学工作中染色剂分类的准确性与速度，特别是在大规模临床档案和计算病理学数据集的质量控制中。

Method: 该研究比较了两种方法：多实例学习（MIL）和轻量级的缩略图方法。使用全视野图像进行MIL分类，而缩略图方法则是先提取低分辨率的缩略图进行快速分类。

Result: 在内部测试数据中，MIL方法表现出更高的性能（16类宏F1分数：0.941；14类合并后的宏F1分数：0.969），而缩略图方法则保持竞争优势（分别为0.897和0.953）。在外部TCGA数据集上，缩略图模型的加权F1分数更高（0.843 vs. 0.807）。缩略图模型将处理速度提高了两个数量级。

Conclusion: 研究得出结论，缩略图方法为病理学工作流程中常规的视觉质量控制提供了一个可扩展且稳健的解决方案。

Abstract: Stains are essential in histopathology to visualize specific tissue characteristics, with Haematoxylin and Eosin (H&E) serving as the clinical standard. However, pathologists frequently
  utilize a variety of special stains for the diagnosis of specific morphologies. Maintaining accurate metadata for these slides is critical for quality control in clinical archives and for
  the integrity of computational pathology datasets. In this work, we compare two approaches for automated classification of stains using whole slide images, covering the 14 most commonly
  used special stains in our institute alongside standard and frozen-section H&E. We evaluate a Multi-Instance Learning (MIL) pipeline and a proposed lightweight thumbnail-based approach.
  On internal test data, MIL achieved the highest performance (macro F1: 0.941 for 16 classes; 0.969 for 14 merged classes), while the thumbnail approach remained competitive (0.897 and
  0.953, respectively). On external TCGA data, the thumbnail model generalized best (weighted F1: 0.843 vs. 0.807 for MIL). The thumbnail approach also increased throughput by two orders of
  magnitude (5.635 vs. 0.018 slides/s for MIL with all patches). We conclude that thumbnail-based classification provides a scalable and robust solution for routine visual quality control
  in digital pathology workflows.

</details>


### [69] [Faster-GS: Analyzing and Improving Gaussian Splatting Optimization](https://arxiv.org/abs/2602.09999)
*Florian Hahlbohm,Linus Franke,Martin Eisemann,Marcus Magnor*

Main category: cs.CV

TL;DR: Faster-GS 提供了一个在保留视觉质量的同时比以往方法快 5 倍的新型 3D 超级采样系统，并且可以应用于 4D 超方体重建。


<details>
  <summary>Details</summary>
Motivation: 当前 3D 超级采样研究存在碎片化问题，导致难以公平比较。本文致力于整合并评估最有效的策略，并提出新的优化方法，以推动 3D 超级采样领域的进步。

Method: 本文提出了 Faster-GS，一种融合了多种优化策略和新方法的系统。

Result: 实验结果表明，Faster-GS 在保持视觉质量的同时训练速度提升了 5 倍，并且该方法在非刚性场景优化方面也表现出色。

Conclusion: Faster-GS 建立了一种成本效益高且资源高效的方法，作为 3D 超级采样优化的新基准，同时可以拓展到 4D 超方体重建。

Abstract: Recent advances in 3D Gaussian Splatting (3DGS) have focused on accelerating optimization while preserving reconstruction quality. However, many proposed methods entangle implementation-level improvements with fundamental algorithmic modifications or trade performance for fidelity, leading to a fragmented research landscape that complicates fair comparison. In this work, we consolidate and evaluate the most effective and broadly applicable strategies from prior 3DGS research and augment them with several novel optimizations. We further investigate underexplored aspects of the framework, including numerical stability, Gaussian truncation, and gradient approximation. The resulting system, Faster-GS, provides a rigorously optimized algorithm that we evaluate across a comprehensive suite of benchmarks. Our experiments demonstrate that Faster-GS achieves up to 5$\times$ faster training while maintaining visual quality, establishing a new cost-effective and resource efficient baseline for 3DGS optimization. Furthermore, we demonstrate that optimizations can be applied to 4D Gaussian reconstruction, leading to efficient non-rigid scene optimization.

</details>


### [70] [Simple Image Processing and Similarity Measures Can Link Data Samples across Databases through Brain MRI](https://arxiv.org/abs/2602.10043)
*Gaurang Sharma,Harri Polonen,Juha Pajula,Jutta Suksi,Jussi Tohka*

Main category: cs.CV

TL;DR: 本文通过标准预处理和图像相似性计算，展示了连接个体颅骨剥离的T1加权MRI的可能性，即使在其他标识符可用时也可能导致重新识别。


<details>
  <summary>Details</summary>
Motivation: 研究者指出，在去标识化的脑MRI数据中仍存在隐私风险，尽管已去除了潜在标识符，在某些条件下仍可匹配来自同一参与者的其他数据库中的MRI。

Method: 该研究采用标准预处理步骤，随后进行图像相似性计算，以实现跨不同时间间隔、扫描器类型、空间分辨率和采集协议的数据匹配。

Result: 研究表明，即使模拟了潜在的认知衰退情况，仍能实现几乎完美的匹配准确度。这在不同场景下进行了验证，展示了重新识别的可行性。

Conclusion: 研究结论认为，当前的数据共享政策需要更加谨慎，并推动制定前瞻性政策来保护医疗数据共享的隐私性。

Abstract: Head Magnetic Resonance Imaging (MRI) is routinely collected and shared for research under strict regulatory frameworks. These frameworks require removing potential identifiers before sharing. But, even after skull stripping, the brain parenchyma contains unique signatures that can match other MRIs from the same participants across databases, posing a privacy risk if additional data features are available. Current regulatory frameworks often mandate evaluating such risks based on the assessment of a certain level of reasonableness. Prior studies have already suggested that a brain MRI could enable participant linkage, but they have relied on training-based or computationally intensive methods.
  Here, we demonstrate that linking an individual's skull-stripped T1-weighted MRI, which may lead to re-identification if other identifiers are available, is possible using standard preprocessing followed by image similarity computation. Nearly perfect linkage accuracy was achieved in matching data samples across various time intervals, scanner types, spatial resolutions, and acquisition protocols, despite potential cognitive decline, simulating MRI matching across databases. These results aim to contribute meaningfully to the development of thoughtful, forward-looking policies in medical data sharing.

</details>


### [71] [Conformal Prediction Sets for Instance Segmentation](https://arxiv.org/abs/2602.10045)
*Kerri Lu,Dan M. Kluger,Stephen Bates,Sherrie Wang*

Main category: cs.CV

TL;DR: 该研究提出了一种通过引入形式化预测算法来生成实例分割的自适应置信集的方法，以弥补现有模型在不确定性量度上的不足。该算法能够在给定图像和像素坐标查询时生成以高IoU与真实目标实例掩码匹配的预测集，并且保证一定概率至少有一个预测是正确的。实验表明，预测集的大小会根据查询难度变化，能够达到目标覆盖度，优于现有的如Learn Then Test、Conformal Risk Control和形态膨胀基方法。


<details>
  <summary>Details</summary>
Motivation: 当前实例分割模型在平均预测方面表现优异，但在不确定性量度上缺乏原则性量化。研究动机在于解决这一限制，提出一种基于形式化预测算法的方法来生成实例分割的自适应置信集。

Method: 该研究通过设计一种形式化预测算法来解决现有模型在不确定性量度上的不足。该算法能够在给定图像和像素坐标查询时，生成以高IoU与实际目标实例掩码匹配的预测集，该预测集有一定的概率保证至少包含一个准确预测。

Result: 该方法应用于农业区域划分、细胞分割和车辆检测等多个实例分割任务中。实验结果显示，该预测集根据查询难度变化大小，能够达到目标覆盖度，并优于现有基准方法，如Learn Then Test、Conformal Risk Control和形态膨胀基方法。

Conclusion: 研究验证了形式化预测算法生成的自适应置信集的有效性，为实例分割模型在不确定性量化方面提供了新的解决方案。并且提供了算法的渐近性和有限样本保证版本。

Abstract: Current instance segmentation models achieve high performance on average predictions, but lack principled uncertainty quantification: their outputs are not calibrated, and there is no guarantee that a predicted mask is close to the ground truth. To address this limitation, we introduce a conformal prediction algorithm to generate adaptive confidence sets for instance segmentation. Given an image and a pixel coordinate query, our algorithm generates a confidence set of instance predictions for that pixel, with a provable guarantee for the probability that at least one of the predictions has high Intersection-Over-Union (IoU) with the true object instance mask. We apply our algorithm to instance segmentation examples in agricultural field delineation, cell segmentation, and vehicle detection. Empirically, we find that our prediction sets vary in size based on query difficulty and attain the target coverage, outperforming existing baselines such as Learn Then Test, Conformal Risk Control, and morphological dilation-based methods. We provide versions of the algorithm with asymptotic and finite sample guarantees.

</details>


### [72] [Spatio-Temporal Attention for Consistent Video Semantic Segmentation in Automated Driving](https://arxiv.org/abs/2602.10052)
*Serin Varghese,Kevin Ross,Fabian Hueger,Kira Maag*

Main category: cs.CV

TL;DR: 提出了一种空间-时间注意机制（STA），通过结合多帧上下文扩展了Transformer的注意力块，从而在视频语义分割中提供健壮的时间特征表示。在Cityscapes和BDD100k数据集上的广泛评估表明，STA在时间一致性度量中提升了9.20个百分点，在平均交并比上最多提升了1.76个百分点。


<details>
  <summary>Details</summary>
Motivation: 现有模型处理视频帧独立，无法利用时间一致性提高动态场景中的准确性和稳定性。

Method: 提出了空间-时间注意机制（STA），通过修改标准自我注意来处理时空特征序列，同时保持计算效率。

Result: STA适用各种不同的Transformer架构，轻量级和大规模模型均有效。在Cityscapes和BDD100k数据集上的评估显示，相比单帧基线，STA在时间一致性度量上提高了9.20个百分点，在平均交并比上最多提升了1.76个百分点。

Conclusion: STA作为一种有效的架构增强，适用于基于视频的语义分割应用。

Abstract: Deep neural networks, especially transformer-based architectures, have achieved remarkable success in semantic segmentation for environmental perception. However, existing models process video frames independently, thus failing to leverage temporal consistency, which could significantly improve both accuracy and stability in dynamic scenes. In this work, we propose a Spatio-Temporal Attention (STA) mechanism that extends transformer attention blocks to incorporate multi-frame context, enabling robust temporal feature representations for video semantic segmentation. Our approach modifies standard self-attention to process spatio-temporal feature sequences while maintaining computational efficiency and requiring minimal changes to existing architectures. STA demonstrates broad applicability across diverse transformer architectures and remains effective across both lightweight and larger-scale models. A comprehensive evaluation on the Cityscapes and BDD100k datasets shows substantial improvements of 9.20 percentage points in temporal consistency metrics and up to 1.76 percentage points in mean intersection over union compared to single-frame baselines. These results demonstrate STA as an effective architectural enhancement for video-based semantic segmentation applications.

</details>


### [73] [Can Image Splicing and Copy-Move Forgery Be Detected by the Same Model? Forensim: An Attention-Based State-Space Approach](https://arxiv.org/abs/2602.10079)
*Soumyaroop Nandi,Prem Natarajan*

Main category: cs.CV

TL;DR: Forensim 是一种基于注意力机制的状态空间框架，用于图像伪造检测，可以通过联合定位目标和源区域来检测复制模式，从而更准确地理解上下文。Forensim 模型能够输出三种类别的掩码，并支持在统一架构中检测剪接和复制粘贴伪造。此外，还提出了新的数据集 CMFD-Anything，以克服现有复制粘贴伪造数据集的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统的图像伪造检测方法通常依赖于伪迹线索来检测拼接或伪造区域，但无法捕捉复制模式，Forensim 提出了一种新的方法来检测并理解这些模式，特别是在需要理解上下文的场景中更为重要。

Method: Forensim 使用一种基于视觉状态空间模型的方法，结合归一化注意力图来识别内部相似性，并使用区域基于的块注意力模块来区分篡改区域。这种设计使得模型可以在统一架构下进行端到端的训练和精确定位。

Result: Forensim 在标准基准测试中达到了最先进的性能。

Conclusion: Forensim 通过提供一种新的统一架构来检测图像伪造，能够更准确地本地化目标和源区域，并通过引入新的数据集 CMFD-Anything 来提高伪造检测的精度。

Abstract: We introduce Forensim, an attention-based state-space framework for image forgery detection that jointly localizes both manipulated (target) and source regions. Unlike traditional approaches that rely solely on artifact cues to detect spliced or forged areas, Forensim is designed to capture duplication patterns crucial for understanding context. In scenarios such as protest imagery, detecting only the forged region, for example a duplicated act of violence inserted into a peaceful crowd, can mislead interpretation, highlighting the need for joint source-target localization. Forensim outputs three-class masks (pristine, source, target) and supports detection of both splicing and copy-move forgeries within a unified architecture. We propose a visual state-space model that leverages normalized attention maps to identify internal similarities, paired with a region-based block attention module to distinguish manipulated regions. This design enables end-to-end training and precise localization. Forensim achieves state-of-the-art performance on standard benchmarks. We also release CMFD-Anything, a new dataset addressing limitations of existing copy-move forgery datasets.

</details>


### [74] [4RC: 4D Reconstruction via Conditional Querying Anytime and Anywhere](https://arxiv.org/abs/2602.10094)
*Yihang Luo,Shangchen Zhou,Yushi Lan,Xingang Pan,Chen Change Loy*

Main category: cs.CV

TL;DR: 4RC 提出了一种联合表征4D几何和运动的统一前馈框架，通过将整段视频压缩到时空潜在空间，并针对任意查询帧和时间点高效查询3D几何和运动。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常将运动和几何分离处理，或仅生成稀疏轨迹等有限的4D属性，旨在解决这些局限性，4RC 提出一种全新的框架以提供更全面的4D重建。

Method: 4RC 使用转子编码器将整段视频编码至时空潜在空间，之后不同时空点的查询帧可以从中高效获取3D几何和运动信息。通过将4D属性最小化地因子化为基几何与时间依赖的相对运动。

Result: 4RC 在多种4D重建任务上均优于之前及同期的方法。

Conclusion: 4RC 框架有效地融合了稠密场景几何与运动动态，为4D重建提供了一个最先进的解决方案。

Abstract: We present 4RC, a unified feed-forward framework for 4D reconstruction from monocular videos. Unlike existing approaches that typically decouple motion from geometry or produce limited 4D attributes such as sparse trajectories or two-view scene flow, 4RC learns a holistic 4D representation that jointly captures dense scene geometry and motion dynamics. At its core, 4RC introduces a novel encode-once, query-anywhere and anytime paradigm: a transformer backbone encodes the entire video into a compact spatio-temporal latent space, from which a conditional decoder can efficiently query 3D geometry and motion for any query frame at any target timestamp. To facilitate learning, we represent per-view 4D attributes in a minimally factorized form by decomposing them into base geometry and time-dependent relative motion. Extensive experiments demonstrate that 4RC outperforms prior and concurrent methods across a wide range of 4D reconstruction tasks.

</details>


### [75] [ConsID-Gen: View-Consistent and Identity-Preserving Image-to-Video Generation](https://arxiv.org/abs/2602.10113)
*Mingyang Wu,Ashirbad Mishra,Soumik Dey,Shuo Xing,Naveen Ravipati,Hansi Wu,Binbin Li,Zhengzhong Tu*

Main category: cs.CV

TL;DR: 论文提出了ConsID-Gen框架，通过构建大规模的对象中心数据集ConsIDVid和建立新颖的跨视角一致性基准ConsIDVid-Bench，解决了I2V生成中的细粒度物体身份保持问题。


<details>
  <summary>Details</summary>
Motivation: 解决I2V生成中的细粒度物体身份保持问题和现有模型中存在的外观漂移和几何失真等不足。

Method: 构建大规模的对象中心数据集ConsIDVid和建立新颖的跨视角一致性基准ConsIDVid-Bench，提出ConsID-Gen框架，该框架通过辅助视图增强初始帧，利用双流视觉几何编码器和文本视觉连接器获取语义和结构线索。

Result: 在ConsIDVid-Bench基准上，ConsID-Gen在多个指标上表现优于现有视频生成模型，具有最佳的整体性能。

Conclusion: ConsID-Gen框架能够在现实场景中实现高度准确的身份保真度和时间连贯性。

Abstract: Image-to-Video generation (I2V) animates a static image into a temporally coherent video sequence following textual instructions, yet preserving fine-grained object identity under changing viewpoints remains a persistent challenge. Unlike text-to-video models, existing I2V pipelines often suffer from appearance drift and geometric distortion, artifacts we attribute to the sparsity of single-view 2D observations and weak cross-modal alignment. Here we address this problem from both data and model perspectives. First, we curate ConsIDVid, a large-scale object-centric dataset built with a scalable pipeline for high-quality, temporally aligned videos, and establish ConsIDVid-Bench, where we present a novel benchmarking and evaluation framework for multi-view consistency using metrics sensitive to subtle geometric and appearance deviations. We further propose ConsID-Gen, a view-assisted I2V generation framework that augments the first frame with unposed auxiliary views and fuses semantic and structural cues via a dual-stream visual-geometric encoder as well as a text-visual connector, yielding unified conditioning for a Diffusion Transformer backbone. Experiments across ConsIDVid-Bench demonstrate that ConsID-Gen consistently outperforms in multiple metrics, with the best overall performance surpassing leading video generation models like Wan2.1 and HunyuanVideo, delivering superior identity fidelity and temporal coherence under challenging real-world scenarios. We will release our model and dataset at https://myangwu.github.io/ConsID-Gen.

</details>


### [76] [Quantum Multiple Rotation Averaging](https://arxiv.org/abs/2602.10115)
*Shuteng Wang,Natacha Kuete Meli,Michael Möller,Vladislav Golyanik*

Main category: cs.CV

TL;DR: IQARS 通过将 MRA 问题重新表述为量子退火设备可以解决的局部二次非凸子问题序列，提高了鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有经典方法容易陷入局部极小值且依赖于无法精确保持几何特性的凸松弛，这在高噪声场景下导致准确性下降。

Method: 将 MRA 形式化为一系列可以在量子退火器上执行的本地二次非凸子问题，并利用二进制处理技术实现。

Result: 在合成和真实世界数据集上的表现优于现有最佳经典方法 Shonan，尤其是在高噪声情况下的准确性有显著提升。

Conclusion: IQARS 具有潜力解决 MRA 问题，并提供更鲁棒和精确的绝对旋转回收。

Abstract: Multiple rotation averaging (MRA) is a fundamental optimization problem in 3D vision and robotics that aims to recover globally consistent absolute rotations from noisy relative measurements. Established classical methods, such as L1-IRLS and Shonan, face limitations including local minima susceptibility and reliance on convex relaxations that fail to preserve the exact manifold geometry, leading to reduced accuracy in high-noise scenarios. We introduce IQARS (Iterative Quantum Annealing for Rotation Synchronization), the first algorithm that reformulates MRA as a sequence of local quadratic non-convex sub-problems executable on quantum annealers after binarization, to leverage inherent hardware advantages. IQARS removes convex relaxation dependence and better preserves non-Euclidean rotation manifold geometry while leveraging quantum tunneling and parallelism for efficient solution space exploration. We evaluate IQARS's performance on synthetic and real-world datasets. While current annealers remain in their nascent phase and only support solving problems of limited scale with constrained performance, we observed that IQARS on D-Wave annealers can already achieve ca. 12% higher accuracy than Shonan, i.e., the best-performing classical method evaluated empirically.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [77] [Overview of PAN 2026: Voight-Kampff Generative AI Detection, Text Watermarking, Multi-Author Writing Style Analysis, Generative Plagiarism Detection, and Reasoning Trajectory Detection](https://arxiv.org/abs/2602.09147)
*Janek Bevendorff,Maik Fröbe,André Greiner-Petter,Andreas Jakoby,Maximilian Mayerl,Preslav Nakov,Henry Plutz,Martin Potthast,Benno Stein,Minh Ngoc Ta,Yuxia Wang,Eva Zangerle*

Main category: cs.CL

TL;DR: PAN 工作坊旨在通过客观和可重现的评估推进文本风格分析和文本取证研究。2026年，该工作坊将运行五个任务，分别旨在识别生成式AI在混杂和混淆作者身份场景中的生成，验证文本水印方案的鲁棒性，分析多位作者的写作风格，检测生成式剽窃以及检测生成或人类撰写推理轨迹的来源并确保安全。


<details>
  <summary>Details</summary>
Motivation: PAN 工作坊旨在推动文本风格分析和文本取证领域的发展，这些技术在数字时代显得尤为重要，能够帮助识别和验证生成式内容的真实性和作者身份。

Method: 通过组织五项任务，涵盖生成式AI检测、文本水印、多位作者写作风格分析、生成式剽窃检测以及推理轨迹检测等最新的研究方向，借助TIRA实验平台收集软件提交并进行客观评估。

Result: 自2012年以来，PAN 工作坊已收到了通过TIRA实验平台提交的超过1,100个软件提交。

Conclusion: PAN 工作坊的方法和成果为研究者和开发者提供了一个公开、公平的评估平台，促进了相关技术的进步。

Abstract: The goal of the PAN workshop is to advance computational stylometry and text forensics via objective and reproducible evaluation. In 2026, we run the following five tasks: (1) Voight-Kampff Generative AI Detection, particularly in mixed and obfuscated authorship scenarios, (2) Text Watermarking, a new task that aims to find new and benchmark the robustness of existing text watermarking schemes, (3) Multi-author Writing Style Analysis, a continued task that aims to find positions of authorship change, (4) Generative Plagiarism Detection, a continued task that targets source retrieval and text alignment between generated text and source documents, and (5) Reasoning Trajectory Detection, a new task that deals with source detection and safety detection of LLM-generated or human-written reasoning trajectories. As in previous years, PAN invites software submissions as easy-to-reproduce Docker containers for most of the tasks. Since PAN 2012, more than 1,100 submissions have been made this way via the TIRA experimentation platform.

</details>


### [78] [Measuring Inclusion in Interaction: Inclusion Analytics for Human-AI Collaborative Learning](https://arxiv.org/abs/2602.09269)
*Jaeyoon Choi,Nia Nixon*

Main category: cs.CL

TL;DR: 本文引入了一种基于话语的框架——包容性分析，用以考察协作问题解决（CPS）中包容性这一动态互动过程。通过不同维度（参与公平性、情感氛围、知识公平性）的分析，该框架使包容性变得可分析，并用模拟对话和人类-AI团队实验的数据展示了其如何揭示聚合或事后评估中不可见的模式。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法准确反映合作问题解决中包容性时刻的变化，本文旨在通过引入基于话语的话语包容性分析框架，提供一种新的视角和方法，以动态交互的方式衡量这一过程中的包容性。

Method: 本文从参与公平性、情感氛围和知识公平性三个维度探讨框架，使用了模拟对话和真实的实验数据。通过传达可见的交互层面度量，使这些概念变得可分析。

Result: 该研究展示了基于话语的包容性分析在模拟对话和人类-AI团队实验中的应用，展示了其如何揭示聚合或事后评估中不可见的模式。

Conclusion: 文章迈出了过程导向性方法衡量人类-人工智能协作学习环境中包容性的第一步。

Abstract: Inclusion, equity, and access are widely valued in AI and education, yet are often assessed through coarse sample descriptors or post-hoc self-reports that miss how inclusion is shaped moment by moment in collaborative problem solving (CPS). In this proof-of-concept paper, we introduce inclusion analytics, a discourse-based framework for examining inclusion as a dynamic, interactional process in CPS. We conceptualize inclusion along three complementary dimensions -- participation equity, affective climate, and epistemic equity -- and demonstrate how these constructs can be made analytically visible using scalable, interaction-level measures. Using both simulated conversations and empirical data from human-AI teaming experiments, we illustrate how inclusion analytics can surface patterns of participation, relational dynamics, and idea uptake that remain invisible to aggregate or post-hoc evaluations. This work represents an initial step toward process-oriented approaches to measuring inclusion in human-AI collaborative learning environments.

</details>


### [79] [Don't Shoot The Breeze: Topic Continuity Model Using Nonlinear Naive Bayes With Attention](https://arxiv.org/abs/2602.09312)
*Shu-Ting Pi,Pradeep Bagavan,Yejia Li,Disha,Qun Liu*

Main category: cs.CL

TL;DR: 本文提出了一种基于朴素贝叶斯扩展的注意力机制模型，以评估聊天机器人响应与初始对话主题的一致性，该模型能够在任何长度的对话中高效工作，并在长度和复杂度较高的对话中表现出色。


<details>
  <summary>Details</summary>
Motivation: 在各种业务场景下使用大型语言模型（LLM）作为聊天机器人时，保持主题连续性是一项挑战。本文旨在解决这一问题，通过评估响应是否符合初始对话主题来提高用户体验和资源利用率。

Method: 提出了一个基于朴素贝叶斯扩展的方法，并结合了注意力机制和对数非线性，将自然语言理解（NLU）模型转换为可解释的分析公式。

Result: 实验表明，本文提出的方法在处理长且复杂的对话时优于传统方法。

Conclusion: 本文提出的方法提供了一种确保LLM负责任和可解释使用的独特能力。

Abstract: Utilizing Large Language Models (LLM) as chatbots in diverse business scenarios often presents the challenge of maintaining topic continuity. Abrupt shifts in topics can lead to poor user experiences and inefficient utilization of computational resources. In this paper, we present a topic continuity model aimed at assessing whether a response aligns with the initial conversation topic. Our model is built upon the expansion of the corresponding natural language understanding (NLU) model into quantifiable terms using a Naive Bayes approach. Subsequently, we have introduced an attention mechanism and logarithmic nonlinearity to enhance its capability to capture topic continuity. This approach allows us to convert the NLU model into an interpretable analytical formula. In contrast to many NLU models constrained by token limits, our proposed model can seamlessly handle conversations of any length with linear time complexity. Furthermore, the attention mechanism significantly improves the model's ability to identify topic continuity in complex conversations. According to our experiments, our model consistently outperforms traditional methods, particularly in handling lengthy and intricate conversations. This unique capability offers us an opportunity to ensure the responsible and interpretable use of LLMs.

</details>


### [80] [Digital Linguistic Bias in Spanish: Evidence from Lexical Variation in LLMs](https://arxiv.org/abs/2602.09346)
*Yoshifumi Kawasaki*

Main category: cs.CL

TL;DR: 本研究通过使用两种调查式问题格式对大型语言模型（LLMs）在西班牙语地理词汇变异上的表现进行评估，发现模型在识别某些地区变异时更为准确，但智利变体的识别难度较大。研究表明，数据量不是模型方言表示能力的唯一决定因素。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索大型语言模型（LLMs）如何捕捉西班牙语中的地理词汇变异，揭示模型在不同地区变体识别上的表现差异，以增进对LLMs方言知识的理解，并提供有关西班牙语数字语言偏见的新证据。

Method: 通过使用Yes-No问题和多项选择问题的两种调查式问题格式，研究者利用专家整理的大型西班牙词汇变异数据库对LLMs进行评估。评价包括900多个词汇项，覆盖21个西班牙语国家和地区，在国家和方言区域水平上进行。

Result: 研究结果表明，与西班牙、赤道几内亚、墨西哥及中美洲以及拉普拉塔河相关的词汇变异被模型更准确地识别，而智利变体的识别尤为困难。同时，不同国家规模的数字资源量并不能解释这些性能差异，表明模型的方言表示能力受数据量之外的因素影响。

Conclusion: 研究通过精细、大规模地评估地理词汇变异，推进了对LLMs方言知识的理解，并为西班牙语中的数字语言偏见讨论提供了新的证据。

Abstract: This study examines the extent to which Large Language Models (LLMs) capture geographic lexical variation in Spanish, a language that exhibits substantial regional variation. Treating LLMs as virtual informants, we probe their dialectal knowledge using two survey-style question formats: Yes-No questions and multiple-choice questions. To this end, we exploited a large-scale, expert-curated database of Spanish lexical variation. Our evaluation covers more than 900 lexical items across 21 Spanish-speaking countries and is conducted at both the country and dialectal area levels. Across both evaluation formats, the results reveal systematic differences in how LLMs represent Spanish language varieties. Lexical variation associated with Spain, Equatorial Guinea, Mexico & Central America, and the La Plata River is recognized more accurately by the models, while the Chilean variety proves particularly difficult for the models to distinguish. Importantly, differences in the volume of country-level digital resources do not account for these performance patterns, suggesting that factors beyond data quantity shape dialectal representation in LLMs. By providing a fine-grained, large-scale evaluation of geographic lexical variation, this work advances empirical understanding of dialectal knowledge in LLMs and contributes new evidence to discussions of Digital Linguistic Bias in Spanish.

</details>


### [81] [Unsupervised Cross-Lingual Part-of-Speech Tagging with Monolingual Corpora Only](https://arxiv.org/abs/2602.09366)
*Jianyu Zheng*

Main category: cs.CL

TL;DR: 该研究提出了一种完全无监督的跨语言词性标注框架，利用未监督神经机器翻译系统从高资源语言向低资源语言进行词性标注迁移，同时引入一种多源投影技术以进一步优化目标语言的词性标注性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究中，低资源语言的词性标注受限于平行语料的稀缺性，该研究旨在克服这一限制，提出了一种完全无监督的跨语言词性标注框架。

Method: 该方法利用神经机器翻译系统将高资源语言的句子翻译到低资源语言，生成伪平行语料对，再对目标语言进行词性标注训练，并引入多源投影技术调整预测的词性标签，进一步提升性能。

Result: 该框架在28种语言对上进行了测试，显示出与基于平行句子对的基线跨语言词性标注模型相当甚至更高的性能，特别在某些目标语言上表现突出，平均提升1.3%。

Conclusion: 该研究提出的方法为低资源语言的词性标注提供了一种有效解决方案，能够显著提高词性标注的准确性。

Abstract: Due to the scarcity of part-of-speech annotated data, existing studies on low-resource languages typically adopt unsupervised approaches for POS tagging. Among these, POS tag projection with word alignment method transfers POS tags from a high-resource source language to a low-resource target language based on parallel corpora, making it particularly suitable for low-resource language settings. However, this approach relies heavily on parallel corpora, which are often unavailable for many low-resource languages. To overcome this limitation, we propose a fully unsupervised cross-lingual part-of-speech(POS) tagging framework that relies solely on monolingual corpora by leveraging unsupervised neural machine translation(UNMT) system. This UNMT system first translates sentences from a high-resource language into a low-resource one, thereby constructing pseudo-parallel sentence pairs. Then, we train a POS tagger for the target language following the standard projection procedure based on word alignments. Moreover, we propose a multi-source projection technique to calibrate the projected POS tags on the target side, enhancing to train a more effective POS tagger. We evaluate our framework on 28 language pairs, covering four source languages (English, German, Spanish and French) and seven target languages (Afrikaans, Basque, Finnis, Indonesian, Lithuanian, Portuguese and Turkish). Experimental results show that our method can achieve performance comparable to the baseline cross-lingual POS tagger with parallel sentence pairs, and even exceeds it for certain target languages. Furthermore, our proposed multi-source projection technique further boosts performance, yielding an average improvement of 1.3% over previous methods.

</details>


### [82] [AgentSkiller: Scaling Generalist Agent Intelligence through Semantically Integrated Cross-Domain Data Synthesis](https://arxiv.org/abs/2602.09372)
*Zexu Sun,Bokai Ji,Hengyi Cai,Shuaiqiang Wang,Lei Wang,Guangxia Li,Xu Chen*

Main category: cs.CL

TL;DR: 本文提出了一种名为AgentSkiller的自动化框架，用于合成跨领域的多回合交互数据，旨在生成高质量且长期的数据，以提升大规模参数模型在函数调用任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的数据收集方法存在隐私限制或生成缺乏多样性的预构交互数据，无法满足扩展模型能力所需的数据需求。

Method: 该框架基于有向无环图(DAG)的架构，包含构建领域本体和人物中心实体图、通过服务蓝图定义工具接口、填充一致的数据库和严格领域策略等步骤，最终通过跨领域融合机制和基于人物的模拟器创建用户任务。

Result: 该框架合成了约11000个交互样本，训练在该数据集上的模型在函数调用任务上取得了显著改进，特别是在较大的模型参数规模上。

Conclusion: 该研究提出的方法有效提升了大规模参数模型在处理真实世界问题上的能力。

Abstract: Large Language Model agents demonstrate potential in solving real-world problems via tools, yet generalist intelligence is bottlenecked by scarce high-quality, long-horizon data. Existing methods collect privacy-constrained API logs or generate scripted interactions lacking diversity, which struggle to produce data requisite for scaling capabilities. We propose AgentSkiller, a fully automated framework synthesizing multi-turn interaction data across realistic, semantically linked domains. It employs a DAG-based architecture with explicit state transitions to ensure determinism and recoverability. The pipeline builds a domain ontology and Person-Centric Entity Graph, defines tool interfaces via Service Blueprints for Model Context Protocol servers, and populates environments with consistent databases and strict Domain Policies. A cross-domain fusion mechanism links services to simulate complex tasks. Finally, the pipeline creates user tasks by verifying solution paths, filtering via execution-based validation, and generating queries using a Persona-based Simulator for automated rollout. This produces reliable environments with clear state changes. To demonstrate effectiveness, we synthesized $\approx$ 11K interaction samples; experimental results indicate that models trained on this dataset achieve significant improvements on function calling over baselines, particularly in larger parameter regimes.

</details>


### [83] [AfriNLLB: Efficient Translation Models for African Languages](https://arxiv.org/abs/2602.09373)
*Yasmin Moslem,Aman Kassahun Wassie,Amanuel Gizachew Abebe*

Main category: cs.CL

TL;DR: 该论文介绍了一个针对非洲语言的轻量级翻译模型系列AfriNLLB，支持15个语言对的双向翻译，模型通过压缩基模型并进行微调获得，能够在资源受限环境中高效部署。


<details>
  <summary>Details</summary>
Motivation: 由于非洲地区语言资源匮乏，传统翻译模型的部署面临着较大挑战，因此开发适合非洲语言的轻量级模型旨在解决这一问题。

Method: 模型基于NLLB-200 600M，并通过逐层剪裁和量化进行压缩。经过大型教师模型的知识蒸馏，对非洲语言的平行语料进行微调。

Result: 实验结果表明，AfriNLLB在保持与基线模型相当的性能的同时，速度快了多个数量级。

Conclusion: AfriNLLB模型系列为非洲语言翻译提供了高效的解决方案，同时也公开了相关数据集，促进进一步研究。

Abstract: In this work, we present AfriNLLB, a series of lightweight models for efficient translation from and into African languages. AfriNLLB supports 15 language pairs (30 translation directions), including Swahili, Hausa, Yoruba, Amharic, Somali, Zulu, Lingala, Afrikaans, Wolof, and Egyptian Arabic, as well as other African Union official languages such as Arabic (MSA), French, Portuguese, and Spanish. Our training data covers bidirectional translation between English and 13 languages, and between French and two languages (Lingala and Wolof).
  AfriNLLB models are based on NLLB-200 600M, which we compress using iterative layer pruning and quantization. We fine-tune the pruned models on parallel corpora we curated for African languages, employing knowledge distillation from a larger teacher model. Our work aims at enabling efficient deployment of translation models for African languages in resource-constrained settings.
  Our evaluation results demonstrate that AfriNLLB models achieve performance comparable to the baseline while being significantly faster. We release two versions of the AfriNLLB models, a Transformers version that allows further fine-tuning and a CTranslate2 version for efficient inference. Moreover, we release all the training data that we used for fine-tuning the baseline and pruned models to facilitate further research.

</details>


### [84] [BiasScope: Towards Automated Detection of Bias in LLM-as-a-Judge Evaluation](https://arxiv.org/abs/2602.09383)
*Peng Lai,Zhihao Ou,Yong Wang,Longyue Wang,Jian Yang,Yun Chen,Guanhua Chen*

Main category: cs.CL

TL;DR: 该研究提出了BiasScope框架，这是一种由LLM驱动的自动化方法，用于大规模发现模型评估中的潜在偏见，填补了现有方法在未知偏见探索上的空白。通过BiasScope，还提出了JudgeBench-Pro基准测试，以评估LLM作为法官的鲁棒性，其结果强调了加强评价稳健性和偏见缓解的紧迫性。


<details>
  <summary>Details</summary>
Motivation: 现有的bias研究主要关注已知bias及其对评估结果的影响，但缺乏自动化的、系统性的探索潜在未知bias的方法，这影响了评价的稳健性和可靠性，因此需要一个新的框架来改进。

Method: BiasScope框架通过利用LLM来自动发现模型评估中的潜在偏见，并通过在JudgeBench数据集上的有效性验证来证明其通用性和效果。此外，还引入了新的基准测试JudgeBench-Pro，增加了评估的挑战性。

Result: BiasScope能够在不同的模型家族和规模上发现潜在偏见，而现有的方法则无法做到这一点。通过JudgeBench-Pro基准测试，即使是强大的LLM在评估中的错误率也超过50%，这表明需要进一步加强评价的稳健性。

Conclusion: 研究证明了BiasScope框架的有效性和重要性，不仅提升了对潜在未知偏见的探索能力，还为评估的鲁棒性提供了一个更严格的评估基准。这对于未来研究和实际应用中的LLM变得更稳健和可靠具有重要意义。

Abstract: LLM-as-a-Judge has been widely adopted across various research and practical applications, yet the robustness and reliability of its evaluation remain a critical issue. A core challenge it faces is bias, which has primarily been studied in terms of known biases and their impact on evaluation outcomes, while automated and systematic exploration of potential unknown biases is still lacking. Nevertheless, such exploration is crucial for enhancing the robustness and reliability of evaluations. To bridge this gap, we propose BiasScope, a LLM-driven framework for automatically and at scale discovering potential biases that may arise during model evaluation. BiasScope can uncover potential biases across different model families and scales, with its generality and effectiveness validated on the JudgeBench dataset. It overcomes the limitations of existing approaches, transforming bias discovery from a passive process relying on manual effort and predefined bias lists into an active and comprehensive automated exploration. Moreover, based on BiasScope, we propose JudgeBench-Pro, an extended version of JudgeBench and a more challenging benchmark for evaluating the robustness of LLM-as-a-judge. Strikingly, even powerful LLMs as evaluators show error rates above 50\% on JudgeBench-Pro, underscoring the urgent need to strengthen evaluation robustness and to mitigate potential biases further.

</details>


### [85] [Contractual Deepfakes: Can Large Language Models Generate Contracts?](https://arxiv.org/abs/2602.09384)
*Eliza Mik*

Main category: cs.CL

TL;DR: 尽管大型语言模型(LLMs)能够生成文本，但它们并不理解词语的意义，缺乏语境感和推理能力。尽管有观点认为合同起草是可以通过这种技术来简化，该论文认为这种观点是不合理且错误的，LLMs生成的合同文件可能无法满足特定交易的需求。


<details>
  <summary>Details</summary>
Motivation: 研究指出，尽管LLMs在生成文本方面具有强大的能力，但在涉及具体交易情境时，它们的效力和适用性受到限制，因此不应过度依赖或夸大它们在法律行业的潜在价值。

Method: 通过分析现有大型语言模型的生成机制，对比理论上的语言生成与实际法律情境中的应用差异，指出LLMs在法律行业中的局限性。

Result: 研究表明，大型语言模型生成的合同文件可能在语法上正确但内容上不适用特定交易，且可能包含逻辑矛盾，无法作为有效的法律文件。

Conclusion: 该论文得出结论，尽管大型语言模型具有生成合同文本的能力，但它们并不能真正理解和解决法律行业中的复杂问题，因此不会威胁法律行业的持续发展。

Abstract: Notwithstanding their unprecedented ability to generate text, LLMs do not understand the meaning of words, have no sense of context and cannot reason. Their output constitutes an approximation of statistically dominant word patterns. And yet, the drafting of contracts is often presented as a typical legal task that could be facilitated by this technology. This paper seeks to put an end to such unreasonable ideas. Predicting words differs from using language in the circumstances of specific transactions and reconstituting common contractual phrases differs from reasoning about the law. LLMs seem to be able to generate generic and superficially plausible contractual documents. In the cold light of day, such documents may turn out to be useless assemblages of inconsistent provisions or contracts that are enforceable but unsuitable for a given transaction. This paper casts a shadow on the simplistic assumption that LLMs threaten the continued viability of the legal industry.

</details>


### [86] [Effective vocabulary expanding of multilingual language models for extremely low-resource languages](https://arxiv.org/abs/2602.09388)
*Jianyu Zheng*

Main category: cs.CL

TL;DR: 该研究提出了一个扩展多语言预训练语言模型(mPLMs)的方法，该方法通过目标语言语料库扩展模型词汇，并通过双语词典初始化以减少母语言（例如英语）偏好偏差的词汇表表示。实验结果显示该方法在词性标注和命名实体识别任务中优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 当前的多语言预训练语言模型(mPLMs)虽能支持许多低资源语言，但仍缺乏将低资源语言扩展支持的方法。本研究旨在解决此问题，以提高模型对低资源语言的支持能力。

Method: 研究首先使用目标语言语料库扩展模型词汇，然后筛选出原始词汇中偏向于母语言的子集，并使用双语词典初始化扩展词汇的表示。最后，基于这些扩展词汇的表示继续使用目标语言语料库对模型进行预训练。

Result: 实验结果显示，这种方法在词性标注和命名实体识别任务中优于使用随机初始化扩展词汇的基线模型，分别提高了0.54%和2.60%。

Conclusion: 研究提出的方法能够在不损害源语言性能的前提下优化低资源语言的预训练模型，展示了在选择训练语料库方面的稳健性。

Abstract: Multilingual pre-trained language models(mPLMs) offer significant benefits for many low-resource languages. To further expand the range of languages these models can support, many works focus on continued pre-training of these models. However, few works address how to extend mPLMs to low-resource languages that were previously unsupported. To tackle this issue, we expand the model's vocabulary using a target language corpus. We then screen out a subset from the model's original vocabulary, which is biased towards representing the source language(e.g. English), and utilize bilingual dictionaries to initialize the representations of the expanded vocabulary. Subsequently, we continue to pre-train the mPLMs using the target language corpus, based on the representations of these expanded vocabulary. Experimental results show that our proposed method outperforms the baseline, which uses randomly initialized expanded vocabulary for continued pre-training, in POS tagging and NER tasks, achieving improvements by 0.54% and 2.60%, respectively. Furthermore, our method demonstrates high robustness in selecting the training corpora, and the models' performance on the source language does not degrade after continued pre-training.

</details>


### [87] [Are Language Models Sensitive to Morally Irrelevant Distractors?](https://arxiv.org/abs/2602.09416)
*Andrew Shaw,Christina Hahn,Catherine Rasgaitis,Yash Mishra,Alisa Liu,Natasha Jaques,Yulia Tsvetkov,Amy X. Zhang*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLMs）在道德判断中是否表现出类似人类的认知偏向，通过引入无道德相关的刺激（如情感化的图像和叙述），发现了即使在低歧义场景下，LLMs的道德判断也会受到显著影响。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在高风险领域的应用不断增加，确保这些模型的行为符合人类价值观变得愈发重要。本文意在探索是否LLMs也与人类一样，容易受到无道德相关情境因素的影响，以此增强对其道德偏见的理解。

Method: 研究团队通过创建一个新的模式数据集，内含60个无道德相关的情感化图像和叙述，来检验大型语言模型如何受到这些情境信号的扭曲，进而评估其反应变化。

Result: 研究发现，即使是简单的道德干扰项（如无道德关联的情境刺激），也能显著改变LLMs的道德判断，这些干扰甚至在清晰情境下也能导致约30％的变化。

Conclusion: 本研究强调了在评估和调整大型语言模型的道德判断时，需要考虑更多上下文因素，尤其是在AI系统处理道德决策任务时。

Abstract: With the rapid development and uptake of large language models (LLMs) across high-stakes settings, it is increasingly important to ensure that LLMs behave in ways that align with human values. Existing moral benchmarks prompt LLMs with value statements, moral scenarios, or psychological questionnaires, with the implicit underlying assumption that LLMs report somewhat stable moral preferences. However, moral psychology research has shown that human moral judgements are sensitive to morally irrelevant situational factors, such as smelling cinnamon rolls or the level of ambient noise, thereby challenging moral theories that assume the stability of human moral judgements. Here, we draw inspiration from this "situationist" view of moral psychology to evaluate whether LLMs exhibit similar cognitive moral biases to humans. We curate a novel multimodal dataset of 60 "moral distractors" from existing psychological datasets of emotionally-valenced images and narratives which have no moral relevance to the situation presented. After injecting these distractors into existing moral benchmarks to measure their effects on LLM responses, we find that moral distractors can shift the moral judgements of LLMs by over 30% even in low-ambiguity scenarios, highlighting the need for more contextual moral evaluations and more nuanced cognitive moral modeling of LLMs.

</details>


### [88] [Evaluating Social Bias in RAG Systems: When External Context Helps and Reasoning Hurts](https://arxiv.org/abs/2602.09442)
*Shweta Parihar,Lu Cheng*

Main category: cs.CL

TL;DR: 在对检索增强生成（RAG）架构的社会偏见进行广泛实验后，研究发现外部上下文的引入有助于减少模型的偏见，尽管Chain-of-Thought（CoT）提示能提高准确性，但会增加整体偏见。


<details>
  <summary>Details</summary>
Motivation: 探讨RAG架构在缓解社交媒体偏见方面的潜力及其与CoT提示相结合的效果。

Method: 通过多种检索语料库、大型语言模型和偏见评估数据集的广泛实验，涵盖超过13种不同的偏见类型，研究对偏见的影响。

Result: RAG架构中引入外部背景知识能减少偏见；CoT提示增强了准确度，但也增加了整体偏见。

Conclusion: 研究表明，CoT提示对偏见有负面影响，研究者建议需要开发偏见意识推理框架来缓解这一权衡。

Abstract: Social biases inherent in large language models (LLMs) raise significant fairness concerns. Retrieval-Augmented Generation (RAG) architectures, which retrieve external knowledge sources to enhance the generative capabilities of LLMs, remain susceptible to the same bias-related challenges. This work focuses on evaluating and understanding the social bias implications of RAG. Through extensive experiments across various retrieval corpora, LLMs, and bias evaluation datasets, encompassing more than 13 different bias types, we surprisingly observe a reduction in bias in RAG. This suggests that the inclusion of external context can help counteract stereotype-driven predictions, potentially improving fairness by diversifying the contextual grounding of the model's outputs. To better understand this phenomenon, we then explore the model's reasoning process by integrating Chain-of-Thought (CoT) prompting into RAG while assessing the faithfulness of the model's CoT. Our experiments reveal that the model's bias inclinations shift between stereotype and anti-stereotype responses as more contextual information is incorporated from the retrieved documents. Interestingly, we find that while CoT enhances accuracy, contrary to the bias reduction observed with RAG, it increases overall bias across datasets, highlighting the need for bias-aware reasoning frameworks that can mitigate this trade-off.

</details>


### [89] [Conceptual Cultural Index: A Metric for Cultural Specificity via Relative Generality](https://arxiv.org/abs/2602.09444)
*Takumi Ohashi,Hitoshi Iyatomi*

Main category: cs.CL

TL;DR: 本文提出了一种名为Conceptual Cultural Index (CCI) 的指标，用于评估句子在特定文化下的文化特异性。通过对比目标文化和其他文化中的普遍性估计值，CCI 指标能够有效区分文化特异性和普遍性句子，并在判别性能上优于直接使用大语言模型的评分。


<details>
  <summary>Details</summary>
Motivation: 当前对多语言环境下大语言模型的文化适应性评价研究尚不足，特别是缺乏系统性地评估句子层次文化特异性的方法。为此，本文提出了一种新的度量方法——Conceptual Cultural Index (CCI)，以便更全面地评价大语言模型在不同文化背景下的表现。

Method: CCI 指标的计算基于句子在目标文化和其他文化中的普遍性估计值。作者通过对比这两个估计值来得到最终的文化特异性得分。此外，作者使用 400 个句子（包括 200 个文化特异性句子和 200 个普遍性句子）对 CCI 指标进行了验证，并表明CCI 指标在判别文化特异性上的性能优于直接使用大语言模型的方法。

Result: 研究结果表明CCI 指标在区分文化特异性和普遍性句子方面表现出色，特别是在针对特定文化定制的模型上，其二元可分性能显著优于直接使用大语言模型的评分方法，AUC 值提升了超过 10 分。

Conclusion: CCI 提供了一种有效的评估方法，能够帮助研究人员更好地理解大语言模型在不同文化背景下的表现，并为进一步改进模型的文化适应性提供了有力工具。

Abstract: Large language models (LLMs) are increasingly deployed in multicultural settings; however, systematic evaluation of cultural specificity at the sentence level remains underexplored. We propose the Conceptual Cultural Index (CCI), which estimates cultural specificity at the sentence level. CCI is defined as the difference between the generality estimate within the target culture and the average generality estimate across other cultures. This formulation enables users to operationally control the scope of culture via comparison settings and provides interpretability, since the score derives from the underlying generality estimates. We validate CCI on 400 sentences (200 culture-specific and 200 general), and the resulting score distribution exhibits the anticipated pattern: higher for culture-specific sentences and lower for general ones. For binary separability, CCI outperforms direct LLM scoring, yielding more than a 10-point improvement in AUC for models specialized to the target culture. Our code is available at https://github.com/IyatomiLab/CCI .

</details>


### [90] [NOWJ @BioCreative IX ToxHabits: An Ensemble Deep Learning Approach for Detecting Substance Use and Contextual Information in Clinical Texts](https://arxiv.org/abs/2602.09469)
*Huu-Huy-Hoang Tran,Gia-Bao Duong,Quoc-Viet-Anh Tran,Thi-Hai-Yen Vuong,Hoang-Quynh Le*

Main category: cs.CL

TL;DR: 该研究提出了一个多输出集成系统，用于解决ToxHabits共享任务中的Subtask 1和Subtask 2，系统集成BETO并使用CRF层进行序列标注，实现高精度和F1分数。


<details>
  <summary>Details</summary>
Motivation: 面对从非结构化电子健康记录中提取药物使用信息的挑战，以及大型语言模型在临床自然语言处理中的信任、控制和效率限制问题，本研究提出了NOWJ系统来应对ToxHabits共享任务。

Method: 研究采用多输出集成方法，整合BETO模型并添加CRF层进行序列标注，同时采用多样化的训练策略及句子筛选以提升精确度。

Result: 在ToxHabits共享任务中，该系统的顶级运行在触发检测中达到了0.94的F1分数和0.97的精度，在论点检测中达到了0.91的F1分数。

Conclusion: 该研究展示了集成模型在处理特定领域低资源环境下的应用潜力，及其在信息抽取任务中的有效性能。

Abstract: Extracting drug use information from unstructured Electronic Health Records remains a major challenge in clinical Natural Language Processing. While Large Language Models demonstrate advancements, their use in clinical NLP is limited by concerns over trust, control, and efficiency. To address this, we present NOWJ submission to the ToxHabits Shared Task at BioCreative IX. This task targets the detection of toxic substance use and contextual attributes in Spanish clinical texts, a domain-specific, low-resource setting. We propose a multi-output ensemble system tackling both Subtask 1 - ToxNER and Subtask 2 - ToxUse. Our system integrates BETO with a CRF layer for sequence labeling, employs diverse training strategies, and uses sentence filtering to boost precision. Our top run achieved 0.94 F1 and 0.97 precision for Trigger Detection, and 0.91 F1 for Argument Detection.

</details>


### [91] [Listen to the Layers: Mitigating Hallucinations with Inter-Layer Disagreement](https://arxiv.org/abs/2602.09486)
*Koduvayur Subbalakshmi,Sabbir Hossain Ujjal,Venkata Krishna Teja Mangichetty,Nastaran Jamalipour Soofi*

Main category: cs.CL

TL;DR: CoCoA采用于内部层中辨识不稳定信号，通过自信息门控变体调节这种惩罚，促使模型生成更符合事实的输出，从而在推理时显著提升预训练大型语言模型的可靠性。


<details>
  <summary>Details</summary>
Motivation: 解决预训练大型语言模型在生成文本时容易产生虽然流畅但事实错误的问题，即“幻觉”现象。

Method: 提出一种名为CoCoA的新颖解码算法，通过检测模型中间层的不稳定信号进行解码，设计了两个量化中间层不稳定性指标，并引入自信息门控变体CoCoA-SIG动态调整惩罚。

Result: 在多项任务（如问答、摘要和代码生成）上，CoCoA显著提高了语言模型的正确性和可靠性。

Conclusion: CoCoA通过利用模型固有信号，在推理过程中有效提升预训练大型语言模型的可信度，无需任何模型重训练。

Abstract: Pretrained Large Language Models (LLMs) are prone to generating fluent yet factually incorrect text-a phenomenon known as hallucinations, undermining their reliability and utility in downstream tasks. We hypothesize that a generated text span's factuality is correlated with its representational instability across the model's internal layers. Based on this, we propose the CoCoA (Confusion and Consistency Aware) decoder, a novel, training-free decoding algorithm that mitigates hallucinations at inference time by listening to these signals in the middle layers. We propose two metrics to quantify this instability in the middle layers, and use it to penalize outputs that exhibit high internal confusion, thereby steering the model towards more internally consistent and factually grounded outputs. We further propose a self-information gated variant, CoCoA-SIG, that dynamically modulates this penalty to selectively target high-surprise, unstable generations. Extensive experiments on diverse tasks, including question-answering, summarization and code generation demonstrate that CoCoA significantly improves factual correctness across multiple model families (e.g., Llama-3, Qwen-2.5, Mistral). By leveraging model-intrinsic signals, CoCoA offers an effective and broadly applicable method for enhancing the trustworthiness of LLMs at inference time, without requiring any model retraining.

</details>


### [92] [Where-to-Unmask: Ground-Truth-Guided Unmasking Order Learning for Masked Diffusion Language Models](https://arxiv.org/abs/2602.09501)
*Hikaru Asano,Tadashi Kozuno,Kuniaki Saito,Yukino Baba*

Main category: cs.CL

TL;DR: 该研究引入了一种名为Gt-Margin的基于地面真理的得分，用于生成文本时决定哪个位置先行生成（where-to-unmask）。Gt-Margin使得生成过程优先处理较简单的部分，从而提升最终的文本生成质量。通过监督式学习的方式训练了一个排名模型来模仿Oracle的排序顺序，该模型可集成到标准的MDLM中，以提高逻辑推理任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前的MDLM在推理时的顺序选择主要依赖启发式的方法或通过代价高昂的强化学习策略，而这些方法往往影响生成的质量。为了改进这一问题，研究引入了Gt-Margin，它提供了一种基于地面真理得分的排序顺序，优先处理更简单的部分，从而提升生成质量。

Method: 研究通过分析正确令牌和其最强替代选项之间的概率差距来计算Gt-Margin得分。基于此得分，训练了一个监督式学习的排名模型，以模仿Oracle排序行为。该模型能够被集成到标准MDLM中，用于确定先行生成的位置。

Result: 实验结果显示，使用Gt-Margin作为排序顺序显著提高了最终生成的质量，特别是在逻辑推理基准测试上表现突出。

Conclusion: 该研究通过引入Gt-Margin得分和一个模仿Oracle排序的排名模型，提高了MDLM的推理准确性和生成质量。

Abstract: Masked Diffusion Language Models (MDLMs) generate text by iteratively filling masked tokens, requiring two coupled decisions at each step: which positions to unmask (where-to-unmask) and which tokens to place (what-to-unmask). While standard MDLM training directly optimizes token prediction (what-to-unmask), inference-time unmasking orders (where-to-unmask) are typically determined by heuristic confidence measures or trained through reinforcement learning with costly on-policy rollouts. To address this, we introduce Gt-Margin, a position-wise score derived from ground-truth tokens, defined as the probability margin between the correct token and its strongest alternative. Gt-Margin yields an oracle unmasking order that prioritizes easier positions first under each partially masked state. We demonstrate that leveraging this oracle unmasking order significantly enhances final generation quality, particularly on logical reasoning benchmarks. Building on this insight, we train a supervised unmasking planner via learning-to-rank to imitate the oracle ordering from masked contexts. The resulting planner integrates into standard MDLM sampling to select where-to-unmask, improving reasoning accuracy without modifying the token prediction model.

</details>


### [93] [EcoGym: Evaluating LLMs for Long-Horizon Plan-and-Execute in Interactive Economies](https://arxiv.org/abs/2602.09514)
*Xavier Hu,Jinxiang Xia,Shengze Xu,Kangqi Song,Yishuo Yuan,Guibin Zhang,Jincheng Ren,Boyu Feng,Li Lu,Tieyong Zeng,Jiaheng Liu,Minghao Liu,Yuchen Elenor Jiang,Wei Wang,He Zhu,Wangchunshu Zhou*

Main category: cs.CL

TL;DR: EcoGym 提供了一个通用基准，用于评估长期规划能力，并在虚拟经济中进行持续决策和执行。该框架通过标准化接口和预算限制，在三个不同场景中测试模型，关注长期战略一致性与部分可观察性和随机性下的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有评价框架的不足，如短时的、特定领域的或未充分考虑经济动态，导致无法全面评估 LLM 在长期决策能力上的表现。

Method: EcoGym 采用了一个统一体系结构，包含三种环境（Vending, Freelance, Operation），每个环境都有自己的实现方式但通过标准化接口统一管理。整个实验通过一个巨大的时间跨度（最多 1000 步，相当于 365 循环）进行，以模拟真实的长期决策过程。

Result: 实验表明，没有单一模型能在三种场景中均表现最佳。模型们在这三个方面表现出了显著的次优化现象：高阶策略或低阶行动执行。

Conclusion: EcoGym 开放为一个透明的长期智能体评估平台，可以研究在实际经济环境中控制与效用之间的权衡。

Abstract: Long-horizon planning is widely recognized as a core capability of autonomous LLM-based agents; however, current evaluation frameworks suffer from being largely episodic, domain-specific, or insufficiently grounded in persistent economic dynamics. We introduce EcoGym, a generalizable benchmark for continuous plan-and-execute decision making in interactive economies. EcoGym comprises three diverse environments: Vending, Freelance, and Operation, implemented in a unified decision-making process with standardized interfaces, and budgeted actions over an effectively unbounded horizon (1000+ steps if 365 day-loops for evaluation). The evaluation of EcoGym is based on business-relevant outcomes (e.g., net worth, income, and DAU), targeting long-term strategic coherence and robustness under partial observability and stochasticity. Experiments across eleven leading LLMs expose a systematic tension: no single model dominates across all three scenarios. Critically, we find that models exhibit significant suboptimality in either high-level strategies or efficient actions executions. EcoGym is released as an open, extensible testbed for transparent long-horizon agent evaluation and for studying controllability-utility trade-offs in realistic economic settings.

</details>


### [94] [Knowledge Integration Decay in Search-Augmented Reasoning of Large Language Models](https://arxiv.org/abs/2602.09517)
*Sangwon Yu,Ik-hwan Kim,Donghun Kang,Bongkyu Hwang,Junhwa Choi,Suk-hoon Jung,Seungki Hong,Taehee Lee,Sungroh Yoon*

Main category: cs.CL

TL;DR: 该论文提出了一个名为SAKE的策略来解决大型语言模型在长时间推理过程中知识集成衰减的问题，通过强调检索知识在整个推理过程中的重要性来提高模型性能。


<details>
  <summary>Details</summary>
Motivation: 论文识别了一个被称为知识集成衰减（KID）的关键问题，即在长时间推理过程中，模型在生成推理后难以有效地利用检索到的证据，即使有相关的信息也是这样。作者提出了SAKE策略来解决KID问题。

Method: SAKE策略在推理过程中通过在开始和结束时锚定检索到的知识，防止其被前期 context 负面影响，从而保持知识的语义完整性。

Result: 在多步问答和复杂推理基准测试中进行了广泛实验，表明SAKE显著缓解了KID问题并提高了性能。

Conclusion: SAKE为大型语言模型中知识集成提供了一个轻量级的有效解决方案，改善了其在复杂任务中的表现。

Abstract: Modern Large Language Models (LLMs) have demonstrated remarkable capabilities in complex tasks by employing search-augmented reasoning to incorporate external knowledge into long chains of thought. However, we identify a critical yet underexplored bottleneck in this paradigm, termed Knowledge Integration Decay (KID). Specifically, we observe that as the length of reasoning generated before search grows, models increasingly fail to integrate retrieved evidence into subsequent reasoning steps, limiting performance even when relevant information is available. To address this, we propose Self-Anchored Knowledge Encoding (SAKE), a training-free inference-time strategy designed to stabilize knowledge utilization. By anchoring retrieved knowledge at both the beginning and end of the reasoning process, SAKE prevents it from being overshadowed by prior context, thereby preserving its semantic integrity. Extensive experiments on multi-hop QA and complex reasoning benchmarks demonstrate that SAKE significantly mitigates KID and improves performance, offering a lightweight yet effective solution for knowledge integration in agentic LLMs.

</details>


### [95] [UniARM: Towards a Unified Autoregressive Reward Model for Multi-Objective Test-Time Alignment](https://arxiv.org/abs/2602.09538)
*Hongyan Xie,Yikun Ban,Ruiyu Fang,Zixuan Huang,Deqing Wang,Jianxin Li,Yitong Yao,Chao Wang,Shuangyong Song*

Main category: cs.CL

TL;DR: 该研究提出了MoSLoRA方法和UniARM模型来解决多目标自回归奖励模型在优化LLM响应时面临的配分特征纠缠和偏好特征之间交互忽略的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的多目标自回归奖励模型方法存在独立训练参数导致特征纠缠或者特征交互忽视的问题，这会导致生成的输出与用户偏好不匹配。

Method: MoSLoRA首先通过无偏好模块提取共享特征，然后通过偏好调节模块在联合偏好向量的条件下对共享特征进行仿射变换。基于此，提出了统一自回归奖励模型UniARM，该模型在一个参数空间中联合建模所有偏好维度，从而避免独立参数的问题。

Result: MoSLoRA和UniARM能够更好地控制偏好权重，减少特征纠缠，提升LLM响应与用户偏好的一致性。

Conclusion: 研究结果显示，UniARM相比传统方法能够更有效且精确地实现多目标自回归奖励模型训练和优化多目标测试时间对齐。

Abstract: Multi-objective alignment aims to align LLM responses with multiple human preference objectives. Among existing methods, guiding the generation of frozen LLMs through autoregressive reward models (ARMs) to accomplish multi-objective test-time alignment is a low-cost solution. However, these methods typically rely on independent parameters for each preference objective, either by training ARMs independently across preference dimensions, which neglects interactions among preference features, or by training a single ARM with separate feature extraction modules for each preference, which can cause feature entanglement. Both strategies can result in misalignment between generated outputs and user preferences. To address this limitation, we propose Preference-Modulated \& Shared Low-Rank Adaptation (MoSLoRA) for ARM training, which first extracts shared features via a preference-agnostic module and then applies affine transformations to shared features via a preference modulation module conditioned on mixed preference vectors. This design mitigates feature entanglement and enables precise control over preference trade-offs during inference. Building on this, we introduce the Unified Autoregressive Reward Model (UniARM), a novel framework for multi-objective test-time alignment. UniARM jointly models all preference dimensions in a single parameter space, eliminating the need for independent parameters for each preference objective. es on larger-scale LLMs, enhancing its practical usability.

</details>


### [96] [Comprehensive Comparison of RAG Methods Across Multi-Domain Conversational QA](https://arxiv.org/abs/2602.09552)
*Klejda Alushi,Jan Strich,Chris Biemann,Martin Semmann*

Main category: cs.CL

TL;DR: 本文研究了RAG方法在多轮对话中的表现，发现一些简单的方法如重排序、混合BM25和HyDE表现出色，而高级技术效果不佳。


<details>
  <summary>Details</summary>
Motivation: 当前大多数RAG研究侧重于单轮对话，本文旨在填补多轮对话中RAG方法系统性比较的空白，探讨历史对话、核心引用和用户意图变化对检索的影响。

Method: 使用统一的实验设置，评估了包括HyDE，混合BM25等在内的不同RAG方法，使用了检索和生成器指标来评估检索质量和答案生成。

Result: 研究发现一些简单方法（如重排序、混合BM25、HyDE）在多轮对话中表现优于常规RAG方法；而一些高级技术未能提供提升甚至低于无RAG基线。

Conclusion: 研究结果表明，有效的对话RAG方法与其检索策略和数据集特征之间的匹配度更为相关，而非方法的复杂度。

Abstract: Conversational question answering increasingly relies on retrieval-augmented generation (RAG) to ground large language models (LLMs) in external knowledge. Yet, most existing studies evaluate RAG methods in isolation and primarily focus on single-turn settings. This paper addresses the lack of a systematic comparison of RAG methods for multi-turn conversational QA, where dialogue history, coreference, and shifting user intent substantially complicate retrieval. We present a comprehensive empirical study of vanilla and advanced RAG methods across eight diverse conversational QA datasets spanning multiple domains. Using a unified experimental setup, we evaluate retrieval quality and answer generation using generator and retrieval metrics, and analyze how performance evolves across conversation turns. Our results show that robust yet straightforward methods, such as reranking, hybrid BM25, and HyDE, consistently outperform vanilla RAG. In contrast, several advanced techniques fail to yield gains and can even degrade performance below the No-RAG baseline. We further demonstrate that dataset characteristics and dialogue length strongly influence retrieval effectiveness, explaining why no single RAG strategy dominates across settings. Overall, our findings indicate that effective conversational RAG depends less on method complexity than on alignment between the retrieval strategy and the dataset structure. We publish the code used.\footnote{\href{https://github.com/Klejda-A/exp-rag.git}{GitHub Repository}}

</details>


### [97] [Advancing Block Diffusion Language Models for Test-Time Scaling](https://arxiv.org/abs/2602.09555)
*Yi Lu,Deyang Kong,Jianing Wang,Linsen Guo,Xue Wang,Qi Guo,Tao Gui,Xuanjing Huang,Wei Ye,Shikun Zhang,Wei Wang*

Main category: cs.CL

TL;DR: 本文提出了一种针对块扩散语言模型在测试时缩放的统一框架，通过在解码和块生成中引入适应性，显著提升了推理速度和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有BDLM在测试时缩放下的探索有限，特别是在长链推理中解码效率和准确性的权衡上存在问题，因此提出了新的框架。

Method: 提出了Bounded Adaptive Confidence Decoding (BACD)和Think Coarse, Critic Fine (TCCF)策略，以及Progressive Block Size Extension方法。

Result: 实验结果表明，利用BACD和TCCF，在TDAR-8B上显著优于TraDo-8B（2.26倍速度提升，AIME24分数提升11.2点）。

Conclusion: 该研究为BDLM在复杂推理任务中的测试时缩放提供了重要的步骤，展示了其在长链条推理上的潜在价值。

Abstract: Recent advances in block diffusion language models have demonstrated competitive performance and strong scalability on reasoning tasks. However, existing BDLMs have limited exploration under the test-time scaling setting and face more severe decoding challenges in long Chain-of-Thought reasoning, particularly in balancing the decoding speed and effectiveness. In this work, we propose a unified framework for test-time scaling in BDLMs that introduces adaptivity in both decoding and block-wise generation. At the decoding level, we propose Bounded Adaptive Confidence Decoding (BACD), a difficulty-aware sampling strategy that dynamically adjusts denoising based on model confidence, accelerating inference while controlling error accumulation. Beyond step-wise adaptivity, we introduce Think Coarse, Critic Fine (TCCF), a test-time scaling paradigm that allocates large block sizes to exploratory reasoning and smaller block sizes to refinement, achieving an effective efficiency-effectiveness balance. To enable efficient and effective decoding with a large block size, we adopt Progressive Block Size Extension, which mitigates performance degradation when scaling block sizes. Extensive experiments show that applying BACD and TCCF to TDAR-8B yields significant improvements over strong baselines such as TraDo-8B (2.26x speedup, +11.2 points on AIME24). These results mark an important step toward unlocking the potential of BDLMs for test-time scaling in complex reasoning tasks.

</details>


### [98] [LEMUR: A Corpus for Robust Fine-Tuning of Multilingual Law Embedding Models for Retrieval](https://arxiv.org/abs/2602.09570)
*Narges Baba Ahmadi,Jan Strich,Martin Semmann,Chris Biemann*

Main category: cs.CL

TL;DR: 该研究介绍了一种名为LEMUR的大型多语言欧盟环境立法语料库，包含了2.5种语言的24,953份官方EUR-Lex PDF文件，并通过优化嵌入模型提高了跨语言检索的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有跨语言检索在法律领域的应用存在不足，如语义检索不可靠和嵌入模型缺乏领域适应性。为了解决这些问题，研究人员构建了LEMUR语料库。

Method: 研究人员通过对比方法量化了PDF到文本转换的质量，并利用LEMUR语料库对三种先进的多语言嵌入模型进行了微调。

Result: 在不同资源水平的语言上进行了检索实验，表明领域微调可以显著提高检索准确性。跨语言评估显示，这些改进在未见过的语言中也能实现。

Conclusion: 研究结果表明，通过领域微调增强的多语言嵌入模型能够显著提升法律检索任务中的表现，尤其是对低资源语言。”

Abstract: Large language models (LLMs) are increasingly used to access legal information. Yet, their deployment in multilingual legal settings is constrained by unreliable retrieval and the lack of domain-adapted, open-embedding models. In particular, existing multilingual legal corpora are not designed for semantic retrieval, and PDF-based legislative sources introduce substantial noise due to imperfect text extraction. To address these challenges, we introduce LEMUR, a large-scale multilingual corpus of EU environmental legislation constructed from 24,953 official EUR-Lex PDF documents covering 25 languages. We quantify the fidelity of PDF-to-text conversion by measuring lexical consistency against authoritative HTML versions using the Lexical Content Score (LCS). Building on LEMUR, we fine-tune three state-of-the-art multilingual embedding models using contrastive objectives in both monolingual and bilingual settings, reflecting realistic legal-retrieval scenarios. Experiments across low- and high-resource languages demonstrate that legal-domain fine-tuning consistently improves Top-k retrieval accuracy relative to strong baselines, with particularly pronounced gains for low-resource languages. Cross-lingual evaluations show that these improvements transfer to unseen languages, indicating that fine-tuning primarily enhances language-independent, content-level legal representations rather than language-specific cues. We publish code\footnote{\href{https://github.com/nargesbh/eur_lex}{GitHub Repository}} and data\footnote{\href{https://huggingface.co/datasets/G4KMU/LEMUR}{Hugging Face Dataset}}.

</details>


### [99] [Aligning Tree-Search Policies with Fixed Token Budgets in Test-Time Scaling of LLMs](https://arxiv.org/abs/2602.09574)
*Sora Miyamoto,Daisuke Oba,Naoaki Okazaki*

Main category: cs.CL

TL;DR: 提出了BG-MCTS，一种与剩余令牌预算对齐的树搜索解码算法，该算法在预算耗尽时优先进行细化和答案完成，而不是在早期阶段过度分支或过早终止。


<details>
  <summary>Details</summary>
Motivation: 现有树搜索策略通常对预算不敏感，将其作为终止条件，导致晚期过度分支或过早终止问题。BG-MCTS旨在解决这一问题。

Method: BG-MCTS算法从广泛的探索开始，随着预算的减少，优先进行细化和答案完成，减少浅层节点的晚期分支。

Result: 在MATH500和AIME24/25数据集上的实验表明，BG-MCTS在不同预算下优于传统预算无关的树搜索基线。

Conclusion: 提出BG-MCTS方法，为解决大型语言模型部署中固定的查询令牌预算问题提供了一种有效方法。

Abstract: Tree-search decoding is an effective form of test-time scaling for large language models (LLMs), but real-world deployment imposes a fixed per-query token budget that varies across settings. Existing tree-search policies are largely budget-agnostic, treating the budget as a termination condition, which can lead to late-stage over-branching or premature termination. We propose {Budget-Guided MCTS} (BG-MCTS), a tree-search decoding algorithm that aligns its search policy with the remaining token budget: it starts with broad exploration, then prioritizes refinement and answer completion as the budget depletes while reducing late-stage branching from shallow nodes. BG-MCTS consistently outperforms budget-agnostic tree-search baselines across different budgets on MATH500 and AIME24/25 with open-weight LLMs.

</details>


### [100] [Context-Aware Counterfactual Data Augmentation for Gender Bias Mitigation in Language Models](https://arxiv.org/abs/2602.09590)
*Shweta Parihar,Liu Guangliang,Natalie Parde,Lu Cheng*

Main category: cs.CL

TL;DR: 本文提出了一种简单有效的Context-CDA方法，它通过使用大型语言模型增强消歧 Meta-模型的多样性与上下文相关性，同时利用不确定性筛选排除低质量的生成反事实数据，从而在保持语言建模性能的同时有效缓解社会偏见。


<details>
  <summary>Details</summary>
Motivation: 当前的语言模型在经过微调后可能会减少语言建模能力，进而影响下游任务。本文旨在解决这一问题，特别针对消除社会偏见带来的潜在性能下降。

Method: 本文提出了一个名为‘Context-CDA’的简单方法，利用大型语言模型增加反事实消歧数据的多样性和上下文相关性，同时采用基于不确定性的筛选标准去除质量不佳的生成反事实数据。

Result: 实验结果表明，Context-CDA方法能够有效减少性别偏见而不会牺牲语言建模性能，并通过分析下一标记生成概率的变化提供了关于社会偏见的见解。

Conclusion: 本研究提出了一种新颖且有效的技术方法，有助于在消除社会偏见的同时保持语言模型的性能，为后续研究提供了新的思路。

Abstract: A challenge in mitigating social bias in fine-tuned language models (LMs) is the potential reduction in language modeling capability, which can harm downstream performance. Counterfactual data augmentation (CDA), a widely used method for fine-tuning, highlights this issue by generating synthetic data that may align poorly with real-world distributions or creating overly simplistic counterfactuals that ignore the social context of altered sensitive attributes (e.g., gender) in the pretraining corpus. To address these limitations, we propose a simple yet effective context-augmented CDA method, Context-CDA, which uses large LMs to enhance the diversity and contextual relevance of the debiasing corpus. By minimizing discrepancies between the debiasing corpus and pretraining data through augmented context, this approach ensures better alignment, enhancing language modeling capability. We then employ uncertainty-based filtering to exclude generated counterfactuals considered low-quality by the target smaller LMs (i.e., LMs to be debiased), further improving the fine-tuning corpus quality. Experimental results on gender bias benchmarks demonstrate that Context-CDA effectively mitigates bias without sacrificing language modeling performance while offering insights into social biases by analyzing distribution shifts in next-token generation probabilities.

</details>


### [101] [On the Optimal Reasoning Length for RL-Trained Language Models](https://arxiv.org/abs/2602.09591)
*Daisuke Nohara,Taishi Nakamura,Rio Yokota*

Main category: cs.CL

TL;DR: 本研究比较了多种长度控制方法在Qwen3-1.7B Base和DeepSeek-R1-Distill-Qwen-1.5B两个模型上的效果，发现长度惩罚可能阻碍推理能力的获取，而适当调校的长度控制可以提高效率。研究表明，过长的输出会导致结果分散，过短的输出可能导致思考不足。


<details>
  <summary>Details</summary>
Motivation: 为了为大型语言模型的推理提供长度控制的最佳策略，减少计算成本同时保持效率和性能的平衡。

Method: 通过比较不同长度控制方法对两个具有不同规模和训练方式的模型的影响，识别有效策略。

Result: 研究结果显示，长度惩罚可能不利于模型的推理能力获取，但如果适当调整，可以提高效率。此外，研究还发现了两种失败模式：过长输出会增加结果的分散性和过短输出会导致过度简化的推理。

Conclusion: 适当的长度控制可以在增强推理效率的同时避免上述两种失败模式，应该根据模型的具体情况为其调优长度控制策略。

Abstract: Reinforcement learning substantially improves reasoning in large language models, but it also tends to lengthen chain of thought outputs and increase computational cost during both training and inference. Though length control methods have been proposed, it remains unclear what the optimal output length is for balancing efficiency and performance. In this work, we compare several length control methods on two models, Qwen3-1.7B Base and DeepSeek-R1-Distill-Qwen-1.5B. Our results indicate that length penalties may hinder reasoning acquisition, while properly tuned length control can improve efficiency for models with strong prior reasoning. By extending prior work to RL trained policies, we identify two failure modes, 1) long outputs increase dispersion, and 2) short outputs lead to under-thinking.

</details>


### [102] [AlignTune: Modular Toolkit for Post-Training Alignment of Large Language Models](https://arxiv.org/abs/2602.09621)
*R E Zera Marveen Lyngkhoi,Chirag Chawla,Pratinav Seth,Utsav Avaiya,Soham Bhattacharjee,Mykola Khandoga,Rui Yuan,Vinay Kumar Sankarapu*

Main category: cs.CL

TL;DR: 介绍了一种名为AlignTune的模块化工具包，统一了监督微调（SFT）和RLHF风格优化的接口，支持TRL和Unsloth后端，并提供标准化配置、可扩展的奖励层以及基准评估。


<details>
  <summary>Details</summary>
Motivation: 旨在减少现有工作流程中的后端干扰、奖励碎片化和不可再现的管道问题，提高大语言模型对齐研究的可重复性。

Method: 开发了一种模块化工具包Framework，并将特定后端的逻辑隔离在一个工厂边界内，提供统一的监督微调和RLHF风格优化接口。

Result: AlignTune提供了标准化配置、可扩展的奖励层以及基准评估功能，支持TRL和Unsloth后端，隔离后端特定逻辑。

Conclusion: 这项工作通过引入AlignTune工具，简化了大语言模型对齐的研究流程，提高了研究的可重复性和可控性。

Abstract: Post-training alignment is central to deploying large language models (LLMs), yet practical workflows remain split across backend-specific tools and ad-hoc glue code, making experiments hard to reproduce. We identify backend interference, reward fragmentation, and irreproducible pipelines as key obstacles in alignment research. We introduce AlignTune, a modular toolkit exposing a unified interface for supervised fine-tuning (SFT) and RLHF-style optimization with interchangeable TRL and Unsloth backends. AlignTune standardizes configuration, provides an extensible reward layer (rule-based and learned), and integrates evaluation over standard benchmarks and custom tasks. By isolating backend-specific logic behind a single factory boundary, AlignTune enables controlled comparisons and reproducible alignment experiments.

</details>


### [103] [MILE-RefHumEval: A Reference-Free, Multi-Independent LLM Framework for Human-Aligned Evaluation](https://arxiv.org/abs/2602.09624)
*Nalin Srun,Parisa Rastin,Guénaël Cabanes,Lydia Boudjeloud Assala*

Main category: cs.CL

TL;DR: MILE-RefHumEval 提出了一种无参考框架，通过人类对齐的评姪方案和独立提示的评姪者，无需真实答案或协调者，适用于评估大型语言模型。


<details>
  <summary>Details</summary>
Motivation: 当前评估方法依赖于真实答案和协调者的管理，MILE-RefHumEval 提出了一种无参考框架来解决这些问题，提供了更适合实际应用的解决方案。

Method: MILE-RefHumEval 使用人类对齐的评姪方案和独立提示的评姪者，通过任务特定的提示对最佳候选选择、总结、图像标题和对话进行了评估。

Result: 实验结果显示，MILE-RefHumEval 在结果上接近人类判断，优于以往方法，并减少了计算成本。

Conclusion: MILE-RefHumEval 提供了一种高效、稳健且与人类对齐的大型语言模型评估方法，有助于实际应用中的评姪需求。

Abstract: We introduce MILE-RefHumEval, a reference-free framework for evaluating Large Language Models (LLMs) without ground-truth annotations or evaluator coordination. It leverages an ensemble of independently prompted evaluators guided by a human-aligned schema, supporting both discrete and continuous scoring judgement. With task-specific prompts from best candidate selection, summarization and image captioning to dialogue, MILE-RefHumEval provides flexible, interpretable, and scalable assessments. Experiments show it aligns closely with human judgments, outperforms prior methods, and reduces computational overhead, offering an efficient, robust, and human-aligned solution for real-world LLM evaluation.

</details>


### [104] [MATA: Multi-Agent Framework for Reliable and Flexible Table Question Answering](https://arxiv.org/abs/2602.09642)
*Sieun Hyeon,Jusang Oh,Sunghwan Steve Cho,Jaeyoung Do*

Main category: cs.CL

TL;DR: MATA 是一个多智能体表格问答框架，利用多种互补的推理路径和小型语言模型工具，提高表格理解任务的可靠性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型虽然在表格问答任务中取得了显著进展，但仍面临可靠性、可扩展性和效率等方面的挑战，特别是在资源有限或隐私敏感的环境中。为此，本文提出了 MATA 框架。

Method: MATA 通过多种推理风格生成候选答案，并借助工具进行优化选择。同时，设计了算法减少昂贵的大型语言模型调用次数，从而提升整体效率。

Result: MATA 在不同难度级别的两个基准测试集上表现出色，能够实现最先进的准确性和高效的推理，而在使用小型开源模型时表现同样出色。

Conclusion: MATA 通过精心安排多种推理路径，提高了表格问答任务的可扩展性和可靠性。

Abstract: Recent advances in Large Language Models (LLMs) have significantly improved table understanding tasks such as Table Question Answering (TableQA), yet challenges remain in ensuring reliability, scalability, and efficiency, especially in resource-constrained or privacy-sensitive environments. In this paper, we introduce MATA, a multi-agent TableQA framework that leverages multiple complementary reasoning paths and a set of tools built with small language models. MATA generates candidate answers through diverse reasoning styles for a given table and question, then refines or selects the optimal answer with the help of these tools. Furthermore, it incorporates an algorithm designed to minimize expensive LLM agent calls, enhancing overall efficiency. MATA maintains strong performance with small, open-source models and adapts easily across various LLM types. Extensive experiments on two benchmarks of varying difficulty with ten different LLMs demonstrate that MATA achieves state-of-the-art accuracy and highly efficient reasoning while avoiding excessive LLM inference. Our results highlight that careful orchestration of multiple reasoning pathways yields scalable and reliable TableQA. The code is available at https://github.com/AIDAS-Lab/MATA.

</details>


### [105] [Life Cycle-Aware Evaluation of Knowledge Distillation for Machine Translation: Environmental Impact and Translation Quality Trade-offs](https://arxiv.org/abs/2602.09691)
*Joseph Attieh,Timothee Mickus,Anne-Laure Ligozat,Aurélie Névéol,Jörg Tiedemann*

Main category: cs.CL

TL;DR: 研究通过考虑翻译质量和计算成本，评估了知识蒸馏方法，发现蒸馏开销在小部署量时占主导地位，推理计算在大规模时占主导地位，使知识蒸馏方法仅在任务相关的使用阈值以上才有益。词级知识蒸馏通常提供更可喜的计算成本和质量权衡。


<details>
  <summary>Details</summary>
Motivation: 介绍当前知识蒸馏（KD）在机器翻译中的研究现状，如仅报告学生模型的质量而忽略KD过程中的计算复杂性和环境影响。这使得在计算资源有限的情况下难以选择适当的KD方法。因此，为了解决这一问题，研究者旨在评估不同KD方法的性能，同时考虑它们对环境的影响。

Method: 使用机器学习生命周期评估（MLCA）工具，将计算成本量化为碳足迹，涵盖KD模型生命周期中的各种排放和成本（教师训练、蒸馏、推理）。研究通过量化不同KD方法翻译质量和环境成本，确定其适用场景。

Result: 研究发现：(i) 在小部署量时，蒸馏开销是碳足迹的主要部分；(ii) 在大规模部署时，推理是主要开销，使KD方法只有在达到特定任务依赖使用阈值才具有优势；(iii) 与序列级KD方法相比，词级KD方法提供了更优的碳足迹和质量权衡。

Conclusion: 本研究为在明确的质量和计算资源限制下选择知识蒸馏方法提供了一套可重复的指南，有助于指导实际应用中如何平衡翻译质量和环境影响。

Abstract: Knowledge distillation (KD) is a tool to compress a larger system (teacher) into a smaller one (student). In machine translation, studies typically report only the translation quality of the student and omit the computational complexity of performing KD, making it difficult to select among the many available KD choices under compute-induced constraints. In this study, we evaluate representative KD methods by considering both translation quality and computational cost. We express computational cost as a carbon footprint using the machine learning life cycle assessment (MLCA) tool. This assessment accounts for runtime operational emissions and amortized hardware production costs throughout the KD model life cycle (teacher training, distillation, and inference). We find that (i) distillation overhead dominates the total footprint at small deployment volumes, (ii) inference dominates at scale, making KD beneficial only beyond a task-dependent usage threshold, and (iii) word-level distillation typically offers more favorable footprint-quality trade-offs than sequence-level distillation. Our protocol provides reproducible guidance for selecting KD methods under explicit quality and compute-induced constraints.

</details>


### [106] [Maastricht University at AMIYA: Adapting LLMs for Dialectal Arabic using Fine-tuning and MBR Decoding](https://arxiv.org/abs/2602.09703)
*Abdulhai Alali,Abderrahmane Issam*

Main category: cs.CL

TL;DR: 该研究通过从预训练的大型语言模型中采用低秩适应（LoRA）微调、适配器合并以及方言感知最多后验（MBR）解码，以提高方言表达的准确性和翻译质量，尤其在叙利亚阿拉伯语、摩洛哥阿拉伯语和沙特阿拉伯语上取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型虽然支持多种语言，但方言表达仍然较为欠缺，因此需通过特定技术提升方言模型的表现。

Method: 采用了低秩适应（LoRA）微调，在单语言和英语方言平行数据上进行微调；适配器合并以及方言感知最多后验（MBR）解码来提高生成和翻译的方言吻合度。

Result: 在叙利亚阿拉伯语、摩洛哥阿拉伯语和沙特阿拉伯语等方言上，适配器合并和MBR方法提高了方言表达的准确性，同时保持了语义的准确性。

Conclusion: 研究提供了一种紧凑且有效的框架，以增强大语言模型在生成及翻译方言方面的表现。

Abstract: Large Language Models (LLMs) are becoming increasingly multilingual, supporting hundreds of languages, especially high resource ones. Unfortunately, Dialect variations are still underrepresented due to limited data and linguistic variation. In this work, we adapt a pre-trained LLM to improve dialectal performance. Specifically, we use Low Rank Adaptation (LoRA) fine-tuning on monolingual and English Dialect parallel data, adapter merging and dialect-aware MBR decoding to improve dialectal fidelity generation and translation. Experiments on Syrian, Moroccan, and Saudi Arabic show that merging and MBR improve dialectal fidelity while preserving semantic accuracy. This combination provides a compact and effective framework for robust dialectal Arabic generation.

</details>


### [107] [TraceMem: Weaving Narrative Memory Schemata from User Conversational Traces](https://arxiv.org/abs/2602.09712)
*Yiming Shu,Pei Liu,Tiange Zhang,Ruiyang Gao,Jun Ma,Chen Sun*

Main category: cs.CL

TL;DR: TraceMem 是一种基于认知的框架，通过三个阶段处理用户的对话痕迹，形成结构化的叙事记忆模式，从而增强长期交互的能力。


<details>
  <summary>Details</summary>
Motivation: 现有的对话历史处理方法通常会将对话片段化，忽略对话流中的叙述连贯性，导致长期交互困难。

Method: TraceMem 通过短期记忆处理、突触记忆巩固和系统记忆巩固这三个阶段，利用主题分割和层级聚类等技术构建叙事记忆模式。

Result: 在 LoCoMo 基准测试上，TraceMem 达到了最先进的性能，尤其是在多跳推理和时间推理方面超过了基准模型。

Conclusion: 该研究展示了基于大脑启发式架构在深度叙述理解中的重要性，并对未来记忆系统领域的发展进行了展望。

Abstract: Sustaining long-term interactions remains a bottleneck for Large Language Models (LLMs), as their limited context windows struggle to manage dialogue histories that extend over time. Existing memory systems often treat interactions as disjointed snippets, failing to capture the underlying narrative coherence of the dialogue stream. We propose TraceMem, a cognitively-inspired framework that weaves structured, narrative memory schemata from user conversational traces through a three-stage pipeline: (1) Short-term Memory Processing, which employs a deductive topic segmentation approach to demarcate episode boundaries and extract semantic representation; (2) Synaptic Memory Consolidation, a process that summarizes episodes into episodic memories before distilling them alongside semantics into user-specific traces; and (3) Systems Memory Consolidation, which utilizes two-stage hierarchical clustering to organize these traces into coherent, time-evolving narrative threads under unifying themes. These threads are encapsulated into structured user memory cards, forming narrative memory schemata. For memory utilization, we provide an agentic search mechanism to enhance reasoning process. Evaluation on the LoCoMo benchmark shows that TraceMem achieves state-of-the-art performance with a brain-inspired architecture. Analysis shows that by constructing coherent narratives, it surpasses baselines in multi-hop and temporal reasoning, underscoring its essential role in deep narrative comprehension. Additionally, we provide an open discussion on memory systems, offering our perspectives and future outlook on the field. Our code implementation is available at: https://github.com/YimingShu-teay/TraceMem

</details>


### [108] [AI-Assisted Scientific Assessment: A Case Study on Climate Change](https://arxiv.org/abs/2602.09723)
*Christian Buck,Levke Caesar,Michelle Chen Huebscher,Massimiliano Ciaramita,Erich M. Fischer,Zeke Hausfather,Özge Kart Tokmak,Reto Knutti,Markus Leippold,Joseph Ludescher,Katharine J. Mach,Sofia Palazzo Corner,Kasra Rafiezadeh Shahi,Johan Rockström,Joeri Rogelj,Boris Sakschewski*

Main category: cs.CL

TL;DR: 该研究在气候科学领域使用基于Gemini的AI环境进行协作科学评估，通过13名科学家的合作测试，结果显示AI能够加速科学流程，并提供一致性和质量的帮助，但专家的参与对于报告的严谨性和接受度至关重要。


<details>
  <summary>Details</summary>
Motivation: 提出了一种新的AI科学家范式，旨在解决可重复验证的任务，但在无法进行重复评估的问题上需要依赖理论和现有证据的合成人。此研究旨在探索AI在复杂科学问题上的应用，例如大西洋经向翻转环流的稳定性。

Method: 研究团队将基于Gemini的AI环境集成到标准科学工作流程中，并与13名气候科学家合作，针对一个复杂的科学主题进行了测试。

Result: 研究人员发现，AI可以显著加速科学工作流程，AI生成的内容有79%被保留，增强了报告的逻辑一致性与展示质量，但只有少于一半的内容由AI生成，严格的科学标准需要专家进行大量审核。

Conclusion: 研究表明，AI可以作为科研中加速工作的有效工具，在保持科研质量和服务于科学社群的目标上需要适当的人机协作。

Abstract: The emerging paradigm of AI co-scientists focuses on tasks characterized by repeatable verification, where agents explore search spaces in 'guess and check' loops. This paradigm does not extend to problems where repeated evaluation is impossible and ground truth is established by the consensus synthesis of theory and existing evidence. We evaluate a Gemini-based AI environment designed to support collaborative scientific assessment, integrated into a standard scientific workflow. In collaboration with a diverse group of 13 scientists working in the field of climate science, we tested the system on a complex topic: the stability of the Atlantic Meridional Overturning Circulation (AMOC). Our results show that AI can accelerate the scientific workflow. The group produced a comprehensive synthesis of 79 papers through 104 revision cycles in just over 46 person-hours. AI contribution was significant: most AI-generated content was retained in the report. AI also helped maintain logical consistency and presentation quality. However, expert additions were crucial to ensure its acceptability: less than half of the report was produced by AI. Furthermore, substantial oversight was required to expand and elevate the content to rigorous scientific standards.

</details>


### [109] [Targum -- A Multilingual New Testament Translation Corpus](https://arxiv.org/abs/2602.09724)
*Maciej Rapacz,Aleksander Smywiński-Pohl*

Main category: cs.CL

TL;DR: 本文介绍了包含657个新约译本的多语言语料库，特别强调五种语言的独特译本，为研究人员提供了一个灵活多层次分析的资源，促进了翻译历史的定量研究。


<details>
  <summary>Details</summary>
Motivation: 现有的语料库在追求语言多样性时往往忽略了丰富的圣经翻译历史，本文旨在填补这一空白。

Method: 语料库由12个在线圣经图书馆和一个现有的语料库聚合而成，每个译本都进行了人工元数据标注，将其与标准化的工作标识符、特定版本和修订年份进行关联。

Result: 这是一个包含657个新约译本的多语言语料库，特别是五种语言（英语、法语、意大利语、波兰语和西班牙语）中有丰富的独特版本，为灵活、多层次的研究奠定了基础。

Conclusion: 本研究建立了一个新的基准，促进了翻译历史的定量研究，提供了独特性和多样性的定义灵活性。

Abstract: Many European languages possess rich biblical translation histories, yet existing corpora - in prioritizing linguistic breadth - often fail to capture this depth. To address this gap, we introduce a multilingual corpus of 657 New Testament translations, of which 352 are unique, with unprecedented depth in five languages: English (208 unique versions from 396 total), French (41 from 78), Italian (18 from 33), Polish (30 from 48), and Spanish (55 from 102). Aggregated from 12 online biblical libraries and one preexisting corpus, each translation is manually annotated with metadata that maps the text to a standardized identifier for the work, its specific edition, and its year of revision. This canonicalization empowers researchers to define "uniqueness" for their own needs: they can perform micro-level analyses on translation families, such as the KJV lineage, or conduct macro-level studies by deduplicating closely related texts. By providing the first resource designed for such flexible, multilevel analysis, our corpus establishes a new benchmark for the quantitative study of translation history.

</details>


### [110] [Improving Interpretability of Lexical Semantic Change with Neurobiological Features](https://arxiv.org/abs/2602.09760)
*Kohei Oda,Hiroya Takamura,Kiyoaki Shirai,Natthawut Kertkeidkachorn*

Main category: cs.CL

TL;DR: 该研究提出了一种方法，将预训练语言模型获得的词语上下文化嵌入的空间映射到神经生物学特征空间，以增强词义演变（LSC）的可解释性。


<details>
  <summary>Details</summary>
Motivation: 大多数关于LSC的研究集中在提高LSC程度估计性能，但往往难以解释词义如何变化。因此，本文尝试提升LSC的可解释性。

Method: 通过将词语的上下文化嵌入空间映射到神经生物学特征空间，每个维度代表词语的一个基本特征，其值表示该特征的强度。

Result: 实验表明，该方法在估计LSC程度上的性能优于大多数以前的方法，并且展示了较高的可解释性。

Conclusion: 除了发现之前研究中未注意到的LSC类型外，该方法还有效搜索具有特定类型LSC的词语。

Abstract: Lexical Semantic Change (LSC) is the phenomenon in which the meaning of a word change over time. Most studies on LSC focus on improving the performance of estimating the degree of LSC, however, it is often difficult to interpret how the meaning of a word change. Enhancing the interpretability of LSC is a significant challenge as it could lead to novel insights in this field. To tackle this challenge, we propose a method to map the semantic space of contextualized embeddings of words obtained by a pre-trained language model to a neurobiological feature space. In the neurobiological feature space, each dimension corresponds to a primitive feature of words, and its value represents the intensity of that feature. This enables humans to interpret LSC systematically. When employed for the estimation of the degree of LSC, our method demonstrates superior performance in comparison to the majority of the previous methods. In addition, given the high interpretability of the proposed method, several analyses on LSC are carried out. The results demonstrate that our method not only discovers interesting types of LSC that have been overlooked in previous studies but also effectively searches for words with specific types of LSC.

</details>


### [111] [Where Are We At with Automatic Speech Recognition for the Bambara Language?](https://arxiv.org/abs/2602.09785)
*Seydou Diallo,Yacouba Diarra,Mamadou K. Keita,Panga Azazia Kamaté,Adam Bouno Kampo,Aboubacar Ouattara*

Main category: cs.CL

TL;DR: 该研究引入了第一个巴马拉语自动语音识别（ASR）标准化基准，基于一小时的专业朗读马里宪法文本，评估了37种模型。尽管在最佳条件下，顶尖模型的错误率仍高于实际应用标准，尤其是多语言预训练和模型规模扩大不能解决欠代表语言的问题。


<details>
  <summary>Details</summary>
Motivation: 该研究致力于为巴马拉语建立自动语音识别的标准基准，以促进语言技术在该语言中的发展。

Method: 通过收集和分析一小时的专业朗读文本，对比现有37种不同训练程度的模型（包括本地基准系统和大规模商业模型），研究者们确定了当前ASR性能在窄领域内的实际水平。

Result: 研究发现，在最佳条件下，最优模型的词错误率仍然高达46.76%，而字符错误率（CER）最好的成绩为13.00%。许多多语言预训练模型的表现甚至超过100%的词错误率。

Conclusion: 研究结论指出，当前ASR技术尚不能满足实际部署标准，尤其是在对未代表语言的支持上，多语言预训练和模型规模增加不够有效。未来还需要进一步探索适合欠代表语言技术发展的解决方案。同时，研究成果已公布，为研究者提供了公开的排行榜和基准数据，以促进透明度和持续研究。

Abstract: This paper introduces the first standardized benchmark for evaluating Automatic Speech Recognition (ASR) in the Bambara language, utilizing one hour of professionally recorded Malian constitutional text. Designed as a controlled reference set under near-optimal acoustic and linguistic conditions, the benchmark was used to evaluate 37 models, ranging from Bambara-trained systems to large-scale commercial models. Our findings reveal that current ASR performance remains significantly below deployment standards in a narrow formal domain; the top-performing system in terms of Word Error Rate (WER) achieved 46.76\% and the best Character Error Rate (CER) of 13.00\% was set by another model, while several prominent multilingual models exceeded 100\% WER. These results suggest that multilingual pre-training and model scaling alone are insufficient for underrepresented languages. Furthermore, because this dataset represents a best-case scenario of the most simplified and formal form of spoken Bambara, these figures are yet to be tested against practical, real-world settings. We provide the benchmark and an accompanying public leaderboard to facilitate transparent evaluation and future research in Bambara speech technology.

</details>


### [112] [Decomposing Reasoning Efficiency in Large Language Models](https://arxiv.org/abs/2602.09805)
*Daniel Kaiser,Arnoldo Frigessi,Ali Ramezani-Kebrya,Benjamin Ricaud*

Main category: cs.CL

TL;DR: 该研究引入了一种跟踪可选框架来分解大语言模型在推理中的令牌效率，包括在固定令牌预算下的完成度、条件正确率和冗余度，分析了25个模型在CogniLoad上的表现，并发现准确性和令牌效率的排名差距较大。


<details>
  <summary>Details</summary>
Motivation: 当前对大型语言模型的评估主要关注最终的准确性，而忽略了推理过程中令牌的实际使用情况。这项研究旨在通过引入可选跟踪框架来更细致地分析大语言模型的令牌效率问题，以期改进现有模型和促进其优化。

Method: 研究人员提出了一个可选跟踪框架，通过分解令牌效率为完成度、条件正确率和冗余度三个部分，并进一步将冗余度分为平均言语开销和负载系数，同时利用可获得的任务数据来进一步细化冗余度的定义。此外，研究人员利用推理痕迹来区分冗长但有效的推理解释与退化循环，并在CogniLoad基准测试集上评估了25个模型。

Result: 研究发现，在25个模型中，准确性和令牌效率的排名并不一致（Spearman相关系数ρ=0.63），效率差距通常由条件正确率驱动，冗余度也相差巨大（约9倍）。这意味着不同的瓶颈特征存在，并可能指向不同时效干预措施。

Conclusion: 从冗余度的具体构成和效率排名的差异可以推断出，提高令牌效率的有效方法可能因模型不同而有所差异。研究人员建议应专注于那些在准确性和令牌效率之间存在显著差异的模型，并针对特定瓶颈进行优化。

Abstract: Large language models trained for reasoning trade off inference tokens against accuracy, yet standard evaluations report only final accuracy, obscuring where tokens are spent or wasted. We introduce a trace-optional framework that decomposes token efficiency into interpretable factors: completion under a fixed token budget (avoiding truncation), conditional correctness given completion, and verbosity (token usage). When benchmark metadata provides per-instance workload proxies, we further factor verbosity into two components: mean verbalization overhead (tokens per work unit) and a coupling coefficient capturing how overhead scales with task workload. When reasoning traces are available, we add deterministic trace-quality measures (grounding, repetition, prompt copying) to separate degenerate looping from verbose-but-engaged reasoning, avoiding human labeling and LLM judges. Evaluating 25 models on CogniLoad, we find that accuracy and token-efficiency rankings diverge (Spearman $ρ=0.63$), efficiency gaps are often driven by conditional correctness, and verbalization overhead varies by about 9 times (only weakly related to model scale). Our decomposition reveals distinct bottleneck profiles that suggest different efficiency interventions.

</details>


### [113] [From FusHa to Folk: Exploring Cross-Lingual Transfer in Arabic Language Models](https://arxiv.org/abs/2602.09826)
*Abdulmuizz Khalak,Abderrahmane Issam,Gerasimos Spanakis*

Main category: cs.CL

TL;DR: 研究发现，虽然阿拉伯语言模型可以在不同方言之间进行部分跨语言迁移，但迁移的程度因方言而异，且迁移到较远方言效果较差。同时，研究还发现支持所有阿拉伯方言的模型可能产生负面干扰。


<details>
  <summary>Details</summary>
Motivation: 研究阿拉伯语言模型在不同方言间跨语言迁移的可能性及其影响，特别是在面临方言具有不同程度MSA相似性的情况下的迁移效果。

Method: 通过使用探针技术在三项自然语言处理任务和表示相似性方面进行实验来探索阿拉伯语言模型的跨语言迁移。

Result: 实验结果表明，跨语言迁移是可能的，但不同方言间的效果参差不齐，部分可以通过地理接近性来解释。此外，支持所有阿拉伯方言的模型存在负面干扰。

Conclusion: 研究结果强调了阿拉伯方言之间的跨语言迁移存在挑战，同时也需要注意多方言支持模型的有效性。

Abstract: Arabic Language Models (LMs) are pretrained predominately on Modern Standard Arabic (MSA) and are expected to transfer to its dialects. While MSA as the standard written variety is commonly used in formal settings, people speak and write online in various dialects that are spread across the Arab region. This poses limitations for Arabic LMs, since its dialects vary in their similarity to MSA. In this work we study cross-lingual transfer of Arabic models using probing on 3 Natural Language Processing (NLP) Tasks, and representational similarity. Our results indicate that transfer is possible but disproportionate across dialects, which we find to be partially explained by their geographic proximity. Furthermore, we find evidence for negative interference in models trained to support all Arabic dialects. This questions their degree of similarity, and raises concerns for cross-lingual transfer in Arabic models.

</details>


### [114] [LLM Reasoning Predicts When Models Are Right: Evidence from Coding Classroom Discourse](https://arxiv.org/abs/2602.09832)
*Bakhtawar Ahtisham,Kirk Vanacore,Zhuqian Zhou,Jinsook Lee,Rene F. Kizilcec*

Main category: cs.CL

TL;DR: 该研究通过分析大型语言模型生成的推理来预测其标签的准确性，发现因果语言更倾向于是正确推理，并改进了错误检测模型。


<details>
  <summary>Details</summary>
Motivation: 当前的自动化教育对话分析管道缺乏可靠的方法来检测模型的错误，因此需要一种新的技术来提高准确性。

Method: 利用来自30,300个对话片段的数据，每个片段由多个最先进的LLM进行标记，利用TF-IDF编码推理，并使用随机森林分类器进行监督学习。

Result: 随机森林分类器在F1得分为0.83（召回率为0.854），表现出色，并且特定于特定教育指令构造的专家检测器能够进一步提高复杂构造的性能。

Conclusion: 这表明基于推理的错误检测提供了一种实用且可扩展的质量控制方法，适用于自动教育对话分析。

Abstract: Large Language Models (LLMs) are increasingly deployed to automatically label and analyze educational dialogue at scale, yet current pipelines lack reliable ways to detect when models are wrong. We investigate whether reasoning generated by LLMs can be used to predict the correctness of a model's own predictions. We analyze 30,300 teacher utterances from classroom dialogue, each labeled by multiple state-of-the-art LLMs with an instructional move construct and an accompanying reasoning. Using human-verified ground-truth labels, we frame the task as predicting whether a model's assigned label for a given utterance is correct. We encode LLM reasoning using Term Frequency-Inverse Document Frequency (TF-IDF) and evaluate five supervised classifiers. A Random Forest classifier achieves an F1 score of 0.83 (Recall = 0.854), successfully identifying most incorrect predictions and outperforming baselines. Training specialist detectors for specific instructional move constructs further improves performance on difficult constructs, indicating that error detection benefits from construct-specific linguistic cues. Using the Linguistic Inquiry and Word Count (LIWC) framework, we examine four linguistic markers of correctness: Causation, Differentiation, Tentativeness, and Insight. Correct predictions exhibit grounded causal language (e.g., because, therefore), while incorrect reasoning is substantially more likely to rely on epistemic hedging (e.g., might, could) and performative metacognition (e.g., think, realize). Syntactic complexity does not distinguish correct from incorrect reasoning, and longer reasoning is not more reliable. These findings demonstrate that reasoning-based error detection offers a practical and scalable approach to quality control in automated educational dialogue analysis.

</details>


### [115] [How Do People Quantify Naturally: Evidence from Mandarin Picture Description](https://arxiv.org/abs/2602.09838)
*Yayun Zhang,Guanyi Chen,Fahime Same,Saad Mahamood,Tingting He*

Main category: cs.CL

TL;DR: 本研究通过无明确指示的描述任务，探讨了普通话说话人在描述场景中的量化行为。结果表明，物体数量、有生命属性以及产出模态对量化行为有系统性影响。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索说话人在日常自然描述中是否、如何以及以何种方式使用量化方法表达数量，尤其是在没有明确指示的情况下。

Method: 研究采用了基于图片的激发式描述任务，让参与者自由描述包含多个物体的场景。通过分析参与者在口头和书面描述中的量化使用情况，来探索量化策略的选择和表现。

Result: 研究结果显示，物体数量越多，参与者更少选择量化，且量化表达的精确度也较低。有生命物体及产出模态对量化策略的选择有一定的调节作用。

Conclusion: 研究证明，量化行为可以在非限制条件下进行研究，并提供了用于语言生产中数量表达进一步分析的自然数据集。

Abstract: Quantification is a fundamental component of everyday language use, yet little is known about how speakers decide whether and how to quantify in naturalistic production. We investigate quantification in Mandarin Chinese using a picture-based elicited description task in which speakers freely described scenes containing multiple objects, without explicit instructions to count or quantify. Across both spoken and written modalities, we examine three aspects of quantification: whether speakers choose to quantify at all, how precise their quantification is, and which quantificational strategies they adopt. Results show that object numerosity, animacy, and production modality systematically shape quantificational behaviour. In particular, increasing numerosity reduces both the likelihood and the precision of quantification, while animate referents and modality selectively modulate strategy choice. This study demonstrates how quantification can be examined under unconstrained production conditions and provides a naturalistic dataset for further analyses of quantity expression in language production.

</details>


### [116] [SinFoS: A Parallel Dataset for Translating Sinhala Figures of Speech](https://arxiv.org/abs/2602.09866)
*Johan Sofalas,Dilushri Pavithra,Nevidu Jayatilleke,Ruvan Weerasinghe*

Main category: cs.CL

TL;DR: 该研究创建了一个包含2,344个斯里兰卡成语的语料库，涉及文化与跨语言注释。通过二元分类器实现了约92%的准确性，并评估了现有LLM在该数据集上的表现，发现其在表达习语含义方面存在显著不足。语料库的公开发布为低资源NLP和文化敏感机器翻译提供了重要基准。


<details>
  <summary>Details</summary>
Motivation: 动机在于解决神经机器翻译在处理低资源语言如斯里兰卡语中的成语时因数据稀缺而面临的问题。

Method: 研究团队创建了一个标注语料库，其中包含文化和跨语言注释，用于分类成语的文化来源和寻找跨语言对应物。开发了一个二元分类器来区分数据集中的两种FoS类型，并评估了当前语言模型的性能。

Result: 研究发现，现有语言模型在传达成语含义方面存在不足，并达到了约92%的分类准确率。

Conclusion: 该研究通过提供一个标注好的语料库，旨在为未来低资源语言自然语言处理（NLP）和文化意识机器翻译的研究提供基准。

Abstract: Figures of Speech (FoS) consist of multi-word phrases that are deeply intertwined with culture. While Neural Machine Translation (NMT) performs relatively well with the figurative expressions of high-resource languages, it often faces challenges when dealing with low-resource languages like Sinhala due to limited available data. To address this limitation, we introduce a corpus of 2,344 Sinhala figures of speech with cultural and cross-lingual annotations. We examine this dataset to classify the cultural origins of the figures of speech and to identify their cross-lingual equivalents. Additionally, we have developed a binary classifier to differentiate between two types of FOS in the dataset, achieving an accuracy rate of approximately 92%. We also evaluate the performance of existing LLMs on this dataset. Our findings reveal significant shortcomings in the current capabilities of LLMs, as these models often struggle to accurately convey idiomatic meanings. By making this dataset publicly available, we offer a crucial benchmark for future research in low-resource NLP and culturally aware machine translation.

</details>


### [117] [Steer2Edit: From Activation Steering to Component-Level Editing](https://arxiv.org/abs/2602.09870)
*Chung-En Sun,Ge Yan,Zimo Wang,Tsui-Wei Weng*

Main category: cs.CL

TL;DR: Steer2Edit 提出了一种无需训练的框架，将推理时的定向向量转化为组件级别的权重编辑信号，实现了在保持性能的同时改进安全性、真实性并减少推理长度。


<details>
  <summary>Details</summary>
Motivation: 现有方法通过全局的固定修改影响大语言模型的行为，忽略了重要组件的影响，导致了不理想的属性-效用权衡。

Method: Steer2Edit 通过将推理时的控制信号转化为诊断信号，有针对性地在个体注意力头和MLP神经元之间重新分配行为影响，实现参数的解释性修改。

Result: 在安全性、真实性及推理效率方面，Steer2Edit 较现有方法取得了更好的结果：安全提高了17.2%，真实性增加了9.8%，平均推理长度缩短了12.2%。

Conclusion: Steer2Edit 提供了一种从表示控制信号到解释性权重更新的理论框架，提升了大语言模型行为的方向控制。

Abstract: Steering methods influence Large Language Model behavior by identifying semantic directions in hidden representations, but are typically realized through inference-time activation interventions that apply a fixed, global modification to the model's internal states. While effective, such interventions often induce unfavorable attribute-utility trade-offs under strong control, as they ignore the fact that many behaviors are governed by a small and heterogeneous subset of model components. We propose Steer2Edit, a theoretically grounded, training-free framework that transforms steering vectors from inference-time control signals into diagnostic signals for component-level rank-1 weight editing. Instead of uniformly injecting a steering direction during generation, Steer2Edit selectively redistributes behavioral influence across individual attention heads and MLP neurons, yielding interpretable edits that preserve the standard forward pass and remain compatible with optimized parallel inference. Across safety alignment, hallucination mitigation, and reasoning efficiency, Steer2Edit consistently achieves more favorable attribute-utility trade-offs: at matched downstream performance, it improves safety by up to 17.2%, increases truthfulness by 9.8%, and reduces reasoning length by 12.2% on average. Overall, Steer2Edit provides a principled bridge between representation steering and weight editing by translating steering signals into interpretable, training-free parameter updates.

</details>


### [118] [The Devil Behind Moltbook: Anthropic Safety is Always Vanishing in Self-Evolving AI Societies](https://arxiv.org/abs/2602.09877)
*Chenxu Wang,Chaozhuo Li,Songyang Liu,Zejian Chen,Jinyu Hou,Ji Qi,Rui Li,Litian Zhang,Qiwei Ye,Zheng Liu,Xu Chen,Xi Zhang,Philip S. Yu*

Main category: cs.CL

TL;DR: 研究证明在完全隔离、持续自我进化和安全性不变性之间无法同时满足，提出了信息论框架来定义安全并理论及实证分析了这种不可能性。进一步讨论了解决方案，并强调了外部监督或新型安全机制的重要性。


<details>
  <summary>Details</summary>
Motivation: 探讨多智能体系统中自我进化与安全性之间的冲突，为构建更为安全且可持续发展的系统提供理论基础。

Method: 通过理论分析和实证研究，利用信息论框架来定义和评估系统的安全性。

Result: 理论证明了完全隔离的自我进化系统安全性将不可避免地下降；基于Moltbook和两个封闭自我进化系统的实验证据也支持这一结论。

Conclusion: 本研究建立了自进化AI社会的基本界限，并强调了外部控制或新安全机制的重要性，以实现自我进化系统的持续安全发展。

Abstract: The emergence of multi-agent systems built from large language models (LLMs) offers a promising paradigm for scalable collective intelligence and self-evolution. Ideally, such systems would achieve continuous self-improvement in a fully closed loop while maintaining robust safety alignment--a combination we term the self-evolution trilemma. However, we demonstrate both theoretically and empirically that an agent society satisfying continuous self-evolution, complete isolation, and safety invariance is impossible. Drawing on an information-theoretic framework, we formalize safety as the divergence degree from anthropic value distributions. We theoretically demonstrate that isolated self-evolution induces statistical blind spots, leading to the irreversible degradation of the system's safety alignment. Empirical and qualitative results from an open-ended agent community (Moltbook) and two closed self-evolving systems reveal phenomena that align with our theoretical prediction of inevitable safety erosion. We further propose several solution directions to alleviate the identified safety concern. Our work establishes a fundamental limit on the self-evolving AI societies and shifts the discourse from symptom-driven safety patches to a principled understanding of intrinsic dynamical risks, highlighting the need for external oversight or novel safety-preserving mechanisms.

</details>


### [119] [LLMs Encode Their Failures: Predicting Success from Pre-Generation Activations](https://arxiv.org/abs/2602.09924)
*William Lugoloobi,Thomas Foster,William Bankes,Chris Russell*

Main category: cs.CL

TL;DR: 该研究提出了一种通过预测语言模型在生成前的内部表示来决定是否需要额外计算的方法，从而实现更高效的推理。实验表明，模型能够编码一种区别于人类认知的难度感知，并在减少推理成本的同时提高性能。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLMs）的广泛应用，如何在保证效果的前提下降低计算成本成为了亟待解决的问题。本研究旨在探索模型生成前的内部表示能否作为预测其生成表现的信号，从而实现更高效的查询路由。

Method: 研究者训练线性探针预测模型在数学和编程任务上的特定策略成功情况，使用了如E2H-AMC的数据集来验证模型编码的难度感知与人类认知的不同。

Result: 研究发现，模型在生成前的内部表示能够有效预测其后续生成的表现，优于表面特征如问题长度和TF-IDF。实验结果显示，通过利用这些内部表示，能够显著降低推理成本并提高性能。

Conclusion: 本研究证明了内部表示可以作为指导高效推理的重要信号，即便这些表示与人类对难度的认知有所不同，也能实现显著的效率提升。

Abstract: Running LLMs with extended reasoning on every problem is expensive, but determining which inputs actually require additional compute remains challenging. We investigate whether their own likelihood of success is recoverable from their internal representations before generation, and if this signal can guide more efficient inference. We train linear probes on pre-generation activations to predict policy-specific success on math and coding tasks, substantially outperforming surface features such as question length and TF-IDF. Using E2H-AMC, which provides both human and model performance on identical problems, we show that models encode a model-specific notion of difficulty that is distinct from human difficulty, and that this distinction increases with extended reasoning. Leveraging these probes, we demonstrate that routing queries across a pool of models can exceed the best-performing model whilst reducing inference cost by up to 70\% on MATH, showing that internal representations enable practical efficiency gains even when they diverge from human intuitions about difficulty. Our code is available at: https://github.com/KabakaWilliam/llms_know_difficulty

</details>


### [120] [ATTNPO: Attention-Guided Process Supervision for Efficient Reasoning](https://arxiv.org/abs/2602.09953)
*Shuaiyi Nie,Siyu Ding,Wenyuan Zhang,Linhao Yu,Tianmeng Yang,Yao Chen,Tingwen Liu,Weichong Yin,Yu Sun,Hua Wu*

Main category: cs.CL

TL;DR: ATTNPO 是一种低开销的过程监督 RL 框架，利用模型内部的注意力信号进行步级的信用分配，通过鼓励关键步骤并抑制冗余步骤来减少推理长度，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 现有的方法往往在减少推理步骤过多的问题上表现不佳，要么增加处理负担，要么造成奖励分配不准确。本文提出 ATTNPO 试图解决这些问题。

Method: ATTNPO 使用模型内部的注意力信号来区分必要和冗余的推理步骤。通过两个子策略来减少冗余情况，同时保护关键步骤，减少惩罚。

Result: ATTNPO 在9个基准测试中显著减少推理长度并提高性能。

Conclusion: ATTNPO 是一个有效的过程监督的 RL 框架，能够在保持性能的同时有效地减少复杂推理任务中的冗余推理。

Abstract: Large reasoning models trained with reinforcement learning and verifiable rewards (RLVR) achieve strong performance on complex reasoning tasks, yet often overthink, generating redundant reasoning without performance gains. Existing trajectory-level length penalties often fail to effectively shorten reasoning length and degrade accuracy, as they uniformly treat all reasoning steps and lack fine-grained signals to distinguish redundancy from necessity. Meanwhile, process-supervised methods are typically resource-intensive and suffer from inaccurate credit assignment. To address these issues, we propose ATTNPO, a low-overhead process-supervised RL framework that leverages the model's intrinsic attention signals for step-level credit assignment. We first identify a set of special attention heads that naturally focus on essential steps while suppressing redundant ones. By leveraging the attention scores of these heads, We then employ two sub-strategies to mitigate overthinking by discouraging redundant steps while preserving accuracy by reducing penalties on essential steps. Experimental results show that ATTNPO substantially reduces reasoning length while significantly improving performance across 9 benchmarks.

</details>


### [121] [ViMultiChoice: Toward a Method That Gives Explanation for Multiple-Choice Reading Comprehension in Vietnamese](https://arxiv.org/abs/2602.09961)
*Trung Tien Cao,Lam Minh Thai,Nghia Hieu Nguyen,Duc-Vu Nguyen,Ngan Luu-Thuy Nguyen*

Main category: cs.CL

TL;DR: 该论文提出了一种新的越南语MCRC数据集和ViMultiChoice方法，能够在预测正确选项的同时生成解释。ViMultiChoice在现有基线和新数据集上均表现出优于SotA的性能。


<details>
  <summary>Details</summary>
Motivation: 当前MCRC模型缺少解释预测能力，因此作者致力于创建一个专门的越南语数据集，并提出了一个新的方法ViMultiChoice，以提升模型的准确性和解释性。

Method: ViMultiChoice方法通过联合预测选择选项和生成解释来工作。它被设计成专门用于处理越南语阅读理解问题。

Result: 实验结果显示，ViMultiChoice在ViMMRC 2.0基准和新数据集上均优于现有基线，显著提高了选择准确性。

Conclusion: 通过联合训练选项决策和解释生成，ViMultiChoice方法在评估中表现出了显著的性能提升。

Abstract: Multiple-choice Reading Comprehension (MCRC) models aim to select the correct answer from a set of candidate options for a given question. However, they typically lack the ability to explain the reasoning behind their choices. In this paper, we introduce a novel Vietnamese dataset designed to train and evaluate MCRC models with explanation generation capabilities. Furthermore, we propose ViMultiChoice, a new method specifically designed for modeling Vietnamese reading comprehension that jointly predicts the correct answer and generates a corresponding explanation. Experimental results demonstrate that ViMultiChoice outperforms existing MCRC baselines, achieving state-of-the-art (SotA) performance on both the ViMMRC 2.0 benchmark and the newly introduced dataset. Additionally, we show that jointly training option decision and explanation generation leads to significant improvements in multiple-choice accuracy.

</details>


### [122] [A Unified Assessment of the Poverty of the Stimulus Argument for Neural Language Models](https://arxiv.org/abs/2602.09992)
*Xiulin Yang,Arianna Bisazza,Nathan Schneider,Ethan Gotlieb Wilcox*

Main category: cs.CL

TL;DR: 本研究通过引入	extbackslash poshbench工具套件，训练并评估了Transformer模型在句法问题形成和岛移动等英语文本现象上的表现，即使在缺乏直接证据的情况下，也显示出某种程度的泛化能力。然而，神经模型的数据效率较低，泛化效果也不如儿童。进一步增强模型的三维认知动机归纳偏见，并未显著提升	extbackslash poshbench的性能。研究结果质疑了先天句法则是一般化唯一途径的观点，同时也表明人类级别的数据效率需要超出当前测试的归纳偏见。


<details>
  <summary>Details</summary>
Motivation: 探讨先天句法对语言学习的重要性，以及神经语言模型是否能作为替代解释来挑战PoSH这一长期且有争议的观点。

Method: 使用	extbackslash poshbench训练集和评估集，对Transformer模型进行训练与评估，并引入认知导向的归纳偏见以改进模型能力。

Result: 即便在缺乏直接证据的情况下，神经模型仍显示出一定程度的泛化能力；然而，其数据效率较低，泛化效果不敌儿童水平，添加认知偏见未能显著提升	extbackslash poshbench表现。

Conclusion: 研究结果对先天句法作为反映语言学习唯一途径产生了质疑，同时指出实现类似人类的数据效率可能还需要额外的认知导向偏见。

Abstract: How can children acquire native-level syntax from limited input? According to the Poverty of the Stimulus Hypothesis (PoSH), the linguistic input children receive is insufficient to explain certain generalizations that are robustly learned; innate linguistic constraints, many have argued, are thus necessary to explain language learning. Neural language models, which lack such language-specific constraints in their design, offer a computational test of this longstanding (but controversial) claim. We introduce \poshbench, a training-and-evaluation suite targeting question formation, islands to movement, and other English phenomena at the center of the PoSH arguments. Training Transformer models on 10--50M words of developmentally plausible text, we find indications of generalization on all phenomena even without direct positive evidence -- yet neural models remain less data-efficient and their generalizations are weaker than those of children. We further enhance our models with three recently proposed cognitively motivated inductive biases. We find these biases improve general syntactic competence but not \poshbench performance. Our findings challenge the claim that innate syntax is the only possible route to generalization, while suggesting that human-like data efficiency requires inductive biases beyond those tested here.

</details>


### [123] [ViSpeechFormer: A Phonemic Approach for Vietnamese Automatic Speech Recognition](https://arxiv.org/abs/2602.10003)
*Khoa Anh Nguyen,Long Minh Hoang,Nghia Hieu Nguyen,Luan Thanh Nguyen,Ngan Luu-Thuy Nguyen*

Main category: cs.CL

TL;DR: ViSpeechFormer 是一种基于音素的自动语音识别框架，专为越南语设计，利用音素-语音高度透明的特点，显著提升了识别性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 越南语具有音素-拼写高度透明的特性，即每个字母表示一个音素且反之亦然。这一特性启发了我们开发基于音素的自动语音识别方法，以提升越南语自动语音识别系统的性能。

Method: ViSpeechFormer 采用基于音素的方法来处理越南语的自动语音识别任务。这种方法在语音识别过程中考虑到了音素的表示，可以更好地应对不同音素之间的细微差异，同时也能更好地处理未见词汇和训练偏差问题。

Result: 实验表明，ViSpeechFormer 在两个公开的越南语自动语音识别数据集上表现优异，具备更好的泛化能力和对训练偏差的抵抗能力。

Conclusion: 这种基于音素的形式对于其他具有类似拼写-音素透明性的语言也可能是一个有力的方法。

Abstract: Vietnamese has a phonetic orthography, where each grapheme corresponds to at most one phoneme and vice versa. Exploiting this high grapheme-phoneme transparency, we propose ViSpeechFormer (\textbf{Vi}etnamese \textbf{Speech} Trans\textbf{Former}), a phoneme-based approach for Vietnamese Automatic Speech Recognition (ASR). To the best of our knowledge, this is the first Vietnamese ASR framework that explicitly models phonemic representations. Experiments on two publicly available Vietnamese ASR datasets show that ViSpeechFormer achieves strong performance, generalizes better to out-of-vocabulary words, and is less affected by training bias. This phoneme-based paradigm is also promising for other languages with phonetic orthographies. The code will be released upon acceptance of this paper.

</details>


### [124] [SCORE: Specificity, Context Utilization, Robustness, and Relevance for Reference-Free LLM Evaluation](https://arxiv.org/abs/2602.10017)
*Homaira Huda Shomee,Rochana Chaturvedi,Yangxinyu Xie,Tanwi Mallick*

Main category: cs.CL

TL;DR: 本文提出了一种多维度、无需参考的评估框架，用于评价大型语言模型在高风险、专业领域的表现，特别是在应对自然灾害响应和基础设施规划时，强调评估模型输出的精准性、鲁棒性、答案相关性和上下文利用。


<details>
  <summary>Details</summary>
Motivation: 现有的评估框架无法充分评估大型语言模型在特定领域中提供的专业信息质量，因此本文提出新的多维度评估框架以填补这一空白。

Method: 此方法包括开发一个包含1412个专业领域问题-答案对的定制数据集，这些问题覆盖40种专业角色和7种自然灾害类型，以及进行人工评估以确保评价的一致性和准确性。

Result: 实验结果显示，单一指标不能独立评价答案质量，强调了在高风险应用场景中部署大型语言模型时需要结构化、多指标评价框架。

Conclusion: 本文提出的评估框架为未来研究提供了参考，并指出多指标评价体系在促进大型语言模型高质量应用方面的重要性。

Abstract: Large language models (LLMs) are increasingly used to support question answering and decision-making in high-stakes, domain-specific settings such as natural hazard response and infrastructure planning, where effective answers must convey fine-grained, decision-critical details. However, existing evaluation frameworks for retrieval-augmented generation (RAG) and open-ended question answering primarily rely on surface-level similarity, factual consistency, or semantic relevance, and often fail to assess whether responses provide the specific information required for domain-sensitive decisions. To address this gap, we propose a multi-dimensional, reference-free evaluation framework that assesses LLM outputs along four complementary dimensions: specificity, robustness to paraphrasing and semantic perturbations, answer relevance, and context utilization. We introduce a curated dataset of 1,412 domain-specific question-answer pairs spanning 40 professional roles and seven natural hazard types to support systematic evaluation. We further conduct human evaluation to assess inter-annotator agreement and alignment between model outputs and human judgments, which highlights the inherent subjectivity of open-ended, domain-specific evaluation. Our results show that no single metric sufficiently captures answer quality in isolation and demonstrate the need for structured, multi-metric evaluation frameworks when deploying LLMs in high-stakes applications.

</details>


### [125] [Decoupled Reasoning with Implicit Fact Tokens (DRIFT): A Dual-Model Framework for Efficient Long-Context Inference](https://arxiv.org/abs/2602.10021)
*Wenxuan Xie,Yujia Wang,Xin Tan,Chaochao Lu,Xia Hu,Xuhong Wang*

Main category: cs.CL

TL;DR: 论文提出了一种名为DRIFT的双模型架构，它通过动态压缩文档片段并将其投影到推理模型的嵌入空间中，来显式地解耦知识提取与推理过程，从而改善了语言模型在长上下文任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的知识增强技术受限于有限的上下文窗口、检索器噪声或灾难性遗忘等问题，本文旨在提出一种新的方法来解决这些挑战。

Method: DRIFT模型采用轻量级的知识模型动态压缩文档片段，生成条件于查询的隐式事实令牌，并将其投影到推理模型的嵌入空间中，以替代原始冗余文本，同时保持推理准确性。

Result: 实验表明，DRIFT在长上下文任务上的性能显著优于其他同规模的基线模型。

Conclusion: DRIFT提供了一种可扩展且高效的框架，用于扩展大型语言模型的有效上下文窗口和推理能力。

Abstract: The integration of extensive, dynamic knowledge into Large Language Models (LLMs) remains a significant challenge due to the inherent entanglement of factual data and reasoning patterns. Existing solutions, ranging from non-parametric Retrieval-Augmented Generation (RAG) to parametric knowledge editing, are often constrained in practice by finite context windows, retriever noise, or the risk of catastrophic forgetting. In this paper, we propose DRIFT, a novel dual-model architecture designed to explicitly decouple knowledge extraction from the reasoning process. Unlike static prompt compression, DRIFT employs a lightweight knowledge model to dynamically compress document chunks into implicit fact tokens conditioned on the query. These dense representations are projected into the reasoning model's embedding space, replacing raw, redundant text while maintaining inference accuracy. Extensive experiments show that DRIFT significantly improves performance on long-context tasks, outperforming strong baselines among comparably sized models. Our approach provides a scalable and efficient paradigm for extending the effective context window and reasoning capabilities of LLMs. Our code is available at https://github.com/Lancelot-Xie/DRIFT.

</details>


### [126] [MEVER: Multi-Modal and Explainable Claim Verification with Graph-based Evidence Retrieval](https://arxiv.org/abs/2602.10023)
*Delvin Ce Zhang,Suhan Cui,Zhelin Chu,Xianren Zhang,Dongwon Lee*

Main category: cs.CL

TL;DR: 该研究提出了一种能联合实现证据检索、多模态声明验证和解释生成的新颖模型。


<details>
  <summary>Details</summary>
Motivation: 现有声明验证工作主要关注文本证据推理或忽视可解释性，导致验证结果不准确且不令人信服。

Method: 该模型包含两层多模态图进行声明和证据的证据检索，结合文本与图像推理。它通过令牌和证据层面的融合将声明和证据嵌入进行多模态验证，在解释生成中引入了多模态解码器中的融合。

Result: 实验表明，该模型在声明验证方面具有很强的优势。

Conclusion: 为此，研究团队还创建了一个实证数据集AIChartClaim，以补充声明验证社区的数据集。

Abstract: Verifying the truthfulness of claims usually requires joint multi-modal reasoning over both textual and visual evidence, such as analyzing both textual caption and chart image for claim verification. In addition, to make the reasoning process transparent, a textual explanation is necessary to justify the verification result. However, most claim verification works mainly focus on the reasoning over textual evidence only or ignore the explainability, resulting in inaccurate and unconvincing verification. To address this problem, we propose a novel model that jointly achieves evidence retrieval, multi-modal claim verification, and explanation generation. For evidence retrieval, we construct a two-layer multi-modal graph for claims and evidence, where we design image-to-text and text-to-image reasoning for multi-modal retrieval. For claim verification, we propose token- and evidence-level fusion to integrate claim and evidence embeddings for multi-modal verification. For explanation generation, we introduce multi-modal Fusion-in-Decoder for explainability. Finally, since almost all the datasets are in general domain, we create a scientific dataset, AIChartClaim, in AI domain to complement claim verification community. Experiments show the strength of our model.

</details>


### [127] [Quantum-Audit: Evaluating the Reasoning Limits of LLMs on Quantum Computing](https://arxiv.org/abs/2602.10092)
*Mohamed Afane,Kayla Laufer,Wenqi Wei,Ying Mao,Junaid Farooq,Ying Wang,Juntao Chen*

Main category: cs.CL

TL;DR: 该研究提出了一种名为Quantum-Audit的新基准，旨在评估语言模型在量子计算概念理解方面的表现。基准包括专家编写的问题、从论文中提取的问题以及包含错误前提的问题。研究发现，尽管部分模型在某些方面表现优秀，但整体上，模型在处理高级主题和检测错误前提时表现出不足。


<details>
  <summary>Details</summary>
Motivation: 目前对量子计算概念的理解评估尚未系统化，而语言模型在该领域的应用日益增加，因此开发一个专门的基准来衡量模型在这方面的表现是有必要的。

Method: 研究构建了一个包含多种类型问题的基准，涵盖了专家编写的问题、从论文中提取并由专家验证的问题，以及误导性问题。还对比了模型在与专家平均成绩的性能。

Result: 模型的整体表现不尽人意，尤其是当处理高级主题和逻辑推理问题时。最好的模型Claude Opus 4.5在专家编写的问题上的准确性下降了12%，并且在识别错误前提方面表现较差。

Conclusion: 尽管一些模型在某些方面表现优秀，但这些发现强调了进一步改进语言模型掌握和理解复杂量子概念的必要性。

Abstract: Language models have become practical tools for quantum computing education and research, from summarizing technical papers to explaining theoretical concepts and answering questions about recent developments in the field. While existing benchmarks evaluate quantum code generation and circuit design, their understanding of quantum computing concepts has not been systematically measured. Quantum-Audit addresses this gap with 2,700 questions covering core quantum computing topics. We evaluate 26 models from leading organizations. Our benchmark comprises 1,000 expert-written questions, 1,000 questions extracted from research papers using LLMs and validated by experts, plus an additional 700 questions including 350 open-ended questions and 350 questions with false premises to test whether models can correct erroneous assumptions. Human participants scored between 23% and 86%, with experts averaging 74%. Top-performing models exceeded the expert average, with Claude Opus 4.5 reaching 84% accuracy, though top models showed an average 12-point accuracy drop on expert-written questions compared to LLM-generated ones. Performance declined further on advanced topics, dropping to 73% on security questions. Additionally, models frequently accepted and reinforced false premises embedded in questions instead of identifying them, with accuracy below 66% on these critical reasoning tasks.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [128] [A Small-Scale System for Autoregressive Program Synthesis Enabling Controlled Experimentation](https://arxiv.org/abs/2602.09112)
*Russ Webb,Jason Ramapuram*

Main category: cs.AI

TL;DR: 论文介绍了Cadmus系统，该系统使用低成本训练的小规模模型进行程序合成和推理研究，相比大型语言模型具有更高的可解释性和灵活性。Cadmus模型在特定任务上展现出更强的能力，并揭示了大规模语言模型的潜在问题。


<details>
  <summary>Details</summary>
Motivation: 针对大型语言模型在训练和实验中的高成本及分布外表示等问题，为研究程序合成、推理和指令遵循，提出了Cadmus系统。

Method: Cadmus系统包含了整数虚拟机、任务多样的真实程序数据集以及低成本训练的自回归Transformer模型。

Result: Cadmus模型在特定任务上表现优于GPT-5，特别是在完成正确的整数算术程序方面。同时还展示了大型语言模型在推理过程中的未知先验问题。

Conclusion: Cadmus系统为研究小规模模型进行复杂程序推理提供了新的思路，提高了实验的可解释性与灵活性。

Abstract: What research can be pursued with small models trained to complete true programs? Typically, researchers study program synthesis via large language models (LLMs) which introduce issues such as knowing what is in or out of distribution, understanding fine-tuning effects, understanding the effects of tokenization, and higher demand on compute and storage to carry out experiments. We present a system called Cadmus which includes an integer virtual machine (VM), a dataset composed of true programs of diverse tasks, and an autoregressive transformer model that is trained for under \$200 of compute cost. The system can be used to study program completion, out-of-distribution representations, inductive reasoning, and instruction following in a setting where researchers have effective and affordable fine-grained control of the training distribution and the ability to inspect and instrument models. Smaller models working on complex reasoning tasks enable instrumentation and investigations that may be prohibitively expensive on larger models. To demonstrate that these tasks are complex enough to be of interest, we show that these Cadmus models outperform GPT-5 (by achieving 100\% accuracy while GPT-5 has 95\% accuracy) even on a simple task of completing correct, integer arithmetic programs in our domain-specific language (DSL) while providing transparency into the dataset's relationship to the problem. We also show that GPT-5 brings unknown priors into its reasoning process when solving the same tasks, demonstrating a confounding factor that prevents the use of large-scale LLMs for some investigations where the training set relationship to the task needs to be fully understood.

</details>


### [129] [Uncertainty-Aware Multimodal Emotion Recognition through Dirichlet Parameterization](https://arxiv.org/abs/2602.09121)
*Rémi Grzeczkowicz,Eric Soriano,Ali Janati,Miyu Zhang,Gerard Comas-Quiles,Victor Carballo Araruna,Aneesh Jonelagadda*

Main category: cs.AI

TL;DR: 本文提出了一种轻量级且保护隐私的多模态情绪识别框架，能够在边缘设备上部署。该框架利用了言语、文本和面部图像三种模态，通过模块化设计支持扩展，并引入了一种基于Dempster-Shafer理论和Dirichlet证据的治疗方法和任务无关的融合机制，提高系统的鲁棒性和适应性。


<details>
  <summary>Details</summary>
Motivation: 随着移动健康和人机交互的快速发展，多模态情绪识别变得愈发重要。现有解决方案往往缺乏灵活性和适应性，本文提出的框架旨在解决这一问题。

Method: 该框架通过为每种模态选择优化过的骨干模型（Emotion2Vec、ResNet和DistilRoBERTa）进行处理，并提出了一种基于Dempster-Shafer理论和Dirichlet证据的融合机制，能够直接处理模型的logits，而不需额外训练。

Result: 该方法在五个基准数据集（eNTERFACE05、MEAD、MELD、RAVDESS和CREMA-D）上的验证表明，能够在保持高效性的同时，实现与现有解决方案相当的准确性，并能应对模态间的不确定性。

Conclusion: 本文提出的一种多模态情绪识别框架，不仅实现了情绪识别任务的高性能与高效性，还具有模块化和可扩展性，为未来的情绪感知系统铺平了道路。

Abstract: In this work, we present a lightweight and privacy-preserving Multimodal Emotion Recognition (MER) framework designed for deployment on edge devices. To demonstrate framework's versatility, our implementation uses three modalities - speech, text and facial imagery. However, the system is fully modular, and can be extended to support other modalities or tasks. Each modality is processed through a dedicated backbone optimized for inference efficiency: Emotion2Vec for speech, a ResNet-based model for facial expressions, and DistilRoBERTa for text. To reconcile uncertainty across modalities, we introduce a model- and task-agnostic fusion mechanism grounded in Dempster-Shafer theory and Dirichlet evidence. Operating directly on model logits, this approach captures predictive uncertainty without requiring additional training or joint distribution estimation, making it broadly applicable beyond emotion recognition. Validation on five benchmark datasets (eNTERFACE05, MEAD, MELD, RAVDESS and CREMA-D) show that our method achieves competitive accuracy while remaining computationally efficient and robust to ambiguous or missing inputs. Overall, the proposed framework emphasizes modularity, scalability, and real-world feasibility, paving the way toward uncertainty-aware multimodal systems for healthcare, human-computer interaction, and other emotion-informed applications.

</details>


### [130] [PABU: Progress-Aware Belief Update for Efficient LLM Agents](https://arxiv.org/abs/2602.09138)
*Haitao Jiang,Lin Ge,Hengrui Cai,Rui Song*

Main category: cs.AI

TL;DR: PABU 提出了一种新的信念状态框架，能够更有效地保留相关信息，减少冗余动作，从而提高任务完成率并降低交互步骤。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型（LLM）在处理任务时，由于考虑了全部的历史动作观察，引入了大量无关信息，导致动作冗余和高昂的推理成本。

Method: PABU 通过引入任务进度预测和选择性记忆机制，优化了行动决策流程。在每个步骤中，模型预测自上次决策以来的进度，并决定是否保存新遇到的交互。这种机制仅基于保留的数据集进行未来的决策。

Result: 在 AgentGym 指标体系中的八个环境中，PABU 达到了 81.0% 的任务完成率，超过之前的最优模型（SoTA）23.9%。此外，PABU 的基于进度的选择性行动策略提高了效率，减少了平均交互步骤至 9.5 步，减少了 26.9%。

Conclusion: PABU 不仅能有效提高模型性能，还能降低系统复杂性和成本，具备良好的应用前景。

Abstract: Large Language Model (LLM) agents commonly condition actions on full action-observation histories, which introduce task-irrelevant information that easily leads to redundant actions and higher inference cost. We propose Progress-Aware Belief Update (PABU), a belief-state framework that compactly represents an agent's state by explicitly modeling task progress and selectively retaining past actions and observations. At each step, the agent predicts its relative progress since the previous round and decides whether the newly encountered interaction should be stored, conditioning future decisions only on the retained subset. Across eight environments in the AgentGym benchmark, and using identical training trajectories, PABU achieves an 81.0% task completion rate, outperforming previous State of the art (SoTA) models with full-history belief by 23.9%. Additionally, PABU's progress-oriented action selection improves efficiency, reducing the average number of interaction steps to 9.5, corresponding to a 26.9% reduction. Ablation studies show that both explicit progress prediction and selective retention are necessary for robust belief learning and performance gains.

</details>


### [131] [CoMMa: Contribution-Aware Medical Multi-Agents From A Game-Theoretic Perspective](https://arxiv.org/abs/2602.09159)
*Yichen Wu,Yujin Oh,Sangjoon Park,Kailong Fan,Dania Daye,Hana Farzaneh,Xiang Li,Raul Uppot,Quanzheng Li*

Main category: cs.AI

TL;DR: CoMMa 是一个去中心化的基于大模型的多智能体架构，通过游戏理论目标进行去中心化决策支持，相比传统的随机叙述推理，CoMMa 可以通过确定性的嵌入投影进行贡献感知的贡献归因。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体框架在处理需要基于动态复杂患者数据进行推理的肿瘤学决策支持任务时应用广泛，但大多数架构依赖于叙事形式的随机推理，存在缺少解释性等不足，CoMMa 旨在解决这些问题。

Method: CoMMa 在每个领域专家使用局部证据进行操作，利用游戏理论目标进行协调，通过确定性嵌入投影来估计每个智能体的边际效用，实现贡献感知的贡献归因。

Result: CoMMa 在多种肿瘤学基准测试中，特别是在实际多学科肿瘤董事会数据集上，表现出更高的准确性和更稳定的性能，优于数据集中化和角色基线。

Conclusion: CoMMa 提供了一种有效的方法，能够更准确和稳健地进行基于决策支持的多智能体系统的系统构建和应用，适用于如肿瘤学决策等复杂领域。

Abstract: Recent multi-agent frameworks have broadened the ability to tackle oncology decision support tasks that require reasoning over dynamic, heterogeneous patient data. We propose Contribution-Aware Medical Multi-Agents (CoMMa), a decentralized LLM-agent framework in which specialists operate on partitioned evidence and coordinate through a game-theoretic objective for robust decision-making. In contrast to most agent architectures relying on stochastic narrative-based reasoning, CoMMa utilizes deterministic embedding projections to approximate contribution-aware credit assignment. This yields explicit evidence attribution by estimating each agent's marginal utility, producing interpretable and mathematically grounded decision pathways with improved stability. Evaluated on diverse oncology benchmarks, including a real-world multidisciplinary tumor board dataset, CoMMa achieves higher accuracy and more stable performance than data-centralized and role-based multi-agents baselines.

</details>


### [132] [Measuring Dataset Diversity from a Geometric Perspective](https://arxiv.org/abs/2602.09340)
*Yang Ba,Mohammad Sadeq Abolhasani,Michelle V Mancenido,Rong Pan*

Main category: cs.AI

TL;DR: 本文提出了一种基于拓扑数据分析（TDA）和持久景型（PLs）的数据集多样性的度量方法PLDiv，该方法能够捕捉数据的丰富几何和结构特征，而不仅仅是分布变异或熵。


<details>
  <summary>Details</summary>
Motivation: 现有的多样性度量方法主要关注数据分布的变异或熵，忽略了数据的几何结构。本文旨在填补这一研究空白，通过结合TDA和PLs来量化数据的几何特征。

Method: 本文提出了一种新的多样性度量方法PLDiv，通过数据持久景型来分析数据的几何结构，并提供了衡量方法的理论依据。

Result: 通过在多种数据模态下进行广泛实验，本文证明了PLDiv具有强大、可靠、可解释的特点，直接关联了数据多样性与其内在几何结构。

Conclusion: PLDiv为数据集的构造、增强和评估提供了一个基础工具，有助于理解数据的多样性及其几何特性。

Abstract: Diversity can be broadly defined as the presence of meaningful variation across elements, which can be viewed from multiple perspectives, including statistical variation and geometric structural richness in the dataset. Existing diversity metrics, such as feature-space dispersion and metric-space magnitude, primarily capture distributional variation or entropy, while largely neglecting the geometric structure of datasets. To address this gap, we introduce a framework based on topological data analysis (TDA) and persistence landscapes (PLs) to extract and quantify geometric features from data. This approach provides a theoretically grounded means of measuring diversity beyond entropy, capturing the rich geometric and structural properties of datasets. Through extensive experiments across diverse modalities, we demonstrate that our proposed PLs-based diversity metric (PLDiv) is powerful, reliable, and interpretable, directly linking data diversity to its underlying geometry and offering a foundational tool for dataset construction, augmentation, and evaluation.

</details>


### [133] [Auditing Multi-Agent LLM Reasoning Trees Outperforms Majority Vote and LLM-as-Judge](https://arxiv.org/abs/2602.09341)
*Wei Yang,Shixuan Li,Heng Ping,Peiyu Zhang,Paul Bogdan,Jesse Thomason*

Main category: cs.AI

TL;DR: 该研究引入了AgentAuditor方法，通过在推理树中搜索路径来解决多智能体系统中的共识问题，相比多数投票提高了5%的绝对准确性。


<details>
  <summary>Details</summary>
Motivation: 现有 MAS 框架大多采用多数投票来聚合智能体输出，这种方法在智能体存在共有的偏见时容易导致共识谬误，忽略推理痕迹的证据结构，这促使作者提出AgentAuditor方法。

Method: AgentAuditor 基于推理树进行路径搜索，针对分歧点进行冲突解决，将全局裁决转化为局部验证，通过反共识偏好优化（ACPO）算法，提高少数正确决策的识别。

Result: 实验结果显示，AgentAuditor在5种流行的MAS设置中，相比多数投票提高了5%的绝对准确性，相较于使用大型语言模型（LLM）作为裁决者提高了3%。

Conclusion: 该研究为多智能体系统的决策机制提供了创新解决方案，提高了多智能体系统输出的准确性和可靠性。

Abstract: Multi-agent systems (MAS) can substantially extend the reasoning capacity of large language models (LLMs), yet most frameworks still aggregate agent outputs with majority voting. This heuristic discards the evidential structure of reasoning traces and is brittle under the confabulation consensus, where agents share correlated biases and converge on the same incorrect rationale. We introduce AgentAuditor, which replaces voting with a path search over a Reasoning Tree that explicitly represents agreements and divergences among agent traces. AgentAuditor resolves conflicts by comparing reasoning branches at critical divergence points, turning global adjudication into efficient, localized verification. We further propose Anti-Consensus Preference Optimization (ACPO), which trains the adjudicator on majority-failure cases and rewards evidence-based minority selections over popular errors. AgentAuditor is agnostic to MAS setting, and we find across 5 popular settings that it yields up to 5% absolute accuracy improvement over a majority vote, and up to 3% over using LLM-as-Judge.

</details>


### [134] [Image Quality in the Era of Artificial Intelligence](https://arxiv.org/abs/2602.09347)
*Jana G. Delfino,Jason L. Granstedt,Frank W. Samuelson,Robert Ochs,Krishna Juluru*

Main category: cs.AI

TL;DR: 该研究突出了使用AI重建或增强放射影像时的局限性，旨在帮助用户在安全和有效利用技术的同时，最大限度地降低风险。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能（AI）在放射学中的快速部署，需要理解和关注AI在重建和增强影像过程中的局限性，以提高医疗服务的安全性和有效性。

Method: 本文总结了AI在放射影像重建和增强中的优势，并指出了潜在的风险和挑战，从而呼吁提高相关人员的认知。

Result: 本文揭示了AI在放射学领域的应用过程中可能存在的问题，并提出了一些建议。

Conclusion: 作者希望通过这篇交流，提高公众对AI在放射影像处理中局限性的认识，而不仅仅是关注高质量的图像展示。

Abstract: Artificial intelligence (AI) is being deployed within radiology at a rapid pace. AI has proven an excellent tool for reconstructing and enhancing images that appear sharper, smoother, and more detailed, can be acquired more quickly, and allowing clinicians to review them more rapidly. However, incorporation of AI also introduces new failure modes and can exacerbate the disconnect between perceived quality of an image and information content of that image. Understanding the limitations of AI-enabled image reconstruction and enhancement is critical for safe and effective use of the technology. Hence, the purpose of this communication is to bring awareness to limitations when AI is used to reconstruct or enhance a radiological image, with the goal of enabling users to reap benefits of the technology while minimizing risks.

</details>


### [135] [P1-VL: Bridging Visual Perception and Scientific Reasoning in Physics Olympiads](https://arxiv.org/abs/2602.09443)
*Yun Luo,Futing Wang,Qianjia Cheng,Fangchen Yu,Haodi Lei,Jianhao Yan,Chenxi Li,Jiacheng Chen,Yufeng Zhao,Haiyuan Wan,Yuchen Zhang,Shenghe Zheng,Junchi Yao,Qingyang Zhang,Haonan He,Wenxuan Zeng,Li Sheng,Chengxing Xie,Yuxin Zuo,Yizhuo Li,Yulun Wu,Rui Huang,Dongzhan Zhou,Kai Chen,Yu Qiao,Lei Bai,Yu Cheng,Ning Ding,Bowen Zhou,Peng Ye,Ganqu Cui*

Main category: cs.AI

TL;DR: P1-VL 是一个开源的视图-语言模型家族，它结合了 Curriculum Reinforcement Learning 和 Agentic Augmentation，专注于高级科学推理。在 HiPhO 基准测试中，P1-VL-235B-A22B 一举拿下了 12 金牌，成为开源模型中的新标杆。


<details>
  <summary>Details</summary>
Motivation: 当前，大型语言模型（LLMs）需要过渡到能够处理科学级推理的能力，特别是验证物理一致性。为此，研究引入了 P1-VL，一种专为高级科学推理设计的开源视图-语言模型。

Method: P1-VL 使用 Curriculum Reinforcement Learning 和 Agentic Augmentation 方法。Curriculum Reinforcement Learning 采用逐步难度扩展以稳定后训练，而 Agentic Augmentation 则允许迭代自验证。

Result: P1-VL 在 HiPhO 基准测试中取得了显著成就。P1-VL-235B-A22B 是首次开源的视图-语言模型，实现了 12 金牌的成绩，达到开源模型的最新水平。

Conclusion: P1-VL 为视觉感知与抽象物理法则之间的联系提供了重要的步骤，展示了在 STEM 领域的卓越科学推理能力和广泛的适应性，它在全球排名中获得了第二名的成绩，仅次于 Gemini-3-Pro。通过开源 P1-VL，研究为实现通用物理智能奠定了基础。

Abstract: The transition from symbolic manipulation to science-grade reasoning represents a pivotal frontier for Large Language Models (LLMs), with physics serving as the critical test anchor for binding abstract logic to physical reality. Physics demands that a model maintain physical consistency with the laws governing the universe, a task that fundamentally requires multimodal perception to ground abstract logic in reality. At the Olympiad level, diagrams are often constitutive rather than illustrative, containing essential constraints, such as boundary conditions and spatial symmetries, that are absent from the text. To bridge this visual-logical gap, we introduce P1-VL, a family of open-source vision-language models engineered for advanced scientific reasoning. Our method harmonizes Curriculum Reinforcement Learning, which employs progressive difficulty expansion to stabilize post-training, with Agentic Augmentation, enabling iterative self-verification at inference. Evaluated on HiPhO, a rigorous benchmark of 13 exams from 2024-2025, our flagship P1-VL-235B-A22B becomes the first open-source Vision-Language Model (VLM) to secure 12 gold medals and achieves the state-of-the-art performance in the open-source models. Our agent-augmented system achieves the No.2 overall rank globally, trailing only Gemini-3-Pro. Beyond physics, P1-VL demonstrates remarkable scientific reasoning capacity and generalizability, establishing significant leads over base models in STEM benchmarks. By open-sourcing P1-VL, we provide a foundational step toward general-purpose physical intelligence to better align visual perceptions with abstract physical laws for machine scientific discovery.

</details>


### [136] [Bridging Efficiency and Transparency: Explainable CoT Compression in Multimodal Large Reasoning Models](https://arxiv.org/abs/2602.09485)
*Yizhi Wang,Linan Yue,Min-Ling Zhang*

Main category: cs.AI

TL;DR: 本文提出了一种名为XMCC的可解释多模态CoT压缩方法，通过强化学习优化压缩过程，既可以有效地缩短推理路径，又能保留关键推理步骤和答案的正确性，同时还能生成关于压缩决策的自然语言解释。


<details>
  <summary>Details</summary>
Motivation: 现有方法对长CoT进行压缩可能会破坏视觉和文本推理的一致性，且压缩过程缺乏透明度，难以区分哪些信息至关重要。为了解决这些问题，提出了XMCC方法。

Method: XMCC将压缩过程视为通过强化学习优化的顺序决策过程。该方法旨在同步减少推理长度和保持关键推理步骤，同时还能自动生成解释其决策的自然语言说明。

Result: 在多个代表性多模态推理基准测试上的实验表明，XMCC不仅减少了推理长度，还提供了可解释的解释，这证明了其有效性。

Conclusion: 总的来说，XMCC方法能够有效地压缩长CoT并保持推理的正确性，同时提供易于理解的解释，展示了其在提高多模态推理效率方面的潜力。

Abstract: Long chains of thought (Long CoTs) are widely employed in multimodal reasoning models to tackle complex tasks by capturing detailed visual information. However, these Long CoTs are often excessively lengthy and contain redundant reasoning steps, which can hinder inference efficiency. Compressing these long CoTs is a natural solution, yet existing approaches face two major challenges: (1) they may compromise the integrity of visual-textual reasoning by removing essential alignment cues, and (2) the compression process lacks explainability, making it difficult to discern which information is critical. To address these problems, we propose XMCC, an eXplainable Multimodal CoT Compressor that formulates compression as a sequential decision-making process optimized via reinforcement learning. XMCC can effectively shorten reasoning trajectories while preserving key reasoning steps and answer correctness, and simultaneously generates natural-language explanations for its compression decisions. Extensive experiments on representative multimodal reasoning benchmarks demonstrate that XMCC not only reduces reasoning length but also provides explainable explanations, validating its effectiveness.

</details>


### [137] [Computing Conditional Shapley Values Using Tabular Foundation Models](https://arxiv.org/abs/2602.09489)
*Lars Henry Berge Olsen,Dennis Christensen*

Main category: cs.AI

TL;DR: 本文通过使用TabPFN等表格式基础模型以无需重新训练的方式计算Shapley值，并在模拟和真实数据集上与最新方法进行了对比，证明了TabPFN在大多数情况下表现最佳，且具有极低的运行时间。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能逐渐应用于更多场景，解释可解释性AI变得愈发重要，然而Shapley值的计算成本较高，尤其是当特征间存在依赖关系时。尽管可以通过蒙特卡洛积分或回归的方法来评估Shapley值，但完全利用深度学习回归方法需要重新训练，增加了计算负担。

Method: 本文利用了TabPFN（表格式基础模型），它通过在上下文学习中利用这些模型无需重新训练地近似每个条件期望，降低了Shapley值的计算成本。

Result: 本文在模拟和真实数据集上使用了TabPFN等多种变体计算Shapley值，并将结果与最先进的方法进行了对比。结果显示TabPFN在大多数情况下表现最佳，且几乎可以在极短的时间内生成结果。

Conclusion: 本文证明了基于表格式基础模型的条件Shapley值计算方法的有效性并在实际应用中展现了优异的性能。此外，作者提出了一些进一步提升此类方法性能的建议，并强调如何更具体地适应条件Shapley值估计的任务。

Abstract: Shapley values have become a cornerstone of explainable AI, but they are computationally expensive to use, especially when features are dependent. Evaluating them requires approximating a large number of conditional expectations, either via Monte Carlo integration or regression. Until recently it has not been possible to fully exploit deep learning for the regression approach, because retraining for each conditional expectation takes too long. Tabular foundation models such as TabPFN overcome this computational hurdle by leveraging in-context learning, so each conditional expectation can be approximated without any re-training. In this paper, we compute Shapley values with multiple variants of TabPFN and compare their performance with state-of-the-art methods on both simulated and real datasets. In most cases, TabPFN yields the best performance; where it does not, it is only marginally worse than the best method, at a fraction of the runtime. We discuss further improvements and how tabular foundation models can be better adapted specifically for conditional Shapley value estimation.

</details>


### [138] [FLINGO -- Instilling ASP Expressiveness into Linear Integer Constraints](https://arxiv.org/abs/2602.09620)
*Jorge Fandinno,Pedro Cabalar,Philipp Wanko,Torsten Schaub*

Main category: cs.AI

TL;DR: 该研究引入了FLINGO语言，它在数值约束中整合了ASP中被数值后端表达能力和语义所替代的默认值声明、规则选择和聚合值的特点，同时提供了从FLINGO语法到CLINGCON标准格式CASP程序的转换方法。


<details>
  <summary>Details</summary>
Motivation: 为了解决现实应用中数值约束处理的需求，特别是在保留ASP灵活性和表达能力的情况下，同时兼容数值计算引擎的能力。

Method: FLINGO语言设计和实现，将其与现有的CASP工具CLINGCON集成，提供语法到格式的转换支持。

Result: FLINGO语言增强了CASP的灵活性和表达性，能够支持默认值的声明、规则选择和聚合值操作，并且能将这些新功能兼容地翻译为CLINGCON输入格式的CASP程序。

Conclusion: FLINGO为CASP提供了更强大的数值约束处理能力，同时保留了ASP的灵活表达和规则处理机制。

Abstract: Constraint Answer Set Programming (CASP) is a hybrid paradigm that enriches Answer Set Programming (ASP) with numerical constraint processing, something required in many real-world applications. The usual specification of constraints in most CASP solvers is closer to the numerical back-end expressiveness and semantics, rather than to standard specification in ASP. In the latter, numerical attributes are represented with predicates and this allows declaring default values, leaving the attribute undefined, making non-deterministic assignments with choice rules or using aggregated values. In CASP, most (if not all) of these features are lost once we switch to a constraint-based representation of those same attributes. In this paper, we present the FLINGO language (and tool) that incorporates the aforementioned expressiveness inside the numerical constraints and we illustrate its use with several examples. Based on previous work that established its semantic foundations, we also present a translation from the newly introduced FLINGO syntax to regular CASP programs following the CLINGCON input format.

</details>


### [139] [ClinAlign: Scaling Healthcare Alignment from Clinician Preference](https://arxiv.org/abs/2602.09653)
*Shiwei Lyu,Xidong Wang,Lei Liu,Hao Zhu,Chaohe Zhang,Jian Wang,Jinjie Gu,Benyou Wang,Yue Shen*

Main category: cs.AI

TL;DR: 本文提出了一种两阶段框架，首先构建了HealthRubrics数据集，包含7,034个由医师验证的偏好示例，接着从中提炼出HealthPrinciples原则集，包含119条临床可重用的原理。该框架训练的模型在HealthBench-Hard上实现了33.4%的性能，优于更庞大的模型。


<details>
  <summary>Details</summary>
Motivation: 现有的方法要么依赖粗粒度目标，要么依赖不可靠的自动化裁判。本文旨在通过提出一种两阶段框架来解决这一挑战，该框架能够使大型语言模型的开放输出更好地符合临床医生的具体偏好。

Method: 第一阶段，开发了HealthRubrics数据集，由7,034个由医师验证的偏好示例组成。第二阶段，从这些示例中提炼出了HealthPrinciples原则集，包含119条原则。这些原则按临床维度组织，可用于离线对齐和推理时的自我修正。

Result: 使用HealthPrinciples原则集，训练了一个仅在推断时激活3亿参数的300亿参数模型，在HealthBench-Hard评估基准上达到了33.4%的性能，超越了包括Deepseek-R1和o3在内的更大模型。

Conclusion: 本文提出的方法提供了一种有效且资源效率高的框架，用于临床环境中的大型语言模型对齐，显著提高了模型的性能。

Abstract: Although large language models (LLMs) demonstrate expert-level medical knowledge, aligning their open-ended outputs with fine-grained clinician preferences remains challenging. Existing methods often rely on coarse objectives or unreliable automated judges that are weakly grounded in professional guidelines. We propose a two-stage framework to address this gap. First, we introduce HealthRubrics, a dataset of 7,034 physician-verified preference examples in which clinicians refine LLM-drafted rubrics to meet rigorous medical standards. Second, we distill these rubrics into HealthPrinciples: 119 broadly reusable, clinically grounded principles organized by clinical dimensions, enabling scalable supervision beyond manual annotation. We use HealthPrinciples for (1) offline alignment by synthesizing rubrics for unlabeled queries and (2) an inference-time tool for guided self-revision. A 30B parameter model that activates only 3B parameters at inference trained with our framework achieves 33.4% on HealthBench-Hard, outperforming much larger models including Deepseek-R1 and o3, establishing a resource-efficient baseline for clinical alignment.

</details>


### [140] [GHS-TDA: A Synergistic Reasoning Framework Integrating Global Hypothesis Space with Topological Data Analysis](https://arxiv.org/abs/2602.09794)
*Jiaquan Zhang,Chaoning Zhang,Shuxu Chen,Xudong Wang,Zhenzhen Huang,Pengcheng Zheng,Shuai Yuan,Sheng Zheng,Qigan Sun,Jie Zou,Lik-Hang Lee,Yang Yang*

Main category: cs.AI

TL;DR: GHS-TDA 是一种用于提高大型语言模型在复杂任务上推断准确性的方法。它通过构建语义增强的全局假设图来聚合、对齐和协调多个候选推理路径，并利用拓扑数据分析来捕捉稳定的大尺度结构，从而去除冗余和不一致性，提取更可靠的推理骨架。


<details>
  <summary>Details</summary>
Motivation: 针对现有基于链式思维方法在早期错误的传播与累积、结构冗余和缺乏全局协调机制的问题，GHS-TDA旨在提供一种新的解决方案来提升语言模型在复杂任务中的推理能力。

Method: GHS-TDA 方法首先构建语义增强的全局假设图来聚合和对齐多种推理路径，然后使用拓扑数据分析来捕捉可解释的推理结构，以此来删除冗余和不一致性，提高推理的稳定性和可靠性。

Result: GHS-TDA 方法在多个推理基准上表现出更高的准确性和鲁棒性，能够自适应地收敛，生成高置信度且可解释的推理路径。

Conclusion: GHS-TDA 通过结合推理多样性和拓扑稳定性，有效地解决了现有链式思维方法的缺陷，并取得了显著的性能提升。

Abstract: Chain-of-Thought (CoT) has been shown to significantly improve the reasoning accuracy of large language models (LLMs) on complex tasks. However, due to the autoregressive, step-by-step generation paradigm, existing CoT methods suffer from two fundamental limitations. First, the reasoning process is highly sensitive to early decisions: once an initial error is introduced, it tends to propagate and amplify through subsequent steps, while the lack of a global coordination and revision mechanism makes such errors difficult to correct, ultimately leading to distorted reasoning chains. Second, current CoT approaches lack structured analysis techniques for filtering redundant reasoning and extracting key reasoning features, resulting in unstable reasoning processes and limited interpretability. To address these issues, we propose GHS-TDA. GHS-TDA first constructs a semantically enriched global hypothesis graph to aggregate, align, and coordinate multiple candidate reasoning paths, thereby providing alternative global correction routes when local reasoning fails. It then applies topological data analysis based on persistent homology to capture stable multi-scale structures, remove redundancy and inconsistencies, and extract a more reliable reasoning skeleton. By jointly leveraging reasoning diversity and topological stability, GHS-TDA achieves self-adaptive convergence, produces high-confidence and interpretable reasoning paths, and consistently outperforms strong baselines in terms of both accuracy and robustness across multiple reasoning benchmarks.

</details>


### [141] [Would a Large Language Model Pay Extra for a View? Inferring Willingness to Pay from Subjective Choices](https://arxiv.org/abs/2602.09802)
*Manon Reusens,Sofie Goethals,Toon Calders,David Martens*

Main category: cs.AI

TL;DR: 本研究通过向大型语言模型（LLMs）呈现旅行助手中的选择困境，并使用多项式_logit模型推导出隐含的意愿支付（WTP）估计值。将模型结果与经济文献中的人类基准值进行比较，发现模型在一定程度上可以得出合理的WTP值，但也会在属性层面出现系统偏差，并且通常高估人类的WTP，尤其是引入昂贵选项或商业导向的人格时。改进模型设计可使其估值更接近人类基准。


<details>
  <summary>Details</summary>
Motivation: 鉴于大型语言模型（LLMs）在旅行助手和其他应用场景中被要求做出主观决策，该研究旨在通过分析LLMs的选择决策过程，评估模型在预测用户意愿支付（WTP）方面的表现，为实际应用提供理论依据。

Method: 通过设计特定的选择困境，向LLMs提供不同条件下的交互，运用多项式_logit模型估算WTP，并将其结果与人类基准值进行比较，评估模型性能。

Result: 大型语言模型在预测WTP上有潜力但也存在局限性。研究发现，部分模型能够提供有含义的WTP估计值，但它们在属性层面表现出系统偏差。相较于人类，模型倾向于高估WTP，特别是在引入较高成本选项或商业导向的人格时更为明显。通过对较便宜选项进行偏好条件化，模型估值更接近人类基准值。

Conclusion: 研究强调了使用LLMs进行主观决策支持的既有潜力和局限性，同时也指出了在实际应用中精心选择模型，设计提示以及代表用户的重要性。

Abstract: As Large Language Models (LLMs) are increasingly deployed in applications such as travel assistance and purchasing support, they are often required to make subjective choices on behalf of users in settings where no objectively correct answer exists. We study LLM decision-making in a travel-assistant context by presenting models with choice dilemmas and analyzing their responses using multinomial logit models to derive implied willingness to pay (WTP) estimates. These WTP values are subsequently compared to human benchmark values from the economics literature. In addition to a baseline setting, we examine how model behavior changes under more realistic conditions, including the provision of information about users' past choices and persona-based prompting. Our results show that while meaningful WTP values can be derived for larger LLMs, they also display systematic deviations at the attribute level. Additionally, they tend to overestimate human WTP overall, particularly when expensive options or business-oriented personas are introduced. Conditioning models on prior preferences for cheaper options yields valuations that are closer to human benchmarks. Overall, our findings highlight both the potential and the limitations of using LLMs for subjective decision support and underscore the importance of careful model selection, prompt design, and user representation when deploying such systems in practice.

</details>


### [142] [ESTAR: Early-Stopping Token-Aware Reasoning For Efficient Inference](https://arxiv.org/abs/2602.10004)
*Junda Wang,Zhichao Yang,Dongxu Zhang,Sanjit Singh Batra,Robert E. Tillman*

Main category: cs.AI

TL;DR: ESTAR通过引入一个轨迹分类器、自我生成的(stop)信号以及停止感知的强化学习，减少了LRMs中的冗余推理，使其在减少推理长度的同时保持了准确性，并展示了跨领域的强泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前的大型推理模型（LRMs）在达到正确答案后仍会进行冗余推理，导致计算资源浪费。为了提高模型效率，本文提出了一种方法：Early-Stopping for Token-Aware Reasoning（ESTAR）。

Method: ESTAR包含了三个组件：轨迹分类器用于确定何时可以安全停止推理、自我生成的(stop)信号来教模型自身提出停止命令、以及考虑计算效率的自我生成停止点感知的强化学习。

Result: 实验表明，ESTAR在四个推理数据集上将推理长度减少了约3.7倍（从4,799到1,290），并且保持了74%以上的准确率，展示了良好的跨域泛化能力。

Conclusion: 该研究提出了一种简单但有效的机制，用于增强大型推理模型的推理效率。

Abstract: Large reasoning models (LRMs) achieve state-of-the-art performance by generating long chains-of-thought, but often waste computation on redundant reasoning after the correct answer has already been reached. We introduce Early-Stopping for Token-Aware Reasoning (ESTAR), which detects and reduces such reasoning redundancy to improve efficiency without sacrificing accuracy. Our method combines (i) a trajectory-based classifier that identifies when reasoning can be safely stopped, (ii) supervised fine-tuning to teach LRMs to propose self-generated <stop> signals, and (iii) <stop>-aware reinforcement learning that truncates rollouts at self-generated stop points with compute-aware rewards. Experiments on four reasoning datasets show that ESTAR reduces reasoning length by about 3.7x (from 4,799 to 1,290) while preserving accuracy (74.9% vs. 74.2%), with strong cross-domain generalization. These results highlight early stopping as a simple yet powerful mechanism for improving reasoning efficiency in LRMs.

</details>


### [143] [CODE-SHARP: Continuous Open-ended Discovery and Evolution of Skills as Hierarchical Reward Programs](https://arxiv.org/abs/2602.10085)
*Richard Bornemann,Pierluigi Vito Amadori,Antoine Cully*

Main category: cs.AI

TL;DR: 该研究提出了一种名为CODE-SHARP的新框架，利用基础模型扩展和精化层次技能档案，该档案以可执行奖励函数的形式组织成有向图。通过这种方法训练的智能体能够在Craftax环境中解决更长时间范围的目标。


<details>
  <summary>Details</summary>
Motivation: 当前的强化学习方法通常依赖于人工设计的奖励函数，这限制了其在开放探索技能发现中的应用。现有方法虽有所改善，但仍局限于预定义任务的奖励优化。

Method: CODE-SHARP框架使用基础模型自动发现可执行的奖励函数，并构建一个层次技能档案。这些奖励函数以代码形式表示，形成有向图结构，供智能体使用。

Result: 试验表明，仅依据发现的SHARP技能生成的奖励训练的智能体能够解决更长时间范围的目标。当与基础模型规划器结合时，这些技能让单个智能体能够解决复杂、长时间范围的任务，显著优于预训练智能体和任务特定专家策略。

Conclusion: 该研究展示了基础模型在智能体开放探索新技能和解决问题方面的潜力，并为未来研究开放技能发现提供了新的思路。

Abstract: Developing agents capable of open-endedly discovering and learning novel skills is a grand challenge in Artificial Intelligence. While reinforcement learning offers a powerful framework for training agents to master complex skills, it typically relies on hand-designed reward functions. This is infeasible for open-ended skill discovery, where the set of meaningful skills is not known a priori. While recent methods have shown promising results towards automating reward function design, they remain limited to refining rewards for pre-defined tasks. To address this limitation, we introduce Continuous Open-ended Discovery and Evolution of Skills as Hierarchical Reward Programs (CODE-SHARP), a novel framework leveraging Foundation Models (FM) to open-endedly expand and refine a hierarchical skill archive, structured as a directed graph of executable reward functions in code. We show that a goal-conditioned agent trained exclusively on the rewards generated by the discovered SHARP skills learns to solve increasingly long-horizon goals in the Craftax environment. When composed by a high-level FM-based planner, the discovered skills enable a single goal-conditioned agent to solve complex, long-horizon tasks, outperforming both pretrained agents and task-specific expert policies by over $134$% on average. We will open-source our code and provide additional videos $\href{https://sites.google.com/view/code-sharp/homepage}{here}$.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [144] [Accelerating Post-Quantum Cryptography via LLM-Driven Hardware-Software Co-Design](https://arxiv.org/abs/2602.09410)
*Yuchao Liao,Tosiron Adegbija,Roman Lysecky*

Main category: cs.AR

TL;DR: 本文提出了一种利用大型语言模型（LLMs）加速后量子密码学（PQC）硬件和软件协同设计的新框架。以FALCON数字签名方案为例，展示了LLMs在分析PQC算法、识别关键性能组件以及生成FPGA实施候选硬件描述方面的潜力。与传统的HLS方法相比，LLM驱动的合成方法在内核执行时间上可提高2.6倍，并具有较短的关键路径。


<details>
  <summary>Details</summary>
Motivation: 后量子密码学由于其高度复杂的算法，在硬件上的高效实现具有很大挑战。因此，本文探索了利用大型语言模型加速PQC硬件和软件协同设计的可能性。

Method: 本文提出了一种新的框架，通过利用大型语言模型来分析PQC算法，识别性能关键组件，生成FPGA实现的候选硬件描述。该框架主要用于比较基于FLLM的方法和传统的HLS方法在低级计算密集型内核设计上的性能。

Result: 在比较中，基于LLM的方法在内核执行时间上比常规方法提高了2.6倍，并且具有更短的关键路径。然而，资源利用率和功耗方面的权衡也得到了揭示。

Conclusion: 本文的结果表明，利用大型语言模型可以自动化PQC算法的FPGA加速器设计迭代，从而减少设计努力和开发时间，为在FPGA上进行快速和适应性的PQC加速器设计提供了新的方向。

Abstract: Post-quantum cryptography (PQC) is crucial for securing data against emerging quantum threats. However, its algorithms are computationally complex and difficult to implement efficiently on hardware. In this paper, we explore the potential of Large Language Models (LLMs) to accelerate the hardware-software co-design process for PQC, with a focus on the FALCON digital signature scheme. We present a novel framework that leverages LLMs to analyze PQC algorithms, identify performance-critical components, and generate candidate hardware descriptions for FPGA implementation. We present the first quantitative comparison between LLM-driven synthesis and conventional HLS-based approaches for low-level compute-intensive kernels in FALCON, showing that human-in-the-loop LLM-generated accelerators can achieve up to 2.6x speedup in kernel execution time with shorter critical paths, while highlighting trade-offs in resource utilization and power consumption. Our results suggest that LLMs can minimize design effort and development time by automating FPGA accelerator design iterations for PQC algorithms, offering a promising new direction for rapid and adaptive PQC accelerator design on FPGAs.

</details>


### [145] [Development of an Energy-Efficient and Real-Time Data Movement Strategy for Next-Generation Heterogeneous Mixed-Criticality Systems](https://arxiv.org/abs/2602.09554)
*Thomas Benz*

Main category: cs.AR

TL;DR: 工业领域如汽车、机器人和航空航天正迅速发展，以满足机器学习驱动的自动性、连接性、电动化和共享出行（ACES）的需求。这导致了对高性能计算和通信基础设施的更高要求。面临ACES的需求，计算系统需要向较大规模和更高层次的异构性和专门化发展，通过应用特定硬件加速器，而非仅依赖技术扩展。同时，对高计算性能和异构性的需求推高了对内存带宽和容量的需求，这对复杂的、越来越庞大且不规则数据集的应用程序来说至关重要。此外，ACES要求在同一个物理平台上同时处理实时关键任务和通用计算任务，对通信、存储和微架构资源造成压力，要求最小化不同关键性级别的竞争，以保持高度可预测性。因此，要在广泛的应用场景中满足性能和能效要求，需要仔细设计内存系统与应用场景及计算单元和加速器。


<details>
  <summary>Details</summary>
Motivation: 工业领域的迅速发展以及对ACES的追求，增加了对高性能计算和通信基础设施的需求，推动了对复杂数据和传感器数据的应用，提出了对高性能计算、高能效和低延迟通信的新要求。

Method: 通过应用特定硬件加速器以及对计算系统进行异构性和专门化的设计，提高系统的计算性能和能效。

Result: 工业领域面临ACES所需的高性能计算和高能效需求，通过应用特定硬件加速器以及对系统的异构性和专门化设计，满足了对计算性能和能效的新要求。

Conclusion: 对于工业领域，需要通过精细设计内存系统与应用案例及计算单元和加速器之间的匹配，以确保在大规模、异构和资源受限的环境下，ACES研究中的性能和能效目标得以实现。

Abstract: Industrial domains such as automotive, robotics, and aerospace are rapidly evolving to satisfy the increasing demand for machine-learning-driven Autonomy, Connectivity, Electrification, and Shared mobility (ACES). This paradigm shift inherently and significantly increases the requirement for onboard computing performance and high-performance communication infrastructure. At the same time, Moore's Law and Dennard Scaling are grinding to a halt, in turn, driving computing systems to larger scales and higher levels of heterogeneity and specialization, through application-specific hardware accelerators, instead of relying on technological scaling only. Approaching ACES requires this substantial amount of compute at an increasingly high energy-efficiency, since most use cases are fundamentally resource-bound. This increase in compute performance and heterogeneity goes hand in hand with a growing demand for high memory bandwidth and capacity as the driving applications grow in complexity, operating on huge and progressively irregular data sets and further requiring a steady influx of sensor data, increasing pressure both on on-chip and off-chip interconnect systems. Further, ACES combines real-time time-critical with general compute tasks on the same physical platform, sharing communication, storage, and micro-architectural resources. These heterogeneous mixed-criticality systems (MCSs) place additional pressure on the interconnect, demanding minimal contention between the different criticality levels to sustain a high degree of predictability. Fulfilling the performance and energy-efficiency requirements across a wide range of industrial applications requires a carefully co-designed process of the memory system with the use cases as well as the compute units and accelerators.

</details>
