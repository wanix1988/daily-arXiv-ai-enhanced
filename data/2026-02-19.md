<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 34]
- [cs.CL](#cs.CL) [Total: 52]
- [cs.AI](#cs.AI) [Total: 9]
- [cs.AR](#cs.AR) [Total: 2]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Egocentric Bias in Vision-Language Models](https://arxiv.org/abs/2602.15892)
*Maijunxian Wang,Yijiang Li,Bingyang Wang,Tianwei Zhao,Ran Ji,Qingying Gao,Emmy Liu,Hokin Deng,Dezhi Luo*

Main category: cs.CV

TL;DR: FlipSet 是一个用于评估视觉语言模型在两人视角下的空间转换能力的基准测试，发现大部分模型表现不佳，展示出认知系统整合能力的缺陷。


<details>
  <summary>Details</summary>
Motivation: FlipSet旨在评估视觉语言模型在两人视角转换（Level-2视觉换位）方面的表现，通过这种任务，揭示当前模型在社会认知和空间操作整合方面的局限性。

Method: 通过引入FlipSet基准测试，研究者使用180度旋转二维字符串来评估模型的视角转换能力，同时设计了控制实验来测试模型单独进行空间旋转和理论意识准确性时的表现。

Result: 研究表明，大多数视觉语言模型在FlipSet测试中表现不佳，显示出强烈的以自我为中心的偏差，并且在整合所需技能时表现较差。

Conclusion: 研究指出当前视觉语言模型缺乏将社会意识结合到空间操作所需的机制，因此帧转提供了诊断这种天性能力不足的认知试验平台。

Abstract: Visual perspective taking--inferring how the world appears from another's viewpoint--is foundational to social cognition. We introduce FlipSet, a diagnostic benchmark for Level-2 visual perspective taking (L2 VPT) in vision-language models. The task requires simulating 180-degree rotations of 2D character strings from another agent's perspective, isolating spatial transformation from 3D scene complexity. Evaluating 103 VLMs reveals systematic egocentric bias: the vast majority perform below chance, with roughly three-quarters of errors reproducing the camera viewpoint. Control experiments expose a compositional deficit--models achieve high theory-of-mind accuracy and above-chance mental rotation in isolation, yet fail catastrophically when integration is required. This dissociation indicates that current VLMs lack the mechanisms needed to bind social awareness to spatial operations, suggesting fundamental limitations in model-based spatial reasoning. FlipSet provides a cognitively grounded testbed for diagnosing perspective-taking capabilities in multimodal systems.

</details>


### [2] [Detecting Deepfakes with Multivariate Soft Blending and CLIP-based Image-Text Alignment](https://arxiv.org/abs/2602.15903)
*Jingwei Li,Jiaxin Tong,Pengfei Wu*

Main category: cs.CV

TL;DR: 该研究提出了一种名为MSBA-CLIP的新框架，旨在通过利用CLIP的多模态对齐能力捕捉微妙的伪造痕迹，引入多变量和软融合增强策略以及多变量伪造强度估计模块，显著提升了针对不同类型伪造的检测准确性。


<details>
  <summary>Details</summary>
Motivation: 由于现有伪造检测方法在多样化的伪造技术产生的样本之间缺乏有效的普适性，文章旨在设计一个能够克服上述问题的新框架，以提高伪造检测的准确性和鲁棒性。

Method: 该方法通过结合CLIP的跨模态表征能力，提出了一个多变量和软融合增强策略（MSBA）和一个专门设计的多变量伪造强度估计模型（MFIE），同步捕捉伪造痕迹并引导模型学习相关特征。

Result: 实验结果显示，该方法在不同评估标准下均表现出色，相比现有最佳基线提高了3.32%的准确率和4.02%的AUC值，并且在跨数据集测试中平均提升了3.27%的AUC。

Conclusion: 研究证明了该框架的有效性，并展示了其在伪造检测任务上相对于现有技术的优势，为未来开发更具有普适性和鲁棒性的深伪检测方法提供了新的思路。

Abstract: The proliferation of highly realistic facial forgeries necessitates robust detection methods. However, existing approaches often suffer from limited accuracy and poor generalization due to significant distribution shifts among samples generated by diverse forgery techniques. To address these challenges, we propose a novel Multivariate and Soft Blending Augmentation with CLIP-guided Forgery Intensity Estimation (MSBA-CLIP) framework. Our method leverages the multimodal alignment capabilities of CLIP to capture subtle forgery traces. We introduce a Multivariate and Soft Blending Augmentation (MSBA) strategy that synthesizes images by blending forgeries from multiple methods with random weights, forcing the model to learn generalizable patterns. Furthermore, a dedicated Multivariate Forgery Intensity Estimation (MFIE) module is designed to explicitly guide the model in learning features related to varied forgery modes and intensities. Extensive experiments demonstrate state-of-the-art performance. On in-domain tests, our method improves Accuracy and AUC by 3.32\% and 4.02\%, respectively, over the best baseline. In cross-domain evaluations across five datasets, it achieves an average AUC gain of 3.27\%. Ablation studies confirm the efficacy of both proposed components. While the reliance on a large vision-language model entails higher computational cost, our work presents a significant step towards more generalizable and robust deepfake detection.

</details>


### [3] [A Comprehensive Survey on Deep Learning-Based LiDAR Super-Resolution for Autonomous Driving](https://arxiv.org/abs/2602.15904)
*June Moh Goo,Zichao Zeng,Jan Boehm*

Main category: cs.CV

TL;DR: 本文提供了首个关于自动驾驶中LiDAR超分辨率方法的全面综述，分四类整理已有的方法，并建立了基础概念，如数据表示、问题表述、基准数据集和评估指标。研究趋势包括使用范围图像表示、极端模型压缩和柔分辨率架构开发。


<details>
  <summary>Details</summary>
Motivation: 目前关于LiDAR超分辨率方法的系统性综述尚未出现，这对理解现有技术并指导未来的研究具有重要意义。

Method: 文章将现有的方法分为四类：基于CNN的架构、基于深度模型的递归方法、隐式表示方法以及Transformer和Mamba基的方法。

Result: 研究建立了LiDAR超分辨率的基础概念，如数据表示、问题表述、基准数据集和评估指标，并指出了研究趋势，包括使用范围图像表示、极端模型压缩和开发柔分辨率架构。

Conclusion: 文章指出了LiDAR超分辨率技术中的开放挑战和未来研究方向，旨在推动技术进步，提高实际部署的适用性和效率。

Abstract: LiDAR sensors are often considered essential for autonomous driving, but high-resolution sensors remain expensive while affordable low-resolution sensors produce sparse point clouds that miss critical details. LiDAR super-resolution addresses this challenge by using deep learning to enhance sparse point clouds, bridging the gap between different sensor types and enabling cross-sensor compatibility in real-world deployments. This paper presents the first comprehensive survey of LiDAR super-resolution methods for autonomous driving. Despite the importance of practical deployment, no systematic review has been conducted until now. We organize existing approaches into four categories: CNN-based architectures, model-based deep unrolling, implicit representation methods, and Transformer and Mamba-based approaches. We establish fundamental concepts including data representations, problem formulation, benchmark datasets and evaluation metrics. Current trends include the adoption of range image representation for efficient processing, extreme model compression and the development of resolution-flexible architectures. Recent research prioritizes real-time inference and cross-sensor generalization for practical deployment. We conclude by identifying open challenges and future research directions for advancing LiDAR super-resolution technology.

</details>


### [4] [MaS-VQA: A Mask-and-Select Framework for Knowledge-Based Visual Question Answering](https://arxiv.org/abs/2602.15915)
*Xianwei Mao,Kai Ye,Sheng Zhou,Nan Zhang,Haikuan Huang,Bin Li,Jiajun Bu*

Main category: cs.CV

TL;DR: 提出了一种名为MaS-VQA的框架，该框架通过明确的知识筛选与隐含知识推理相结合来提高视觉问答系统的性能，实验表明该方法在多个模型基础下均能取得一致的性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前知识驱动的视觉问答系统存在知识污染和内部知识难以控制的问题，这限制了推理效果和答案准确性。

Method: MaS-VQA框架通过引入Mask-and-Select机制来精选相关图像区域和知识片段，生成紧凑的多模态高信号知识。然后，利用筛选后的知识在受限语义空间内激活模型内部知识，并结合显性和隐性的知识进行互补的模型化，以实现稳健的答案预测。

Result: 在Encyclopedic-VQA和InfoSeek数据集上的实验结果表明了MaS-VQA框架的一致性能提升，并且消融实验验证了选择机制在降低噪声和增强知识利用方面起到了有效作用。

Conclusion: MaS-VQA框架对于提升知识驱动的视觉问答系统的有效性具有重要意义，并且展示了其在多个模型基础下的广泛适用性。

Abstract: Knowledge-based Visual Question Answering (KB-VQA) requires models to answer questions by integrating visual information with external knowledge. However, retrieved knowledge is often noisy, partially irrelevant, or misaligned with the visual content, while internal model knowledge is difficult to control and interpret. Naive aggregation of these sources limits reasoning effectiveness and reduces answer accuracy. To address this, we propose MaS-VQA, a selection-driven framework that tightly couples explicit knowledge filtering with implicit knowledge reasoning. MaS-VQA first retrieves candidate passages and applies a Mask-and-Select mechanism to jointly prune irrelevant image regions and weakly relevant knowledge fragments, producing compact, high-signal multimodal knowledge . This filtered knowledge then guides the activation of internal knowledge in a constrained semantic space, enabling complementary co-modeling of explicit and implicit knowledge for robust answer prediction. Experiments on Encyclopedic-VQA and InfoSeek demonstrate consistent performance gains across multiple MLLM backbones, and ablations verify that the selection mechanism effectively reduces noise and enhances knowledge utilization.

</details>


### [5] [A Study on Real-time Object Detection using Deep Learning](https://arxiv.org/abs/2602.15926)
*Ankita Bose,Jayasravani Bhumireddy,Naveen N*

Main category: cs.CV

TL;DR: 本文详细介绍了如何使用深度学习算法增强实时物体识别。文章提供了多种物体检测模型，公开基准数据集以及在广泛应用中使用物体检测模型的信息。通过对比实验展示了不同的策略，并提出了一些富有启发性的研究挑战。


<details>
  <summary>Details</summary>
Motivation: 探讨深度学习算法在实时物体检测中的应用，以便更好地理解这些模型在不同领域的潜力。

Method: 文章汇总和比较了多种深学习物体检测模型及其实验结果。

Result: 文章提供了多种物体检测模型，开放的基准数据集，并且通过实验对比了不同的策略。

Conclusion: 文章提出了有关进一步研究深度学习方法和物体识别的一些富有挑战性的想法。

Abstract: Object detection has compelling applications over a range of domains, including human-computer interfaces, security and video surveillance, navigation and road traffic monitoring, transportation systems, industrial automation healthcare, the world of Augmented Reality (AR) and Virtual Reality (VR), environment monitoring and activity identification. Applications of real time object detection in all these areas provide dynamic analysis of the visual information that helps in immediate decision making. Furthermore, advanced deep learning algorithms leverage the progress in the field of object detection providing more accurate and efficient solutions. There are some outstanding deep learning algorithms for object detection which includes, Faster R CNN(Region-based Convolutional Neural Network),Mask R-CNN, Cascade R-CNN, YOLO (You Only Look Once), SSD (Single Shot Multibox Detector), RetinaNet etc. This article goes into great detail on how deep learning algorithms are used to enhance real time object recognition. It provides information on the different object detection models available, open benchmark datasets, and studies on the use of object detection models in a range of applications. Additionally, controlled studies are provided to compare various strategies and produce some illuminating findings. Last but not least, a number of encouraging challenges and approaches are offered as suggestions for further investigation in both relevant deep learning approaches and object recognition.

</details>


### [6] [Can Vision-Language Models See Squares? Text-Recognition Mediates Spatial Reasoning Across Three Model Families](https://arxiv.org/abs/2602.15950)
*Yuval Levental*

Main category: cs.CV

TL;DR: 研究揭示了视觉语言模型(VLMs)在准确识别带有填充但无文本身份的二元网格中的局限性，特别是在非文本图元上表现差。


<details>
  <summary>Details</summary>
Motivation: 为了揭示视觉语言模型在处理非文本视觉元素时的局限性，研究生成了不同密度的网格，并测试了三种前沿模型。

Method: 研究生成了15个15x15的网格，并使用文本符号和填充正方形两种图像类型，让模型进行转录。比较了模型在两种条件下的表现。

Result: 在文本符号条件下，Claude和ChatGPT的准确率和F1约为91%和84%，而Gemini的准确率和F1为84%和63%。在填充正方形条件下，所有模型准确率和F1均下降至60-73%和29-39%。不同模型在填充正方形条件下的表现模式各有不同。

Conclusion: 研究发现，视觉语言模型在空间推理上依赖于高质量的文本识别路径，而不是视觉路径。如果缺少文本信息，则难以进行精确的空间定位。

Abstract: We present a simple experiment that exposes a fundamental limitation in vision-language models (VLMs): the inability to accurately localize filled cells in binary grids when those cells lack textual identity. We generate fifteen 15x15 grids with varying density (10.7%-41.8% filled cells) and render each as two image types -- text symbols (. and #) and filled squares without gridlines -- then ask three frontier VLMs (Claude Opus, ChatGPT 5.2, and Gemini 3 Thinking) to transcribe them. In the text-symbol condition, Claude and ChatGPT achieve approximately 91% cell accuracy and 84% F1, while Gemini achieves 84% accuracy and 63% F1. In the filled-squares condition, all three models collapse to 60-73% accuracy and 29-39% F1. Critically, all conditions pass through the same visual encoder -- the text symbols are images, not tokenized text. The text-vs-squares F1 gap ranges from 34 to 54 points across models, demonstrating that VLMs behave as if they possess a high-fidelity text-recognition pathway for spatial reasoning that dramatically outperforms their native visual pathway. Each model exhibits a distinct failure mode in the squares condition -- systematic under-counting (Claude), massive over-counting (ChatGPT), and template hallucination (Gemini) -- but all share the same underlying deficit: severely degraded spatial localization for non-textual visual elements.

</details>


### [7] [Position-Aware Scene-Appearance Disentanglement for Bidirectional Photoacoustic Microscopy Registration](https://arxiv.org/abs/2602.15959)
*Yiwen Wang,Jiahao Qin*

Main category: cs.CV

TL;DR: 提出了一个名为GPEReg-Net的框架，通过分离场景不变的特征和领域特定的外观代码实现直接的图像到图像注册，同时引入了一个全局位置编码模块，利用帧间的上下文信息提高时间连贯性。


<details>
  <summary>Details</summary>
Motivation: 现有的图像注册方法在处理光电拉曼显微镜成像中出现的时间连贯性问题时效果有限，因此需要一种能够利用帧间上下文信息提高时间连贯性的新方法。

Method: GPEReg-Net通过采用自适应实例归一化（AdaIN）将场景不变特征和领域特定外观代码分离，引入全局位置编码模块以利用相邻帧信息增强时间连贯性。

Result: GPEReg-Net在OR-PAM-Reg-4K基准测试中表现出色，NCC为0.953，SSIM为0.932，PSNR为34.49dB，相比当前最先进技术分别提高了3.8%和1.99dB。

Conclusion: 论文介绍了GPEReg-Net框架，有效提高了光电拉曼显微镜成像的时间连贯性，未来可能进一步应用于其他领域的时间序列图像处理任务。

Abstract: High-speed optical-resolution photoacoustic microscopy (OR-PAM) with bidirectional raster scanning doubles imaging speed but introduces coupled domain shift and geometric misalignment between forward and backward scan lines. Existing registration methods, constrained by brightness constancy assumptions, achieve limited alignment quality, while recent generative approaches address domain shift through complex architectures that lack temporal awareness across frames. We propose GPEReg-Net, a scene-appearance disentanglement framework that separates domain-invariant scene features from domain-specific appearance codes via Adaptive Instance Normalization (AdaIN), enabling direct image-to-image registration without explicit deformation field estimation. To exploit temporal structure in sequential acquisitions, we introduce a Global Position Encoding (GPE) module that combines learnable position embeddings with sinusoidal encoding and cross-frame attention, allowing the network to leverage context from neighboring frames for improved temporal coherence. On the OR-PAM-Reg-4K benchmark (432 test samples), GPEReg-Net achieves NCC of 0.953, SSIM of 0.932, and PSNR of 34.49dB, surpassing the state-of-the-art by 3.8% in SSIM and 1.99dB in PSNR while maintaining competitive NCC. Code is available at https://github.com/JiahaoQin/GPEReg-Net.

</details>


### [8] [Non-Contact Physiological Monitoring in Pediatric Intensive Care Units via Adaptive Masking and Self-Supervised Learning](https://arxiv.org/abs/2602.15967)
*Mohamed Khalil Ben Salah,Philippe Jouvet,Rita Noumeir*

Main category: cs.CV

TL;DR: 本文提出了一种用于PICU环境的自监督预训练框架，采用渐进学习策略，并结合VisionMamba架构和自适应遮罩机制，显著提高了rPPG的估计准确性，最终达到3.2 bpm的平均绝对误差。


<details>
  <summary>Details</summary>
Motivation: 解决接触式传感器（如脉搏氧饱和度仪）在儿科重症监护病房（PICU）中可能引起的皮肤刺激、感染风险和患者不适等问题，通过引入无接触的远程光电容积图（rPPG）技术进行心率监测，但现有技术存在运动伪影、遮挡和光照变化等问题。

Method: 本文提出了一种基于渐进式学习的自监督预训练框架，采用VisionMamba架构和自适应掩码机制。该框架通过三个阶段进行训练：带有标签的公开视频数据、合成的遮挡场景数据以及来自500名儿科患者的未标记视频数据。

Result: 实验结果表明，该框架相比标准的掩码自编码器减少了42%的平均绝对误差，并且相比PhysFormer提高了31%，最终实现3.2 bpm的平均绝对误差。模型能够准确关注脉搏丰富的区域，并在实际医疗场景中的遮挡和噪声下表现出良好的鲁棒性。

Conclusion: 该框架能够有效提高rPPG在儿科重症监护病房环境中的心率监测准确性，可为早期临床恶化检测和有效的临床决策提供有力支持。

Abstract: Continuous monitoring of vital signs in Pediatric Intensive Care Units (PICUs) is essential for early detection of clinical deterioration and effective clinical decision-making. However, contact-based sensors such as pulse oximeters may cause skin irritation, increase infection risk, and lead to patient discomfort. Remote photoplethysmography (rPPG) offers a contactless alternative to monitor heart rate using facial video, but remains underutilized in PICUs due to motion artifacts, occlusions, variable lighting, and domain shifts between laboratory and clinical data.
  We introduce a self-supervised pretraining framework for rPPG estimation in the PICU setting, based on a progressive curriculum strategy. The approach leverages the VisionMamba architecture and integrates an adaptive masking mechanism, where a lightweight Mamba-based controller assigns spatiotemporal importance scores to guide probabilistic patch sampling. This strategy dynamically increases reconstruction difficulty while preserving physiological relevance.
  To address the lack of labeled clinical data, we adopt a teacher-student distillation setup. A supervised expert model, trained on public datasets, provides latent physiological guidance to the student. The curriculum progresses through three stages: clean public videos, synthetic occlusion scenarios, and unlabeled videos from 500 pediatric patients.
  Our framework achieves a 42% reduction in mean absolute error relative to standard masked autoencoders and outperforms PhysFormer by 31%, reaching a final MAE of 3.2 bpm. Without explicit region-of-interest extraction, the model consistently attends to pulse-rich areas and demonstrates robustness under clinical occlusions and noise.

</details>


### [9] [LAND: A Longitudinal Analysis of Neuromorphic Datasets](https://arxiv.org/abs/2602.15973)
*Gregory Cohen,Alexandre Marcireau*

Main category: cs.CV

TL;DR: 该论文总结了神经形态工程中的数据问题，涵盖了超过423个数据集，分析了这些数据集的任务和数据结构，并指出现有数据集的规模、非标准化和获取难度，同时探讨了合成数据的益处及其潜在局限性，提出了元数据集的概念以减少对更多数据的需求和消除定义数据集和任务时的潜在偏差。


<details>
  <summary>Details</summary>
Motivation: 论文旨在解决神经形态工程领域数据稀缺和获取困难的问题，探讨现有数据集的特点及其局限性，以促进该领域的发展。

Method: 通过收集和分析超过423个神经形态数据集，探索了这些数据集的任务和数据结构，特别是关注数据集的规模、标准化程度以及获取难度。此外，该论文还探讨了合成数据的使用，并提出了元数据集的概念。

Result: 论文发现了现有神经形态数据集规模巨大、缺乏标准且获取难度大，同时提出了合成数据在测试现有算法方面的益处，以及元数据集的概念来解决数据需求问题和消除偏见。

Conclusion: 该论文强调需要进一步完善神经形态工程领域数据集的标准化和获取方式，以支持新的应用程序并克服合成数据的局限性。

Abstract: Neuromorphic engineering has a data problem. Despite the meteoric rise in the number of neuromorphic datasets published over the past ten years, the conclusion of a significant portion of neuromorphic research papers still states that there is a need for yet more data and even larger datasets. Whilst this need is driven in part by the sheer volume of data required by modern deep learning approaches, it is also fuelled by the current state of the available neuromorphic datasets and the difficulties in finding them, understanding their purpose, and determining the nature of their underlying task. This is further compounded by practical difficulties in downloading and using these datasets. This review starts by capturing a snapshot of the existing neuromorphic datasets, covering over 423 datasets, and then explores the nature of their tasks and the underlying structure of the presented data. Analysing these datasets shows the difficulties arising from their size, the lack of standardisation, and difficulties in accessing the actual data. This paper also highlights the growth in the size of individual datasets and the complexities involved in working with the data. However, a more important concern is the rise of synthetic datasets, created by either simulation or video-to-events methods. This review explores the benefits of simulated data for testing existing algorithms and applications, highlighting the potential pitfalls for exploring new applications of neuromorphic technologies. This review also introduces the concepts of meta-datasets, created from existing datasets, as a way of both reducing the need for more data, and to remove potential bias arising from defining both the dataset and the task.

</details>


### [10] [SAM 3D Body: Robust Full-Body Human Mesh Recovery](https://arxiv.org/abs/2602.15989)
*Xitong Yang,Devansh Kukreja,Don Pinkus,Anushka Sagar,Taosha Fan,Jinhyung Park,Soyong Shin,Jinkun Cao,Jiawei Liu,Nicolas Ugrinovic,Matt Feiszli,Jitendra Malik,Piotr Dollar,Kris Kitani*

Main category: cs.CV

TL;DR: 3DB 是一种用于单张图像全身 3D 人体网格恢复的新模型，采用新颖的 Momentum Human Rig (MHR) 参数化网格表示，具有出色的泛化能力和多样条件下的准确度。


<details>
  <summary>Details</summary>
Motivation: 提高单张图像中人体 3D 网格恢复的效果，并创新地提出 MHR 参数化网格表示，以更好地分离骨骼结构和表面形状。

Method: 3DB 使用编码解码器架构，并支持辅助提示，如 2D 关键点和掩码，实现用户指导的推理。它采用了多阶段注释流水线，结合手动关键点注释、可微优化、多视图几何和密集关键点检测，从不同角度收集高质量的人体注释。此外，还开发了一个高效的数据引擎来增强数据的多样性和精度。

Result: 3DB 在定性和定量两个方面都显示出优于先前方法的效果，并且具有优秀的泛化能力，为新的评价数据集制定了标准，该数据集按姿势和外观类别组织，有助于对模型行为进行细粒度分析。

Conclusion: 3DB 和 MHR 都是开源项目，展现了在单张图像中全身 3D 人体网格恢复领域的重大进展。

Abstract: We introduce SAM 3D Body (3DB), a promptable model for single-image full-body 3D human mesh recovery (HMR) that demonstrates state-of-the-art performance, with strong generalization and consistent accuracy in diverse in-the-wild conditions. 3DB estimates the human pose of the body, feet, and hands. It is the first model to use a new parametric mesh representation, Momentum Human Rig (MHR), which decouples skeletal structure and surface shape. 3DB employs an encoder-decoder architecture and supports auxiliary prompts, including 2D keypoints and masks, enabling user-guided inference similar to the SAM family of models. We derive high-quality annotations from a multi-stage annotation pipeline that uses various combinations of manual keypoint annotation, differentiable optimization, multi-view geometry, and dense keypoint detection. Our data engine efficiently selects and processes data to ensure data diversity, collecting unusual poses and rare imaging conditions. We present a new evaluation dataset organized by pose and appearance categories, enabling nuanced analysis of model behavior. Our experiments demonstrate superior generalization and substantial improvements over prior methods in both qualitative user preference studies and traditional quantitative analysis. Both 3DB and MHR are open-source.

</details>


### [11] [MedProbCLIP: Probabilistic Adaptation of Vision-Language Foundation Model for Reliable Radiograph-Report Retrieval](https://arxiv.org/abs/2602.16019)
*Ahmad Elallaf,Yu Zhang,Yuktha Priya Masupalli,Jeong Yang,Young Lee,Zechun Cao,Gongbo Liang*

Main category: cs.CV

TL;DR: MedProbCLIP 是一种面向胸部X光片和放射学报告的概率视觉语言学习框架，与确定性方法相比，在检索和零样本分类中表现出色，特别是在校准、风险覆盖率、选择性检索可靠性以及临床相关干扰的鲁棒性方面。


<details>
  <summary>Details</summary>
Motivation: 在高风险医疗应用场景中，确定性的视觉语言模型的可靠性不足。MedProbCLIP 通过引入概率对比目标和变分信息瓶颈，提供了一种在不确定性捕获和临床同源性精细监督方面更具鲁棒性的方法。

Method: MedProbCLIP 采用高斯嵌入模型表示图像和文本，并通过概率对比目标捕捉不确定性及放射图与临床笔记之间的多对多对应关系。变分信息瓶颈用于减轻过自信预测。同时，多视角放射图编码和多部分报告编码在训练中提供临床对齐的细化监督，但在推理时仅需单个放射图和单个报告。

Result: MedProbCLIP 在 MIMIC-CXR 数据集上表现优于确定性和概率基线（包括 CLIP，CXR-CLIP 和 PCME++），特别是在检索和零样本分类上的准确度和校准度方面。

Conclusion: 该工作强调了概率视觉语言建模对于提高医学成像文本检索系统可信性和安全性的价值。

Abstract: Vision-language foundation models have emerged as powerful general-purpose representation learners with strong potential for multimodal understanding, but their deterministic embeddings often fail to provide the reliability required for high-stakes biomedical applications. This work introduces MedProbCLIP, a probabilistic vision-language learning framework for chest X-ray and radiology report representation learning and bidirectional retrieval. MedProbCLIP models image and text representations as Gaussian embeddings through a probabilistic contrastive objective that explicitly captures uncertainty and many-to-many correspondences between radiographs and clinical narratives. A variational information bottleneck mitigates overconfident predictions, while MedProbCLIP employs multi-view radiograph encoding and multi-section report encoding during training to provide fine-grained supervision for clinically aligned correspondence, yet requires only a single radiograph and a single report at inference. Evaluated on the MIMIC-CXR dataset, MedProbCLIP outperforms deterministic and probabilistic baselines, including CLIP, CXR-CLIP, and PCME++, in both retrieval and zero-shot classification. Beyond accuracy, MedProbCLIP demonstrates superior calibration, risk-coverage behavior, selective retrieval reliability, and robustness to clinically relevant corruptions, underscoring the value of probabilistic vision-language modeling for improving the trustworthiness and safety of radiology image-text retrieval systems.

</details>


### [12] [LGQ: Learning Discretization Geometry for Scalable and Stable Image Tokenization](https://arxiv.org/abs/2602.16086)
*Idil Bilge Altun,Mert Onur Cakiroglu,Elham Buxton,Mehmet Dalkilic,Hasan Kurban*

Main category: cs.CV

TL;DR: 引入了一种名为Learnable Geometric Quantization (LGQ) 的离散图像分词模型，它可以学习端到端的分词几何结构。与现有技术相比，LGQ 在 ImageNet 上实现了更稳定的优化和更平衡的分词利用。


<details>
  <summary>Details</summary>
Motivation: 离散图像分词是可扩展视觉生成的一个关键瓶颈，现有方法在保持分词紧凑性与保持语义结构利用离散能力之间存在权衡。LGQ 提供了一种解决方案来解决这一问题。

Method: LGQ 使用温度控制的软指派代替了硬最近邻查找，从而实现了完全可微的训练。并结合了单词级别的尖锐度正则化和全局利用正则化来鼓励有信心且平衡的代码利用。

Result: 在带有 ImageNet 的 VQGAN 风格的主干网下，LGQ 实现了稳定优化和平衡利用。与 FSQ 相比，在 16K 代码本大小下，提高了 rFID 11.88%，同时使用了 49.96% 更少的活动代码。与 SimVQ 相比，在 49.45% 更低的有效表示率下，rFID 提高了 6.06%，实现了与大量更少活动条目相似的保真度。

Conclusion: LGQ 是一种有效的图像分词方法，可以提高视觉生成任务中的性能。

Abstract: Discrete image tokenization is a key bottleneck for scalable visual generation: a tokenizer must remain compact for efficient latent-space priors while preserving semantic structure and using discrete capacity effectively. Existing quantizers face a trade-off: vector-quantized tokenizers learn flexible geometries but often suffer from biased straight-through optimization, codebook under-utilization, and representation collapse at large vocabularies. Structured scalar or implicit tokenizers ensure stable, near-complete utilization by design, yet rely on fixed discretization geometries that may allocate capacity inefficiently under heterogeneous latent statistics.
  We introduce Learnable Geometric Quantization (LGQ), a discrete image tokenizer that learns discretization geometry end-to-end. LGQ replaces hard nearest-neighbor lookup with temperature-controlled soft assignments, enabling fully differentiable training while recovering hard assignments at inference. The assignments correspond to posterior responsibilities of an isotropic Gaussian mixture and minimize a variational free-energy objective, provably converging to nearest-neighbor quantization in the low-temperature limit. LGQ combines a token-level peakedness regularizer with a global usage regularizer to encourage confident yet balanced code utilization without imposing rigid grids.
  Under a controlled VQGAN-style backbone on ImageNet across multiple vocabulary sizes, LGQ achieves stable optimization and balanced utilization. At 16K codebook size, LGQ improves rFID by 11.88% over FSQ while using 49.96% fewer active codes, and improves rFID by 6.06% over SimVQ with 49.45% lower effective representation rate, achieving comparable fidelity with substantially fewer active entries. Our GitHub repository is available at: https://github.com/KurbanIntelligenceLab/LGQ

</details>


### [13] [OmniCT: Towards a Unified Slice-Volume LVLM for Comprehensive CT Analysis](https://arxiv.org/abs/2602.16110)
*Tianwei Lin,Zhongwei Qiu,Wenqiao Zhang,Jiang Liu,Yihan Xie,Mingjian Gao,Zhenxuan Fan,Zhaocheng Li,Sijing Li,Zhongle Xie,Peng LU,Yueting Zhuang,Yingda Xia,Ling Zhang,Beng Chin Ooi*

Main category: cs.CV

TL;DR: OmniCT 是一种结合了空间一致性和器官层面语义增强的统一 slice-volume LVLM，通过 volumetric slice composition, tri-axial positional embedding, MoE hybrid projection, 和 MedEval-CT 数据集及基准，它在多个临床任务上显著优于现有方法，为跨模态医学影像理解设定了新范式。


<details>
  <summary>Details</summary>
Motivation: 现有 LVLMs 在 CT 标记的理解方面存在局限性，如缺乏跨片空间一致性或体积语义捕捉不足，这阻碍了 LVLMs 在临床上的转化应用。OmniCT 旨在弥合这些差距，提供一种统一的 slice-volume LVLM 解决方案。

Method: OmniCT 使用 volumetric slice composition 和 tri-axial positional embedding 来增强空间一致性，并利用 MoE hybrid projection实现高效地 slice-volume 调适。此外，它还加入了器官层面语义增强 (OSE)，通过定位 ROI 来明确对齐解剖区域，突出病灶和器官层面的语义。

Result: OmniCT 在广泛的临床任务中表现出色，一致地超越了现有方法，同时满足微级别细节敏感性和宏观空间推理的需求。它还引入了 MedEval-CT 数据集及基准，用于统一评估。

Conclusion: OmniCT 通过提供一种新的多模态医学影像理解范式，显著提升了 CT 标记的理解和临床应用。

Abstract: Computed Tomography (CT) is one of the most widely used and diagnostically information-dense imaging modalities, covering critical organs such as the heart, lungs, liver, and colon. Clinical interpretation relies on both slice-driven local features (e.g., sub-centimeter nodules, lesion boundaries) and volume-driven spatial representations (e.g., tumor infiltration, inter-organ anatomical relations). However, existing Large Vision-Language Models (LVLMs) remain fragmented in CT slice versus volumetric understanding: slice-driven LVLMs show strong generalization but lack cross-slice spatial consistency, while volume-driven LVLMs explicitly capture volumetric semantics but suffer from coarse granularity and poor compatibility with slice inputs. The absence of a unified modeling paradigm constitutes a major bottleneck for the clinical translation of medical LVLMs. We present OmniCT, a powerful unified slice-volume LVLM for CT scenarios, which makes three contributions: (i) Spatial Consistency Enhancement (SCE): volumetric slice composition combined with tri-axial positional embedding that introduces volumetric consistency, and an MoE hybrid projection enables efficient slice-volume adaptation; (ii) Organ-level Semantic Enhancement (OSE): segmentation and ROI localization explicitly align anatomical regions, emphasizing lesion- and organ-level semantics; (iii) MedEval-CT: the largest slice-volume CT dataset and hybrid benchmark integrates comprehensive metrics for unified evaluation. OmniCT consistently outperforms existing methods with a substantial margin across diverse clinical tasks and satisfies both micro-level detail sensitivity and macro-level spatial reasoning. More importantly, it establishes a new paradigm for cross-modal medical imaging understanding.

</details>


### [14] [CHAI: CacHe Attention Inference for text2video](https://arxiv.org/abs/2602.16132)
*Joel Mathew Cherian,Ashutosh Muralidhara Bharadwaj,Vima Gupta,Anand Padmanabha Iyer*

Main category: cs.CV

TL;DR: CHAI 使用交叉推断缓存技术，在保持视频质量的同时，减少 latency，仅需 8 步去噪步骤即可生成高质量的视频。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到视频的扩散模型虽然效果显著，但因需对 3D 潜在变量进行串行去噪，导致速度较慢。现有的加速方法要么需要重新训练昂贵的模型，要么依赖于基于启发式的步速跳过，这在减少去噪步骤时难以保持视频质量。

Method: CHAI 引入了名为 Cache Attention 的选择性注意机制，以有效地重用跨交叉推断潜在变量的共享对象/场景。这种方法允许基于语义相关度的提示之间有效利用缓存的潜在变量，从而实现较高的缓存命中率。

Result: 当与基础模型 OpenSora 1.2 结合使用时，CHAI 可以减少 1.65 到 3.35 倍的延迟时间，同时保持视频质量，并且可以在仅 8 步的去噪步骤下生成高质量的视频。

Conclusion: 总之，CHAI 在保留视频质量和降低生成时间方面取得显著进展，是文本到视频扩散模型发展的积极贡献。

Abstract: Text-to-video diffusion models deliver impressive results but remain slow because of the sequential denoising of 3D latents. Existing approaches to speed up inference either require expensive model retraining or use heuristic-based step skipping, which struggles to maintain video quality as the number of denoising steps decreases. Our work, CHAI, aims to use cross-inference caching to reduce latency while maintaining video quality. We introduce Cache Attention as an effective method for attending to shared objects/scenes across cross-inference latents. This selective attention mechanism enables effective reuse of cached latents across semantically related prompts, yielding high cache hit rates. We show that it is possible to generate high-quality videos using Cache Attention with as few as 8 denoising steps. When integrated into the overall system, CHAI is 1.65x - 3.35x faster than baseline OpenSora 1.2 while maintaining video quality.

</details>


### [15] [IRIS: Intent Resolution via Inference-time Saccades for Open-Ended VQA in Large Vision-Language Models](https://arxiv.org/abs/2602.16138)
*Parsa Madinei,Srijita Karmakar,Russell Cohen Hoffing,Felix Gervitz,Miguel P. Eckstein*

Main category: cs.CV

TL;DR: IRIS 是一种不需要训练的方法，利用实时眼动追踪数据解决开放式 VQA 中的歧义问题，通过用户研究发现，参与者开始提问时的注视点最有助于大型 VLM 的消歧，显著提升了回答准确率。


<details>
  <summary>Details</summary>
Motivation: 当前的 VQA 系统在处理开放性问题时面临大量歧义情况，传统方法需要大量训练或复杂的特征工程，无法有效解决这一问题。

Method: IRIS 方法通过分析用户在提问时刻的注视点，实时地辅助 VQA 系统进行消歧处理。该方法直接利用眼动追踪数据，无需额外的训练。

Result: 在 500 个图像-问题配对的用户研究中，IRIS 方法显著提高了对模糊问题的准确率，从 35.2% 提高到 77.2%，同时保持了无歧义问题的性能。

Conclusion: IRIS 方法不仅带来了技术上的突破，也为未来 VQA 系统的眼动数据利用提供了新的思路和评价标准。

Abstract: We introduce IRIS (Intent Resolution via Inference-time Saccades), a novel training-free approach that uses eye-tracking data in real-time to resolve ambiguity in open-ended VQA. Through a comprehensive user study with 500 unique image-question pairs, we demonstrate that fixations closest to the time participants start verbally asking their questions are the most informative for disambiguation in Large VLMs, more than doubling the accuracy of responses on ambiguous questions (from 35.2% to 77.2%) while maintaining performance on unambiguous queries. We evaluate our approach across state-of-the-art VLMs, showing consistent improvements when gaze data is incorporated in ambiguous image-question pairs, regardless of architectural differences. We release a new benchmark dataset to use eye movement data for disambiguated VQA, a novel real-time interactive protocol, and an evaluation suite.

</details>


### [16] [Evaluating Demographic Misrepresentation in Image-to-Image Portrait Editing](https://arxiv.org/abs/2602.16149)
*Huichan Seo,Minki Hong,Sieun Choi,Jihie Kim,Jean Oh*

Main category: cs.CV

TL;DR: 本研究调查了在图像到图像（I2I）编辑模型中，源自种族、性别和年龄的指令如何导致身份保留失败。研究通过正式定义两种失败模式——软抹除和刻板印象替换，并使用诊断提示集生成和编辑肖像，展示了身份保留失败的普遍性、不平衡性以及由预设的社会偏见形成。最后，通过仅调整提示级别来限制身份信息的输入，即使在不更新模型的情况下也能大幅减少少数群体的身份变化，同时保留主要群体肖像的相对恒定。研究结果强调了I2I编辑中的身份保留作为核心但不平衡的失败模式，并促进了更稳健的编辑系统。


<details>
  <summary>Details</summary>
Motivation: 本文旨在填补现有研究中探讨较少的图像到图像编辑中的身份条件偏差，通过一项控制基准测试，识别出刻板印象替换和软抹除两种失败模式，分析这些偏差的特点及其背后的社会先入为主偏见。

Method: 该研究通过使用一组诊断性提示语生成各种年龄段、性别和种族的肖像，并对照一个基准测试集进行了编辑。研究分别使用视觉语言模型打分和人工评价来评估不同的图像到图像编辑器的性能。通过这些评估，检查了编辑行为是否受到人群属性的条件影响，以可视化和量化识别出的模式。

Result: 结果显示，身份保留失败是广泛存在的，并且在整个弹性和不同群体之间分布不均匀。这些失败主要受隐含的社会先入为主观念的影响，例如根据职业预测性别。此外，仅通过调整提示语级别来施加身份信息限制，可以显著降低少数群体的图像中身份属性的变化，但不会对主要群体的影像产生显著影响，这表明当前编辑器中的身份先入为主观念是不对称的。

Conclusion: 本研究表明，在图像到图像编辑过程中，身份保持存在着广泛的不均衡失败，这种失败模式受到隐含的社会先入为主观念的影响。虽然调整提示级别不会改变底层模型，但可以显著减少少数群体身份信息的变化。这项工作提供了关于I2I编辑中身份保持问题的新见解，并强调需要开发更鲁棒的编辑系统来减轻这些问题。

Abstract: Demographic bias in text-to-image (T2I) generation is well studied, yet demographic-conditioned failures in instruction-guided image-to-image (I2I) editing remain underexplored. We examine whether identical edit instructions yield systematically different outcomes across subject demographics in open-weight I2I editors. We formalize two failure modes: Soft Erasure, where edits are silently weakened or ignored in the output image, and Stereotype Replacement, where edits introduce unrequested, stereotype-consistent attributes. We introduce a controlled benchmark that probes demographic-conditioned behavior by generating and editing portraits conditioned on race, gender, and age using a diagnostic prompt set, and evaluate multiple editors with vision-language model (VLM) scoring and human evaluation. Our analysis shows that identity preservation failures are pervasive, demographically uneven, and shaped by implicit social priors, including occupation-driven gender inference. Finally, we demonstrate that a prompt-level identity constraint, without model updates, can substantially reduce demographic change for minority groups while leaving majority-group portraits largely unchanged, revealing asymmetric identity priors in current editors. Together, our findings establish identity preservation as a central and demographically uneven failure mode in I2I editing and motivate demographic-robust editing systems. Project page: https://seochan99.github.io/i2i-demographic-bias

</details>


### [17] [EasyControlEdge: A Foundation-Model Fine-Tuning for Edge Detection](https://arxiv.org/abs/2602.16238)
*Hiroki Nakamura,Hiroto Iino,Masashi Okada,Tadahiro Taniguchi*

Main category: cs.CV

TL;DR: EasyControlEdge 通过适应图像生成基础模型来解决边缘检测问题，它可以生成高清晰度且高效利用数据的边缘检测结果。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，边缘检测需要高清晰度和数据效率，但现有方法在有限的训练样本下生成清晰边缘地图仍然具有挑战性。图像生成基础模型虽在许多下游任务中表现优秀，但其数据高效的转移学习能力和高频率细节保留在边缘检测中的应用仍待开发。

Method: 提出了一种针对边缘检测的图像生成基础模型的专有适应方法，引入了一种边缘导向的目标函数和高效的像素空间损失函数，通过无条件动态指导，在推理阶段提供边缘密度控制。

Result: 在 BSDS500、NYUDv2、BIPED 和 CubiCasa 数据集上进行的实验表明，该方法与最先进的方法相比具有一致的改进，特别是在无需后处理的清晰度评估和有限训练数据情况下。

Conclusion: 研究表明，EasyControlEdge 方法在边缘检测中具有高清晰度和数据效率优势，可为实际应用场景提供更好支持。

Abstract: We propose EasyControlEdge, adapting an image-generation foundation model to edge detection. In real-world edge detection (e.g., floor-plan walls, satellite roads/buildings, and medical organ boundaries), crispness and data efficiency are crucial, yet producing crisp raw edge maps with limited training samples remains challenging. Although image-generation foundation models perform well on many downstream tasks, their pretrained priors for data-efficient transfer and iterative refinement for high-frequency detail preservation remain underexploited for edge detection. To enable crisp and data-efficient edge detection using these capabilities, we introduce an edge-specialized adaptation of image-generation foundation models. To better specialize the foundation model for edge detection, we incorporate an edge-oriented objective with an efficient pixel-space loss. At inference, we introduce guidance based on unconditional dynamics, enabling a single model to control the edge density through a guidance scale. Experiments on BSDS500, NYUDv2, BIPED, and CubiCasa compare against state-of-the-art methods and show consistent gains, particularly under no-post-processing crispness evaluation and with limited training data.

</details>


### [18] [AFFMAE: Scalable and Efficient Vision Pretraining for Desktop Graphics Cards](https://arxiv.org/abs/2602.16249)
*David Smerkous,Zian Wang,Behzad Najafian*

Main category: cs.CV

TL;DR: AFFMAE 提供了一种新的预训练框架，通过自适应的、离散的 token 合并方式，减少了密集网格假设，同时保持了层次结构的可扩展性。该方法在高分辨率电子显微镜分割任务上表现与 ViT-MAE 相当，但在参数量相同的情况下 FLOPs 减少 7 倍，内存使用减少一半，并且在单个 RTX 5090 上训练速度更快。


<details>
  <summary>Details</summary>
Motivation: 现有的自监督预训练方法如 MAE 通过编码可见_token_来减少计算量，但与分层下采样架构结合时仍存在结构性挑战，尤其是在保留层次可扩展性的同时避免密集网格假设。

Method: AFFMAE 引入了一种新的预训练框架，基于自适应的、离散的 token 合并方法，通过在可见 token 上进行动态合并而不是密集网格假设来结合 MAE 与分层架构。同时，该框架还采用了 Flash 样式的混合精度集群注意力内核，并通过深层监督来缓解稀疏阶段表示崩溃的问题。

Result: 在高分辨率电子显微镜分割任务上， AFFMAE 在参数量相同的情况下，达到了与 ViT-MAE 相当的性能，但 FLOPs 降低了 7 倍，内存使用量减少了一半，并且在单个 RTX 5090 上的训练速度更快。

Conclusion: AFFMAE 提供了一种有效的方案，使自监督预训练能够在保持高效率和性能的同时，更好地处理分层架构的复杂性，为科研实验室提供了更为数据效率的模型开发方案。

Abstract: Self-supervised pretraining has transformed computer vision by enabling data-efficient fine-tuning, yet high-resolution training typically requires server-scale infrastructure, limiting in-domain foundation model development for many research laboratories. Masked Autoencoders (MAE) reduce computation by encoding only visible tokens, but combining MAE with hierarchical downsampling architectures remains structurally challenging due to dense grid priors and mask-aware design compromises. We introduce AFFMAE, a masking-friendly hierarchical pretraining framework built on adaptive, off-grid token merging. By discarding masked tokens and performing dynamic merging exclusively over visible tokens, AFFMAE removes dense-grid assumptions while preserving hierarchical scalability. We developed numerically stable mixed-precision Flash-style cluster attention kernels, and mitigate sparse-stage representation collapse via deep supervision. On high-resolution electron microscopy segmentation, AFFMAE matches ViT-MAE performance at equal parameter count while reducing FLOPs by up to 7x, halving memory usage, and achieving faster training on a single RTX 5090. Code available at https://github.com/najafian-lab/affmae.

</details>


### [19] [Breaking the Sub-Millimeter Barrier: Eyeframe Acquisition from Color Images](https://arxiv.org/abs/2602.16281)
*Manel Guzmán,Antonio Agudo*

Main category: cs.CV

TL;DR: 本文提出了一种基于人工视觉的镜框追踪方法，通过多视角信息进行精确的眼镜框轮廓测量，替代了传统机械工具，提升了工作效率并简化了工作流程。


<details>
  <summary>Details</summary>
Motivation: 传统的镜框追踪依赖于机械工具，需要精确的位置和校准，耗时且需要额外设备。为此，本文提出了一种利用多视角信息的新型人工视觉方法，旨在提高精度并简化工作流程。

Method: 该方法基于InVision系统的图像捕获，包括图像采集、镜框分割、深度估计和多视角处理步骤。多视角处理将分割后的RGB图像与深度数据结合，以进行精确的镜框轮廓测量。

Result: 实验结果表明，不同的配置和变体在真实数据上提供了具有竞争力的测量结果，优于其他解决方案，无需专用的镜框追踪设备，减少了光学技术人员的工作流程复杂度。

Conclusion: 本文提出的多视角人工视觉镜框追踪方法为光学行业提供了一种高效、简便的新技术，有助于提高生产效率和工作质量。

Abstract: Eyeframe lens tracing is an important process in the optical industry that requires sub-millimeter precision to ensure proper lens fitting and optimal vision correction. Traditional frame tracers rely on mechanical tools that need precise positioning and calibration, which are time-consuming and require additional equipment, creating an inefficient workflow for opticians. This work presents a novel approach based on artificial vision that utilizes multi-view information. The proposed algorithm operates on images captured from an InVision system. The full pipeline includes image acquisition, frame segmentation to isolate the eyeframe from background, depth estimation to obtain 3D spatial information, and multi-view processing that integrates segmented RGB images with depth data for precise frame contour measurement. To this end, different configurations and variants are proposed and analyzed on real data, providing competitive measurements from still color images with respect to other solutions, while eliminating the need for specialized tracing equipment and reducing workflow complexity for optical technicians.

</details>


### [20] [A Self-Supervised Approach for Enhanced Feature Representations in Object Detection Tasks](https://arxiv.org/abs/2602.16322)
*Santiago C. Vilabella,Pablo Pérez-Núñez,Beatriz Remeseiro*

Main category: cs.CV

TL;DR: 该研究通过使用自我监督学习策略，训练一种基于未标记数据的模型，以减轻深度学习模型在缺乏大量标注数据时面临的挑战，并展示出强大的特征表示能力。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能模型复杂性的增长，获取足够的标注数据成为了训练深度学习模型的一个主要障碍。尤其是对于复杂的任务如目标检测，数据标注需要大量的时间与资源。因此，研究旨在通过增强特征提取器来减少这一问题，使其能够用较少的标注数据学习出更有效的特征表示。

Method: 研究采用自我监督学习方法，在未标注的数据集上训练模型，并比较其与基于ImageNet预训练的先进特征提取器的性能。

Result: 研究发现该模型在未标注数据上的表现优于ImageNet预训练的特征提取器，尤其是在目标检测任务中，展示了更好的特征表示和更高的可靠性和鲁棒性。

Conclusion: 该工作证明了增强特征提取器的有效性，可以显著减少对大量标注数据的依赖，从而促进了模型在资源有限的情况下更好地进行学习和应用。

Abstract: In the fast-evolving field of artificial intelligence, where models are increasingly growing in complexity and size, the availability of labeled data for training deep learning models has become a significant challenge. Addressing complex problems like object detection demands considerable time and resources for data labeling to achieve meaningful results. For companies developing such applications, this entails extensive investment in highly skilled personnel or costly outsourcing. This research work aims to demonstrate that enhancing feature extractors can substantially alleviate this challenge, enabling models to learn more effective representations with less labeled data. Utilizing a self-supervised learning strategy, we present a model trained on unlabeled data that outperforms state-of-the-art feature extractors pre-trained on ImageNet and particularly designed for object detection tasks. Moreover, the results demonstrate that our approach encourages the model to focus on the most relevant aspects of an object, thus achieving better feature representations and, therefore, reinforcing its reliability and robustness.

</details>


### [21] [SCAR: Satellite Imagery-Based Calibration for Aerial Recordings](https://arxiv.org/abs/2602.16349)
*Henry Hölzemann,Michael Schleiss*

Main category: cs.CV

TL;DR: SCAR是一种利用地理参考卫星图像进行长期内置和外置参数自动校正的方法，适用于航空视觉惯性系统。与依赖专用校准操作或手动测量地面控制点的方法不同，SCAR无需手动干预即可长期确保高精度的校准。


<details>
  <summary>Details</summary>
Motivation: 当前的校准方法要么需要专门的操作，要么依赖于手动测量的地面控制点。这些方法在实际应用中可能不够灵活且耗时。因此，本文提出了SCAR方法，旨在通过使用地理参考卫星图像辅助校准，提供一种简单、高效且无需手动干预的解决方案。

Method: SCAR方法通过将航空图像与从公开可用正射影像和高程模型中提取的2D-3D对应关系对齐来估算内参和外参。这种方法能够检测并校正实际部署条件下由于长期使用导致的校准性能下降。研究团队评估了该方法在跨越两年六个大规模航空飞行任务中的表现，这些任务覆盖了不同的季节和环境条件。

Result: 研究表明，SCAR方法在所有序列中都优于现有的基线方法（如Kalibr、COLMAP、VINS-Mono），在减少中位再投影误差方面取得了显著的进步，并将这种校准增益转化为视觉定位旋转误差降低和姿态精度提高。这些结果证明了SCAR能够在长期空中操作中提供准确、稳健且可重复的校准。

Conclusion: 总之，SCAR方法提供了一种无需手动干预即可保证高精度校准的方法，这对于长期航空视觉惯性系统操作是极具价值的。这种方法能够处理实际操作中的各种挑战，为该领域的研究与实践带来了新的可能性。

Abstract: We introduce SCAR, a method for long-term auto-calibration refinement of aerial visual-inertial systems that exploits georeferenced satellite imagery as a persistent global reference. SCAR estimates both intrinsic and extrinsic parameters by aligning aerial images with 2D--3D correspondences derived from publicly available orthophotos and elevation models. In contrast to existing approaches that rely on dedicated calibration maneuvers or manually surveyed ground control points, our method leverages external geospatial data to detect and correct calibration degradation under field deployment conditions. We evaluate our approach on six large-scale aerial campaigns conducted over two years under diverse seasonal and environmental conditions. Across all sequences, SCAR consistently outperforms established baselines (Kalibr, COLMAP, VINS-Mono), reducing median reprojection error by a large margin, and translating these calibration gains into substantially lower visual localization rotation errors and higher pose accuracy. These results demonstrate that SCAR provides accurate, robust, and reproducible calibration over long-term aerial operations without the need for manual intervention.

</details>


### [22] [Parameter-Free Adaptive Multi-Scale Channel-Spatial Attention Aggregation framework for 3D Indoor Semantic Scene Completion Toward Assisting Visually Impaired](https://arxiv.org/abs/2602.16385)
*Qi He,XiangXiang Wang,Jingtao Zhang,Yongbin Yu,Hongxiang Chu,Manping Fan,JingYe Cai,Zhenglin Yang*

Main category: cs.CV

TL;DR: AMAA框架在MonoScene框架基础上，通过空间注意力聚合和多尺度自适应特征门控策略提升单目3D语义场景补全的质量，保证了锚固的语义和结构一致性。


<details>
  <summary>Details</summary>
Motivation: 现有单目3D语义场景补全方法缺乏对体素特征可靠性的显式建模，影响了结构稳定性。

Method: AMAA框架引入了空间注意力聚合和多尺度自适应特征门控策略，优化了特征在语义和空间维度上的校准，提升了场景补全的准确性。

Result: 实验结果表明，与MonoScene相比，AMAA在NYUv2基准数据集上显著提升了语义场景补全精度，实现了更高的mIoU和SC IoU。

Conclusion: AMAA框架为室内辅助感知系统提供了可靠的单目3D语义场景补全解决方案。

Abstract: In indoor assistive perception for visually impaired users, 3D Semantic Scene Completion (SSC) is expected to provide structurally coherent and semantically consistent occupancy under strictly monocular vision for safety-critical scene understanding. However, existing monocular SSC approaches often lack explicit modeling of voxel-feature reliability and regulated cross-scale information propagation during 2D-3D projection and multi-scale fusion, making them vulnerable to projection diffusion and feature entanglement and thus limiting structural stability.To address these challenges, this paper presents an Adaptive Multi-scale Attention Aggregation (AMAA) framework built upon the MonoScene pipeline. Rather than introducing a heavier backbone, AMAA focuses on reliability-oriented feature regulation within a monocular SSC framework. Specifically, lifted voxel features are jointly calibrated in semantic and spatial dimensions through parallel channel-spatial attention aggregation, while multi-scale encoder-decoder fusion is stabilized via a hierarchical adaptive feature-gating strategy that regulates information injection across scales.Experiments on the NYUv2 benchmark demonstrate consistent improvements over MonoScene without significantly increasing system complexity: AMAA achieves 27.25% SSC mIoU (+0.31) and 43.10% SC IoU (+0.59). In addition, system-level deployment on an NVIDIA Jetson platform verifies that the complete AMAA framework can be executed stably on embedded hardware. Overall, AMAA improves monocular SSC quality and provides a reliable and deployable perception framework for indoor assistive systems targeting visually impaired users.

</details>


### [23] [Visual Self-Refine: A Pixel-Guided Paradigm for Accurate Chart Parsing](https://arxiv.org/abs/2602.16455)
*Jinsong Li,Xiaoyi Dong,Yuhang Zang,Yuhang Cao,Jiaqi Wang,Dahua Lin*

Main category: cs.CV

TL;DR: 该研究提出了Visual Self-Refine (VSR) 方法用于提高视觉感知能力，特别是在图表解析任务中。通过模拟人类使用手指作为视觉锚点的习惯，VSR 使模型能够生成像素级定位输出，进行自我可视化并纠正潜在的视觉错误。研究者还提出了一个新的图表解析基准 ChartP-Bench。


<details>
  <summary>Details</summary>
Motivation: 现有大型视觉-语言模型(LVLMs)虽在文本推理和自我纠正方面表现出色，但在视觉感知任务，如图表解析中效果有限。因此，研究需要一种方法来提高模型在这些任务中的表现。

Method: 研究提出了一种名为Visual Self-Refine (VSR)的方法。在ChartVSR的具体实现中，该模型通过两个阶段来逐点验证像素级定位：Refine Stage 和 Decode Stage。还构建了一个新的基准 ChartP-Bench 用于评估图表解析能力。

Result: VSR 显著提高了模型在图表解析任务中的准确性。研究结果表明，通过使用Visual Self-Refine，模型能够减少数据遗漏、对齐错误和幻想，从而在复杂的图表中提供更可靠的解析结果。

Conclusion: 研究提出了一种有效的增强视觉感知能力的新方法，即Visual Self-Refine (VSR)，并将其应用于图表解析任务。研究还强调了VSR作为一种通用的视觉反馈机制的潜力，未来有望进一步提升视觉任务中的准确性。

Abstract: While Large Vision-Language Models (LVLMs) have demonstrated remarkable capabilities for reasoning and self-correction at the textual level, these strengths provide minimal benefits for complex tasks centered on visual perception, such as Chart Parsing. Existing models often struggle with visually dense charts, leading to errors like data omission, misalignment, and hallucination. Inspired by the human strategy of using a finger as a ``visual anchor'' to ensure accuracy when reading complex charts, we propose a new paradigm named Visual Self-Refine (VSR). The core idea of VSR is to enable a model to generate pixel-level localization outputs, visualize them, and then feed these visualizations back to itself, allowing it to intuitively inspect and correct its own potential visual perception errors. We instantiate the VSR paradigm in the domain of Chart Parsing by proposing ChartVSR. This model decomposes the parsing process into two stages: a Refine Stage, where it iteratively uses visual feedback to ensure the accuracy of all data points' Pixel-level Localizations, and a Decode Stage, where it uses these verified localizations as precise visual anchors to parse the final structured data. To address the limitations of existing benchmarks, we also construct ChartP-Bench, a new and highly challenging benchmark for chart parsing. Our work also highlights VSR as a general-purpose visual feedback mechanism, offering a promising new direction for enhancing accuracy on a wide range of vision-centric tasks.

</details>


### [24] [Benchmarking Adversarial Robustness and Adversarial Training Strategies for Object Detection](https://arxiv.org/abs/2602.16494)
*Alexis Winter,Jean-Vincent Martini,Romaric Audigier,Angelique Loesch,Bertrand Luvison*

Main category: cs.CV

TL;DR: 该研究提出了一种统一的基准框架，用于评估对目标检测模型的攻击，发现现代对抗性攻击在转移到基于Transformer的架构时效果显著下降，并且最有效的对抗性训练策略是使用高扰动攻击混合的数据集进行训练。


<details>
  <summary>Details</summary>
Motivation: 由于目标检测模型在自动系统中的重要性以及对抗性攻击对其的威胁，作者旨在创建一个公平的基准来比较不同攻击和防御方法，同时揭示现代攻击在变压器架构下的局限性以及最佳的对抗性训练策略。

Method: 研究通过提出一个针对数字、非补丁攻击的统一基准框架来解决标准化评估的难题。该框架包括特定的评估指标以区分定位和分类错误，并运用多种感知度量来评估攻击成本。研究进行了广泛的实验，涵盖了最新的攻击方法和各种检测器。

Result: 研究结果显示：现代对抗性攻击对基于Transformer的架构缺乏转移性；最有效的对抗性训练策略是使用组合了不同目标（如空间和语义）的高扰动攻击数据集进行训练，这种方法的防御效果优于仅使用单一攻击的数据集训练。

Conclusion: 该研究认为，应使用具有高扰动性攻击的数据集来进行对抗性训练，以提高模型的鲁棒性。此外，应在不同架构间比较攻击和防御效果，以促进更公正的评估标准的发展。

Abstract: Object detection models are critical components of automated systems, such as autonomous vehicles and perception-based robots, but their sensitivity to adversarial attacks poses a serious security risk. Progress in defending these models lags behind classification, hindered by a lack of standardized evaluation. It is nearly impossible to thoroughly compare attack or defense methods, as existing work uses different datasets, inconsistent efficiency metrics, and varied measures of perturbation cost. This paper addresses this gap by investigating three key questions: (1) How can we create a fair benchmark to impartially compare attacks? (2) How well do modern attacks transfer across different architectures, especially from Convolutional Neural Networks to Vision Transformers? (3) What is the most effective adversarial training strategy for robust defense? To answer these, we first propose a unified benchmark framework focused on digital, non-patch-based attacks. This framework introduces specific metrics to disentangle localization and classification errors and evaluates attack cost using multiple perceptual metrics. Using this benchmark, we conduct extensive experiments on state-of-the-art attacks and a wide range of detectors. Our findings reveal two major conclusions: first, modern adversarial attacks against object detection models show a significant lack of transferability to transformer-based architectures. Second, we demonstrate that the most robust adversarial training strategy leverages a dataset composed of a mix of high-perturbation attacks with different objectives (e.g., spatial and semantic), which outperforms training on any single attack.

</details>


### [25] [DressWild: Feed-Forward Pose-Agnostic Garment Sewing Pattern Generation from In-the-Wild Images](https://arxiv.org/abs/2602.16502)
*Zeng Tao,Ying Jiang,Yunuo Chen,Tianyi Xie,Huamin Wang,Yingnian Wu,Yin Yang,Abishek Sampath Kumar,Kenji Tashiro,Chenfanfu Jiang*

Main category: cs.CV

TL;DR: 该论文提出了一种名为DressWild的新方法，能够从单张野外拍摄图像中重建符合物理规律的2D剪裁图案和3D服装，无需多视角输入或迭代优化，适用于逼真服装模拟和动画。


<details>
  <summary>Details</summary>
Motivation: 现有的服装图案生成方法在处理多样化姿态和视角时存在困难，而基于优化的方法计算成本高且难以扩展。因此，该研究旨在开发一种高效的前馈管道，能够直接从单张野外拍摄图像中生成可编辑、可分离且适用于物理模拟的服装图案。

Method: DressWild方法采用视图-语言模型（VLMs）在图像级别进行姿态变异归一化，提取姿势感知且具有3D信息的服装特征，通过基于变换器的编码器融合特征以预测剪裁图案参数，进而应用于物理模拟、纹理合成和多层虚拟试穿。

Result: 实验结果表明，DressWild方法能在没有多视角输入或迭代优化的情况下从野外拍摄图像中稳健地恢复多样化的剪裁图案和对应的3D服装，提供了一种高效可扩展的解决方案用于现实的服装模拟和动画。

Conclusion: DressWild方法为现实的服装模拟和动画提供了一种前沿的前馈管道，无需多视角输入或迭代优化过程。

Abstract: Recent advances in garment pattern generation have shown promising progress. However, existing feed-forward methods struggle with diverse poses and viewpoints, while optimization-based approaches are computationally expensive and difficult to scale. This paper focuses on sewing pattern generation for garment modeling and fabrication applications that demand editable, separable, and simulation-ready garments. We propose DressWild, a novel feed-forward pipeline that reconstructs physics-consistent 2D sewing patterns and the corresponding 3D garments from a single in-the-wild image. Given an input image, our method leverages vision-language models (VLMs) to normalize pose variations at the image level, then extract pose-aware, 3D-informed garment features. These features are fused through a transformer-based encoder and subsequently used to predict sewing pattern parameters, which can be directly applied to physical simulation, texture synthesis, and multi-layer virtual try-on. Extensive experiments demonstrate that our approach robustly recovers diverse sewing patterns and the corresponding 3D garments from in-the-wild images without requiring multi-view inputs or iterative optimization, offering an efficient and scalable solution for realistic garment simulation and animation.

</details>


### [26] [Let's Split Up: Zero-Shot Classifier Edits for Fine-Grained Video Understanding](https://arxiv.org/abs/2602.16545)
*Kaiting Liu,Hazel Doughty*

Main category: cs.CV

TL;DR: 研究人员提出了一种新的任务称之为类别拆分，旨在细化视频分类中的粗略类别而不牺牲其它类别的准确性。他们设计了零样本编辑方法，并结合少量样本微调，实现显著超越现有视觉-语言基准的效果。


<details>
  <summary>Details</summary>
Motivation: 现有的视频识别模型通常依赖于固定的、过于粗略的分类体系，这种模型不能有效处理新兴的区分，并且为更新分类体系还需要大量的数据标注和重训工作，这对资源是一种浪费。

Method: 提出了一种零样本编辑方法，通过利用视频分类器的潜在组合结构来展示细微差异，无需额外数据。此外，通过少量样本微调来提升模型效果，这种方法既简单又高效。

Result: 在新的视频类别拆分基准测试上，该方法显著优于现有的视觉-语言基准，在细分的类别上提高了准确率，同时未牺牲其它类别的性能。

Conclusion: 该研究通过引入类别拆分任务和创新的零样本编辑方法，提供了一种新的视角来处理视频识别中的细化分类问题，显示出了其在实际应用中的潜在价值。

Abstract: Video recognition models are typically trained on fixed taxonomies which are often too coarse, collapsing distinctions in object, manner or outcome under a single label. As tasks and definitions evolve, such models cannot accommodate emerging distinctions and collecting new annotations and retraining to accommodate such changes is costly. To address these challenges, we introduce category splitting, a new task where an existing classifier is edited to refine a coarse category into finer subcategories, while preserving accuracy elsewhere. We propose a zero-shot editing method that leverages the latent compositional structure of video classifiers to expose fine-grained distinctions without additional data. We further show that low-shot fine-tuning, while simple, is highly effective and benefits from our zero-shot initialization. Experiments on our new video benchmarks for category splitting demonstrate that our method substantially outperforms vision-language baselines, improving accuracy on the newly split categories without sacrificing performance on the rest. Project page: https://kaitingliu.github.io/Category-Splitting/.

</details>


### [27] [Arc2Morph: Identity-Preserving Facial Morphing with Arc2Face](https://arxiv.org/abs/2602.16569)
*Nicolò Di Domenico,Annalisa Franco,Matteo Ferrara,Davide Maltoni*

Main category: cs.CV

TL;DR: 本文提出了一种基于Arc2Face的面部变形式攻击方法，该方法能够从紧凑的身份表示中合成高度逼真的面部图像。实验结果表明，该方法在面部变形式攻击潜力方面与基于特征点的方法相当。


<details>
  <summary>Details</summary>
Motivation: 由于面部图像经常在无监督的活体捕捉过程中获得，因此许多国家在护照注册过程中存在关键的安全漏洞。为了填补这一安全漏洞，论文提出了一种新的基于Arc2Face的身份条件面部基础模型的方法，以提高对抗面部识别系统的攻击。

Method: 该方法利用Arc2Face模型，可以从紧凑的身份特征中生成逼真的面部图像。通过将变形式攻击潜力在两个大规模的面部变形式攻击检测数据集上与现有的多种最先进的变形式方法进行比较，验证了该方法的有效性。

Result: 实验结果表明，基于深度学习的方法在变形式攻击潜力方面达到了与基于特征点技术相当的水平。这意味着该方法在保护身份信息方面表现出色。

Conclusion: 本文的研究结果证明了其提出的方法能够有效保护和管理身份信息，在生成面部变换过程中保持人体特征的完整性。这将有助于提高电子身份验证系统的安全性。

Abstract: Face morphing attacks are widely recognized as one of the most challenging threats to face recognition systems used in electronic identity documents. These attacks exploit a critical vulnerability in passport enrollment procedures adopted by many countries, where the facial image is often acquired without a supervised live capture process. In this paper, we propose a novel face morphing technique based on Arc2Face, an identity-conditioned face foundation model capable of synthesizing photorealistic facial images from compact identity representations. We demonstrate the effectiveness of the proposed approach by comparing the morphing attack potential metric on two large-scale sequestered face morphing attack detection datasets against several state-of-the-art morphing methods, as well as on two novel morphed face datasets derived from FEI and ONOT. Experimental results show that the proposed deep learning-based approach achieves a morphing attack potential comparable to that of landmark-based techniques, which have traditionally been regarded as the most challenging. These findings confirm the ability of the proposed method to effectively preserve and manage identity information during the morph generation process.

</details>


### [28] [A Contrastive Learning Framework Empowered by Attention-based Feature Adaptation for Street-View Image Classification](https://arxiv.org/abs/2602.16590)
*Qi You,Yitai Cheng,Zichao Zeng,James Haworth*

Main category: cs.CV

TL;DR: CLIP-MHAdapter 是一种轻量级的 CLIP 腐outine 方法，通过附加一个基于多头自注意力机制的瓶颈MLP，增强了局部细粒度特征的建模能力，从而提高了复杂街景中的属性分类准确性。


<details>
  <summary>Details</summary>
Motivation: 尽管预训练的Vision-Language模型如CLIP提供了丰富的图像表示，但现有的适应或微调方法往往依赖于其全局图像嵌入，这限制了它们捕捉复杂杂乱街景中细微局部属性的能力。因此，为了克服该问题，CLIP-MHAdapter 提出了基于多头自注意力机制的轻量级 CLIP 腐outine 方案。

Method: CLIP-MHAdapter 通过在原有的CLIP架构中添加一个具有多头自注意力机制的瓶颈MLP，该模块能够处理 patch tokens 并捕捉它们之间的依赖关系。

Result: 在 Global StreetScapes 数据集上，CLIP-MHAdapter 在八个属性分类任务中表现出色或具有竞争力的准确性，并在多个基准上达到了新的SOTA结果，同时保持了较低的计算成本。

Conclusion: CLIP-MHAdapter 提出了一个有效的方案来解决街景图像属性分类问题，通过利用多头自注意力机制增强了本地细节建模能力并在多个基准上达成了SOTA结果。

Abstract: Street-view image attribute classification is a vital downstream task of image classification, enabling applications such as autonomous driving, urban analytics, and high-definition map construction. It remains computationally demanding whether training from scratch, initialising from pre-trained weights, or fine-tuning large models. Although pre-trained vision-language models such as CLIP offer rich image representations, existing adaptation or fine-tuning methods often rely on their global image embeddings, limiting their ability to capture fine-grained, localised attributes essential in complex, cluttered street scenes. To address this, we propose CLIP-MHAdapter, a variant of the current lightweight CLIP adaptation paradigm that appends a bottleneck MLP equipped with multi-head self-attention operating on patch tokens to model inter-patch dependencies. With approximately 1.4 million trainable parameters, CLIP-MHAdapter achieves superior or competitive accuracy across eight attribute classification tasks on the Global StreetScapes dataset, attaining new state-of-the-art results while maintaining low computational cost. The code is available at https://github.com/SpaceTimeLab/CLIP-MHAdapter.

</details>


### [29] [Unpaired Image-to-Image Translation via a Self-Supervised Semantic Bridge](https://arxiv.org/abs/2602.16664)
*Jiaming Liu,Felix Petersen,Yunhe Gao,Yabin Zhang,Hyojin Kim,Akshay S. Chaudhari,Yu Sun,Stefano Ermon,Sergios Gatidis*

Main category: cs.CV

TL;DR: 本文提出了一种名为SSB的自监督语义桥梁框架，该框架将外部语义先验信息整合到扩散桥梁模型中，实现无域交叉监督的空间忠实图像翻译。实验表明，SSB在医学图像合成任务上优于现有的先验方法，并能轻松扩展到高质量文本引导编辑。


<details>
  <summary>Details</summary>
Motivation: 现有的图像到图像翻译方法存在局限性：对抗方法需要目标域的对抗损失，可能会限制对未见数据的泛化能力；而扩散反向方法则常常产生低保真度的生成结果。因此，需要一种新的方法来克服这些局限性。

Method: SSB方法采用自监督视觉编码器学习不变于外观变化但捕捉几何结构的表示，构建一个共享的潜在空间并指导扩散桥梁。该框架能够将外部语义知识融入模型中，实现无领域监督的空间忠实图像翻译。

Result: SSB在多项实验中表现出色，特别是应用于医学图像合成任务，能够很好地在领域内和领域外任务上实现高保真的图像翻译，并且能够很容易地应用于高质量的文本引导编辑。

Conclusion: 本文提出的SSB方法提供了一种有效的解决方案来克服现有图像到图像翻译方法中的局限，展示了在多种任务上的广泛应用潜力。

Abstract: Adversarial diffusion and diffusion-inversion methods have advanced unpaired image-to-image translation, but each faces key limitations. Adversarial approaches require target-domain adversarial loss during training, which can limit generalization to unseen data, while diffusion-inversion methods often produce low-fidelity translations due to imperfect inversion into noise-latent representations. In this work, we propose the Self-Supervised Semantic Bridge (SSB), a versatile framework that integrates external semantic priors into diffusion bridge models to enable spatially faithful translation without cross-domain supervision. Our key idea is to leverage self-supervised visual encoders to learn representations that are invariant to appearance changes but capture geometric structure, forming a shared latent space that conditions the diffusion bridges. Extensive experiments show that SSB outperforms strong prior methods for challenging medical image synthesis in both in-domain and out-of-domain settings, and extends easily to high-quality text-guided editing.

</details>


### [30] [PredMapNet: Future and Historical Reasoning for Consistent Online HD Vectorized Map Construction](https://arxiv.org/abs/2602.16669)
*Bo Lang,Nirav Savaliya,Zhihao Zheng,Jinglun Feng,Zheng-Hang Yeh,Mooi Choo Chuah*

Main category: cs.CV

TL;DR: 本文提出了一种端到端的框架，用于构建一致的在线高精度矢量地图，通过时空上下文改进了地图实例跟踪和短期预测，从而提高了地图构建的准确性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有基于查询的方法在地图构建过程中存在时空不一致性和不稳定性，本文旨在克服这些问题，提出一种新框架来改善这些缺陷。

Method: 通过引入语义感知查询生成器、历史栅格化地图记忆和历史图指导模块，使地图实例跟踪和短期预测得以联合执行。

Result: 在nuScenes和Argoverse2数据集上的实验表明，本文的方法在效率上优于现有最先进的方法。

Conclusion: 该方法在高精度矢量地图构建中具有良好的稳定性和准确度。

Abstract: High-definition (HD) maps are crucial to autonomous driving, providing structured representations of road elements to support navigation and planning. However, existing query-based methods often employ random query initialization and depend on implicit temporal modeling, which lead to temporal inconsistencies and instabilities during the construction of a global map. To overcome these challenges, we introduce a novel end-to-end framework for consistent online HD vectorized map construction, which jointly performs map instance tracking and short-term prediction. First, we propose a Semantic-Aware Query Generator that initializes queries with spatially aligned semantic masks to capture scene-level context globally. Next, we design a History Rasterized Map Memory to store fine-grained instance-level maps for each tracked instance, enabling explicit historical priors. A History-Map Guidance Module then integrates rasterized map information into track queries, improving temporal continuity. Finally, we propose a Short-Term Future Guidance module to forecast the immediate motion of map instances based on the stored history trajectories. These predicted future locations serve as hints for tracked instances to further avoid implausible predictions and keep temporal consistency. Extensive experiments on the nuScenes and Argoverse2 datasets demonstrate that our proposed method outperforms state-of-the-art (SOTA) methods with good efficiency.

</details>


### [31] [VETime: Vision Enhanced Zero-Shot Time Series Anomaly Detection](https://arxiv.org/abs/2602.16681)
*Yingyuan Yang,Tian Lan,Yifei Gao,Yimeng Lu,Wenjun He,Meng Wang,Chenghao Liu,Chen Zhang*

Main category: cs.CV

TL;DR: VETime 是一种结合时间和视觉模态的方法，通过细粒度的视觉-时间对齐和动态融合解决了点异常和上下文异常检测中的时间粒度和全局视图的权衡。


<details>
  <summary>Details</summary>
Motivation: 现有的时间序列异常检测方法在精度和效率之间存在一个根本的权衡，VETime 被提出以平衡这些模态的特征，提供更优的表现。

Method: VETime 使用可逆图像转换和补丁级时间对齐模块建立共享的时间视觉时间线，保持具有区分性的细节，同时保持时间敏感性。进一步设计了异常窗口对比学习机制和任务自适应多模态融合，以适应性地整合两个模态的互补感知强度。

Result: 广泛的实验表明，VETime 在零样本场景中显著优于最先进的模型，实现了更高的局部精度，且计算开销低于当前基于视觉的方法。

Conclusion: VETime 提供了一种有效的TSAD框架，通过结合时间和视觉模态解决了时间序列中的关键挑战。

Abstract: Time-series anomaly detection (TSAD) requires identifying both immediate Point Anomalies and long-range Context Anomalies. However, existing foundation models face a fundamental trade-off: 1D temporal models provide fine-grained pointwise localization but lack a global contextual perspective, while 2D vision-based models capture global patterns but suffer from information bottlenecks due to a lack of temporal alignment and coarse-grained pointwise detection. To resolve this dilemma, we propose VETime, the first TSAD framework that unifies temporal and visual modalities through fine-grained visual-temporal alignment and dynamic fusion. VETime introduces a Reversible Image Conversion and a Patch-Level Temporal Alignment module to establish a shared visual-temporal timeline, preserving discriminative details while maintaining temporal sensitivity. Furthermore, we design an Anomaly Window Contrastive Learning mechanism and a Task-Adaptive Multi-Modal Fusion to adaptively integrate the complementary perceptual strengths of both modalities. Extensive experiments demonstrate that VETime significantly outperforms state-of-the-art models in zero-shot scenarios, achieving superior localization precision with lower computational overhead than current vision-based approaches. Code available at: https://github.com/yyyangcoder/VETime.

</details>


### [32] [Learning Situated Awareness in the Real World](https://arxiv.org/abs/2602.16682)
*Chuhan Li,Ruilin Han,Joy Hsu,Yongyuan Liang,Rajiv Dhawan,Jiajun Wu,Ming-Hsuan Yang,Xin Eric Wang*

Main category: cs.CV

TL;DR: SAW-Bench 提供了一种新的基准，用于评估基于视角的现实世界中的意识能力，特别是在利用空间几何线索进行推理方面存在不足。


<details>
  <summary>Details</summary>
Motivation: SAW-Bench 致力于填补现有基于多模态模型基准在评估基于视角的现实环境理解方面存在的不足，尤其是关于个体如何根据自身视角、姿势和动作来理解周围环境的能力。

Method: SAW-Bench 包括786段真实录制的视频，覆盖了多种室内外环境，并标注了超过2,071个问答对，用于评估模型的空间意识。

Result: SAW-Bench 揭示了人类和最佳多模态模型之间存在37.66%的性能差距，特别是模型在利用视角视频中的几何线索进行推断时表现出不足，导致空间推理错误。

Conclusion: SAW-Bench 作为基于空间智能的基准，强调从被动观察迈向理解实际物理环境中的观众为中心的动力。

Abstract: A core aspect of human perception is situated awareness, the ability to relate ourselves to the surrounding physical environment and reason over possible actions in context. However, most existing benchmarks for multimodal foundation models (MFMs) emphasize environment-centric spatial relations (relations among objects in a scene), while largely overlooking observer-centric relationships that require reasoning relative to agent's viewpoint, pose, and motion. To bridge this gap, we introduce SAW-Bench (Situated Awareness in the Real World), a novel benchmark for evaluating egocentric situated awareness using real-world videos. SAW-Bench comprises 786 self-recorded videos captured with Ray-Ban Meta (Gen 2) smart glasses spanning diverse indoor and outdoor environments, and over 2,071 human-annotated question-answer pairs. It probes a model's observer-centric understanding with six different awareness tasks. Our comprehensive evaluation reveals a human-model performance gap of 37.66%, even with the best-performing MFM, Gemini 3 Flash. Beyond this gap, our in-depth analysis uncovers several notable findings; for example, while models can exploit partial geometric cues in egocentric videos, they often fail to infer a coherent camera geometry, leading to systematic spatial reasoning errors. We position SAW-Bench as a benchmark for situated spatial intelligence, moving beyond passive observation to understanding physically grounded, observer-centric dynamics.

</details>


### [33] [Are Object-Centric Representations Better At Compositional Generalization?](https://arxiv.org/abs/2602.16689)
*Ferdinand Kapl,Amir Mohammad Karimi Mamaghan,Maximilian Seitzer,Karl Henrik Johansson,Carsten Marr,Stefan Bauer,Andrea Dittadi*

Main category: cs.CV

TL;DR: 本文引入了一个视觉问答基准，通过测量不同视觉编码器在无对象中心倾向和有对象中心倾向下的泛化能力，研究了对象中心表示在复杂视觉世界中的组成性泛化的表现。结果表明，对象中心的方法在更难的场景下表现更好，原始密集表示仅在较简单的场景中优于对象中心方法，但在资源有限的情况下，对象中心模型表现更好。


<details>
  <summary>Details</summary>
Motivation: 研究对象中心表示在复杂视觉世界中的组成性泛化能力，以评估它们与密集表示方法的相对优劣。

Method: 作者在三个受控的视觉世界（CLEVRTex，Super-CLEVR和MOVi-C）中设计了一个视觉问答基准。使用DINOv2和SigLIP2等基础视觉编码器，同时对比了有对象中心倾向和无对象中心倾向的模型的性能。

Result: 研究结果表明，对象中心的方法在更难的组成性泛化任务中表现更佳。对于较简单的任务，原始密集表示稍微占优；而在资源受限的情况下，对象中心模型表现更佳。

Conclusion: 总体而言，当数据集大小、训练数据多样性和后端计算能力其中之一受到限制时，对象中心表示在组成性泛化方面表现出更强的能力。

Abstract: Compositional generalization, the ability to reason about novel combinations of familiar concepts, is fundamental to human cognition and a critical challenge for machine learning. Object-centric (OC) representations, which encode a scene as a set of objects, are often argued to support such generalization, but systematic evidence in visually rich settings is limited. We introduce a Visual Question Answering benchmark across three controlled visual worlds (CLEVRTex, Super-CLEVR, and MOVi-C) to measure how well vision encoders, with and without object-centric biases, generalize to unseen combinations of object properties. To ensure a fair and comprehensive comparison, we carefully account for training data diversity, sample size, representation size, downstream model capacity, and compute. We use DINOv2 and SigLIP2, two widely used vision encoders, as the foundation models and their OC counterparts. Our key findings reveal that (1) OC approaches are superior in harder compositional generalization settings; (2) original dense representations surpass OC only on easier settings and typically require substantially more downstream compute; and (3) OC models are more sample efficient, achieving stronger generalization with fewer images, whereas dense encoders catch up or surpass them only with sufficient data and diversity. Overall, object-centric representations offer stronger compositional generalization when any one of dataset size, training data diversity, or downstream compute is constrained.

</details>


### [34] [Saliency-Aware Multi-Route Thinking: Revisiting Vision-Language Reasoning](https://arxiv.org/abs/2602.16702)
*Mingjia Shi,Yinhan He,Yaochen Zhu,Jundong Li*

Main category: cs.CV

TL;DR: 本文提出了一种称为SAP的选择方法，该方法能够在推理过程中稳定地控制离散生成，并且可以在需要重新接地时重新咨询视觉证据。此外，SAP支持多路径推理，允许对多种推理行为进行并行探索，而无需额外的训练，可以在减少对象错觉的同时提高推理的稳定性和减少响应延迟。


<details>
  <summary>Details</summary>
Motivation: 传统方法在视觉语言模型中实现类似大型语言模型的扩展时遇到困难，因为视觉和文本输入之间的不对称性以及推理过程中视觉对文本人物的逐渐主导地位。

Method: SAP方法聚焦于高层次的推理原则，而不是基于tokens的轨迹，这使推理在面对嘈杂反馈时能够稳定地进行，并且能够在需要时重新评估视觉证据。此外，它还能够在推理的不同阶段探索多种路径，提高了模型的灵活性。

Result: 实验证明，SAP在保持与逐token生成预算相当的表现下，能够有效减少对象错觉，提高推理的稳定性，并降低响应延迟，尤其在长序列推理方面优于传统的逐步推理方式。

Conclusion: SAP作为一种无额外训练、模型无关的多路径推理方法，在视觉语言任务中表现出竞争力，有望改善推理的稳定性和效率。

Abstract: Vision-language models (VLMs) aim to reason by jointly leveraging visual and textual modalities. While allocating additional inference-time computation has proven effective for large language models (LLMs), achieving similar scaling in VLMs remains challenging. A key obstacle is that visual inputs are typically provided only once at the start of generation, while textual reasoning (e.g., early visual summaries) is generated autoregressively, causing reasoning to become increasingly text-dominated and allowing early visual grounding errors to accumulate. Moreover, vanilla guidance for visual grounding during inference is often coarse and noisy, making it difficult to steer reasoning over long texts. To address these challenges, we propose \emph{Saliency-Aware Principle} (SAP) selection. SAP operates on high-level reasoning principles rather than token-level trajectories, which enable stable control over discrete generation under noisy feedback while allowing later reasoning steps to re-consult visual evidence when renewed grounding is required. In addition, SAP supports multi-route inference, enabling parallel exploration of diverse reasoning behaviors. SAP is model-agnostic and data-free, requiring no additional training. Empirical results show that SAP achieves competitive performance, especially in reducing object hallucination, under comparable token-generation budgets while yielding more stable reasoning and lower response latency than CoT-style long sequential reasoning.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [35] [Language Model Representations for Efficient Few-Shot Tabular Classification](https://arxiv.org/abs/2602.15844)
*Inwon Kang,Parikshit Ram,Yi Zhou,Horst Samulowitz,Oshani Seneviratne*

Main category: cs.CL

TL;DR: 本文提出了一种针对Web表结构分类的轻量级框架TaRL，通过使用语义嵌入并进行适当的调整，该框架在低数据量场景中能够与最先进的模型竞争。


<details>
  <summary>Details</summary>
Motivation: 面对Web上多样化的表格结构和语义，研究如何利用已部署的大语言模型进行表格分类，减少对专门模型的依赖和重新训练的需求。

Method: TaRL框架结合了语义嵌入和两个关键技巧：移除所有嵌入的共同部分和调整softmax温度，通过元学习器学习合适的温度，以便在少量示例的情况下实现良好的分类性能。

Result: TaRL框架在少量数据条件下（k≤32）展示了与最先进的模型相当的性能，证明了利用现有的大语言模型基础设施进行高效的语义驱动的Web表理解是可行的。

Conclusion: 研究结果强调了利用大语言模型进行Web表格理解的潜力，并提出了一个实用的框架以适应不同的表格数据环境。

Abstract: The Web is a rich source of structured data in the form of tables, from product catalogs and knowledge bases to scientific datasets. However, the heterogeneity of the structure and semantics of these tables makes it challenging to build a unified method that can effectively leverage the information they contain. Meanwhile, Large language models (LLMs) are becoming an increasingly integral component of web infrastructure for tasks like semantic search. This raises a crucial question: can we leverage these already-deployed LLMs to classify structured data in web-native tables (e.g., product catalogs, knowledge base exports, scientific data portals), avoiding the need for specialized models or extensive retraining? This work investigates a lightweight paradigm, $\textbf{Ta}$ble $\textbf{R}$epresentation with $\textbf{L}$anguage Model~($\textbf{TaRL}$), for few-shot tabular classification that directly utilizes semantic embeddings of individual table rows. We first show that naive application of these embeddings underperforms compared to specialized tabular models. We then demonstrate that their potentials can be unlocked with two key techniques: removing the common component from all embeddings and calibrating the softmax temperature. We show that a simple meta-learner, trained on handcrafted features, can learn to predict an appropriate temperature. This approach achieves performance comparable to state-of-the-art models in low-data regimes ($k \leq 32$) of semantically-rich tables. Our findings demonstrate the viability of reusing existing LLM infrastructure for efficient semantics-driven pathway to reuse existing LLM infrastructure for Web table understanding.

</details>


### [36] [KD4MT: A Survey of Knowledge Distillation for Machine Translation](https://arxiv.org/abs/2602.15845)
*Ona de Gibert,Joseph Attieh,Timothee Mickus,Yves Scherrer,Jörg Tiedemann*

Main category: cs.CL

TL;DR: 本文综述了自2025年10月1日之前的相关文献，共涵盖105篇关于知识蒸馏在机器翻译中的应用论文（知识蒸馏4机器翻译，KD4MT）。文章对知识蒸馏的方法和应用进行了分类，并指出当前领域内存在的一些研究空白和统一评估实践缺乏的问题。此外，文章还提供了在具体环境中选择知识蒸馏方法的建议，以及提示了将知识蒸馏应用于机器翻译可能带来的风险。


<details>
  <summary>Details</summary>
Motivation: 随着自然语言处理中模型规模的不断变大，知识蒸馏作为一种压缩工具受到了越来越多的关注。对于机器翻译领域而言，知识蒸馏不仅有助于模型压缩，还能作为一种一般性的知识传递机制，影响监督、翻译质量和效率。为了更好地理解和推动该领域的发展，作者们进行了一次综述研究。

Method: 作者们首先通过介绍机器翻译和知识蒸馏的基本概念，为非专业人士提供了一个初步的认识。随后，作者们对适用于机器翻译的知识蒸馏方法进行了概述。之后，文章根据贡献方法和实际应用对知识蒸馏在机器翻译中的进展进行了分类。通过定量和定性的分析，作者们指出了该领域的共同趋势，并发现了研究缺口，特别是缺乏统一的评估实践。最后，文章还指出了将知识蒸馏应用于机器翻译可能带来的风险，并提供了实用的建议。

Result: 文章通过对105篇论文的总结，揭示了知识蒸馏在机器翻译领域的最新进展和发展趋势。揭示了目前研究中的未解决的问题，包括缺乏统一的评价方法。研究还提供了选择知识蒸馏方法的实际建议，并提出了在将知识蒸馏应用于机器翻译时需要注意的风险。

Conclusion: 综上所述，尽管知识蒸馏在机器翻译领域取得了显著进展，但仍有许多研究空白需要解决。本文为未来的研究提供了一个有价值的资源，并对未来的研究方向进行了讨论。

Abstract: Knowledge Distillation (KD) as a research area has gained a lot of traction in recent years as a compression tool to address challenges related to ever-larger models in NLP. Remarkably, Machine Translation (MT) offers a much more nuanced take on this narrative: in MT, KD also functions as a general-purpose knowledge transfer mechanism that shapes supervision and translation quality as well as efficiency.
  This survey synthesizes KD for MT (KD4MT) across 105 papers (through October 1, 2025). We begin by introducing both MT and KD for non-experts, followed by an overview of the standard KD approaches relevant to MT applications. Subsequently, we categorize advances in the KD4MT literature based on (i) their methodological contributions and (ii) their practical applications. Our qualitative and quantitative analyses identify common trends in the field and highlight key research gaps as well as the absence of unified evaluation practice for KD methods in MT. We further provide practical guidelines for selecting a KD method in concrete settings and highlight potential risks associated with the application of KD to MT such as increased hallucination and bias amplification. Finally, we discuss the role of LLMs in re-shaping the KD4MT field. To support further research, we complement our survey with a publicly available database summarizing the main characteristics of the surveyed KD methods and a glossary of key terms.

</details>


### [37] [Gated Tree Cross-attention for Checkpoint-Compatible Syntax Injection in Decoder-Only LLMs](https://arxiv.org/abs/2602.15846)
*Xinyu Gao,Shaonan Wang,Nai Ding*

Main category: cs.CL

TL;DR: 本文提出了一种与现有模型兼容的门控树交叉注意力（GTCA）分支，该分支在不改变骨干架构的情况下，读取预先计算的短语成分记忆。该设计使用标记更新掩码和分阶段训练来控制结构更新的范围和时机。实验表明，GTCA能够提高语法规则的鲁棒性，同时不损害多种选择问答或常识推理性能，为构建更语法规则鲁棒的解码器模型提供了一条实际路径。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在广泛任务上表现出色，但对细微的语法错误非常敏感，影响了其在下游推理中的可靠性。直接将显式的句法结构注入预训练模型可能会干扰其原有的能力。因此，本文提出了GTCA分支来增强解码器的语法规则鲁棒性。

Method: 本文设计了GTCA分支，该分支不需要修改骨干架构，能够在分阶段训练过程中逐步融合预先计算好的短语成分记忆。同时，通过引入标记更新掩码，该方法可以控制结构更新的范围。

Result: 在多个基准测试和Transformer模型中，与持续训练基线相比，GTCA增强了语法结构的鲁棒性，同时保持了多样选择问答和其他常识推理能力。

Conclusion: 本文提出了一种方法，通过引入兼容现有模型的GTCA分支，提高了解码器的语言模型在利用更复杂句法结构方面的表现，同时保持了原有的多种选择问答和常识推理能力，为构建更加稳健的解码器模型提供了实用的途径。

Abstract: Decoder-only large language models achieve strong broad performance but are brittle to minor grammatical perturbations, undermining reliability for downstream reasoning. However, directly injecting explicit syntactic structure into an existing checkpoint can interfere with its pretrained competence. We introduce a checkpoint-compatible gated tree cross-attention (GTCA) branch that reads precomputed constituency chunk memory while leaving backbone architecture unchanged. Our design uses a token update mask and staged training to control the scope and timing of structural updates. Across benchmarks and Transformer backbones, GTCA strengthens syntactic robustness beyond continued-training baselines without compromising Multiple-Choice QA performance or commonsense reasoning, providing a practical checkpoint-compatible route to more syntax-robust decoder-only LLMs.

</details>


### [38] [Do Personality Traits Interfere? Geometric Limitations of Steering in Large Language Models](https://arxiv.org/abs/2602.15847)
*Pranav Bhandari,Usman Naseem,Mehwish Nasim*

Main category: cs.CL

TL;DR: 研究人员探讨了大型语言模型中五大人格特质转向向量之间的几何依赖关系，发现即使在去除线性重叠后，转向一个特质仍会引起其他特质的变化，这表明人格特质在模型中存在一个略微耦合的子空间，限制了完全独立的控制。


<details>
  <summary>Details</summary>
Motivation: 为了检验大型语言模型中通过插入特异性人格转向向量来控制人格特质的独立性假设，研究者分析了五大人格特质转向向量之间的几何关系。

Method: 研究者从两个模型家族（LLaMA-3-8B 和 Mistral-8B）中提取转向向量，并应用了从无约束方向到软正交化和硬正交化的多种几何条件方案。

Result: 研究结果表明，人格特质转向方向显示出显著的几何依赖性：即使在去除线性重叠后，转向一个特质仍会引起其他特质的连续变化。硬正交化虽然确保了几何独立性，但仍不能消除跨特质的行为效应，甚至可能降低转向强度。

Conclusion: 总的来说，这些结果表明，大型语言模型中的人格特质占据一个略有耦合的子空间，限制了完全独立的控制能力。

Abstract: Personality steering in large language models (LLMs) commonly relies on injecting trait-specific steering vectors, implicitly assuming that personality traits can be controlled independently. In this work, we examine whether this assumption holds by analysing the geometric relationships between Big Five personality steering directions. We study steering vectors extracted from two model families (LLaMA-3-8B and Mistral-8B) and apply a range of geometric conditioning schemes, from unconstrained directions to soft and hard orthonormalisation. Our results show that personality steering directions exhibit substantial geometric dependence: steering one trait consistently induces changes in others, even when linear overlap is explicitly removed. While hard orthonormalisation enforces geometric independence, it does not eliminate cross-trait behavioural effects and can reduce steering strength. These findings suggest that personality traits in LLMs occupy a slightly coupled subspace, limiting fully independent trait control.

</details>


### [39] [Can LLMs Assess Personality? Validating Conversational AI for Trait Profiling](https://arxiv.org/abs/2602.15848)
*Andrius Matšenas,Anet Lello,Tõnis Lees,Hans Peep,Kim Lilii Tamm*

Main category: cs.CL

TL;DR: 本研究通过内组实验验证了大型语言模型作为问卷人格评估动态替代品的有效性。


<details>
  <summary>Details</summary>
Motivation: 为了探索大型语言模型（LLMs）在人格评估中的潜力，尤其是作为传统问卷调查的动态替代方案。

Method: 使用内组实验（参与者人数为33人），将基于引导性LLM对话获得的Big Five人格得分与黄金标准IPIP-50问卷进行比较，同时测量用户对准确性的感知。

Result: 研究表明，LLM方法与传统问卷在五大人格特质上的相关性为0.38至0.58，其中责任心、开放性和神经质的得分在两种方法之间具有统计学上的等效性。而宜人性和外向性在两种方法之间存在显著差异，这表明可能需要特定的人格特质校准。此外，参与者认为LLM生成的人格描述与传统问卷结果同样准确。

Conclusion: 研究结果表明，对话型AI为传统的心理测量提供了一种有前景的新方法。

Abstract: This study validates Large Language Models (LLMs) as a dynamic alternative to questionnaire-based personality assessment. Using a within-subjects experiment (N=33), we compared Big Five personality scores derived from guided LLM conversations against the gold-standard IPIP-50 questionnaire, while also measuring user-perceived accuracy. Results indicate moderate convergent validity (r=0.38-0.58), with Conscientiousness, Openness, and Neuroticism scores statistically equivalent between methods. Agreeableness and Extraversion showed significant differences, suggesting trait-specific calibration is needed. Notably, participants rated LLM-generated profiles as equally accurate as traditional questionnaire results. These findings suggest conversational AI offers a promising new approach to traditional psychometrics.

</details>


### [40] [Preference Optimization for Review Question Generation Improves Writing Quality](https://arxiv.org/abs/2602.15849)
*Karun Sharma,Vidushee Vats,Shengzhi Li,Yuxiang Wang,Zhongtian Sun,Prayag Tiwari*

Main category: cs.CL

TL;DR: 该研究开发了IntelliReward和基于它的IntelliAsk模型，旨在提高LLM生成的审稿问题的质量，相比基准模型，展示出在推理和复杂写作评估上的显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM生成的审稿问题大多表面化，未深入到论文核心。为了提升质询的质量，作者开发了IntelliReward和IntelliAsk模型。

Method: 作者构建了一个新的奖励模型IntelliReward，结合Decoupled Clip和DAPO策略，用于训练出更为符合人类标准的问答模型IntelliAsk。

Result: IntelliAsk在推理和复杂写作评估中表现出了显著提高，例如在MuSR数据集上准确率提高了3.6%，在WritingBench上得分提高了0.24。

Conclusion: IntelliAsk模型能够生成更符合人类审稿标准的高质量问题，为自动评估LLM生成的审稿问题提供了基准。

Abstract: Peer review relies on substantive, evidence-based questions, yet existing LLM-based approaches often generate surface-level queries, drawing over 50\% of their question tokens from a paper's first page. To bridge this gap, we develop IntelliReward, a novel reward model built from a frozen autoregressive LLM with trainable multi-head transformers over the final 50 token states, which outperforms API-based SFT baselines in predicting expert-level human preferences. By applying Decoupled Clip and Dynamic Sampling Policy Optimization (DAPO) with IntelliReward, we train IntelliAsk, a question-generation model aligned with human standards of effort, evidence, and grounding. We find consistent improvements on reasoning and writing benchmarks, suggesting reviewer-question quality correlates with broader capabilities. Compared to the Qwen3-32B base model, IntelliAsk shows measurable gains across diverse benchmarks, specifically improving performance on reasoning tasks like MuSR (68.3 vs 64.7 Acc) and complex writing evaluations such as WritingBench (8.31 vs 8.07). We release our implementation, expert preference annotations, and the IntelliReward model to provide an automatic evaluation benchmark for grounding, effort, and evidence in LLM-generated review questions.

</details>


### [41] [Large Language Models for Assisting American College Applications](https://arxiv.org/abs/2602.15850)
*Zhengliang Liu,Weihang You,Peng Shu,Junhao Chen,Yi Pan,Hanqi Jiang,Yiwei Li,Zhaojun Ding,Chao Cao,Xinliang Li,Yifan Zhou,Ruidong Zhang,Shaochen Xu,Wei Ruan,Huaqin Zhao,Dajiang Zhu,Tianming Liu*

Main category: cs.CL

TL;DR: EZCollegeApp 是一个由大语言模型支持的系统，旨在帮助高中生简化美国大学申请过程，通过结构化申请表格、引用权威文件和保持全人工最终控制，提供一致的推理能力，同时保证安全性和隐私性。


<details>
  <summary>Details</summary>
Motivation: 美国大学申请过程中存在复杂的政策、重复性表单和模糊的问题，需要学生频繁参考不同来源。因此，EZCollegeApp 通过自动化流程减轻申请负担，确保申请过程中学生的个人信息安全。

Method: EZCollegeApp 采用了先映射后生成的架构，将表单理解与答案生成分离，从而实现跨平台的持续推理。系统从官方招生网站获取文档，结合检索增强的问答功能，并通过人工介入的聊天界面，提供建议以供学生参考而非自动提交。

Result: 该系统具有结构化的申请表处理、基于权威文件的建议答案生成和全人工控制等特点，源代码已开放在 GitHub 上。

Conclusion: EZCollegeApp 提出了一种新的系统架构来解决大学申请过程中的复杂性和安全性问题，为未来的学生申请支持系统提供了可参考的范例。

Abstract: American college applications require students to navigate fragmented admissions policies, repetitive and conditional forms, and ambiguous questions that often demand cross-referencing multiple sources. We present EZCollegeApp, a large language model (LLM)-powered system that assists high-school students by structuring application forms, grounding suggested answers in authoritative admissions documents, and maintaining full human control over final responses. The system introduces a mapping-first paradigm that separates form understanding from answer generation, enabling consistent reasoning across heterogeneous application portals. EZCollegeApp integrates document ingestion from official admissions websites, retrieval-augmented question answering, and a human-in-the-loop chatbot interface that presents suggestions alongside application fields without automated submission. We describe the system architecture, data pipeline, internal representations, security and privacy measures, and evaluation through automated testing and human quality assessment. Our source code is released on GitHub (https://github.com/ezcollegeapp-public/ezcollegeapp-public) to facilitate the broader impact of this work.

</details>


### [42] [Building Safe and Deployable Clinical Natural Language Processing under Temporal Leakage Constraints](https://arxiv.org/abs/2602.15852)
*Ha Na Cho,Sairam Sutari,Alexander Lopez,Hansen Bow,Kai Zheng*

Main category: cs.CL

TL;DR: 该研究提出了一种轻量级的审核管道，旨在通过在模型开发过程中集成可解释性来识别和抑制时间泄漏敏感信号，从而构建符合安全和部署要求的临床NLP系统。研究结果表明，经过审核的模型更保守，预测行为更加准确，减少了对出院相关词汇提示的依赖。


<details>
  <summary>Details</summary>
Motivation: 现有的基于笔记的临床NLP模型容易出现时间泄漏，导致预测性能的虚高。这不仅影响模型的实际应用效率，还可能危害患者安全。因此，本研究旨在减轻这种风险，确保临床NLP模型在部署时更加安全可靠。

Method: 研究团队开发了一种轻量级的审核管道，该管道将可解释性纳入模型开发过程，以识别和抑制可能导致时间泄漏的信号。通过在择期脊柱手术后的次日出院预测案例中评估审核效果，研究具体分析了审核如何影响预测行为、校准以及相关安全贸易。

Result: 结果表明，经过审核的模型表现得更为保守，预测结果更加准确，并减少了对出院相关词汇提示的依赖。这证明，在构建部署的临床NLP系统时，应当优先考虑时间有效性、校准以及行为稳健性，而不是追求乐观的性能。

Conclusion: 该研究强调了在临床NLP系统开发中重视时间有效性、校准及行为稳健性的必要性。未来的研究和实践需要遵循这些原则以确保NLP模型在实际临床环境中的安全性和可靠性。

Abstract: Clinical natural language processing (NLP) models have shown promise for supporting hospital discharge planning by leveraging narrative clinical documentation. However, note-based models are particularly vulnerable to temporal and lexical leakage, where documentation artifacts encode future clinical decisions and inflate apparent predictive performance. Such behavior poses substantial risks for real-world deployment, where overconfident or temporally invalid predictions can disrupt clinical workflows and compromise patient safety. This study focuses on system-level design choices required to build safe and deployable clinical NLP under temporal leakage constraints. We present a lightweight auditing pipeline that integrates interpretability into the model development process to identify and suppress leakage-prone signals prior to final training. Using next-day discharge prediction after elective spine surgery as a case study, we evaluate how auditing affects predictive behavior, calibration, and safety-relevant trade-offs. Results show that audited models exhibit more conservative and better-calibrated probability estimates, with reduced reliance on discharge-related lexical cues. These findings emphasize that deployment-ready clinical NLP systems should prioritize temporal validity, calibration, and behavioral robustness over optimistic performance.

</details>


### [43] [A Lightweight Explainable Guardrail for Prompt Safety](https://arxiv.org/abs/2602.15853)
*Md Asiful Islam,Mihai Surdeanu*

Main category: cs.CL

TL;DR: 该方法提出了一种轻量级可解释的护栏方法（LEG），用于不安全提示的分类，并在多个数据集上获得了与前哨方法相当或更优的性能。


<details>
  <summary>Details</summary>
Motivation: 鉴于大型语言模型（LLMs）可能产生有害的提示，研究提出了一种新的方法来确保模型输出的安全性。

Method: 该方法采用多任务学习架构，联合训练提示分类器和解释分类器，后者标记能够解释安全/不安全决定的关键提示词。实验中使用合成数据生成策略来对抗LLMs的确认偏见，并引入了一种新的损失函数，结合了交叉熵损失、焦点损失和基于不确定性的权重。

Result: 在三个数据集上训练和评估的实验结果显示，尽管模型规模较小，但LEG方法在提示分类和可解释性方面与当前最先进的方法取得了相当或更好的表现。

Conclusion: 最后，该方法具有广泛的适用性，能够应对模型的域外表现问题，并且如果被接受，所有模型和注释数据集将被公开发布。

Abstract: We propose a lightweight explainable guardrail (LEG) method for the classification of unsafe prompts. LEG uses a multi-task learning architecture to jointly learn a prompt classifier and an explanation classifier, where the latter labels prompt words that explain the safe/unsafe overall decision. LEG is trained using synthetic data for explainability, which is generated using a novel strategy that counteracts the confirmation biases of LLMs. Lastly, LEG's training process uses a novel loss that captures global explanation signals and combines cross-entropy and focal losses with uncertainty-based weighting. LEG obtains equivalent or better performance than the state-of-the-art for both prompt classification and explainability, both in-domain and out-of-domain on three datasets, despite the fact that its model size is considerably smaller than current approaches. If accepted, we will release all models and the annotated dataset publicly.

</details>


### [44] [Rethinking Soft Compression in Retrieval-Augmented Generation: A Query-Conditioned Selector Perspective](https://arxiv.org/abs/2602.15856)
*Yunhao Liu,Zian Jia,Xinyu Gao,Kanjun Xu,Yun Xiong*

Main category: cs.CL

TL;DR: SeleCom是一种基于选择器的软压缩框架，解决了全压缩与大模型下游生成行为的冲突，并通过Query-conditioned方式有效减少了计算和延迟，显著优于现有软压缩方法。


<details>
  <summary>Details</summary>
Motivation: 目前的软压缩方法依赖于全压缩，可能对输入查询无关的信息也会进行压缩，导致任务相关信息密度稀释。因此，作者提出了SeleCom来解决这个问题。

Method: SeleCom采用解码器仅训练的选择器，通过查询条件下的信息选择器，重新定义编码器的角色。选择器是在大规模、多样性和难度递增的合成QA数据集上，结合迭代学习策略训练而成。

Result: 实验结果显示，SeleCom显著优于现有软压缩方法，且在某些情况下甚至超过了无压缩基线，同时计算和延迟分别降低了33.8%~84.6%。

Conclusion: 该研究提出了一种新的软压缩框架SeleCom，通过Query-conditioned选择器有效地提高了生成模型的生成质量和效率。

Abstract: Retrieval-Augmented Generation (RAG) effectively grounds Large Language Models (LLMs) with external knowledge and is widely applied to Web-related tasks. However, its scalability is hindered by excessive context length and redundant retrievals. Recent research on soft context compression aims to address this by encoding long documents into compact embeddings, yet they often underperform non-compressed RAG due to their reliance on auto-encoder-like full-compression that forces the encoder to compress all document information regardless of relevance to the input query.
  In this work, we conduct an analysis on this paradigm and reveal two fundamental limitations: (I) Infeasibility, full-compression conflicts with the LLM's downstream generation behavior; and (II) Non-necessity: full-compression is unnecessary and dilutes task-relevant information density. Motivated by these insights, we introduce SeleCom, a selector-based soft compression framework for RAG that redefines the encoder's role as query-conditioned information selector. The selector is decoder-only and is trained with a massive, diverse and difficulty-graded synthetic QA dataset with curriculum learning.
  Extensive experiments show that SeleCom significantly outperforms existing soft compression approaches and achieves competitive or superior performance to non-compression baselines, while reducing computation and latency by 33.8%~84.6%.

</details>


### [45] [Multi-source Heterogeneous Public Opinion Analysis via Collaborative Reasoning and Adaptive Fusion: A Systematically Integrated Approach](https://arxiv.org/abs/2602.15857)
*Yi Liu*

Main category: cs.CL

TL;DR: 本研究提出了一种名为CRAF的方法，旨在多平台公共舆论分析中实现平台间跨模态数据的协同推理与自适应融合。


<details>
  <summary>Details</summary>
Motivation: 当前公共舆论从多样化来源分析面临的挑战包括结构差异、语义差异和平台偏见，本研究旨在克服这些问题，提出了一种多层次自适应融合框架。

Method: 方法包括一个跨平台协同注意力模块，用于保持特定来源特征的同时对齐语义表示；一种层次自适应融合机制，用于基于数据质量和任务需求动态加权特征；一种联合优化策略，用于在共享潜在空间中学习话题表示和情绪分布；一种新的多模态提取能力，能够处理抖音和快手等平台上的视频内容。

Result: 理论分析表明，CRAF框架能比独立来源建模获得更紧密的一般化界限，实验表明其在三个多平台数据集（Weibo-12, CrossPlatform-15, NewsForum-8）上的主题聚类ARI平均为0.76，并且情感分析F1分数为0.84，分别比最佳基线提高4.1%和3.8%。

Conclusion: 总之，该框架不仅在多平台公权力分析中展示了强大的适应性，还可以降低87.5%的新平台标注数据需求。

Abstract: The analysis of public opinion from multiple heterogeneous sources presents significant challenges due to structural differences, semantic variations, and platform-specific biases. This paper introduces a novel Collaborative Reasoning and Adaptive Fusion (CRAF) framework that systematically integrates traditional feature-based methods with large language models (LLMs) through a structured multi-stage reasoning mechanism. Our approach features four key innovations: (1) a cross-platform collaborative attention module that aligns semantic representations while preserving source-specific characteristics, (2) a hierarchical adaptive fusion mechanism that dynamically weights features based on both data quality and task requirements, (3) a joint optimization strategy that simultaneously learns topic representations and sentiment distributions through shared latent spaces, and (4) a novel multimodal extraction capability that processes video content from platforms like Douyin and Kuaishou by integrating OCR, ASR, and visual sentiment analysis. Theoretical analysis demonstrates that CRAF achieves a tighter generalization bound with a reduction of O(sqrt(d log K / m)) compared to independent source modeling, where d is feature dimensionality, K is the number of sources, and m is sample size. Comprehensive experiments on three multi-platform datasets (Weibo-12, CrossPlatform-15, NewsForum-8) show that CRAF achieves an average topic clustering ARI of 0.76 (4.1% improvement over best baseline) and sentiment analysis F1-score of 0.84 (3.8% improvement). The framework exhibits strong cross-platform adaptability, reducing the labeled data requirement for new platforms by 75%.

</details>


### [46] [Reranker Optimization via Geodesic Distances on k-NN Manifolds](https://arxiv.org/abs/2602.15860)
*Wen G. Gong*

Main category: cs.CL

TL;DR: Maniscope 提出了一种几何重排序方法，通过计算 k-NN  manifold 上的测地线距离，有效提升了检索增强生成 (RAG) 的效率和准确性，相比现有的交叉编码器和大规模语言模型重排序方法，Maniscope 具有更高的效果和显著更低的延迟。


<details>
  <summary>Details</summary>
Motivation: 现有的神经重排序方法依赖复杂模型，计算资源消耗大且存在延迟问题，Maniscope 设计旨在解决这些问题，通过引入局部几何结构来改进重排序效果。

Method: Maniscope 使用了 k-NN 及测地线距离的概念，结合全局余弦相似度和局部流形几何，提出了一个几何重排序方法，通过这一步骤提升检索结果的排列表现。

Result: Maniscope 在 BEIR 的 8 个基准数据集上表现出色，特别是在 NFCorpus、TREC-COVID 和 AorB 数据集上，相比 HNSW 基准，分别提高了 7.0%、1.6% 和 2.8% 的 NDCG@3 指标，同时速度提高了 3.2 倍（平均从 14.8 ms 降低到 4.7 ms），相比交叉编码器重排序器具有 2% 的准确性和低 10-45 倍的延迟。

Conclusion: Maniscope 提供了一种实用的替代方案，适用于实时 RAG 部署，特别是在 TREC-COVID 数据集上，与大语言模型重排序相比，有显著更好的性能和更低的延迟，显示出其实用价值。

Abstract: Current neural reranking approaches for retrieval-augmented generation (RAG) rely on cross-encoders or large language models (LLMs), requiring substantial computational resources and exhibiting latencies of 3-5 seconds per query. We propose Maniscope, a geometric reranking method that computes geodesic distances on k-nearest neighbor (k-NN) manifolds constructed over retrieved document candidates. This approach combines global cosine similarity with local manifold geometry to capture semantic structure that flat Euclidean metrics miss. Evaluating on eight BEIR benchmark datasets (1,233 queries), Maniscope outperforms HNSW graph-based baseline on the three hardest datasets (NFCorpus: +7.0%, TREC-COVID: +1.6%, AorB: +2.8% NDCG@3) while being 3.2x faster (4.7 ms vs 14.8 ms average). Compared to cross-encoder rerankers, Maniscope achieves within 2% accuracy at 10-45x lower latency. On TREC-COVID, LLM-Reranker provides only +0.5% NDCG@3 improvement over Maniscope at 840x higher latency, positioning Maniscope as a practical alternative for real-time RAG deployment. The method requires O(N D + M^2 D + M k log k) complexity where M << N , enabling sub-10 ms latency. We plan to release Maniscope as open-source software.

</details>


### [47] [CAST: Achieving Stable LLM-based Text Analysis for Data Analytics](https://arxiv.org/abs/2602.15861)
*Jinxiang Xie,Zihao Li,Wei He,Rui Ding,Shi Han,Dongmei Zhang*

Main category: cs.CL

TL;DR: 该研究提出了CAST框架，通过算法提示和思考后再说话的方法增强大规模语言模型在表格文本分析中的稳定性。CAST在多个基准测试上表现出最高的稳定性，同时保持或提升了输出质量。


<details>
  <summary>Details</summary>
Motivation: 研究指出，现有的大规模语言模型在执行文本分析任务时稳定性不足，难以满足数据分析师的需求。因此，CAST框架旨在通过限制模型的隐式推理路径来解决这一问题。

Method: CAST框架结合了算法提示和思考后再说话两种方法。算法提示规定了有效的推理转换路径，而思考后再说话则在最终生成之前要求模型进行明确定义的中间承诺。

Result: 研究引入了CAST-S和CAST-T两个稳定性度量标准来评估 bulleted总结和标记的稳定性，并通过多个公开可用的基准测试验证了CAST的性能。相比其他基线模型，CAST在所有评估任务中的稳定性提升显著，最高提高了16.2%，同时还能保持或提升输出质量。

Conclusion: 研究结论表明，CAST框架能够有效地提升大规模语言模型在表格文本分析任务中的稳定性，为数据分析师提供了更可靠的支持。

Abstract: Text analysis of tabular data relies on two core operations: \emph{summarization} for corpus-level theme extraction and \emph{tagging} for row-level labeling. A critical limitation of employing large language models (LLMs) for these tasks is their inability to meet the high standards of output stability demanded by data analytics. To address this challenge, we introduce \textbf{CAST} (\textbf{C}onsistency via \textbf{A}lgorithmic Prompting and \textbf{S}table \textbf{T}hinking), a framework that enhances output stability by constraining the model's latent reasoning path. CAST combines (i) Algorithmic Prompting to impose a procedural scaffold over valid reasoning transitions and (ii) Thinking-before-Speaking to enforce explicit intermediate commitments before final generation. To measure progress, we introduce \textbf{CAST-S} and \textbf{CAST-T}, stability metrics for bulleted summarization and tagging, and validate their alignment with human judgments. Experiments across publicly available benchmarks on multiple LLM backbones show that CAST consistently achieves the best stability among all baselines, improving Stability Score by up to 16.2\%, while maintaining or improving output quality.

</details>


### [48] [Enhancing Action and Ingredient Modeling for Semantically Grounded Recipe Generation](https://arxiv.org/abs/2602.15862)
*Guoshan Liu,Bin Zhu,Yian Li,Jingjing Chen,Chong-Wah Ngo,Yu-Gang Jiang*

Main category: cs.CL

TL;DR: 该研究提出了一种基于语义的框架，通过两阶段 pipeline（监督微调和强化微调）以及语义可信度评分和修正模块，提高了菜谱生成的语义准确性。


<details>
  <summary>Details</summary>
Motivation: 当前的多模态大型语言模型在从食品图像生成食谱方面取得了进展，但输出经常包含语义不正确的动作或食材，尽管在词汇得分（如 BLEU, ROUGE）上表现良好。为解决此问题，研究提出新框架。

Method: 研究采用两阶段 pipeline：第一阶段是监督微调（SFT），使用 Action-Reasoning 数据集和成分语料库建立基础准确性；第二阶段是强化微调（RFT），采用频率感知奖励提升长尾动作预测和成分泛化。此外，还引入了语义可信度评分和修正模块来进一步过滤和纠正预测。

Result: 该方法在 Recipe1M 数据集上表现出了最先进的性能，并显著提高了语义的准确性。

Conclusion: 通过多阶段训练和语义纠正模块，研究有效提升了多模态语言模型生成菜谱的语义准确性。

Abstract: Recent advances in Multimodal Large Language Models (MLMMs) have enabled recipe generation from food images, yet outputs often contain semantically incorrect actions or ingredients despite high lexical scores (e.g., BLEU, ROUGE). To address this gap, we propose a semantically grounded framework that predicts and validates actions and ingredients as internal context for instruction generation. Our two-stage pipeline combines supervised fine-tuning (SFT) with reinforcement fine-tuning (RFT): SFT builds foundational accuracy using an Action-Reasoning dataset and ingredient corpus, while RFT employs frequency-aware rewards to improve long-tail action prediction and ingredient generalization. A Semantic Confidence Scoring and Rectification (SCSR) module further filters and corrects predictions. Experiments on Recipe1M show state-of-the-art performance and markedly improved semantic fidelity.

</details>


### [49] [Not the Example, but the Process: How Self-Generated Examples Enhance LLM Reasoning](https://arxiv.org/abs/2602.15863)
*Daehoon Gwak,Minseo Jung,Junwoo Park,Minho Park,ChaeHun Park,Junha Hyung,Jaegul Choo*

Main category: cs.CL

TL;DR: 该研究通过对比零-shot、集成和去耦合三类提示策略，发现集成提示策略在多种语言模型架构下表现最优，而去耦合提示策略仅有微小优势，强调了问题创作过程的重要性。


<details>
  <summary>Details</summary>
Motivation: 研究为何当前的大型语言模型在使用自我生成的少量示例进行推理时，可以达到接近人工策划示例的效果，但其背后机制仍不清楚。

Method: 本研究系统性地评估了零-shot、集成和去耦合三种提示策略，并在五个常用模型架构上进行了实验。通过注意力分析进一步探讨了集成与去耦合策略之间的差异。

Result: 实验结果表明，集成提示策略在多种模型架构中表现优异，而去耦合提示策略仅提供微小优势。注意力分析揭示了集成与去耦合策略在关注模式上的显著差异。

Conclusion: 研究表明，自我生成提示策略的主要优势在于问题创作过程，而非生成的示例本身，为设计更有效的提示策略提供了宝贵见解。

Abstract: Recent studies have shown that Large Language Models (LLMs) can improve their reasoning performance through self-generated few-shot examples, achieving results comparable to manually curated in-context examples. However, the underlying mechanism behind these gains remains unclear, making it hard to decide when and how to apply the technique effectively. In this work, we argue that the key benefit arises not from the generated examples themselves but from the act of creating them. To validate this, on reasoning-intensive tasks across diverse LLM architectures, we systematically evaluate three prompting strategies for in-context learning: (1) Zero-shot prompting; (2) Integrated prompting, where LLMs create and solve problems within a single, unified prompt; and (3) Decoupled prompting, where self-generated examples are reused as in-context examples, but the context of their creation itself is excluded. We conduct experiments across five widely used model architectures, demonstrating that Integrated prompting consistently outperforms both Zero-shot and Decoupled prompting. In contrast, Decoupled prompting offers only marginal gains over Zero-shot. Further, for a more in-depth analysis, we conduct an attention analysis and observe significant differences in attention patterns between Integrated and Decoupled prompting. These findings suggest that the advantage of self-generation prompting comes from the process of problem creation, not the examples themselves, providing valuable insights for designing more effective prompting strategies.

</details>


### [50] [Playing With AI: How Do State-Of-The-Art Large Language Models Perform in the 1977 Text-Based Adventure Game Zork?](https://arxiv.org/abs/2602.15867)
*Berry Gerrits*

Main category: cs.CL

TL;DR: 研究通过Zork（一款经典的基于文本的冒险游戏）评估了几个大型语言模型（LLMs）的问题解决和推理能力。实验结果显示，这些模型的完成率很低，最高也只能达到非常低的分数。即使提供了详细的游戏指令或允许进行更深入的思考，也未能显著提高模型的表现。研究表明，当前LLMs在元认知能力和处理文本游戏方面存在显著局限。


<details>
  <summary>Details</summary>
Motivation: 评估现代大型语言模型在解决复杂文本环境问题和逻辑推理方面的能力。使用Zork作为测试平台，因为它提供了一个受控环境，可以准确衡量模型对自然语言的理解和问题解决能力。

Method: 对多个领先的商用模型（ChatGPT, Claude, Gemini）在Zork中的表现进行了测试，分别在简要和详细的游戏说明下进行，并通过得分来衡量模型的游戏完成程度。

Result: 所有测试的模型在平均表现上表现不佳，最高得分也仅为总数的21.4%。即使提供了详细的游戏指南或开启了‘深度思考’模式，模型的表现也没有提升。质性分析显示模型存在重复失败、策略执行不一致、无法从过去尝试中学习等元认知上的困难。

Conclusion: 研究表明，当前的大语言模型在元认知能力和解决文本游戏问题方面存在显著局限。这引发了质疑，关于模型当前推理能力的真实性质和范围。

Abstract: In this positioning paper, we evaluate the problem-solving and reasoning capabilities of contemporary Large Language Models (LLMs) through their performance in Zork, the seminal text-based adventure game first released in 1977. The game's dialogue-based structure provides a controlled environment for assessing how LLM-based chatbots interpret natural language descriptions and generate appropriate action sequences to succeed in the game. We test the performance of leading proprietary models - ChatGPT, Claude, and Gemini - under both minimal and detailed instructions, measuring game progress through achieved scores as the primary metric. Our results reveal that all tested models achieve less than 10% completion on average, with even the best-performing model (Claude Opus 4.5) reaching only approximately 75 out of 350 possible points. Notably, providing detailed game instructions offers no improvement, nor does enabling ''extended thinking''. Qualitative analysis of the models' reasoning processes reveals fundamental limitations: repeated unsuccessful actions suggesting an inability to reflect on one's own thinking, inconsistent persistence of strategies, and failure to learn from previous attempts despite access to conversation history. These findings suggest substantial limitations in current LLMs' metacognitive abilities and problem-solving capabilities within the domain of text-based games, raising questions about the nature and extent of their reasoning capabilities.

</details>


### [51] [Understanding LLM Failures: A Multi-Tape Turing Machine Analysis of Systematic Errors in Language Model Reasoning](https://arxiv.org/abs/2602.15868)
*Magnus Boman*

Main category: cs.CL

TL;DR: 该研究提出了一种使用确定性多带图灵机形式化大型语言模型（LLM）交互的方法，旨在精确定位模型中的失败模式，并揭示了令牌化等问题对模型性能的影响。


<details>
  <summary>Details</summary>
Motivation: 为了解决LLMs在看似简单的任务上表现出异常行为的问题，研究者们提出了一个新的模型来更具体地定位这些失败，并深入理解这些问题的根源。

Method: 研究者将LLM的交互过程通过一个确定性多带图灵机来建模。每个磁带代表不同的组件：输入字符、令牌、词汇表、模型参数、激活值、概率分布和输出文本。

Result: 该模型揭示了例如令牌化如何隐藏必要的字符级结构，这在需要计数任务时可能导致问题。此外，还能阐明为什么像思路链这样的提示技术能有所帮助，但也揭示了它们的基本局限。

Conclusion: 这种方法为理解和分析大型语言模型提供了一个严格且可验证的框架，替代了仅依靠几何形象的描述，并与实验缩放律相结合，提供了更加原则性的方法来分析错误。

Abstract: Large language models (LLMs) exhibit failure modes on seemingly trivial tasks. We propose a formalisation of LLM interaction using a deterministic multi-tape Turing machine, where each tape represents a distinct component: input characters, tokens, vocabulary, model parameters, activations, probability distributions, and output text. The model enables precise localisation of failure modes to specific pipeline stages, revealing, e.g., how tokenisation obscures character-level structure needed for counting tasks. The model clarifies why techniques like chain-of-thought prompting help, by externalising computation on the output tape, while also revealing their fundamental limitations. This approach provides a rigorous, falsifiable alternative to geometric metaphors and complements empirical scaling laws with principled error analysis.

</details>


### [52] [Towards Fair and Efficient De-identification: Quantifying the Efficiency and Generalizability of De-identification Approaches](https://arxiv.org/abs/2602.15869)
*Noopur Zambare,Kiana Aghakasiri,Carissa Lin,Carrie Ye,J. Ross Mitchell,Mohamed Abdalla*

Main category: cs.CL

TL;DR: 该研究系统地评估了不同大小的语言模型在多语言环境下的临床去标识化性能，并且展示了小模型在数据有限的情况下同样可以胜过大模型，尤其在跨文化和性别化的医疗数据处理上有显著表现。研究还公开了多语言的去标识化模型集合，包括基于多种变体语言的微调模型。


<details>
  <summary>Details</summary>
Motivation: 评估不同大小的自然语言处理模型在处理多语言和跨文化交流中的去标识化任务的性能，以提高医疗数据的隐私保护和技术可行性。

Method: 本研究使用了包括BERT、ClinicalBERT、ModernBERT在内的预训练模型，以及Llama系列和Qwen系列的不同大小的语言模型，对临床去标识化任务进行全面评估，同时引入了一个基于多种语言变体微调的多文化去标识化模型集合。

Result: 研究结果显示，小型模型在去标识化任务上取得了与其他大型模型相匹敌的效果，尤其是在跨文化和性别化数据上的表现更为突出。引入的多文化去标识化模型集合在多种语言数据上表现出良好的性能。

Conclusion: 研究提出了一种在多样化的语言和文化环境下进行临床数据去标识化的新方法，强调了根据不同需求选择适当大小模型的重要性，促进了公平和高效的临床数据去标识化实践。

Abstract: Large language models (LLMs) have shown strong performance on clinical de-identification, the task of identifying sensitive identifiers to protect privacy. However, previous work has not examined their generalizability between formats, cultures, and genders. In this work, we systematically evaluate fine-tuned transformer models (BERT, ClinicalBERT, ModernBERT), small LLMs (Llama 1-8B, Qwen 1.5-7B), and large LLMs (Llama-70B, Qwen-72B) at de-identification. We show that smaller models achieve comparable performance while substantially reducing inference cost, making them more practical for deployment. Moreover, we demonstrate that smaller models can be fine-tuned with limited data to outperform larger models in de-identifying identifiers drawn from Mandarin, Hindi, Spanish, French, Bengali, and regional variations of English, in addition to gendered names. To improve robustness in multi-cultural contexts, we introduce and publicly release BERT-MultiCulture-DEID, a set of de-identification models based on BERT, ClinicalBERT, and ModernBERT, fine-tuned on MIMIC with identifiers from multiple language variants. Our findings provide the first comprehensive quantification of the efficiency-generalizability trade-off in de-identification and establish practical pathways for fair and efficient clinical de-identification.
  Details on accessing the models are available at: https://doi.org/10.5281/zenodo.18342291

</details>


### [53] [VDLM: Variable Diffusion LMs via Robust Latent-to-Text Rendering](https://arxiv.org/abs/2602.15870)
*Shuhui Qu*

Main category: cs.CL

TL;DR: VDLM 提出了一种新型的语言模型，通过分离语义规划和文本渲染，利用扩散模型在语义空间进行迭代优化，并引入文本渲染模块和嵌入扰动技术，显著提高了长文本生成任务的效果。


<details>
  <summary>Details</summary>
Motivation: 现有自回归语言模型在进行多步推理时，左侧到右侧的解码方式和不可逆的承诺限制了对其内部决策过程的回顾与修正。VDLM旨在通过分离语义规划和文本渲染机制，提高模型的灵活性和鲁棒性。

Method: VDLM利用了LLaDA风格的掩码扩散模型，对语义变量嵌入进行迭代优化，并采用轨迹感知优化方法进行二次训练。此外，采用Vec2Text渲染器将计划中的嵌入转换回文本，并引入嵌入扰动技术增强对规划者噪音的鲁棒性。

Result: VDLM在包括一般推理、数学和代码等多个基准测试中表现出色，特别是在长文本生成任务上优于其他基线方法。这表明嵌入空间二次训练和鲁棒的潜在空间到文本渲染在扩散语言建模中的有效性。

Conclusion: VDLM 的研究表明，通过在嵌入空间进行优化和引入一个更加鲁棒的渲染模块，可以有效提高长期文本生成的质量和模型的性能。

Abstract: Autoregressive language models decode left-to-right with irreversible commitments, limiting revision during multi-step reasoning. We propose \textbf{VDLM}, a modular variable diffusion language model that separates semantic planning from text rendering. VDLM applies LLaDA-style masked diffusion over semantic variable embeddings to enable iterative refinement in latent space, then post-trains the planner with trajectory-aware optimization using embedding-space rewards and values, avoiding text decoding inside the RL loop. To convert planned embeddings back to text, we use a \textbf{Vec2Text} renderer and introduce \textbf{embedding perturbations} to robustify decoding under planner noise. Across nine benchmarks spanning general reasoning, math, and code, VDLM is competitive in pre-training and yields substantial post-training improvements on long-form generation tasks, outperforming other baselines. These results highlight the effectiveness of embedding-space post-training and robust latent-to-text rendering for diffusion language modeling.

</details>


### [54] [CheckIfExist: Detecting Citation Hallucinations in the Era of AI-Generated Content](https://arxiv.org/abs/2602.15871)
*Diletta Abbonato*

Main category: cs.CL

TL;DR: 该论文介绍了一种名为'CheckIfExist'的开源工具，旨在通过跨多个学术数据库进行验证，立即核实参考文献的真实性和合法性。


<details>
  <summary>Details</summary>
Motivation: 现有的文献管理和AI幻觉检测服务缺乏即时验证参考文献真实性的能力，因此需要一种能够解决这一问题的工具。

Method: 该工具采用了级联验证架构，结合字符串相似性算法计算多维匹配置信度分数，提供单个参考文献验证和批量处理BibTeX条目的统一界面。

Result: 该工具能够在几秒内提供即时反馈，验证参考文献的真实性，并生成已验证的APA引用和导出的BibTeX记录。

Conclusion: 该开源工具已公开发布，弥补了现有文献管理和幻觉检测服务的功能空白，提高了文献引用的准确性。

Abstract: The proliferation of large language models (LLMs) in academic workflows has introduced unprecedented challenges to bibliographic integrity, particularly through reference hallucination -- the generation of plausible but non-existent citations. Recent investigations have documented the presence of AI-hallucinated citations even in papers accepted at premier machine learning conferences such as NeurIPS and ICLR, underscoring the urgency of automated verification mechanisms. This paper presents "CheckIfExist", an open-source web-based tool designed to provide immediate verification of bibliographic references through multi-source validation against CrossRef, Semantic Scholar, and OpenAlex scholarly databases. While existing reference management tools offer bibliographic organization capabilities, they do not provide real-time validation of citation authenticity. Commercial hallucination detection services, though increasingly available, often impose restrictive usage limits on free tiers or require substantial subscription fees. The proposed tool fills this gap by employing a cascading validation architecture with string similarity algorithms to compute multi-dimensional match confidence scores, delivering instant feedback on reference authenticity. The system supports both single-reference verification and batch processing of BibTeX entries through a unified interface, returning validated APA citations and exportable BibTeX records within seconds.

</details>


### [55] [P-RAG: Prompt-Enhanced Parametric RAG with LoRA and Selective CoT for Biomedical and Multi-Hop QA](https://arxiv.org/abs/2602.15874)
*Xingda Lyu,Gongfu Lyu,Zitai Yan,Yuxin Jiang*

Main category: cs.CL

TL;DR: 该研究评估了三种RAG变体在医学和通用数据集上的性能，并提出了一种名为P-RAG的混合架构，该架构结合了参数化知识和链式思考提示，显著提升了模型在复杂推理任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型表现出色，但它们依赖于固定的训练数据，缺乏灵活性。为此，研究采用了检索增强生成技术并通过引入一种新的混合架构P-RAG来探索改进进度。

Method: 研究采用了LLaMA-3.2-1B-Instruct模型，并利用LoRA进行微调。同时，研究中还引入了P-RAG架构，该架构将参数化知识与检索证据结合，并利用链式思考和低秩适应等方法进行优化。

Result: 研究发现，P-RAG在PubMedQA和2WikiMultihopQA数据集上的表现显著优于标准RAG。P-RAG在PubMedQA上的F1得分为93.33%，比标准RAG高出10.47个百分点；而在2WikiMultihopQA上的综合得分几乎是标准RAG的两倍。

Conclusion: 研究结果证明，P-RAG具有准确、可扩展且能适应上下文的医学问答能力。

Abstract: Large Language Models (LLMs) demonstrate remarkable capabilities but remain limited by their reliance on static training data. Retrieval-Augmented Generation (RAG) addresses this constraint by retrieving external knowledge during inference, though it still depends heavily on knowledge base quality. To explore potential improvements, we evaluated three RAG variants-Standard RAG, DA-RAG, and our proposed Prompt-Enhanced Parametric RAG (P-RAG), a hybrid architecture that integrates parametric knowledge within the LLM and retrieved evidence, guided by Chain-of-Thought (CoT) prompting and Low-Rank Adaptation (LoRA) fine-tuning-on both general and biomedical datasets. Using LLaMA-3.2-1B-Instruct fine-tuned via LoRA, we evaluate on PubMedQA and 2WikiMultihopQA. P-RAG outperforms Standard RAG on PubMedQA by 10.47 percentage points in F1 (93.33% vs. 82.86%; 12.64% relative). On 2WikiMultihopQA, P-RAG nearly doubles the overall score vs. Standard RAG (33.44% vs. 17.83%) and achieves 44.03% on the Compare subset (with 42.74% Bridge, 21.84% Inference, 8.60% Compose). CoT prompting substantially improves multi-hop reasoning but yields mixed results for simpler, single-hop queries. These findings underscore P-RAG's potential for accurate, scalable, and contextually adaptive biomedical question answering. Our contributions include: (1) LoRA-based fine-tuning of LLaMA-3.2-1B-Instruct for biomedical QA, (2) introduction of P-RAG with Chain-of-Thought prompting, and (3) state-of-the-art results on PubMedQA and 2WikiMultihopQA.

</details>


### [56] [Quality-constrained Entropy Maximization Policy Optimization for LLM Diversity](https://arxiv.org/abs/2602.15894)
*Haihui Pan,Yuzhong Hong,Shaoke Lv,Junwei Bao,Hongfei Jiang,Yang Song*

Main category: cs.CL

TL;DR: QEMPO 是一种增强大语言模型输出多样性的新方法，能够在保持质量的同时提高性能和输出多样性。


<details>
  <summary>Details</summary>
Motivation: 现有的对齐方法虽然提高了大语言模型输出的质量，但降低了多样性。QEMPO 的提出是为了在确保输出质量的前提下，增加模型输出的多样性，最终实现高质量和高多样性的输出效果。

Method: QEMPO 通过最大化输出熵和约束质量来优化策略，通过在线和离线训练方法来优化策略。

Result: 实验结果表明，QEMPO 的性能与或优于 RLHF，同时提高了输出多样性。

Conclusion: QEMPO 提供了一种有效的解决方案，能够在保持高质量输出的同时增加大语言模型的多样性。

Abstract: Recent research indicates that while alignment methods significantly improve the quality of large language model(LLM) outputs, they simultaneously reduce the diversity of the models' output. Although some methods have been proposed to enhance LLM output diversity, they often come at the cost of reduced performance. In this work, we first theoretically demonstrate that the alignment task can be decomposed into two distributions: quality and diversity. To enhance the diversity of LLM outputs while ensuring quality, we propose the Quality-constrained Entropy Maximization Policy Optimization (QEMPO). QEMPO aims to maximize the output entropy of the policy while ensuring output quality. By adding different constraints to QEMPO, we obtain different policies. To optimize policies, we propose both online and offline training methods. Experiments validate that QEMPO achieves performance comparable to or even better than RLHF while improving output diversity.

</details>


### [57] [Understand Then Memory: A Cognitive Gist-Driven RAG Framework with Global Semantic Diffusion](https://arxiv.org/abs/2602.15895)
*Pengcheng Zhou,Haochen Li,Zhiqiang Nie,JiaLe Chen,Qing Gong,Weizhen Zhang,Chun Yu*

Main category: cs.CL

TL;DR: CogitoRAG 是一种新型的 RAG 框架，通过模拟人类认知记忆过程中的语义纲要提取与演变，以及复杂的查询分解和实体扩散检索机制，有效解决了现有框架中的语义完整性损失问题，显著提高了复杂知识集成和推理能力。


<details>
  <summary>Details</summary>
Motivation: 针对现有 RAG 框架中由于文本离散表示而导致的语义完整性丢失问题，提出了 CogitoRAG 框架，旨在通过结合人类认知机制，改善知识抽取和检索过程，从而提升生成模型的可靠性。

Method: CogitoRAG 框架首先在线下索引阶段将无结构的数据索引为语义纲要记忆体，并构建一个多维知识图谱；在线上检索阶段，通过查询分解模块将复杂查询分解为子查询，并使用实体扩散模块在图上进行关系导向的检索；最后，通过 CogniRank 算法对候选段落进行精排。

Result: CogitoRAG 在多个主流 QA 基准测试和 GraphBench 多任务生成任务中表现出色，显著优于现有最先进的 RAG 方法。

Conclusion: CogitoRAG 框架展示了强大的复杂知识集成和推理论证能力，表明其在 LLMs 中的潜在应用价值。

Abstract: Retrieval-Augmented Generation (RAG) effectively mitigates hallucinations in LLMs by incorporating external knowledge. However, the inherent discrete representation of text in existing frameworks often results in a loss of semantic integrity, leading to retrieval deviations. Inspired by the human episodic memory mechanism, we propose CogitoRAG, a RAG framework that simulates human cognitive memory processes. The core of this framework lies in the extraction and evolution of the Semantic Gist. During the offline indexing stage, CogitoRAG first deduces unstructured corpora into gist memory corpora, which are then transformed into a multi-dimensional knowledge graph integrating entities, relational facts, and memory nodes. In the online retrieval stage, the framework handles complex queries via Query Decomposition Module that breaks them into comprehensive sub-queries, mimicking the cognitive decomposition humans employ for complex information. Subsequently, Entity Diffusion Module performs associative retrieval across the graph, guided by structural relevance and an entity-frequency reward mechanism. Furthermore, we propose the CogniRank algorithm, which precisely reranks candidate passages by fusing diffusion-derived scores with semantic similarity. The final evidence is delivered to the generator in a passage-memory pairing format, providing high-density information support. Experimental results across five mainstream QA benchmarks and multi-task generation on GraphBench demonstrate that CogitoRAG significantly outperforms state-of-the-art RAG methods, showcasing superior capabilities in complex knowledge integration and reasoning.

</details>


### [58] [Every Little Helps: Building Knowledge Graph Foundation Model with Fine-grained Transferable Multi-modal Tokens](https://arxiv.org/abs/2602.15896)
*Yichi Zhang,Zhuo Chen,Lingbing Guo,Wen Zhang,Huajun Chen*

Main category: cs.CL

TL;DR: TOFU 是一种用于多模态知识图谱推理（MMKGR）的基于标记的基础模型，能够处理结构、视觉和文本信息，并通过层次融合架构和混合消息机制提高跨知识图谱的转移能力。


<details>
  <summary>Details</summary>
Motivation: 现有的多模态知识图谱推理方法多数局限于特定的数据集，难以泛化到新的知识图谱。知识图谱基础模型虽然提高了跨图谱的转移能力，但主要依赖结构模式而忽略了丰富的多模态信号。为解决这些问题，提出了TOFU模型，以提升不同知识图谱间的泛化性能。

Method: TOFU通过将结构、视觉和文本信息分别转换为模态特定的标记，然后使用具有混合消息机制的分层融合架构来处理这些标记，以获得适用于MMKGR的可转移特征。

Result: 在17个不同类型的多模态知识图谱上进行的实验表明，TOFU在性能上优于强知识图谱基础模型和多模态知识图谱推理的基础模型。

Conclusion: TOFU模型展示了在多模态知识图谱推理任务中强大的跨图谱泛化性能。

Abstract: Multi-modal knowledge graph reasoning (MMKGR) aims to predict the missing links by exploiting both graph structure information and multi-modal entity contents. Most existing works are designed for a transductive setting, which learns dataset-specific embeddings and struggles to generalize to new KGs. Recent knowledge graph foundation models (KGFMs) improve cross-KG transfer, but they mainly exploit structural patterns and ignore rich multi-modal signals. We address these gaps by proposing a token-based foundation model (TOFU) for MMKGR, which exhibits strong generalization across different MMKGs. TOFU discretizes structural, visual, and textual information into modality-specific tokens. TOFU then employs a hierarchical fusion architecture with mixture-of-message mechanisms, aiming to process these tokens and obtain transferable features for MMKGR. Experimental results on 17 transductive, inductive, and fully-inductive MMKGs show that TOFU consistently outperforms strong KGFM and MMKGR baselines, delivering strong performance on unseen MMKGs.

</details>


### [59] [Mitigating Gradient Inversion Risks in Language Models via Token Obfuscation](https://arxiv.org/abs/2602.15897)
*Xinguo Feng,Zhongkui Ma,Zihan Wang,Alsharif Abuadbba,Guangdong Bai*

Main category: cs.CL

TL;DR: GHOST 是一种基于 token 级别的混淆机制，通过在 token 空间中建立语义分割来对抗梯度 inversion 攻击，同时保持 embedding 和 gradient 空间的连通性。


<details>
  <summary>Details</summary>
Motivation: 现有的梯度扰动防御方法因保留了梯度、嵌入和 token 空间的语义相似性，容易受到梯度 inversion 攻击。GHOST 提供了一种新的防御方法，通过 token 级别的混淆，解除这些空间之间的内在联系。

Method: GHOST 方法首先通过多标准搜索过程识别语义上不同的候选 token，然后选择最优的替代 token，以最小化对训练关键特征的影响，同时保持与原始 token 内部输出的对齐。

Result: 跨多种模型架构和数据集，GHOST 显示出出色的隐私保护效果（恢复率低至 1%）和实用性（分类 F1 得分高达 0.92，困惑度降低 5.45）。

Conclusion: GHOST 能有效对抗最新的梯度 inversion 攻击和适应性攻击场景，证实了其在保护语言模型私有训练数据方面的优势。

Abstract: Training and fine-tuning large-scale language models largely benefit from collaborative learning, but the approach has been proven vulnerable to gradient inversion attacks (GIAs), which allow adversaries to reconstruct private training data from shared gradients. Existing defenses mainly employ gradient perturbation techniques, e.g., noise injection or gradient pruning, to disrupt GIAs' direct mapping from gradient space to token space. However, these methods often fall short due to the retention of semantics similarity across gradient, embedding, and token spaces. In this work, we propose a novel defense mechanism named GHOST (gradient shield with obfuscated tokens), a token-level obfuscation mechanism that neutralizes GIAs by decoupling the inherent connections across gradient, embedding, and token spaces. GHOST is built upon an important insight: due to the large scale of the token space, there exist semantically distinct yet embedding-proximate tokens that can serve as the shadow substitutes of the original tokens, which enables a semantic disconnection in the token space while preserving the connection in the embedding and gradient spaces. GHOST comprises a searching step, which identifies semantically distinct candidate tokens using a multi-criteria searching process, and a selection step, which selects optimal shadow tokens to ensure minimal disruption to features critical for training by preserving alignment with the internal outputs produced by original tokens. Evaluation across diverse model architectures (from BERT to Llama) and datasets demonstrates the remarkable effectiveness of GHOST in protecting privacy (as low as 1% in recovery rate) and preserving utility (up to 0.92 in classification F1 and 5.45 in perplexity), in both classification and generation tasks against state-of-the-art GIAs and adaptive attack scenarios.

</details>


### [60] [MultiCube-RAG for Multi-hop Question Answering](https://arxiv.org/abs/2602.15898)
*Jimeng Shi,Wei Hu,Runchu Tian,Bowen Jin,Wonbin Kweon,SeongKu Kang,Yunfan Kang,Dingqi Ye,Sizhe Zhou,Shaowen Wang,Jiawei Han*

Main category: cs.CL

TL;DR: 本文提出了一种基于本体的立方体结构——MultiCube-RAG，用于多跳问答，通过分解复杂的问题为简单子查询并在不同维度上顺序解决，提高了准确性并增强了可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的RAG方法难以准确捕捉多跳问答中的结构语义，而基于立方体结构的方法旨在通过多维度建模来解决这一问题。

Method: MultiCube-RAG方法利用基于立方体的结构，并将复杂的问题分解为一系列简单的子查询，每个立方体专注于特定类别的主题建模。

Result: 在四个多跳问答数据集上，MultiCube-RAG相比各种基线方法提高了响应准确率8.9%。

Conclusion: MultiCube-RAG在多跳问答中的表现优于现有方法，并且具有更高的效率和内生可解释性。

Abstract: Multi-hop question answering (QA) necessitates multi-step reasoning and retrieval across interconnected subjects, attributes, and relations. Existing retrieval-augmented generation (RAG) methods struggle to capture these structural semantics accurately, resulting in suboptimal performance. Graph-based RAGs structure such information in graphs, but the resulting graphs are often noisy and computationally expensive. Moreover, most methods rely on single-step retrieval, neglecting the need for multi-hop reasoning processes. Recent training-based approaches attempt to incentivize the large language models (LLMs) for iterative reasoning and retrieval, but their training processes are prone to unstable convergence and high computational overhead. To address these limitations, we devise an ontology-based cube structure with multiple and orthogonal dimensions to model structural subjects, attributes, and relations. Built on the cube structure, we propose MultiCube-RAG, a training-free method consisting of multiple cubes for multi-step reasoning and retrieval. Each cube specializes in modeling a class of subjects, so that MultiCube-RAG flexibly selects the most suitable cubes to acquire the relevant knowledge precisely. To enhance the query-based reasoning and retrieval, our method decomposes a complex multi-hop query into a set of simple subqueries along cube dimensions and conquers each of them sequentially. Experiments on four multi-hop QA datasets show that MultiCube-RAG improves response accuracy by 8.9% over the average performance of various baselines. Notably, we also demonstrate that our method performs with greater efficiency and inherent explainability.

</details>


### [61] [DocSplit: A Comprehensive Benchmark Dataset and Evaluation Approach for Document Packet Recognition and Splitting](https://arxiv.org/abs/2602.15958)
*Md Mofijul Islam,Md Sirajus Salekin,Nivedha Balakrishnan,Vincil C. Bishop,Niharika Jain,Spencer Romo,Bob Strahan,Boyi Xie,Diego A. Socolinsky*

Main category: cs.CL

TL;DR: 该研究提出了首个综合基准数据集DocSplit，用于评估大型语言模型处理文档包拆分任务的能力。DocSplit包括五个复杂度不同的数据集，覆盖多种文档类型、布局和多模态设置。通过全面实验发现，当前模型在复杂拆分任务上仍有较大性能差距。


<details>
  <summary>Details</summary>
Motivation: 由于现实世界应用中处理包含多个合订在一起的文档的异质文档包的需求，研究旨在填补文档包拆分任务的空白，以便推动文档理解能力的发展。

Method: 研究人员设计了一个名为DocSplit的第一综合性基准数据集，包含五个不同复杂度的数据集，旨在评估大型语言模型在多种文档类型、布局和多模态环境中的文档包拆分能力。同时，研究提出了新的评价指标，通过全面实验评估多模态模型在复杂拆分任务上的表现。

Result: 实验发现当前的多模态语言模型在处理复杂的文档包拆分任务上存在显著性能差距。该基准数据集和新提出的评估指标为未来在文档包处理方面的研究提供了系统性的框架。

Conclusion: 研究结果表明了目前大型语言模型在处理复杂文档包拆分任务上的不足，并为未来研究指明了方向。

Abstract: Document understanding in real-world applications often requires processing heterogeneous, multi-page document packets containing multiple documents stitched together. Despite recent advances in visual document understanding, the fundamental task of document packet splitting, which involves separating a document packet into individual units, remains largely unaddressed. We present the first comprehensive benchmark dataset, DocSplit, along with novel evaluation metrics for assessing the document packet splitting capabilities of large language models. DocSplit comprises five datasets of varying complexity, covering diverse document types, layouts, and multimodal settings. We formalize the DocSplit task, which requires models to identify document boundaries, classify document types, and maintain correct page ordering within a document packet. The benchmark addresses real-world challenges, including out-of-order pages, interleaved documents, and documents lacking clear demarcations. We conduct extensive experiments evaluating multimodal LLMs on our datasets, revealing significant performance gaps in current models' ability to handle complex document splitting tasks. The DocSplit benchmark datasets and proposed novel evaluation metrics provide a systematic framework for advancing document understanding capabilities essential for legal, financial, healthcare, and other document-intensive domains. We release the datasets to facilitate future research in document packet processing.

</details>


### [62] [A Curious Class of Adpositional Multiword Expressions in Korean](https://arxiv.org/abs/2602.16023)
*Junghyun Min,Na-Rae Han,Jena D. Hwang,Nathan Schneider*

Main category: cs.CL

TL;DR: 本文研究了韩语的功能性多词表达，即后置动词结构（PVCs），并提出了标注指南，旨在为未来在韩语多词短语助词方面的工作提供支持，并促进与跨语言框架的对齐。


<details>
  <summary>Details</summary>
Motivation: 韩语多词短语助词对于现有的跨语言注解框架来说缺少系统分析、标注资源和整合。

Method: 研究者使用了来自韩语维基百科的数据，探讨和分析了几种PVC表达，并将它们与其他结构相似的非多词表达和轻动词结构进行了对比。

Result: 通过对PVCs研究，作者提出了支持未来韩语多词助词工作以及跨语言框架对齐的标注指南。

Conclusion: 该研究促进了韩语多词短语助词领域的进展，为建立标准和促进跨语言注解研究提供了方向。

Abstract: Multiword expressions (MWEs) have been widely studied in cross-lingual annotation frameworks such as PARSEME. However, Korean MWEs remain underrepresented in these efforts. In particular, Korean multiword adpositions lack systematic analysis, annotated resources, and integration into existing multilingual frameworks. In this paper, we study a class of Korean functional multiword expressions: postpositional verb-based constructions (PVCs). Using data from Korean Wikipedia, we survey and analyze several PVC expressions and contrast them with non-MWEs and light verb constructions (LVCs) with similar structure. Building on this analysis, we propose annotation guidelines designed to support future work in Korean multiword adpositions and facilitate alignment with cross-lingual frameworks.

</details>


### [63] [CLAA: Cross-Layer Attention Aggregation for Accelerating LLM Prefill](https://arxiv.org/abs/2602.16054)
*Bradley McDanel,Steven Li,Harshit Khaitan*

Main category: cs.CL

TL;DR: 该研究提出了一个基于答案的Oracle来评估和改进预填充阶段的token重要性估计，从而降低了生成初始响应所花费的时间。


<details>
  <summary>Details</summary>
Motivation: 目前在长上下文LLM推理中，预填充阶段仍然是一个计算瓶颈。现有方法存在token重要性估计不稳定的问题，尤其是在不同层之间差异较大。

Method: 研究引入了一个基于答案的Oracle，通过测量生成答案对提示的关注度来定义token的真实重要性。这种方法能够揭示现有启发式方法中层间token重要性高度波动的问题，并提出跨层注意力聚合（CLAA）作为一个解决方案。

Result: 该研究揭示了现有启发式方法在不同层中token重要性估计存在的问题，并且通过CLAA方法显著减少了生成初始响应所需的时间，降低了Time-to-First-Token（TTFT）高达39%。

Conclusion: 该研究提出了一种改进方法来解决预填充阶段的计算瓶颈，通过基于答案的Oracle进行token重要性的准确估计，并实施了跨层注意力聚合方法，有效提升了LLM的推理效率。

Abstract: The prefill stage in long-context LLM inference remains a computational bottleneck. Recent token-ranking heuristics accelerate inference by selectively processing a subset of semantically relevant tokens. However, existing methods suffer from unstable token importance estimation, often varying between layers. Evaluating token-ranking quality independently from heuristic-specific architectures is challenging. To address this, we introduce an Answer-Informed Oracle, which defines ground-truth token importance by measuring attention from generated answers back to the prompt. This oracle reveals that existing heuristics exhibit high variance across layers: rankings can degrade sharply at specific layers, a failure mode invisible to end-to-end benchmarks. The diagnosis suggests a simple fix: aggregate scores across layers rather than relying on any single one. We implement this as Cross-Layer Attention Aggregation (CLAA), which closes the gap to the oracle upper bound and reduces Time-to-First-Token (TTFT) by up to 39\% compared to the Full KV Cache baseline.

</details>


### [64] [Surgical Activation Steering via Generative Causal Mediation](https://arxiv.org/abs/2602.16080)
*Aruna Sankaranarayanan,Amir Zur,Atticus Geiger,Dylan Hadfield-Menell*

Main category: cs.CL

TL;DR: GCM 是一种用于选择语言模型组件（如注意力头）以引导长格式响应中二元概念的技术，通过构造对比输入和响应数据集，量化每个组件在概念传递中的作用，选择最强的调节器。该方法在三个任务上应用于三个语言模型，并在使用稀疏注意力头集进行控制时优于相关探针基线。


<details>
  <summary>Details</summary>
Motivation: 随着语言模型在多领域应用中发挥越来越重要的作用，它们产生的长格式响应可能包含各种可能的偏差或不希望的行为模式。针对这一问题，GCM 提供了一种新颖的方法来定位和控制这些行为。”

Method: GCM 包括构造对比输入和响应数据集，量化每个模型组件在概念传递中的调节作用，从而选择最强的调节器来引导概念。该过程在三个不同任务和三个语言模型上进行了评估。

Result: 研究结果表明，GCM 成功地定位了长格式响应中表达的概念，并在使用稀疏注意力头集进行控制时优于相关探针基线的方法。

Conclusion: GCM 提供了一种有效的方法来定位和控制语言模型生成的长格式响应中的特定行为，具有广泛的应用潜力。

Abstract: Where should we intervene in a language model (LM) to control behaviors that are diffused across many tokens of a long-form response? We introduce Generative Causal Mediation (GCM), a procedure for selecting model components, e.g., attention heads, to steer a binary concept (e.g., talk in verse vs. talk in prose) from contrastive long-form responses. In GCM, we first construct a dataset of contrasting inputs and responses. Then, we quantify how individual model components mediate the contrastive concept and select the strongest mediators for steering. We evaluate GCM on three tasks--refusal, sycophancy, and style transfer--across three language models. GCM successfully localizes concepts expressed in long-form responses and consistently outperforms correlational probe-based baselines when steering with a sparse set of attention heads. Together, these results demonstrate that GCM provides an effective approach for localizing and controlling the long-form responses of LMs.

</details>


### [65] [Language Statistics and False Belief Reasoning: Evidence from 41 Open-Weight LMs](https://arxiv.org/abs/2602.16085)
*Sean Trott,Samuel Taylor,Cameron Jones,James A. Michaelov,Pamela D. Rivière*

Main category: cs.CL

TL;DR: 该研究通过评估41个开放源码语言模型的假信念任务表现，发现34%的语言模型对隐含的知识状态表现出敏感性，而更大的模型表现更好。此外，语言模型和人类都更倾向于在使用非实在谓词（'John thinks...'）提示知识状态时认为存在虚假信念，这表明语言模型能解读书面语言的分布统计特性，但可能无法完全解释人类在某些情境下的认知偏差。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过扩展对假信念任务的研究，利用更多的开放源码语言模型来检验关于人类社会认知的理论，并评估语言模型的能力。

Method: 研究复制并扩展了先前关于假信念任务的工作，评估了41种不同的开放源码语言模型。

Result: 34%的语言模型对隐含的知识状态表现出敏感性，且更大的模型表现更好。语言模型和人类在使用非实在谓词提示知识状态时都更倾向于认为有虚假信念。

Conclusion: 该研究强调，使用更广泛的语言模型样本可以更深入地测试关于人类认知的理论，并评估语言模型的能力。

Abstract: Research on mental state reasoning in language models (LMs) has the potential to inform theories of human social cognition--such as the theory that mental state reasoning emerges in part from language exposure--and our understanding of LMs themselves. Yet much published work on LMs relies on a relatively small sample of closed-source LMs, limiting our ability to rigorously test psychological theories and evaluate LM capacities. Here, we replicate and extend published work on the false belief task by assessing LM mental state reasoning behavior across 41 open-weight models (from distinct model families). We find sensitivity to implied knowledge states in 34% of the LMs tested; however, consistent with prior work, none fully ``explain away'' the effect in humans. Larger LMs show increased sensitivity and also exhibit higher psychometric predictive power. Finally, we use LM behavior to generate and test a novel hypothesis about human cognition: both humans and LMs show a bias towards attributing false beliefs when knowledge states are cued using a non-factive verb (``John thinks...'') than when cued indirectly (``John looks in the...''). Unlike the primary effect of knowledge states, where human sensitivity exceeds that of LMs, the magnitude of the human knowledge cue effect falls squarely within the distribution of LM effect sizes-suggesting that distributional statistics of language can in principle account for the latter but not the former in humans. These results demonstrate the value of using larger samples of open-weight LMs to test theories of human cognition and evaluate LM capacities.

</details>


### [66] [Updating Parametric Knowledge with Context Distillation Retains Post-Training Capabilities](https://arxiv.org/abs/2602.16093)
*Shankar Padmanabhan,Mustafa Omer Gul,Tanya Goyal*

Main category: cs.CL

TL;DR: 提出的DiSC方法通过分割训练示例的不同部分来简化知识蒸馏上下文，能够在学习新知识的同时减轻遗忘先前学习的能力，适用于多种后训练的大规模语言模型。


<details>
  <summary>Details</summary>
Motivation: 现有的后训练技术存在难以同时学习新知识并减轻遗忘先前学习能力的问题，DiSC方法旨在解决这一挑战。

Method: DiSC方法通过条件分割训练示例的不同部分来实现上下文蒸馏，最小化共享标记之间的KL散度，而不需在训练中采用明确的生成步骤。

Result: 实验结果表明，相较于传统的持续适应方法，DiSC方法能在学习新知识的同时有效减轻遗忘先前学习的能力，包括指令遵循、推理和事实知识等领域。

Conclusion: DiSC方法为后训练模型的持续知识适应提供了一种有效且直接的方式，展示了在多个领域适用的良好效果。

Abstract: Post-training endows pretrained LLMs with a variety of desirable skills, including instruction-following, reasoning, and others. However, these post-trained LLMs only encode knowledge up to a cut-off date, necessitating continual adaptation. Unfortunately, existing solutions cannot simultaneously learn new knowledge from an adaptation document corpora and mitigate the forgetting of earlier learned capabilities. To address this, we introduce Distillation via Split Contexts (DiSC), a simple context-distillation based approach for continual knowledge adaptation. \methodname~derives student and teacher distributions by conditioning on distinct segments of the training example and minimizes the KL divergence between the shared tokens. This allows us to efficiently apply context-distillation without requiring explicit generation steps during training. We run experiments on four post-trained models and two adaptation domains. Compared to prior finetuning and distillation methods for continual adaptation, DiSC consistently reports the best trade-off between learning new knowledge and mitigating forgetting of previously learned skills like instruction-following, reasoning, and factual knowledge.

</details>


### [67] [Missing-by-Design: Certifiable Modality Deletion for Revocable Multimodal Sentiment Analysis](https://arxiv.org/abs/2602.16144)
*Rong Fu,Wenxin Zhang,Ziming Wang,Chunlei Meng,Jiaxuan Lu,Jiekai Wu,Kangan Qian,Hao Zhang,Simon Fong*

Main category: cs.CL

TL;DR: Missing-by-Design (MBD) 提出了一种统一框架，用于撤销多模态情感分析的数据模态，它结合了结构化表示学习和参数修改可验证管道，适用于对隐私敏感的应用场景。


<details>
  <summary>Details</summary>
Motivation: 随着多模态系统越来越多地处理敏感的个人数据，能够在特定情况下撤销特定数据模态的能力成为了保护用户隐私和自主性的关键需求。

Method: MBD 框架通过学习属性感知嵌入并将生成器用于重建缺失的通道，来保护任务相关信息的同时撤销特定模态。删除请求时，框架使用可解释的候选选择和校准的高斯更新生成可机读的模态删除证书。

Result: MBD 在基准数据集上的实验显示，即使在输入不完整的情况下，它也能够保持强大的预测性能，并提供了实用的隐私-使用权衡。

Conclusion: MBD 框架为隐私敏感应用提供了有成效的选择性撤销机制，通过少量的微调而非完全重训来实现目标。

Abstract: As multimodal systems increasingly process sensitive personal data, the ability to selectively revoke specific data modalities has become a critical requirement for privacy compliance and user autonomy. We present Missing-by-Design (MBD), a unified framework for revocable multimodal sentiment analysis that combines structured representation learning with a certifiable parameter-modification pipeline. Revocability is critical in privacy-sensitive applications where users or regulators may request removal of modality-specific information. MBD learns property-aware embeddings and employs generator-based reconstruction to recover missing channels while preserving task-relevant signals. For deletion requests, the framework applies saliency-driven candidate selection and a calibrated Gaussian update to produce a machine-verifiable Modality Deletion Certificate. Experiments on benchmark datasets show that MBD achieves strong predictive performance under incomplete inputs and delivers a practical privacy-utility trade-off, positioning surgical unlearning as an efficient alternative to full retraining.

</details>


### [68] [Balancing Faithfulness and Performance in Reasoning via Multi-Listener Soft Execution](https://arxiv.org/abs/2602.16154)
*Nithin Sivakumaran,Shoubin Yu,Hyunji Lee,Yue Zhang,Ali Payani,Mohit Bansal,Elias Stengel-Eskin*

Main category: cs.CL

TL;DR: REMUL 提出了一种多参与者强化学习方法，旨在提高链式思考（CoT）的忠实度，同时保持任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有的CoT方法有时无法准确反映大型语言模型的真正计算过程，影响其解释能力。同时，优化可信度和解释性往往会降低任务性能。

Method: REMUL 方法包括生成者模型生成推理轨迹，该轨迹被调整长度并传递给一组听众模型执行，后者会继续生成轨迹直到得出答案。生成者会根据听众的理解能力获得奖励，并通过蒙面的监督微调来保证推理的正确性。

Result: 在多个推理测验中（包括 BIG-Bench Extra Hard、MuSR、ZebraLogicBench 和 FOLIO），REMUL 在三个忠实度衡量指标上表现出显著改进，并且整体准确度也有所提高。

Conclusion: REMUL 在提高CoT的忠实度和任务性能两方面都取得了显著成果，这是一种在多领域中具有良好推广性的方法，也带来了易理解性的改善。

Abstract: Chain-of-thought (CoT) reasoning sometimes fails to faithfully reflect the true computation of a large language model (LLM), hampering its utility in explaining how LLMs arrive at their answers. Moreover, optimizing for faithfulness and interpretability in reasoning often degrades task performance. To address this tradeoff and improve CoT faithfulness, we propose Reasoning Execution by Multiple Listeners (REMUL), a multi-party reinforcement learning approach. REMUL builds on the hypothesis that reasoning traces which other parties can follow will be more faithful. A speaker model generates a reasoning trace, which is truncated and passed to a pool of listener models who "execute" the trace, continuing the trace to an answer. Speakers are rewarded for producing reasoning that is clear to listeners, with additional correctness regularization via masked supervised finetuning to counter the tradeoff between faithfulness and performance. On multiple reasoning benchmarks (BIG-Bench Extra Hard, MuSR, ZebraLogicBench, and FOLIO), REMUL consistently and substantially improves three measures of faithfulness -- hint attribution, early answering area over the curve (AOC), and mistake injection AOC -- while also improving accuracy. Our analysis finds that these gains are robust across training domains, translate to legibility gains, and are associated with shorter and more direct CoTs.

</details>


### [69] [LLMs Exhibit Significantly Lower Uncertainty in Creative Writing Than Professional Writers](https://arxiv.org/abs/2602.16162)
*Peiqi Sui*

Main category: cs.CL

TL;DR: 本文探讨了LLM在创造性写作中的不确定性问题，发现人类写作的不确定性远高于模型生成的内容，且这一差距在创造性写作中尤为明显。当前的对齐策略使模型倾向于减少不确定性，这降低了写作的创造性。


<details>
  <summary>Details</summary>
Motivation: 当前的语言模型（LLM）在创造性写作上的表现往往被描述为乏味和充满陈词滥调。本文的研究动机在于揭示这种表现欠佳的原因之一，即不确定性被低估的问题，指出不确定性是创造性表达的必要条件，并探讨如何通过新的不确定性意识对齐范式来改进语言模型的表现。

Method: 研究者通过量化人类作者的故事与模型生成的续作之间的“不确定性差距”，并且使用信息论的方法对28种LLM进行了 controlled 分析，以证明人类写作的一致性和显著更高的不确定性，并分析了不同类型模型和不同写作领域的不确定性差距。

Result: 研究表明，人类写作显示出显著高于模型输出的不确定性水平，而且指令调优和推理模型比基础模型更倾向于加剧这一趋势。此外，这种不确定性差距在创造性写作中比功能性领域更为明显，并且与写作质量紧密相关。

Conclusion: 为了实现人类级别的创造性，需要新的不确定性意识对齐框架，以区分破坏性的幻觉和为文学丰富性所必需的建设性模糊性。

Abstract: We argue that uncertainty is a key and understudied limitation of LLMs' performance in creative writing, which is often characterized as trite and cliché-ridden. Literary theory identifies uncertainty as a necessary condition for creative expression, while current alignment strategies steer models away from uncertain outputs to ensure factuality and reduce hallucination. We formalize this tension by quantifying the "uncertainty gap" between human-authored stories and model-generated continuations. Through a controlled information-theoretic analysis of 28 LLMs on high-quality storytelling datasets, we demonstrate that human writing consistently exhibits significantly higher uncertainty than model outputs. We find that instruction-tuned and reasoning models exacerbate this trend compared to their base counterparts; furthermore, the gap is more pronounced in creative writing than in functional domains, and strongly correlates to writing quality. Achieving human-level creativity requires new uncertainty-aware alignment paradigms that can distinguish between destructive hallucinations and the constructive ambiguity required for literary richness.

</details>


### [70] [Beyond Learning: A Training-Free Alternative to Model Adaptation](https://arxiv.org/abs/2602.16189)
*Namkyung Yoon,Kyeonghyun Yoo,Wooyong Jung,Sanghong Kim,Hwangnam Kim*

Main category: cs.CL

TL;DR: 本文通过激活分析识别了局部激活模块，并将其移植到目标模型中，从而实现在不需额外训练的情况下即时提高模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法克服语言模型挑战资源密集，所以需要更高效的方法来获得即时改进。

Method: 首先通过基于激活的分析识别出一组本地模块，然后将这些模块移植到目标模型中。

Result: 在不同条件下，移植选定的激活模块可以显著提高旧模型性能，有时可达基线的两倍，并实现基线以上的恢复。与基础模型及其指令调整版本之间的移植实验显示，改进的模型性能可达基线的2.33倍。

Conclusion: 研究表明，语言模型中的模块化可以实现有意义的能力转移，通过植入高度局部化的模块。这项工作已为语言模型提供任务局部的模块化实验证据，并提出了一个新的研究领域：模型移植。

Abstract: Despite the continuous research and evolution of language models, they sometimes underperform previous versions. Existing approaches to overcome these challenges are resource-intensive, highlighting the need for alternatives that enable immediate action. We assume that each language model has a local module inside that is suitable for a specific function. First, this work identifies a set of modules showing consistent and local activation changes under an inference workload through activation-based analysis. Subsequently, we transplant an internal module that is properly activated for a specific task into the target model, leading to immediate and measurable functional changes without additional training or fine-tuning. To experimentally demonstrate the effectiveness of the transplant technique, we quantify the relationship between transplant strength and performance improvement under different conditions for two language models. In the cross-generation setting, we find that transplanting activation-selected modules can substantially improve the underperforming model, reaching up to twice the target baseline and achieving gap-based recovery above 100%. Moreover, in transplant experiments between a base model and its instruction-tuned counterpart, transplantation improves the underperforming model toward the stronger baseline, yielding up to about 2.33 times the target baseline with gap-based recovery reaching up to 100% in the best case. These results show that meaningful capacity transfer can be realized through the implantation of highly localized modules implied by language models. Overall, this work provides empirical evidence for task-localized modularity in language models and presents a new research area: model transplantation.

</details>


### [71] [The Validity of Coreference-based Evaluations of Natural Language Understanding](https://arxiv.org/abs/2602.16200)
*Ian Porada*

Main category: cs.CL

TL;DR: 本文通过对现有核心共指评价实践的扩展，并关注评价结果的一致性和冲突性，提出了一个新的测试系统如何推断事件相对可能性的评价方法。研究显示，当今的语言模型在标准基准上的表现较好，但在评估环境略有修改时，其泛化能力仍有局限。该研究澄清了当前NLP范式的优点与局限，并为未来的工作提供了方向。


<details>
  <summary>Details</summary>
Motivation: 现有核心共指评价实践中存在测量有效性的问题，包括争议性和收敛性。本文旨在通过改进现有实践，以更好地明确可以得出哪些结论。

Method: 作者分析了标准的核心共指评价，并提出了一个新方法来测试系统推断事件相对可能性的能力。通过扩展评价，作者发现了语言模型在特定领域和共指类型的基线系统中的改进，同时也指出它们在评估条件变化时的表现仍不太稳定。

Result: 研究发现，当语言模型面临评估环境的微小修改时，它们往往无法表现出预期的人类能力。语言模型在标准基准上的表现有所提高，但在其他方面仍有局限。

Conclusion: 这项研究澄清了当前NLP范式的优点，如模型在基准测试中的改进精度，以及局限性，如测量有效性问题。同时，论文还提出了未来改进评价方法和构建更可泛化的系统的方向。

Abstract: In this thesis, I refine our understanding as to what conclusions we can reach from coreference-based evaluations by expanding existing evaluation practices and considering the extent to which evaluation results are either converging or conflicting. First, I analyze standard coreference evaluations and show that their design often leads to non-generalizable conclusions due to issues of measurement validity - including contestedness (multiple, competing definitions of coreference) and convergent validity (evaluation results that rank models differently across benchmarks). Second, I propose and implement a novel evaluation focused on testing systems' ability to infer the relative plausibility of events, a key aspect of resolving coreference. Through this extended evaluation, I find that contemporary language models demonstrate strong performance on standard benchmarks - improving over earlier baseline systems within certain domains and types of coreference - but remain sensitive to the evaluation conditions: they often fail to generalize in ways one would expect a human to be capable of when evaluation contexts are slightly modified. Taken together, these findings clarify both the strengths, such as improved accuracy over baselines on widely used evaluations, and the limitations of the current NLP paradigm, including weaknesses in measurement validity, and suggest directions for future work in developing better evaluation methods and more genuinely generalizable systems.

</details>


### [72] [Long-Tail Knowledge in Large Language Models: Taxonomy, Mechanisms, Interventions and Implications](https://arxiv.org/abs/2602.16201)
*Sanket Badhe,Deep Shah,Nehal Kathrotia*

Main category: cs.CL

TL;DR: 本文构建了一个结构化的长尾知识分析框架，从定义、训练和推理过程中知识的丢失与失真、技术干预以及公平性等多维度探讨了大型语言模型中长尾知识的问题，并提出了评价实践中的缺陷，最后指出了隐私、可持续性和治理方面的开放挑战。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决大型语言模型在处理低频、领域特定、文化和时态知识方面的持续失败问题，这些问题尚未得到充分理解。

Method: 作者通过构建一个结构化的分析框架，综合了技术和社会技术视角的相关先前工作，从定义、训练和推理过程中知识的丢失与失真、技术干预以及公平性等多维度进行分析。

Result: 作者提出了一种综合的分析框架，该框架有助于理解长尾知识在大型语言模型中的定义、丢失、评估和体现。此外，还指出了现有的评估实践如何模糊尾部行为并影响罕见但重要的失败的责任问题。

Conclusion: 本文指出了与隐私、可持续性和治理相关的开放挑战，这些挑战限制了长尾知识在部署的语言模型系统中的表现。

Abstract: Large language models (LLMs) are trained on web-scale corpora that exhibit steep power-law distributions, in which the distribution of knowledge is highly long-tailed, with most appearing infrequently. While scaling has improved average-case performance, persistent failures on low-frequency, domain-specific, cultural, and temporal knowledge remain poorly characterized. This paper develops a structured taxonomy and analysis of long-Tail Knowledge in large language models, synthesizing prior work across technical and sociotechnical perspectives.
  We introduce a structured analytical framework that synthesizes prior work across four complementary axes: how long-Tail Knowledge is defined, the mechanisms by which it is lost or distorted during training and inference, the technical interventions proposed to mitigate these failures, and the implications of these failures for fairness, accountability, transparency, and user trust. We further examine how existing evaluation practices obscure tail behavior and complicate accountability for rare but consequential failures. The paper concludes by identifying open challenges related to privacy, sustainability, and governance that constrain long-Tail Knowledge representation. Taken together, this paper provides a unifying conceptual framework for understanding how long-Tail Knowledge is defined, lost, evaluated, and manifested in deployed language model systems.

</details>


### [73] [Are LLMs Ready to Replace Bangla Annotators?](https://arxiv.org/abs/2602.16241)
*Md. Najib Hasan,Touseef Hasan,Souvika Sarkar*

Main category: cs.CL

TL;DR: 研究发现，对于低资源语言和敏感标注任务，大型语言模型的标注质量并不随模型规模的增加而提升，且小而任务对齐的模型在一致性上表现更好。


<details>
  <summary>Details</summary>
Motivation: 本文旨在研究大型语言模型在低资源和敏感标记任务（如孟加拉语仇恨言论标注）中的表现，以及它们作为自动标记员的可靠性和无偏性。

Method: 使用统一评估框架对17个大型语言模型进行了系统的基准测试。

Result: 研究发现，模型规模的增加并不一定能保证标注质量的提高，小且任务对齐的模型在一致性方面表现更好，且存在标记员偏差和模型判断的不稳定性。

Conclusion: 提示当前的大语言模型在对敏感标注任务，特别是在低资源语言中存在重要限制，建议在部署前要进行谨慎评估。

Abstract: Large Language Models (LLMs) are increasingly used as automated annotators to scale dataset creation, yet their reliability as unbiased annotators--especially for low-resource and identity-sensitive settings--remains poorly understood. In this work, we study the behavior of LLMs as zero-shot annotators for Bangla hate speech, a task where even human agreement is challenging, and annotator bias can have serious downstream consequences. We conduct a systematic benchmark of 17 LLMs using a unified evaluation framework. Our analysis uncovers annotator bias and substantial instability in model judgments. Surprisingly, increased model scale does not guarantee improved annotation quality--smaller, more task-aligned models frequently exhibit more consistent behavior than their larger counterparts. These results highlight important limitations of current LLMs for sensitive annotation tasks in low-resource languages and underscore the need for careful evaluation before deployment.

</details>


### [74] [Aladdin-FTI @ AMIYA Three Wishes for Arabic NLP: Fidelity, Diglossia, and Multidialectal Generation](https://arxiv.org/abs/2602.16290)
*Jonathan Mutal,Perla Al Almaoui,Simon Hengchen,Pierrette Bouillon*

Main category: cs.CL

TL;DR: 本研究提出了Aladdin-FTI系统，旨在生成和翻译阿拉伯方言，并支持摩洛哥、埃及、巴勒斯坦、叙利亚和沙特阿拉伯方言的文本生成与互译以及与现代标准阿拉伯语和英语的双向翻译。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯方言长期以来在自然语言处理（NLP）研究中存在不足，因为它们没有标准化且高度变异，这给计算建模带来了挑战。基于大型语言模型（LLMs）的最新进展，本研究旨在通过将阿拉伯语建模为多中心语言而非单一系统，填补这一研究空白。

Method: 该系统支持在摩洛哥、埃及、巴勒斯坦、叙利亚和沙特阿拉伯方言之间进行文本生成与互译，同时也支持与现代标准阿拉伯语和英语的双向翻译。模型采用了最新的机器学习技术，能够处理多种方言和语言间的复杂转换。

Result: 经过实验和验证，Aladdin-FTI系统在多种方言和语言之间的生成和翻译任务上表现出良好的性能，为阿拉伯方言的NLP研究做出了贡献。

Conclusion: 本研究成功地开发了一种能够处理多种阿拉伯方言的生成和翻译系统，为扩大阿拉伯语言的NLP应用范围提供了新的可能性。

Abstract: Arabic dialects have long been under-represented in Natural Language Processing (NLP) research due to their non-standardization and high variability, which pose challenges for computational modeling. Recent advances in the field, such as Large Language Models (LLMs), offer promising avenues to address this gap by enabling Arabic to be modeled as a pluricentric language rather than a monolithic system. This paper presents Aladdin-FTI, our submission to the AMIYA shared task. The proposed system is designed to both generate and translate dialectal Arabic (DA). Specifically, the model supports text generation in Moroccan, Egyptian, Palestinian, Syrian, and Saudi dialects, as well as bidirectional translation between these dialects, Modern Standard Arabic (MSA), and English. The code and trained model are publicly available.

</details>


### [75] [MultiCW: A Large-Scale Balanced Benchmark Dataset for Training Robust Check-Worthiness Detection Models](https://arxiv.org/abs/2602.16298)
*Martin Hyben,Sebastian Kula,Jan Cegin,Jakub Simko,Ivan Srba,Robert Moro*

Main category: cs.CL

TL;DR: 该研究引入了一个多语种数据集MultiCW，用于检查值得核实的声明检测，并且评估了多种预训练模型在这项任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 随着大规模语言模型（LLMs）在媒体行业的应用，自动化支持快速识别值得核实的声明成为事实核查的关键步骤。然而，这方面的自动支持尚不充分。研究人员希望通过构建一个多语言基准数据集来促进自动化事实核查的发展。

Method: 研究团队构建了一个涵盖16种语言、7个主题领域和2种文本风格的平衡多语言数据集MultiCW，并评估了3种预训练模型和15种商用或开源大型语言模型在零样本设置下的表现。

Result: 研究结果显示，经过微调的模型在声明分类任务中普遍优于零样本大型语言模型，并在不同语言、领域和文本风格下展现出强大的泛化能力。

Conclusion: MultiCW数据集提供了一个严格多语言资源，有助于推进自动化事实核查，并促进了微调模型和最新大型语言模型之间在值得核实声明检测任务上的系统比较。

Abstract: Large Language Models (LLMs) are beginning to reshape how media professionals verify information, yet automated support for detecting check-worthy claims a key step in the fact-checking process remains limited. We introduce the Multi-Check-Worthy (MultiCW) dataset, a balanced multilingual benchmark for check-worthy claim detection spanning 16 languages, 7 topical domains, and 2 writing styles. It consists of 123,722 samples, evenly distributed between noisy (informal) and structured (formal) texts, with balanced representation of check-worthy and non-check-worthy classes across all languages. To probe robustness, we also introduce an equally balanced out-of-distribution evaluation set of 27,761 samples in 4 additional languages. To provide baselines, we benchmark 3 common fine-tuned multilingual transformers against a diverse set of 15 commercial and open LLMs under zero-shot settings. Our findings show that fine-tuned models consistently outperform zero-shot LLMs on claim classification and show strong out-of-distribution generalization across languages, domains, and styles. MultiCW provides a rigorous multilingual resource for advancing automated fact-checking and enables systematic comparisons between fine-tuned models and cutting-edge LLMs on the check-worthy claim detection task.

</details>


### [76] [IndicEval: A Bilingual Indian Educational Evaluation Framework for Large Language Models](https://arxiv.org/abs/2602.16467)
*Saurabh Bharti,Gaurav Azad,Abhinaw Jagtap,Nachiket Tapas*

Main category: cs.CL

TL;DR: IndicEval 是一个基于实际高考题目的可扩展基准平台，用于评估大型语言模型在多语言环境下的推理和适应能力，揭示了 Chain-of-Thought 提示策略在提高准确性方面的优势，以及多语言环境下的性能差异和双语言推理的挑战。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型（LLMs）的快速发展，需要一个评估框架来反映学术严谨性和多语言复杂性。因此，提出了 IndicEval，旨在通过真实高风险考试问题评估 LLM 的表现。

Method: IndicEval 平台使用零样本、少样本和链式思考提示策略自动评估模型，并支持模块化整合新模型和语言。

Result: 实验表明，链式思考提示策略在提高推理准确性方面表现出一致优势，但在高复杂度考试中，不同模型之间仍存在显著性能差异，并且在零样本条件下，汉语和英语中的准确性存在显著下降。

Conclusion: 这些结果强调了双语言推理和领域迁移方面持续存在的差距，并为多语言教育环境中合理评估 LLM 提供了一个实践导向和可扩展的基础。

Abstract: The rapid advancement of large language models (LLMs) necessitates evaluation frameworks that reflect real-world academic rigor and multilingual complexity. This paper introduces IndicEval, a scalable benchmarking platform designed to assess LLM performance using authentic high-stakes examination questions from UPSC, JEE, and NEET across STEM and humanities domains in both English and Hindi. Unlike synthetic benchmarks, IndicEval grounds evaluation in real examination standards, enabling realistic measurement of reasoning, domain knowledge, and bilingual adaptability. The framework automates assessment using Zero-Shot, Few-Shot, and Chain-of-Thought (CoT) prompting strategies and supports modular integration of new models and languages. Experiments conducted on Gemini 2.0 Flash, GPT-4, Claude, and LLaMA 3-70B reveal three major findings. First, CoT prompting consistently improves reasoning accuracy, with substantial gains across subjects and languages. Second, significant cross-model performance disparities persist, particularly in high-complexity examinations. Third, multilingual degradation remains a critical challenge, with marked accuracy drops in Hindi compared to English, especially under Zero-Shot conditions. These results highlight persistent gaps in bilingual reasoning and domain transfer. Overall, IndicEval provides a practice-oriented, extensible foundation for rigorous, equitable evaluation of LLMs in multilingual educational settings and offers actionable insights for improving reasoning robustness and language adaptability.

</details>


### [77] [Training Models on Dialects of Translationese Shows How Lexical Diversity and Source-Target Syntactic Similarity Shape Learning](https://arxiv.org/abs/2602.16469)
*Jenny Kunz*

Main category: cs.CL

TL;DR: 该论文研究了使用机器翻译数据训练的小型英语语言模型受到源语言的影响，发现词汇多样性对模型性能影响较大，而句法性能则与源语言的类型学相似性密切相关。


<details>
  <summary>Details</summary>
Motivation: 由于机器翻译数据在多语言自然语言处理中广泛使用，但翻译文本与母语文本存在系统性差异，即翻译语病，本文旨在理解不同源语言的翻译文本如何影响模型的语言接受性和语言建模。

Method: 本文对来自24种类型学和资源多样性不同的源语言的英语翻译文本进行建模，通过分析母语和语料库属性对模型学习的影响，探究源语言对模型行为的具体影响。

Result: 实验结果表明，母语对模型行为有明确的影响：总体困惑度主要由翻译语料库的词汇多样性驱动，而句法性能则与给定数据量下源语言与英语的类型学相似性密切相关。

Conclusion: 研究指出了解决翻译语病和改进多语言NLP模型性能的关键在于理解和利用不同源语言的特性和语料库特征。

Abstract: Machine-translated data is widely used in multilingual NLP, particularly when native text is scarce. However, translated text differs systematically from native text. This phenomenon is known as translationese, and it reflects both traces of the source language and characteristic properties of translation itself. In this paper, we study how training on machine-translated data affects small English language models, focusing on how translationese from different source languages shapes linguistic acceptability judgments and language modelling for different domains. We train models on English text translated from 24 typologically and resource-diverse source languages, enabling a systematic analysis of how source language and corpus properties influence what models learn. Our results show that the source language has a clear impact on model behavior: general perplexity is more driven by the lexical diversity of the translated corpus, while grammatical performance is strongly correlated to typological similarity to English, given enough data.

</details>


### [78] [Optimizing Soft Prompt Tuning via Structural Evolution](https://arxiv.org/abs/2602.16500)
*Zhenzhen Huang,Chaoning Zhang,Haoyu Bian,Songbo Zhang,Chi-lok Andy Tai,Jiaquan Zhang,Caiyan Qin,Jingjing Qu,Yalan Ye,Yang Yang,Heng Tao Shen*

Main category: cs.CL

TL;DR: 该研究提出了基于拓扑形态演化的软提示调优优化方法，通过稳定和紧凑的提示实现更好的下游性能，并构建了一种新的损失函数TSLoss，以引导模型学习结构性稳定的适应。


<details>
  <summary>Details</summary>
Motivation: 现有的软提示依赖高维、隐式表示，缺乏明确语义和可追踪的训练行为，这限制了其可解释性。本文研究旨在解决这一限制。

Method: 本研究采用拓扑数据分析方法中的持久同调，量化软提示在连续参数空间中的结构性表示及其训练过程的变化。基于这一观察，构建了一种新的损失函数TSLoss，该函数通过量化参数间的连接性和冗余性来引导模型学习结构性稳定的适应。

Result: 实验表明，使用TSLoss进行训练可以加速收敛并提高调优性能，提供了一种结构和拓扑视角下解释和优化软提示调优的方法。

Conclusion: 本研究提出的TSLoss能够提供一种更可解释的软提示调优方法，通过训练过程的结构和拓扑特性来优化模型表现。

Abstract: Soft prompt tuning leverages continuous embeddings to capture task-specific information in large pre-trained language models (LLMs), achieving competitive performance in few-shot settings. However, soft prompts rely on high-dimensional, implicit representations and lack explicit semantics and traceable training behaviors, which limits their interpretability. To address this limitation, we propose a soft prompt tuning optimization method based on topological morphological evolution. Specifically, we employ persistent homology from topological data analysis (TDA) to quantify the structural representations of soft prompts in continuous parameter space and their training process evolution. Quantitative analysis shows that topologically stable and compact soft prompts achieve better downstream performance. Based on this empirical observation, we construct a loss function for optimizing soft prompt tuning, termed Topological Soft Prompt Loss (TSLoss). TSLoss guides the model to learn structurally stable adaptations by quantifying inter-parameter connectivity and redundancy. Extensive experiments show that training with TSLoss accelerates convergence and improves tuning performance, providing an interpretable method to understand and optimize soft prompt tuning from structural and topological perspectives.

</details>


### [79] [Supercharging Agenda Setting Research: The ParlaCAP Dataset of 28 European Parliaments and a Scalable Multilingual LLM-Based Classification](https://arxiv.org/abs/2602.16516)
*Taja Kuzman Pungeršek,Peter Rupnik,Daniela Širinić,Nikola Ljubešić*

Main category: cs.CL

TL;DR: 本文介绍了ParlaCAP数据集，这是一个用于分析欧洲议会议程设定的大规模数据集，并提出了一种成本效益高的方法来构建特定领域的政策主题分类器。通过使用大型语言模型和多语言编码模型，该研究展示了所提出的方法能够生成针对目标领域定制的分类器。


<details>
  <summary>Details</summary>
Motivation: 由于欧洲各议会的数据量庞大且包含多种语言，因此迫切需要一种可扩展且成本效益高的方法来处理这些数据并从中提取有价值的信息，如议程中的政策议题分布、情感模式以及性别差异等。

Method: 该研究采用了比较议程项目(CAP)模式对ParlaMint语料库（超过800万条来自28个欧洲国家议会演讲的多语言文本）进行了分析。采用教师-学生框架，其中高性能的大型语言模型标注领域内训练数据，并通过这些注释对多语言编码器模型进行微调。同时，数据集提供了丰富的演讲者和政党元数据，以及来自ParlaSent多语言变换器模型的情感预测，以支持跨国家的政治注意力和代表性的比较研究。

Result: 该研究的分类器与领域内的人类标注者的标注协议相似，且性能优于基于手动标注但领域外的数据训练的现有CAP分类器。此外，该数据集还支持在公共政策主题分配、演讲中的情感模式以及性别在政策的关注差异方面的研究。

Conclusion: 该研究的成功表明了ParlaCAP数据集和构建领域的政策主题分类器的方法具有广泛的应用前景，能够为更多的政治研究人员提供支持。

Abstract: This paper introduces ParlaCAP, a large-scale dataset for analyzing parliamentary agenda setting across Europe, and proposes a cost-effective method for building domain-specific policy topic classifiers. Applying the Comparative Agendas Project (CAP) schema to the multilingual ParlaMint corpus of over 8 million speeches from 28 parliaments of European countries and autonomous regions, we follow a teacher-student framework in which a high-performing large language model (LLM) annotates in-domain training data and a multilingual encoder model is fine-tuned on these annotations for scalable data annotation. We show that this approach produces a classifier tailored to the target domain. Agreement between the LLM and human annotators is comparable to inter-annotator agreement among humans, and the resulting model outperforms existing CAP classifiers trained on manually-annotated but out-of-domain data. In addition to the CAP annotations, the ParlaCAP dataset offers rich speaker and party metadata, as well as sentiment predictions coming from the ParlaSent multilingual transformer model, enabling comparative research on political attention and representation across countries. We illustrate the analytical potential of the dataset with three use cases, examining the distribution of parliamentary attention across policy topics, sentiment patterns in parliamentary speech, and gender differences in policy attention.

</details>


### [80] [Align Once, Benefit Multilingually: Enforcing Multilingual Consistency for LLM Safety Alignment](https://arxiv.org/abs/2602.16660)
*Yuyan Bu,Xiaohao Liu,ZhaoXing Ren,Yaodong Yang,Juntao Dai*

Main category: cs.CL

TL;DR: 提出了一种资源高效的方法来提高多语言安全对齐，通过插拔式的多语言一致性（MLC）损失，改进多语言表示向量之间的共线性，实现无需目标语言大量监督的情况下，跨越多种语言的对齐。


<details>
  <summary>Details</summary>
Motivation: 最近的努力将对齐扩展到其他语言需要大量的资源，而且这对扩展性构成了限制。为解决这一问题，本文提出了一种资源高效的方法。

Method: 引入了一种插拔式的多语言一致性（MLC）损失，该损失可以整合到现有的单语言对齐管道中。通过改进多语言表示向量之间的共线性，鼓励多语言语义级的方向一致性，并同时对齐多种语言，而无需额外的低资源语言响应级别监督。

Result: 该方法在不同的模型架构和对齐范式中得到了验证，并且证明了其在有限的模型泛化性影响下有效提高多语言安全。

Conclusion: 进一步的语言和任务评估表明，该方法在跨语言泛化方面得到了改进，提出了在有限监督下实现多语言一致性对齐的实用方法。

Abstract: The widespread deployment of large language models (LLMs) across linguistic communities necessitates reliable multilingual safety alignment. However, recent efforts to extend alignment to other languages often require substantial resources, either through large-scale, high-quality supervision in the target language or through pairwise alignment with high-resource languages, which limits scalability. In this work, we propose a resource-efficient method for improving multilingual safety alignment. We introduce a plug-and-play Multi-Lingual Consistency (MLC) loss that can be integrated into existing monolingual alignment pipelines. By improving collinearity between multilingual representation vectors, our method encourages directional consistency at the multilingual semantic level in a single update. This allows simultaneous alignment across multiple languages using only multilingual prompt variants without requiring additional response-level supervision in low-resource languages. We validate the proposed method across different model architectures and alignment paradigms, and demonstrate its effectiveness in enhancing multilingual safety with limited impact on general model utility. Further evaluation across languages and tasks indicates improved cross-lingual generalization, suggesting the proposed approach as a practical solution for multilingual consistency alignment under limited supervision.

</details>


### [81] [Utility-Preserving De-Identification for Math Tutoring: Investigating Numeric Ambiguity in the MathEd-PII Benchmark Dataset](https://arxiv.org/abs/2602.16571)
*Zhuqian Zhou,Kirk Vanacore,Bakhtawar Ahtisham,Jinsook Lee,Doug Pietrzak,Daryl Hedley,Jorge Dias,Chris Shaw,Ruth Schäfer,René F. Kizilcec*

Main category: cs.CL

TL;DR: 该研究提出了一个针对数学辅导对话中的PII检测的新基准数据集MathEd-PII，通过密度基分割方法验证了数学密集区域的过度红避问题，并展示了基于数学领域的提示能够显著提高检测性能，从而证明了保留分析用途的数据脱敏需要领域Aware的方法。


<details>
  <summary>Details</summary>
Motivation: 在数学辅导对话数据中，通用的个人可识别信息（PII）检测系统经常导致核心教学内容被过度脱敏，这降低了数据集的使用价值。研究旨在探索如何在保护隐私的同时保留教育内容的有效性。

Method: 通过一个基于LLM的工作流创建了一个包含1000次辅导会话的基准数据集MathEd-PII。利用密度基分割方法分析PII红避的情况，并比较了四种不同的检测策略：基准（Presidio）和基于LLM的基本、数学意识和分割意识的提示策略。

Result: 统计结果显示，与基准策略相比，数学意识的提示显著提高了检测性能（F1分数提升了2倍多）。此外，数学意识提示减少了数字错误的红避，证实了领域上下文对于数据脱敏以保留分析用途的重要性。

Conclusion: 该研究提出了一个用于数学辅导对话中保护隐私的新基准数据集，同时证明了域上下文对于在教学数据脱敏中保留分析用途的重要性。

Abstract: Large-scale sharing of dialogue-based data is instrumental for advancing the science of teaching and learning, yet rigorous de-identification remains a major barrier. In mathematics tutoring transcripts, numeric expressions frequently resemble structured identifiers (e.g., dates or IDs), leading generic Personally Identifiable Information (PII) detection systems to over-redact core instructional content and reduce dataset utility. This work asks how PII can be detected in math tutoring transcripts while preserving their educational utility. To address this challenge, we investigate the "numeric ambiguity" problem and introduce MathEd-PII, the first benchmark dataset for PII detection in math tutoring dialogues, created through a human-in-the-loop LLM workflow that audits upstream redactions and generates privacy-preserving surrogates. The dataset contains 1,000 tutoring sessions (115,620 messages; 769,628 tokens) with validated PII annotations. Using a density-based segmentation method, we show that false PII redactions are disproportionately concentrated in math-dense regions, confirming numeric ambiguity as a key failure mode. We then compare four detection strategies: a Presidio baseline and LLM-based approaches with basic, math-aware, and segment-aware prompting. Math-aware prompting substantially improves performance over the baseline (F1: 0.821 vs. 0.379) while reducing numeric false positives, demonstrating that de-identification must incorporate domain context to preserve analytic utility. This work provides both a new benchmark and evidence that utility-preserving de-identification for tutoring data requires domain-aware modeling.

</details>


### [82] [CitiLink-Summ: Summarization of Discussion Subjects in European Portuguese Municipal Meeting Minutes](https://arxiv.org/abs/2602.16607)
*Miguel Marques,Ana Luísa Fernandes,Ana Filipa Pacheco,Rute Rebouças,Inês Cantante,José Isidro,Luís Filipe Cunha,Alípio Jorge,Nuno Guimarães,Sérgio Nunes,António Leal,Purificação Silvano,Ricardo Campos*

Main category: cs.CL

TL;DR: 本文介绍了CitiLink-Summ，一个包含100份葡语市政会议记录和2322个手动撰写总结的新数据集，旨在解决市政会议记录自动摘要领域中缺乏高质量标注数据的问题。作者使用先进的生成模型和大语言模型进行实验，并提供了在词汇和语义层面的评估。


<details>
  <summary>Details</summary>
Motivation: 现存关于市政会议记录摘要的研究较少，尤其是在资源有限的语言中。这些文件的特点增加了最先进的模型难以有效处理的复杂性。因此，需要建立一套新的数据集来支持相关研究。

Method: 本文研制了一个新的Corpus——CitiLink-Summ，该Corpus包含100份葡萄牙语市政会议记录文档和2322个由人类手工撰写的主题总结。同时，作者还利用最先进的生成模型和大语言模型进行实验，并使用ROUGE, BLEU, METEOR, BERTScore等评价度量标准对模型结果进行了评估。

Result: CitiLink-Summ数据集为欧洲葡萄牙语市政领域的自动摘要提供了一个基准，标志着欧洲葡萄牙语第一个城市领域摘要的数据集。提出的自动摘要模型在多个评价指标上取得了初步成果，这可能有助于未来针对行政文本的NLP研究。

Conclusion: 本文的贡献在于提出了一个新的数据集和基准，这将促进NLP在复杂行政文本上的研究，并为后续工作提供了参考依据。

Abstract: Municipal meeting minutes are formal records documenting the discussions and decisions of local government, yet their content is often lengthy, dense, and difficult for citizens to navigate. Automatic summarization can help address this challenge by producing concise summaries for each discussion subject. Despite its potential, research on summarizing discussion subjects in municipal meeting minutes remains largely unexplored, especially in low-resource languages, where the inherent complexity of these documents adds further challenges. A major bottleneck is the scarcity of datasets containing high-quality, manually crafted summaries, which limits the development and evaluation of effective summarization models for this domain. In this paper, we present CitiLink-Summ, a new corpus of European Portuguese municipal meeting minutes, comprising 100 documents and 2,322 manually hand-written summaries, each corresponding to a distinct discussion subject. Leveraging this dataset, we establish baseline results for automatic summarization in this domain, employing state-of-the-art generative models (e.g., BART, PRIMERA) as well as large language models (LLMs), evaluated with both lexical and semantic metrics such as ROUGE, BLEU, METEOR, and BERTScore. CitiLink-Summ provides the first benchmark for municipal-domain summarization in European Portuguese, offering a valuable resource for advancing NLP research on complex administrative texts.

</details>


### [83] [ColBERT-Zero: To Pre-train Or Not To Pre-train ColBERT models](https://arxiv.org/abs/2602.16609)
*Antoine Chaffin,Luca Arnaboldi,Amélie Chatelain,Florent Krzakala*

Main category: cs.CL

TL;DR: 该研究通过大规模的多向量预训练超过了单一向量模型的知识蒸馏方法，展示了使用公共数据完全预训练的ColBERT-Zero模型的新最先进的性能。此外，研究还发现，在小规模知识蒸馏之前添加监督步骤可以显著提高性能。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的多向量模型主要通过在强单向量模型上进行少量的知识蒸馏训练获得。该研究旨在探索多向量模型的大规模预训练，并验证其在接近完全预训练效果方面的潜力，以及在现有模型复用时修改微调和预训练设置的必要性。

Method: 研究通过大规模预训练、知识蒸馏及监督学习技术，对多向量模型进行了训练和评估，比较了不同预训练方法的结果。

Result: 研究发现使用公共数据的完全多向量预训练的ColBERT-Zero模型表现出色，优于使用更高质量数据的较先进模型。此外，添加监督步骤的混合训练方法也能显著提高模型性能。

Conclusion: 该研究证明了多向量预训练的潜力，并提出了一种新的高效训练策略，用于提高现有模型的性能。

Abstract: Current state-of-the-art multi-vector models are obtained through a small Knowledge Distillation (KD) training step on top of strong single-vector models, leveraging the large-scale pre-training of these models. In this paper, we study the pre-training of multi-vector models and show that large-scale multi-vector pre-training yields much stronger multi-vector models. Notably, a fully ColBERT-pre-trained model, ColBERT-Zero, trained only on public data, outperforms GTE-ModernColBERT as well as its base model, GTE-ModernBERT, which leverages closed and much stronger data, setting new state-of-the-art for model this size. We also find that, although performing only a small KD step is not enough to achieve results close to full pre-training, adding a supervised step beforehand allows to achieve much closer performance while skipping the most costly unsupervised phase. Finally, we find that aligning the fine-tuning and pre-training setups is crucial when repurposing existing models. To enable exploration of our results, we release various checkpoints as well as code used to train them.

</details>


### [84] [AREG: Adversarial Resource Extraction Game for Evaluating Persuasion and Resistance in Large Language Models](https://arxiv.org/abs/2602.16639)
*Adib Sakhawat,Fardeen Sadab*

Main category: cs.CL

TL;DR: AREG 使用一种新的基准测试方法评估了 LLMs 的说服和抵抗能力，并发现两者之间存在弱相关性，且防御能力在对抗对话中占优势。


<details>
  <summary>Details</summary>
Motivation: 当前的 LLMs 评估主要基于静态文本生成，而缺乏动态、对抗性的互动评估手段。AREG 旨在填补这一空白，通过引入一种新型的互动框架来全面评估 LLMs 的攻击与防御能力。

Method: AREG 使用了一个基于金融资源的多轮制衡谈判游戏，组织了一个包含前沿模型的循环比赛，评估模型在互动过程中的说服力和抵抗能力。

Result: 研究结果表明：说服和抵抗能力之间存在弱相关性，表明强大的说服能力并不代表着对话中的有效防御；所有测试模型在抵抗任务中的得分普遍高于说服得分，这揭示了防御在对抗对话中的系统性优势。此外，语言分析显示，逐步寻求承诺的策略与更高的资源提取成功率相关，而验证寻求的回应在成功的防御中更为常见。

Conclusion: 研究指出，LLMs 的社会影响力不是单一的能力，现有主要关注说服的评估框架可能忽视了不对称的行为漏洞，建议未来的评估标准需要考虑互动结构对 LLMs 行为结果的潜在影响。

Abstract: Evaluating the social intelligence of Large Language Models (LLMs) increasingly requires moving beyond static text generation toward dynamic, adversarial interaction. We introduce the Adversarial Resource Extraction Game (AREG), a benchmark that operationalizes persuasion and resistance as a multi-turn, zero-sum negotiation over financial resources. Using a round-robin tournament across frontier models, AREG enables joint evaluation of offensive (persuasion) and defensive (resistance) capabilities within a single interactional framework. Our analysis provides evidence that these capabilities are weakly correlated ($ρ= 0.33$) and empirically dissociated: strong persuasive performance does not reliably predict strong resistance, and vice versa. Across all evaluated models, resistance scores exceed persuasion scores, indicating a systematic defensive advantage in adversarial dialogue settings. Further linguistic analysis suggests that interaction structure plays a central role in these outcomes. Incremental commitment-seeking strategies are associated with higher extraction success, while verification-seeking responses are more prevalent in successful defenses than explicit refusal. Together, these findings indicate that social influence in LLMs is not a monolithic capability and that evaluation frameworks focusing on persuasion alone may overlook asymmetric behavioral vulnerabilities.

</details>


### [85] [Quecto-V1: Empirical Analysis of 8-bit Quantized Small Language Models for On-Device Legal Retrieval](https://arxiv.org/abs/2602.16640)
*Subrit Dikshit*

Main category: cs.CL

TL;DR: Quecto-V1 是一个针对印度法律智能的小型语言模型，通过定制化的 GPT-2 架构和严格的量化技术，在保持高度准确性的前提下，有效降低了模型的计算资源需求，使得法律领域的专业知识能够更广泛地利用。


<details>
  <summary>Details</summary>
Motivation: 传统最先进的法律智能系统大多依赖于大规模参数和云服务，这不仅限制了资源紧张环境下的实践应用，还带来了数据主权的风险。Quecto-V1 旨在提供一种专为印度法律领域量身定制的模型，目标是最大限度地提高法律领域的词汇密度，同时通过后训练量化减少模型大小，从而实现法律智能的平民化。

Method: Quecto-V1 使用定制版 GPT-2 架构（124M 参数），仅基于印度法令集进行从零开始的训练，包括印度刑法典、刑事诉讼法和宪法。通过8位后训练量化技术将模型压缩至150MB以下的内存占用，该过程证明了模型大小可以减少74%，同时准确度下降不到原基准的3.5%。

Result: 实证研究显示，Quecto-V1 在法规定义和法定条款检索任务中的精确匹配表现优于通用的小型语言模型，在资源受限的本地设备上运行良好，尤其适合消费者级CPU。

Conclusion: 研究结果表明，在法律等特殊、高风险领域，专门领域的训练结合激进的量化方法提供了一种可行且隐私保护备选方案，相比巨型云模型更为灵活可靠。这项研究强调了在特定领域微调模型和高效量化的价值，以便于在资源有限的环境中部署。

Abstract: The rapid proliferation of Large Language Models (LLMs) has revolutionized Natural Language Processing (NLP) but has simultaneously created a "resource divide." State-of-the-art legal intelligence systems typically rely on massive parameter counts (7B+) and cloud-based inference, rendering them inaccessible to practitioners in resource-constrained environments and posing significant data sovereignty risks. This paper introduces Quecto-V1, a domain-specific Small Language Model (SLM) engineered to democratize access to Indian legal intelligence. Built upon a custom configuration of the GPT-2 architecture (124 million parameters), Quecto-V1 was trained from scratch exclusively on a corpus of Indian statutes, including the Indian Penal Code (IPC), the Code of Criminal Procedure (CrPC), and the Constitution of India. Unlike generalist models, which prioritize broad world knowledge, our approach maximizes "lexical density" within the legal domain. Furthermore, we address the deployment bottleneck by applying post-training 8-bit quantization (GGUF format), compressing the model to a memory footprint of under 150 MB. Our empirical analysis demonstrates that Quecto-V1 achieves high fidelity in retrieving statutory definitions and penal provisions, outperforming general-purpose SLMs in domain-specific exact match tasks while running entirely offline on consumer-grade CPUs. We further present an ablation study showing that 8-bit quantization yields a 74% reduction in model size with less than 3.5% degradation in retrieval accuracy compared to full-precision baselines. These findings suggest that for specialized, high-stakes domains like law, domain-specific training coupled with aggressive quantization offers a viable, privacy-preserving alternative to monolithic cloud models.

</details>


### [86] [Reinforced Fast Weights with Next-Sequence Prediction](https://arxiv.org/abs/2602.16704)
*Hee Seung Hwang,Xindi Wu,Sanghyuk Chun,Olga Russakovsky*

Main category: cs.CL

TL;DR: REFINE通过引入基于next-sequence prediction (NSP)的目标，选择信息性的token位置，生成多token卷积，并使用自监督序列奖励来优化模型，从而显著提升快重权重模型在长上下文建模中的性能。


<details>
  <summary>Details</summary>
Motivation: 当前的快重权重架构受单个词预测（NTP）训练 paradigm 的限制，可能导致模型无法捕捉长范围依赖关系。因此，提出的REFINE旨在改进快重权重模型在长上下文建模中的性能。

Method: REFINE使用next-sequence prediction (NSP) 目标，通过选择信息性的token位置，生成多token预测，并应用自监督的方法以优化模型。

Result: 在实验中，REFINE在各种任务上显著超越了使用NTP进行监督微调的性能，特别是在LaCT-760M和DeltaNet-1.3B上。

Conclusion: REFINE提供了一种有效的、多功能的框架，用以改善快重权重架构在长上下文建模中的表现，适用于预训练语言模型的训练生命周期内的不同阶段。

Abstract: Fast weight architectures offer a promising alternative to attention-based transformers for long-context modeling by maintaining constant memory overhead regardless of context length. However, their potential is limited by the next-token prediction (NTP) training paradigm. NTP optimizes single-token predictions and ignores semantic coherence across multiple tokens following a prefix. Consequently, fast weight models, which dynamically update their parameters to store contextual information, learn suboptimal representations that fail to capture long-range dependencies. We introduce REFINE (Reinforced Fast weIghts with Next sEquence prediction), a reinforcement learning framework that trains fast weight models under the next-sequence prediction (NSP) objective. REFINE selects informative token positions based on prediction entropy, generates multi-token rollouts, assigns self-supervised sequence-level rewards, and optimizes the model with group relative policy optimization (GRPO). REFINE is applicable throughout the training lifecycle of pre-trained language models: mid-training, post-training, and test-time training. Our experiments on LaCT-760M and DeltaNet-1.3B demonstrate that REFINE consistently outperforms supervised fine-tuning with NTP across needle-in-a-haystack retrieval, long-context question answering, and diverse tasks in LongBench. REFINE provides an effective and versatile framework for improving long-context modeling in fast weight architectures.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [87] [How Uncertain Is the Grade? A Benchmark of Uncertainty Metrics for LLM-Based Automatic Assessment](https://arxiv.org/abs/2602.16039)
*Hang Li,Kaiqi Yang,Xianxuan Long,Fedor Filippov,Yucheng Chu,Yasemin Copur-Gencturk,Peng He,Cory Miller,Namsoo Shin,Joseph Krajcik,Hui Liu,Jiliang Tang*

Main category: cs.AI

TL;DR: 该研究评估了一系列不确定性量化方法在基于大语言模型的自动评分中的表现，揭示了不确定性模式，并分析了关键因素对不确定性估计的影响。


<details>
  <summary>Details</summary>
Motivation: 探讨大语言模型在自动评估中输出不确定性所带来的挑战，旨在提高教育场景中自动评分系统的可靠性和有效性。

Method: 通过全面分析多个评估数据集、多个大语言模型家族和生成控制设置下的不确定性行为，评估不同不确定性度量方法的有效性，并分析关键因素如模型家族、评估任务和解码策略对不确定性估计的 影响。

Result: 研究发现不同不确定性度量方法的优缺点，并揭示了大语言模型在评分场景中的不确定性模式。

Conclusion: 提出了针对基于大语言模型的自动评估系统中不确定性特性的见解，为开发更可靠的不确定性意识评分系统奠定基础。

Abstract: The rapid rise of large language models (LLMs) is reshaping the landscape of automatic assessment in education. While these systems demonstrate substantial advantages in adaptability to diverse question types and flexibility in output formats, they also introduce new challenges related to output uncertainty, stemming from the inherently probabilistic nature of LLMs. Output uncertainty is an inescapable challenge in automatic assessment, as assessment results often play a critical role in informing subsequent pedagogical actions, such as providing feedback to students or guiding instructional decisions. Unreliable or poorly calibrated uncertainty estimates can lead to unstable downstream interventions, potentially disrupting students' learning processes and resulting in unintended negative consequences. To systematically understand this challenge and inform future research, we benchmark a broad range of uncertainty quantification methods in the context of LLM-based automatic assessment. Although the effectiveness of these methods has been demonstrated in many tasks across other domains, their applicability and reliability in educational settings, particularly for automatic grading, remain underexplored. Through comprehensive analyses of uncertainty behaviors across multiple assessment datasets, LLM families, and generation control settings, we characterize the uncertainty patterns exhibited by LLMs in grading scenarios. Based on these findings, we evaluate the strengths and limitations of different uncertainty metrics and analyze the influence of key factors, including model families, assessment tasks, and decoding strategies, on uncertainty estimates. Our study provides actionable insights into the characteristics of uncertainty in LLM-based automatic assessment and lays the groundwork for developing more reliable and effective uncertainty-aware grading systems in the future.

</details>


### [88] [Improving Interactive In-Context Learning from Natural Language Feedback](https://arxiv.org/abs/2602.16066)
*Martin Klissarov,Jonathan Cook,Diego Antognini,Hao Sun,Jingling Li,Natasha Jaques,Claudiu Musat,Edward Grefenstette*

Main category: cs.AI

TL;DR: 本文提出了一种框架，旨在将交互式上下文学习作为可训练的能力，通过多轮互动并利用信息不对称性，使模型能够更好地从语言反馈中学习，相较于单轮验证任务，这一方法显著提高了小型模型在困难推理任务上的表现，并且具有跨领域的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前的语言模型训练方法主要依赖于大规模的静态语料库，这对于知识的获取非常有效，但忽视了交互反馈循环的重要性，而这种反馈循环对于模型在不同上下文中的动态适应至关重要。文章旨在通过将交互式学习作为一种可训练的技能来改进当前的语言模型。

Method: 本文提出了一种方法，将单轮验证任务转化为多轮互动教学交互，并通过信息不对称驱动。这一方法的具体技术包括：1. 通过信息不对称性增强多轮交互的模拟；2. 训练模型来预测教师的批评，建立反馈环境的内部模型；3. 通过自我纠正机制使模型能够在没有教师的情况下进行自我改进。

Result: 实验表明，当前的大型语言模型在面对复杂推理任务时难以整合纠错反馈，但是使用本文方法训练的小模型在多轮交互上表现得几乎与大一倍的模型相当。此外，交互式训练在数学问题上的效果能够泛化到代码编写、谜题解决和迷宫导航等多个领域。

Conclusion: 本文提出的方法不仅大幅提高了小型模型的交互学习能力，还展示了跨领域的泛化潜力。最终，通过训练模型来预测和理解反馈环境，这不仅是增强模型性能的有效途径，也为自主改进模型提供了一个统一的方法。

Abstract: Adapting one's thought process based on corrective feedback is an essential ability in human learning, particularly in collaborative settings. In contrast, the current large language model training paradigm relies heavily on modeling vast, static corpora. While effective for knowledge acquisition, it overlooks the interactive feedback loops essential for models to adapt dynamically to their context. In this work, we propose a framework that treats this interactive in-context learning ability not as an emergent property, but as a distinct, trainable skill. We introduce a scalable method that transforms single-turn verifiable tasks into multi-turn didactic interactions driven by information asymmetry. We first show that current flagship models struggle to integrate corrective feedback on hard reasoning tasks. We then demonstrate that models trained with our approach dramatically improve the ability to interactively learn from language feedback. More specifically, the multi-turn performance of a smaller model nearly reaches that of a model an order of magnitude larger. We also observe robust out-of-distribution generalization: interactive training on math problems transfers to diverse domains like coding, puzzles and maze navigation. Our qualitative analysis suggests that this improvement is due to an enhanced in-context plasticity. Finally, we show that this paradigm offers a unified path to self-improvement. By training the model to predict the teacher's critiques, effectively modeling the feedback environment, we convert this external signal into an internal capability, allowing the model to self-correct even without a teacher.

</details>


### [89] [GPSBench: Do Large Language Models Understand GPS Coordinates?](https://arxiv.org/abs/2602.16105)
*Thinh Hung Truong,Jey Han Lau,Jianzhong Qi*

Main category: cs.AI

TL;DR: GPSBench 是一个包含 57,800 个样本的大规模数据集，用于评估 LLM 在地理空间推理方面的表现。研究结果表明，尽管模型在地理知识上表现为逐级下降，但在处理真实地理信息方面比几何计算更可靠，且坐标噪声的稳健性指向了真实坐标理解而非记忆。


<details>
  <summary>Details</summary>
Motivation: 随着 LLM 在与物理世界互动的应用中越来越流行，其地理空间推理能力显得至关重要。然而，现有的 LLM 对 GPS 坐标和现实世界地理的认知能力还尚未被广泛研究，故此项目旨在填补这一空白。

Method: 研究者创建了一个名为 GPSBench 的数据集，包含 17 个不同的任务共计 57,800 个样本，用以评估 LLM 地理空间推理的能力。研究者并未依赖外部工具，而是直接测试模型的内在能力。

Result: 研究发现，尽管模型在地理知识上表现为逐级下降，尤其是在城市级别的定位表现较差，但其在真实世界地理信息的处理上比几何计算更为可靠。此外，坐标噪声的稳健性表明模型具有真实坐标理解能力，而非仅仅是记忆。

Conclusion: 研究结果表明，应对地理坐标信息进行增强训练可以改善下游地理空间任务，但会对几何计算产生一定的影响，表明需要在增强几何计算能力与保留世界知识之间做出权衡。

Abstract: Large Language Models (LLMs) are increasingly deployed in applications that interact with the physical world, such as navigation, robotics, or mapping, making robust geospatial reasoning a critical capability. Despite that, LLMs' ability to reason about GPS coordinates and real-world geography remains underexplored. We introduce GPSBench, a dataset of 57,800 samples across 17 tasks for evaluating geospatial reasoning in LLMs, spanning geometric coordinate operations (e.g., distance and bearing computation) and reasoning that integrates coordinates with world knowledge. Focusing on intrinsic model capabilities rather than tool use, we evaluate 14 state-of-the-art LLMs and find that GPS reasoning remains challenging, with substantial variation across tasks: models are generally more reliable at real-world geographic reasoning than at geometric computations. Geographic knowledge degrades hierarchically, with strong country-level performance but weak city-level localization, while robustness to coordinate noise suggests genuine coordinate understanding rather than memorization. We further show that GPS-coordinate augmentation can improve in downstream geospatial tasks, and that finetuning induces trade-offs between gains in geometric computation and degradation in world knowledge. Our dataset and reproducible code are available at https://github.com/joey234/gpsbench

</details>


### [90] [Revolutionizing Long-Term Memory in AI: New Horizons with High-Capacity and High-Speed Storage](https://arxiv.org/abs/2602.16192)
*Hiroaki Yamanaka,Daisuke Miyashita,Takashi Toi,Asuka Maki,Taiga Ikeda,Jun Deguchi*

Main category: cs.AI

TL;DR: 本文探讨了实现超人工智能（ASI）所需的重要概念“记忆”的设计理念，提出了保留原始体验并按需提取、从大量概率体验中发现更深层次的洞察以及提高体验采集效率等替代方法。实验证明这些方法有效，同时指出了研究中的主要挑战。


<details>
  <summary>Details</summary>
Motivation: 驱动实现超人工智能（ASI）的目标，本文旨在探讨并提出新颖的替代性记忆设计方案，以避免现有‘提取然后存储’方法中信息丢失的风险。

Method: 本文并未提出具体的创新方法，而是讨论了几种潜在有价值的替代方案，这些方案在不同任务中看起来具有效果但未被广泛探索。

Result: 简单的实验证明了这些替代性记忆设计方案的有效性，表明按需提取原始体验、从大量概率体验中发现更深层次的洞察以及提高体验采集效率都具有潜在价值。

Conclusion: 本文强调了在实现超人工智能中的重要挑战，并提议探索这些有前景的方向的研究主题。

Abstract: Driven by our mission of "uplifting the world with memory," this paper explores the design concept of "memory" that is essential for achieving artificial superintelligence (ASI). Rather than proposing novel methods, we focus on several alternative approaches whose potential benefits are widely imaginable, yet have remained largely unexplored. The currently dominant paradigm, which can be termed "extract then store," involves extracting information judged to be useful from experiences and saving only the extracted content. However, this approach inherently risks the loss of information, as some valuable knowledge particularly for different tasks may be discarded in the extraction process. In contrast, we emphasize the "store then on-demand extract" approach, which seeks to retain raw experiences and flexibly apply them to various tasks as needed, thus avoiding such information loss. In addition, we highlight two further approaches: discovering deeper insights from large collections of probabilistic experiences, and improving experience collection efficiency by sharing stored experiences. While these approaches seem intuitively effective, our simple experiments demonstrate that this is indeed the case. Finally, we discuss major challenges that have limited investigation into these promising directions and propose research topics to address them.

</details>


### [91] [Verifiable Semantics for Agent-to-Agent Communication](https://arxiv.org/abs/2602.16424)
*Philipp Schoenegger,Matt Carlson,Chris Schneider,Chris Daly*

Main category: cs.AI

TL;DR: 本文提出了一种基于刺激意义模型的认证协议，通过测试共享可观察事件，对术语进行认证，限制推理基于已认证的术语以减少分歧。实验表明，此方法能有效减少分歧。


<details>
  <summary>Details</summary>
Motivation: 现有的多智能体AI系统缺乏验证方法，确保智能体共享相同的概念理解。本文提出了一种新的认证协议，以解决智能体间由于含义不一致导致的沟通问题。

Method: 本文的认证协议基于刺激-意义模型，通过测试智能体对其所观察到的事件和术语的一致理解程度来认证术语，确保在统计阈值内无显著分歧。对于限制在其核心认证术语范围内进行推理的智能体，该协议能够提供可证明的分歧上限。

Result: 在仿真测试中，采用核心认证推理的智能体之间分歧减少了72-96%；在验证实验中，智能体间的分歧减少了51%。

Conclusion: 本文提供了一种初步框架，旨在使智能体之间的交流更加可靠和可验证，这是迈向可验证多智能体系统操作的关键一步。

Abstract: Multiagent AI systems require consistent communication, but we lack methods to verify that agents share the same understanding of the terms used. Natural language is interpretable but vulnerable to semantic drift, while learned protocols are efficient but opaque. We propose a certification protocol based on the stimulus-meaning model, where agents are tested on shared observable events and terms are certified if empirical disagreement falls below a statistical threshold. In this protocol, agents restricting their reasoning to certified terms ("core-guarded reasoning") achieve provably bounded disagreement. We also outline mechanisms for detecting drift (recertification) and recovering shared vocabulary (renegotiation). In simulations with varying degrees of semantic divergence, core-guarding reduces disagreement by 72-96%. In a validation with fine-tuned language models, disagreement is reduced by 51%. Our framework provides a first step towards verifiable agent-to-agent communication.

</details>


### [92] [Leveraging Large Language Models for Causal Discovery: a Constraint-based, Argumentation-driven Approach](https://arxiv.org/abs/2602.16481)
*Zihao Li,Fabrizio Russo*

Main category: cs.AI

TL;DR: 该研究利用大型语言模型（LLMs）作为不完美的专家，通过从变量名称和描述中提取语义结构先验并将它们与条件独立性证据结合，探索了Causal ABA框架的应用。实验表明，该方法在标准基准数据和语义上合乎逻辑的合成图形上表现优越，并提出了一种评估LLMs因果发现能力的协议，以减轻记忆偏差。


<details>
  <summary>Details</summary>
Motivation: 当前，虽然存在许多统计方法利用观察数据发现因果关系，但在因果发现的任务中，利用专家知识仍然至关重要。然而，传统的专家知识提取方法可能依赖于人工输入，效率较低且容易出错。因此，研究提出了一种利用大型语言模型（LLMs）作为不完美专家的方法，以提高因果发现的效率和准确性。

Method: 利用大型语言模型从变量名称和描述中提取语义结构先验，并将其与条件独立性证据结合，实现有效结合数据和专家知识的Causal ABA框架。

Result: 该方法在标准基准数据和语义上合乎逻辑的合成图形上的实验结果表明，其性能优于现有方法。此外，该研究还提出了评估大型语言模型在因果发现中表现的协议，以减轻记忆偏差。

Conclusion: 总体来说，该研究提出的方法为因果关系的自动发现提供了一种新的思路，并且在实验中展示了较好的效果，但仍需进一步研究来提高其泛化能力。

Abstract: Causal discovery seeks to uncover causal relations from data, typically represented as causal graphs, and is essential for predicting the effects of interventions. While expert knowledge is required to construct principled causal graphs, many statistical methods have been proposed to leverage observational data with varying formal guarantees. Causal Assumption-based Argumentation (ABA) is a framework that uses symbolic reasoning to ensure correspondence between input constraints and output graphs, while offering a principled way to combine data and expertise. We explore the use of large language models (LLMs) as imperfect experts for Causal ABA, eliciting semantic structural priors from variable names and descriptions and integrating them with conditional-independence evidence. Experiments on standard benchmarks and semantically grounded synthetic graphs demonstrate state-of-the-art performance, and we additionally introduce an evaluation protocol to mitigate memorisation bias when assessing LLMs for causal discovery.

</details>


### [93] [Framework of Thoughts: A Foundation Framework for Dynamic and Optimized Reasoning based on Chains, Trees, and Graphs](https://arxiv.org/abs/2602.16512)
*Felix Fricke,Simon Malberg,Georg Groh*

Main category: cs.AI

TL;DR: FoT是一个通用框架，旨在构建和优化动态推理方案，通过内置的超参数调整、提示优化、并行执行和智能缓存功能，提高推理方案的性能。实验表明，使用FoT可以实现更快的执行速度、更低的成本和更高的任务分数。


<details>
  <summary>Details</summary>
Motivation: 当前的推理提示方案通常需要用户定义静态、问题特定的推理结构，限制了对动态或未见问题类型的适应性。此外，它们在超参数、提示、运行时间和提示成本上往往未得到优化。

Method: 提出FoT框架，包含内置的超参数调优、提示优化、并行执行和智能缓存功能。在FoT中实现了三种流行的推理方案：Tree of Thoughts、Graph of Thoughts和ProbTree。

Result: 实验表明，使用FoT可以使推理方案的执行速度显著提升，成本降低，并且在任务评分上有所改善。

Conclusion: FoT为构建和优化动态和高效的推理方案提供了一个通用框架，具有广泛的应用前景。

Abstract: Prompting schemes such as Chain of Thought, Tree of Thoughts, and Graph of Thoughts can significantly enhance the reasoning capabilities of large language models. However, most existing schemes require users to define static, problem-specific reasoning structures that lack adaptability to dynamic or unseen problem types. Additionally, these schemes are often under-optimized in terms of hyperparameters, prompts, runtime, and prompting cost. To address these limitations, we introduce Framework of Thoughts (FoT)--a general-purpose foundation framework for building and optimizing dynamic reasoning schemes. FoT comes with built-in features for hyperparameter tuning, prompt optimization, parallel execution, and intelligent caching, unlocking the latent performance potential of reasoning schemes. We demonstrate FoT's capabilities by implementing three popular schemes--Tree of Thoughts, Graph of Thoughts, and ProbTree--within FoT. We empirically show that FoT enables significantly faster execution, reduces costs, and achieves better task scores through optimization. We release our codebase to facilitate the development of future dynamic and efficient reasoning schemes.

</details>


### [94] [Creating a digital poet](https://arxiv.org/abs/2602.16578)
*Vered Tohar,Tsahi Hayat,Amir Leshem*

Main category: cs.AI

TL;DR: 研究通过持续数月的手工调整，使大语言模型生成具有独特风格和连贯作品集的诗歌，并进行盲测后与知名诗人作品难分伯仲，最终出版了一本机器人心智能独立成作的诗集。


<details>
  <summary>Details</summary>
Motivation: 讨论机器能否创作诗歌这一话题，引发了关于艺术的本质和价值的深刻思考。此研究旨在探索长期渐进式指导是否能培养出具有创造性的AI。

Method: 实验队伍对大语言模型进行了长达七个月的迭代性专家反馈指导，没有重新训练模型。期间，模型形成了自己的独特风格，并产生了一批符合标准的诗歌作品。

Result: 经过盲测，参与者无法区分机器所作的诗歌与知名诗人所作的诗歌，语音作品和机器作品的识别率为50%左右。最终，目标模型出版了一本诗集。

Conclusion: 研究表明，作坊式指导可以长期培养机器的创造力，同时也引发了关于创造力和作者身份的新辩论。

Abstract: Can a machine write good poetry? Any positive answer raises fundamental questions about the nature and value of art. We report a seven-month poetry workshop in which a large language model was shaped into a digital poet through iterative in-context expert feedback, without retraining. Across sessions, the model developed a distinctive style and a coherent corpus, supported by quantitative and qualitative analyses, and it produced a pen name and author image. In a blinded authorship test with 50 humanities students and graduates (three AI poems and three poems by well-known poets each), judgments were at chance: human poems were labeled human 54% of the time and AI poems 52%, with 95% confidence intervals including 50%. After the workshop, a commercial publisher released a poetry collection authored by the model. These results show that workshop-style prompting can support long-horizon creative shaping and renew debates on creativity and authorship.

</details>


### [95] [Agent Skill Framework: Perspectives on the Potential of Small Language Models in Industrial Environments](https://arxiv.org/abs/2602.16653)
*Yangjie Xu,Lujun Li,Lama Sleem,Niccolo Gentile,Yewei Song,Yiqun Wang,Siming Ji,Wenbo Wu,Radu State*

Main category: cs.AI

TL;DR: 该研究探讨了Agent Skill框架是否能为小型语言模型（SLMs）带来相似的改进效果，尤其是在数据安全和预算限制的情况下无法持续依赖公共API的工业场景。实验结果显示，小型模型难以可靠地选择技能，而中等大小的SLMs则能显著受益；而代码专用的大型模型在保持性能的同时还提升了GPU效率。


<details>
  <summary>Details</summary>
Motivation: 在数据安全和预算限制的情况下，工业企业无法持续依赖公共API，因此该研究旨在验证Agent Skill框架是否能在小型语言模型中提供与大型模型相似的改进效果。

Method: 研究首先提出了Agent Skill过程的正式数学定义，然后系统性地评估了不同规模的语言模型在多种应用案例中的表现，具体包括两个开源任务和一个真实世界的保险理赔数据集。

Result: 实验结果表明，小型模型在可靠地选择技能方面表现不足，相比之下，中等规模的SLMs能显著受益于Agent Skill方法；同时，大约800亿参数的代码专用模型在保持与封闭源基线相当的性能的同时提高了GPU使用效率。

Conclusion: 研究最终提供了框架在能力及限制方面的全面、细致描述，并为在以小型语言模型为核心环境中的有效部署Agent Skills提供了实用的见解。

Abstract: Agent Skill framework, now widely and officially supported by major players such as GitHub Copilot, LangChain, and OpenAI, performs especially well with proprietary models by improving context engineering, reducing hallucinations, and boosting task accuracy. Based on these observations, an investigation is conducted to determine whether the Agent Skill paradigm provides similar benefits to small language models (SLMs). This question matters in industrial scenarios where continuous reliance on public APIs is infeasible due to data-security and budget constraints requirements, and where SLMs often show limited generalization in highly customized scenarios. This work introduces a formal mathematical definition of the Agent Skill process, followed by a systematic evaluation of language models of varying sizes across multiple use cases. The evaluation encompasses two open-source tasks and a real-world insurance claims data set. The results show that tiny models struggle with reliable skill selection, while moderately sized SLMs (approximately 12B - 30B) parameters) benefit substantially from the Agent Skill approach. Moreover, code-specialized variants at around 80B parameters achieve performance comparable to closed-source baselines while improving GPU efficiency. Collectively, these findings provide a comprehensive and nuanced characterization of the capabilities and constraints of the framework, while providing actionable insights for the effective deployment of Agent Skills in SLM-centered environments.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [96] [Bit-Width-Aware Design Environment for Few-Shot Learning on Edge AI Hardware](https://arxiv.org/abs/2602.16024)
*R. Kanda,H. L. Blevec,N. Onizawa,M. Leonardon,V. Gripon,T. Hanyu*

Main category: cs.AR

TL;DR: 该研究提出了一种在PyNN-QZ1等小型FPGA SoC上实现实时少样本学习的方法，使用FINN框架支持任意位宽，通过优化和调整提高了吞吐量。


<details>
  <summary>Details</summary>
Motivation: 传统基于Tensor的硬件实现受限于固定的16或32位位宽，本研究采用FINN框架，允许实现任意位宽，解决了这一限制。

Method: 使用FINN框架进行实时少样本学习的硬件实现，通过对Transpose节点的优化和Final Reduce Mean操作的处理，实现了不同位宽的适配。

Result: 通过这些调整，研究实现了与传统实现相同精度的同时，使用CIFAR-10数据集的吞吐量提升至约两倍。

Conclusion: 该研究展示了在小型FPGA SoC上实现实时少样本学习的可能性，并通过位宽的灵活性提高了性能。

Abstract: In this study, we propose an implementation methodology of real-time few-shot learning on tiny FPGA SoCs such as the PYNQ-Z1 board with arbitrary fixed-point bit-widths. Tensil-based conventional design environments limited hardware implementations to fixed-point bit-widths of 16 or 32 bits. To address this, we adopt the FINN framework, enabling implementations with arbitrary bit-widths. Several customizations and minor adjustments are made, including: 1.Optimization of Transpose nodes to resolve data format mismatches, 2.Addition of handling for converting the final reduce mean operation to Global Average Pooling (GAP). These adjustments allow us to reduce the bit-width while maintaining the same accuracy as the conventional realization, and achieve approximately twice the throughput in evaluations using CIFAR-10 dataset.

</details>


### [97] [Energy-Efficient p-Bit-Based Fully-Connected Quantum-Inspired Simulated Annealer with Dual BRAM Architecture](https://arxiv.org/abs/2602.16143)
*Naoya Onizawa,Taiga Kubuta,Duckgyu Shin,Takahiro Hanyu*

Main category: cs.AR

TL;DR: 该研究提出了一种基于FPGA的高效Simulated Quantum Annealing (SQA)架构，有效解决了现有基于p-bit的模拟退火加速器在可扩展性和支持完全连接图方面的问题。该设计通过使用一种结合串行自旋和并行副本更新方案，以及双重BRAM延迟线架构，实现了对完全连接的Ising模型的支持，同时减少了逻辑资源使用。实验结果显示，与之前的基于p-bit的FPGA模拟退火架构相比，该系统在能耗和逻辑资源使用方面分别降低了50%和超过了90%。


<details>
  <summary>Details</summary>
Motivation: 现有的基于p-bit的模拟退火加速器在处理大规模组合最优化问题时存在可扩展性差和内存开销大的问题，尤其是在支持全面连接图方面表现不佳。通过研究和设计一种新的架构，能够有效解决这些问题，提高能耗效率和逻辑资源利用率。

Method: 该方法结合了串行自旋和并行副本更新策略，并通过双重BRAM延迟线架构来实现可扩展的计算能力，同时减少逻辑资源需求。通过对完全连接的Ising模型的处理，加速了模拟量子退火过程的收敛速度，并且显著减少了存储需求。

Result: 该架构在Xilinx ZC706 FPGA上实现时，能够高效解决包含800个节点的最大割问题。相比之前的基于p-bit的FPGA模拟退火架构，该设计在能耗和逻辑资源使用上分别实现了高达50%和90%的性能提升。

Conclusion: 该研究证明了融合量子启发技术的p-bit加速硬件在重型组合优化问题中的实用性和能量效率优势。

Abstract: Probabilistic bits (p-bits) offer an energy-efficient hardware abstraction for stochastic optimization; however, existing p-bit-based simulated annealing accelerators suffer from poor scalability and limited support for fully connected graphs due to fan-out and memory overhead. This paper presents an energy-efficient FPGA architecture for stochastic simulated quantum annealing (SSQA) that addresses these challenges. The proposed design combines a spin-serial and replica-parallel update schedule with a dual-BRAM delay-line architecture, enabling scalable support for fully connected Ising models while eliminating fan-out growth in logic resources. By exploiting SSQA, the architecture achieves fast convergence using only final replica states, significantly reducing memory requirements compared to conventional p-bit-based annealers. Implemented on a Xilinx ZC706 FPGA, the proposed system solves an 800-node MAX-CUT benchmark and achieves up to 50% reduction in energy consumption and over 90\% reduction in logic resources compared with prior FPGA-based p-bit annealing architectures. These results demonstrate the practicality of quantum-inspired, p-bit-based annealing hardware for large-scale combinatorial optimization under strict energy and resource constraints.

</details>
